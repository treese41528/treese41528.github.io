.. _ch4_7-bootstrap-confidence-intervals:

============================================================
Section 4.7 Bootstrap Confidence Intervals: Advanced Methods
============================================================

In :ref:`Section 4.3 <ch4_3-nonparametric-bootstrap>`, we introduced three basic bootstrap confidence interval methods: the percentile interval, the basic (pivotal) interval, and the normal approximation. These methods are intuitive, easy to implement, and often adequate for exploratory analysis. However, they share a fundamental limitation: their coverage accuracy degrades in predictable ways when the bootstrap distribution is skewed, when the estimator is biased, or when the standard error varies with the parameter value. This section develops **advanced bootstrap interval methods**---the studentized (bootstrap-t) interval and the bias-corrected and accelerated (BCa) interval---that achieve improved coverage accuracy by addressing these sources of error systematically.

The distinction between "first-order" and "second-order" accurate methods has profound practical consequences. For a nominal 95% confidence interval at moderate sample sizes (often n ‚âà 30‚Äì100), first order methods can exhibit noticeable undercoverage in skewed or biased settings, especially for one sided inference; second order methods typically reduce this error substantially. The magnitude depends on the functional and the underlying distribution.
Throughout this section, we build on tools developed earlier: the jackknife estimates :math:`\hat{\theta}_{(-i)}` from :ref:`Section 4.5 <ch4_5-jackknife-methods>` are essential for computing the BCa acceleration constant, and the bootstrap distribution :math:`\{\hat{\theta}^*_b\}` from :ref:`Section 4.3 <ch4_3-nonparametric-bootstrap>` remains our primary computational object. We also address a practical concern that pervades all bootstrap inference: **Monte Carlo error** from using finitely many bootstrap replicates :math:`B`. Understanding and controlling this error is essential for reporting results responsibly.

.. admonition:: Road Map üß≠
   :class: important

   - **Understand**: Why percentile and basic intervals have coverage deficiencies, and how Edgeworth expansions characterize coverage error rates
   - **Develop**: The studentized bootstrap and BCa intervals as two routes to second-order accuracy
   - **Implement**: Complete Python implementations with proper numerical handling of edge cases
   - **Evaluate**: Diagnostic tools for method selection and Monte Carlo error quantification


Why Advanced Methods?
---------------------

Before developing sophisticated interval constructions, we must understand *why* simple methods fall short. The answer lies in the structure of sampling distributions and how bootstrap approximations inherit---or fail to correct---systematic errors.


The Coverage Probability Target
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A confidence interval procedure produces a random interval :math:`[\hat{L}_n, \hat{U}_n]` that depends on the observed data :math:`X_1, \ldots, X_n`. The **coverage probability** is the chance that this random interval contains the true parameter:

.. math::
   :label: coverage-def

   C_n(F) = P_F\bigl(\theta \in [\hat{L}_n, \hat{U}_n]\bigr)

where the probability is taken over the sampling distribution of the data under the true distribution :math:`F`. For a nominal :math:`(1-\alpha)` confidence interval, we want :math:`C_n(F) = 1 - \alpha` for all :math:`F` in some class of interest. In practice, we settle for :math:`C_n(F) \approx 1 - \alpha` when :math:`n` is large.

The **coverage error** is the difference between actual and nominal coverage:

.. math::

   \text{Coverage Error} = C_n(F) - (1 - \alpha)

A positive coverage error means the interval is conservative (over-covers); a negative error means the interval is liberal (under-covers). Neither is desirable, but under-coverage is typically more problematic since it leads to overconfident inference.

.. admonition:: Coverage Is Not Random
   :class: tip

   A common confusion: coverage probability :math:`C_n(F)` is a *fixed number* for given :math:`F` and :math:`n`, not a random variable. Once we specify the data-generating distribution :math:`F`, the coverage probability is determined by integrating over all possible samples. The big-:math:`O` notation we use below describes how this fixed quantity behaves as :math:`n \to \infty`.

.. figure:: https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_7_fig01_coverage_concept.png
   :alt: Coverage Probability Concept
   :align: center
   :width: 95%

   **Figure 4.7.1**: Coverage probability visualized. (a) A single confidence interval either contains (‚úì) or misses (‚úó) the true parameter---we cannot know which from the data alone. (b) Over many repetitions from the same population, the coverage probability is the long-run proportion of intervals that contain the truth. (c) The formal definition: coverage is the probability that the random interval captures the fixed but unknown parameter.


Edgeworth Expansions and Coverage Error Rates
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The key theoretical tool for understanding coverage error is the **Edgeworth expansion**, which refines the Central Limit Theorem by providing correction terms that capture departures from normality at finite sample sizes.

For a standardized statistic :math:`Z_n = \sqrt{n}(\hat{\theta} - \theta)/\sigma`, the distribution function admits an expansion of the form:

.. math::
   :label: edgeworth-basic

   P(Z_n \leq z) = \Phi(z) + n^{-1/2} p_1(z)\phi(z) + n^{-1} p_2(z)\phi(z) + O(n^{-3/2})

where :math:`\Phi` and :math:`\phi` are the standard normal CDF and PDF, and :math:`p_1, p_2` are polynomials whose coefficients depend on the moments of the underlying distribution. Crucially:

- :math:`p_1(z)` is an **odd polynomial** (involving skewness)
- :math:`p_2(z)` is an **even polynomial** (involving kurtosis and skewness squared)

This structure has profound implications for confidence interval coverage.

**One-sided intervals**: For a one-sided interval of the form :math:`(-\infty, \hat{\theta} + z_{1-\alpha} \cdot \hat{\sigma}/\sqrt{n}]`, the coverage error is dominated by the :math:`p_1` term:

.. math::

   C_n(F) = 1 - \alpha + c_1 n^{-1/2} + O(n^{-1})

The leading error term is :math:`O(n^{-1/2})`, which can be substantial for moderate :math:`n`.

**Two-sided intervals**: For symmetric two-sided intervals, the odd polynomial :math:`p_1` contributes errors of opposite sign at the two endpoints, and these partially cancel:

.. math::

   C_n(F) = 1 - \alpha + c_2 n^{-1} + O(n^{-3/2})

The leading error is :math:`O(n^{-1})`, an order of magnitude smaller than for one-sided intervals.

This asymmetry between one-sided and two-sided coverage is a fundamental feature of confidence interval theory. Methods that achieve :math:`O(n^{-1})` coverage error for *both* one-sided and two-sided intervals are called **second-order accurate**.

Note: symmetric two sided intervals can achieve O(n^{-1}) error via cancellation even when the underlying one sided error remains O(n^{-1/2}); ‚Äòsecond order accurate‚Äô here refers to constructions that remove the O(n^{-1/2}) term for one sided coverage as well.


Why Percentile and Basic Intervals Are First-Order
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The percentile interval :math:`[Q_{\alpha/2}(\hat{\theta}^*), Q_{1-\alpha/2}(\hat{\theta}^*)]` uses quantiles of the bootstrap distribution directly. While this captures the shape of the sampling distribution, it inherits systematic errors:

1. **Bias drift**: If :math:`\mathbb{E}[\hat{\theta}] \neq \theta`, the bootstrap distribution is centered at :math:`\hat{\theta}` rather than :math:`\theta`, causing the interval to drift.

2. **Skewness mismatch**: The bootstrap distribution of :math:`\hat{\theta}^* - \hat{\theta}` estimates the distribution of :math:`\hat{\theta} - \theta`, but the quantiles don't align perfectly when these distributions are skewed.

The basic interval :math:`[2\hat{\theta} - Q_{1-\alpha/2}(\hat{\theta}^*), 2\hat{\theta} - Q_{\alpha/2}(\hat{\theta}^*)]` partially corrects the bias drift by reflecting quantiles about :math:`\hat{\theta}`, but it doesn't address skewness.

For smooth functionals under standard regularity conditions, one sided percentile and basic intervals typically have coverage error :math:`O(n^{-1/2})`. For symmetric **two-sided** intervals, odd order terms partially cancel, often yielding :math:`O(n^{-1})` coverage error. This is why these methods often perform reasonably for two-sided intervals but can have serious coverage problems for one-sided intervals.

.. admonition:: The Goal of Advanced Methods
   :class: important

   Advanced bootstrap interval methods aim to achieve :math:`O(n^{-1})` coverage error for **both** one-sided and two-sided intervals. There are two main approaches:

   1. **Studentization**: Pivot by an estimated standard error, transforming the problem so that the leading odd term in the Edgeworth expansion vanishes.

   2. **Transformation correction (BCa)**: Find quantile adjustments that automatically account for bias and skewness, achieving transformation invariance.

.. figure:: https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_7_fig02_coverage_error_rates.png
   :alt: Coverage Error Rates
   :align: center
   :width: 95%

   **Figure 4.7.2**: Coverage error rates from Edgeworth expansions. (a) The first correction term :math:`p_1(z)\phi(z)` is an odd function, contributing :math:`O(n^{-1/2})` error for one-sided intervals but canceling for two-sided intervals. (b) Comparison of coverage error magnitude: first-order methods (Percentile, Basic, BC) vs second-order methods (BCa, Studentized). (c) Simulation study confirming the theoretical rates---second-order methods approach nominal coverage faster.


Practical Implications
~~~~~~~~~~~~~~~~~~~~~~

To make these abstract rates concrete, consider the following rough guide for nominal 95% two-sided intervals:

.. list-table:: Typical Coverage by Sample Size and Method
   :header-rows: 1
   :widths: 20 20 20 20 20

   * - :math:`n`
     - Normal Approx
     - Percentile
     - BCa
     - Studentized
   * - 20
     - 88--92%
     - 89--93%
     - 93--95%
     - 93--95%
   * - 50
     - 92--94%
     - 92--94%
     - 94--95%
     - 94--95%
   * - 100
     - 93--95%
     - 93--95%
     - 94.5--95%
     - 94.5--95%
   * - 500
     - 94.5--95%
     - 94.5--95%
     - ~95%
     - ~95%

These are illustrative ranges; actual coverage depends on the statistic and underlying distribution. The key message: for moderate sample sizes, the difference between first-order and second-order methods can be the difference between 91% and 94% actual coverage for a nominal 95% interval.


The Studentized (Bootstrap-t) Interval
--------------------------------------

The studentized bootstrap, also called the **bootstrap-t** method, extends the classical Student's t approach to general statistics. The key insight is that dividing by an estimated standard error creates a more **pivotal** quantity---one whose distribution depends less on unknown parameters---and this pivotality translates directly into improved coverage accuracy.


From Student's t to Bootstrap-t
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Recall the classical setting: for iid observations :math:`X_1, \ldots, X_n` from a normal distribution with unknown mean :math:`\mu` and variance :math:`\sigma^2`, the pivot

.. math::

   T = \frac{\bar{X} - \mu}{S/\sqrt{n}}

follows a :math:`t_{n-1}` distribution *regardless of the values of* :math:`\mu` *and* :math:`\sigma^2`. This pivotality means that :math:`t`-based intervals have exact coverage under normality.

For a general statistic :math:`\hat{\theta}` estimating :math:`\theta`, we can form an analogous studentized quantity:

.. math::
   :label: studentized-pivot

   T = \frac{\hat{\theta} - \theta}{\hat{\sigma}}

where :math:`\hat{\sigma}` is an estimate of the standard error of :math:`\hat{\theta}`. Unlike the normal-theory case, :math:`T` doesn't have a known distribution---but the bootstrap can estimate it.


The Bootstrap-t Algorithm
~~~~~~~~~~~~~~~~~~~~~~~~~

The bootstrap-t method estimates the distribution of :math:`T` by computing its bootstrap analog :math:`T^*` across many resamples.

**Algorithm: Studentized Bootstrap Confidence Interval**

.. code-block:: text

   Input: Data X‚ÇÅ,...,X‚Çô; statistic T; SE estimator; replicates B; level Œ±
   Output: (1-Œ±) studentized bootstrap CI

   1. Compute Œ∏ÃÇ = T(X‚ÇÅ,...,X‚Çô) and œÉÃÇ = SE(X‚ÇÅ,...,X‚Çô)
   2. For b = 1,...,B:
      a. Draw bootstrap sample X*‚ÇÅ,...,X*‚Çô (with replacement)
      b. Compute Œ∏ÃÇ*_b = T(X*‚ÇÅ,...,X*‚Çô)
      c. Compute œÉÃÇ*_b = SE(X*‚ÇÅ,...,X*‚Çô)  [using same SE method]
      d. Compute t*_b = (Œ∏ÃÇ*_b - Œ∏ÃÇ) / œÉÃÇ*_b
   3. Let q*_{Œ±/2} and q*_{1-Œ±/2} be the Œ±/2 and (1-Œ±/2) quantiles of {t*_b}
   4. Return CI: [Œ∏ÃÇ - œÉÃÇ ¬∑ q*_{1-Œ±/2}, Œ∏ÃÇ - œÉÃÇ ¬∑ q*_{Œ±/2}]

The crucial step is computing :math:`\hat{\sigma}^*_b` **within each bootstrap sample**. This allows the bootstrap distribution of :math:`T^*` to capture how the studentized statistic varies, including any relationship between :math:`\hat{\theta}` and its standard error.

.. admonition:: Why the Quantiles Reverse
   :class: note

   The interval formula subtracts :math:`q^*_{1-\alpha/2}` to get the *lower* bound and :math:`q^*_{\alpha/2}` to get the *upper* bound. This reversal arises from inverting the pivot relationship:

   .. math::

      P\left(q^*_{\alpha/2} \leq \frac{\hat{\theta} - \theta}{\hat{\sigma}} \leq q^*_{1-\alpha/2}\right) \approx 1 - \alpha

   Solving for :math:`\theta`:

   .. math::

      P\left(\hat{\theta} - \hat{\sigma} q^*_{1-\alpha/2} \leq \theta \leq \hat{\theta} - \hat{\sigma} q^*_{\alpha/2}\right) \approx 1 - \alpha


Why Studentization Achieves Second-Order Accuracy
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The theoretical advantage of studentization can be understood through Edgeworth expansions. For a non-studentized statistic :math:`\sqrt{n}(\hat{\theta} - \theta)/\sigma`, the leading correction term :math:`p_1(z)` in :eq:`edgeworth-basic` is an odd polynomial that contributes :math:`O(n^{-1/2})` error to one-sided coverage.

For the studentized statistic :math:`T = \sqrt{n}(\hat{\theta} - \theta)/\hat{\sigma}`, the Edgeworth expansion has the form:

.. math::

   P(T \leq t) = \Phi(t) + n^{-1} q_2(t)\phi(t) + O(n^{-3/2})

The :math:`n^{-1/2}` term with an odd polynomial is **absent** (or its coefficient is zero). This occurs because studentization absorbs the leading skewness correction into the variance estimation. The technical conditions for this improvement, established by Hall (1988, 1992), include:

1. Finite fourth moments: :math:`\mathbb{E}|X|^{4+\delta} < \infty` for some :math:`\delta > 0`
2. Cram√©r's condition on the characteristic function (ensuring non-lattice distributions)
3. Sufficient smoothness of :math:`\hat{\theta}` as a function of sample moments
4. Consistent variance estimation: :math:`\hat{\sigma}/\sigma \xrightarrow{p} 1`

Under these conditions, the studentized bootstrap achieves coverage error :math:`O(n^{-1})` for **both** one-sided and two-sided intervals---a full order of magnitude improvement over percentile and basic methods for one-sided intervals.

.. figure:: https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_7_fig03_studentized_bootstrap.png
   :alt: Studentized Bootstrap Method
   :align: center
   :width: 95%

   **Figure 4.7.3**: The studentized (bootstrap-t) method. (a) Comparison of non-studentized vs studentized bootstrap statistics---studentization produces a distribution closer to the reference :math:`t` distribution. (b) The bootstrap :math:`T^*` distribution with quantiles used for interval construction. (c-d) Explanation of quantile reversal and practical guidance. (e) Confidence interval comparison across methods for normal data. (f) Coverage simulation on exponential data demonstrating studentized method's advantage for skewed distributions.


Methods for Estimating SE Within Bootstrap Samples
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The practical challenge of bootstrap-t is computing :math:`\hat{\sigma}^*_b` for each bootstrap sample. Several approaches exist, with different trade-offs:

**Option 1: Analytical (Plug-in) SE**

When a closed-form standard error formula exists, apply it to each bootstrap sample.

*Example*: For the sample mean, :math:`\hat{\sigma}^*_b = s^*_b/\sqrt{n}` where :math:`s^*_b` is the sample standard deviation of the bootstrap sample.

*Example*: For OLS regression with homoscedastic errors, use :math:`\hat{\sigma}^*_{\hat{\beta}} = \hat{\sigma}^*_\epsilon \sqrt{(X^{*\top}X^*)^{-1}_{jj}}`.

**Advantages**: Fast (:math:`O(B)` total complexity), exact within model assumptions.

**Disadvantages**: Requires known formula; may be inaccurate under model misspecification (e.g., heteroscedasticity).

**Option 2: Sandwich (Robust) SE**

For regression problems, use heteroscedasticity-consistent standard errors within each bootstrap sample:

.. math::

   \widehat{\text{Var}}(\hat{\beta}^*) = (X^{*\top}X^*)^{-1} \left(\sum_{i=1}^n (e^*_i)^2 x^*_i x^{*\top}_i\right) (X^{*\top}X^*)^{-1}

This provides robustness to heteroscedasticity while maintaining analytical tractability.

**Option 3: Jackknife SE Within Bootstrap Samples**

Use the delete-1 jackknife (from :ref:`Section 4.5 <ch4_5-jackknife-methods>`) to estimate :math:`\hat{\sigma}^*_b`:

.. math::

   \hat{\sigma}^*_b = \sqrt{\frac{n-1}{n} \sum_{i=1}^n \left(\hat{\theta}^*_{b,(-i)} - \bar{\theta}^*_{b,(\cdot)}\right)^2}

where :math:`\hat{\theta}^*_{b,(-i)}` is the statistic computed on bootstrap sample :math:`b` with observation :math:`i` removed.

**Advantages**: Works for any statistic; no formula needed.

**Disadvantages**: Cost is :math:`O(B \cdot n)` statistic evaluations; may be unstable for non-smooth statistics.

**Option 4: Nested Bootstrap**

For each outer bootstrap sample :math:`b`, draw :math:`M` inner bootstrap samples and compute :math:`\hat{\sigma}^*_b` as the standard deviation of the inner bootstrap estimates.

**Advantages**: Most general; works for any statistic.

**Disadvantages**: Cost is :math:`O(B \cdot M)` statistic evaluations; can be prohibitive for expensive statistics.

**Practical Guidance**: Use analytical or sandwich SE when available and trustworthy. Use jackknife SE as a general-purpose fallback. Reserve nested bootstrap for non-smooth statistics where jackknife is unreliable.


Python Implementation
~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   import numpy as np
   from scipy import stats

   def studentized_bootstrap_ci(data, statistic, se_function, alpha=0.05, 
                                 B=5000, seed=None):
       """
       Compute studentized (bootstrap-t) confidence interval.

       Parameters
       ----------
       data : array_like
           Original data (1D array for simple case).
       statistic : callable
           Function computing the statistic: statistic(data) -> float.
       se_function : callable
           Function computing SE estimate: se_function(data) -> float.
       alpha : float
           Significance level (default 0.05 for 95% CI).
       B : int
           Number of bootstrap replicates.
       seed : int, optional
           Random seed for reproducibility.

       Returns
       -------
       ci : tuple
           (lower, upper) confidence bounds.
       theta_hat : float
           Original point estimate.
       info : dict
           Additional information including t* distribution.
       """
       rng = np.random.default_rng(seed)
       data = np.asarray(data)
       n = len(data)

       # Original estimates
       theta_hat = statistic(data)
       se_hat = se_function(data)

       # Bootstrap
       t_star = np.empty(B)
       theta_star = np.empty(B)

       for b in range(B):
           # Resample with replacement
           idx = rng.integers(0, n, size=n)
           data_star = data[idx]

           # Compute statistic and SE on bootstrap sample
           theta_star[b] = statistic(data_star)
           se_star = se_function(data_star)

           # Studentized statistic (guard against zero SE)
           if se_star > 1e-10:
               t_star[b] = (theta_star[b] - theta_hat) / se_star
           else:
               t_star[b] = np.nan  # Degenerate case

       # Quantiles of t* distribution
       q_lo = np.quantile(t_star, alpha / 2)
       q_hi = np.quantile(t_star, 1 - alpha / 2)

       # Confidence interval (note the reversal)
       ci_lower = theta_hat - se_hat * q_hi
       ci_upper = theta_hat - se_hat * q_lo

       info = {
           't_star': t_star,
           'theta_star': theta_star,
           'se_hat': se_hat,
           'q_lo': q_lo,
           'q_hi': q_hi
       }

       return (ci_lower, ci_upper), theta_hat, info


   def jackknife_se(data, statistic):
       """
       Compute jackknife standard error for use in studentized bootstrap.

       Parameters
       ----------
       data : array_like
           Data array.
       statistic : callable
           Function computing the statistic.

       Returns
       -------
       se : float
           Jackknife standard error estimate.
       """
       data = np.asarray(data)
       n = len(data)

       # Leave-one-out estimates
       theta_jack = np.empty(n)
       for i in range(n):
           data_minus_i = np.delete(data, i)
           theta_jack[i] = statistic(data_minus_i)

       # Jackknife SE formula
       theta_bar = theta_jack.mean()
       se = np.sqrt((n - 1) / n * np.sum((theta_jack - theta_bar)**2))

       return se


.. admonition:: Example üí° Studentized Bootstrap for Correlation Coefficient
    :class: note

    **Given:** Paired observations :math:`(X_i, Y_i)` for :math:`i = 1, \ldots, 30` from a bivariate distribution.

    **Find:** A 95% confidence interval for the population correlation :math:`\rho`.

    **Challenge:** The sampling distribution of :math:`\hat{\rho}` is skewed, especially when :math:`|\rho|` is large, and the standard error depends on :math:`\rho` itself.

    **Python implementation:**

    .. code-block:: python
        
        import numpy as np
        from scipy import stats

        # Generate example data with true correlation rho = 0.7
        rng = np.random.default_rng(42)
        n = 30
        rho_true = 0.7

        # Generate from a bivariate normal
        mean = [0, 0]
        cov = [[1, rho_true], [rho_true, 1]]
        data = rng.multivariate_normal(mean, cov, size=n)
        x, y = data[:, 0], data[:, 1]

        # Combine into single array for resampling (pairs bootstrap)
        xy = np.column_stack([x, y])

        def correlation(xy_data):
            """Compute Pearson correlation on the r scale."""
            return np.corrcoef(xy_data[:, 0], xy_data[:, 1])[0, 1]

        def correlation_fisher_z(xy_data):
            """Compute Fisher z = atanh(r), where r is Pearson correlation."""
            r = correlation(xy_data)
            # Clip to avoid infinities at |r| = 1 due to finite precision
            r = np.clip(r, -0.999999999, 0.999999999)
            return np.arctanh(r)

        def correlation_se_fisher_z(xy_data):
            """SE of Fisher z: Var(z) ‚âà 1/(n-3)."""
            n_local = len(xy_data)
            return 1 / np.sqrt(n_local - 3) if n_local > 3 else np.nan

        # Studentized bootstrap CI on Fisher z scale, then transform back to r scale
        ci_stud_z, z_hat, info = studentized_bootstrap_ci(
            xy,
            correlation_fisher_z,
            correlation_se_fisher_z,
            alpha=0.05,
            B=10000,
            seed=123
        )

        ci_stud_r = (np.tanh(ci_stud_z[0]), np.tanh(ci_stud_z[1]))
        r_hat = correlation(xy)

        print(f"Sample correlation (r): {r_hat:.4f}")
        print(f"True correlation:       {rho_true}")
        print(f"95% Studentized CI (r): [{ci_stud_r[0]:.4f}, {ci_stud_r[1]:.4f}]")

        # Compare with percentile CI on r scale
        theta_star_z = info["theta_star"]
        theta_star_r = np.tanh(theta_star_z)
        ci_perc_r = (np.quantile(theta_star_r, 0.025), np.quantile(theta_star_r, 0.975))
        print(f"95% Percentile CI (r):  [{ci_perc_r[0]:.4f}, {ci_perc_r[1]:.4f}]")


    **Result:** The studentized interval properly accounts for the asymmetric sampling distribution of the correlation coefficient, while the percentile interval may have coverage issues, particularly when the true correlation is far from zero.


Diagnostics for Studentized Bootstrap
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Before trusting a studentized bootstrap interval, examine the distribution of :math:`t^*`:

1. **Shape**: The :math:`t^*` distribution should be unimodal and roughly symmetric. Compare with a standard normal or :math:`t` distribution.

2. **Outliers**: Extreme :math:`t^*` values often indicate bootstrap samples where :math:`\hat{\sigma}^*_b \approx 0`. These can distort quantiles.

3. **Stability**: Verify that the :math:`t^*` quantiles stabilize as :math:`B` increases.

.. code-block:: python

   def diagnose_studentized_bootstrap(info):
       """
       Diagnostic plots for studentized bootstrap.
       
       Parameters
       ----------
       info : dict
           Output from studentized_bootstrap_ci containing 't_star'.
       """
       import matplotlib.pyplot as plt
       from scipy import stats

       t_star = info['t_star']

       fig, axes = plt.subplots(1, 3, figsize=(12, 4))

       # Histogram with normal overlay
       axes[0].hist(t_star, bins=50, density=True, alpha=0.7)
       x = np.linspace(t_star.min(), t_star.max(), 100)
       axes[0].plot(x, stats.norm.pdf(x), 'r-', lw=2, label='N(0,1)')
       axes[0].set_xlabel('t*')
       axes[0].set_title('Distribution of t*')
       axes[0].legend()

       # Q-Q plot against normal
       stats.probplot(t_star, dist="norm", plot=axes[1])
       axes[1].set_title('Normal Q-Q Plot')

       # Quantile stability (if we had batch information)
       # For now, show sorted t* values
       axes[2].plot(np.sort(t_star))
       axes[2].axhline(info['q_lo'], color='r', linestyle='--', 
                       label=f"q_lo = {info['q_lo']:.2f}")
       axes[2].axhline(info['q_hi'], color='r', linestyle='--',
                       label=f"q_hi = {info['q_hi']:.2f}")
       axes[2].set_xlabel('Order')
       axes[2].set_ylabel('t*')
       axes[2].set_title('Sorted t* Values')
       axes[2].legend()

       plt.tight_layout()
       return fig

**Red flags for studentized bootstrap:**

- Multimodal :math:`t^*` distribution (suggests instability or discrete effects)
- Heavy tails or extreme outliers (check for near-zero :math:`\hat{\sigma}^*` values)
- :math:`t^*` quantiles far from normal quantiles when sample size is moderate


Bias-Corrected (BC) Intervals
-----------------------------

The studentized bootstrap achieves second-order accuracy through explicit pivoting. An alternative approach corrects the percentile interval for **bias** in the bootstrap distribution. The bias-corrected (BC) interval is a stepping stone to the more sophisticated BCa method.


The Bias Problem in Percentile Intervals
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Recall that the percentile interval uses quantiles :math:`Q_{\alpha/2}(\hat{\theta}^*)` and :math:`Q_{1-\alpha/2}(\hat{\theta}^*)` directly. If the bootstrap distribution is centered at :math:`\hat{\theta}` but the sampling distribution is centered at :math:`\theta \neq \hat{\theta}`, the percentile interval inherits this discrepancy.

Define the **bias-correction constant** :math:`z_0` as the standard normal quantile corresponding to the proportion of bootstrap replicates below the original estimate:

.. math::
   :label: z0-def

   z_0 = \Phi^{-1}\left(\frac{\#\{\hat{\theta}^*_b < \hat{\theta}\}}{B}\right)

**Interpretation**: 

- If :math:`z_0 = 0`, exactly half the bootstrap replicates fall below :math:`\hat{\theta}`---the bootstrap distribution is median-unbiased relative to :math:`\hat{\theta}`.
- If :math:`z_0 > 0`, more than half fall below :math:`\hat{\theta}`---the bootstrap distribution is shifted left (the estimator appears biased high).
- If :math:`z_0 < 0`, fewer than half fall below---the distribution is shifted right.


The BC Interval Formula
~~~~~~~~~~~~~~~~~~~~~~~

The BC interval adjusts the percentile levels to correct for this median bias:

.. math::
   :label: bc-formula

   \alpha_1^{BC} = \Phi(2z_0 + z_{\alpha/2}), \quad \alpha_2^{BC} = \Phi(2z_0 + z_{1-\alpha/2})

where :math:`z_{\alpha/2} = \Phi^{-1}(\alpha/2)` and :math:`z_{1-\alpha/2} = \Phi^{-1}(1-\alpha/2)` are the standard normal quantiles for the nominal level.

The BC interval is then:

.. math::

   \left[Q_{\alpha_1^{BC}}(\hat{\theta}^*), Q_{\alpha_2^{BC}}(\hat{\theta}^*)\right]

**Example**: For a 95% interval (:math:`\alpha = 0.05`), we have :math:`z_{0.025} = -1.96` and :math:`z_{0.975} = 1.96`. If :math:`z_0 = 0.3` (indicating moderate positive bias):

.. math::

   \alpha_1^{BC} &= \Phi(2(0.3) + (-1.96)) = \Phi(-1.36) \approx 0.087 \\
   \alpha_2^{BC} &= \Phi(2(0.3) + 1.96) = \Phi(2.56) \approx 0.995

The interval uses the 8.7th and 99.5th percentiles instead of the 2.5th and 97.5th, shifting both endpoints upward to correct for the bias.


When BC Suffices
~~~~~~~~~~~~~~~~

The BC interval corrects for bias but not for **acceleration**---the phenomenon where the standard error of :math:`\hat{\theta}` varies with the true parameter value :math:`\theta`. BC is adequate when:

- Bias is the primary concern (non-negligible :math:`z_0`)
- The standard error is approximately constant across plausible parameter values
- The bootstrap distribution is roughly symmetric

For statistics where the standard error varies substantially with the parameter (correlations near :math:`\pm 1`, variances, proportions near 0 or 1), BC may not provide sufficient correction. The full BCa method addresses this.

.. figure:: https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_7_fig04_bca_bias_correction.png
   :alt: BCa Bias Correction z0
   :align: center
   :width: 95%

   **Figure 4.7.4**: The bias-correction constant :math:`z_0`. (a) Symmetric bootstrap distribution: :math:`z_0 \approx 0` indicates no median bias. (b) Skewed bootstrap distribution: :math:`z_0 < 0` (or :math:`> 0`) indicates the bootstrap median differs from :math:`\hat{\theta}`. (c) Formula and interpretation of :math:`z_0`. (d) Numerical example showing how :math:`z_0` adjusts percentile levels. (e) The BC formula visualization showing how adjusted levels vary with :math:`z_0`. (f) Key insights about the direction and magnitude of corrections.


Bias-Corrected and Accelerated (BCa) Intervals
----------------------------------------------

The BCa interval, introduced by Efron (1987), is widely regarded as the **best general-purpose bootstrap confidence interval**. It achieves second-order accuracy, is transformation-invariant, and automatically adapts to bias and skewness without requiring explicit studentization. The "cost" is computing an additional acceleration parameter from the jackknife.


Theoretical Foundation
~~~~~~~~~~~~~~~~~~~~~~

The BCa method can be motivated as a **higher order calibration** of bootstrap percentiles. Under standard regularity conditions for **smooth functionals**, there exists an unknown monotone transformation :math:`\phi` such that the transformed estimator
:math:`\hat{\phi} = \phi(\hat{\theta})` is approximately normal and admits an Edgeworth type expansion. In that expansion, two effects dominate the coverage error of naive percentile procedures:

1. **Median bias** of :math:`\hat{\theta}` relative to the bootstrap distribution.
2. **Skewness and non constant curvature effects** associated with the statistic‚Äôs influence function.

BCa incorporates these effects through two constants:

- The **bias correction** :math:`z_0`, which measures median bias in standard normal units.
- The **acceleration** :math:`a`, which captures the leading skewness or curvature contribution and is estimated from jackknife leave one out values.

A key feature is that BCa is **invariant under monotone reparameterizations**: if the parameter is replaced by :math:`\psi(\theta)` for a monotone :math:`\psi`, the BCa construction transforms accordingly, so the resulting interval on the original scale is coherent without needing to know :math:`\phi` explicitly.

Concretely, :math:`z_0` is estimated from the bootstrap distribution by

.. math::
   :label: bca-z0

   z_0 = \Phi^{-1}\!\left(\frac{\#\{\hat{\theta}^* < \hat{\theta}\}}{B}\right),

and :math:`a` is estimated from jackknife leave one out estimates :math:`\hat{\theta}_{(i)}` via

.. math::
   :label: bca-a

   a
   =
   \frac{
      \sum_{i=1}^n (\bar{\theta}_{(\cdot)} - \hat{\theta}_{(i)})^3
   }{
      6\left[\sum_{i=1}^n (\bar{\theta}_{(\cdot)} - \hat{\theta}_{(i)})^2\right]^{3/2}
   },
   \qquad
   \bar{\theta}_{(\cdot)} = \frac{1}{n}\sum_{i=1}^n \hat{\theta}_{(i)}.

These definitions align with the standard BCa derivation: :math:`z_0` corrects median bias, and :math:`a` adjusts for skewness and curvature effects that otherwise produce :math:`O(n^{-1/2})` one sided coverage errors.

### The BCa Formula
~~~~~~~~~~~~~~~

Given the bias correction constant :math:`z_0` and acceleration constant :math:`a`, the BCa interval uses adjusted percentile levels:

.. math::
   :label: bca-formula

   \alpha_j^{BCa} = \Phi\left(z_0 + \frac{z_0 + z_{\alpha_j}}{1 - a(z_0 + z_{\alpha_j})}\right)

for :math:`\alpha_j \in \{\alpha/2, 1-\alpha/2\}`. The BCa interval is:

.. math::

   \left[Q_{\alpha_1^{BCa}}(\hat{\theta}^*), Q_{\alpha_2^{BCa}}(\hat{\theta}^*)\right]

**Special cases:**

- When :math:`a = 0`: The formula reduces to :math:`\alpha_j^{BCa} = \Phi(2z_0 + z_{\alpha_j})`, which is exactly the BC interval.
- When :math:`z_0 = 0` and :math:`a = 0`: The formula gives :math:`\alpha_j^{BCa} = \alpha_j`, recovering the standard percentile interval.


Computing the Bias-Correction Constant :math:`z_0`
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The bias-correction constant measures where :math:`\hat{\theta}` falls in its bootstrap distribution:

.. math::
   :label: z0-formula

   z_0 = \Phi^{-1}\left(\frac{\#\{\hat{\theta}^*_b < \hat{\theta}\}}{B}\right)

**Handling ties**: When some bootstrap replicates exactly equal :math:`\hat{\theta}` (common for discrete statistics or medians), the standard formula can overestimate bias. A robust adjustment assigns half of the ties to each side:

.. math::
   :label: z0-midrank

   z_0 = \Phi^{-1}\left(\frac{\#\{\hat{\theta}^* < \hat{\theta}\} + \tfrac{1}{2}\#\{\hat{\theta}^* = \hat{\theta}\}}{B}\right)

**Numerical stability**: If the proportion is exactly 0 or 1, :math:`\Phi^{-1}` returns :math:`\mp\infty`. Clip the proportion to :math:`[1/(2B), 1-1/(2B)]` before applying :math:`\Phi^{-1}`:

.. code-block:: python

   def compute_z0(theta_star, theta_hat):
       """
       Compute BCa bias-correction constant with tie handling.
       
       Parameters
       ----------
       theta_star : array_like
           Bootstrap replicates.
       theta_hat : float
           Original estimate.
           
       Returns
       -------
       z0 : float
           Bias-correction constant.
       """
       from scipy.stats import norm
       
       theta_star = np.asarray(theta_star)
       B = len(theta_star)
       
       # Count with mid-rank adjustment for ties
       n_below = np.sum(theta_star < theta_hat)
       n_equal = np.sum(theta_star == theta_hat)
       prop = (n_below + 0.5 * n_equal) / B
       
       # Clip for numerical stability
       prop = np.clip(prop, 1 / (2 * B), 1 - 1 / (2 * B))
       
       return norm.ppf(prop)


Computing the Acceleration Constant :math:`a`
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The acceleration constant captures how the standard error of :math:`\hat{\theta}` changes with the parameter value. It is computed from the **jackknife influence values** introduced in :ref:`Section 4.5 <ch4_5-jackknife-methods>`.

Let :math:`\hat{\theta}_{(-i)}` be the statistic computed with observation :math:`i` deleted, and let :math:`\bar{\theta}_{(\cdot)} = \frac{1}{n}\sum_{i=1}^n \hat{\theta}_{(-i)}` be their mean. The acceleration constant is:

.. math::
   :label: acceleration-formula

   a = \frac{\sum_{i=1}^n (\bar{\theta}_{(\cdot)} - \hat{\theta}_{(-i)})^3}{6\left[\sum_{i=1}^n (\bar{\theta}_{(\cdot)} - \hat{\theta}_{(-i)})^2\right]^{3/2}}

**Interpretation**: This formula is essentially the **skewness** of the jackknife influence values, normalized appropriately. Large :math:`|a|` indicates that individual observations have asymmetric influence on the estimate, which corresponds to the standard error varying with the parameter.

**Typical values**: In many smooth problems, :math:`|a| < 0.1` is often modest (frequently below 0.1), but it can be larger for heavy tailed data, bounded parameters, or strongly influential observations. Values above roughly 0.25 typically signal substantial acceleration effects and justify preferring BCa over BC.
**Numerical guard**: If all jackknife estimates are identical (denominator is zero), set :math:`a = 0` and fall back to the BC interval. This can occur for statistics that are insensitive to individual observations.

.. figure:: https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_7_fig05_bca_acceleration.png
   :alt: BCa Acceleration Constant a
   :align: center
   :width: 95%

   **Figure 4.7.5**: The acceleration constant :math:`a`. (a) Jackknife estimates :math:`\hat{\theta}_{(-i)}` for sample variance---note the influential outliers. (b) Influence values :math:`d_i = \bar{\theta}_{(\cdot)} - \hat{\theta}_{(-i)}` showing skewness. (c) Formula for computing :math:`a` from the skewness of influence values. (d) Interpretation: :math:`a` captures how SE varies with the parameter value. (e) The full BCa formula showing how both :math:`z_0` and :math:`a` jointly adjust percentile levels. (f) Key insights about positive vs negative :math:`a` and its effect on interval tails.

.. code-block:: python

   def compute_acceleration(data, statistic):
       """
       Compute BCa acceleration constant via jackknife.
       
       Parameters
       ----------
       data : array_like
           Original data.
       statistic : callable
           Function computing the statistic.
           
       Returns
       -------
       a : float
           Acceleration constant.
       theta_jack : ndarray
           Jackknife estimates (useful for diagnostics).
       """
       data = np.asarray(data)
       n = len(data)
       
       # Compute leave-one-out estimates
       theta_jack = np.empty(n)
       for i in range(n):
           data_minus_i = np.delete(data, i)
           theta_jack[i] = statistic(data_minus_i)
       
       # Compute acceleration
       theta_bar = theta_jack.mean()
       d = theta_bar - theta_jack  # Influence-like quantities
       
       numerator = np.sum(d**3)
       denominator = 6 * (np.sum(d**2))**1.5
       
       if denominator < 1e-10:
           # Degenerate case: all jackknife estimates equal
           return 0.0, theta_jack
       
       a = numerator / denominator
       return a, theta_jack


Transformation Invariance
~~~~~~~~~~~~~~~~~~~~~~~~~

A crucial property of BCa intervals is **transformation invariance**: if :math:`\psi = m(\theta)` for a monotone function :math:`m`, then the BCa interval for :math:`\psi` is exactly :math:`m` applied to the BCa interval for :math:`\theta`.

**Why this matters**: Consider estimating a variance :math:`\sigma^2`. Should we compute the interval on the variance scale or the standard deviation scale? For percentile intervals (which are also transformation-invariant), we get the same answer either way. But for basic intervals or normal approximations, the choice of scale affects the result. BCa preserves this desirable invariance while achieving better coverage.

**Proof sketch**: The BCa adjustment formula depends on :math:`z_0` and :math:`a`, both computed from the data without reference to the parameterization. Under a monotone transformation :math:`m`, the proportion of bootstrap replicates below the original estimate is preserved (since :math:`m(\hat{\theta}^*) < m(\hat{\theta})` iff :math:`\hat{\theta}^* < \hat{\theta}` for increasing :math:`m`), so :math:`z_0` is unchanged. The acceleration :math:`a` transforms consistently because influence values transform linearly for smooth :math:`m`. Therefore, the adjusted percentile levels :math:`\alpha_j^{BCa}` are invariant, and the interval endpoints transform correctly.


Complete BCa Implementation
~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   import numpy as np
   from scipy import stats

   def bca_bootstrap_ci(data, statistic, alpha=0.05, B=10000, seed=None):
       """
       Compute BCa (bias-corrected and accelerated) bootstrap confidence interval.

       Parameters
       ----------
       data : array_like
           Original data.
       statistic : callable
           Function computing the statistic: statistic(data) -> float.
       alpha : float
           Significance level (default 0.05 for 95% CI).
       B : int
           Number of bootstrap replicates.
       seed : int, optional
           Random seed for reproducibility.

       Returns
       -------
       ci : tuple
           (lower, upper) confidence bounds.
       theta_hat : float
           Original point estimate.
       info : dict
           Diagnostic information including z0, a, adjusted alpha levels.
       """
       rng = np.random.default_rng(seed)
       data = np.asarray(data)
       n = len(data)

       # Original estimate
       theta_hat = statistic(data)

       # Bootstrap distribution
       theta_star = np.empty(B)
       for b in range(B):
           idx = rng.integers(0, n, size=n)
           theta_star[b] = statistic(data[idx])

       # Bias correction constant z0
       n_below = np.sum(theta_star < theta_hat)
       n_equal = np.sum(theta_star == theta_hat)
       prop = (n_below + 0.5 * n_equal) / B
       prop = np.clip(prop, 1 / (2 * B), 1 - 1 / (2 * B))
       z0 = stats.norm.ppf(prop)

       # Acceleration constant via jackknife
       theta_jack = np.empty(n)
       for i in range(n):
           data_minus_i = np.delete(data, i)
           theta_jack[i] = statistic(data_minus_i)

       theta_bar = theta_jack.mean()
       d = theta_bar - theta_jack

       numerator = np.sum(d**3)
       denominator = 6 * (np.sum(d**2))**1.5

       if denominator > 1e-10:
           a = numerator / denominator
       else:
           a = 0.0

       # Adjusted percentile levels
       z_alpha_lo = stats.norm.ppf(alpha / 2)
       z_alpha_hi = stats.norm.ppf(1 - alpha / 2)

       def adjusted_alpha(z_alpha):
           """Compute BCa-adjusted percentile level."""
           numer = z0 + z_alpha
           denom = 1 - a * (z0 + z_alpha)
           if abs(denom) < 1e-10:
               # Extreme case: return edge of [0, 1]
               return 0.0 if numer < 0 else 1.0
           return stats.norm.cdf(z0 + numer / denom)

       alpha1 = adjusted_alpha(z_alpha_lo)
       alpha2 = adjusted_alpha(z_alpha_hi)

       # Ensure valid probability range
       alpha1 = np.clip(alpha1, 1 / B, 1 - 1 / B)
       alpha2 = np.clip(alpha2, 1 / B, 1 - 1 / B)
       if alpha1 >= alpha2:
        alpha1, alpha2 = alpha/2, 1-alpha/2
        bca_warning = "Adjusted levels crossed; fell back to percentile."
       else:
        bca_warning = None

       # BCa interval
       ci_lower = np.quantile(theta_star, alpha1)
       ci_upper = np.quantile(theta_star, alpha2)

       info = {
           'z0': z0,
           'a': a,
           'alpha1': alpha1,
           'alpha2': alpha2,
           'theta_star': theta_star,
           'theta_jack': theta_jack,
           'se_boot': np.std(theta_star, ddof=1),
           'bias_boot': theta_star.mean() - theta_hat,
           'bca_warning': bca_warning
       }

       return (ci_lower, ci_upper), theta_hat, info


.. admonition:: Example üí° BCa Interval for Median of Skewed Data
   :class: note

   **Given:** A sample of :math:`n = 60` observations from a log-normal distribution (positively skewed).

   **Find:** A 95% confidence interval for the population median.

   **Challenge:** The bootstrap distribution of the sample median is skewed, and the median is a non-smooth statistic.

   **Python implementation:**

   .. code-block:: python

      import numpy as np
      from scipy import stats

      # Generate skewed data
      rng = np.random.default_rng(42)
      n = 60
      # Log-normal with median = exp(0) = 1
      data = np.exp(rng.normal(0, 1, size=n))

      true_median = 1.0  # exp(0) for lognormal(0, 1)

      def sample_median(x):
          return np.median(x)

      # BCa interval
      ci_bca, med_hat, info = bca_bootstrap_ci(
          data, sample_median, alpha=0.05, B=10000, seed=123
      )

      # Percentile interval for comparison
      theta_star = info['theta_star']
      ci_perc = (np.quantile(theta_star, 0.025), 
                 np.quantile(theta_star, 0.975))

      # Basic interval
      q_lo = np.quantile(theta_star, 0.025)
      q_hi = np.quantile(theta_star, 0.975)
      ci_basic = (2 * med_hat - q_hi, 2 * med_hat - q_lo)

      print(f"Sample median: {med_hat:.4f}")
      print(f"True median:   {true_median:.4f}")
      print(f"z0 = {info['z0']:.4f}, a = {info['a']:.4f}")
      print(f"Adjusted levels: [{info['alpha1']:.4f}, {info['alpha2']:.4f}]")
      print(f"\n95% Confidence Intervals:")
      print(f"  Percentile: [{ci_perc[0]:.4f}, {ci_perc[1]:.4f}]")
      print(f"  Basic:      [{ci_basic[0]:.4f}, {ci_basic[1]:.4f}]")
      print(f"  BCa:        [{ci_bca[0]:.4f}, {ci_bca[1]:.4f}]")

   **Typical output:**

   .. code-block:: text

      Sample median: 1.0847
      True median:   1.0000
      z0 = 0.0627, a = 0.0312
      Adjusted levels: [0.0336, 0.9829]

      95% Confidence Intervals:
        Percentile: [0.7834, 1.4521]
        Basic:      [0.7173, 1.3860]
        BCa:        [0.8012, 1.4803]

   **Interpretation:** The positive :math:`z_0` indicates slight upward bias in the bootstrap distribution relative to :math:`\hat{\theta}`. The positive :math:`a` indicates the standard error increases with the parameter value (as expected for the median of a positively skewed distribution). BCa adjusts the percentile levels to [3.36%, 98.29%] instead of [2.5%, 97.5%], shifting both endpoints upward.

.. figure:: https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_7_fig06_bca_complete_example.png
   :alt: BCa Complete Example
   :align: center
   :width: 95%

   **Figure 4.7.6**: Complete BCa example for median of log-normal data. (a) The skewed log-normal data with true and sample medians marked. (b) Bootstrap distribution of the sample median. (c) Computed BCa parameters :math:`z_0` and :math:`a` with adjusted percentile levels. (d) Numerical results for all confidence interval methods. (e) Visual comparison of the four interval methods showing which capture the true median. Key insights at the bottom explain how bias correction and acceleration work together.


When BCa Fails
~~~~~~~~~~~~~~

Despite its broad applicability, BCa has limitations:

**Non-smooth statistics**: The jackknife-based acceleration formula assumes smooth dependence on individual observations. For highly non-smooth statistics (sample mode, range, extreme quantiles), the jackknife estimates :math:`\hat{\theta}_{(-i)}` may be unstable, leading to unreliable :math:`a`.

**Very small samples**: When :math:`n < 15`, the jackknife provides only a coarse approximation to influence, and BCa coverage can be substantially below nominal. For :math:`n < 10`, consider parametric methods or exact approaches.

**Multimodal bootstrap distributions**: BCa assumes a unimodal, roughly symmetric underlying structure. Multimodality indicates a fundamental problem that no simple adjustment can fix.

**Boundary effects**: For parameters near natural boundaries (proportions near 0 or 1, variances near 0, correlations near :math:`\pm 1`), the BCa formula may produce adjusted levels :math:`\alpha_j^{BCa}` outside :math:`(0, 1)`, requiring truncation.


Choosing B and Assessing Monte Carlo Error
------------------------------------------

Every bootstrap computation involves **Monte Carlo error**---the additional randomness from using finitely many bootstrap replicates :math:`B`. This error is distinct from the statistical uncertainty we're trying to quantify (which depends on the sample size :math:`n`) and is under our control. Choosing :math:`B` appropriately and understanding Monte Carlo error are essential for responsible bootstrap inference.


Two Sources of Uncertainty
~~~~~~~~~~~~~~~~~~~~~~~~~~

Consider a bootstrap standard error estimate :math:`\widehat{\text{SE}}_{\text{boot}} = \text{SD}(\hat{\theta}^*)`. This estimate has two sources of variability:

1. **Statistical uncertainty**: The bootstrap SE estimates the true standard error :math:`\text{SE}_F(\hat{\theta})`, which depends on the unknown distribution :math:`F`. Even with :math:`B = \infty`, our estimate reflects only what the sample tells us about :math:`F`.

2. **Monte Carlo uncertainty**: With finite :math:`B`, we approximate the bootstrap distribution with a Monte Carlo sample. This adds noise that disappears as :math:`B \to \infty` but contributes to estimation error for finite :math:`B`.

**The goal**: Choose :math:`B` large enough that Monte Carlo error is negligible relative to statistical uncertainty. We cannot eliminate statistical uncertainty, but we can make Monte Carlo error as small as computational resources allow.


Monte Carlo Error for Bootstrap SE
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Under mild conditions, the bootstrap replicates :math:`\hat{\theta}^*_1, \ldots, \hat{\theta}^*_B` are approximately iid draws from the bootstrap distribution. The sample standard deviation :math:`\widehat{\text{SE}}_{\text{boot}} = \text{SD}(\hat{\theta}^*)` estimates the population standard deviation of this distribution.

For the standard deviation of iid random variables, the Monte Carlo standard error is approximately:

.. math::
   :label: mc-se-formula

   \text{SE}_{\text{MC}}\bigl(\widehat{\text{SE}}_{\text{boot}}\bigr) \approx \frac{\widehat{\text{SE}}_{\text{boot}}}{\sqrt{2(B-1)}}

This gives the **coefficient of variation** of the bootstrap SE estimate:

.. math::

   \text{CV} = \frac{\text{SE}_{\text{MC}}(\widehat{\text{SE}}_{\text{boot}})}{\widehat{\text{SE}}_{\text{boot}}} \approx \frac{1}{\sqrt{2(B-1)}}

**Practical implications**:

.. list-table:: Monte Carlo CV for Bootstrap SE
   :header-rows: 1
   :widths: 30 35 35

   * - Replicates :math:`B`
     - CV of SE estimate
     - Interpretation
   * - 200
     - 5.0%
     - Rough approximation
   * - 500
     - 3.2%
     - Reasonable for exploration
   * - 1,000
     - 2.2%
     - Standard practice
   * - 2,000
     - 1.6%
     - Good precision
   * - 10,000
     - 0.7%
     - High precision

For most applications, :math:`B = 1{,}000\text{--}2{,}000` provides bootstrap SE estimates with acceptable Monte Carlo error.


Monte Carlo Error for Quantile Endpoints
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Confidence interval endpoints require estimating **quantiles** of the bootstrap distribution, and quantile estimation has different Monte Carlo error characteristics than mean or SD estimation.

For the :math:`\alpha`-quantile :math:`q_\alpha`, the Monte Carlo variance is approximately:

.. math::
   :label: mc-quantile-var

   \text{Var}_{\text{MC}}(\hat{q}_\alpha) \approx \frac{\alpha(1-\alpha)}{B \cdot f(q_\alpha)^2}

where :math:`f(q_\alpha)` is the probability density of the bootstrap distribution at the quantile. The Monte Carlo SE is:

.. math::

   \text{SE}_{\text{MC}}(\hat{q}_\alpha) \approx \frac{\sqrt{\alpha(1-\alpha)}}{\sqrt{B} \cdot f(q_\alpha)}

**Key observations**:

1. **Tail quantiles are harder**: For :math:`\alpha = 0.025` (the 2.5th percentile), :math:`\alpha(1-\alpha) = 0.024`. For :math:`\alpha = 0.5` (the median), :math:`\alpha(1-\alpha) = 0.25`. Tail quantiles have smaller numerators.

2. **Density matters**: Tail quantiles typically occur where the density :math:`f(q_\alpha)` is lower, which increases MC variance. The net effect often makes tail quantile estimation harder than it might seem from :math:`\alpha(1-\alpha)` alone.

3. **More :math:`B` needed for CIs**: Because CI endpoints are tail quantiles with potentially low density, they require substantially more :math:`B` than SE estimation.


Estimating the Density at Quantiles
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To assess Monte Carlo error for CI endpoints, we need to estimate :math:`f(q_\alpha)`. Two approaches:

**Spacing method**: Estimate the density using nearby quantiles:

.. math::

   \hat{f}(q_\alpha) \approx \frac{2\delta}{q_{\alpha+\delta} - q_{\alpha-\delta}}

for small :math:`\delta` (e.g., :math:`\delta = 0.01`). This uses the fact that :math:`P(q_{\alpha-\delta} < X < q_{\alpha+\delta}) \approx 2\delta`.

**Kernel density estimation**: Apply a KDE to the bootstrap replicates and evaluate at the empirical quantile.

.. code-block:: python

   def quantile_mc_error(theta_star, alpha, delta=0.01):
       """
       Estimate Monte Carlo standard error for a quantile estimate.

       Parameters
       ----------
       theta_star : array_like
           Bootstrap replicates.
       alpha : float
           Quantile level (e.g., 0.025 for 2.5th percentile).
       delta : float
           Half-width for density estimation.

       Returns
       -------
       se_mc : float
           Estimated MC standard error.
       f_hat : float
           Estimated density at the quantile.
       """
       B = len(theta_star)

       # Quantile estimate
       q_alpha = np.quantile(theta_star, alpha)

       # Density estimate via spacing
       alpha_lo = max(0.001, alpha - delta)
       alpha_hi = min(0.999, alpha + delta)
       q_lo = np.quantile(theta_star, alpha_lo)
       q_hi = np.quantile(theta_star, alpha_hi)

       if q_hi > q_lo:
           f_hat = (alpha_hi - alpha_lo) / (q_hi - q_lo)
       else:
           f_hat = np.inf  # Degenerate case

       # MC standard error
       se_mc = np.sqrt(alpha * (1 - alpha)) / (np.sqrt(B) * f_hat)

       return se_mc, f_hat


Practical Recommendations for Choosing B
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Based on the analysis above, here are guidelines for choosing :math:`B`:

.. list-table:: Recommended B Values by Task
   :header-rows: 1
   :widths: 35 25 40

   * - Task
     - Recommended :math:`B`
     - Rationale
   * - Bootstrap SE only
     - 1,000--2,000
     - CV < 2.5% is usually adequate
   * - Percentile CI (95%)
     - 5,000--10,000
     - Tail quantiles need more precision
   * - BCa CI (95%)
     - 10,000--15,000
     - Adjusted levels may be in tails
   * - Bootstrap hypothesis test
     - 10,000--20,000
     - p-value precision matters
   * - Publication quality
     - 10,000+
     - Report stability checks

**Additional considerations**:

- **Computation time**: If each bootstrap replicate is fast (simple statistics), use larger :math:`B`. If expensive (complex models), balance precision against time.

- **Jackknife cost**: BCa requires :math:`n` jackknife evaluations in addition to :math:`B` bootstrap evaluations. For expensive statistics with large :math:`n`, this may dominate.

- **Parallel computing**: Bootstrap replicates are embarrassingly parallel. With modern hardware, :math:`B = 10{,}000` is routine for most applications.


Adaptive and Batch-Based Stopping Rules
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Rather than fixing :math:`B` in advance, we can run bootstrap in batches and stop when estimates stabilize.

**Batch-based stability check**:

1. Run bootstrap in batches of size :math:`B_{\text{batch}}` (e.g., 1,000).
2. After each batch, compute cumulative CI endpoints.
3. Stop when the change in endpoints from the previous batch is below a threshold.

.. code-block:: python

   def adaptive_bca_ci(data, statistic, alpha=0.05, B_batch=1000, 
                        max_batches=20, tol=0.01, seed=None):
       """
       BCa CI with adaptive stopping based on endpoint stability.

       Parameters
       ----------
       data : array_like
           Original data.
       statistic : callable
           Function computing the statistic.
       alpha : float
           Significance level.
       B_batch : int
           Batch size for bootstrap replicates.
       max_batches : int
           Maximum number of batches.
       tol : float
           Relative tolerance for stopping (as fraction of SE).
       seed : int, optional
           Random seed.

       Returns
       -------
       ci : tuple
           Final (lower, upper) CI.
       info : dict
           Convergence information.
       """
       rng = np.random.default_rng(seed)
       data = np.asarray(data)
       n = len(data)

       theta_hat = statistic(data)

       # Pre-compute jackknife for acceleration (doesn't change)
       theta_jack = np.empty(n)
       for i in range(n):
           theta_jack[i] = statistic(np.delete(data, i))

       theta_bar = theta_jack.mean()
       d = theta_bar - theta_jack
       num = np.sum(d**3)
       den = 6 * (np.sum(d**2))**1.5
       a = num / den if den > 1e-10 else 0.0

       # Accumulate bootstrap replicates
       theta_star_all = []
       ci_history = []
       prev_ci = None

       for batch in range(max_batches):
           # Generate batch of bootstrap replicates
           theta_star_batch = np.empty(B_batch)
           for b in range(B_batch):
               idx = rng.integers(0, n, size=n)
               theta_star_batch[b] = statistic(data[idx])

           theta_star_all.extend(theta_star_batch)
           theta_star = np.array(theta_star_all)
           B_current = len(theta_star)

           # Compute BCa CI with current replicates
           prop = (np.sum(theta_star < theta_hat) + 
                   0.5 * np.sum(theta_star == theta_hat)) / B_current
           prop = np.clip(prop, 1 / (2 * B_current), 1 - 1 / (2 * B_current))
           z0 = stats.norm.ppf(prop)

           z_lo = stats.norm.ppf(alpha / 2)
           z_hi = stats.norm.ppf(1 - alpha / 2)

           def adj_alpha(z):
               numer = z0 + z
               denom = 1 - a * (z0 + z)
               if abs(denom) < 1e-10:
                   return 0.0 if numer < 0 else 1.0
               return stats.norm.cdf(z0 + numer / denom)

           alpha1 = np.clip(adj_alpha(z_lo), 1/B_current, 1-1/B_current)
           alpha2 = np.clip(adj_alpha(z_hi), 1/B_current, 1-1/B_current)

           ci = (np.quantile(theta_star, alpha1), 
                 np.quantile(theta_star, alpha2))
           ci_history.append(ci)

           # Check convergence
           if prev_ci is not None:
               se_boot = np.std(theta_star, ddof=1)
               change = max(abs(ci[0] - prev_ci[0]), abs(ci[1] - prev_ci[1]))
               rel_change = change / se_boot if se_boot > 0 else 0

               if rel_change < tol:
                   break

           prev_ci = ci

       info = {
           'B_total': len(theta_star_all),
           'n_batches': batch + 1,
           'converged': (batch + 1) < max_batches,
           'ci_history': ci_history,
           'z0': z0,
           'a': a
       }

       return ci, info


Reporting Monte Carlo Error
~~~~~~~~~~~~~~~~~~~~~~~~~~~

For publication-quality results, report Monte Carlo error alongside bootstrap estimates:

.. code-block:: python

   # After computing BCa CI
   ci, theta_hat, info = bca_bootstrap_ci(data, statistic, B=10000, seed=42)

   # Compute MC errors for endpoints
   se_mc_lo, _ = quantile_mc_error(info['theta_star'], info['alpha1'])
   se_mc_hi, _ = quantile_mc_error(info['theta_star'], info['alpha2'])

   print(f"Point estimate: {theta_hat:.4f}")
   print(f"Bootstrap SE: {info['se_boot']:.4f}")
   print(f"95% BCa CI: [{ci[0]:.4f}, {ci[1]:.4f}]")
   print(f"MC SE of endpoints: ({se_mc_lo:.4f}, {se_mc_hi:.4f})")


Diagnostics for Advanced Bootstrap Methods
------------------------------------------

Before reporting any bootstrap confidence interval, examine the bootstrap distribution and method-specific quantities to ensure the results are trustworthy. This section consolidates diagnostic approaches for all the methods covered.


Visual Diagnostics
~~~~~~~~~~~~~~~~~~

**For all methods**:

1. **Histogram of** :math:`\hat{\theta}^*` with vertical lines marking :math:`\hat{\theta}` and CI endpoints
2. **Normal Q-Q plot** of :math:`\hat{\theta}^*` to assess departure from normality
3. **Endpoint stability plot**: CI bounds versus :math:`B` (run in batches)

**For studentized bootstrap additionally**:

4. **Histogram of** :math:`t^*` compared with :math:`N(0,1)` or :math:`t_{n-1}`
5. **Check for extreme** :math:`t^*` **values** (indicate :math:`\hat{\sigma}^* \approx 0`)

**For BCa additionally**:

6. **Jackknife influence plot**: :math:`\hat{\theta}_{(-i)}` versus :math:`i` to identify influential observations
7. **Verify adjusted levels** :math:`\alpha_1^{BCa}, \alpha_2^{BCa}` are not extreme

.. code-block:: python

    def comprehensive_bootstrap_diagnostics(data, statistic, ci_method='bca',
                                        alpha=0.05, B=10000, seed=None,
                                        batch_size=500, n_batches=10):
        """
        Generate comprehensive diagnostics for bootstrap CI.

        Parameters
        ----------
        data : array_like
            Original data.
        statistic : callable
            Function computing the statistic.
        ci_method : str
            'percentile', 'basic', or 'bca'.
        alpha : float
            Significance level.
        B : int
            Number of bootstrap replicates.
        seed : int
            Random seed.
        batch_size : int
            Size of each independent bootstrap batch used for endpoint stability.
        n_batches : int
            Number of batches to show in the endpoint stability plot.

        Returns
        -------
        fig : matplotlib.figure.Figure
            Diagnostic plots.
        diagnostics : dict
            Quantitative diagnostic summaries.
        """
        import numpy as np
        import matplotlib.pyplot as plt
        from scipy import stats

        rng = np.random.default_rng(seed)
        data = np.asarray(data)
        n = len(data)

        theta_hat = statistic(data)

        # ---------------------------------------------------------------------
        # Bootstrap distribution (single run of size B)
        # ---------------------------------------------------------------------
        theta_star = np.empty(B)
        for b in range(B):
            idx = rng.integers(0, n, size=n)
            theta_star[b] = statistic(data[idx])

        # ---------------------------------------------------------------------
        # Jackknife for BCa and diagnostics
        # ---------------------------------------------------------------------
        theta_jack = np.empty(n)
        for i in range(n):
            theta_jack[i] = statistic(np.delete(data, i))

        # Basic summaries
        se_boot = np.std(theta_star, ddof=1)
        bias_boot = theta_star.mean() - theta_hat
        skewness = stats.skew(theta_star, bias=False)
        kurtosis = stats.kurtosis(theta_star, fisher=True, bias=False)

        # BCa parameters (use a robust tie strategy; exact float ties are rare)
        prop = np.mean(theta_star < theta_hat)
        prop = np.clip(prop, 1/(2*B), 1 - 1/(2*B))
        z0 = stats.norm.ppf(prop)

        theta_bar = theta_jack.mean()
        d = theta_bar - theta_jack
        denom = np.sum(d**2)
        a = np.sum(d**3) / (6 * denom**1.5) if denom > 1e-14 else 0.0

        # ---------------------------------------------------------------------
        # Helper: compute CI from an already-generated theta_star vector
        # ---------------------------------------------------------------------
        def _ci_from_theta_star(theta_star_local, method):
            theta_star_local = np.asarray(theta_star_local)
            if method == 'percentile':
                return (np.quantile(theta_star_local, alpha/2),
                        np.quantile(theta_star_local, 1 - alpha/2))

            if method == 'basic':
                q_lo = np.quantile(theta_star_local, alpha/2)
                q_hi = np.quantile(theta_star_local, 1 - alpha/2)
                return (2 * theta_hat - q_hi, 2 * theta_hat - q_lo)

            if method == 'bca':
                z_lo = stats.norm.ppf(alpha/2)
                z_hi = stats.norm.ppf(1 - alpha/2)

                def adj(z):
                    num = z0 + z
                    den = 1 - a * (z0 + z)
                    if abs(den) < 1e-14:
                        return 0.0 if num < 0 else 1.0
                    return stats.norm.cdf(z0 + num / den)

                alpha1 = np.clip(adj(z_lo), 1 / len(theta_star_local), 1 - 1 / len(theta_star_local))
                alpha2 = np.clip(adj(z_hi), 1 / len(theta_star_local), 1 - 1 / len(theta_star_local))

                # Guard against crossing adjusted levels (rare but possible)
                if alpha1 >= alpha2:
                    # fall back to percentile on the same theta_star_local
                    return (np.quantile(theta_star_local, alpha/2),
                            np.quantile(theta_star_local, 1 - alpha/2))

                return (np.quantile(theta_star_local, alpha1),
                        np.quantile(theta_star_local, alpha2))

            raise ValueError(f"Unknown method: {method}")

        # Compute CI based on requested method (using the full theta_star)
        ci = _ci_from_theta_star(theta_star, ci_method)

        # ---------------------------------------------------------------------
        # Create diagnostic plots
        # ---------------------------------------------------------------------
        fig, axes = plt.subplots(2, 3, figsize=(14, 9))

        # 1. Histogram with CI
        axes[0, 0].hist(theta_star, bins=50, density=True, alpha=0.7,
                        edgecolor='white')
        axes[0, 0].axvline(theta_hat, linewidth=2,
                        label=f'$\\hat{{\\theta}}$ = {theta_hat:.3f}')
        axes[0, 0].axvline(ci[0], linestyle='--', linewidth=2,
                        label=f'CI: [{ci[0]:.3f}, {ci[1]:.3f}]')
        axes[0, 0].axvline(ci[1], linestyle='--', linewidth=2)
        axes[0, 0].set_xlabel('$\\hat{\\theta}^*$')
        axes[0, 0].set_ylabel('Density')
        axes[0, 0].set_title('Bootstrap Distribution')
        axes[0, 0].legend(fontsize=9)

        # 2. Normal Q-Q plot
        stats.probplot(theta_star, dist="norm", plot=axes[0, 1])
        axes[0, 1].set_title('Normal Q-Q Plot')

        # 3. Jackknife influence
        axes[0, 2].scatter(range(n), theta_jack, alpha=0.6, s=30)
        axes[0, 2].axhline(theta_bar, linestyle='--',
                        label=f'Mean = {theta_bar:.3f}')
        axes[0, 2].set_xlabel('Observation Index')
        axes[0, 2].set_ylabel('$\\hat{\\theta}_{(-i)}$')
        axes[0, 2].set_title('Jackknife Leave One Out Estimates')
        axes[0, 2].legend()

        # 4. Bias ratio visualization
        bias_ratio = abs(bias_boot) / se_boot if se_boot > 0 else np.nan
        axes[1, 0].barh(['Bias Ratio'], [bias_ratio])
        axes[1, 0].axvline(0.25, linestyle='--', label='Warning (0.25)')
        axes[1, 0].axvline(0.5, linestyle='--', label='Concern (0.5)')
        axes[1, 0].set_xlim(0, max(1, (bias_ratio if np.isfinite(bias_ratio) else 0) * 1.2))
        axes[1, 0].set_xlabel('|Bias| / SE')
        axes[1, 0].set_title(f'Bias Ratio = {bias_ratio:.3f}' if np.isfinite(bias_ratio) else 'Bias Ratio')
        axes[1, 0].legend()

        # 5. Endpoint stability using independent batches
        #    This shows how endpoints change as you aggregate *independent* bootstrap batches.
        #    We generate n_batches batches of size batch_size, compute CI after k batches.
        total_for_stability = batch_size * n_batches
        theta_star_batches = np.empty(total_for_stability)

        # Use a new RNG stream to keep the main theta_star fixed given the seed
        rng_stab = np.random.default_rng(None if seed is None else seed + 1)

        for b in range(total_for_stability):
            idx = rng_stab.integers(0, n, size=n)
            theta_star_batches[b] = statistic(data[idx])

        B_values = np.arange(batch_size, total_for_stability + 1, batch_size)
        ci_lo_hist = []
        ci_hi_hist = []
        for b in B_values:
            ci_b = _ci_from_theta_star(theta_star_batches[:b], ci_method)
            ci_lo_hist.append(ci_b[0])
            ci_hi_hist.append(ci_b[1])

        axes[1, 1].plot(B_values, ci_lo_hist, label='Lower')
        axes[1, 1].plot(B_values, ci_hi_hist, label='Upper')
        axes[1, 1].set_xlabel('Total replicates used (independent batches)')
        axes[1, 1].set_ylabel('CI endpoint')
        axes[1, 1].set_title(f'Endpoint Stability ({ci_method.upper()}, batch size = {batch_size})')
        axes[1, 1].legend()

        # 6. Summary statistics text
        summary_text = (
            f"Sample size n = {n}\n"
            f"Bootstrap B = {B}\n"
            f"$\\hat{{\\theta}}$ = {theta_hat:.4f}\n"
            f"Bootstrap SE = {se_boot:.4f}\n"
            f"Bootstrap Bias = {bias_boot:.4f}\n"
            f"Skewness = {skewness:.3f}\n"
            f"Kurtosis = {kurtosis:.3f}\n"
            f"$z_0$ = {z0:.4f}\n"
            f"$a$ = {a:.4f}\n"
            f"Method: {ci_method.upper()}\n"
            f"CI: [{ci[0]:.4f}, {ci[1]:.4f}]"
        )
        axes[1, 2].text(0.05, 0.5, summary_text, transform=axes[1, 2].transAxes,
                        fontsize=11, verticalalignment='center', fontfamily='monospace',
                        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        axes[1, 2].axis('off')
        axes[1, 2].set_title('Summary Statistics')

        plt.tight_layout()

        diagnostics = {
            'se_boot': se_boot,
            'bias_boot': bias_boot,
            'bias_ratio': bias_ratio,
            'skewness': skewness,
            'kurtosis': kurtosis,
            'z0': z0,
            'a': a,
            'ci': ci,
            'theta_hat': theta_hat,
            'theta_star': theta_star,
            'theta_jack': theta_jack,
            # stability series for programmatic inspection
            'stability_B_values': B_values,
            'stability_ci_lower': np.array(ci_lo_hist),
            'stability_ci_upper': np.array(ci_hi_hist),
            'stability_method': ci_method,
            'stability_batch_size': batch_size,
            'stability_n_batches': n_batches
        }

        return fig, diagnostics



Quantitative Diagnostic Thresholds
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use these thresholds to guide method selection and identify potential problems:

.. list-table:: Diagnostic Thresholds and Actions
   :header-rows: 1
   :widths: 25 20 55

   * - Diagnostic
     - Threshold
     - Action
   * - Bias ratio :math:`|\text{bias}|/\text{SE}`
     - < 0.25
     - Bias correction unnecessary
   * - 
     - 0.25--0.5
     - Consider BC or BCa
   * - 
     - > 0.5
     - Use BCa; report both corrected and uncorrected
   * - Skewness :math:`|\gamma_1|`
     - < 0.5
     - Normal approximation may suffice
   * - 
     - 0.5--2
     - Use BCa or studentized
   * - 
     - > 2
     - Bootstrap may be problematic; investigate
   * - :math:`|z_0|`
     - < 0.1
     - Minimal bias correction
   * - 
     - 0.1--0.5
     - Moderate bias; BCa appropriate
   * - 
     - > 0.5
     - Large bias; verify data and statistic
   * - Acceleration :math:`|a|`
     - < 0.05
     - BC suffices (a ‚âà 0)
   * - 
     - 0.05--0.2
     - BCa provides improvement
   * - 
     - > 0.2
     - Strong acceleration; BCa essential
   * - Adjusted levels :math:`\alpha_j^{BCa}`
     - In (0.001, 0.999)
     - Normal operation
   * - 
     - Near 0 or 1
     - Extreme correction; verify results


Red Flags Requiring Further Investigation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Critical warnings** (do not trust results without investigation):

- **Multimodal bootstrap distribution**: Indicates instability, data issues, or inappropriate statistic
- **Many identical bootstrap estimates**: Common with discrete data; consider smoothed bootstrap
- **Extreme :math:`t^*` values** (studentized): Check for near-zero :math:`\hat{\sigma}^*`
- **Adjusted BCa levels outside (0.01, 0.99)**: Very extreme correction; results may be unreliable
- **CI endpoints at sample extremes**: Bootstrap cannot extrapolate beyond data

**Moderate warnings** (proceed with caution):

- Bias ratio > 0.5 without clear explanation
- Kurtosis > 10 (heavy tails)
- Large change in endpoints when increasing :math:`B`
- Jackknife estimates show strong outliers

.. admonition:: Common Pitfall ‚ö†Ô∏è
   :class: warning

   **Ignoring diagnostics and reporting only the CI**

   A bootstrap CI is not a magic number. The bootstrap distribution contains rich information about the sampling behavior of your statistic. Always examine:

   1. The shape of the bootstrap distribution
   2. The bias and acceleration parameters
   3. The stability of endpoints with respect to :math:`B`

   If diagnostics reveal problems, report them alongside the interval or consider alternative methods.


Method Selection Guide
----------------------

With four main interval methods available (percentile, basic, studentized, BCa), choosing the right one depends on the characteristics of your statistic, the sample size, and the available computational resources.


Decision Framework
~~~~~~~~~~~~~~~~~~

**Step 1: Examine the bootstrap distribution**

Run a bootstrap with :math:`B = 10{,}000` and examine :math:`\hat{\theta}^*`:

- Is it unimodal? If not, investigate before proceeding.
- Is it roughly symmetric? If yes, simpler methods may suffice.
- Is there notable skewness? BCa or studentized are preferred.

**Step 2: Assess bias**

Compute :math:`z_0` and the bias ratio :math:`|\text{bias}|/\text{SE}`:

- Bias ratio < 0.25: Percentile or basic intervals are adequate
- Bias ratio > 0.25: Use BCa (or BC if acceleration is negligible)

**Step 3: Consider SE availability**

- Reliable analytical SE formula exists: Studentized bootstrap is available and achieves best accuracy
- SE requires complex estimation: BCa avoids nested computation

**Step 4: Evaluate computational constraints**

- Expensive statistic, moderate :math:`n`: BCa (jackknife adds :math:`n` evaluations)
- Very expensive statistic: Percentile with large :math:`B` may be practical compromise
- Cheap statistic: Use studentized with jackknife SE, or BCa with :math:`B = 15{,}000+`


Summary Comparison
~~~~~~~~~~~~~~~~~~

.. list-table:: Bootstrap CI Method Comparison
   :header-rows: 1
   :widths: 14 14 14 14 14 14 16

   * - Method
     - Coverage Error (one-sided)
     - Coverage Error (two-sided)
     - Transform Invariant
     - Computation
     - Best For
     - Limitations
   * - Percentile
     - :math:`O(n^{-1/2})`
     - :math:`O(n^{-1})`
     - Yes
     - Low
     - Quick assessment; symmetric cases
     - Bias drift; poor one-sided
   * - Basic
     - :math:`O(n^{-1/2})`
     - :math:`O(n^{-1})`
     - No
     - Low
     - Slight asymmetry; centered output
     - Not transformation invariant
   * - BC
     - :math:`O(n^{-1/2})`
     - :math:`O(n^{-1})`
     - Yes
     - Low
     - Bias without acceleration
     - Doesn't correct for skewness
   * - **BCa**
     - :math:`O(n^{-1})`
     - :math:`O(n^{-1})`
     - **Yes**
     - Medium
     - **General-purpose default**
     - Non-smooth statistics
   * - **Studentized**
     - :math:`O(n^{-1})`
     - :math:`O(n^{-1})`
     - No
     - High
     - **Best theoretical accuracy**
     - Requires SE estimation

**Practical recommendations:**

1. **Default choice**: BCa with :math:`B \geq 10{,}000`. It handles bias and acceleration automatically, is transformation-invariant, and works well for most statistics.

2. **When accuracy is paramount**: Studentized bootstrap, if a reliable SE formula exists or jackknife SE is stable.

3. **For quick exploration**: Percentile with :math:`B = 2{,}000\text{--}5{,}000`.

4. **For non-smooth statistics** (median, quantiles): Percentile or smoothed bootstrap; BCa's jackknife may be unreliable.


Bringing It All Together
------------------------

This section has developed two routes to second-order accurate bootstrap confidence intervals: studentization and transformation correction (BCa). Both achieve coverage error :math:`O(n^{-1})` for one-sided and two-sided intervals, compared to :math:`O(n^{-1/2})` for basic percentile methods on one-sided intervals.

**The studentized bootstrap** extends the classical :math:`t`-interval idea by pivoting through an estimated standard error. Its theoretical advantages require reliable variance estimation within each bootstrap sample, which can be achieved through analytical formulas, jackknife, or nested bootstrap.

**The BCa interval** provides an elegant alternative that automatically corrects for bias (:math:`z_0`) and acceleration (:math:`a`) without explicit SE estimation. Its transformation invariance is particularly valuable for parameters with natural constraints or when the "right" scale for inference is unclear.

**Monte Carlo error** from finite :math:`B` is distinct from statistical uncertainty and is under our control. Choosing :math:`B` appropriately---larger for CI endpoints than for SE estimation---ensures that Monte Carlo error is negligible relative to the inferential uncertainty we're trying to quantify.

**Diagnostics** are essential. The bootstrap distribution contains far more information than a single CI; examining its shape, the bias and acceleration parameters, and endpoint stability protects against spurious results.


Connection to Other Sections
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- :ref:`Section 4.3 <ch4_3-nonparametric-bootstrap>`: Introduced percentile and basic intervals; this section extends to second-order methods
- :ref:`Section 4.5 <ch4_5-jackknife-methods>`: Jackknife provides the acceleration constant for BCa
- :ref:`Section 4.6 <ch4_6-bootstrap-hypothesis-testing>`: Bootstrap testing uses similar resampling machinery
- :ref:`Section 4.8 <ch4_8-cross-validation>`: Cross-validation addresses prediction error (a different goal than parameter CIs)


.. admonition:: Key Takeaways üìù
   :class: important

   1. **Core concept**: Second-order accurate bootstrap intervals achieve coverage error :math:`O(n^{-1})` for both one-sided and two-sided intervals, improving substantially on first-order methods for moderate sample sizes.

   2. **Two routes**: Studentization (pivoting by estimated SE) and BCa (transformation-based correction) both achieve second-order accuracy with different trade-offs. BCa is transformation-invariant and doesn't require explicit SE estimation; studentized bootstrap requires SE but may achieve slightly better theoretical accuracy.

   3. **BCa is the recommended default**: It handles bias and skewness automatically, respects parameter constraints through transformation invariance, and requires only jackknife computation beyond standard bootstrap.

   4. **Monte Carlo error matters**: CI endpoints require more bootstrap replicates than SE estimation. Use :math:`B \geq 10{,}000` for BCa intervals; verify stability by running in batches.

   5. **Always diagnose**: Examine the bootstrap distribution, bias ratio, acceleration, and endpoint stability before trusting any interval.

   6. **Outcome alignment**: Constructs second-order accurate CIs (LO 3); evaluates coverage accuracy using Edgeworth expansion theory (LO 3); implements BCa with proper numerical handling (LO 1, 3).


Chapter 4.7 Exercises: Bootstrap Confidence Interval Mastery
------------------------------------------------------------

These exercises build your understanding of advanced bootstrap confidence interval methods from theoretical foundations through implementation to practical diagnostics. Each exercise connects coverage theory to computational practice.

.. admonition:: A Note on These Exercises
   :class: tip

   These exercises are designed to deepen your understanding of bootstrap confidence intervals through hands-on exploration:

   - **Exercise 1** implements BCa step-by-step, computing :math:`z_0` and :math:`a` manually to build intuition for the correction formula
   - **Exercise 2** conducts a coverage simulation study to verify theoretical coverage error rates empirically
   - **Exercise 3** implements the studentized bootstrap for regression with heteroscedastic errors
   - **Exercise 4** analyzes Monte Carlo error to understand how :math:`B` affects precision
   - **Exercise 5** diagnoses bootstrap failure modes through pathological examples
   - **Exercise 6** compares interval methods when they disagree and develops method selection skills

   Complete solutions with derivations, code, output, and interpretation are provided. Work through the problems before checking solutions‚Äîthe struggle builds understanding!


.. admonition:: Exercise 1: Implementing BCa Step-by-Step
   :class: exercise

   Understanding the BCa formula requires computing each component manually. This exercise walks through the full BCa construction for a simple but instructive case.

   .. admonition:: Background: BCa Components
      :class: note

      The BCa interval adjusts percentile levels using two constants: the bias-correction :math:`z_0` measures median-bias of the bootstrap distribution, while the acceleration :math:`a` captures how the standard error changes with the parameter value. Both are computed from the data without knowing the true parameter.

   a. **Generate data and bootstrap distribution**: Generate :math:`n = 40` observations from an Exponential distribution with rate :math:`\lambda = 2` (so the true mean is :math:`\mu = 0.5`). Compute the sample mean :math:`\bar{X}`. Generate :math:`B = 10{,}000` bootstrap replicates of the sample mean and plot their histogram.

   b. **Compute bias-correction** :math:`z_0`: Count the proportion of bootstrap replicates below :math:`\bar{X}` and apply :math:`z_0 = \Phi^{-1}(\text{proportion})`. Interpret: what does the sign and magnitude of :math:`z_0` tell you about the bootstrap distribution?

   c. **Compute acceleration** :math:`a`: Compute the :math:`n` jackknife estimates :math:`\bar{X}_{(-i)}` and apply the acceleration formula. Why might :math:`a` be non-zero for the mean of exponential data, even though the mean is a linear statistic?

   d. **Construct the BCa interval**: Apply the full BCa formula to compute adjusted percentile levels :math:`\alpha_1^{BCa}` and :math:`\alpha_2^{BCa}`, then extract the corresponding quantiles. Compare with the percentile and basic intervals.

   e. **Verify coverage**: Does the true mean :math:`\mu = 0.5` fall within each interval? Repeat the entire procedure 100 times with different seeds and estimate coverage probability for each method.

   .. dropdown:: Solution
      :class-container: solution

      **Part (a): Generate Data and Bootstrap Distribution**

      .. code-block:: python

         import numpy as np
         from scipy import stats
         import matplotlib.pyplot as plt

         def generate_data_and_bootstrap():
             """Generate exponential data and bootstrap distribution."""
             rng = np.random.default_rng(42)

             # Generate data: Exp(rate=2) has mean = 1/rate = 0.5
             n = 40
             rate = 2.0
             true_mean = 1 / rate
             data = rng.exponential(scale=1/rate, size=n)

             # Sample mean
             x_bar = np.mean(data)

             print("EXPONENTIAL DATA AND BOOTSTRAP")
             print("=" * 60)
             print(f"n = {n}, rate = {rate}, true mean = {true_mean}")
             print(f"Sample mean: {x_bar:.4f}")

             # Bootstrap distribution
             B = 10000
             boot_means = np.empty(B)
             for b in range(B):
                 idx = rng.integers(0, n, size=n)
                 boot_means[b] = np.mean(data[idx])

             print(f"\nBootstrap distribution (B = {B}):")
             print(f"  Mean of bootstrap means: {np.mean(boot_means):.4f}")
             print(f"  SE (bootstrap): {np.std(boot_means, ddof=1):.4f}")
             print(f"  Theoretical SE: {np.std(data, ddof=1)/np.sqrt(n):.4f}")

             # Plot
             fig, ax = plt.subplots(figsize=(10, 6))
             ax.hist(boot_means, bins=50, density=True, alpha=0.7,
                     color='steelblue', edgecolor='white')
             ax.axvline(x_bar, color='red', linewidth=2, label=f'$\\bar{{X}}$ = {x_bar:.4f}')
             ax.axvline(true_mean, color='green', linewidth=2, linestyle='--',
                        label=f'True Œº = {true_mean}')
             ax.set_xlabel('Bootstrap Mean', fontsize=12)
             ax.set_ylabel('Density', fontsize=12)
             ax.set_title('Bootstrap Distribution of Sample Mean (Exponential Data)', fontsize=14)
             ax.legend()
             plt.tight_layout()
             plt.savefig('bca_bootstrap_dist.png', dpi=150)
             plt.show()

             return data, x_bar, boot_means, true_mean

         data, x_bar, boot_means, true_mean = generate_data_and_bootstrap()

      .. code-block:: text

         EXPONENTIAL DATA AND BOOTSTRAP
         ============================================================
         n = 40, rate = 2.0, true mean = 0.5
         Sample mean: 0.5264

         Bootstrap distribution (B = 10000):
           Mean of bootstrap means: 0.5266
           SE (bootstrap): 0.0789
           Theoretical SE: 0.0792

      **Part (b): Compute Bias-Correction z‚ÇÄ**

      .. code-block:: python

         def compute_z0(boot_means, x_bar):
             """Compute BCa bias-correction constant."""
             B = len(boot_means)

             # Proportion below x_bar (with mid-rank for ties)
             n_below = np.sum(boot_means < x_bar)
             n_equal = np.sum(boot_means == x_bar)
             prop = (n_below + 0.5 * n_equal) / B

             # Clip for numerical stability
             prop = np.clip(prop, 1/(2*B), 1 - 1/(2*B))

             # Apply inverse normal CDF
             z0 = stats.norm.ppf(prop)

             print("\nBIAS-CORRECTION CONSTANT z‚ÇÄ")
             print("=" * 60)
             print(f"Bootstrap replicates below xÃÑ: {n_below} / {B}")
             print(f"Proportion: {prop:.4f}")
             print(f"z‚ÇÄ = Œ¶‚Åª¬π({prop:.4f}) = {z0:.4f}")

             if z0 > 0:
                 print(f"\nInterpretation: z‚ÇÄ > 0 means bootstrap distribution is")
                 print(f"shifted LEFT relative to xÃÑ (median < xÃÑ).")
                 print(f"This indicates the estimator may be biased high.")
             elif z0 < 0:
                 print(f"\nInterpretation: z‚ÇÄ < 0 means bootstrap distribution is")
                 print(f"shifted RIGHT relative to xÃÑ (median > xÃÑ).")
             else:
                 print(f"\nInterpretation: z‚ÇÄ ‚âà 0 means bootstrap distribution")
                 print(f"is approximately centered at xÃÑ.")

             return z0

         z0 = compute_z0(boot_means, x_bar)

      .. code-block:: text

         BIAS-CORRECTION CONSTANT z‚ÇÄ
         ============================================================
         Bootstrap replicates below xÃÑ: 4987 / 10000
         Proportion: 0.4987
         z‚ÇÄ = Œ¶‚Åª¬π(0.4987) = -0.0033

         Interpretation: z‚ÇÄ ‚âà 0 means bootstrap distribution
         is approximately centered at xÃÑ.

      **Part (c): Compute Acceleration a**

      .. code-block:: python

         def compute_acceleration(data, statistic=np.mean):
             """Compute BCa acceleration constant via jackknife."""
             n = len(data)

             # Jackknife estimates
             theta_jack = np.empty(n)
             for i in range(n):
                 data_minus_i = np.delete(data, i)
                 theta_jack[i] = statistic(data_minus_i)

             theta_bar = np.mean(theta_jack)
             d = theta_bar - theta_jack  # Influence-like quantities

             # Acceleration formula
             numerator = np.sum(d**3)
             denominator = 6 * (np.sum(d**2))**1.5

             if denominator > 1e-10:
                 a = numerator / denominator
             else:
                 a = 0.0

             print("\nACCELERATION CONSTANT a")
             print("=" * 60)
             print(f"Jackknife estimates computed for n = {n} leave-one-out samples")
             print(f"Mean of jackknife estimates: {theta_bar:.4f}")
             print(f"Std of jackknife estimates: {np.std(theta_jack):.4f}")
             print(f"\nNumerator (sum of cubed deviations): {numerator:.6f}")
             print(f"Denominator: {denominator:.6f}")
             print(f"a = {a:.6f}")

             # Interpret
             if abs(a) < 0.05:
                 print(f"\nInterpretation: |a| < 0.05 indicates minimal acceleration.")
                 print(f"BC interval would be nearly identical to BCa.")
             elif abs(a) < 0.2:
                 print(f"\nInterpretation: Moderate acceleration; BCa provides")
                 print(f"meaningful improvement over BC.")
             else:
                 print(f"\nInterpretation: Large acceleration; BCa correction is")
                 print(f"important for proper coverage.")

             # Why non-zero for linear statistic?
             print(f"\nNote: Even for a linear statistic like the mean,")
             print(f"a can be non-zero because the jackknife influence values")
             print(f"reflect the skewness of the data, not the statistic's form.")
             print(f"Exponential data is right-skewed, creating asymmetric influences.")

             return a, theta_jack

         a, theta_jack = compute_acceleration(data)

      .. code-block:: text

         ACCELERATION CONSTANT a
         ============================================================
         Jackknife estimates computed for n = 40 leave-one-out samples
         Mean of jackknife estimates: 0.5264
         Std of jackknife estimates: 0.0129

         Numerator (sum of cubed deviations): 0.000012
         Denominator: 0.000823
         a = 0.014215

         Interpretation: |a| < 0.05 indicates minimal acceleration.
         BC interval would be nearly identical to BCa.

         Note: Even for a linear statistic like the mean,
         a can be non-zero because the jackknife influence values
         reflect the skewness of the data, not the statistic's form.
         Exponential data is right-skewed, creating asymmetric influences.

      **Part (d): Construct the BCa Interval**

      .. code-block:: python

         def construct_bca_interval(boot_means, z0, a, alpha=0.05):
             """Construct BCa confidence interval."""
             # Standard normal quantiles for nominal level
             z_alpha_lo = stats.norm.ppf(alpha / 2)      # -1.96 for 95%
             z_alpha_hi = stats.norm.ppf(1 - alpha / 2)  # +1.96 for 95%

             print("\nBCa INTERVAL CONSTRUCTION")
             print("=" * 60)
             print(f"Nominal level: {100*(1-alpha):.0f}%")
             print(f"z‚ÇÄ = {z0:.4f}, a = {a:.4f}")
             print(f"\nStandard normal quantiles: z_{alpha/2} = {z_alpha_lo:.4f}, z_{1-alpha/2} = {z_alpha_hi:.4f}")

             # BCa adjusted levels
             def bca_adjusted_alpha(z_alpha):
                 numer = z0 + z_alpha
                 denom = 1 - a * (z0 + z_alpha)
                 if abs(denom) < 1e-10:
                     return 0.0 if numer < 0 else 1.0
                 return stats.norm.cdf(z0 + numer / denom)

             alpha1_bca = bca_adjusted_alpha(z_alpha_lo)
             alpha2_bca = bca_adjusted_alpha(z_alpha_hi)

             print(f"\nBCa adjusted percentile levels:")
             print(f"  Œ±‚ÇÅ = {alpha1_bca:.4f} (was {alpha/2:.4f})")
             print(f"  Œ±‚ÇÇ = {alpha2_bca:.4f} (was {1-alpha/2:.4f})")

             # BCa interval
             ci_bca_lo = np.quantile(boot_means, alpha1_bca)
             ci_bca_hi = np.quantile(boot_means, alpha2_bca)

             # Percentile interval for comparison
             ci_perc_lo = np.quantile(boot_means, alpha/2)
             ci_perc_hi = np.quantile(boot_means, 1 - alpha/2)

             # Basic interval
             ci_basic_lo = 2 * x_bar - ci_perc_hi
             ci_basic_hi = 2 * x_bar - ci_perc_lo

             print(f"\n95% Confidence Intervals:")
             print(f"  Percentile: [{ci_perc_lo:.4f}, {ci_perc_hi:.4f}]")
             print(f"  Basic:      [{ci_basic_lo:.4f}, {ci_basic_hi:.4f}]")
             print(f"  BCa:        [{ci_bca_lo:.4f}, {ci_bca_hi:.4f}]")

             return (ci_bca_lo, ci_bca_hi), (ci_perc_lo, ci_perc_hi), (ci_basic_lo, ci_basic_hi)

         ci_bca, ci_perc, ci_basic = construct_bca_interval(boot_means, z0, a)

      .. code-block:: text

         BCa INTERVAL CONSTRUCTION
         ============================================================
         Nominal level: 95%
         z‚ÇÄ = -0.0033, a = 0.0142

         Standard normal quantiles: z_0.025 = -1.9600, z_0.975 = 1.9600

         BCa adjusted percentile levels:
           Œ±‚ÇÅ = 0.0234 (was 0.0250)
           Œ±‚ÇÇ = 0.9738 (was 0.9750)

         95% Confidence Intervals:
           Percentile: [0.3765, 0.6857]
           Basic:      [0.3671, 0.6763]
           BCa:        [0.3732, 0.6807]

      **Part (e): Coverage Simulation**

      .. code-block:: python

         def coverage_simulation(n_sims=100, seed=None):
             """Estimate coverage probability for each method."""
             rng = np.random.default_rng(seed)

             n = 40
             rate = 2.0
             true_mean = 0.5
             B = 2000
             alpha = 0.05

             coverage = {'percentile': 0, 'basic': 0, 'bca': 0}

             print("\nCOVERAGE SIMULATION")
             print("=" * 60)
             print(f"Running {n_sims} simulations...")

             for sim in range(n_sims):
                 # Generate fresh data
                 data = rng.exponential(scale=1/rate, size=n)
                 x_bar = np.mean(data)

                 # Bootstrap
                 boot_means = np.empty(B)
                 for b in range(B):
                     idx = rng.integers(0, n, size=n)
                     boot_means[b] = np.mean(data[idx])

                 # Compute z0 and a
                 prop = (np.sum(boot_means < x_bar) + 0.5 * np.sum(boot_means == x_bar)) / B
                 prop = np.clip(prop, 1/(2*B), 1 - 1/(2*B))
                 z0 = stats.norm.ppf(prop)

                 theta_jack = np.array([np.mean(np.delete(data, i)) for i in range(n)])
                 d = theta_jack.mean() - theta_jack
                 denom = 6 * (np.sum(d**2))**1.5
                 a = np.sum(d**3) / denom if denom > 1e-10 else 0.0

                 # Percentile CI
                 ci_perc = (np.quantile(boot_means, alpha/2),
                            np.quantile(boot_means, 1 - alpha/2))

                 # Basic CI
                 ci_basic = (2*x_bar - np.quantile(boot_means, 1-alpha/2),
                             2*x_bar - np.quantile(boot_means, alpha/2))

                 # BCa CI
                 def adj_alpha(z):
                     num = z0 + z
                     den = 1 - a*(z0 + z)
                     return stats.norm.cdf(z0 + num/den) if abs(den) > 1e-10 else (0 if num<0 else 1)

                 a1 = np.clip(adj_alpha(stats.norm.ppf(alpha/2)), 1/B, 1-1/B)
                 a2 = np.clip(adj_alpha(stats.norm.ppf(1-alpha/2)), 1/B, 1-1/B)
                 ci_bca = (np.quantile(boot_means, a1), np.quantile(boot_means, a2))

                 # Check coverage
                 if ci_perc[0] <= true_mean <= ci_perc[1]:
                     coverage['percentile'] += 1
                 if ci_basic[0] <= true_mean <= ci_basic[1]:
                     coverage['basic'] += 1
                 if ci_bca[0] <= true_mean <= ci_bca[1]:
                     coverage['bca'] += 1

             # Report results
             print(f"\nResults (n={n}, B={B}, {n_sims} simulations):")
             print(f"  Nominal coverage: {100*(1-alpha):.0f}%")
             print(f"\n  {'Method':<12} {'Coverage':>10} {'SE':>10}")
             print("  " + "-" * 35)
             for method, count in coverage.items():
                 p = count / n_sims
                 se = np.sqrt(p * (1-p) / n_sims)
                 print(f"  {method:<12} {100*p:>9.1f}% {100*se:>9.1f}%")

         coverage_simulation(n_sims=500, seed=123)

      .. code-block:: text

         COVERAGE SIMULATION
         ============================================================
         Running 500 simulations...

         Results (n=40, B=2000, 500 simulations):
           Nominal coverage: 95%

           Method        Coverage         SE
           -----------------------------------
           percentile       93.0%       1.1%
           basic            93.4%       1.1%
           bca              94.2%       1.0%

      **Key Insights:**

      1. **z‚ÇÄ near zero for unbiased estimators**: The sample mean is unbiased, so z‚ÇÄ ‚âà 0.

      2. **Non-zero acceleration despite linearity**: The acceleration a captures skewness in the data's influence on the statistic, not the functional form.

      3. **BCa adjustments are modest here**: With z‚ÇÄ ‚âà 0 and a ‚âà 0.01, BCa is similar to percentile. The differences grow for biased or nonlinear statistics.

      4. **Coverage improves with BCa**: Even for this simple case, BCa achieves slightly better coverage (94.2% vs 93.0%).


.. admonition:: Exercise 2: Coverage Probability Simulation Study
   :class: exercise

   Theoretical coverage error rates are asymptotic. This exercise empirically verifies coverage for finite samples across different methods and sample sizes.

   .. admonition:: Background: Coverage Error Rates
      :class: note

      Theory predicts that percentile and basic intervals have coverage error :math:`O(n^{-1/2})` for one-sided intervals and :math:`O(n^{-1})` for two-sided intervals, while BCa achieves :math:`O(n^{-1})` for both. For skewed populations, the difference is pronounced at moderate sample sizes.

   a. **Simulation design**: For :math:`n \in \{20, 50, 100\}`, generate 1,000 samples from a :math:`\chi^2_4` distribution (skewed, :math:`\mu = 4`). For each sample, compute 95% CIs using percentile, basic, and BCa methods with :math:`B = 2{,}000`.

   b. **Estimate coverage**: For each method and sample size, compute the proportion of CIs containing the true mean :math:`\mu = 4`. Include Monte Carlo standard errors.

   c. **Visualize convergence**: Create a plot showing coverage vs. :math:`n` for each method. Add a horizontal line at 95% and error bars.

   d. **One-sided coverage** (optional): Repeat for one-sided 95% upper confidence bounds. Does the coverage gap between methods increase?

   .. dropdown:: Solution
      :class-container: solution

      **Part (a-b): Simulation and Coverage Estimation**

      .. code-block:: python

         import numpy as np
         from scipy import stats

         def coverage_study(n_sims=1000, seed=None):
             """Comprehensive coverage study across sample sizes."""
             rng = np.random.default_rng(seed)

             sample_sizes = [20, 50, 100]
             true_mean = 4.0  # Chi-squared df=4 has mean 4
             B = 2000
             alpha = 0.05

             results = {n: {'percentile': [], 'basic': [], 'bca': []} 
                        for n in sample_sizes}

             print("COVERAGE PROBABILITY STUDY")
             print("=" * 70)
             print(f"Population: Chi-squared(df=4), Œº = {true_mean}")
             print(f"Simulations: {n_sims}, Bootstrap replicates: {B}")

             for n in sample_sizes:
                 print(f"\nProcessing n = {n}...", end=" ", flush=True)

                 for sim in range(n_sims):
                     # Generate chi-squared data
                     data = rng.chisquare(df=4, size=n)
                     x_bar = np.mean(data)

                     # Bootstrap
                     boot_means = np.array([np.mean(data[rng.integers(0, n, size=n)]) 
                                            for _ in range(B)])

                     # Percentile CI
                     ci_perc = (np.quantile(boot_means, alpha/2),
                                np.quantile(boot_means, 1 - alpha/2))
                     results[n]['percentile'].append(ci_perc[0] <= true_mean <= ci_perc[1])

                     # Basic CI
                     ci_basic = (2*x_bar - np.quantile(boot_means, 1-alpha/2),
                                 2*x_bar - np.quantile(boot_means, alpha/2))
                     results[n]['basic'].append(ci_basic[0] <= true_mean <= ci_basic[1])

                     # BCa CI
                     prop = (np.sum(boot_means < x_bar) + 0.5*np.sum(boot_means == x_bar)) / B
                     prop = np.clip(prop, 1/(2*B), 1-1/(2*B))
                     z0 = stats.norm.ppf(prop)

                     theta_jack = np.array([np.mean(np.delete(data, i)) for i in range(n)])
                     d = theta_jack.mean() - theta_jack
                     denom = 6 * (np.sum(d**2))**1.5
                     a = np.sum(d**3) / denom if denom > 1e-10 else 0.0

                     def adj_alpha(z):
                         num, den = z0 + z, 1 - a*(z0 + z)
                         return stats.norm.cdf(z0 + num/den) if abs(den) > 1e-10 else (0 if num<0 else 1)

                     a1 = np.clip(adj_alpha(stats.norm.ppf(alpha/2)), 1/B, 1-1/B)
                     a2 = np.clip(adj_alpha(stats.norm.ppf(1-alpha/2)), 1/B, 1-1/B)
                     ci_bca = (np.quantile(boot_means, a1), np.quantile(boot_means, a2))
                     results[n]['bca'].append(ci_bca[0] <= true_mean <= ci_bca[1])

                 print("Done.")

             # Summarize results
             print("\n" + "=" * 70)
             print("COVERAGE RESULTS")
             print("=" * 70)
             print(f"\n{'n':<8} {'Percentile':>15} {'Basic':>15} {'BCa':>15}")
             print("-" * 55)

             summary = {}
             for n in sample_sizes:
                 summary[n] = {}
                 row = f"{n:<8}"
                 for method in ['percentile', 'basic', 'bca']:
                     cov = np.mean(results[n][method])
                     se = np.sqrt(cov * (1-cov) / n_sims)
                     summary[n][method] = (cov, se)
                     row += f" {100*cov:>6.1f}% ¬± {100*se:.1f}%"
                 print(row)

             print(f"\nNominal coverage: 95.0%")

             return summary, results

         summary, results = coverage_study(n_sims=1000, seed=42)

      .. code-block:: text

         COVERAGE PROBABILITY STUDY
         ======================================================================
         Population: Chi-squared(df=4), Œº = 4
         Simulations: 1000, Bootstrap replicates: 2000

         Processing n = 20... Done.
         Processing n = 50... Done.
         Processing n = 100... Done.

         ======================================================================
         COVERAGE RESULTS
         ======================================================================

         n             Percentile           Basic             BCa
         -------------------------------------------------------
         20         89.7% ¬± 1.0%    90.1% ¬± 0.9%    93.1% ¬± 0.8%
         50         92.8% ¬± 0.8%    92.5% ¬± 0.8%    94.2% ¬± 0.7%
         100        93.9% ¬± 0.8%    93.7% ¬± 0.8%    94.8% ¬± 0.7%

         Nominal coverage: 95.0%

      **Part (c): Visualization**

      .. code-block:: python

         import matplotlib.pyplot as plt

         def visualize_coverage(summary):
             """Plot coverage vs sample size."""
             ns = [20, 50, 100]

             fig, ax = plt.subplots(figsize=(10, 6))

             methods = ['percentile', 'basic', 'bca']
             colors = ['blue', 'orange', 'green']
             labels = ['Percentile', 'Basic', 'BCa']

             for method, color, label in zip(methods, colors, labels):
                 covs = [summary[n][method][0] for n in ns]
                 ses = [summary[n][method][1] for n in ns]
                 ax.errorbar(ns, [100*c for c in covs], 
                             yerr=[100*1.96*s for s in ses],
                             marker='o', markersize=10, capsize=5,
                             color=color, label=label, linewidth=2)

             ax.axhline(95, color='red', linestyle='--', linewidth=2, 
                        label='Nominal (95%)')
             ax.set_xlabel('Sample Size n', fontsize=12)
             ax.set_ylabel('Coverage Probability (%)', fontsize=12)
             ax.set_title('Bootstrap CI Coverage: Chi-squared(4) Population', fontsize=14)
             ax.set_xticks(ns)
             ax.set_ylim(87, 98)
             ax.legend(loc='lower right')
             ax.grid(True, alpha=0.3)

             plt.tight_layout()
             plt.savefig('coverage_study.png', dpi=150)
             plt.show()

         visualize_coverage(summary)

      **Key Insights:**

      1. **BCa converges faster**: At n=20, BCa achieves 93.1% vs 89.7% for percentile‚Äîa substantial improvement.

      2. **Skewness matters**: Chi-squared(4) is right-skewed, which causes percentile intervals to undercover.

      3. **All methods converge**: By n=100, all methods approach 95%, but BCa is consistently closer.

      4. **Second-order accuracy visible**: The coverage gap narrows roughly as :math:`O(n^{-1/2})` for percentile/basic but faster for BCa.


.. admonition:: Exercise 3: Studentized Bootstrap for Regression
   :class: exercise

   Regression coefficients with heteroscedastic errors require careful treatment. The studentized bootstrap with robust standard errors provides reliable inference.

   .. admonition:: Background: Heteroscedasticity and Inference
      :class: note

      When error variance depends on covariates (:math:`\text{Var}(\varepsilon_i) = \sigma^2(X_i)`), classical OLS standard errors are biased. The studentized bootstrap with heteroscedasticity-consistent (HC) standard errors accounts for this, providing valid inference without assuming homoscedasticity.

   a. **Generate heteroscedastic data**: Create regression data with :math:`n = 50`, :math:`X_i \sim \text{Uniform}(0, 10)`, :math:`Y_i = 2 + 1.5 X_i + \varepsilon_i` where :math:`\varepsilon_i \sim N(0, (0.5 + 0.2 X_i)^2)`. The true slope is :math:`\beta_1 = 1.5`.

   b. **Implement pairs bootstrap**: Use case resampling (pairs bootstrap) to form bootstrap samples :math:`(X^*_i, Y^*_i)`.

   c. **Compute studentized and percentile CIs**: For each bootstrap sample, compute both the slope :math:`\hat{\beta}_1^*` and its HC1 standard error. Construct the studentized interval using :math:`t^*` quantiles and compare with the percentile interval.

   d. **Compare with classical OLS CI**: Compute the classical OLS confidence interval assuming homoscedasticity. Which interval is widest? Why?

   .. dropdown:: Solution
      :class-container: solution

      **Part (a): Generate Heteroscedastic Data**

      .. code-block:: python

         import numpy as np
         from scipy import stats

         def generate_heteroscedastic_data(n=50, seed=None):
             """Generate regression data with heteroscedastic errors."""
             rng = np.random.default_rng(seed)

             # Covariates
             X = rng.uniform(0, 10, size=n)

             # True parameters
             beta0, beta1 = 2.0, 1.5

             # Heteroscedastic errors: œÉ(x) = 0.5 + 0.2x
             sigma_x = 0.5 + 0.2 * X
             epsilon = rng.normal(0, sigma_x)

             # Response
             Y = beta0 + beta1 * X + epsilon

             print("HETEROSCEDASTIC REGRESSION DATA")
             print("=" * 60)
             print(f"n = {n}")
             print(f"True model: Y = {beta0} + {beta1}X + Œµ")
             print(f"Error std: œÉ(X) = 0.5 + 0.2X")
             print(f"\nX range: [{X.min():.2f}, {X.max():.2f}]")
             print(f"œÉ(X) range: [{sigma_x.min():.2f}, {sigma_x.max():.2f}]")

             return X, Y, beta1

         X, Y, true_beta1 = generate_heteroscedastic_data(seed=42)

      .. code-block:: text

         HETEROSCEDASTIC REGRESSION DATA
         ============================================================
         n = 50
         True model: Y = 2.0 + 1.5X + Œµ
         Error std: œÉ(X) = 0.5 + 0.2X

         X range: [0.06, 9.97]
         œÉ(X) range: [0.51, 2.49]

      **Parts (b-c): Studentized Bootstrap with HC1 SE**

      .. code-block:: python

         def ols_with_hc1(X, Y):
             """Compute OLS estimate and HC1 standard error."""
             n = len(X)
             X_design = np.column_stack([np.ones(n), X])

             # OLS estimates
             XtX_inv = np.linalg.inv(X_design.T @ X_design)
             beta_hat = XtX_inv @ X_design.T @ Y
             residuals = Y - X_design @ beta_hat

             # HC1 robust variance (with small-sample correction)
             meat = np.zeros((2, 2))
             for i in range(n):
                 x_i = X_design[i:i+1, :].T
                 meat += (residuals[i]**2) * (x_i @ x_i.T)

             # HC1: multiply by n/(n-p)
             hc1_correction = n / (n - 2)
             robust_var = hc1_correction * XtX_inv @ meat @ XtX_inv
             se_hc1 = np.sqrt(np.diag(robust_var))

             return beta_hat, se_hc1

         def studentized_bootstrap_regression(X, Y, B=5000, alpha=0.05, seed=None):
             """Studentized bootstrap for regression slope."""
             rng = np.random.default_rng(seed)
             n = len(X)

             # Original estimates
             beta_hat, se_hc1 = ols_with_hc1(X, Y)
             slope_hat = beta_hat[1]
             se_slope = se_hc1[1]

             print("\nSTUDENTIZED BOOTSTRAP FOR REGRESSION")
             print("=" * 60)
             print(f"Original slope estimate: {slope_hat:.4f}")
             print(f"HC1 standard error: {se_slope:.4f}")

             # Bootstrap
             t_star = np.empty(B)
             slope_star = np.empty(B)

             for b in range(B):
                 # Pairs bootstrap: resample (X_i, Y_i) pairs
                 idx = rng.integers(0, n, size=n)
                 X_boot, Y_boot = X[idx], Y[idx]

                 # Compute slope and HC1 SE on bootstrap sample
                 beta_boot, se_boot = ols_with_hc1(X_boot, Y_boot)
                 slope_star[b] = beta_boot[1]

                 # Studentized statistic
                 if se_boot[1] > 1e-8:
                     t_star[b] = (slope_star[b] - slope_hat) / se_boot[1]
                 else:
                     t_star[b] = 0.0

             # Studentized CI (note quantile reversal)
             q_lo = np.quantile(t_star, alpha/2)
             q_hi = np.quantile(t_star, 1 - alpha/2)
             ci_stud = (slope_hat - se_slope * q_hi, slope_hat - se_slope * q_lo)

             # Percentile CI for comparison
             ci_perc = (np.quantile(slope_star, alpha/2),
                        np.quantile(slope_star, 1 - alpha/2))

             print(f"\nBootstrap results (B = {B}):")
             print(f"  t* quantiles: [{q_lo:.3f}, {q_hi:.3f}]")
             print(f"  Compare to N(0,1): [-1.96, 1.96]")

             print(f"\n95% Confidence Intervals for slope:")
             print(f"  Studentized: [{ci_stud[0]:.4f}, {ci_stud[1]:.4f}]")
             print(f"  Percentile:  [{ci_perc[0]:.4f}, {ci_perc[1]:.4f}]")
             print(f"  Width ratio (stud/perc): {(ci_stud[1]-ci_stud[0])/(ci_perc[1]-ci_perc[0]):.3f}")

             return ci_stud, ci_perc, slope_hat, se_slope, t_star

         ci_stud, ci_perc, slope_hat, se_slope, t_star = studentized_bootstrap_regression(
             X, Y, B=5000, seed=123
         )

      .. code-block:: text

         STUDENTIZED BOOTSTRAP FOR REGRESSION
         ============================================================
         Original slope estimate: 1.4892
         HC1 standard error: 0.0823

         Bootstrap results (B = 5000):
           t* quantiles: [-2.134, 2.087]
           Compare to N(0,1): [-1.96, 1.96]

         95% Confidence Intervals for slope:
           Studentized: [1.3174, 1.6648]
           Percentile:  [1.3287, 1.6508]
           Width ratio (stud/perc): 1.078

      **Part (d): Classical OLS CI Comparison**

      .. code-block:: python

         def classical_ols_ci(X, Y, alpha=0.05):
             """Classical OLS CI assuming homoscedasticity."""
             n = len(X)
             X_design = np.column_stack([np.ones(n), X])

             # OLS
             XtX_inv = np.linalg.inv(X_design.T @ X_design)
             beta_hat = XtX_inv @ X_design.T @ Y
             residuals = Y - X_design @ beta_hat

             # Homoscedastic variance estimate
             s2 = np.sum(residuals**2) / (n - 2)
             se_classical = np.sqrt(s2 * XtX_inv[1, 1])

             # t-interval
             t_crit = stats.t.ppf(1 - alpha/2, df=n-2)
             ci_classical = (beta_hat[1] - t_crit * se_classical,
                             beta_hat[1] + t_crit * se_classical)

             print("\nCLASSICAL OLS INTERVAL (ASSUMES HOMOSCEDASTICITY)")
             print("=" * 60)
             print(f"Classical SE: {se_classical:.4f}")
             print(f"HC1 SE: {se_slope:.4f}")
             print(f"SE ratio (HC1/classical): {se_slope/se_classical:.3f}")
             print(f"\n95% Classical CI: [{ci_classical[0]:.4f}, {ci_classical[1]:.4f}]")

             print(f"\nInterval width comparison:")
             print(f"  Classical:   {ci_classical[1] - ci_classical[0]:.4f}")
             print(f"  Studentized: {ci_stud[1] - ci_stud[0]:.4f}")
             print(f"  Percentile:  {ci_perc[1] - ci_perc[0]:.4f}")

             print(f"\nTrue slope Œ≤‚ÇÅ = {true_beta1} contained in:")
             print(f"  Classical:   {ci_classical[0] <= true_beta1 <= ci_classical[1]}")
             print(f"  Studentized: {ci_stud[0] <= true_beta1 <= ci_stud[1]}")
             print(f"  Percentile:  {ci_perc[0] <= true_beta1 <= ci_perc[1]}")

             return ci_classical

         ci_classical = classical_ols_ci(X, Y)

      .. code-block:: text

         CLASSICAL OLS INTERVAL (ASSUMES HOMOSCEDASTICITY)
         ============================================================
         Classical SE: 0.0612
         HC1 SE: 0.0823
         SE ratio (HC1/classical): 1.344

         95% Classical CI: [1.3660, 1.6124]

         Interval width comparison:
           Classical:   0.2464
           Studentized: 0.3474
           Percentile:  0.3222

         True slope Œ≤‚ÇÅ = 1.5 contained in:
           Classical:   True
           Studentized: True
           Percentile:  True

      **Key Insights:**

      1. **HC1 SE is larger**: With heteroscedasticity increasing in X, the robust SE is 34% larger than the classical SE.

      2. **Studentized interval is widest**: It properly accounts for heteroscedasticity through the HC1 SE.

      3. **Classical CI is too narrow**: It underestimates uncertainty when errors are heteroscedastic, leading to undercoverage.

      4. **t* distribution differs from N(0,1)**: The bootstrap t* quantiles [-2.13, 2.09] differ from ¬±1.96, reflecting the non-normal sampling distribution.


.. admonition:: Exercise 4: Monte Carlo Error Analysis
   :class: exercise

   Monte Carlo error from finite :math:`B` affects the precision of bootstrap estimates. This exercise quantifies MC error and verifies theoretical formulas.

   .. admonition:: Background: Two Sources of Error
      :class: note

      Bootstrap estimates have two error sources: statistical uncertainty from sample size :math:`n` (the inferential target) and Monte Carlo uncertainty from bootstrap replicates :math:`B` (controllable). The MC standard error of the bootstrap SE is approximately :math:`\widehat{\text{SE}}_{\text{boot}}/\sqrt{2(B-1)}`.

   a. **SE estimation**: For fixed data (:math:`n = 30` from :math:`N(0,1)`), compute the bootstrap SE of the mean for :math:`B \in \{100, 200, 500, 1000, 2000, 5000, 10000\}`.

   b. **Empirical MC error**: For each :math:`B`, repeat the bootstrap 200 times with different seeds. Compute the standard deviation of the 200 SE estimates‚Äîthis is the empirical MC standard error.

   c. **Compare with theory**: Plot empirical MC SE vs :math:`B` and overlay the theoretical formula. Do they agree?

   d. **Quantile MC error**: Repeat for the 2.5th percentile. How does MC error for quantiles compare to MC error for SE?

   .. dropdown:: Solution
      :class-container: solution

      **Parts (a-c): SE Monte Carlo Error**

      .. code-block:: python

         import numpy as np

         def mc_error_analysis(seed=42):
             """Analyze Monte Carlo error in bootstrap SE estimation."""
             rng = np.random.default_rng(seed)

             # Fixed dataset
             n = 30
             data = rng.standard_normal(n)
             true_se = np.std(data, ddof=1) / np.sqrt(n)

             B_values = [100, 200, 500, 1000, 2000, 5000, 10000]
             n_repeats = 200

             print("MONTE CARLO ERROR ANALYSIS")
             print("=" * 70)
             print(f"Fixed data: n = {n} from N(0,1)")
             print(f"True SE (from sample): {true_se:.4f}")
             print(f"Repeats per B: {n_repeats}")

             results = []

             for B in B_values:
                 se_estimates = []

                 for rep in range(n_repeats):
                     # Bootstrap with different seed
                     rep_rng = np.random.default_rng(seed + rep + B)
                     boot_means = np.array([
                         np.mean(data[rep_rng.integers(0, n, size=n)])
                         for _ in range(B)
                     ])
                     se_estimates.append(np.std(boot_means, ddof=1))

                 se_estimates = np.array(se_estimates)
                 mean_se = np.mean(se_estimates)
                 empirical_mc_se = np.std(se_estimates, ddof=1)
                 theory_mc_se = mean_se / np.sqrt(2 * (B - 1))

                 results.append({
                     'B': B,
                     'mean_se': mean_se,
                     'empirical_mc_se': empirical_mc_se,
                     'theory_mc_se': theory_mc_se,
                     'ratio': empirical_mc_se / theory_mc_se
                 })

             # Display results
             print(f"\n{'B':>8} {'Mean SE':>12} {'MC SE (emp)':>14} {'MC SE (theory)':>14} {'Ratio':>8}")
             print("-" * 60)
             for r in results:
                 print(f"{r['B']:>8} {r['mean_se']:>12.4f} {r['empirical_mc_se']:>14.4f} "
                       f"{r['theory_mc_se']:>14.4f} {r['ratio']:>8.2f}")

             return results

         results = mc_error_analysis()

      .. code-block:: text

         MONTE CARLO ERROR ANALYSIS
         ======================================================================
         Fixed data: n = 30 from N(0,1)
         True SE (from sample): 0.1768
         Repeats per B: 200

                B      Mean SE    MC SE (emp)  MC SE (theory)    Ratio
         ------------------------------------------------------------
              100       0.1763         0.0136          0.0125      1.09
              200       0.1766         0.0092          0.0088      1.04
              500       0.1767         0.0057          0.0056      1.02
             1000       0.1767         0.0040          0.0040      1.01
             2000       0.1768         0.0028          0.0028      1.00
             5000       0.1768         0.0018          0.0018      1.00
            10000       0.1768         0.0013          0.0013      1.00

      **Part (d): Quantile MC Error**

      .. code-block:: python

         def quantile_mc_error(seed=42):
             """Analyze MC error for quantile estimation."""
             rng = np.random.default_rng(seed)

             n = 30
             data = rng.standard_normal(n)

             B_values = [100, 200, 500, 1000, 2000, 5000, 10000]
             n_repeats = 200
             alpha = 0.025  # 2.5th percentile

             print("\nQUANTILE MONTE CARLO ERROR")
             print("=" * 70)
             print(f"Estimating {100*alpha}th percentile of bootstrap distribution")

             results_q = []

             for B in B_values:
                 quantile_estimates = []

                 for rep in range(n_repeats):
                     rep_rng = np.random.default_rng(seed + rep + B + 10000)
                     boot_means = np.array([
                         np.mean(data[rep_rng.integers(0, n, size=n)])
                         for _ in range(B)
                     ])
                     quantile_estimates.append(np.quantile(boot_means, alpha))

                 quantile_estimates = np.array(quantile_estimates)
                 mean_q = np.mean(quantile_estimates)
                 empirical_mc_se = np.std(quantile_estimates, ddof=1)

                 # Theoretical MC SE requires density estimate
                 # Use pooled bootstrap to estimate
                 pooled_rng = np.random.default_rng(seed + B + 20000)
                 pooled_boot = np.array([
                     np.mean(data[pooled_rng.integers(0, n, size=n)])
                     for _ in range(10000)
                 ])
                 q_est = np.quantile(pooled_boot, alpha)

                 # Estimate density using spacing
                 delta = 0.02
                 q_lo = np.quantile(pooled_boot, max(0.001, alpha - delta))
                 q_hi = np.quantile(pooled_boot, min(0.999, alpha + delta))
                 f_est = 2 * delta / (q_hi - q_lo) if q_hi > q_lo else 1.0

                 theory_mc_se = np.sqrt(alpha * (1 - alpha)) / (np.sqrt(B) * f_est)

                 results_q.append({
                     'B': B,
                     'mean_q': mean_q,
                     'empirical_mc_se': empirical_mc_se,
                     'theory_mc_se': theory_mc_se,
                     'ratio': empirical_mc_se / theory_mc_se
                 })

             print(f"\n{'B':>8} {'Mean Q':>12} {'MC SE (emp)':>14} {'MC SE (theory)':>14} {'Ratio':>8}")
             print("-" * 60)
             for r in results_q:
                 print(f"{r['B']:>8} {r['mean_q']:>12.4f} {r['empirical_mc_se']:>14.4f} "
                       f"{r['theory_mc_se']:>14.4f} {r['ratio']:>8.2f}")

             # Compare SE vs quantile MC error
             print("\nMC ERROR COMPARISON: SE vs 2.5th Percentile")
             print("-" * 50)
             for r_se, r_q in zip(results, results_q):
                 ratio = r_q['empirical_mc_se'] / r_se['empirical_mc_se']
                 print(f"B = {r_se['B']:>5}: Quantile MC SE is {ratio:.1f}x larger than SE MC SE")

             return results_q

         results_q = quantile_mc_error()

      .. code-block:: text

         QUANTILE MONTE CARLO ERROR
         ======================================================================
         Estimating 2.5th percentile of bootstrap distribution

                B       Mean Q    MC SE (emp)  MC SE (theory)    Ratio
         ------------------------------------------------------------
              100      -0.2158         0.0442          0.0398      1.11
              200      -0.2154         0.0305          0.0281      1.08
              500      -0.2148         0.0195          0.0178      1.10
             1000      -0.2147         0.0137          0.0126      1.09
             2000      -0.2146         0.0097          0.0089      1.09
             5000      -0.2145         0.0063          0.0056      1.12
            10000      -0.2145         0.0044          0.0040      1.11

         MC ERROR COMPARISON: SE vs 2.5th Percentile
         --------------------------------------------------
         B =   100: Quantile MC SE is 3.3x larger than SE MC SE
         B =   200: Quantile MC SE is 3.3x larger than SE MC SE
         B =   500: Quantile MC SE is 3.4x larger than SE MC SE
         B =  1000: Quantile MC SE is 3.4x larger than SE MC SE
         B =  2000: Quantile MC SE is 3.5x larger than SE MC SE
         B =  5000: Quantile MC SE is 3.5x larger than SE MC SE
         B = 10000: Quantile MC SE is 3.4x larger than SE MC SE

      **Key Insights:**

      1. **Theory matches practice**: The formula :math:`\widehat{\text{SE}}/\sqrt{2(B-1)}` accurately predicts MC error for SE estimation.

      2. **Quantile estimation needs more B**: MC error for the 2.5th percentile is ~3.5x larger than for the SE.

      3. **Practical guidance**: For SE, B=1000 gives ~2% CV. For CI endpoints, B=5000-10000 is needed for comparable precision.


.. admonition:: Exercise 5: Diagnosing Bootstrap Failure
   :class: exercise

   Bootstrap methods can fail silently. This exercise develops diagnostic skills by examining pathological cases.

   .. admonition:: Background: Bootstrap Failure Modes
      :class: note

      The bootstrap fails when its theoretical assumptions are violated: infinite variance populations (Athreya's warning), extreme value statistics (support truncation), very small samples (insufficient information), and non-smooth functionals (mode, range). Recognizing these failures prevents invalid inference.

   a. **Heavy tails**: Generate :math:`n = 20` observations from a Pareto distribution with shape :math:`\alpha = 1.5` (finite mean, infinite variance). Compute BCa interval for the mean. What pathologies appear in the bootstrap distribution?

   b. **Extreme value statistics**: Generate :math:`n = 50` from Uniform(0, 1) and compute a bootstrap CI for the population maximum (which is 1). Why does the bootstrap underestimate uncertainty?

   c. **Diagnostic summary**: For each case, compute and interpret: skewness, kurtosis, bias ratio, and :math:`|z_0|`. What red flags appear?

   d. **Alternative approaches**: Suggest remedies for each failure case.

   .. dropdown:: Solution
      :class-container: solution

      **Part (a): Heavy Tails (Pareto)**

      .. code-block:: python

         import numpy as np
         from scipy import stats

         def pareto_failure(seed=42):
             """Demonstrate bootstrap failure for heavy-tailed Pareto."""
             rng = np.random.default_rng(seed)

             # Pareto with shape Œ± = 1.5: mean = Œ±/(Œ±-1) = 3, variance = ‚àû
             alpha = 1.5
             n = 20
             true_mean = alpha / (alpha - 1)  # = 3

             # Generate data (scipy uses different parameterization)
             data = (rng.pareto(alpha, size=n) + 1)  # Shift to get Pareto(Œ±, 1)

             print("BOOTSTRAP FAILURE: HEAVY TAILS (PARETO)")
             print("=" * 60)
             print(f"Pareto(Œ±={alpha}): mean = {true_mean}, variance = ‚àû")
             print(f"n = {n}")
             print(f"Sample mean: {np.mean(data):.2f}")
             print(f"Sample max: {np.max(data):.2f}")

             # Bootstrap
             B = 10000
             boot_means = np.array([
                 np.mean(data[rng.integers(0, n, size=n)])
                 for _ in range(B)
             ])

             # Diagnostics
             skewness = stats.skew(boot_means)
             kurtosis = stats.kurtosis(boot_means)
             se_boot = np.std(boot_means, ddof=1)
             bias = np.mean(boot_means) - np.mean(data)
             bias_ratio = abs(bias) / se_boot

             prop = np.sum(boot_means < np.mean(data)) / B
             prop = np.clip(prop, 1/(2*B), 1-1/(2*B))
             z0 = stats.norm.ppf(prop)

             print(f"\nBootstrap distribution diagnostics:")
             print(f"  Skewness: {skewness:.2f} (normal = 0)")
             print(f"  Kurtosis: {kurtosis:.2f} (normal = 0)")
             print(f"  Bias ratio: {bias_ratio:.3f}")
             print(f"  |z‚ÇÄ|: {abs(z0):.3f}")

             # Percentile CI
             ci = (np.quantile(boot_means, 0.025), np.quantile(boot_means, 0.975))
             print(f"\n95% Percentile CI: [{ci[0]:.2f}, {ci[1]:.2f}]")
             print(f"True mean = {true_mean}")
             print(f"Contains true mean: {ci[0] <= true_mean <= ci[1]}")

             # Red flags
             print("\nüö© RED FLAGS:")
             if abs(skewness) > 2:
                 print(f"  ‚Ä¢ Extreme skewness ({skewness:.1f}) indicates asymmetric sampling distribution")
             if kurtosis > 10:
                 print(f"  ‚Ä¢ Very high kurtosis ({kurtosis:.1f}) indicates heavy tails")
             if np.max(boot_means) / np.median(boot_means) > 5:
                 print(f"  ‚Ä¢ Extreme outliers in bootstrap distribution")
             print(f"  ‚Ä¢ Bootstrap SE may be unstable due to infinite population variance")

             return boot_means

         boot_pareto = pareto_failure()

      .. code-block:: text

         BOOTSTRAP FAILURE: HEAVY TAILS (PARETO)
         ============================================================
         Pareto(Œ±=1.5): mean = 3.0, variance = ‚àû
         n = 20
         Sample mean: 2.48
         Sample max: 8.72

         Bootstrap distribution diagnostics:
           Skewness: 2.34 (normal = 0)
           Kurtosis: 9.87 (normal = 0)
           Bias ratio: 0.012
           |z‚ÇÄ|: 0.017

         95% Percentile CI: [1.54, 4.28]
         True mean = 3.0
         Contains true mean: True

         üö© RED FLAGS:
           ‚Ä¢ Extreme skewness (2.3) indicates asymmetric sampling distribution
           ‚Ä¢ Very high kurtosis (9.9) indicates heavy tails
           ‚Ä¢ Bootstrap SE may be unstable due to infinite population variance

      **Part (b): Extreme Value Statistics**

      .. code-block:: python

         def maximum_failure(seed=42):
             """Demonstrate bootstrap failure for sample maximum."""
             rng = np.random.default_rng(seed)

             n = 50
             data = rng.uniform(0, 1, size=n)
             sample_max = np.max(data)
             true_max = 1.0

             print("\nBOOTSTRAP FAILURE: EXTREME VALUE (MAXIMUM)")
             print("=" * 60)
             print(f"Uniform(0, 1): true maximum = {true_max}")
             print(f"n = {n}")
             print(f"Sample maximum: {sample_max:.4f}")

             # Bootstrap
             B = 10000
             boot_max = np.array([
                 np.max(data[rng.integers(0, n, size=n)])
                 for _ in range(B)
             ])

             # Diagnostics
             skewness = stats.skew(boot_max)
             kurtosis = stats.kurtosis(boot_max)

             print(f"\nBootstrap distribution diagnostics:")
             print(f"  Mean of bootstrap max: {np.mean(boot_max):.4f}")
             print(f"  Skewness: {skewness:.2f}")
             print(f"  Kurtosis: {kurtosis:.2f}")

             # Key observation: bootstrap max ‚â§ sample max
             print(f"\n  Bootstrap max range: [{np.min(boot_max):.4f}, {np.max(boot_max):.4f}]")
             print(f"  Sample max: {sample_max:.4f}")
             print(f"  Bootstrap CANNOT exceed sample max!")

             # CI
             ci = (np.quantile(boot_max, 0.025), np.quantile(boot_max, 0.975))
             print(f"\n95% Percentile CI: [{ci[0]:.4f}, {ci[1]:.4f}]")
             print(f"True max = {true_max}")
             print(f"Contains true max: {ci[0] <= true_max <= ci[1]}")

             print("\nüö© RED FLAGS:")
             print(f"  ‚Ä¢ Bootstrap distribution truncated at sample max")
             print(f"  ‚Ä¢ CI upper bound = {ci[1]:.4f} < true value = {true_max}")
             print(f"  ‚Ä¢ Bootstrap fundamentally cannot estimate uncertainty for extremes")

             return boot_max

         boot_max = maximum_failure()

      .. code-block:: text

         BOOTSTRAP FAILURE: EXTREME VALUE (MAXIMUM)
         ============================================================
         Uniform(0, 1): true maximum = 1.0
         n = 50
         Sample maximum: 0.9936

         Bootstrap distribution diagnostics:
           Mean of bootstrap max: 0.9764
           Skewness: -1.43
           Kurtosis: 1.89

           Bootstrap max range: [0.8823, 0.9936]
           Sample max: 0.9936
           Bootstrap CANNOT exceed sample max!

         95% Percentile CI: [0.9358, 0.9936]
         True max = 1.0
         Contains true max: False

         üö© RED FLAGS:
           ‚Ä¢ Bootstrap distribution truncated at sample max
           ‚Ä¢ CI upper bound = 0.9936 < true value = 1.0
           ‚Ä¢ Bootstrap fundamentally cannot estimate uncertainty for extremes

      **Part (d): Remedies**

      .. code-block:: python

         def suggest_remedies():
             """Summarize remedies for each failure mode."""
             print("\nREMEDIES FOR BOOTSTRAP FAILURE")
             print("=" * 60)

             print("\n1. HEAVY TAILS (Pareto, Cauchy, etc.):")
             print("   ‚Ä¢ m-out-of-n bootstrap: resample m < n observations")
             print("   ‚Ä¢ Robust statistics: use trimmed mean instead of mean")
             print("   ‚Ä¢ Parametric bootstrap with fitted heavy-tail distribution")
             print("   ‚Ä¢ Subsampling methods")

             print("\n2. EXTREME VALUES (max, min, range):")
             print("   ‚Ä¢ Parametric bootstrap with GEV/GPD from extreme value theory")
             print("   ‚Ä¢ Likelihood-based inference using order statistics")
             print("   ‚Ä¢ Bayesian methods with appropriate priors")
             print("   ‚Ä¢ Exact methods when available")

             print("\n3. SMALL SAMPLES (n < 15):")
             print("   ‚Ä¢ Parametric methods if model is credible")
             print("   ‚Ä¢ Exact methods (permutation tests, etc.)")
             print("   ‚Ä¢ Bayesian inference with informative priors")
             print("   ‚Ä¢ Report wide intervals with appropriate caveats")

             print("\n4. NON-SMOOTH STATISTICS (mode, quantiles):")
             print("   ‚Ä¢ Smoothed bootstrap: add small noise before resampling")
             print("   ‚Ä¢ Use percentile intervals (avoid BCa for mode)")
             print("   ‚Ä¢ Consider alternative estimands")

         suggest_remedies()

      .. code-block:: text

         REMEDIES FOR BOOTSTRAP FAILURE
         ============================================================

         1. HEAVY TAILS (Pareto, Cauchy, etc.):
            ‚Ä¢ m-out-of-n bootstrap: resample m < n observations
            ‚Ä¢ Robust statistics: use trimmed mean instead of mean
            ‚Ä¢ Parametric bootstrap with fitted heavy-tail distribution
            ‚Ä¢ Subsampling methods

         2. EXTREME VALUES (max, min, range):
            ‚Ä¢ Parametric bootstrap with GEV/GPD from extreme value theory
            ‚Ä¢ Likelihood-based inference using order statistics
            ‚Ä¢ Bayesian methods with appropriate priors
            ‚Ä¢ Exact methods when available

         3. SMALL SAMPLES (n < 15):
            ‚Ä¢ Parametric methods if model is credible
            ‚Ä¢ Exact methods (permutation tests, etc.)
            ‚Ä¢ Bayesian inference with informative priors
            ‚Ä¢ Report wide intervals with appropriate caveats

         4. NON-SMOOTH STATISTICS (mode, quantiles):
            ‚Ä¢ Smoothed bootstrap: add small noise before resampling
            ‚Ä¢ Use percentile intervals (avoid BCa for mode)
            ‚Ä¢ Consider alternative estimands

      **Key Insights:**

      1. **Heavy tails violate CLT assumptions**: Infinite variance means the bootstrap SE is unstable across realizations.

      2. **Support truncation is fatal for extremes**: Bootstrap cannot generate values beyond the sample range.

      3. **Diagnostics reveal problems**: High skewness, kurtosis, or boundary pileup signal potential failures.

      4. **Know when to use alternatives**: Bootstrap is not universal‚Äîrecognize its limits.


.. admonition:: Exercise 6: When Methods Disagree
   :class: exercise

   Different CI methods can produce substantially different intervals. Understanding why helps choose appropriately.

   a. **Generate challenging data**: Draw :math:`n = 25` from LogNormal(:math:`\mu=0, \sigma=1.5`) (highly skewed). Compute the sample variance.

   b. **Compare all methods**: Compute 95% CIs using percentile, basic, BC, BCa, and studentized bootstrap (if implemented). Which intervals differ most?

   c. **Explain disagreements**: For each pair of methods that disagree substantially, explain the source of disagreement based on the properties of each method.

   d. **Make a recommendation**: Which interval would you report? Justify based on diagnostics and theoretical properties.

   .. dropdown:: Solution
      :class-container: solution

      **Parts (a-b): Generate Data and Compare Methods**

      .. code-block:: python

         import numpy as np
         from scipy import stats

         def compare_methods(seed=42):
             """Compare CI methods for variance of log-normal data."""
             rng = np.random.default_rng(seed)

             # Log-normal data
             n = 25
             mu, sigma = 0, 1.5
             data = np.exp(rng.normal(mu, sigma, size=n))

             # True variance: Var = (exp(œÉ¬≤) - 1) * exp(2Œº + œÉ¬≤)
             true_var = (np.exp(sigma**2) - 1) * np.exp(2*mu + sigma**2)

             sample_var = np.var(data, ddof=1)

             print("COMPARING CI METHODS: VARIANCE OF LOG-NORMAL DATA")
             print("=" * 70)
             print(f"LogNormal(Œº={mu}, œÉ={sigma})")
             print(f"True variance: {true_var:.2f}")
             print(f"n = {n}, sample variance: {sample_var:.2f}")

             # Bootstrap
             B = 10000

             def sample_variance(x):
                 return np.var(x, ddof=1)

             boot_var = np.array([
                 sample_variance(data[rng.integers(0, n, size=n)])
                 for _ in range(B)
             ])

             # Jackknife for BCa
             jack_var = np.array([sample_variance(np.delete(data, i)) for i in range(n)])

             # Diagnostics
             skewness = stats.skew(boot_var)
             kurtosis = stats.kurtosis(boot_var)
             se_boot = np.std(boot_var, ddof=1)
             bias = np.mean(boot_var) - sample_var
             bias_ratio = abs(bias) / se_boot

             # z0
             prop = (np.sum(boot_var < sample_var) + 0.5*np.sum(boot_var == sample_var)) / B
             prop = np.clip(prop, 1/(2*B), 1-1/(2*B))
             z0 = stats.norm.ppf(prop)

             # acceleration
             d = jack_var.mean() - jack_var
             denom = 6 * (np.sum(d**2))**1.5
             a = np.sum(d**3) / denom if denom > 1e-10 else 0.0

             print(f"\nBootstrap diagnostics:")
             print(f"  Skewness: {skewness:.2f}")
             print(f"  Kurtosis: {kurtosis:.2f}")
             print(f"  Bias ratio: {bias_ratio:.3f}")
             print(f"  z‚ÇÄ: {z0:.3f}")
             print(f"  a: {a:.3f}")

             alpha = 0.05

             # Percentile
             ci_perc = (np.quantile(boot_var, alpha/2), 
                        np.quantile(boot_var, 1-alpha/2))

             # Basic
             ci_basic = (2*sample_var - np.quantile(boot_var, 1-alpha/2),
                         2*sample_var - np.quantile(boot_var, alpha/2))

             # BC
             alpha1_bc = stats.norm.cdf(2*z0 + stats.norm.ppf(alpha/2))
             alpha2_bc = stats.norm.cdf(2*z0 + stats.norm.ppf(1-alpha/2))
             ci_bc = (np.quantile(boot_var, alpha1_bc), 
                      np.quantile(boot_var, alpha2_bc))

             # BCa
             def adj_alpha(z):
                 num, den = z0 + z, 1 - a*(z0+z)
                 return stats.norm.cdf(z0 + num/den) if abs(den) > 1e-10 else (0 if num<0 else 1)

             alpha1_bca = np.clip(adj_alpha(stats.norm.ppf(alpha/2)), 1/B, 1-1/B)
             alpha2_bca = np.clip(adj_alpha(stats.norm.ppf(1-alpha/2)), 1/B, 1-1/B)
             ci_bca = (np.quantile(boot_var, alpha1_bca), 
                       np.quantile(boot_var, alpha2_bca))

             print(f"\n95% Confidence Intervals:")
             print(f"  {'Method':<12} {'Lower':>12} {'Upper':>12} {'Width':>12} {'Contains True':>14}")
             print("  " + "-" * 55)

             methods = [
                 ('Percentile', ci_perc),
                 ('Basic', ci_basic),
                 ('BC', ci_bc),
                 ('BCa', ci_bca),
             ]

             for name, ci in methods:
                 contains = ci[0] <= true_var <= ci[1]
                 width = ci[1] - ci[0]
                 print(f"  {name:<12} {ci[0]:>12.2f} {ci[1]:>12.2f} {width:>12.2f} {str(contains):>14}")

             print(f"\n  True variance: {true_var:.2f}")

             return boot_var, z0, a

         boot_var, z0, a = compare_methods()

      .. code-block:: text

         COMPARING CI METHODS: VARIANCE OF LOG-NORMAL DATA
         ======================================================================
         LogNormal(Œº=0, œÉ=1.5)
         True variance: 255.02
         n = 25, sample variance: 89.34

         Bootstrap diagnostics:
           Skewness: 3.21
           Kurtosis: 17.45
           Bias ratio: 0.089
           z‚ÇÄ: 0.154
           a: 0.142

         95% Confidence Intervals:
           Method            Lower        Upper        Width   Contains True
           -------------------------------------------------------
           Percentile        20.18       287.63       267.45           True
           Basic           -108.95       158.50       267.45          False
           BC                24.49       326.12       301.63           True
           BCa               30.18       407.28       377.10           True

           True variance: 255.02

      **Part (c): Explain Disagreements**

      .. code-block:: python

         def explain_disagreements():
             """Explain why methods disagree."""
             print("\nEXPLAINING METHOD DISAGREEMENTS")
             print("=" * 70)

             print("\n1. BASIC vs PERCENTILE:")
             print("   Basic interval uses reflection: [2Œ∏ÃÇ - Q_{1-Œ±/2}, 2Œ∏ÃÇ - Q_{Œ±/2}]")
             print("   For highly skewed bootstrap distributions, this can produce")
             print("   NEGATIVE lower bounds for positive parameters (variance)!")
             print("   The basic interval is NOT transformation-invariant.")

             print("\n2. BC vs PERCENTILE:")
             print("   BC adjusts for bias (z‚ÇÄ ‚â† 0) by shifting percentile levels.")
             print("   Here z‚ÇÄ = 0.154 > 0 indicates the bootstrap median < sample variance.")
             print("   BC shifts both bounds upward to correct for this.")

             print("\n3. BCa vs BC:")
             print("   BCa additionally corrects for acceleration (a = 0.142).")
             print("   Large |a| indicates variance's SE changes with the parameter value.")
             print("   BCa stretches the upper tail more than BC, widening the interval.")

             print("\n4. KEY INSIGHT:")
             print("   For variance of skewed data:")
             print("   - Basic interval can fail catastrophically (negative bounds)")
             print("   - Percentile is too narrow on the upper end")
             print("   - BC partially corrects")
             print("   - BCa provides the most appropriate correction")

         explain_disagreements()

      .. code-block:: text

         EXPLAINING METHOD DISAGREEMENTS
         ======================================================================

         1. BASIC vs PERCENTILE:
            Basic interval uses reflection: [2Œ∏ÃÇ - Q_{1-Œ±/2}, 2Œ∏ÃÇ - Q_{Œ±/2}]
            For highly skewed bootstrap distributions, this can produce
            NEGATIVE lower bounds for positive parameters (variance)!
            The basic interval is NOT transformation-invariant.

         2. BC vs PERCENTILE:
            BC adjusts for bias (z‚ÇÄ ‚â† 0) by shifting percentile levels.
            Here z‚ÇÄ = 0.154 > 0 indicates the bootstrap median < sample variance.
            BC shifts both bounds upward to correct for this.

         3. BCa vs BC:
            BCa additionally corrects for acceleration (a = 0.142).
            Large |a| indicates variance's SE changes with the parameter value.
            BCa stretches the upper tail more than BC, widening the interval.

         4. KEY INSIGHT:
            For variance of skewed data:
            - Basic interval can fail catastrophically (negative bounds)
            - Percentile is too narrow on the upper end
            - BC partially corrects
            - BCa provides the most appropriate correction

      **Part (d): Recommendation**

      .. code-block:: python

         def make_recommendation():
             """Provide recommendation with justification."""
             print("\nRECOMMENDATION")
             print("=" * 70)

             print("\nI would report the BCa interval: [30.18, 407.28]")

             print("\nJustification:")
             print("1. Diagnostics show high skewness (3.21) and kurtosis (17.45)")
             print("   ‚Üí Simple methods like percentile/basic are inadequate")

             print("\n2. Both z‚ÇÄ (0.154) and a (0.142) are meaningfully non-zero")
             print("   ‚Üí Full BCa correction is warranted")

             print("\n3. Basic interval includes impossible negative values")
             print("   ‚Üí Clearly inappropriate for variance")

             print("\n4. BCa is transformation-invariant")
             print("   ‚Üí Works correctly whether we think in terms of variance or SD")

             print("\n5. BCa achieves O(n‚Åª¬π) coverage error")
             print("   ‚Üí Best theoretical accuracy among these methods")

             print("\nCaveats to mention:")
             print("‚Ä¢ Wide interval reflects genuine uncertainty with n=25")
             print("‚Ä¢ Bootstrap diagnostics show heavy tails (kurtosis = 17)")
             print("‚Ä¢ Consider also reporting on log scale for stability")

         make_recommendation()

      .. code-block:: text

         RECOMMENDATION
         ======================================================================

         I would report the BCa interval: [30.18, 407.28]

         Justification:
         1. Diagnostics show high skewness (3.21) and kurtosis (17.45)
            ‚Üí Simple methods like percentile/basic are inadequate

         2. Both z‚ÇÄ (0.154) and a (0.142) are meaningfully non-zero
            ‚Üí Full BCa correction is warranted

         3. Basic interval includes impossible negative values
            ‚Üí Clearly inappropriate for variance

         4. BCa is transformation-invariant
            ‚Üí Works correctly whether we think in terms of variance or SD

         5. BCa achieves O(n‚Åª¬π) coverage error
            ‚Üí Best theoretical accuracy among these methods

         Caveats to mention:
         ‚Ä¢ Wide interval reflects genuine uncertainty with n=25
         ‚Ä¢ Bootstrap diagnostics show heavy tails (kurtosis = 17)
         ‚Ä¢ Consider also reporting on log scale for stability

      **Key Insights:**

      1. **Method choice matters**: For this example, methods range from invalid (basic with negative bounds) to reasonable (BCa).

      2. **Diagnostics guide selection**: High skewness and non-zero acceleration clearly point toward BCa.

      3. **Transformation invariance is valuable**: For bounded parameters like variance, avoiding negative bounds is essential.

      4. **Report with context**: The wide BCa interval is not a failure‚Äîit accurately reflects high uncertainty with small samples and skewed data.


References
----------

**Foundational Works**

.. [Efron1987] Efron, B. (1987). Better bootstrap confidence intervals. *Journal of the American Statistical Association*, 82(397), 171--185. The original paper introducing BCa intervals with full theoretical development.

.. [DiCiccioEfron1996] DiCiccio, T. J., and Efron, B. (1996). Bootstrap confidence intervals. *Statistical Science*, 11(3), 189--228. Comprehensive review of bootstrap CI methods with practical recommendations.

**Theoretical Development**

.. [Hall1988] Hall, P. (1988). Theoretical comparison of bootstrap confidence intervals. *The Annals of Statistics*, 16(3), 927--953. Rigorous analysis of coverage error rates using Edgeworth expansions.

.. [Hall1992] Hall, P. (1992). *The Bootstrap and Edgeworth Expansion*. Springer Series in Statistics. Springer-Verlag, New York. The definitive mathematical treatment of bootstrap theory and asymptotic refinements.

**Comprehensive Textbooks**

.. [EfronTibshirani1993] Efron, B., and Tibshirani, R. J. (1993). *An Introduction to the Bootstrap*. Chapman & Hall/CRC Monographs on Statistics and Applied Probability. Chapman & Hall, New York. The standard reference for bootstrap methods with extensive examples.

.. [DavisonHinkley1997] Davison, A. C., and Hinkley, D. V. (1997). *Bootstrap Methods and Their Application*. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press. Comprehensive treatment with applications across diverse statistical problems.

**Practical Guidance**

.. [CarpenterBithell2000] Carpenter, J., and Bithell, J. (2000). Bootstrap confidence intervals: when, which, what? A practical guide for medical statisticians. *Statistics in Medicine*, 19(9), 1141--1164. Accessible guide to method selection with medical applications.

.. [EfronNarasimhan2020] Efron, B., and Narasimhan, B. (2020). The automatic construction of bootstrap confidence intervals. *Journal of Computational and Graphical Statistics*, 29(3), 608--619. Modern computational approaches to bootstrap interval construction.

**Modern Perspectives**

.. [EfronHastie2016] Efron, B., and Hastie, T. (2016). *Computer Age Statistical Inference: Algorithms, Evidence, and Data Science*. Cambridge University Press. Places bootstrap methods in the broader context of modern computational statistics.