.. _part1-foundations:

===================================================
Part I: Foundations of Probability and Computation
===================================================

*What is probability?* The number 0.7 might represent "seven out of ten coin flips landed heads," or "I'm 70% confident it will rain tomorrow," or "this radioactive atom has a 70% chance of decaying within the hour." These statements feel different—one describes observed frequency, another expresses belief, a third characterizes physical propensity—yet all use the same mathematical machinery. Part I establishes this machinery and confronts the interpretive questions that shape everything that follows.

The remarkable fact about probability theory is that everyone agrees on the rules while disagreeing profoundly about what the rules mean. Kolmogorov's axioms—non-negativity, normalization, and countable additivity—provide the mathematical foundation that all practitioners accept. But frequentists see probabilities as long-run frequencies revealed through repetition, while Bayesians see them as degrees of rational belief updated through evidence. These aren't merely philosophical positions; they lead to different methods, different interpretations of results, and ultimately different answers to the question "What should we conclude from these data?"

Part I doesn't resolve this debate—it can't be resolved, because the interpretations answer different questions. Instead, we develop fluency in both perspectives, understanding when each provides the more natural framework for a given problem. The computational data scientist needs both tools in their kit.

.. toctree::
   :maxdepth: 2
   :caption: Chapter

   chapter1/index

|

**The Arc of Part I**

**Chapter 1** weaves together three essential threads: philosophical foundations, mathematical machinery, and computational tools.

We begin with **Kolmogorov's axioms**—the mathematical bedrock accepted by all camps. These three simple rules (probabilities are non-negative, sum to one over the sample space, and add for disjoint events) generate the entire edifice of probability theory. From them we derive everything: conditional probability and Bayes' theorem, independence and exchangeability, expectation and variance, the law of large numbers and the central limit theorem. The axioms are interpretation-neutral—they specify how probabilities must behave without dictating what they represent.

The **interpretive question** then takes center stage. The frequentist views probability as the limiting relative frequency in an infinite sequence of identical trials—grounding probability in observable, repeatable phenomena but struggling with one-time events. The Bayesian views probability as a coherent degree of belief, quantifying uncertainty about anything through the calculus of conditional probability—handling unique events naturally but requiring specification of prior beliefs. We explore these perspectives not to declare a winner but to understand what each offers and when each applies.

From philosophy we turn to **random variables and their distributions**. A random variable is a function from outcomes to numbers; a distribution describes how probability mass spreads across those numbers. We develop probability mass functions for discrete variables, probability density functions for continuous ones, cumulative distribution functions that unify both, and quantile functions that invert them.

The chapter catalogues the **probability distributions** that appear throughout data science. Discrete distributions—Bernoulli, Binomial, Poisson, Geometric, Negative Binomial—model counts, trials, and events. Continuous distributions—Uniform, Normal, Exponential, Gamma, Beta—model measurements, durations, and proportions. Inference distributions—Student's t, Chi-square, F—arise when estimating parameters from normally distributed data. For each, we develop not just formulas but understanding: why does this distribution arise? What phenomena does it model? How does it relate to other distributions?

Finally, we bridge theory and practice through **Python's computational ecosystem**. The standard library's ``random`` module provides quick scalar sampling. NumPy's ``Generator`` API delivers high-performance vectorized computation. SciPy's ``stats`` module offers the complete statistical toolkit. We learn not just which functions to call but how to think computationally—managing seeds for reproducibility, spawning independent streams for parallel computation, choosing between exact calculation and Monte Carlo approximation.

**Why Foundations Matter**

It's tempting to rush past foundations toward simulation, inference, and machine learning. But foundations aren't preliminary throat-clearing; they're the soil from which everything else grows.

*Interpretation shapes method.* The frequentist designs methods with good long-run properties—unbiased estimators, confidence intervals with guaranteed coverage, tests that control error rates. The Bayesian computes posterior distributions that directly express uncertainty about parameters. The same data, analyzed under different paradigms, can yield different answers because the questions differ. Understanding both frameworks lets you choose the right tool for each problem.

*Distribution knowledge enables simulation.* When we generate random variables in Part II, we'll exploit deep properties: the probability integral transform, the reproductive property of the Gamma, the relationship between Normal and Chi-square. The richer your distribution vocabulary, the more simulation strategies become available.

*Mathematical precision prevents errors.* Sloppy probability reasoning leads to famous fallacies: the prosecutor's fallacy, base rate neglect, the gambler's fallacy. Part I builds the habits of precision that prevent these errors.

**Connections**

*Part II: Frequentist Inference* operationalizes the frequentist thought experiment computationally. Monte Carlo simulation performs the infinite repetitions that define frequentist quantities. Bootstrap resampling estimates sampling distributions without analytical derivation. Maximum likelihood finds parameters that make observed data most probable. All rest on the distribution theory and computational tools established here.

*Part III: Bayesian Inference* develops the Bayesian paradigm computationally. Bayes' theorem becomes the engine of inference—prior times likelihood yields posterior. The distributions catalogued here reappear as priors and posteriors. MCMC generates samples from posteriors too complex for analytical treatment.

*Part IV: LLMs in Data Science* applies computational thinking to a new domain. The mindset—using computation to solve intractable problems—transfers directly from probability foundations to modern AI integration.

**Prerequisites**

Part I assumes comfort with calculus (derivatives, integrals, series), linear algebra (vectors, matrices), and Python programming (functions, arrays, basic NumPy). Prior exposure to probability and statistics helps but isn't strictly required—we develop the necessary material from first principles, though at a pace that assumes mathematical maturity.

By Part I's end, you'll have the conceptual vocabulary, mathematical tools, and computational skills to engage with the serious methods that follow.