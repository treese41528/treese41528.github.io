.. _index:

==========================================
Computational Methods in Data Science
==========================================


.. image:: https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/STAT418_brightspace_banner_2400x960.png
   :alt: STAT 418: Computational Methods in Data Science course banner
   :align: center
   :width: 100%

*STAT 418 ¬∑ Spring 2026 ¬∑ Purdue University*

.. toctree::
   :maxdepth: 2
   :caption: Course Content

   part1_foundations/index
   part2_frequentist/index
   part3_bayesian/index
   part4_llms_datascience/index

.. toctree::
   :maxdepth: 1
   :caption: Assignments

   homework/index
   capstone/index


----

Modern statistics is computational statistics. The elegant formulas of classical theory‚Äîderived under assumptions of normality, independence, and large samples‚Äîoften fail when confronted with real data: messy, high-dimensional, and decidedly non-normal. Yet the core questions remain: How certain should we be? What can we conclude? How might we be wrong?

This course develops the computational methods that answer these questions without relying on fragile assumptions. We replace analytical derivations with simulation, asymptotic approximations with resampling, and conjugate convenience with general-purpose algorithms. The computer becomes not just a calculator but a laboratory for statistical thought experiments.

The intellectual journey moves through four parts. We begin with **foundations**: what probability means, how distributions behave, and how Python's scientific stack turns theory into computation. We then develop **frequentist inference** computationally: Monte Carlo simulation as the engine, maximum likelihood as the estimator, and bootstrap resampling as the uncertainty quantifier. Next, we explore **Bayesian inference**: prior beliefs, posterior updating, and Markov chain Monte Carlo for models too complex for analytical treatment. Finally, we address **large language models in data science**: integrating pre-trained models into analytical workflows for text preprocessing, feature extraction, data annotation, and retrieval-augmented generation‚Äîwith careful attention to responsible use, reliability, and privacy.

Throughout, we emphasize both rigor and practice. Every method receives mathematical justification‚Äîyou'll understand *why* these techniques work, not just *how* to call the functions. But every derivation leads to working code. By course end, you'll have built a complete toolkit for modern statistical computation, from foundational simulation through cutting-edge AI integration.

----

Course Information
------------------

.. list-table::
   :widths: 20 35 20 25
   :header-rows: 0

   * - **Instructor**
     - Dr. Timothy Reese
     - **Email**
     - reese18@purdue.edu
   * - **Office**
     - MATH 210
     - **Phone**
     - 765-494-4129
   * - **Lectures**
     - Tue/Thu 1:30‚Äì2:45 PM
     - **Room**
     - UNIV 127
   * - **Office Hours**
     - Wed/Fri 1:00‚Äì3:00 PM
     - **Location**
     - MATH 210
   * - **Credits**
     - 3.00
     - **Website**
     - `Course Site <https://treese41528.github.io/ComputationalDataScience/Website/index.html>`_

Prerequisites
~~~~~~~~~~~~~

Enrollment requires completion of the following with a grade of C- or better:

**Probability** (one of):
  - STAT 41600: Probability (undergraduate)
  - STAT 51600: Basic Probability and Applications (graduate)

**Statistical Inference** (one of):
  - STAT 35000: Introduction to Statistics
  - STAT 35500: Statistics for Data Science
  - STAT 51100: Statistical Methods (graduate)

**Programming** (one of):
  - MA 16290: Integrated Calculus and Linear Algebra II
  - CS 38003: Python Programming


You should be comfortable with: probability axioms and random variables; discrete and continuous distributions (PMFs, PDFs, CDFs); expectation, variance, and covariance; joint, marginal, and conditional distributions; functions and transformations of random variables; moment generating functions (MGFs); the Law of Large Numbers and Central Limit Theorem; Bayes' theorem; hypothesis testing and confidence intervals; Python programming with NumPy arrays; and calculus through multiple integrals.

----

Course Structure
----------------

The course divides into four parts, each building on the previous:

**Part I: Foundations of Probability and Computation**

What does probability mean? How do we describe and compute with random variables? Part I establishes the mathematical and philosophical groundwork: Kolmogorov's axioms, frequentist and Bayesian interpretations, probability distributions and their properties, and Python's ecosystem for statistical computing. These foundations support everything that follows.

**Part II: Frequentist Inference**

The frequentist asks: "What would happen if I repeated this procedure many times?" Part II develops this perspective computationally. Monte Carlo simulation provides the engine for approximating expectations and probabilities. Maximum likelihood estimation and generalized linear models provide the parametric toolkit. Bootstrap resampling, the jackknife, and permutation tests provide distribution-free inference when parametric assumptions fail.

**Part III: Bayesian Inference**

The Bayesian asks: "What should I believe given this evidence?" Part III develops posterior inference from prior specification through MCMC computation. We construct models, check their fit, compare alternatives, and extract predictions‚Äîall while properly quantifying uncertainty through posterior distributions.

**Part IV: Large Language Models in Data Science**

How can we leverage pre-trained language models to enhance data science workflows? Part IV addresses the practical and responsible integration of LLMs into analytical pipelines. We cover text preprocessing and feature extraction using embeddings, leveraging pre-trained models for data annotation and augmentation, retrieval-augmented generation (RAG) for domain-specific applications, and responsible AI practices including prompt engineering, reliability assessment, and privacy considerations.

The course culminates in a **capstone project** where you synthesize these methods to address a substantial data science problem, demonstrating both theoretical understanding and practical implementation skill.

----

Learning Outcomes
-----------------

Upon completing this course, you will be able to:

1. **Apply simulation techniques** including Monte Carlo methods, transformation approaches, and rejection sampling to analyze probabilistic behavior in data science applications

2. **Compare and evaluate frequentist and Bayesian inference** paradigms by examining their theoretical foundations, identifying their strengths and limitations, and explaining their roles in statistical modeling and decision-making

3. **Design, implement, and assess resampling methods** focusing on both nonparametric and parametric forms of the bootstrap to estimate variability, construct confidence intervals, and improve statistical estimates through bias correction techniques

4. **Apply cross-validation principles** to compute model performance metrics, detect overfitting and underfitting, and select models with reliable predictive accuracy using Python libraries

5. **Construct and interpret Bayesian models** including posterior distributions and credible intervals, apply Markov chain Monte Carlo methods to approximate posteriors, and evaluate the role of prior distributions in Bayesian inference

6. **Utilize large language models** in data science workflows for contextual data augmentation, feature engineering, and integrating structured and unstructured data to enhance predictive models, while addressing challenges of privacy and reliability

7. **Synthesize course methods** in a capstone project to design, develop, and present robust solutions to real-world data science challenges, demonstrating both theoretical understanding and applied expertise

----

Assessment
----------

.. list-table::
   :widths: 25 15 60
   :header-rows: 1

   * - Component
     - Weight
     - Details
   * - **Homework**
     - 40%
     - 6‚Äì7 assignments on ~2-week cadence; lowest score dropped; late submissions accepted up to 3 days with 20% penalty
   * - **Midterm Exams**
     - 30%
     - Two exams (15% each): Midterm I covers Chapters 1‚Äì3 (Foundations, Monte Carlo, Frequentist Inference); Midterm II covers Chapters 4‚Äì5 (Resampling Methods, Bayesian Inference)
   * - **Capstone Project**
     - 30%
     - Proposal (2%), progress report (1%),  presentation (7%), final submission (20%); demonstrates synthesis of course methods on substantive problem

**Academic Integrity**: All work governed by Purdue's Honor Pledge. Collaboration encouraged on concepts; submitted work must be your own. AI tools (ChatGPT, Copilot, Claude) permitted for debugging, studying, and exploring ideas; prohibited for generating turnkey solutions. Disclose AI assistance; verify all AI-generated content for accuracy.

----

Schedule & Syllabus
-------------------

üìÖ `Course Schedule <https://treese41528.github.io/ComputationalDataScience/Website/STAT_418_Schedule.html>`_
   Interactive weekly schedule with topics, assignments, and exam dates.

üìã `Course Syllabus (PDF) <https://treese41528.github.io/ComputationalDataScience/Website/Computational%20Methods%20in%20Data%20Science%20Syllabus.pdf>`_
   Complete syllabus with policies, grading breakdown, and academic integrity guidelines.

----


Lecture Slides
--------------
Interactive slide presentations for each chapter. These slides contain key concepts, visualizations, and speaker notes for in-class lectures.

*Additional slide decks will be released as the course progresses.*

.. raw:: html

   <table class="docutils align-default" style="width:100%">
   <thead>
   <tr><th>Chapter</th><th>View Slides</th></tr>
   </thead>
   <tbody>
   <tr>
     <td><strong>Chapter 1</strong>: Probability Foundations &amp; Inference Paradigms</td>
     <td><a href="https://treese41528.github.io/ComputationalDataScience/Slides/stat418-ch1-slides.html">üé¨ View Slides</a></td>
   </tr>
   <tr>
     <td><strong>Chapter 2</strong>: Monte Carlo Simulation</td>
     <td><a href="https://treese41528.github.io/ComputationalDataScience/Slides/stat418-ch2-slides.html">üé¨ View Slides</a></td>
   </tr>
   <tr>
     <td><strong>Chapter 3</strong>: Parametric Inference and Likelihood Methods</td>
     <td><a href="https://treese41528.github.io/ComputationalDataScience/Slides/stat418-ch3-slides.html">üé¨ View Slides</a></td>
   </tr>
   </tbody>
   </table>


Additional Notes
----------------
Supplementary materials providing additional coverage of specific topics.

*Additional notes will be added throughout the semester.*

.. raw:: html

   <table class="docutils align-default" style="width:100%">
   <thead>
   <tr><th>Topic</th><th>View Notes</th></tr>
   </thead>
   <tbody>
   <tr>
     <td><strong>Bernoulli Distribution Relationships</strong>: Connections between Bernoulli, Binomial, Geometric, and Negative Binomial distributions</td>
     <td><a href="https://treese41528.github.io/ComputationalDataScience/Notes/bernoulli-distribution-relationships.html">üìÑ View Notes</a></td>
   </tr>
   </tbody>
   </table>


Companion Notebooks
-------------------

These Jupyter notebooks accompany the course lectures. Each chapter notebook contains worked examples, visualizations, and code you can run and modify. View the rendered HTML online or download the ``.ipynb`` files to run locally (right-click and "Save Link As" if needed).

*Additional notebooks will be released as the course progresses.*

.. raw:: html

   <table class="docutils align-default" style="width:100%">
   <thead>
   <tr><th>Chapter</th><th>View Online</th><th>Download</th></tr>
   </thead>
   <tbody>
   <tr>
     <td><strong>Chapter 1</strong>: Probability Foundations &amp; Python Review</td>
     <td><a href="https://treese41528.github.io/ComputationalDataScience/Website/Notebooks/PartI/chapter1_review.html">üîó View HTML</a></td>
     <td><a href="https://treese41528.github.io/ComputationalDataScience/Website/Notebooks/PartI/chapter1_review.ipynb" download="chapter1_review.ipynb">‚¨á Download .ipynb</a></td>
   </tr>
   <tr>
     <td><strong>Chapter 2</strong>: Monte Carlo Methods</td>
     <td><a href="https://treese41528.github.io/ComputationalDataScience/Website/Notebooks/PartII/Chapter2/chapter2_monte_carlo_methods.html">üîó View HTML</a></td>
     <td><a href="https://treese41528.github.io/ComputationalDataScience/Website/Notebooks/PartII/Chapter2/chapter2_monte_carlo_methods.ipynb" download="chapter2_monte_carlo_methods.ipynb">‚¨á Download .ipynb</a></td>
   </tr>
   <tr>
     <td><strong>Chapter 3</strong>: Parametric Inference</td>
     <td><a href="https://treese41528.github.io/ComputationalDataScience/Website/Notebooks/PartII/Chapter3/chapter3_parametric_inference.html">üîó View HTML</a></td>
     <td><a href="https://treese41528.github.io/ComputationalDataScience/Website/Notebooks/PartII/Chapter3/chapter3_parametric_inference.ipynb" download="chapter3_parametric_inference.ipynb">‚¨á Download .ipynb</a></td>
   </tr>
   </tbody>
   </table>

----


Computing Resources
-------------------
This course has access to **Purdue's Scholar Cluster**, a high-performance computing resource designed for classroom learning. Scholar provides the computational power needed for Monte Carlo simulations, MCMC sampling, bootstrap resampling, and other computationally intensive methods covered in this course.

What You Get
~~~~~~~~~~~~
As a registered student in STAT 418, you automatically receive:

- **25 GB Home Directory** for your course files and projects
- **Scratch Storage** for temporary computational work
- **Jupyter Notebook Server** ‚Äî run Python notebooks directly in your browser
- **Batch Job Submission** ‚Äî run longer simulations via the Slurm scheduler

Access Methods
~~~~~~~~~~~~~~
You can access Scholar through several interfaces, depending on your needs and comfort level:

.. raw:: html

   <table class="docutils align-default" style="width:100%">
   <thead>
   <tr><th>Method</th><th>Best For</th><th>Access</th></tr>
   </thead>
   <tbody>
   <tr>
     <td><strong>Gateway (Open OnDemand)</strong></td>
     <td>Jupyter notebooks, file management, job monitoring</td>
     <td><a href="https://gateway.scholar.rcac.purdue.edu/">üöÄ Launch Gateway</a></td>
   </tr>
   <tr>
     <td><strong>Remote Desktop (ThinLinc)</strong></td>
     <td>Full graphical Linux desktop experience</td>
     <td><a href="https://desktop.scholar.rcac.purdue.edu/">üñ•Ô∏è Launch Desktop</a></td>
   </tr>
   <tr>
     <td><strong>SSH (Command Line)</strong></td>
     <td>Direct terminal access, batch job submission</td>
     <td><code>ssh yourname@scholar.rcac.purdue.edu</code></td>
   </tr>
   </tbody>
   </table>

**Recommended for this course:** The **Gateway** interface provides the easiest access to Jupyter notebooks, which is how most course assignments are structured.

Getting Started
~~~~~~~~~~~~~~~
1. **Login:** Use your Purdue Career Account credentials (same as BoilerKey)
2. **Launch Jupyter:** From the Gateway, select "Jupyter Notebook" under Interactive Apps
3. **Select Resources:** For most assignments, 1-2 cores and 4 GB memory is sufficient
4. **Upload Notebooks:** Transfer course notebooks to your home directory

.. admonition:: First Time Setup
   :class: tip

   The first time you log in, you may need to wait a few minutes for your account to be fully provisioned. If you encounter issues, try again after 15 minutes or contact RCAC support.

When to Use Scholar
~~~~~~~~~~~~~~~~~~~
Scholar is particularly useful when your local machine is insufficient:

- **Large Monte Carlo studies** ‚Äî running millions of simulations
- **MCMC sampling** ‚Äî chains with many iterations or multiple chains in parallel
- **Bootstrap resampling** ‚Äî thousands of resamples on large datasets
- **Cross-validation** ‚Äî parallelizing fold computations
- **Capstone projects** ‚Äî scaling up analyses for real-world datasets

Reserve Scholar for computationally demanding tasks.

Documentation & Help
~~~~~~~~~~~~~~~~~~~~

.. raw:: html

   <table class="docutils align-default" style="width:100%">
   <thead>
   <tr><th>Resource</th><th>Link</th></tr>
   </thead>
   <tbody>
   <tr>
     <td>Scholar User Guide</td>
     <td><a href="https://www.rcac.purdue.edu/knowledge/scholar">üìñ User Guide</a></td>
   </tr>
   <tr>
     <td>Running Jobs (Slurm)</td>
     <td><a href="https://www.rcac.purdue.edu/knowledge/scholar/run">üìã Job Submission Guide</a></td>
   </tr>
   <tr>
     <td>Jupyter on Scholar</td>
     <td><a href="https://www.rcac.purdue.edu/knowledge/scholar/jupyter">üêç Jupyter Guide</a></td>
   </tr>
   <tr>
     <td>File Storage & Transfer</td>
     <td><a href="https://www.rcac.purdue.edu/knowledge/scholar/storage">üíæ Storage Guide</a></td>
   </tr>
   <tr>
     <td>Frequently Asked Questions</td>
     <td><a href="https://www.rcac.purdue.edu/knowledge/scholar/faq">‚ùì FAQ</a></td>
   </tr>
   <tr>
     <td>RCAC Help Desk</td>
     <td><a href="mailto:rcac-help@purdue.edu">üìß rcac-help@purdue.edu</a></td>
   </tr>
   </tbody>
   </table>

.. admonition:: Troubleshooting
   :class: warning

   If you experience issues accessing Scholar or running jobs, first check the `RCAC Status Page <https://www.rcac.purdue.edu/news/outages-and-maintenance>`_ for scheduled maintenance or outages. For persistent problems, email rcac-help@purdue.edu with your username, the time of the issue, and any error messages.



Python Tutorials for Further Study
----------------------------------

The resources below are curated for students who want to deepen their Python skills beyond the course material. All are free, open-source, and maintained by recognized experts or framework developers.

**Course Environment Setup**

.. raw:: html

   <p><a href="https://treese41528.github.io/ComputationalDataScience/Website/requirements.txt" download="requirements.txt">‚¨á Download requirements.txt</a> ‚Äî Python package dependencies for the course. Install with <code>pip install -r requirements.txt</code> in a virtual environment.</p>

**Data Science Foundations**

- `Python Data Science Handbook <https://jakevdp.github.io/PythonDataScienceHandbook/>`_ by Jake VanderPlas
    Complete free book covering NumPy, Pandas, Matplotlib, and Scikit-learn. All content available as executable Jupyter notebooks. Essential reference for the scientific Python stack.

- `Scientific Python Lectures <https://lectures.scientific-python.org/>`_
    From core SciPy contributors. Structured modules progressing from fundamentals to expert topics including memory optimization and performance tuning.

- `From Python to NumPy <https://www.labri.fr/perso/nrougier/from-python-to-numpy/>`_ by Nicolas Rougier
    Focused entirely on vectorization techniques‚Äîtransforming Python loops into efficient NumPy operations. Essential for writing fast numerical code.

**NumPy and Pandas Deep Dives**

- `SciPy Lecture Notes: Advanced NumPy <https://scipy-lectures.org/advanced/advanced_numpy/>`_
    Written by Pauli Virtanen (NumPy core developer). Covers ndarray internals, strides, memory layout, and creating ufuncs. Graduate-level depth.

- `Pandas User Guide <https://pandas.pydata.org/docs/user_guide/index.html>`_
    Official comprehensive documentation. Essential sections: `GroupBy <https://pandas.pydata.org/docs/user_guide/groupby.html>`_, `Reshaping <https://pandas.pydata.org/docs/user_guide/reshaping.html>`_, and `Enhancing Performance <https://pandas.pydata.org/docs/user_guide/enhancingperf.html>`_.

- `Modern Random Generator API <https://numpy.org/doc/stable/reference/random/generator.html>`_
    Official documentation for ``np.random.default_rng()``‚Äîthe modern approach we use throughout the course.

**Machine Learning**

- `INRIA Scikit-learn MOOC <https://inria.github.io/scikit-learn-mooc/>`_
    Gold standard for ML education‚Äîdeveloped by scikit-learn core developers. 70% hands-on notebooks covering model selection, cross-validation, and ensemble methods.

- `Scikit-learn User Guide <https://scikit-learn.org/stable/user_guide.html>`_
    Official documentation with mathematical formulations for all algorithms. Includes the famous "Choosing the Right Estimator" flowchart.

**Bayesian Statistics**

- `Think Bayes 2nd Edition <https://allendowney.github.io/ThinkBayes2/>`_ by Allen Downey
    Computational approach to Bayesian statistics using Python code instead of calculus. All Jupyter notebooks available for Colab.

- `PyMC Documentation <https://www.pymc.io/projects/docs/en/stable/>`_
    Official tutorials for the probabilistic programming library we'll use. Start with the `Overview Tutorial <https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/pymc_overview.html>`_.

- `Statistical Rethinking with PyMC <https://github.com/pymc-devs/pymc-resources/tree/main/Rethinking>`_
    Full port of McElreath's acclaimed course materials to Python/PyMC.

- `ArviZ Documentation <https://python.arviz.org/en/stable/>`_
    Bayesian model visualization and diagnostics. Works with PyMC, NumPyro, Stan, and other backends.

**Large Language Models**

- `Hugging Face LLM Course <https://huggingface.co/learn/llm-course/>`_
    Comprehensive 12-chapter course covering transformer architecture, fine-tuning, and building applications. Updated for current models.

- `Prompt Engineering Guide <https://www.promptingguide.ai/>`_
    Techniques including Chain-of-Thought, ReAct, and RAG. Model-specific guidance for GPT-4, Claude, and open models.

- `LangChain Tutorials <https://python.langchain.com/docs/tutorials/>`_
    RAG implementation, agents, and complex LLM workflows. Industry-standard orchestration framework.

- `OpenAI Cookbook <https://cookbook.openai.com/>`_
    Production-ready patterns for API integration, embeddings, function calling, and cost optimization.

----

Recommended Textbooks
---------------------

There is no single textbook that covers all course topics in depth. Students seeking one comprehensive resource should start with:

  Efron, B. & Hastie, T. (2016). *Computer Age Statistical Inference: Algorithms, Evidence, and Data Science*. Cambridge University Press. https://doi.org/10.1017/CBO9781316576533

This text bridges classical frequentist methods, bootstrap and resampling, and Bayesian approaches, which partially mirrors the arc of the course.

For deeper study, the following topic-specific texts are recommended. Within each category, texts are ranked by accessibility and relevance to course material (‚òÖ‚òÖ‚òÖ = primary recommendation).

**Statistical Foundations and Inference Theory**

- ‚òÖ‚òÖ‚òÖ Abramovich, F. & Ritov, Y. (2022). *Statistical Theory: A Concise Introduction* (2nd ed.). Chapman and Hall/CRC. Concise, modern treatment of estimation, hypothesis testing, and asymptotic theory. Best for building theoretical intuition.

**Monte Carlo and Simulation Methods**

- ‚òÖ‚òÖ‚òÖ Robert, C. P. & Casella, G. (2004). *Monte Carlo Statistical Methods* (2nd ed.). Springer. https://doi.org/10.1007/978-1-4757-4145-2 The definitive reference for simulation techniques. Chapters 2‚Äì4 cover foundational methods used in Weeks 2‚Äì3.

**Resampling Methods**

- ‚òÖ‚òÖ‚òÖ Efron, B. & Tibshirani, R. J. (1994). *An Introduction to the Bootstrap*. Chapman and Hall/CRC. https://doi.org/10.1201/9780429246593 The foundational text by the method's creators. Exceptionally clear exposition; essential reading for Weeks 6‚Äì8.
- ‚òÖ‚òÖ Shao, J. & Tu, D. (1995). *The Jackknife and Bootstrap*. Springer. https://doi.org/10.1007/978-1-4612-0795-5 [Advanced] More theoretical treatment with rigorous asymptotic analysis. Recommended after Efron & Tibshirani.

**Bayesian Data Analysis**

- ‚òÖ‚òÖ‚òÖ McElreath, R. (2020). *Statistical Rethinking: A Bayesian Course with Examples in R and Stan* (2nd ed.). Chapman and Hall/CRC. https://doi.org/10.1201/9780429029608 Outstanding pedagogical approach that builds intuition before formalism. Primary recommendation for Weeks 10‚Äì12.
- ‚òÖ‚òÖ Martin, O. A. (2024). *Bayesian Analysis with Python: A Practical Guide to Probabilistic Modeling* (3rd ed.). Packt Publishing. Practical implementation focus using PyMC. Excellent for translating Bayesian concepts into working Python code.
- ‚òÖ‚òÖ Gelman, A. et al. (2013). *Bayesian Data Analysis* (3rd ed.). Chapman and Hall/CRC. https://doi.org/10.1201/b16018 [Advanced] Comprehensive reference ("BDA3"). More encyclopedic; best used for specific topics or deeper theoretical study.

