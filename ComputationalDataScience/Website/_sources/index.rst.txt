.. _index:

==========================================
Computational Methods in Data Science
==========================================

*STAT 418 · Spring 2026 · Purdue University*

----

Modern statistics is computational statistics. The elegant formulas of classical theory—derived under assumptions of normality, independence, and large samples—often fail when confronted with real data: messy, high-dimensional, and decidedly non-normal. Yet the core questions remain: How certain should we be? What can we conclude? How might we be wrong?

This course develops the computational methods that answer these questions without relying on fragile assumptions. We replace analytical derivations with simulation, asymptotic approximations with resampling, and conjugate convenience with general-purpose algorithms. The computer becomes not just a calculator but a laboratory for statistical thought experiments.

The intellectual journey moves through four parts. We begin with **foundations**: what probability means, how distributions behave, and how Python's scientific stack turns theory into computation. We then develop **frequentist inference** computationally: Monte Carlo simulation as the engine, maximum likelihood as the estimator, and bootstrap resampling as the uncertainty quantifier. Next, we explore **Bayesian inference**: prior beliefs, posterior updating, and Markov chain Monte Carlo for models too complex for analytical treatment. Finally, we address **large language models in data science**: integrating pre-trained models into analytical workflows for text preprocessing, feature extraction, data annotation, and retrieval-augmented generation—with careful attention to responsible use, reliability, and privacy.

Throughout, we emphasize both rigor and practice. Every method receives mathematical justification—you'll understand *why* these techniques work, not just *how* to call the functions. But every derivation leads to working code. By course end, you'll have built a complete toolkit for modern statistical computation, from foundational simulation through cutting-edge AI integration.

----

Course Information
------------------

.. list-table::
   :widths: 20 35 20 25
   :header-rows: 0

   * - **Instructor**
     - Dr. Timothy Reese
     - **Email**
     - reese18@purdue.edu
   * - **Office**
     - MATH 210
     - **Phone**
     - 765-494-4129
   * - **Lectures**
     - Tue/Thu 1:30–2:45 PM
     - **Room**
     - UNIV 127
   * - **Office Hours**
     - Wed/Fri 1:00–3:00 PM
     - **Location**
     - MATH 210
   * - **Credits**
     - 3.00
     - **Website**
     - `Course Site <https://treese41528.github.io/ComputationalDataScience/Website/index.html>`_

Prerequisites
~~~~~~~~~~~~~

Enrollment requires completion of the following with a grade of C- or better:

**Probability** (one of):
  - STAT 41600: Probability (undergraduate)
  - STAT 51600: Basic Probability and Applications (graduate)

**Statistical Inference** (one of):
  - STAT 35000: Introduction to Statistics
  - STAT 35500: Statistics for Data Science
  - STAT 51100: Statistical Methods (graduate)

**Programming** (one of):
  - MA 16290: Integrated Calculus and Linear Algebra II
  - CS 38003: Python Programming

You should be comfortable with: probability distributions and expectation, Bayes' theorem, the Central Limit Theorem, hypothesis testing and confidence intervals, Python programming with NumPy arrays, and calculus through multiple integrals.

----

Course Structure
----------------

The course divides into four parts, each building on the previous:

**Part I: Foundations of Probability and Computation**

What does probability mean? How do we describe and compute with random variables? Part I establishes the mathematical and philosophical groundwork: Kolmogorov's axioms, frequentist and Bayesian interpretations, probability distributions and their properties, and Python's ecosystem for statistical computing. These foundations support everything that follows.

**Part II: Frequentist Inference**

The frequentist asks: "What would happen if I repeated this procedure many times?" Part II develops this perspective computationally. Monte Carlo simulation provides the engine for approximating expectations and probabilities. Maximum likelihood estimation and generalized linear models provide the parametric toolkit. Bootstrap resampling, the jackknife, and permutation tests provide distribution-free inference when parametric assumptions fail.

**Part III: Bayesian Inference**

The Bayesian asks: "What should I believe given this evidence?" Part III develops posterior inference from prior specification through MCMC computation. We construct models, check their fit, compare alternatives, and extract predictions—all while properly quantifying uncertainty through posterior distributions.

**Part IV: Large Language Models in Data Science**

How can we leverage pre-trained language models to enhance data science workflows? Part IV addresses the practical and responsible integration of LLMs into analytical pipelines. We cover text preprocessing and feature extraction using embeddings, leveraging pre-trained models for data annotation and augmentation, retrieval-augmented generation (RAG) for domain-specific applications, and responsible AI practices including prompt engineering, reliability assessment, and privacy considerations.

The course culminates in a **capstone project** where you synthesize these methods to address a substantial data science problem, demonstrating both theoretical understanding and practical implementation skill.

----

Learning Outcomes
-----------------

Upon completing this course, you will be able to:

1. **Apply simulation techniques** including Monte Carlo methods, transformation approaches, and rejection sampling to analyze probabilistic behavior in data science applications

2. **Compare and evaluate frequentist and Bayesian inference** paradigms by examining their theoretical foundations, identifying their strengths and limitations, and explaining their roles in statistical modeling and decision-making

3. **Design, implement, and assess resampling methods** focusing on both nonparametric and parametric forms of the bootstrap to estimate variability, construct confidence intervals, and improve statistical estimates through bias correction techniques

4. **Apply cross-validation principles** to compute model performance metrics, detect overfitting and underfitting, and select models with reliable predictive accuracy using Python libraries

5. **Construct and interpret Bayesian models** including posterior distributions and credible intervals, apply Markov chain Monte Carlo methods to approximate posteriors, and evaluate the role of prior distributions in Bayesian inference

6. **Utilize large language models** in data science workflows for contextual data augmentation, feature engineering, and integrating structured and unstructured data to enhance predictive models, while addressing challenges of privacy and reliability

7. **Synthesize course methods** in a capstone project to design, develop, and present robust solutions to real-world data science challenges, demonstrating both theoretical understanding and applied expertise

----

Assessment
----------

.. list-table::
   :widths: 25 15 60
   :header-rows: 1

   * - Component
     - Weight
     - Details
   * - **Homework**
     - 40%
     - 6–7 assignments on ~2-week cadence; lowest score dropped; late submissions accepted up to 3 days with 20% penalty
   * - **Midterm Exams**
     - 30%
     - Two exams (15% each): Midterm I covers Chapters 1–4 (Foundations and Frequentist Inference); Midterm II covers Chapters 5–6 (Bayesian Inference and LLMs)
   * - **Capstone Project**
     - 30%
     - Proposal (5%), progress report (10%), final submission (15%); demonstrates synthesis of course methods on substantive problem

**Academic Integrity**: All work governed by Purdue's Honor Pledge. Collaboration encouraged on concepts; submitted work must be your own. AI tools (ChatGPT, Copilot, Claude) permitted for debugging, studying, and exploring ideas; prohibited for generating turnkey solutions. Disclose AI assistance; verify all AI-generated content for accuracy.

----

Weekly Schedule
---------------

.. list-table::
   :widths: 8 46 46
   :header-rows: 1

   * - Week
     - Tuesday
     - Thursday
   * - 1
     - Course introduction; Interpretations of probability; Inference paradigms
     - Python ecosystem; Distribution review; Distribution relationships
   * - 2
     - Monte Carlo fundamentals; Uniform random variates
     - Inverse CDF method; Transformation methods; Box–Muller
   * - 3
     - Rejection sampling
     - Variance reduction methods
   * - 4
     - Exponential families; Maximum likelihood theory
     - Statistical estimators; Sampling variability; Delta method
   * - 5
     - Least squares estimation; Assumptions and diagnostics
     - Least squares (continued); Begin generalized linear models (GLMs)
   * - 6
     - Generalized linear models (continued)
     - Sampling distribution problem; ECDF and plug-in principle
   * - 7
     - Nonparametric bootstrap
     - Nonparametric bootstrap (continued); Begin parametric bootstrap
   * - 8
     - Parametric bootstrap
     - Jackknife fundamentals (leave-one-out resampling)
   * - 9
     - Bootstrap confidence intervals; Bias correction (BCa, studentized)
     - Cross-validation for model selection and predictive performance
   * - 10
     - **Midterm I** (Chapters 1–4)
     - Bayesian foundations; Prior specification
   * - 11
     - Conjugate priors; Analytical posteriors
     - Credible intervals
   * - 12
     - Markov chain foundations
     - MCMC: Metropolis-Hastings; Gibbs sampling
   * - 13
     - Convergence diagnostics; Model comparison
     - Posterior predictive checks; Hierarchical models
   * - 14
     - LLMs in data science: integration and workflows
     - Text preprocessing; Feature extraction with embeddings
   * - 15
     - **Midterm II** (Chapters 5–6)
     - Pre-trained models for analysis and annotation; RAG systems
   * - 16
     - Responsible AI; Privacy; Prompt engineering
     - Additional LLM topics
   * - 17
     - Capstone presentations (date TBA)
     -

----

Recommended Textbooks
---------------------

There is no single textbook that covers all course topics in depth. Students seeking one comprehensive resource should start with:

  Efron, B. & Hastie, T. (2016). *Computer Age Statistical Inference: Algorithms, Evidence, and Data Science*. Cambridge University Press. https://doi.org/10.1017/CBO9781316576533

This text bridges classical frequentist methods, bootstrap and resampling, and Bayesian approaches, which partially mirrors the arc of the course.

For deeper study, the following topic-specific texts are recommended. Within each category, texts are ranked by accessibility and relevance to course material (★★★ = primary recommendation).

**Statistical Foundations and Inference Theory**

- ★★★ Abramovich, F. & Ritov, Y. (2022). *Statistical Theory: A Concise Introduction* (2nd ed.). Chapman and Hall/CRC. Concise, modern treatment of estimation, hypothesis testing, and asymptotic theory. Best for building theoretical intuition.

**Monte Carlo and Simulation Methods**

- ★★★ Robert, C. P. & Casella, G. (2004). *Monte Carlo Statistical Methods* (2nd ed.). Springer. https://doi.org/10.1007/978-1-4757-4145-2 The definitive reference for simulation techniques. Chapters 2–4 cover foundational methods used in Weeks 2–3.

**Resampling Methods**

- ★★★ Efron, B. & Tibshirani, R. J. (1994). *An Introduction to the Bootstrap*. Chapman and Hall/CRC. https://doi.org/10.1201/9780429246593 The foundational text by the method's creators. Exceptionally clear exposition; essential reading for Weeks 6–8.
- ★★ Shao, J. & Tu, D. (1995). *The Jackknife and Bootstrap*. Springer. https://doi.org/10.1007/978-1-4612-0795-5 [Advanced] More theoretical treatment with rigorous asymptotic analysis. Recommended after Efron & Tibshirani.

**Bayesian Data Analysis**

- ★★★ McElreath, R. (2020). *Statistical Rethinking: A Bayesian Course with Examples in R and Stan* (2nd ed.). Chapman and Hall/CRC. https://doi.org/10.1201/9780429029608 Outstanding pedagogical approach that builds intuition before formalism. Primary recommendation for Weeks 10–12.
- ★★ Martin, O. A. (2024). *Bayesian Analysis with Python: A Practical Guide to Probabilistic Modeling* (3rd ed.). Packt Publishing. Practical implementation focus using PyMC. Excellent for translating Bayesian concepts into working Python code.
- ★★ Gelman, A. et al. (2013). *Bayesian Data Analysis* (3rd ed.). Chapman and Hall/CRC. https://doi.org/10.1201/b16018 [Advanced] Comprehensive reference ("BDA3"). More encyclopedic; best used for specific topics or deeper theoretical study.

**Software Documentation**

- `NumPy Random Generator <https://numpy.org/doc/stable/reference/random/generator.html>`_
- `SciPy Statistics <https://docs.scipy.org/doc/scipy/reference/stats.html>`_
- `PyMC Documentation <https://www.pymc.io/>`_
- `OpenAI API Documentation <https://platform.openai.com/docs/>`_
- `Hugging Face Transformers <https://huggingface.co/docs/transformers/>`_

----

.. toctree::
   :maxdepth: 2
   :caption: Course Content

   part1_foundations/index
   part2_frequentist/index
   part3_bayesian/index
   part4_llms_datascience/index

.. toctree::
   :maxdepth: 1
   :caption: Assignments

   homework/index
   capstone/index