.. _index:

==========================================
Computational Methods in Data Science
==========================================


.. image:: https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/STAT418_brightspace_banner_2400x960.png
   :alt: STAT 418: Computational Methods in Data Science course banner
   :align: center
   :width: 100%

*STAT 418 Â· Spring 2026 Â· Purdue University*

.. toctree::
   :maxdepth: 2
   :caption: Course Content

   part1_foundations/index
   part2_frequentist/index
   part3_bayesian/index
   part4_llms_datascience/index

.. toctree::
   :maxdepth: 1
   :caption: Assignments

   homework/index
   capstone/index


----

Modern statistics is computational statistics. The elegant formulas of classical theoryâ€”derived under assumptions of normality, independence, and large samplesâ€”often fail when confronted with real data: messy, high-dimensional, and decidedly non-normal. Yet the core questions remain: How certain should we be? What can we conclude? How might we be wrong?

This course develops the computational methods that answer these questions without relying on fragile assumptions. We replace analytical derivations with simulation, asymptotic approximations with resampling, and conjugate convenience with general-purpose algorithms. The computer becomes not just a calculator but a laboratory for statistical thought experiments.

The intellectual journey moves through four parts. We begin with **foundations**: what probability means, how distributions behave, and how Python's scientific stack turns theory into computation. We then develop **frequentist inference** computationally: Monte Carlo simulation as the engine, maximum likelihood as the estimator, and bootstrap resampling as the uncertainty quantifier. Next, we explore **Bayesian inference**: prior beliefs, posterior updating, and Markov chain Monte Carlo for models too complex for analytical treatment. Finally, we address **large language models in data science**: integrating pre-trained models into analytical workflows for text preprocessing, feature extraction, data annotation, and retrieval-augmented generationâ€”with careful attention to responsible use, reliability, and privacy.

Throughout, we emphasize both rigor and practice. Every method receives mathematical justificationâ€”you'll understand *why* these techniques work, not just *how* to call the functions. But every derivation leads to working code. By course end, you'll have built a complete toolkit for modern statistical computation, from foundational simulation through cutting-edge AI integration.

----

Course Information
------------------

.. list-table::
   :widths: 20 35 20 25
   :header-rows: 0

   * - **Instructor**
     - Dr. Timothy Reese
     - **Email**
     - reese18@purdue.edu
   * - **Office**
     - MATH 210
     - **Phone**
     - 765-494-4129
   * - **Lectures**
     - Tue/Thu 1:30â€“2:45 PM
     - **Room**
     - UNIV 127
   * - **Office Hours**
     - Wed/Fri 1:00â€“3:00 PM
     - **Location**
     - MATH 210
   * - **Credits**
     - 3.00
     - **Website**
     - `Course Site <https://treese41528.github.io/ComputationalDataScience/Website/index.html>`_

Prerequisites
~~~~~~~~~~~~~

Enrollment requires completion of the following with a grade of C- or better:

**Probability** (one of):
  - STAT 41600: Probability (undergraduate)
  - STAT 51600: Basic Probability and Applications (graduate)

**Statistical Inference** (one of):
  - STAT 35000: Introduction to Statistics
  - STAT 35500: Statistics for Data Science
  - STAT 51100: Statistical Methods (graduate)

**Programming** (one of):
  - MA 16290: Integrated Calculus and Linear Algebra II
  - CS 38003: Python Programming


You should be comfortable with: probability axioms and random variables; discrete and continuous distributions (PMFs, PDFs, CDFs); expectation, variance, and covariance; joint, marginal, and conditional distributions; functions and transformations of random variables; moment generating functions (MGFs); the Law of Large Numbers and Central Limit Theorem; Bayes' theorem; hypothesis testing and confidence intervals; Python programming with NumPy arrays; and calculus through multiple integrals.

----

Course Structure
----------------

The course divides into four parts, each building on the previous:

**Part I: Foundations of Probability and Computation**

What does probability mean? How do we describe and compute with random variables? Part I establishes the mathematical and philosophical groundwork: Kolmogorov's axioms, frequentist and Bayesian interpretations, probability distributions and their properties, and Python's ecosystem for statistical computing. These foundations support everything that follows.

**Part II: Frequentist Inference**

The frequentist asks: "What would happen if I repeated this procedure many times?" Part II develops this perspective computationally. Monte Carlo simulation provides the engine for approximating expectations and probabilities. Maximum likelihood estimation and generalized linear models provide the parametric toolkit. Bootstrap resampling, the jackknife, and permutation tests provide distribution-free inference when parametric assumptions fail.

**Part III: Bayesian Inference**

The Bayesian asks: "What should I believe given this evidence?" Part III develops posterior inference from prior specification through MCMC computation. We construct models, check their fit, compare alternatives, and extract predictionsâ€”all while properly quantifying uncertainty through posterior distributions.

**Part IV: Large Language Models in Data Science**

How can we leverage pre-trained language models to enhance data science workflows? Part IV addresses the practical and responsible integration of LLMs into analytical pipelines. We cover text preprocessing and feature extraction using embeddings, leveraging pre-trained models for data annotation and augmentation, retrieval-augmented generation (RAG) for domain-specific applications, and responsible AI practices including prompt engineering, reliability assessment, and privacy considerations.

The course culminates in a **capstone project** where you synthesize these methods to address a substantial data science problem, demonstrating both theoretical understanding and practical implementation skill.

----

Learning Outcomes
-----------------

Upon completing this course, you will be able to:

1. **Apply simulation techniques** including Monte Carlo methods, transformation approaches, and rejection sampling to analyze probabilistic behavior in data science applications

2. **Compare and evaluate frequentist and Bayesian inference** paradigms by examining their theoretical foundations, identifying their strengths and limitations, and explaining their roles in statistical modeling and decision-making

3. **Design, implement, and assess resampling methods** focusing on both nonparametric and parametric forms of the bootstrap to estimate variability, construct confidence intervals, and improve statistical estimates through bias correction techniques

4. **Apply cross-validation principles** to compute model performance metrics, detect overfitting and underfitting, and select models with reliable predictive accuracy using Python libraries

5. **Construct and interpret Bayesian models** including posterior distributions and credible intervals, apply Markov chain Monte Carlo methods to approximate posteriors, and evaluate the role of prior distributions in Bayesian inference

6. **Utilize large language models** in data science workflows for contextual data augmentation, feature engineering, and integrating structured and unstructured data to enhance predictive models, while addressing challenges of privacy and reliability

7. **Synthesize course methods** in a capstone project to design, develop, and present robust solutions to real-world data science challenges, demonstrating both theoretical understanding and applied expertise

----

Assessment
----------

.. list-table::
   :widths: 25 15 60
   :header-rows: 1

   * - Component
     - Weight
     - Details
   * - **Homework**
     - 40%
     - 6â€“7 assignments on ~2-week cadence; lowest score dropped; late submissions accepted up to 3 days with 20% penalty
   * - **Midterm Exams**
     - 30%
     - Two exams (15% each): Midterm I covers Chapters 1â€“3 (Foundations, Monte Carlo, Frequentist Inference); Midterm II covers Chapters 4â€“5 (Resampling Methods, Bayesian Inference)
   * - **Capstone Project**
     - 30%
     - Proposal (2%), progress report (1%),  presentation (7%), final submission (20%); demonstrates synthesis of course methods on substantive problem

**Academic Integrity**: All work governed by Purdue's Honor Pledge. Collaboration encouraged on concepts; submitted work must be your own. AI tools (ChatGPT, Copilot, Claude) permitted for debugging, studying, and exploring ideas; prohibited for generating turnkey solutions. Disclose AI assistance; verify all AI-generated content for accuracy.

----

Schedule & Syllabus
-------------------

ðŸ“… `Course Schedule <https://treese41528.github.io/ComputationalDataScience/Website/STAT_418_Schedule.html>`_
   Interactive weekly schedule with topics, assignments, and exam dates.

ðŸ“‹ `Course Syllabus (PDF) <https://treese41528.github.io/ComputationalDataScience/Website/Computational%20Methods%20in%20Data%20Science%20Syllabus.pdf>`_
   Complete syllabus with policies, grading breakdown, and academic integrity guidelines.

----

Companion Notebooks
-------------------

These Jupyter notebooks accompany the course lectures. Each chapter notebook contains worked examples, visualizations, and code you can run and modify. View the rendered HTML online or download the ``.ipynb`` files to run locally (right-click and "Save Link As" if needed).

*Additional notebooks will be released as the course progresses.*

.. raw:: html

   <table class="docutils align-default" style="width:100%">
   <thead>
   <tr><th>Chapter</th><th>View Online</th><th>Download</th></tr>
   </thead>
   <tbody>
   <tr>
     <td><strong>Chapter 1</strong>: Probability Foundations &amp; Python Review</td>
     <td><a href="https://treese41528.github.io/ComputationalDataScience/Website/Notebooks/PartI/chapter1_review.html">ðŸ”— View HTML</a></td>
     <td><a href="https://treese41528.github.io/ComputationalDataScience/Website/Notebooks/PartI/chapter1_review.ipynb" download="chapter1_review.ipynb">â¬‡ Download .ipynb</a></td>
   </tr>
   <tr>
     <td><strong>Chapter 2</strong>: Monte Carlo Methods</td>
     <td><a href="https://treese41528.github.io/ComputationalDataScience/Website/Notebooks/PartII/Chapter2/chapter2_monte_carlo_methods.html">ðŸ”— View HTML</a></td>
     <td><a href="https://treese41528.github.io/ComputationalDataScience/Website/Notebooks/PartII/Chapter2/chapter2_monte_carlo_methods.ipynb" download="chapter2_monte_carlo_methods.ipynb">â¬‡ Download .ipynb</a></td>
   </tr>
   <tr>
     <td><strong>Chapter 3</strong>: Parametric Inference</td>
     <td><a href="https://treese41528.github.io/ComputationalDataScience/Website/Notebooks/PartII/Chapter3/chapter3_parametric_inference.html">ðŸ”— View HTML</a></td>
     <td><a href="https://treese41528.github.io/ComputationalDataScience/Website/Notebooks/PartII/Chapter3/chapter3_parametric_inference.ipynb" download="chapter3_parametric_inference.ipynb">â¬‡ Download .ipynb</a></td>
   </tr>
   </tbody>
   </table>

----

Python Tutorials for Further Study
----------------------------------

The resources below are curated for students who want to deepen their Python skills beyond the course material. All are free, open-source, and maintained by recognized experts or framework developers.

**Course Environment Setup**

.. raw:: html

   <p><a href="https://treese41528.github.io/ComputationalDataScience/Website/requirements.txt" download="requirements.txt">â¬‡ Download requirements.txt</a> â€” Python package dependencies for the course. Install with <code>pip install -r requirements.txt</code> in a virtual environment.</p>

**Data Science Foundations**

- `Python Data Science Handbook <https://jakevdp.github.io/PythonDataScienceHandbook/>`_ by Jake VanderPlas
    Complete free book covering NumPy, Pandas, Matplotlib, and Scikit-learn. All content available as executable Jupyter notebooks. Essential reference for the scientific Python stack.

- `Scientific Python Lectures <https://lectures.scientific-python.org/>`_
    From core SciPy contributors. Structured modules progressing from fundamentals to expert topics including memory optimization and performance tuning.

- `From Python to NumPy <https://www.labri.fr/perso/nrougier/from-python-to-numpy/>`_ by Nicolas Rougier
    Focused entirely on vectorization techniquesâ€”transforming Python loops into efficient NumPy operations. Essential for writing fast numerical code.

**NumPy and Pandas Deep Dives**

- `SciPy Lecture Notes: Advanced NumPy <https://scipy-lectures.org/advanced/advanced_numpy/>`_
    Written by Pauli Virtanen (NumPy core developer). Covers ndarray internals, strides, memory layout, and creating ufuncs. Graduate-level depth.

- `Pandas User Guide <https://pandas.pydata.org/docs/user_guide/index.html>`_
    Official comprehensive documentation. Essential sections: `GroupBy <https://pandas.pydata.org/docs/user_guide/groupby.html>`_, `Reshaping <https://pandas.pydata.org/docs/user_guide/reshaping.html>`_, and `Enhancing Performance <https://pandas.pydata.org/docs/user_guide/enhancingperf.html>`_.

- `Modern Random Generator API <https://numpy.org/doc/stable/reference/random/generator.html>`_
    Official documentation for ``np.random.default_rng()``â€”the modern approach we use throughout the course.

**Machine Learning**

- `INRIA Scikit-learn MOOC <https://inria.github.io/scikit-learn-mooc/>`_
    Gold standard for ML educationâ€”developed by scikit-learn core developers. 70% hands-on notebooks covering model selection, cross-validation, and ensemble methods.

- `Scikit-learn User Guide <https://scikit-learn.org/stable/user_guide.html>`_
    Official documentation with mathematical formulations for all algorithms. Includes the famous "Choosing the Right Estimator" flowchart.

**Bayesian Statistics**

- `Think Bayes 2nd Edition <https://allendowney.github.io/ThinkBayes2/>`_ by Allen Downey
    Computational approach to Bayesian statistics using Python code instead of calculus. All Jupyter notebooks available for Colab.

- `PyMC Documentation <https://www.pymc.io/projects/docs/en/stable/>`_
    Official tutorials for the probabilistic programming library we'll use. Start with the `Overview Tutorial <https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/pymc_overview.html>`_.

- `Statistical Rethinking with PyMC <https://github.com/pymc-devs/pymc-resources/tree/main/Rethinking>`_
    Full port of McElreath's acclaimed course materials to Python/PyMC.

- `ArviZ Documentation <https://python.arviz.org/en/stable/>`_
    Bayesian model visualization and diagnostics. Works with PyMC, NumPyro, Stan, and other backends.

**Large Language Models**

- `Hugging Face LLM Course <https://huggingface.co/learn/llm-course/>`_
    Comprehensive 12-chapter course covering transformer architecture, fine-tuning, and building applications. Updated for current models.

- `Prompt Engineering Guide <https://www.promptingguide.ai/>`_
    Techniques including Chain-of-Thought, ReAct, and RAG. Model-specific guidance for GPT-4, Claude, and open models.

- `LangChain Tutorials <https://python.langchain.com/docs/tutorials/>`_
    RAG implementation, agents, and complex LLM workflows. Industry-standard orchestration framework.

- `OpenAI Cookbook <https://cookbook.openai.com/>`_
    Production-ready patterns for API integration, embeddings, function calling, and cost optimization.

----

Recommended Textbooks
---------------------

There is no single textbook that covers all course topics in depth. Students seeking one comprehensive resource should start with:

  Efron, B. & Hastie, T. (2016). *Computer Age Statistical Inference: Algorithms, Evidence, and Data Science*. Cambridge University Press. https://doi.org/10.1017/CBO9781316576533

This text bridges classical frequentist methods, bootstrap and resampling, and Bayesian approaches, which partially mirrors the arc of the course.

For deeper study, the following topic-specific texts are recommended. Within each category, texts are ranked by accessibility and relevance to course material (â˜…â˜…â˜… = primary recommendation).

**Statistical Foundations and Inference Theory**

- â˜…â˜…â˜… Abramovich, F. & Ritov, Y. (2022). *Statistical Theory: A Concise Introduction* (2nd ed.). Chapman and Hall/CRC. Concise, modern treatment of estimation, hypothesis testing, and asymptotic theory. Best for building theoretical intuition.

**Monte Carlo and Simulation Methods**

- â˜…â˜…â˜… Robert, C. P. & Casella, G. (2004). *Monte Carlo Statistical Methods* (2nd ed.). Springer. https://doi.org/10.1007/978-1-4757-4145-2 The definitive reference for simulation techniques. Chapters 2â€“4 cover foundational methods used in Weeks 2â€“3.

**Resampling Methods**

- â˜…â˜…â˜… Efron, B. & Tibshirani, R. J. (1994). *An Introduction to the Bootstrap*. Chapman and Hall/CRC. https://doi.org/10.1201/9780429246593 The foundational text by the method's creators. Exceptionally clear exposition; essential reading for Weeks 6â€“8.
- â˜…â˜… Shao, J. & Tu, D. (1995). *The Jackknife and Bootstrap*. Springer. https://doi.org/10.1007/978-1-4612-0795-5 [Advanced] More theoretical treatment with rigorous asymptotic analysis. Recommended after Efron & Tibshirani.

**Bayesian Data Analysis**

- â˜…â˜…â˜… McElreath, R. (2020). *Statistical Rethinking: A Bayesian Course with Examples in R and Stan* (2nd ed.). Chapman and Hall/CRC. https://doi.org/10.1201/9780429029608 Outstanding pedagogical approach that builds intuition before formalism. Primary recommendation for Weeks 10â€“12.
- â˜…â˜… Martin, O. A. (2024). *Bayesian Analysis with Python: A Practical Guide to Probabilistic Modeling* (3rd ed.). Packt Publishing. Practical implementation focus using PyMC. Excellent for translating Bayesian concepts into working Python code.
- â˜…â˜… Gelman, A. et al. (2013). *Bayesian Data Analysis* (3rd ed.). Chapman and Hall/CRC. https://doi.org/10.1201/b16018 [Advanced] Comprehensive reference ("BDA3"). More encyclopedic; best used for specific topics or deeper theoretical study.

