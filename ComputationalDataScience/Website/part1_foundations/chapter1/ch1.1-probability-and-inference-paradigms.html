

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>1.1.1. Paradigms of Probability and Statistical Inference &mdash; STAT 418</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=dc393e06" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT418/Website/part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=30646c52"></script>
      <script>let toggleHintShow = 'Show solution';</script>
      <script>let toggleHintHide = 'Hide solution';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>window.MathJax = {"options": {"enableMenu": true, "menuOptions": {"settings": {"enrich": true, "speech": true, "braille": true, "collapsible": true, "assistiveMml": false}}}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
      <script src="../../_static/custom.js?v=8718e0ab"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="1.1.2. Probability Distributions: Theory and Computation" href="ch1.2-probability_distributions_review.html" />
    <link rel="prev" title="1.1. Chapter 1: Statistical Paradigms and Core Concepts" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Course Content</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">1. Part I: Foundations of Probability and Computation</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="index.html">1.1. Chapter 1: Statistical Paradigms and Core Concepts</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">1.1.1. Paradigms of Probability and Statistical Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#the-mathematical-foundation-kolmogorov-s-axioms">The Mathematical Foundation: Kolmogorov’s Axioms</a></li>
<li class="toctree-l4"><a class="reference internal" href="#interpretations-of-probability">Interpretations of Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="#statistical-inference-paradigms">Statistical Inference Paradigms</a></li>
<li class="toctree-l4"><a class="reference internal" href="#historical-and-philosophical-debates">Historical and Philosophical Debates</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="#looking-ahead-our-course-focus">Looking Ahead: Our Course Focus</a></li>
<li class="toctree-l4"><a class="reference internal" href="#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch1.2-probability_distributions_review.html">1.1.2. Probability Distributions: Theory and Computation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch1.2-probability_distributions_review.html#from-abstract-foundations-to-concrete-tools">From Abstract Foundations to Concrete Tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.2-probability_distributions_review.html#the-python-ecosystem-for-probability">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.2-probability_distributions_review.html#introduction-why-probability-distributions-matter">Introduction: Why Probability Distributions Matter</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.2-probability_distributions_review.html#id1">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.2-probability_distributions_review.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.2-probability_distributions_review.html#continuous-distributions">Continuous Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.2-probability_distributions_review.html#additional-important-distributions">Additional Important Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.2-probability_distributions_review.html#summary-and-practical-guidelines">Summary and Practical Guidelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.2-probability_distributions_review.html#conclusion">Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.2-probability_distributions_review.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch1.3-python_random_generation.html">1.1.3. Python Random Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch1.3-python_random_generation.html#from-mathematical-distributions-to-computational-samples">From Mathematical Distributions to Computational Samples</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.3-python_random_generation.html#the-python-ecosystem-at-a-glance">The Python Ecosystem at a Glance</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.3-python_random_generation.html#understanding-pseudo-random-number-generation">Understanding Pseudo-Random Number Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.3-python_random_generation.html#the-standard-library-random-module">The Standard Library: <code class="docutils literal notranslate"><span class="pre">random</span></code> Module</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.3-python_random_generation.html#numpy-fast-vectorized-random-sampling">NumPy: Fast Vectorized Random Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.3-python_random_generation.html#scipy-stats-the-complete-statistical-toolkit">SciPy Stats: The Complete Statistical Toolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.3-python_random_generation.html#bringing-it-all-together-library-selection-guide">Bringing It All Together: Library Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.3-python_random_generation.html#looking-ahead-from-random-numbers-to-monte-carlo-methods">Looking Ahead: From Random Numbers to Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.3-python_random_generation.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch1.4-chapter-summary.html">1.1.4. Chapter 1 Summary: Foundations in Place</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch1.4-chapter-summary.html#the-three-pillars-of-chapter-1">The Three Pillars of Chapter 1</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.4-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.4-chapter-summary.html#what-lies-ahead-the-road-to-simulation">What Lies Ahead: The Road to Simulation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.4-chapter-summary.html#chapter-1-exercises-synthesis-problems">Chapter 1 Exercises: Synthesis Problems</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.4-chapter-summary.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part2_simulation/index.html">2. Part II: Simulation-Based Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part2_simulation/chapter2/index.html">2.1. Chapter 2: Monte Carlo Simulation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html">2.1.1. Monte Carlo Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html#the-historical-development-of-monte-carlo-methods">The Historical Development of Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html#the-core-principle-expectation-as-integration">The Core Principle: Expectation as Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html#theoretical-foundations">Theoretical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html#variance-estimation-and-confidence-intervals">Variance Estimation and Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html#comparison-with-deterministic-methods">Comparison with Deterministic Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html#sample-size-determination">Sample Size Determination</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html#convergence-diagnostics-and-monitoring">Convergence Diagnostics and Monitoring</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html#chapter-2-1-exercises-monte-carlo-fundamentals-mastery">Chapter 2.1 Exercises: Monte Carlo Fundamentals Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_2-uniform-random-variates.html">2.1.2. Uniform Random Variates</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_2-uniform-random-variates.html#why-uniform-the-universal-currency-of-randomness">Why Uniform? The Universal Currency of Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_2-uniform-random-variates.html#the-paradox-of-computational-randomness">The Paradox of Computational Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_2-uniform-random-variates.html#chaotic-dynamical-systems-an-instructive-failure">Chaotic Dynamical Systems: An Instructive Failure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_2-uniform-random-variates.html#linear-congruential-generators">Linear Congruential Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_2-uniform-random-variates.html#shift-register-generators">Shift-Register Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_2-uniform-random-variates.html#the-kiss-generator-combining-strategies">The KISS Generator: Combining Strategies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_2-uniform-random-variates.html#modern-generators-mersenne-twister-and-pcg">Modern Generators: Mersenne Twister and PCG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_2-uniform-random-variates.html#statistical-testing-of-random-number-generators">Statistical Testing of Random Number Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_2-uniform-random-variates.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_2-uniform-random-variates.html#chapter-2-2-exercises-uniform-random-variates-mastery">Chapter 2.2 Exercises: Uniform Random Variates Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_2-uniform-random-variates.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_2-uniform-random-variates.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_2-uniform-random-variates.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_3-inverse-cdf-method.html">2.1.3. Inverse CDF Method</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_3-inverse-cdf-method.html#mathematical-foundations">Mathematical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_3-inverse-cdf-method.html#continuous-distributions-with-closed-form-inverses">Continuous Distributions with Closed-Form Inverses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_3-inverse-cdf-method.html#numerical-inversion">Numerical Inversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_3-inverse-cdf-method.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_3-inverse-cdf-method.html#mixed-distributions">Mixed Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_3-inverse-cdf-method.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_3-inverse-cdf-method.html#chapter-2-3-exercises-inverse-cdf-method-mastery">Chapter 2.3 Exercises: Inverse CDF Method Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_3-inverse-cdf-method.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_3-inverse-cdf-method.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_3-inverse-cdf-method.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_3-inverse-cdf-method.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_4-transformation-methods.html">2.1.4. Transformation Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_4-transformation-methods.html#why-transformation-methods">Why Transformation Methods?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_4-transformation-methods.html#the-boxmuller-transform">The Box–Muller Transform</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_4-transformation-methods.html#the-polar-marsaglia-method">The Polar (Marsaglia) Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_4-transformation-methods.html#method-comparison-boxmuller-vs-polar-vs-ziggurat">Method Comparison: Box–Muller vs Polar vs Ziggurat</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_4-transformation-methods.html#the-ziggurat-algorithm">The Ziggurat Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_4-transformation-methods.html#the-clt-approximation-historical">The CLT Approximation (Historical)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_4-transformation-methods.html#distributions-derived-from-the-normal">Distributions Derived from the Normal</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_4-transformation-methods.html#multivariate-normal-generation">Multivariate Normal Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_4-transformation-methods.html#implementation-guidance">Implementation Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_4-transformation-methods.html#chapter-2-4-exercises-transformation-methods-mastery">Chapter 2.4 Exercises: Transformation Methods Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_4-transformation-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_4-transformation-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html">2.1.5. Rejection Sampling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html#the-dartboard-intuition">The Dartboard Intuition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html#the-accept-reject-algorithm">The Accept-Reject Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html#efficiency-analysis">Efficiency Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html#choosing-the-proposal-distribution">Choosing the Proposal Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html#the-squeeze-principle">The Squeeze Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html#geometric-example-sampling-from-the-unit-disk">Geometric Example: Sampling from the Unit Disk</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html#limitations-and-the-curse-of-dimensionality">Limitations and the Curse of Dimensionality</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html#connections-to-other-methods">Connections to Other Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html#chapter-2-5-exercises-rejection-sampling-mastery">Chapter 2.5 Exercises: Rejection Sampling Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_6-variance-reduction-methods.html">2.1.6. Variance Reduction Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_6-variance-reduction-methods.html#the-variance-reduction-paradigm">The Variance Reduction Paradigm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_6-variance-reduction-methods.html#importance-sampling">Importance Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_6-variance-reduction-methods.html#control-variates">Control Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_6-variance-reduction-methods.html#antithetic-variates">Antithetic Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_6-variance-reduction-methods.html#stratified-sampling">Stratified Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_6-variance-reduction-methods.html#common-random-numbers">Common Random Numbers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_6-variance-reduction-methods.html#conditional-monte-carlo-raoblackwellization">Conditional Monte Carlo (Rao–Blackwellization)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_6-variance-reduction-methods.html#combining-variance-reduction-techniques">Combining Variance Reduction Techniques</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_6-variance-reduction-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_6-variance-reduction-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_6-variance-reduction-methods.html#chapter-2-6-exercises-variance-reduction-mastery">Chapter 2.6 Exercises: Variance Reduction Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_6-variance-reduction-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_7-chapter-summary.html">2.1.7. Chapter Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_7-chapter-summary.html#the-complete-monte-carlo-workflow">The Complete Monte Carlo Workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_7-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_7-chapter-summary.html#quick-reference-tables">Quick Reference Tables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_7-chapter-summary.html#common-pitfalls-checklist">Common Pitfalls Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_7-chapter-summary.html#connections-to-later-chapters">Connections to Later Chapters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_7-chapter-summary.html#learning-outcomes-checklist">Learning Outcomes Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_7-chapter-summary.html#further-reading-optimization-and-missing-data">Further Reading: Optimization and Missing Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_7-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_7-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../part2_simulation/chapter3/index.html">2.2. Chapter 3: Parametric Inference and Likelihood Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_1-exponential-families.html">2.2.1. Exponential Families</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_1-exponential-families.html#historical-origins-from-scattered-results-to-unified-theory">Historical Origins: From Scattered Results to Unified Theory</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_1-exponential-families.html#the-canonical-exponential-family">The Canonical Exponential Family</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_1-exponential-families.html#converting-familiar-distributions">Converting Familiar Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_1-exponential-families.html#the-log-partition-function-a-moment-generating-machine">The Log-Partition Function: A Moment-Generating Machine</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_1-exponential-families.html#sufficiency-capturing-all-parameter-information">Sufficiency: Capturing All Parameter Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_1-exponential-families.html#minimal-sufficiency-and-completeness">Minimal Sufficiency and Completeness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_1-exponential-families.html#conjugate-priors-and-bayesian-inference">Conjugate Priors and Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_1-exponential-families.html#exponential-dispersion-models-and-glms">Exponential Dispersion Models and GLMs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_1-exponential-families.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_1-exponential-families.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_1-exponential-families.html#chapter-3-1-exercises-exponential-families-mastery">Chapter 3.1 Exercises: Exponential Families Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_1-exponential-families.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_1-exponential-families.html#further-reading">Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html">2.2.2. Maximum Likelihood Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#the-likelihood-function">The Likelihood Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#the-score-function">The Score Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#fisher-information">Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#closed-form-maximum-likelihood-estimators">Closed-Form Maximum Likelihood Estimators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#numerical-optimization-for-mle">Numerical Optimization for MLE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#asymptotic-properties-of-mles">Asymptotic Properties of MLEs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#the-cramer-rao-lower-bound">The Cramér-Rao Lower Bound</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#the-invariance-property">The Invariance Property</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#likelihood-based-hypothesis-testing">Likelihood-Based Hypothesis Testing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#confidence-intervals-from-likelihood">Confidence Intervals from Likelihood</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#connection-to-bayesian-inference">Connection to Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#chapter-3-2-exercises-maximum-likelihood-estimation-mastery">Chapter 3.2 Exercises: Maximum Likelihood Estimation Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_3-sampling-variability.html">2.2.3. Sampling Variability and Variance Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_3-sampling-variability.html#statistical-estimators-and-their-properties">Statistical Estimators and Their Properties</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_3-sampling-variability.html#sampling-distributions">Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_3-sampling-variability.html#the-delta-method">The Delta Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_3-sampling-variability.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_3-sampling-variability.html#variance-estimation-methods">Variance Estimation Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_3-sampling-variability.html#applications-and-worked-examples">Applications and Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_3-sampling-variability.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_3-sampling-variability.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_3-sampling-variability.html#exercises">Exercises</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../part2_simulation/chapter4/index.html">2.3. Chapter 4: Resampling Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter4/jackknife_introduction.html">2.3.1. Jackknife Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/jackknife_introduction.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/jackknife_introduction.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/jackknife_introduction.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/jackknife_introduction.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/jackknife_introduction.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/jackknife_introduction.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter4/bootstrap_fundamentals.html">2.3.2. Bootstrap Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/bootstrap_fundamentals.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/bootstrap_fundamentals.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/bootstrap_fundamentals.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/bootstrap_fundamentals.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/bootstrap_fundamentals.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/bootstrap_fundamentals.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter4/nonparametric_bootstrap.html">2.3.3. Nonparametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/nonparametric_bootstrap.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/nonparametric_bootstrap.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/nonparametric_bootstrap.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/nonparametric_bootstrap.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/nonparametric_bootstrap.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/nonparametric_bootstrap.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter4/parametric_bootstrap.html">2.3.4. Parametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/parametric_bootstrap.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/parametric_bootstrap.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/parametric_bootstrap.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/parametric_bootstrap.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/parametric_bootstrap.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/parametric_bootstrap.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter4/confidence_intervals.html">2.3.5. Confidence Intervals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/confidence_intervals.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/confidence_intervals.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/confidence_intervals.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/confidence_intervals.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/confidence_intervals.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/confidence_intervals.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter4/bias_correction.html">2.3.6. Bias Correction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/bias_correction.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/bias_correction.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/bias_correction.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/bias_correction.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/bias_correction.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/bias_correction.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter4/cross_validation_loo.html">2.3.7. Cross Validation Loo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/cross_validation_loo.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/cross_validation_loo.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/cross_validation_loo.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/cross_validation_loo.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/cross_validation_loo.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/cross_validation_loo.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter4/cross_validation_k_fold.html">2.3.8. Cross Validation K Fold</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/cross_validation_k_fold.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/cross_validation_k_fold.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/cross_validation_k_fold.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/cross_validation_k_fold.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/cross_validation_k_fold.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/cross_validation_k_fold.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter4/model_selection.html">2.3.9. Model Selection</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/model_selection.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/model_selection.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/model_selection.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/model_selection.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/model_selection.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/model_selection.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part3_bayesian/index.html">Part III: Bayesian Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part3_bayesian/index.html#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html">1. Bayesian Philosophy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#introduction">1.1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#key-concepts">1.2. Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#mathematical-framework">1.3. Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#python-implementation">1.4. Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#examples">1.5. Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#summary">1.6. Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html"><span class="section-number">1. </span>Part I: Foundations of Probability and Computation</a></li>
          <li class="breadcrumb-item"><a href="index.html"><span class="section-number">1.1. </span>Chapter 1: Statistical Paradigms and Core Concepts</a></li>
      <li class="breadcrumb-item active"><span class="section-number">1.1.1. </span>Paradigms of Probability and Statistical Inference</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="paradigms-of-probability-and-statistical-inference">
<span id="ch1-1-probability-and-inference-paradigms"></span><h1><span class="section-number">1.1.1. </span>Paradigms of Probability and Statistical Inference<a class="headerlink" href="#paradigms-of-probability-and-statistical-inference" title="Link to this heading"></a></h1>
<p>Understanding probability and statistical inference is essential for modern data science. Whether we are simulating complex stochastic systems, quantifying prediction confidence, or choosing between competing models, we must first confront a fundamental question: <strong>What does probability mean?</strong> The answer shapes not only our philosophical perspective but also our computational methods, our interpretation of results, and ultimately the questions we can answer with data.</p>
<p>This chapter establishes the mathematical and philosophical foundation for all computational methods in this course. We will see that while everyone agrees on the mathematical rules governing probability (Kolmogorov’s axioms), profound disagreements exist about what these numbers actually represent—and these disagreements lead to radically different approaches to statistical inference.</p>
<div class="important admonition">
<p class="admonition-title">Road Map 🧭</p>
<ul class="simple">
<li><p><strong>Understand</strong>: Kolmogorov’s axiomatic foundation and why it’s interpretation-neutral</p></li>
<li><p><strong>Develop</strong>: Ability to distinguish frequentist, Bayesian, and likelihood-based reasoning</p></li>
<li><p><strong>Implement</strong>: Basic probability calculations in Python that work across all paradigms</p></li>
<li><p><strong>Evaluate</strong>: When to apply different inference frameworks based on problem structure and goals</p></li>
</ul>
</div>
<section id="the-mathematical-foundation-kolmogorov-s-axioms">
<h2>The Mathematical Foundation: Kolmogorov’s Axioms<a class="headerlink" href="#the-mathematical-foundation-kolmogorov-s-axioms" title="Link to this heading"></a></h2>
<p>Before discussing what probability means philosophically, we must establish its mathematical foundation. In 1933, Andrey Kolmogorov provided a rigorous axiomatic framework that unified centuries of probabilistic thinking and that all modern approaches accept, regardless of their philosophical interpretation.</p>
<section id="the-probability-space">
<h3>The Probability Space<a class="headerlink" href="#the-probability-space" title="Link to this heading"></a></h3>
<p>All of probability theory is built on the concept of a <strong>probability space</strong>. In full mathematical generality, this is formalized using measure theory as a triple <span class="math notranslate nohighlight">\((\Omega, \mathcal{F}, P)\)</span> involving σ-algebras. However, for computational data science, we can work with a more intuitive framework that covers all practical cases.</p>
<div class="note admonition">
<p class="admonition-title">Note on Mathematical Rigor 📚</p>
<p>In advanced probability theory, σ-algebras (collections of “measurable” events)
are essential for handling continuous probability spaces rigorously. Throughout
this course, we’ll work with the simplified framework below, which is sufficient
for all computational applications. Readers interested in measure-theoretic
foundations can consult <a class="reference external" href="https://www.colorado.edu/amath/sites/default/files/attached-files/billingsley.pdf">Billingsley (1995)</a>
or <a class="reference external" href="https://sites.math.duke.edu/~rtd/PTE/PTE5_011119.pdf">Durrett (2019)</a>.</p>
</div>
<figure class="align-center" id="id11">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/Part1/ch1_1_probability_space.png"><img alt="Venn diagram showing sample space Omega containing three events A, B (overlapping), and C (disjoint), with scattered sample points" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/Part1/ch1_1_probability_space.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 1.1 </span><span class="caption-text">A probability space consists of a sample space Ω (rectangle) containing all possible outcomes, with events as subsets. Events A and B overlap, illustrating <span class="math notranslate nohighlight">\(P(A \cup B) = P(A) + P(B) - P(A \cap B)\)</span>. Event C is disjoint from both.</span><a class="headerlink" href="#id11" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>For this course, a probability space consists of:</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\Omega\)</span> is the <strong>sample space</strong>—the set of all possible outcomes of a random experiment.</p>
<ul>
<li><p>For a coin flip: <span class="math notranslate nohighlight">\(\Omega = \{H, T\}\)</span></p></li>
<li><p>For a die roll: <span class="math notranslate nohighlight">\(\Omega = \{1, 2, 3, 4, 5, 6\}\)</span></p></li>
<li><p>For measuring a continuous quantity: <span class="math notranslate nohighlight">\(\Omega = \mathbb{R}\)</span> or an interval like <span class="math notranslate nohighlight">\([0, 1]\)</span></p></li>
</ul>
</li>
<li><p><strong>Events</strong> are subsets of <span class="math notranslate nohighlight">\(\Omega\)</span> to which we can assign probabilities.</p>
<ul>
<li><p>In discrete spaces: any subset is an event</p></li>
<li><p>In continuous spaces: “reasonable” subsets (intervals, unions of intervals, etc.) are events</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(P\)</span> is a <strong>probability measure</strong>—a function assigning probabilities to events that satisfies Kolmogorov’s axioms.</p></li>
</ul>
</section>
<section id="kolmogorov-s-three-axioms">
<h3>Kolmogorov’s Three Axioms<a class="headerlink" href="#kolmogorov-s-three-axioms" title="Link to this heading"></a></h3>
<p>For computational work, we can state Kolmogorov’s axioms in an accessible form that works for both discrete and continuous probability spaces:</p>
<p><strong>Axiom 1 (Non-negativity)</strong>: For any event <span class="math notranslate nohighlight">\(E\)</span>,</p>
<div class="math notranslate nohighlight">
\[P(E) \geq 0\]</div>
<p>Probabilities cannot be negative—they represent a degree of certainty that is at minimum zero.</p>
<p><strong>Axiom 2 (Normalization)</strong>: The probability of the entire sample space is unity,</p>
<div class="math notranslate nohighlight">
\[P(\Omega) = 1\]</div>
<p>Something in the sample space must occur with certainty.</p>
<p><strong>Axiom 3 (Additivity)</strong>: For any countable sequence of <strong>pairwise disjoint</strong> events <span class="math notranslate nohighlight">\(E_1, E_2, E_3, \ldots\)</span> where <span class="math notranslate nohighlight">\(E_i \cap E_j = \emptyset\)</span> for <span class="math notranslate nohighlight">\(i \neq j\)</span>,</p>
<div class="math notranslate nohighlight">
\[P\left(\bigcup_{i=1}^{\infty} E_i\right) = \sum_{i=1}^{\infty} P(E_i)\]</div>
<p>If events cannot occur simultaneously (are mutually exclusive), the probability that at least one occurs equals the sum of their individual probabilities.</p>
<p><strong>In practice:</strong> For finite sample spaces (dice, coins, cards), we only need finite additivity. For continuous spaces (normal distribution, exponential distribution), the full countable additivity ensures consistency when dealing with limits and infinite sums.</p>
<p>From these three axioms, we can derive all familiar properties of probability:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(\emptyset) = 0\)</span> (the impossible event has probability zero)</p></li>
<li><p><span class="math notranslate nohighlight">\(0 \leq P(E) \leq 1\)</span> for any event <span class="math notranslate nohighlight">\(E\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(E^c) = 1 - P(E)\)</span> (complement rule)</p></li>
<li><p><span class="math notranslate nohighlight">\(P(A \cup B) = P(A) + P(B) - P(A \cap B)\)</span> (inclusion-exclusion)</p></li>
<li><p>If <span class="math notranslate nohighlight">\(A \subseteq B\)</span>, then <span class="math notranslate nohighlight">\(P(A) \leq P(B)\)</span> (monotonicity)</p></li>
</ul>
</section>
<section id="computational-perspective">
<h3>Computational Perspective<a class="headerlink" href="#computational-perspective" title="Link to this heading"></a></h3>
<p>In Python, we represent probability spaces through data structures and functions. For discrete sample spaces, we can use dictionaries; for continuous spaces, we rely on probability density functions from libraries like SciPy.</p>
<p><strong>Algorithm: Discrete Probability Space Construction</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input: Sample space Ω = {ω₁, ω₂, ..., ωₙ}, probability function p: Ω → [0,1]
Output: Valid probability space or error

1. Verify non-negativity: Check p(ωᵢ) ≥ 0 for all i
2. Verify normalization: Check Σᵢ p(ωᵢ) = 1 (within numerical tolerance)
3. If valid, return probability space representation
4. Else, raise ValueError with specific axiom violation
</pre></div>
</div>
<p><strong>Computational Complexity</strong>: O(n) for verification of a discrete probability space with n outcomes.</p>
</section>
<section id="python-implementation">
<h3>Python Implementation<a class="headerlink" href="#python-implementation" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Union</span>

<span class="k">class</span><span class="w"> </span><span class="nc">DiscreteProbabilitySpace</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Represents a discrete probability space satisfying Kolmogorov&#39;s axioms.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    outcomes : list</span>
<span class="sd">        Sample space Ω - list of possible outcomes</span>
<span class="sd">    probabilities : list or dict</span>
<span class="sd">        Probability mass function - either list of probabilities</span>
<span class="sd">        (same order as outcomes) or dict mapping outcomes to probabilities</span>
<span class="sd">    tol : float, optional</span>
<span class="sd">        Numerical tolerance for normalization check (default 1e-10)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outcomes</span><span class="p">,</span> <span class="n">probabilities</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">outcomes</span> <span class="o">=</span> <span class="n">outcomes</span>

        <span class="c1"># Convert to dictionary for convenient lookup</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pmf</span> <span class="o">=</span> <span class="n">probabilities</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pmf</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">outcomes</span><span class="p">,</span> <span class="n">probabilities</span><span class="p">))</span>

        <span class="c1"># Verify Kolmogorov&#39;s axioms</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_verify_axioms</span><span class="p">(</span><span class="n">tol</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_verify_axioms</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tol</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Verify non-negativity and normalization.&quot;&quot;&quot;</span>
        <span class="c1"># Axiom 1: Non-negativity</span>
        <span class="k">for</span> <span class="n">outcome</span><span class="p">,</span> <span class="n">prob</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">pmf</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">prob</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Axiom 1 violated: P(</span><span class="si">{</span><span class="n">outcome</span><span class="si">}</span><span class="s2">) = </span><span class="si">{</span><span class="n">prob</span><span class="si">}</span><span class="s2"> &lt; 0&quot;</span><span class="p">)</span>

        <span class="c1"># Axiom 2: Normalization</span>
        <span class="n">total_prob</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pmf</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">total_prob</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">tol</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Axiom 2 violated: Total probability = </span><span class="si">{</span><span class="n">total_prob</span><span class="si">}</span><span class="s2"> ≠ 1&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Axiom 3: For discrete finite spaces, additivity is satisfied by</span>
        <span class="c1"># construction when we compute event probabilities as sums over</span>
        <span class="c1"># disjoint outcomes.</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">event</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculate probability of an event.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        event : callable or iterable</span>
<span class="sd">            Either a function returning True for outcomes in event,</span>
<span class="sd">            or an iterable of outcomes in the event</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        float</span>
<span class="sd">            Probability of the event</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">callable</span><span class="p">(</span><span class="n">event</span><span class="p">):</span>
            <span class="c1"># Event defined by indicator function</span>
            <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pmf</span><span class="p">[</span><span class="n">outcome</span><span class="p">]</span> <span class="k">for</span> <span class="n">outcome</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">outcomes</span>
                      <span class="k">if</span> <span class="n">event</span><span class="p">(</span><span class="n">outcome</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Event defined as set of outcomes</span>
            <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pmf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">outcome</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">outcome</span> <span class="ow">in</span> <span class="n">event</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">complement_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">event</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate P(E^c) = 1 - P(E).&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>
</pre></div>
</div>
<div class="note admonition">
<p class="admonition-title">Example 💡: Fair Die Probability Space</p>
<p><strong>Given:</strong> A fair six-sided die with outcomes {1, 2, 3, 4, 5, 6}.</p>
<p><strong>Find:</strong> Verify Kolmogorov’s axioms and compute probabilities of compound events.</p>
<p><strong>Mathematical approach:</strong></p>
<p>The sample space is <span class="math notranslate nohighlight">\(\Omega = \{1, 2, 3, 4, 5, 6\}\)</span>. For a fair die, each outcome has equal probability:</p>
<div class="math notranslate nohighlight">
\[P(\{\omega\}) = \frac{1}{6} \quad \text{for each } \omega \in \Omega\]</div>
<p><strong>Verify Axiom 1</strong> (Non-negativity): <span class="math notranslate nohighlight">\(P(\{\omega\}) = 1/6 &gt; 0\)</span> ✓</p>
<p><strong>Verify Axiom 2</strong> (Normalization):</p>
<div class="math notranslate nohighlight">
\[P(\Omega) = \sum_{i=1}^{6} P(\{i\}) = 6 \times \frac{1}{6} = 1 \quad ✓\]</div>
<p><strong>Verify Axiom 3</strong> (Countable Additivity): For disjoint events <span class="math notranslate nohighlight">\(E_1 = \{2, 4, 6\}\)</span> (even) and <span class="math notranslate nohighlight">\(E_2 = \{1, 3, 5\}\)</span> (odd):</p>
<div class="math notranslate nohighlight">
\[P(E_1 \cup E_2) = P(\Omega) = 1 = \frac{3}{6} + \frac{3}{6} = P(E_1) + P(E_2) \quad ✓\]</div>
<p><strong>Python implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create fair die probability space</span>
<span class="n">die</span> <span class="o">=</span> <span class="n">DiscreteProbabilitySpace</span><span class="p">(</span>
    <span class="n">outcomes</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
    <span class="n">probabilities</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># Compute probability of rolling even</span>
<span class="n">even_prob</span> <span class="o">=</span> <span class="n">die</span><span class="o">.</span><span class="n">prob</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(even) = </span><span class="si">{</span><span class="n">even_prob</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># 0.5000</span>

<span class="c1"># Compute probability of rolling greater than 4</span>
<span class="n">greater_than_4</span> <span class="o">=</span> <span class="n">die</span><span class="o">.</span><span class="n">prob</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mi">4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(X &gt; 4) = </span><span class="si">{</span><span class="n">greater_than_4</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># 0.3333</span>

<span class="c1"># Verify complement rule: P(even) + P(odd) = 1</span>
<span class="n">odd_prob</span> <span class="o">=</span> <span class="n">die</span><span class="o">.</span><span class="n">complement_prob</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(odd) = </span><span class="si">{</span><span class="n">odd_prob</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># 0.5000</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(even) + P(odd) = </span><span class="si">{</span><span class="n">even_prob</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">odd_prob</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># 1.0000</span>
</pre></div>
</div>
<p><strong>Result:</strong> All axioms verified. P(even) = 0.5, P(X &gt; 4) = 0.3333, complement rule holds numerically.</p>
</div>
<div class="note admonition">
<p class="admonition-title">Why This Matters 💡</p>
<p>These axioms are <strong>interpretation-neutral</strong>—they specify how probabilities must behave mathematically without dictating what probability actually means. All major schools of statistical thought (frequentist, Bayesian, likelihoodist) accept these axioms, yet they fundamentally disagree about how to interpret probability values and conduct inference. The axioms provide a common mathematical language while leaving philosophical room for different interpretations.</p>
</div>
<div class="note admonition">
<p class="admonition-title">Why This Matters 💡</p>
<p>These axioms are <strong>interpretation-neutral</strong>—they specify how probabilities must behave mathematically without dictating what probability actually means. All major schools of statistical thought (frequentist, Bayesian, likelihoodist) accept these axioms, yet they fundamentally disagree about how to interpret probability values and conduct inference. The axioms provide a common mathematical language while leaving philosophical room for different interpretations.</p>
</div>
</section>
<section id="mathematical-preliminaries-random-variables-connecting-events-to-numbers">
<h3>Mathematical Preliminaries: Random Variables: Connecting Events to Numbers<a class="headerlink" href="#mathematical-preliminaries-random-variables-connecting-events-to-numbers" title="Link to this heading"></a></h3>
<p>While Kolmogorov’s axioms assign probabilities to events (subsets of the sample space), data science typically deals with numerical outcomes. A <strong>random variable</strong> provides this connection:</p>
<div class="definition admonition">
<p class="admonition-title">Definition: Random Variable</p>
<p>A <strong>random variable</strong> <span class="math notranslate nohighlight">\(X\)</span> is a function from the sample space to the real numbers:</p>
<div class="math notranslate nohighlight">
\[X: \Omega \to \mathbb{R}\]</div>
<p>For this course, we work with random variables without formal measure-theoretic details. Intuitively, <span class="math notranslate nohighlight">\(X\)</span> assigns a numerical value to each outcome <span class="math notranslate nohighlight">\(\omega \in \Omega\)</span>.</p>
</div>
<p><strong>Example</strong>: Consider flipping a coin twice.</p>
<ul class="simple">
<li><p>Sample space: <span class="math notranslate nohighlight">\(\Omega = \{HH, HT, TH, TT\}\)</span></p></li>
<li><p>Random variable <span class="math notranslate nohighlight">\(X\)</span> = “number of heads”</p></li>
<li><p>Mapping: <span class="math notranslate nohighlight">\(X(HH) = 2, X(HT) = 1, X(TH) = 1, X(TT) = 0\)</span></p></li>
</ul>
<p>The random variable <span class="math notranslate nohighlight">\(X\)</span> transforms events about outcomes (“got HT”) into events about numbers (<span class="math notranslate nohighlight">\(X = 1\)</span>). This allows us to write:</p>
<div class="math notranslate nohighlight">
\[P(X = 1) = P(\{HT, TH\}) = P(HT) + P(TH) = \frac{1}{4} + \frac{1}{4} = \frac{1}{2}\]</div>
<p><strong>Types of Random Variables</strong>:</p>
<ul class="simple">
<li><p><strong>Discrete</strong>: Takes countable values (e.g., <span class="math notranslate nohighlight">\(\{0, 1, 2, \ldots\}\)</span> or <span class="math notranslate nohighlight">\(\{0, 1\}\)</span>)</p></li>
<li><p><strong>Continuous</strong>: Takes uncountable values in an interval (e.g., <span class="math notranslate nohighlight">\([0, 1]\)</span> or <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>)</p></li>
</ul>
<p>The type determines which mathematical tools we use to describe the distribution.</p>
</section>
<section id="probability-mass-functions-pmf-discrete-distributions">
<h3>Probability Mass Functions (PMF): Discrete Distributions<a class="headerlink" href="#probability-mass-functions-pmf-discrete-distributions" title="Link to this heading"></a></h3>
<p>For discrete random variables, the <strong>probability mass function</strong> directly specifies the probability of each value.</p>
<div class="definition admonition">
<p class="admonition-title">Definition: Probability Mass Function (PMF)</p>
<p>For a discrete random variable <span class="math notranslate nohighlight">\(X\)</span>, the <strong>probability mass function</strong> is:</p>
<div class="math notranslate nohighlight">
\[p_X(x) = P(X = x)\]</div>
<p>The PMF must satisfy:</p>
<ol class="arabic simple">
<li><p><strong>Non-negativity</strong>: <span class="math notranslate nohighlight">\(p_X(x) \geq 0\)</span> for all <span class="math notranslate nohighlight">\(x\)</span></p></li>
<li><p><strong>Normalization</strong>: <span class="math notranslate nohighlight">\(\sum_{\text{all } x} p_X(x) = 1\)</span></p></li>
</ol>
</div>
<p><strong>Key properties</strong>:</p>
<ul class="simple">
<li><p>The PMF gives <em>exact</em> probabilities for discrete values</p></li>
<li><p><span class="math notranslate nohighlight">\(P(X \in A) = \sum_{x \in A} p_X(x)\)</span> for any set <span class="math notranslate nohighlight">\(A\)</span></p></li>
<li><p>The PMF inherits non-negativity and normalization from Kolmogorov’s axioms</p></li>
</ul>
<div class="note admonition">
<p class="admonition-title">Example 💡: Fair Die PMF</p>
<p><strong>Given:</strong> Roll a fair six-sided die. Let <span class="math notranslate nohighlight">\(X\)</span> = outcome.</p>
<p><strong>Find:</strong> The PMF and verify it satisfies the axioms.</p>
<p><strong>Solution:</strong></p>
<p>The sample space is <span class="math notranslate nohighlight">\(\Omega = \{1, 2, 3, 4, 5, 6\}\)</span>, and each outcome has probability <span class="math notranslate nohighlight">\(1/6\)</span> (fair die). The PMF is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}p_X(x) = \begin{cases}
\frac{1}{6} &amp; \text{if } x \in \{1, 2, 3, 4, 5, 6\} \\
0 &amp; \text{otherwise}
\end{cases}\end{split}\]</div>
<p><strong>Verify non-negativity</strong>: <span class="math notranslate nohighlight">\(p_X(x) = 1/6 &gt; 0\)</span> for all valid <span class="math notranslate nohighlight">\(x\)</span> ✓</p>
<p><strong>Verify normalization</strong>:</p>
<div class="math notranslate nohighlight">
\[\sum_{x=1}^{6} p_X(x) = 6 \times \frac{1}{6} = 1 \quad ✓\]</div>
<p><strong>Compute probabilities</strong>:</p>
<div class="math notranslate nohighlight">
\[P(X \text{ is even}) = P(X \in \{2, 4, 6\}) = p_X(2) + p_X(4) + p_X(6) = \frac{3}{6} = \frac{1}{2}\]</div>
<div class="math notranslate nohighlight">
\[P(X &gt; 4) = p_X(5) + p_X(6) = \frac{2}{6} = \frac{1}{3}\]</div>
<p><strong>Python implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Define the PMF</span>
<span class="n">outcomes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="n">pmf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">])</span>

<span class="c1"># Verify normalization</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sum of PMF: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pmf</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># Should be 1.0</span>

<span class="c1"># Compute probabilities</span>
<span class="n">prob_even</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pmf</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">]])</span>  <span class="c1"># indices for 2, 4, 6</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(X is even): </span><span class="si">{</span><span class="n">prob_even</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Visualize PMF</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">outcomes</span><span class="p">,</span> <span class="n">pmf</span><span class="p">,</span> <span class="n">basefmt</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Outcome (x)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;P(X = x)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;PMF of Fair Die&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Result:</strong> The PMF clearly shows equal probability mass on each outcome, characteristic of a uniform discrete distribution.</p>
</div>
</section>
<section id="probability-density-functions-pdf-continuous-distributions">
<h3>Probability Density Functions (PDF): Continuous Distributions<a class="headerlink" href="#probability-density-functions-pdf-continuous-distributions" title="Link to this heading"></a></h3>
<p>For continuous random variables, the probability of any <em>exact</em> value is zero: <span class="math notranslate nohighlight">\(P(X = x) = 0\)</span>. Instead, we work with probabilities of <em>intervals</em> using the <strong>probability density function</strong>.</p>
<div class="definition admonition">
<p class="admonition-title">Definition: Probability Density Function (PDF)</p>
<p>For a continuous random variable <span class="math notranslate nohighlight">\(X\)</span>, the <strong>probability density function</strong> <span class="math notranslate nohighlight">\(f_X(x)\)</span> satisfies:</p>
<div class="math notranslate nohighlight">
\[P(a \leq X \leq b) = \int_a^b f_X(x) \, dx\]</div>
<p>The PDF must satisfy:</p>
<ol class="arabic simple">
<li><p><strong>Non-negativity</strong>: <span class="math notranslate nohighlight">\(f_X(x) \geq 0\)</span> for all <span class="math notranslate nohighlight">\(x\)</span></p></li>
<li><p><strong>Normalization</strong>: <span class="math notranslate nohighlight">\(\int_{-\infty}^{\infty} f_X(x) \, dx = 1\)</span></p></li>
</ol>
</div>
<p><strong>Critical distinctions from PMF</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f_X(x)\)</span> is <strong>not</strong> a probability—it’s a <em>density</em></p></li>
<li><p><span class="math notranslate nohighlight">\(f_X(x)\)</span> can be greater than 1 (unlike PMF)</p></li>
<li><p><span class="math notranslate nohighlight">\(P(X = x) = 0\)</span> for any specific <span class="math notranslate nohighlight">\(x\)</span></p></li>
<li><p>Only <em>intervals</em> have non-zero probability</p></li>
</ul>
<p><strong>Intuition</strong>: The PDF describes how probability is “spread” over the real line. Think of it as probability per unit length:</p>
<div class="math notranslate nohighlight">
\[P(x &lt; X \leq x + dx) \approx f_X(x) \cdot dx \quad \text{for small } dx\]</div>
<div class="note admonition">
<p class="admonition-title">Example 💡: Uniform(0, 1) PDF</p>
<p><strong>Given:</strong> A random variable <span class="math notranslate nohighlight">\(X\)</span> uniformly distributed on <span class="math notranslate nohighlight">\([0, 1]\)</span>.</p>
<p><strong>Find:</strong> The PDF, verify properties, and compute probabilities.</p>
<p><strong>Solution:</strong></p>
<p>For uniform distribution on <span class="math notranslate nohighlight">\([0, 1]\)</span>, the PDF is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}f_X(x) = \begin{cases}
1 &amp; \text{if } 0 \leq x \leq 1 \\
0 &amp; \text{otherwise}
\end{cases}\end{split}\]</div>
<p><strong>Verify non-negativity</strong>: <span class="math notranslate nohighlight">\(f_X(x) = 1 \geq 0\)</span> on support ✓</p>
<p><strong>Verify normalization</strong>:</p>
<div class="math notranslate nohighlight">
\[\int_{-\infty}^{\infty} f_X(x) \, dx = \int_0^1 1 \, dx = 1 \quad ✓\]</div>
<p><strong>Note</strong>: <span class="math notranslate nohighlight">\(f_X(0.5) = 1\)</span>, which is <strong>not a probability</strong>. It means probability density is 1 unit per unit length.</p>
<p><strong>Compute probabilities</strong>:</p>
<div class="math notranslate nohighlight">
\[P(0.2 \leq X \leq 0.7) = \int_{0.2}^{0.7} 1 \, dx = 0.7 - 0.2 = 0.5\]</div>
<div class="math notranslate nohighlight">
\[P(X = 0.5) = \int_{0.5}^{0.5} 1 \, dx = 0 \quad \text{(exact point has zero probability)}\]</div>
<div class="math notranslate nohighlight">
\[P(X \leq 0.3) = \int_0^{0.3} 1 \, dx = 0.3\]</div>
<p><strong>Python implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Define PDF</span>
<span class="k">def</span><span class="w"> </span><span class="nf">uniform_pdf</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">((</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">),</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>

<span class="c1"># Visualize PDF</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">pdf_values</span> <span class="o">=</span> <span class="n">uniform_pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pdf_values</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;PDF&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">pdf_values</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Highlight P(0.2 ≤ X ≤ 0.7)</span>
<span class="n">x_interval</span> <span class="o">=</span> <span class="n">x</span><span class="p">[(</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="mf">0.2</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="mf">0.7</span><span class="p">)]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_interval</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">uniform_pdf</span><span class="p">(</span><span class="n">x_interval</span><span class="p">),</span>
                 <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;P(0.2 ≤ X ≤ 0.7) = 0.5&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;f(x)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;PDF of Uniform(0, 1)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Verify normalization numerically</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">integrate</span>
<span class="n">integral</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">integrate</span><span class="o">.</span><span class="n">quad</span><span class="p">(</span><span class="n">uniform_pdf</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Integral of PDF: </span><span class="si">{</span><span class="n">integral</span><span class="si">:</span><span class="s2">.10f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># Should be 1.0</span>

<span class="c1"># Compute probability using scipy</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="n">uniform_dist</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">prob</span> <span class="o">=</span> <span class="n">uniform_dist</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mf">0.7</span><span class="p">)</span> <span class="o">-</span> <span class="n">uniform_dist</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(0.2 ≤ X ≤ 0.7): </span><span class="si">{</span><span class="n">prob</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Result:</strong> The PDF is flat at height 1 over <span class="math notranslate nohighlight">\([0, 1]\)</span>, representing equal density everywhere. The shaded area equals the probability.</p>
</div>
<div class="warning admonition">
<p class="admonition-title">Common Misconception ⚠️</p>
<p><strong>Wrong</strong>: “If <span class="math notranslate nohighlight">\(f_X(5) = 2.3\)</span>, then <span class="math notranslate nohighlight">\(P(X = 5) = 2.3\)</span>”</p>
<p><strong>Right</strong>: “<span class="math notranslate nohighlight">\(f_X(5) = 2.3\)</span> means the probability <em>density</em> at <span class="math notranslate nohighlight">\(x = 5\)</span> is 2.3. The actual probability <span class="math notranslate nohighlight">\(P(X = 5) = 0\)</span> for any continuous distribution. We can only compute <span class="math notranslate nohighlight">\(P(a \leq X \leq b)\)</span> for intervals.”</p>
<p><strong>Physical analogy</strong>: Think of PDF like mass density. A steel beam might have density 7850 kg/m³ at a point, but the mass of an <em>exact</em> point is zero. Only intervals have mass, just as only intervals have probability.</p>
</div>
<figure class="align-center" id="id12">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/Part1/ch1_1_pmf_vs_pdf.png"><img alt="Side-by-side comparison of discrete PMF as bar chart for Binomial distribution and continuous PDF as smooth curve for Normal distribution" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/Part1/ch1_1_pmf_vs_pdf.png" style="width: 90%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 1.2 </span><span class="caption-text"><strong>Left:</strong> Probability Mass Function (PMF) for Binomial(10, 0.3)—each bar represents the exact probability <span class="math notranslate nohighlight">\(P(X = k)\)</span>. <strong>Right:</strong> Probability Density Function (PDF) for Normal(0, 1)—the shaded area represents <span class="math notranslate nohighlight">\(P(-1 \leq X \leq 1) \approx 0.683\)</span>. Note that PDF values are densities, not probabilities.</span><a class="headerlink" href="#id12" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="cumulative-distribution-functions-cdf-universal-description">
<h3>Cumulative Distribution Functions (CDF): Universal Description<a class="headerlink" href="#cumulative-distribution-functions-cdf-universal-description" title="Link to this heading"></a></h3>
<p>The <strong>cumulative distribution function</strong> works for <em>both</em> discrete and continuous random variables, providing a unified framework.</p>
<div class="definition admonition">
<p class="admonition-title">Definition: Cumulative Distribution Function (CDF)</p>
<p>For any random variable <span class="math notranslate nohighlight">\(X\)</span>, the <strong>cumulative distribution function</strong> is:</p>
<div class="math notranslate nohighlight">
\[F_X(x) = P(X \leq x)\]</div>
<p>The CDF must satisfy:</p>
<ol class="arabic simple">
<li><p><strong>Monotonicity</strong>: If <span class="math notranslate nohighlight">\(x_1 \leq x_2\)</span>, then <span class="math notranslate nohighlight">\(F_X(x_1) \leq F_X(x_2)\)</span></p></li>
<li><p><strong>Right-continuous</strong>: <span class="math notranslate nohighlight">\(\lim_{h \to 0^+} F_X(x + h) = F_X(x)\)</span></p></li>
<li><p><strong>Limits</strong>: <span class="math notranslate nohighlight">\(\lim_{x \to -\infty} F_X(x) = 0\)</span> and <span class="math notranslate nohighlight">\(\lim_{x \to \infty} F_X(x) = 1\)</span></p></li>
</ol>
</div>
<figure class="align-center" id="id13">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/Part1/ch1_1_cdf_comparison.png"><img alt="Comparison of discrete CDF as step function for fair die and continuous CDF as smooth S-curve for standard normal distribution" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/Part1/ch1_1_cdf_comparison.png" style="width: 90%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 1.3 </span><span class="caption-text"><strong>Left:</strong> CDF for a discrete distribution (fair die) is a step function with jumps at each outcome. Open circles indicate left-limits; closed circles indicate the CDF value. <strong>Right:</strong> CDF for a continuous distribution (standard normal) is smooth and strictly increasing, with quartiles marked.</span><a class="headerlink" href="#id13" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Relationships to PMF and PDF</strong>:</p>
<p>For <strong>discrete</strong> <span class="math notranslate nohighlight">\(X\)</span>:</p>
<div class="math notranslate nohighlight">
\[F_X(x) = \sum_{x_i \leq x} p_X(x_i)\]</div>
<p>For <strong>continuous</strong> <span class="math notranslate nohighlight">\(X\)</span>:</p>
<div class="math notranslate nohighlight">
\[F_X(x) = \int_{-\infty}^{x} f_X(t) \, dt\]</div>
<p>And conversely (for continuous):</p>
<div class="math notranslate nohighlight">
\[f_X(x) = \frac{d}{dx} F_X(x)\]</div>
<p><strong>Computing probabilities from CDF</strong>:</p>
<div class="math notranslate nohighlight">
\[P(a &lt; X \leq b) = F_X(b) - F_X(a)\]</div>
<div class="math notranslate nohighlight">
\[P(X &gt; a) = 1 - F_X(a)\]</div>
<div class="math notranslate nohighlight">
\[P(X = a) = F_X(a) - \lim_{h \to 0^+} F_X(a - h) \quad \text{(discrete only)}\]</div>
<div class="note admonition">
<p class="admonition-title">Example 💡: CDF for Fair Die</p>
<p><strong>Given:</strong> Fair die with <span class="math notranslate nohighlight">\(X \in \{1, 2, 3, 4, 5, 6\}\)</span>, each with probability <span class="math notranslate nohighlight">\(1/6\)</span>.</p>
<p><strong>Find:</strong> The CDF and use it to compute probabilities.</p>
<p><strong>Solution:</strong></p>
<p>Build the CDF by accumulating probabilities:</p>
<div class="math notranslate nohighlight">
\[\begin{split}F_X(x) = P(X \leq x) = \begin{cases}
0 &amp; \text{if } x &lt; 1 \\
1/6 &amp; \text{if } 1 \leq x &lt; 2 \\
2/6 &amp; \text{if } 2 \leq x &lt; 3 \\
3/6 &amp; \text{if } 3 \leq x &lt; 4 \\
4/6 &amp; \text{if } 4 \leq x &lt; 5 \\
5/6 &amp; \text{if } 5 \leq x &lt; 6 \\
1 &amp; \text{if } x \geq 6
\end{cases}\end{split}\]</div>
<p>The CDF is a <strong>step function</strong> for discrete random variables, with jumps at each possible value.</p>
<p><strong>Compute probabilities</strong>:</p>
<div class="math notranslate nohighlight">
\[P(X \leq 4) = F_X(4) = \frac{4}{6} = \frac{2}{3}\]</div>
<div class="math notranslate nohighlight">
\[P(2 &lt; X \leq 5) = F_X(5) - F_X(2) = \frac{5}{6} - \frac{2}{6} = \frac{1}{2}\]</div>
<div class="math notranslate nohighlight">
\[P(X = 3) = F_X(3) - F_X(3^-) = \frac{3}{6} - \frac{2}{6} = \frac{1}{6}\]</div>
<p><strong>Python implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Define the die distribution</span>
<span class="n">outcomes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">pmf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span> <span class="o">/</span> <span class="mi">6</span>

<span class="c1"># Compute CDF manually</span>
<span class="k">def</span><span class="w"> </span><span class="nf">compute_cdf</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pmf</span><span class="p">[</span><span class="n">outcomes</span> <span class="o">&lt;=</span> <span class="n">x</span><span class="p">])</span>

<span class="c1"># Vectorize for plotting</span>
<span class="n">compute_cdf_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vectorize</span><span class="p">(</span><span class="n">compute_cdf</span><span class="p">)</span>

<span class="c1"># Plot PMF and CDF</span>
<span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># PMF (stick plot)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">outcomes</span><span class="p">,</span> <span class="n">pmf</span><span class="p">,</span> <span class="n">basefmt</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;P(X = x)&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;PMF of Fair Die&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">)</span>

<span class="c1"># CDF (step function)</span>
<span class="n">x_plot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">cdf_plot</span> <span class="o">=</span> <span class="n">compute_cdf_vec</span><span class="p">(</span><span class="n">x_plot</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">cdf_plot</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">outcomes</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pmf</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;F(x) = P(X ≤ x)&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;CDF of Fair Die&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Using SciPy</span>
<span class="n">die_dist</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>  <span class="c1"># discrete uniform on {1,2,3,4,5,6}</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(X ≤ 4): </span><span class="si">{</span><span class="n">die_dist</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(2 &lt; X ≤ 5): </span><span class="si">{</span><span class="n">die_dist</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">die_dist</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Result:</strong> The CDF shows characteristic “staircase” pattern for discrete distributions, jumping at each possible value.</p>
</div>
<div class="note admonition">
<p class="admonition-title">Example 💡: CDF for Uniform(0, 1)</p>
<p><strong>Given:</strong> <span class="math notranslate nohighlight">\(X \sim \text{Uniform}(0, 1)\)</span> with PDF <span class="math notranslate nohighlight">\(f_X(x) = 1\)</span> on <span class="math notranslate nohighlight">\([0, 1]\)</span>.</p>
<p><strong>Find:</strong> Derive the CDF.</p>
<p><strong>Solution:</strong></p>
<p>For <span class="math notranslate nohighlight">\(x &lt; 0\)</span>: No probability mass to the left, so <span class="math notranslate nohighlight">\(F_X(x) = 0\)</span>.</p>
<p>For <span class="math notranslate nohighlight">\(0 \leq x \leq 1\)</span>:</p>
<div class="math notranslate nohighlight">
\[F_X(x) = \int_{-\infty}^{x} f_X(t) \, dt = \int_0^x 1 \, dt = x\]</div>
<p>For <span class="math notranslate nohighlight">\(x &gt; 1\)</span>: All probability mass is to the left, so <span class="math notranslate nohighlight">\(F_X(x) = 1\)</span>.</p>
<p>Therefore:</p>
<div class="math notranslate nohighlight">
\[\begin{split}F_X(x) = \begin{cases}
0 &amp; \text{if } x &lt; 0 \\
x &amp; \text{if } 0 \leq x \leq 1 \\
1 &amp; \text{if } x &gt; 1
\end{cases}\end{split}\]</div>
<p>The CDF is a <strong>continuous function</strong> for continuous random variables (no jumps).</p>
<p><strong>Verify by differentiation</strong>:</p>
<div class="math notranslate nohighlight">
\[f_X(x) = \frac{d}{dx} F_X(x) = \frac{d}{dx}(x) = 1 \quad \text{for } 0 \leq x \leq 1 \quad ✓\]</div>
<p><strong>Python implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Define PDF and CDF</span>
<span class="k">def</span><span class="w"> </span><span class="nf">uniform_pdf</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">((</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">),</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">uniform_cdf</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>

<span class="c1"># Plot</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># PDF</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">uniform_pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">uniform_pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;f(x)&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;PDF of Uniform(0, 1)&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">)</span>

<span class="c1"># CDF</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">uniform_cdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;F(x) = P(X ≤ x)&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;CDF of Uniform(0, 1)&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>

<span class="c1"># Add specific probability</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.7</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="p">[</span><span class="n">uniform_cdf</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">uniform_cdf</span><span class="p">(</span><span class="n">b</span><span class="p">)],</span> <span class="s1">&#39;ro&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;P(</span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s1"> &lt; X ≤ </span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s1">) = </span><span class="si">{</span><span class="n">b</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">a</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
             <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">0.45</span><span class="p">,</span> <span class="mf">0.45</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Using SciPy</span>
<span class="n">uniform_dist</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;F(0.5): </span><span class="si">{</span><span class="n">uniform_dist</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(0.2 &lt; X ≤ 0.7): </span><span class="si">{</span><span class="n">uniform_dist</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mf">0.7</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">uniform_dist</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Result:</strong> The CDF is a straight line from (0, 0) to (1, 1), showing linear accumulation of probability.</p>
</div>
</section>
<section id="quantile-function-inverse-cdf">
<h3>Quantile Function (Inverse CDF)<a class="headerlink" href="#quantile-function-inverse-cdf" title="Link to this heading"></a></h3>
<p>The <strong>quantile function</strong> (or <strong>inverse CDF</strong>) is crucial for random number generation and statistical inference.</p>
<div class="definition admonition">
<p class="admonition-title">Definition: Quantile Function</p>
<p>For a random variable <span class="math notranslate nohighlight">\(X\)</span> with CDF <span class="math notranslate nohighlight">\(F_X\)</span>, the <strong>quantile function</strong> (or <strong>percent point function</strong>) is:</p>
<div class="math notranslate nohighlight">
\[F_X^{-1}(p) = \inf\{x : F_X(x) \geq p\} \quad \text{for } 0 &lt; p &lt; 1\]</div>
<p>This gives the value <span class="math notranslate nohighlight">\(x\)</span> such that <span class="math notranslate nohighlight">\(P(X \leq x) = p\)</span>.</p>
</div>
<p><strong>Common quantiles</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(F_X^{-1}(0.5)\)</span> = <strong>median</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(F_X^{-1}(0.25)\)</span>, <span class="math notranslate nohighlight">\(F_X^{-1}(0.75)\)</span> = <strong>quartiles</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(F_X^{-1}(0.025)\)</span>, <span class="math notranslate nohighlight">\(F_X^{-1}(0.975)\)</span> = boundaries of central 95%</p></li>
</ul>
<p><strong>Key property</strong> (Probability Integral Transform): If <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0, 1)\)</span>, then:</p>
<div class="math notranslate nohighlight">
\[X = F_X^{-1}(U) \text{ has CDF } F_X\]</div>
<p>This is the foundation for the <strong>inverse transform method</strong> of random number generation (Chapter 2).</p>
<div class="note admonition">
<p class="admonition-title">Example 💡: Quantiles for Exponential Distribution</p>
<p><strong>Given:</strong> <span class="math notranslate nohighlight">\(X \sim \text{Exponential}(\lambda)\)</span> with CDF <span class="math notranslate nohighlight">\(F_X(x) = 1 - e^{-\lambda x}\)</span> for <span class="math notranslate nohighlight">\(x \geq 0\)</span>.</p>
<p><strong>Find:</strong> The quantile function and specific quantiles.</p>
<p><strong>Solution:</strong></p>
<p>To find <span class="math notranslate nohighlight">\(F_X^{-1}(p)\)</span>, solve <span class="math notranslate nohighlight">\(F_X(x) = p\)</span>:</p>
<div class="math notranslate nohighlight">
\[1 - e^{-\lambda x} = p\]</div>
<div class="math notranslate nohighlight">
\[e^{-\lambda x} = 1 - p\]</div>
<div class="math notranslate nohighlight">
\[-\lambda x = \ln(1 - p)\]</div>
<div class="math notranslate nohighlight">
\[x = -\frac{\ln(1 - p)}{\lambda}\]</div>
<p>Therefore, the quantile function is:</p>
<div class="math notranslate nohighlight">
\[F_X^{-1}(p) = -\frac{\ln(1 - p)}{\lambda}\]</div>
<p><strong>Compute specific quantiles</strong> for <span class="math notranslate nohighlight">\(\lambda = 1\)</span>:</p>
<ul class="simple">
<li><p>Median: <span class="math notranslate nohighlight">\(F_X^{-1}(0.5) = -\ln(0.5) = \ln(2) \approx 0.693\)</span></p></li>
<li><p>First quartile: <span class="math notranslate nohighlight">\(F_X^{-1}(0.25) = -\ln(0.75) \approx 0.288\)</span></p></li>
<li><p>95th percentile: <span class="math notranslate nohighlight">\(F_X^{-1}(0.95) = -\ln(0.05) \approx 2.996\)</span></p></li>
</ul>
<p><strong>Python implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="n">lam</span> <span class="o">=</span> <span class="mf">1.0</span>  <span class="c1"># rate parameter</span>
<span class="n">exp_dist</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">expon</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">lam</span><span class="p">)</span>

<span class="c1"># Plot CDF and quantile function</span>
<span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># CDF</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">cdf</span> <span class="o">=</span> <span class="n">exp_dist</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">cdf</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;F(x)&#39;</span><span class="p">)</span>

<span class="c1"># Mark quantiles</span>
<span class="n">quantiles_p</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">]</span>
<span class="n">quantiles_x</span> <span class="o">=</span> <span class="p">[</span><span class="n">exp_dist</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">quantiles_p</span><span class="p">]</span>

<span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">q</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">quantiles_p</span><span class="p">,</span> <span class="n">quantiles_x</span><span class="p">):</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">q</span><span class="p">,</span> <span class="n">q</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">p</span><span class="p">],</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">q</span><span class="p">],</span> <span class="p">[</span><span class="n">p</span><span class="p">,</span> <span class="n">p</span><span class="p">],</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="s1">&#39;ro&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;F(x) = P(X ≤ x)&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;CDF of Exponential(λ=</span><span class="si">{</span><span class="n">lam</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">)</span>

<span class="c1"># Quantile function (inverse CDF)</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">quantiles</span> <span class="o">=</span> <span class="n">exp_dist</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">quantiles</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;F⁻¹(p)&#39;</span><span class="p">)</span>

<span class="c1"># Mark same quantiles</span>
<span class="k">for</span> <span class="n">p_val</span><span class="p">,</span> <span class="n">q_val</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">quantiles_p</span><span class="p">,</span> <span class="n">quantiles_x</span><span class="p">):</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">p_val</span><span class="p">,</span> <span class="n">p_val</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">q_val</span><span class="p">],</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">p_val</span><span class="p">],</span> <span class="p">[</span><span class="n">q_val</span><span class="p">,</span> <span class="n">q_val</span><span class="p">],</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p_val</span><span class="p">,</span> <span class="n">q_val</span><span class="p">,</span> <span class="s1">&#39;ro&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">p_val</span><span class="p">,</span> <span class="n">q_val</span> <span class="o">+</span> <span class="mf">0.2</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">q_val</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>

<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;p (probability)&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;F⁻¹(p) = x&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Quantile Function of Exponential(λ=</span><span class="si">{</span><span class="n">lam</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Print quantiles</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Exponential(λ=</span><span class="si">{</span><span class="n">lam</span><span class="si">}</span><span class="s2">) Quantiles:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;25th percentile (Q1): </span><span class="si">{</span><span class="n">exp_dist</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.25</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;50th percentile (median): </span><span class="si">{</span><span class="n">exp_dist</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;75th percentile (Q3): </span><span class="si">{</span><span class="n">exp_dist</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.75</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95th percentile: </span><span class="si">{</span><span class="n">exp_dist</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.95</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Result:</strong> The quantile function is the “inverse” of the CDF—input a probability, get the corresponding value.</p>
</div>
</section>
<section id="relationships-and-conversions">
<h3>Relationships and Conversions<a class="headerlink" href="#relationships-and-conversions" title="Link to this heading"></a></h3>
<p>The following table summarizes how to convert between PMF/PDF, CDF, and quantile function:</p>
<table class="docutils align-default" id="id14">
<caption><span class="caption-number">Table 1.1 </span><span class="caption-text">Converting Between Distribution Functions</span><a class="headerlink" href="#id14" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 40.0%" />
<col style="width: 40.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><strong>Given</strong></p></th>
<th class="head"><p><strong>Discrete: Compute PMF</strong></p></th>
<th class="head"><p><strong>Continuous: Compute PDF</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>PMF/PDF</strong></p></td>
<td><p>Identity: <span class="math notranslate nohighlight">\(p_X(x)\)</span></p></td>
<td><p>Identity: <span class="math notranslate nohighlight">\(f_X(x)\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><strong>CDF</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(p_X(x) = F_X(x) - \lim_{h \to 0^+} F_X(x-h)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(f_X(x) = \frac{d}{dx} F_X(x)\)</span></p></td>
</tr>
<tr class="row-even"><td><p><strong>Quantile</strong></p></td>
<td><p>Not directly possible</p></td>
<td><p>Not directly possible</p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default" id="id15">
<caption><span class="caption-number">Table 1.2 </span><span class="caption-text">Computing CDF and Quantile Function</span><a class="headerlink" href="#id15" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 40.0%" />
<col style="width: 40.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><strong>Given</strong></p></th>
<th class="head"><p><strong>Compute CDF</strong></p></th>
<th class="head"><p><strong>Compute Quantile</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>PMF</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(F_X(x) = \sum_{x_i \leq x} p_X(x_i)\)</span></p></td>
<td><p>Solve <span class="math notranslate nohighlight">\(F_X(x) = p\)</span> for <span class="math notranslate nohighlight">\(x\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><strong>PDF</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(F_X(x) = \int_{-\infty}^x f_X(t) \, dt\)</span></p></td>
<td><p>Solve <span class="math notranslate nohighlight">\(F_X(x) = p\)</span> for <span class="math notranslate nohighlight">\(x\)</span></p></td>
</tr>
<tr class="row-even"><td><p><strong>CDF</strong></p></td>
<td><p>Identity: <span class="math notranslate nohighlight">\(F_X(x)\)</span></p></td>
<td><p>Solve <span class="math notranslate nohighlight">\(F_X(x) = p\)</span> for <span class="math notranslate nohighlight">\(x\)</span></p></td>
</tr>
</tbody>
</table>
</section>
<section id="mathematical-preliminaries-limits-and-convergence">
<h3>Mathematical Preliminaries: Limits and Convergence<a class="headerlink" href="#mathematical-preliminaries-limits-and-convergence" title="Link to this heading"></a></h3>
<p>Before exploring different interpretations of probability, we review essential mathematical concepts that will appear throughout this chapter and course. These concepts are especially important for understanding the frequentist interpretation, which relies on limiting behavior.</p>
<p><strong>Limit Notation</strong></p>
<p>We use standard limit notation:</p>
<div class="math notranslate nohighlight">
\[\lim_{n \to \infty} a_n = L\]</div>
<p>means “as <span class="math notranslate nohighlight">\(n\)</span> grows without bound, <span class="math notranslate nohighlight">\(a_n\)</span> approaches <span class="math notranslate nohighlight">\(L\)</span>.”</p>
<p><strong>Asymptotic Notation (Deterministic Sequences)</strong></p>
<p>For comparing rates of growth of deterministic sequences, we use Bachmann–Landau notation:</p>
<ul>
<li><p><strong>Big-O</strong> (upper bound): <span class="math notranslate nohighlight">\(f(n) = O(g(n))\)</span> means <span class="math notranslate nohighlight">\(|f(n)| \leq C|g(n)|\)</span> for some constant <span class="math notranslate nohighlight">\(C &gt; 0\)</span> and sufficiently large <span class="math notranslate nohighlight">\(n\)</span>.</p>
<p><em>Example</em>: <span class="math notranslate nohighlight">\(3n^2 + 5n + 7 = O(n^2)\)</span> because the <span class="math notranslate nohighlight">\(n^2\)</span> term dominates.</p>
</li>
<li><p><strong>Big-Theta</strong> (tight bound): <span class="math notranslate nohighlight">\(f(n) = \Theta(g(n))\)</span> means both <span class="math notranslate nohighlight">\(f(n) = O(g(n))\)</span> and <span class="math notranslate nohighlight">\(g(n) = O(f(n))\)</span>.</p>
<p>Equivalently, for sufficiently large <span class="math notranslate nohighlight">\(n\)</span>,</p>
<div class="math notranslate nohighlight">
\[c \, g(n) \le |f(n)| \le C \, g(n)\]</div>
<p>for some constants <span class="math notranslate nohighlight">\(c, C &gt; 0\)</span>.</p>
<p><em>Example</em>: <span class="math notranslate nohighlight">\(3n^2 + 5n = \Theta(n^2)\)</span>.</p>
</li>
<li><p><strong>Little-o</strong> (strict upper bound): <span class="math notranslate nohighlight">\(f(n) = o(g(n))\)</span> means</p>
<div class="math notranslate nohighlight">
\[\lim_{n \to \infty} \frac{f(n)}{g(n)} = 0.\]</div>
<p><em>Example</em>: <span class="math notranslate nohighlight">\(n = o(n^2)\)</span>.</p>
</li>
<li><p><strong>Asymptotic equivalence</strong>: <span class="math notranslate nohighlight">\(f(n) \sim g(n)\)</span> means</p>
<div class="math notranslate nohighlight">
\[\lim_{n \to \infty} \frac{f(n)}{g(n)} = 1.\]</div>
<p><em>Example</em>: <span class="math notranslate nohighlight">\(n^2 + n \sim n^2\)</span> as <span class="math notranslate nohighlight">\(n \to \infty\)</span>.</p>
</li>
</ul>
<p><strong>Relationships</strong></p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(f = o(g)\)</span>, then <span class="math notranslate nohighlight">\(f = O(g)\)</span> (little-o is stronger than big-O).</p></li>
<li><p>If <span class="math notranslate nohighlight">\(f \sim g\)</span> and <span class="math notranslate nohighlight">\(g\)</span> is eventually positive, then <span class="math notranslate nohighlight">\(f = \Theta(g)\)</span>.</p></li>
<li><p>Common ordering: <span class="math notranslate nohighlight">\(1 \ll \log n \ll \sqrt{n} \ll n \ll n \log n \ll n^2 \ll e^n \ll n!\)</span></p></li>
</ul>
<div class="note admonition">
<p class="admonition-title">Example: Asymptotic Analysis 📐</p>
<p>Consider estimating a population mean <span class="math notranslate nohighlight">\(\mu\)</span> with sample mean <span class="math notranslate nohighlight">\(\bar{X}_n\)</span>:</p>
<ul class="simple">
<li><p><strong>Standard error</strong>: <span class="math notranslate nohighlight">\(\text{SE}(\bar{X}_n) = \sigma/\sqrt{n} = \Theta(n^{-1/2})\)</span>.</p></li>
<li><p><strong>Bias</strong>: <span class="math notranslate nohighlight">\(\mathbb{E}[\bar{X}_n] - \mu = 0\)</span> (exactly zero).</p></li>
<li><p><strong>MSE</strong>: <span class="math notranslate nohighlight">\(\text{MSE}(\bar{X}_n) = \sigma^2/n = \Theta(n^{-1})\)</span>.</p></li>
</ul>
<p>The standard error decreases at rate <span class="math notranslate nohighlight">\(n^{-1/2}\)</span>, which is slower than <span class="math notranslate nohighlight">\(n^{-1}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac{1}{n} = o\!\left(\frac{1}{\sqrt{n}}\right)
\quad \text{since} \quad
\lim_{n \to \infty} \frac{1/n}{1/\sqrt{n}} = \lim_{n \to \infty} \frac{1}{\sqrt{n}} = 0.\]</div>
<p>Therefore MSE decays faster than SE.</p>
</div>
<p><strong>Probabilistic Order Notation (Stochastic Sequences)</strong></p>
<p>When working with random sequences, we need probabilistic versions of asymptotic notation. We define convergence in probability formally in the next subsection, but introduce the notation here:</p>
<ul>
<li><p><strong>O_p notation</strong>: <span class="math notranslate nohighlight">\(X_n = O_p(a_n)\)</span> means <span class="math notranslate nohighlight">\(\{|X_n|/a_n\}\)</span> is <strong>tight</strong> (bounded in probability).</p>
<p><strong>Precise definition</strong>: For every <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span>, there exists a constant <span class="math notranslate nohighlight">\(M &gt; 0\)</span> such that</p>
<div class="math notranslate nohighlight">
\[P\left(\frac{|X_n|}{a_n} &gt; M\right) &lt; \epsilon \quad \text{for all } n \text{ sufficiently large}.\]</div>
<p><em>Intuition</em>: The ratio <span class="math notranslate nohighlight">\(|X_n|/a_n\)</span> doesn’t explode—its probability mass doesn’t escape to infinity.</p>
<p><em>Example</em>: If <span class="math notranslate nohighlight">\(X_n \sim \mathcal{N}(0, 1/n)\)</span>, then <span class="math notranslate nohighlight">\(X_n = O_p(n^{-1/2})\)</span> because <span class="math notranslate nohighlight">\(\sqrt{n} X_n \sim \mathcal{N}(0,1)\)</span> has fixed distribution.</p>
</li>
<li><p><strong>o_p notation</strong>: <span class="math notranslate nohighlight">\(X_n = o_p(a_n)\)</span> means <span class="math notranslate nohighlight">\(|X_n|/a_n\)</span> converges in probability to zero.</p>
<p><strong>Precise definition</strong>:</p>
<div class="math notranslate nohighlight">
\[\lim_{n \to \infty} P\left(\frac{|X_n|}{a_n} &gt; \epsilon\right) = 0 \quad \text{for every } \epsilon &gt; 0.\]</div>
<p><em>Intuition</em>: The ratio <span class="math notranslate nohighlight">\(|X_n|/a_n\)</span> vanishes in probability.</p>
<p><em>Example</em>: If <span class="math notranslate nohighlight">\(\bar{X}_n\)</span> is the sample mean with mean <span class="math notranslate nohighlight">\(\mu\)</span>, then <span class="math notranslate nohighlight">\(\bar{X}_n - \mu = o_p(1)\)</span> by the Law of Large Numbers (below).</p>
</li>
</ul>
<p><strong>Relationship</strong>: <span class="math notranslate nohighlight">\(X_n = o_p(a_n) \Rightarrow X_n = O_p(a_n)\)</span> (just as <span class="math notranslate nohighlight">\(o\)</span> implies <span class="math notranslate nohighlight">\(O\)</span>).</p>
<p><strong>Why “bounded in probability”?</strong></p>
<p>Tightness means probability mass doesn’t leak to infinity. We can find <span class="math notranslate nohighlight">\(M\)</span> such that <span class="math notranslate nohighlight">\(|X_n|/a_n\)</span> stays below <span class="math notranslate nohighlight">\(M\)</span> with probability at least <span class="math notranslate nohighlight">\(1 - \epsilon\)</span>, no matter how large <span class="math notranslate nohighlight">\(n\)</span> gets.</p>
<p><strong>Contrast</strong>:
- <strong>Deterministic</strong> <span class="math notranslate nohighlight">\(O\)</span>: Hard bound <span class="math notranslate nohighlight">\(|f(n)| \leq C|g(n)|\)</span> for all large <span class="math notranslate nohighlight">\(n\)</span>
- <strong>Stochastic</strong> <span class="math notranslate nohighlight">\(O_p\)</span>: Soft bound <span class="math notranslate nohighlight">\(|X_n| \leq M \cdot a_n\)</span> with probability <span class="math notranslate nohighlight">\(\geq 1 - \epsilon\)</span></p>
<div class="note admonition">
<p class="admonition-title">Example: Monte Carlo Error 💡</p>
<p>Consider estimating <span class="math notranslate nohighlight">\(\theta = \mathbb{E}[g(X)]\)</span> using <span class="math notranslate nohighlight">\(\hat{\theta}_n = \frac{1}{n}\sum_{i=1}^n g(X_i)\)</span>.</p>
<p>By the Central Limit Theorem (below), if <span class="math notranslate nohighlight">\(\text{Var}(g(X)) = \sigma^2 &lt; \infty\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac{\sqrt{n}(\hat{\theta}_n - \theta)}{\sigma} \xrightarrow{d} \mathcal{N}(0,1)\]</div>
<p>This means <span class="math notranslate nohighlight">\(\sqrt{n}(\hat{\theta}_n - \theta) = O_p(1)\)</span>, so <span class="math notranslate nohighlight">\(\hat{\theta}_n - \theta = O_p(n^{-1/2})\)</span>.</p>
<p>We write <span class="math notranslate nohighlight">\(O_p(n^{-1/2})\)</span>, <strong>not</strong> <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span>, because:</p>
<ul class="simple">
<li><p>Error is <strong>random</strong>, not deterministic</p></li>
<li><p>Cannot bound by <span class="math notranslate nohighlight">\(C/\sqrt{n}\)</span> for all realizations</p></li>
<li><p>Can only bound probabilistically</p></li>
</ul>
<p><strong>Practical implication</strong>: To halve Monte Carlo error, need 4× more samples.</p>
</div>
<p><strong>Usage in this Course</strong></p>
<ul class="simple">
<li><p><strong>Big-O, Θ, o</strong>: Deterministic algorithm complexity and approximations</p></li>
<li><p><strong>O_p, o_p</strong>: Stochastic convergence rates (Monte Carlo error, estimation error)</p></li>
<li><p><strong>Tilde (~)</strong>: Dominant asymptotic behavior</p></li>
</ul>
</section>
<section id="convergence-of-random-variables">
<h3>Convergence of Random Variables<a class="headerlink" href="#convergence-of-random-variables" title="Link to this heading"></a></h3>
<p>We will use three primary modes of convergence.</p>
<div class="definition admonition">
<p class="admonition-title">Definition: Convergence in Probability</p>
<p>A sequence <span class="math notranslate nohighlight">\(X_n\)</span> <strong>converges in probability</strong> to <span class="math notranslate nohighlight">\(X\)</span>, written <span class="math notranslate nohighlight">\(X_n \xrightarrow{P} X\)</span>, if</p>
<div class="math notranslate nohighlight">
\[\lim_{n \to \infty} P\!\left(|X_n - X| &gt; \epsilon\right) = 0
\quad \text{for all } \epsilon &gt; 0.\]</div>
<p><em>Intuition</em>: For large <span class="math notranslate nohighlight">\(n\)</span>, <span class="math notranslate nohighlight">\(X_n\)</span> is close to <span class="math notranslate nohighlight">\(X\)</span> with high probability.</p>
</div>
<div class="definition admonition">
<p class="admonition-title">Definition: Almost Sure Convergence</p>
<p><span class="math notranslate nohighlight">\(X_n\)</span> <strong>converges almost surely</strong> to <span class="math notranslate nohighlight">\(X\)</span>, written <span class="math notranslate nohighlight">\(X_n \xrightarrow{a.s.} X\)</span>, if</p>
<div class="math notranslate nohighlight">
\[P\!\Big(\lim_{n \to \infty} X_n = X\Big) = 1,\]</div>
<p>equivalently</p>
<div class="math notranslate nohighlight">
\[P\!\left(\{\omega : \lim_{n \to \infty} X_n(\omega) = X(\omega)\}\right) = 1.\]</div>
<p><em>Intuition</em>: Convergence holds along almost every sample path.</p>
</div>
<div class="definition admonition">
<p class="admonition-title">Definition: Convergence in Distribution</p>
<p><span class="math notranslate nohighlight">\(X_n\)</span> <strong>converges in distribution</strong> to <span class="math notranslate nohighlight">\(X\)</span>, written <span class="math notranslate nohighlight">\(X_n \xrightarrow{d} X\)</span>, if</p>
<div class="math notranslate nohighlight">
\[\lim_{n \to \infty} F_{X_n}(x) = F_X(x)\]</div>
<p>at all continuity points of <span class="math notranslate nohighlight">\(F_X\)</span>.</p>
<p><em>Intuition</em>: The CDFs converge pointwise. Individual realizations need not converge.</p>
</div>
<p><strong>Relationships Between Convergence Modes</strong></p>
<div class="math notranslate nohighlight">
\[X_n \xrightarrow{a.s.} X \;\Rightarrow\; X_n \xrightarrow{P} X \;\Rightarrow\; X_n \xrightarrow{d} X.\]</div>
<ul class="simple">
<li><p>Almost sure convergence is strongest; it implies convergence in probability.</p></li>
<li><p>Convergence in probability implies convergence in distribution.</p></li>
<li><p>The reverse implications generally do <strong>not</strong> hold.</p></li>
<li><p><strong>Important exception</strong>: If <span class="math notranslate nohighlight">\(X_n \xrightarrow{d} c\)</span> for a constant <span class="math notranslate nohighlight">\(c\)</span>, then <span class="math notranslate nohighlight">\(X_n \xrightarrow{P} c\)</span>.</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Tools You Will Use Constantly 🔧</p>
<p><strong>Continuous Mapping Theorem</strong>: If <span class="math notranslate nohighlight">\(X_n \xrightarrow{P} X\)</span> (or <span class="math notranslate nohighlight">\(\xrightarrow{d}\)</span>) and <span class="math notranslate nohighlight">\(g\)</span> is continuous, then <span class="math notranslate nohighlight">\(g(X_n) \xrightarrow{P} g(X)\)</span> (or <span class="math notranslate nohighlight">\(\xrightarrow{d}\)</span>).</p>
<p><strong>Slutsky’s Theorem</strong>: If <span class="math notranslate nohighlight">\(X_n \xrightarrow{d} X\)</span> and <span class="math notranslate nohighlight">\(Y_n \xrightarrow{P} c\)</span>, then:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X_n + Y_n \xrightarrow{d} X + c\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(X_n Y_n \xrightarrow{d} cX\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(X_n / Y_n \xrightarrow{d} X / c\)</span> (if <span class="math notranslate nohighlight">\(c \ne 0\)</span>)</p></li>
</ul>
</div>
<p><strong>Examples Distinguishing the Modes</strong></p>
<ol class="arabic">
<li><p><strong>Convergence in probability without almost sure convergence</strong>:</p>
<p>Consider a sequence of <strong>independent</strong> random variables <span class="math notranslate nohighlight">\(X_1, X_2, X_3, \ldots\)</span> where each <span class="math notranslate nohighlight">\(X_n\)</span> takes values in <span class="math notranslate nohighlight">\(\{0, 1\}\)</span> with</p>
<div class="math notranslate nohighlight">
\[P(X_n = 1) = \frac{1}{n}, \qquad P(X_n = 0) = 1 - \frac{1}{n}.\]</div>
<p>Then:</p>
<ul>
<li><p><strong>Convergence in probability holds</strong>: <span class="math notranslate nohighlight">\(X_n \xrightarrow{P} 0\)</span> because</p>
<div class="math notranslate nohighlight">
\[P(|X_n - 0| &gt; \epsilon) = P(X_n = 1) = \frac{1}{n} \to 0 \quad \text{for any } \epsilon &lt; 1.\]</div>
</li>
<li><p><strong>Almost sure convergence fails</strong>: <span class="math notranslate nohighlight">\(X_n \not\xrightarrow{a.s.} 0\)</span> because <span class="math notranslate nohighlight">\(\sum_{n=1}^{\infty} P(X_n = 1) = \sum_{n=1}^{\infty} \frac{1}{n} = \infty\)</span> diverges. By the second Borel–Cantelli lemma (which applies because the events <span class="math notranslate nohighlight">\(\{X_n = 1\}\)</span> are independent), this implies</p>
<div class="math notranslate nohighlight">
\[P\left(\limsup_{n \to \infty} \{X_n = 1\}\right) = P\left(\text{infinitely many } X_n = 1\right) = 1.\]</div>
</li>
</ul>
<p><strong>What this means</strong>: With probability 1, the sequence <span class="math notranslate nohighlight">\(X_n(\omega)\)</span> contains infinitely many 1’s (though they become increasingly sparse). For such sample paths, <span class="math notranslate nohighlight">\(\lim_{n \to \infty} X_n(\omega)\)</span> does not exist—the sequence oscillates between 0 and 1 forever rather than settling to a limit.</p>
<p><strong>The key insight</strong>: Convergence in probability is a statement about <strong>individual time points</strong> (“at time <span class="math notranslate nohighlight">\(n\)</span>, probably close to 0”), while almost sure convergence is about <strong>entire sample paths</strong> (“the sequence converges as real numbers”). Here we have the former without the latter.</p>
<div class="note admonition">
<p class="admonition-title">Note for the Mathematically Inclined 📐</p>
<p>Formally, this example lives on the probability space <span class="math notranslate nohighlight">\((\Omega, \mathcal{F}, P)\)</span> where <span class="math notranslate nohighlight">\(\Omega = \{0,1\}^{\mathbb{N}}\)</span> is the space of infinite binary sequences, <span class="math notranslate nohighlight">\(X_n(\omega) = \omega_n\)</span> is the <span class="math notranslate nohighlight">\(n\)</span>-th coordinate projection, and <span class="math notranslate nohighlight">\(P\)</span> is the product measure with marginals <span class="math notranslate nohighlight">\(P(\omega_n = 1) = 1/n\)</span>. The Borel–Cantelli result tells us that <span class="math notranslate nohighlight">\(P\)</span>-almost every <span class="math notranslate nohighlight">\(\omega \in \Omega\)</span> has infinitely many coordinates equal to 1, hence no pointwise limit exists.</p>
</div>
</li>
<li><p><strong>Convergence in distribution without convergence in probability</strong>:</p>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, X_3, \ldots\)</span> be independent with <span class="math notranslate nohighlight">\(X_n \sim \mathcal{N}(0,1)\)</span> for all <span class="math notranslate nohighlight">\(n\)</span>. Then:</p>
<ul>
<li><p><strong>Convergence in distribution holds</strong> (trivially): <span class="math notranslate nohighlight">\(X_n \xrightarrow{d} \mathcal{N}(0,1)\)</span> because the distribution is constant in <span class="math notranslate nohighlight">\(n\)</span>.</p></li>
<li><p><strong>No convergence in probability to any random variable</strong>: Suppose <span class="math notranslate nohighlight">\(X_n \xrightarrow{P} X\)</span> for some random variable <span class="math notranslate nohighlight">\(X\)</span>. Then any subsequence also converges in probability to <span class="math notranslate nohighlight">\(X\)</span>. In particular, both <span class="math notranslate nohighlight">\(X_{2n} \xrightarrow{P} X\)</span> and <span class="math notranslate nohighlight">\(X_{2n-1} \xrightarrow{P} X\)</span>, which implies</p>
<div class="math notranslate nohighlight">
\[|X_{2n} - X_{2n-1}| \xrightarrow{P} 0.\]</div>
<p>However, <span class="math notranslate nohighlight">\(X_{2n} - X_{2n-1} \sim \mathcal{N}(0, 2)\)</span> for every <span class="math notranslate nohighlight">\(n\)</span> by independence. For any <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span>,</p>
<div class="math notranslate nohighlight">
\[P(|X_{2n} - X_{2n-1}| &gt; \epsilon) = 2\left[1 - \Phi\left(\frac{\epsilon}{\sqrt{2}}\right)\right]\]</div>
<p>is bounded away from 0 (e.g., for <span class="math notranslate nohighlight">\(\epsilon = 1\)</span>, this probability is approximately 0.48). This contradicts convergence to 0 in probability. Therefore, no convergence in probability occurs.</p>
</li>
</ul>
<p><strong>The key insight</strong>: Convergence in distribution is about the <strong>distribution</strong> converging, not the random variables themselves. The variables <span class="math notranslate nohighlight">\(X_n\)</span> keep “jumping around” independently without approaching any particular random variable, but their distribution remains fixed.</p>
</li>
</ol>
</section>
<section id="law-of-large-numbers-and-central-limit-theorem">
<h3>Law of Large Numbers and Central Limit Theorem<a class="headerlink" href="#law-of-large-numbers-and-central-limit-theorem" title="Link to this heading"></a></h3>
<p>Assume <span class="math notranslate nohighlight">\(X_1, X_2, \ldots\)</span> are i.i.d. with mean <span class="math notranslate nohighlight">\(\mu = \mathbb{E}[X_1]\)</span>.</p>
<p><strong>Weak Law of Large Numbers (WLLN)</strong>: If <span class="math notranslate nohighlight">\(\mathbb{E}[|X_1|] &lt; \infty\)</span>, then</p>
<div class="math notranslate nohighlight">
\[\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i \xrightarrow{P} \mu.\]</div>
<p><strong>Strong Law of Large Numbers (SLLN)</strong>: If <span class="math notranslate nohighlight">\(\mathbb{E}[|X_1|] &lt; \infty\)</span>, then</p>
<div class="math notranslate nohighlight">
\[\bar{X}_n \xrightarrow{a.s.} \mu.\]</div>
<figure class="align-center" id="id16">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/Part1/ch1_1_law_of_large_numbers.png"><img alt="Two panels showing convergence of sample proportion to true probability p=0.6, with single trajectory on left and 20 trajectories on right" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/Part1/ch1_1_law_of_large_numbers.png" style="width: 90%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 1.4 </span><span class="caption-text"><strong>Law of Large Numbers in action.</strong> (A) A single sequence of coin flips converges toward <span class="math notranslate nohighlight">\(p = 0.6\)</span>, with the ±2 SE band shrinking as <span class="math notranslate nohighlight">\(n\)</span> increases. (B) Twenty independent trajectories all converge to the same limit, demonstrating that <span class="math notranslate nohighlight">\(\bar{X}_n \xrightarrow{P} \mu\)</span> regardless of the particular sample path.</span><a class="headerlink" href="#id16" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Central Limit Theorem (Lindeberg–Lévy)</strong>: If in addition <span class="math notranslate nohighlight">\(\text{Var}(X_1) = \sigma^2 &lt; \infty\)</span>, then</p>
<div class="math notranslate nohighlight">
\[\frac{\sqrt{n}\,(\bar{X}_n - \mu)}{\sigma} \xrightarrow{d} \mathcal{N}(0,1).\]</div>
<p>Unless specified otherwise, “Law of Large Numbers” will refer to the weak version (convergence in probability).</p>
<figure class="align-center" id="id17">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/Part1/ch1_1_central_limit_theorem.png"><img alt="Eight-panel figure showing CLT convergence from Exponential and Uniform source distributions to standard normal as sample size increases from 1 to 30" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/Part1/ch1_1_central_limit_theorem.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 1.5 </span><span class="caption-text"><strong>Central Limit Theorem demonstration.</strong> Regardless of the source distribution (top row: Exponential; bottom row: Uniform), the standardized sample mean converges to <span class="math notranslate nohighlight">\(N(0,1)\)</span> as <span class="math notranslate nohighlight">\(n\)</span> increases. At <span class="math notranslate nohighlight">\(n = 1\)</span>, the shape matches the source; by <span class="math notranslate nohighlight">\(n = 30\)</span>, both are nearly indistinguishable from the normal curve (red).</span><a class="headerlink" href="#id17" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="common-limits-in-probability">
<h3>Common Limits in Probability<a class="headerlink" href="#common-limits-in-probability" title="Link to this heading"></a></h3>
<p>Several limits recur throughout probability and statistics:</p>
<div class="math notranslate nohighlight">
\[\lim_{n \to \infty} \left(1 + \frac{x}{n}\right)^n = e^x,
\qquad
\lim_{n \to \infty} \left(1 - \frac{\lambda}{n}\right)^n = e^{-\lambda}.\]</div>
<p>These motivate classical approximations. For example, if <span class="math notranslate nohighlight">\(X_n \sim \text{Binomial}(n, \lambda/n)\)</span> then <span class="math notranslate nohighlight">\(X_n \xrightarrow{d} \text{Poisson}(\lambda)\)</span> as <span class="math notranslate nohighlight">\(n \to \infty\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Quick Check ✓</p>
<p>Before proceeding, verify you understand:</p>
<ol class="arabic simple">
<li><p>What is the difference between <span class="math notranslate nohighlight">\(X_n \xrightarrow{P} X\)</span> and <span class="math notranslate nohighlight">\(X_n \xrightarrow{a.s.} X\)</span>?</p></li>
<li><p>Can a sequence converge in distribution but not in probability? (Hint: <span class="math notranslate nohighlight">\(X_n \sim \mathcal{N}(0,1)\)</span> i.i.d.)</p></li>
<li><p>Evaluate: <span class="math notranslate nohighlight">\(\lim_{n \to \infty} \left(1 - \frac{2}{n}\right)^n\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(f(n) = 5n^2 + 3n\)</span>, what is <span class="math notranslate nohighlight">\(f(n)\)</span> in big-O notation?</p></li>
</ol>
<p><em>Answers</em>: (1) Probability convergence means high probability of being close; almost sure convergence means pathwise convergence with probability 1 and is stronger. (2) Yes. The i.i.d. normal example has constant distribution in <span class="math notranslate nohighlight">\(n\)</span> but does not converge in probability to any random variable. (3) <span class="math notranslate nohighlight">\(e^{-2} \approx 0.135\)</span>. (4) <span class="math notranslate nohighlight">\(O(n^2)\)</span>.</p>
</div>
<p>With these mathematical tools in hand, we are ready to explore interpretations of probability and their profound philosophical and practical implications.</p>
</section>
</section>
<section id="interpretations-of-probability">
<h2>Interpretations of Probability<a class="headerlink" href="#interpretations-of-probability" title="Link to this heading"></a></h2>
<p>All modern approaches to probability and statistics—frequentist, Bayesian, and likelihoodist—accept Kolmogorov’s axioms as the mathematical foundation, established in his 1933 monograph <em>Grundbegriffe der Wahrscheinlichkeitsrechnung</em> (Foundations of the Theory of Probability). <a class="footnote-reference brackets" href="#id2" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> While everyone agrees on these mathematical rules, there is profound disagreement about what probability actually <em>represents</em>. The main interpretations fall into two broad categories: <strong>objective</strong> (probability as a feature of the world) and <strong>epistemic</strong> (probability as a degree of knowledge or belief). These different interpretations have enormous practical consequences for how we conduct statistical inference and computational analysis.</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id2" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Specialized domains like quantum mechanics use modified probability axioms
for non-commuting observables, and researchers have explored generalizations
like imprecise probability. These are extensions or domain-specific adaptations
rather than rejections of the classical framework, which remains universal for
statistical data science.</p>
</aside>
</aside>
<section id="objective-interpretations-probability-as-physical-reality">
<h3>Objective Interpretations: Probability as Physical Reality<a class="headerlink" href="#objective-interpretations-probability-as-physical-reality" title="Link to this heading"></a></h3>
<p>Objective interpretations treat probability as a property of the physical world, independent of any observer’s knowledge or beliefs.</p>
<section id="a-frequentist-interpretation">
<h4>A. Frequentist Interpretation<a class="headerlink" href="#a-frequentist-interpretation" title="Link to this heading"></a></h4>
<p>The <strong>frequentist interpretation</strong> defines probability as the long-run relative frequency of an event in repeated trials under identical conditions.</p>
<p><strong>Core Idea</strong>: If we repeat a random experiment many times, the probability of an event <span class="math notranslate nohighlight">\(E\)</span> equals the limiting proportion of trials in which <span class="math notranslate nohighlight">\(E\)</span> occurs:</p>
<div class="math notranslate nohighlight">
\[P(E) = \lim_{n \to \infty} \frac{\text{Number of times } E \text{ occurs in } n \text{ trials}}{n}\]</div>
<p>For example, saying “the probability of heads is 0.5” means that if we flip a fair coin infinitely many times, approximately half will be heads. This limiting frequency is an objective property of the coin, not dependent on anyone’s beliefs.</p>
<p><strong>Connection to Law of Large Numbers</strong>: The frequentist interpretation is closely tied to the mathematical Law of Large Numbers, which states that the sample mean of independent, identically distributed (i.i.d.) random variables converges to the population mean as sample size increases. For a coin with true probability <span class="math notranslate nohighlight">\(p\)</span> of heads, if <span class="math notranslate nohighlight">\(X_i = 1\)</span> for heads and <span class="math notranslate nohighlight">\(X_i = 0\)</span> for tails, then:</p>
<div class="math notranslate nohighlight">
\[\bar{X}_n = \frac{1}{n}\sum_{i=1}^{n} X_i \xrightarrow{P} p \quad \text{as } n \to \infty\]</div>
<p><strong>Strengths</strong>:</p>
<ul class="simple">
<li><p><strong>Objective</strong>: Doesn’t depend on subjective beliefs or opinions</p></li>
<li><p><strong>Empirically verifiable</strong>: Can be tested through repeated experimentation</p></li>
<li><p><strong>Clear operational meaning</strong>: Provides concrete, observable interpretation</p></li>
<li><p><strong>Mathematical foundation</strong>: Connects directly to limit theorems in probability theory</p></li>
</ul>
<p><strong>Limitations</strong>:</p>
<ul class="simple">
<li><p><strong>Non-repeatable events</strong>: Cannot assign probabilities to unique events (e.g., “What is the probability that this specific patient will survive surgery?” or “What is the probability that the Higgs boson exists?”). Frequentists handle such cases by positing a repeatable data-generating mechanism (e.g., patients with similar characteristics), but they resist assigning probabilities to the truth values of fixed parameters or one-time hypotheses.</p></li>
<li><p><strong>Infinite sequence required</strong>: The limit as <span class="math notranslate nohighlight">\(n \to \infty\)</span> is a mathematical idealization that can never be physically realized</p></li>
<li><p><strong>Circularity</strong>: How many trials are “enough” to approximate the limiting frequency? We need probability to answer this question.</p></li>
<li><p><strong>Fixed parameters</strong>: Cannot treat population parameters as random, making statements like “the probability that <span class="math notranslate nohighlight">\(\mu &gt; 0\)</span>” meaningless in the frequentist framework</p></li>
</ul>
<div class="note admonition">
<p class="admonition-title">Example 💡: Coin Flipping Simulation</p>
<p><strong>Given:</strong> A biased coin with true probability <span class="math notranslate nohighlight">\(p = 0.6\)</span> of heads.</p>
<p><strong>Find:</strong> Demonstrate convergence of relative frequency to true probability.</p>
<p><strong>Mathematical approach:</strong></p>
<p>The relative frequency after <span class="math notranslate nohighlight">\(n\)</span> flips is <span class="math notranslate nohighlight">\(\hat{p}_n = \frac{1}{n}\sum_{i=1}^{n} X_i\)</span> where <span class="math notranslate nohighlight">\(X_i \sim \text{Bernoulli}(0.6)\)</span>. By the Law of Large Numbers:</p>
<div class="math notranslate nohighlight">
\[\hat{p}_n \xrightarrow{P} 0.6 \quad \text{as } n \to \infty\]</div>
<p>The standard error decreases as <span class="math notranslate nohighlight">\(\text{SE}(\hat{p}_n) = \sqrt{p(1-p)/n} = \sqrt{0.24/n}\)</span>.</p>
<p><strong>Python implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># True probability</span>
<span class="n">p_true</span> <span class="o">=</span> <span class="mf">0.6</span>

<span class="c1"># Simulate coin flips</span>
<span class="n">n_max</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">flips</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p_true</span><span class="p">,</span> <span class="n">n_max</span><span class="p">)</span>

<span class="c1"># Compute cumulative relative frequency</span>
<span class="n">cumsum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">flips</span><span class="p">)</span>
<span class="n">n_trials</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_max</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">rel_freq</span> <span class="o">=</span> <span class="n">cumsum</span> <span class="o">/</span> <span class="n">n_trials</span>

<span class="c1"># Compute standard error bounds</span>
<span class="n">se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">p_true</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_true</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_trials</span><span class="p">)</span>

<span class="c1"># Print convergence at key milestones</span>
<span class="n">milestones</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Convergence of relative frequency to true probability:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;n&#39;</span><span class="si">:</span><span class="s2">&gt;6s</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="s1">&#39;Rel. Freq.&#39;</span><span class="si">:</span><span class="s2">&gt;12s</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="s1">&#39;Std. Error&#39;</span><span class="si">:</span><span class="s2">&gt;12s</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="s1">&#39;Distance&#39;</span><span class="si">:</span><span class="s2">&gt;12s</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">milestones</span><span class="p">:</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">distance</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">rel_freq</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">-</span> <span class="n">p_true</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">n</span><span class="si">:</span><span class="s2">6d</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">rel_freq</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="si">:</span><span class="s2">12.6f</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">se</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="si">:</span><span class="s2">12.6f</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">distance</span><span class="si">:</span><span class="s2">12.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Result:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Convergence</span> <span class="n">of</span> <span class="n">relative</span> <span class="n">frequency</span> <span class="n">to</span> <span class="n">true</span> <span class="n">probability</span><span class="p">:</span>
     <span class="n">n</span> <span class="o">|</span>  <span class="n">Rel</span><span class="o">.</span> <span class="n">Freq</span><span class="o">.</span> <span class="o">|</span>  <span class="n">Std</span><span class="o">.</span> <span class="n">Error</span> <span class="o">|</span>     <span class="n">Distance</span>
<span class="o">--------------------------------------------------</span>
    <span class="mi">10</span> <span class="o">|</span>     <span class="mf">0.600000</span> <span class="o">|</span>     <span class="mf">0.154919</span> <span class="o">|</span>     <span class="mf">0.000000</span>
   <span class="mi">100</span> <span class="o">|</span>     <span class="mf">0.640000</span> <span class="o">|</span>     <span class="mf">0.048990</span> <span class="o">|</span>     <span class="mf">0.040000</span>
  <span class="mi">1000</span> <span class="o">|</span>     <span class="mf">0.608000</span> <span class="o">|</span>     <span class="mf">0.015492</span> <span class="o">|</span>     <span class="mf">0.008000</span>
 <span class="mi">10000</span> <span class="o">|</span>     <span class="mf">0.601300</span> <span class="o">|</span>     <span class="mf">0.004899</span> <span class="o">|</span>     <span class="mf">0.001300</span>
</pre></div>
</div>
<p>After 10,000 flips, the relative frequency (0.6013) is within 0.0013 of the true probability, and the standard error has decreased to approximately 0.005. This demonstrates the Law of Large Numbers in action, though note that even with 10,000 trials, we haven’t reached the theoretical limit—we’ve only approximated it.</p>
</div>
</section>
<section id="b-propensity-interpretation">
<h4>B. Propensity Interpretation<a class="headerlink" href="#b-propensity-interpretation" title="Link to this heading"></a></h4>
<p>The <strong>propensity interpretation</strong>, developed by philosopher Karl Popper in 1957, treats probability as a physical disposition or tendency of a system to produce certain outcomes.</p>
<p><strong>Core Idea</strong>: Probability is an objective property of a chance setup—a “propensity” or physical tendency that exists even for single trials. A radioactive atom has a propensity of 0.1 to decay in the next hour; this is neither a long-run frequency (which would require many identical atoms) nor a belief, but a real physical property of this particular atom.</p>
<p><strong>Strengths</strong>:</p>
<ul class="simple">
<li><p><strong>Single-case application</strong>: Can meaningfully assign probabilities to unique, non-repeatable events</p></li>
<li><p><strong>Maintains objectivity</strong>: Probability remains a feature of the world, not dependent on beliefs</p></li>
<li><p><strong>Explains convergence</strong>: Long-run frequencies stabilize because underlying propensities determine outcomes</p></li>
<li><p><strong>Quantum mechanics</strong>: Provides a natural interpretation for inherently stochastic quantum processes</p></li>
</ul>
<p><strong>Limitations</strong>:</p>
<ul class="simple">
<li><p><strong>Conceptual vagueness</strong>: What exactly is a “propensity”? How do we measure or verify it independently of frequencies?</p></li>
<li><p><strong>Empirical collapse</strong>: In practice, propensities are estimated from frequencies, potentially reducing to frequentism</p></li>
<li><p><strong>Theoretical paradoxes</strong>: Humphreys’ Paradox shows that propensity interpretations face logical difficulties when applying Bayes’ theorem</p></li>
<li><p><strong>Limited adoption</strong>: Rarely used in practical statistical analysis despite philosophical appeal</p></li>
</ul>
</section>
</section>
<section id="epistemic-interpretations-probability-as-degree-of-belief">
<h3>Epistemic Interpretations: Probability as Degree of Belief<a class="headerlink" href="#epistemic-interpretations-probability-as-degree-of-belief" title="Link to this heading"></a></h3>
<p>Epistemic interpretations treat probability not as a property of the external world, but as a quantification of uncertainty, knowledge, or degree of belief about propositions.</p>
<section id="a-subjective-bayesian-interpretation">
<h4>A. Subjective (Bayesian) Interpretation<a class="headerlink" href="#a-subjective-bayesian-interpretation" title="Link to this heading"></a></h4>
<p>The <strong>subjective Bayesian interpretation</strong> defines probability as a rational agent’s degree of belief or confidence in a proposition, given their available information and prior knowledge.</p>
<p><strong>Core Idea</strong>: Probability quantifies how strongly someone believes a proposition is true. Different individuals with different information can rationally assign different probabilities to the same event. Italian mathematician Bruno de Finetti famously declared: <strong>“Probability does not exist”</strong> as an objective feature of reality—only subjective degrees of belief constrained by the rationality requirements of coherence exist.</p>
<p>This interpretation was developed independently by Frank Ramsey (1926), Bruno de Finetti (1937), and Leonard Savage (1954) in the early to mid-20th century, forming the foundation of modern Bayesian statistics.</p>
<p><strong>The Dutch Book Argument for Coherence</strong></p>
<p>How do we know subjective probabilities should follow Kolmogorov’s axioms? The <strong>Dutch Book argument</strong> provides the justification: If your probability assignments violate the axioms, someone can construct a set of bets (a “Dutch book”) that guarantees you lose money regardless of which outcomes occur. Rational agents should therefore maintain <strong>coherent</strong> beliefs that satisfy the probability axioms.</p>
<p>For example, suppose you assign probabilities that violate additivity for disjoint events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(A) = 0.6\)</span> (you’ll pay $0.60 for a bet that pays $1 if <span class="math notranslate nohighlight">\(A\)</span> occurs)</p></li>
<li><p><span class="math notranslate nohighlight">\(P(B) = 0.5\)</span> (you’ll pay $0.50 for a bet that pays $1 if <span class="math notranslate nohighlight">\(B\)</span> occurs)</p></li>
<li><p><span class="math notranslate nohighlight">\(P(A \cup B) = 0.9\)</span> (you’ll accept $0.90 to provide a bet that pays $1 if <span class="math notranslate nohighlight">\(A\)</span> or <span class="math notranslate nohighlight">\(B\)</span> occurs)</p></li>
</ul>
<p>If <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are disjoint, these probabilities violate Axiom 3: they should satisfy <span class="math notranslate nohighlight">\(P(A \cup B) = P(A) + P(B) = 1.1\)</span>, not 0.9. Since your <span class="math notranslate nohighlight">\(P(A \cup B) = 0.9 &lt; 1.1\)</span>, you’ve undervalued the combined event.</p>
<p>A clever opponent exploits this by:</p>
<ol class="arabic simple">
<li><p><strong>Buying</strong> from you the bet on <span class="math notranslate nohighlight">\(A \cup B\)</span> for $0.90 (you receive $0.90, must pay $1 if <span class="math notranslate nohighlight">\(A\)</span> or <span class="math notranslate nohighlight">\(B\)</span> occurs)</p></li>
<li><p><strong>Selling</strong> to you the bet on <span class="math notranslate nohighlight">\(A\)</span> for $0.60 (you pay $0.60, receive $1 if <span class="math notranslate nohighlight">\(A\)</span> occurs)</p></li>
<li><p><strong>Selling</strong> to you the bet on <span class="math notranslate nohighlight">\(B\)</span> for $0.50 (you pay $0.50, receive $1 if <span class="math notranslate nohighlight">\(B\)</span> occurs)</p></li>
</ol>
<p>Your net position:</p>
<ul class="simple">
<li><p><strong>Upfront cash flow</strong>: <span class="math notranslate nohighlight">\(+0.90 - 0.60 - 0.50 = -0.20\)</span> (you lose $0.20 immediately)</p></li>
<li><p><strong>If</strong> <span class="math notranslate nohighlight">\(A\)</span> <strong>occurs</strong> (but not <span class="math notranslate nohighlight">\(B\)</span>): Pay $1 on combined bet, receive $1 on <span class="math notranslate nohighlight">\(A\)</span> bet = net $0, <strong>total loss = $0.20</strong></p></li>
<li><p><strong>If</strong> <span class="math notranslate nohighlight">\(B\)</span> <strong>occurs</strong> (but not <span class="math notranslate nohighlight">\(A\)</span>): Pay $1 on combined bet, receive $1 on <span class="math notranslate nohighlight">\(B\)</span> bet = net $0, <strong>total loss = $0.20</strong></p></li>
<li><p><strong>If neither occurs</strong>: Pay $0 on all bets, <strong>total loss = $0.20</strong></p></li>
</ul>
<p>You lose $0.20 guaranteed, regardless of outcome—this is the <strong>Dutch Book</strong>. Maintaining coherent probabilities that satisfy Kolmogorov’s axioms prevents such exploitation.</p>
<p><strong>Strengths</strong>:</p>
<ul class="simple">
<li><p><strong>Universal applicability</strong>: Can assign probabilities to any proposition—hypotheses, parameters, future events, past events we’re uncertain about</p></li>
<li><p><strong>Incorporates prior information</strong>: Naturally combines existing knowledge with new data via Bayes’ theorem</p></li>
<li><p><strong>Direct probability statements</strong>: Can say “there is a 95% probability that :math:<a href="#id3"><span class="problematic" id="id4">`</span></a>mu &gt; 0$” directly</p></li>
<li><p><strong>Handles unique events</strong>: Natural framework for one-time events like “Will it rain tomorrow?” or “Is this hypothesis true?”</p></li>
<li><p><strong>Coherent updating</strong>: Bayes’ theorem provides the unique coherent way to update beliefs given new evidence</p></li>
</ul>
<p><strong>Limitations</strong>:</p>
<ul class="simple">
<li><p><strong>Prior dependence</strong>: Requires specifying a prior distribution, which may be subjective or controversial</p></li>
<li><p><strong>Prior sensitivity</strong>: With limited data, conclusions can be sensitive to prior choice</p></li>
<li><p><strong>Subjectivity concerns</strong>: Critics argue personal beliefs have no place in scientific inference</p></li>
<li><p><strong>Computational demands</strong>: Bayesian inference often requires intensive computation (MCMC, numerical integration)</p></li>
<li><p><strong>Communication challenges</strong>: Different priors can lead to different conclusions, complicating scientific consensus</p></li>
</ul>
<div class="note admonition">
<p class="admonition-title">Example 💡: Medical Diagnosis with Bayesian Updating</p>
<p><strong>Given:</strong> A rare disease affects 1% of the population. A diagnostic test has 95% sensitivity (true positive rate) and 90% specificity (true negative rate). A patient tests positive.</p>
<p><strong>Find:</strong> The probability the patient has the disease, and demonstrate Bayesian updating.</p>
<p><strong>Mathematical approach:</strong></p>
<p>Let <span class="math notranslate nohighlight">\(D\)</span> = patient has disease, <span class="math notranslate nohighlight">\(T^+\)</span> = positive test result. We know:</p>
<ul class="simple">
<li><p>Prior: <span class="math notranslate nohighlight">\(P(D) = 0.01\)</span>, <span class="math notranslate nohighlight">\(P(D^c) = 0.99\)</span></p></li>
<li><p>Likelihood: <span class="math notranslate nohighlight">\(P(T^+ \mid D) = 0.95\)</span> (sensitivity), <span class="math notranslate nohighlight">\(P(T^+ \mid D^c) = 0.10\)</span> (false positive rate = 1 - specificity)</p></li>
</ul>
<p>By Bayes’ theorem:</p>
<div class="math notranslate nohighlight">
\[P(D \mid T^+) = \frac{P(T^+ \mid D) \cdot P(D)}{P(T^+)}\]</div>
<p>where the marginal likelihood is:</p>
<div class="math notranslate nohighlight">
\[P(T^+) = P(T^+ \mid D) \cdot P(D) + P(T^+ \mid D^c) \cdot P(D^c) = 0.95(0.01) + 0.10(0.99) = 0.1085\]</div>
<p>Therefore:</p>
<div class="math notranslate nohighlight">
\[P(D \mid T^+) = \frac{0.95 \times 0.01}{0.1085} = \frac{0.0095}{0.1085} \approx 0.0876\]</div>
<p><strong>Interpretation</strong>: Despite the positive test, there’s only an 8.76% probability the patient has the disease! This counterintuitive result occurs because the disease is rare (low prior) and the false positive rate is non-negligible.</p>
<p><strong>Python implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">bayesian_diagnostic</span><span class="p">(</span><span class="n">prior_disease</span><span class="p">,</span> <span class="n">sensitivity</span><span class="p">,</span> <span class="n">specificity</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute posterior probability of disease given positive test.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    prior_disease : float</span>
<span class="sd">        Prior probability of having the disease, P(D)</span>
<span class="sd">    sensitivity : float</span>
<span class="sd">        True positive rate, P(T+ | D)</span>
<span class="sd">    specificity : float</span>
<span class="sd">        True negative rate, P(T- | D^c)</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        Dictionary with prior, likelihood components, and posterior</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># False positive rate</span>
    <span class="n">fpr</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">specificity</span>

    <span class="c1"># Marginal likelihood P(T+)</span>
    <span class="n">p_test_pos</span> <span class="o">=</span> <span class="n">sensitivity</span> <span class="o">*</span> <span class="n">prior_disease</span> <span class="o">+</span> <span class="n">fpr</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">prior_disease</span><span class="p">)</span>

    <span class="c1"># Posterior by Bayes&#39; theorem</span>
    <span class="n">posterior</span> <span class="o">=</span> <span class="p">(</span><span class="n">sensitivity</span> <span class="o">*</span> <span class="n">prior_disease</span><span class="p">)</span> <span class="o">/</span> <span class="n">p_test_pos</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;prior&#39;</span><span class="p">:</span> <span class="n">prior_disease</span><span class="p">,</span>
        <span class="s1">&#39;likelihood_disease&#39;</span><span class="p">:</span> <span class="n">sensitivity</span><span class="p">,</span>
        <span class="s1">&#39;likelihood_healthy&#39;</span><span class="p">:</span> <span class="n">fpr</span><span class="p">,</span>
        <span class="s1">&#39;marginal_likelihood&#39;</span><span class="p">:</span> <span class="n">p_test_pos</span><span class="p">,</span>
        <span class="s1">&#39;posterior&#39;</span><span class="p">:</span> <span class="n">posterior</span>
    <span class="p">}</span>

<span class="c1"># Compute for our scenario</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">bayesian_diagnostic</span><span class="p">(</span><span class="n">prior_disease</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">sensitivity</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">specificity</span><span class="o">=</span><span class="mf">0.90</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prior probability of disease: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;prior&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Marginal probability of positive test: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;marginal_likelihood&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Posterior probability of disease | positive test: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;posterior&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Compute update factor (Bayes factor)</span>
<span class="n">bayes_factor</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;likelihood_disease&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;likelihood_healthy&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Bayes factor (evidence strength): </span><span class="si">{</span><span class="n">bayes_factor</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Posterior odds: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;posterior&#39;</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;posterior&#39;</span><span class="p">])</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prior odds: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;prior&#39;</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;prior&#39;</span><span class="p">])</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Result:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Prior</span> <span class="n">probability</span> <span class="n">of</span> <span class="n">disease</span><span class="p">:</span> <span class="mf">0.0100</span>
<span class="n">Marginal</span> <span class="n">probability</span> <span class="n">of</span> <span class="n">positive</span> <span class="n">test</span><span class="p">:</span> <span class="mf">0.1085</span>
<span class="n">Posterior</span> <span class="n">probability</span> <span class="n">of</span> <span class="n">disease</span> <span class="o">|</span> <span class="n">positive</span> <span class="n">test</span><span class="p">:</span> <span class="mf">0.0876</span>

<span class="n">Bayes</span> <span class="n">factor</span> <span class="p">(</span><span class="n">evidence</span> <span class="n">strength</span><span class="p">):</span> <span class="mf">9.50</span>
<span class="n">Posterior</span> <span class="n">odds</span><span class="p">:</span> <span class="mf">0.0960</span>
<span class="n">Prior</span> <span class="n">odds</span><span class="p">:</span> <span class="mf">0.0101</span>
</pre></div>
</div>
<p>The positive test increases our degree of belief in the disease from 1% to 8.76%—a 9.5-fold increase in odds. However, because the prior probability was so low, the posterior remains below 10%. This example illustrates the crucial role of prior information in Bayesian inference and the often counterintuitive nature of diagnostic reasoning.</p>
</div>
</section>
<section id="b-logical-and-objective-bayesian-interpretations">
<h4>B. Logical and Objective Bayesian Interpretations<a class="headerlink" href="#b-logical-and-objective-bayesian-interpretations" title="Link to this heading"></a></h4>
<p>These interpretations attempt to remove or minimize subjectivity from Bayesian probability by using formal principles to construct priors.</p>
<p><strong>Logical Interpretation</strong> (Keynes, Carnap): Probability is a logical relation between propositions, analogous to how deductive logic provides certain relations. The probability <span class="math notranslate nohighlight">\(P(H \mid E)\)</span> represents the degree to which evidence <span class="math notranslate nohighlight">\(E\)</span> logically supports hypothesis <span class="math notranslate nohighlight">\(H\)</span>, independent of any particular person’s beliefs.</p>
<p><strong>Objective Bayesian Interpretation</strong> (Jaynes, Jeffreys): Use formal principles like <strong>maximum entropy</strong> or <strong>symmetry requirements</strong> to construct “non-informative” or “reference” priors that represent ignorance in a principled, objective way. The goal is priors that any rational person would agree on given the same state of information.</p>
<p>For example, Jeffreys’ prior uses the Fisher information (discussed in Chapter 3) to define a prior invariant under reparameterization:</p>
<div class="math notranslate nohighlight">
\[\pi(\theta) \propto \sqrt{\det I(\theta)}\]</div>
<p>where <span class="math notranslate nohighlight">\(I(\theta)\)</span> is the Fisher information matrix.</p>
<p><strong>Key Insight</strong>: Even these “objective” approaches yield probabilities conditional on assumptions (choice of reference class, parameterization, symmetry principles). They represent <strong>conditional objectivity</strong>—objective given the framework, but the framework itself involves choices.</p>
</section>
</section>
<section id="summary-of-interpretations">
<h3>Summary of Interpretations<a class="headerlink" href="#summary-of-interpretations" title="Link to this heading"></a></h3>
<p>The landscape of probability interpretations:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 20.0%" />
<col style="width: 25.0%" />
<col style="width: 30.0%" />
<col style="width: 25.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><strong>Interpretation</strong></p></th>
<th class="head"><p><strong>Type</strong></p></th>
<th class="head"><p><strong>Core Idea</strong></p></th>
<th class="head"><p><strong>Key Limitation</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Frequentist</p></td>
<td><p>Objective</p></td>
<td><p>Long-run relative frequency</p></td>
<td><p>Cannot handle unique events or parameters</p></td>
</tr>
<tr class="row-odd"><td><p>Propensity</p></td>
<td><p>Objective</p></td>
<td><p>Physical disposition/tendency</p></td>
<td><p>Conceptually vague, hard to measure</p></td>
</tr>
<tr class="row-even"><td><p>Subjective Bayesian</p></td>
<td><p>Epistemic</p></td>
<td><p>Personal degree of belief</p></td>
<td><p>Requires subjective priors</p></td>
</tr>
<tr class="row-odd"><td><p>Logical</p></td>
<td><p>Epistemic</p></td>
<td><p>Evidential support relation</p></td>
<td><p>Still requires assumptions</p></td>
</tr>
<tr class="row-even"><td><p>Objective Bayesian</p></td>
<td><p>Epistemic</p></td>
<td><p>Principled ignorance</p></td>
<td><p>“Objectivity” is conditional</p></td>
</tr>
</tbody>
</table>
<div class="important admonition">
<p class="admonition-title">Key Insight 🔑</p>
<p>All interpretations agree on the mathematical rules (Kolmogorov’s axioms) but profoundly disagree on what probability means. This philosophical difference has enormous practical implications for statistical inference, computational methods, and how we interpret results in data science applications.</p>
</div>
</section>
</section>
<section id="statistical-inference-paradigms">
<h2>Statistical Inference Paradigms<a class="headerlink" href="#statistical-inference-paradigms" title="Link to this heading"></a></h2>
<p>Statistical inference is the process of drawing conclusions about populations, processes, or hypotheses from observed data. Different interpretations of probability naturally lead to radically different approaches to inference. We examine three major paradigms that shape modern statistical practice.</p>
<section id="frequentist-inference-classical-approach">
<h3>Frequentist Inference (Classical Approach)<a class="headerlink" href="#frequentist-inference-classical-approach" title="Link to this heading"></a></h3>
<p>Frequentist inference aligns with the frequentist interpretation of probability. <strong>Parameters are fixed but unknown constants</strong>; randomness enters only through the sampling process. Inference focuses on procedures that have good long-run frequency properties across repeated samples.</p>
<section id="key-principles">
<h4>Key Principles<a class="headerlink" href="#key-principles" title="Link to this heading"></a></h4>
<ol class="arabic simple">
<li><p><strong>Parameters are fixed</strong>: Population quantities like the mean <span class="math notranslate nohighlight">\(\mu\)</span> or proportion <span class="math notranslate nohighlight">\(p\)</span> have true, fixed values—they are not random variables.</p></li>
<li><p><strong>Data are random</strong>: The sample <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> is randomly drawn from a population; the randomness is in the sampling process.</p></li>
<li><p><strong>Procedures have frequency properties</strong>: We evaluate inference methods by how they perform across hypothetical repeated samples from the same population.</p></li>
</ol>
</section>
<section id="core-methods">
<h4>Core Methods<a class="headerlink" href="#core-methods" title="Link to this heading"></a></h4>
<p><strong>A. Point Estimation</strong></p>
<p>A <strong>point estimator</strong> <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is a function of the data used to estimate parameter <span class="math notranslate nohighlight">\(\theta\)</span>. We evaluate estimators by their properties across repeated samples:</p>
<ul class="simple">
<li><p><strong>Unbiased</strong>: <span class="math notranslate nohighlight">\(E[\hat{\theta}] = \theta\)</span> (on average, the estimator equals the true parameter)</p></li>
<li><p><strong>Consistent</strong>: <span class="math notranslate nohighlight">\(\hat{\theta} \xrightarrow{P} \theta\)</span> as <span class="math notranslate nohighlight">\(n \to \infty\)</span> (converges to truth with more data)</p></li>
<li><p><strong>Efficient</strong>: Among unbiased estimators, has minimum variance</p></li>
</ul>
<p>The <strong>maximum likelihood estimator (MLE)</strong> is frequently used because it has optimal asymptotic properties:</p>
<div class="math notranslate nohighlight">
\[\hat{\theta}_{\text{MLE}} = \arg\max_{\theta} L(\theta; \text{data}) = \arg\max_{\theta} \prod_{i=1}^{n} f(x_i \mid \theta)\]</div>
<p>where <span class="math notranslate nohighlight">\(f(x \mid \theta)\)</span> is the probability density/mass function of the data given <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p><strong>B. Confidence Intervals</strong></p>
<p>A <span class="math notranslate nohighlight">\(100(1-\alpha)\%\)</span> <strong>confidence interval</strong> is an interval <span class="math notranslate nohighlight">\([\hat{\theta}_L, \hat{\theta}_U]\)</span> constructed so that if we repeated the sampling procedure many times and computed the interval each time, approximately <span class="math notranslate nohighlight">\(100(1-\alpha)\%\)</span> of those intervals would contain the true parameter <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>Mathematically:</p>
<div class="math notranslate nohighlight">
\[P(\hat{\theta}_L(\mathbf{X}) &lt; \theta &lt; \hat{\theta}_U(\mathbf{X})) = 1 - \alpha\]</div>
<p>where the probability is over the random sample <span class="math notranslate nohighlight">\(\mathbf{X} = (X_1, \ldots, X_n)\)</span>.</p>
<p><strong>Critical interpretation point</strong>: The parameter <span class="math notranslate nohighlight">\(\theta\)</span> is fixed—it’s not random and doesn’t have a probability distribution. The interval <span class="math notranslate nohighlight">\((\hat{\theta}_L, \hat{\theta}_U)\)</span> is random because it depends on the random sample. Once we observe a specific sample and compute a specific interval, that interval either contains <span class="math notranslate nohighlight">\(\theta\)</span> or it doesn’t—we cannot say “there is a 95% probability <span class="math notranslate nohighlight">\(\theta\)</span> is in this specific interval” in the frequentist framework.</p>
<p><strong>C. Hypothesis Testing</strong></p>
<p>Test a <strong>null hypothesis</strong> <span class="math notranslate nohighlight">\(H_0\)</span> against an <strong>alternative hypothesis</strong> <span class="math notranslate nohighlight">\(H_a\)</span> using the following procedure:</p>
<ol class="arabic simple">
<li><p>Choose a <strong>test statistic</strong> <span class="math notranslate nohighlight">\(T(\mathbf{X})\)</span> that measures deviation from <span class="math notranslate nohighlight">\(H_0\)</span></p></li>
<li><p>Determine the <strong>sampling distribution</strong> of <span class="math notranslate nohighlight">\(T\)</span> assuming <span class="math notranslate nohighlight">\(H_0\)</span> is true</p></li>
<li><p>Calculate the <strong>p-value</strong>: <span class="math notranslate nohighlight">\(P(T \geq t_{\text{obs}} \mid H_0)\)</span> where <span class="math notranslate nohighlight">\(t_{\text{obs}}\)</span> is the observed test statistic</p></li>
<li><p>Compare to a pre-specified <strong>significance level</strong> <span class="math notranslate nohighlight">\(\alpha\)</span> (commonly 0.05)</p></li>
</ol>
<p><strong>Interpretation of p-value</strong>: The p-value is the probability of observing a test statistic as extreme or more extreme than what we observed, <strong>if the null hypothesis were true</strong>. It is <strong>not</strong> the probability that <span class="math notranslate nohighlight">\(H_0\)</span> is true, nor is it the probability that <span class="math notranslate nohighlight">\(H_a\)</span> is false.</p>
<p>Mathematically: <span class="math notranslate nohighlight">\(p = P(\text{Data as or more extreme} \mid H_0 \text{ true})\)</span></p>
<p><strong>Not</strong>: <span class="math notranslate nohighlight">\(P(H_0 \text{ true} \mid \text{Data})\)</span> ← This would be the Bayesian posterior probability.</p>
</section>
<section id="neyman-pearson-vs-fisherian-approaches">
<h4>Neyman-Pearson vs. Fisherian Approaches<a class="headerlink" href="#neyman-pearson-vs-fisherian-approaches" title="Link to this heading"></a></h4>
<p>Within frequentist inference, two sub-schools have different emphases:</p>
<p><strong>Neyman-Pearson Framework</strong> (Jerzy Neyman, Egon Pearson, 1930s):</p>
<ul class="simple">
<li><p>Formalize hypothesis testing as a decision problem</p></li>
<li><p>Control <strong>Type I error</strong> <span class="math notranslate nohighlight">\(\alpha = P(\text{Reject } H_0 \mid H_0 \text{ true})\)</span> and <strong>Type II error</strong> <span class="math notranslate nohighlight">\(\beta = P(\text{Fail to reject } H_0 \mid H_0 \text{ false})\)</span></p></li>
<li><p><strong>Power</strong> <span class="math notranslate nohighlight">\(= 1 - \beta = P(\text{Reject } H_0 \mid H_a \text{ true})\)</span></p></li>
<li><p>Optimize test procedures to maximize power for fixed <span class="math notranslate nohighlight">\(\alpha\)</span></p></li>
<li><p>View testing as leading to a binary decision: reject or fail to reject</p></li>
</ul>
<p><strong>Fisherian Framework</strong> (Ronald Fisher, 1920s-1950s):</p>
<ul class="simple">
<li><p>Emphasize the <strong>p-value</strong> as a continuous measure of evidence against <span class="math notranslate nohighlight">\(H_0\)</span></p></li>
<li><p>Less rigid about fixed <span class="math notranslate nohighlight">\(\alpha\)</span> levels; p-value interpreted as strength of evidence</p></li>
<li><p>Focus on the <strong>likelihood function</strong> as containing all information data provide about parameters</p></li>
<li><p>View testing as evidence assessment rather than decision-making</p></li>
<li><p>More flexible, less formalized than Neyman-Pearson</p></li>
</ul>
<p>Modern practice often blends both approaches, sometimes inconsistently.</p>
<div class="note admonition">
<p class="admonition-title">Example 💡: Two-Sample T-Test (Frequentist Framework)</p>
<p><strong>Given:</strong> Two groups of measurements. Group A (treatment): [5.1, 5.5, 5.3, 5.7, 5.4, 5.6] and Group B (control): [4.8, 4.9, 5.0, 4.7, 4.9, 5.1].</p>
<p><strong>Find:</strong> Test whether the treatment increases the mean response using a one-sided t-test at <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span>.</p>
<p><strong>Mathematical approach:</strong></p>
<ul class="simple">
<li><p>Null hypothesis: <span class="math notranslate nohighlight">\(H_0: \mu_A = \mu_B\)</span> (or <span class="math notranslate nohighlight">\(\mu_A - \mu_B = 0\)</span>)</p></li>
<li><p>Alternative: <span class="math notranslate nohighlight">\(H_a: \mu_A &gt; \mu_B\)</span> (treatment increases mean)</p></li>
<li><p>Test statistic (assuming equal variances):</p></li>
</ul>
<div class="math notranslate nohighlight">
\[t = \frac{\bar{X}_A - \bar{X}_B}{s_p \sqrt{1/n_A + 1/n_B}}\]</div>
<p>where <span class="math notranslate nohighlight">\(s_p\)</span> is the pooled standard deviation:</p>
<div class="math notranslate nohighlight">
\[s_p = \sqrt{\frac{(n_A - 1)s_A^2 + (n_B - 1)s_B^2}{n_A + n_B - 2}}\]</div>
<p>Under <span class="math notranslate nohighlight">\(H_0\)</span>, <span class="math notranslate nohighlight">\(t\)</span> follows a t-distribution with <span class="math notranslate nohighlight">\(n_A + n_B - 2\)</span> degrees of freedom.</p>
<p><strong>Python implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Data</span>
<span class="n">group_a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">5.1</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">,</span> <span class="mf">5.3</span><span class="p">,</span> <span class="mf">5.7</span><span class="p">,</span> <span class="mf">5.4</span><span class="p">,</span> <span class="mf">5.6</span><span class="p">])</span>
<span class="n">group_b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">4.8</span><span class="p">,</span> <span class="mf">4.9</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">4.7</span><span class="p">,</span> <span class="mf">4.9</span><span class="p">,</span> <span class="mf">5.1</span><span class="p">])</span>

<span class="c1"># Summary statistics</span>
<span class="n">n_a</span><span class="p">,</span> <span class="n">n_b</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">group_a</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">group_b</span><span class="p">)</span>
<span class="n">mean_a</span><span class="p">,</span> <span class="n">mean_b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">group_a</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">group_b</span><span class="p">)</span>
<span class="n">std_a</span><span class="p">,</span> <span class="n">std_b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">group_a</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">group_b</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Pooled standard deviation</span>
<span class="n">s_p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(((</span><span class="n">n_a</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">std_a</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">n_b</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">std_b</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_a</span> <span class="o">+</span> <span class="n">n_b</span> <span class="o">-</span> <span class="mi">2</span><span class="p">))</span>

<span class="c1"># Test statistic</span>
<span class="n">t_stat</span> <span class="o">=</span> <span class="p">(</span><span class="n">mean_a</span> <span class="o">-</span> <span class="n">mean_b</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">s_p</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">n_a</span> <span class="o">+</span> <span class="mi">1</span><span class="o">/</span><span class="n">n_b</span><span class="p">))</span>

<span class="c1"># Degrees of freedom</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">n_a</span> <span class="o">+</span> <span class="n">n_b</span> <span class="o">-</span> <span class="mi">2</span>

<span class="c1"># One-sided p-value (alternative: mean_a &gt; mean_b)</span>
<span class="n">p_value</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">sf</span><span class="p">(</span><span class="n">t_stat</span><span class="p">,</span> <span class="n">df</span><span class="p">)</span>

<span class="c1"># Using scipy&#39;s built-in function with matching assumptions</span>
<span class="n">t_stat_scipy</span><span class="p">,</span> <span class="n">p_value_scipy</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">ttest_ind</span><span class="p">(</span>
    <span class="n">group_a</span><span class="p">,</span> <span class="n">group_b</span><span class="p">,</span> <span class="n">equal_var</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alternative</span><span class="o">=</span><span class="s1">&#39;greater&#39;</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample means: A = </span><span class="si">{</span><span class="n">mean_a</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, B = </span><span class="si">{</span><span class="n">mean_b</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean difference: </span><span class="si">{</span><span class="n">mean_a</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">mean_b</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Pooled SD: </span><span class="si">{</span><span class="n">s_p</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test statistic: t = </span><span class="si">{</span><span class="n">t_stat</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Degrees of freedom: </span><span class="si">{</span><span class="n">df</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;One-sided p-value: </span><span class="si">{</span><span class="n">p_value</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;SciPy p-value (verification): </span><span class="si">{</span><span class="n">p_value_scipy</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Decision at α=0.05: </span><span class="si">{</span><span class="s1">&#39;Reject H₀&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">p_value</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mf">0.05</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;Fail to reject H₀&#39;</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 95% confidence interval for difference in means</span>
<span class="n">t_critical</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">df</span><span class="p">)</span>
<span class="n">se_diff</span> <span class="o">=</span> <span class="n">s_p</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">n_a</span> <span class="o">+</span> <span class="mi">1</span><span class="o">/</span><span class="n">n_b</span><span class="p">)</span>
<span class="n">ci_lower</span> <span class="o">=</span> <span class="p">(</span><span class="n">mean_a</span> <span class="o">-</span> <span class="n">mean_b</span><span class="p">)</span> <span class="o">-</span> <span class="n">t_critical</span> <span class="o">*</span> <span class="n">se_diff</span>
<span class="n">ci_upper</span> <span class="o">=</span> <span class="p">(</span><span class="n">mean_a</span> <span class="o">-</span> <span class="n">mean_b</span><span class="p">)</span> <span class="o">+</span> <span class="n">t_critical</span> <span class="o">*</span> <span class="n">se_diff</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">95% CI for mean difference: [</span><span class="si">{</span><span class="n">ci_lower</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_upper</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Result:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Sample means: A = 5.433, B = 4.900
Mean difference: 0.533
Pooled SD: 0.278
Test statistic: t = 3.319
Degrees of freedom: 10
One-sided p-value: 0.0039
SciPy p-value (verification): 0.0039

Decision at α=0.05: Reject H₀

95% CI for mean difference: [0.276, 0.791]
</pre></div>
</div>
<p><strong>Frequentist interpretation</strong>: “If the null hypothesis (no treatment effect) were true, we would observe a difference in means as large as 0.533 or larger in only 0.39% of repeated samples. This provides strong evidence against <span class="math notranslate nohighlight">\(H_0\)</span>, so we reject it at <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span>.”</p>
<p><strong>What we do NOT say</strong>: “There is a 99.61% probability the treatment works” or “There is a 0.39% probability the null is true.” In the frequentist framework, hypotheses are not random—they are either true or false. Only data have probability distributions.</p>
<p><strong>Confidence interval interpretation</strong>: “If we repeated this study many times and calculated 95% CIs each time, approximately 95% of those intervals would contain the true mean difference.” Once we have a specific interval [0.276, 0.791], it either contains the true difference or it doesn’t—we don’t assign a probability.</p>
</div>
</section>
<section id="strengths-and-limitations-of-frequentist-inference">
<h4>Strengths and Limitations of Frequentist Inference<a class="headerlink" href="#strengths-and-limitations-of-frequentist-inference" title="Link to this heading"></a></h4>
<p><strong>Strengths</strong>:</p>
<ul class="simple">
<li><p><strong>Objective</strong>: No need for subjective prior beliefs about parameters</p></li>
<li><p><strong>Well-established</strong>: Extensive theory with clear guarantees on error rates and convergence properties</p></li>
<li><p><strong>Regulatory acceptance</strong>: Standard in clinical trials, FDA submissions, and scientific publications</p></li>
<li><p><strong>Computational simplicity</strong>: Many standard procedures have closed-form solutions or simple algorithms</p></li>
<li><p><strong>Long track record</strong>: Decades of successful application across sciences</p></li>
</ul>
<p><strong>Limitations</strong>:</p>
<ul class="simple">
<li><p><strong>No probability statements about parameters</strong>: Cannot say “probability that <span class="math notranslate nohighlight">\(\mu &gt; 0$&quot; since :math:\)</span>mu` is fixed</p></li>
<li><p><strong>Confidence intervals often misinterpreted</strong>: Frequently confused with Bayesian credible intervals</p></li>
<li><p><strong>P-values widely misunderstood</strong>: Not the probability of hypothesis, often leads to binary thinking</p></li>
<li><p><strong>Cannot incorporate prior information formally</strong>: Expert knowledge must be handled informally</p></li>
<li><p><strong>Stopping rules matter</strong>: Results depend on whether you planned to sample <span class="math notranslate nohighlight">\(n\)</span> observations or sample until seeing <span class="math notranslate nohighlight">\(k\)</span> events (violates likelihood principle)</p></li>
<li><p><strong>Counterintuitive results</strong>: Optional stopping and sequential testing can produce seemingly paradoxical results</p></li>
</ul>
</section>
</section>
<section id="bayesian-inference">
<h3>Bayesian Inference<a class="headerlink" href="#bayesian-inference" title="Link to this heading"></a></h3>
<p>Bayesian inference treats <strong>all unknowns as random variables</strong> with probability distributions. It aligns with the subjective (or objective Bayesian) interpretation where probability represents degree of belief or uncertainty.</p>
<section id="id5">
<h4>Key Principles<a class="headerlink" href="#id5" title="Link to this heading"></a></h4>
<ol class="arabic simple">
<li><p><strong>Parameters are random</strong>: We assign probability distributions to unknown parameters representing our uncertainty</p></li>
<li><p><strong>Update beliefs with data</strong>: Bayes’ theorem mechanically converts prior beliefs into posterior beliefs after observing data</p></li>
<li><p><strong>Probability as belief</strong>: We make direct probability statements about anything unknown—parameters, hypotheses, predictions</p></li>
</ol>
</section>
<section id="bayes-theorem">
<h4>Bayes’ Theorem<a class="headerlink" href="#bayes-theorem" title="Link to this heading"></a></h4>
<p>The foundation of all Bayesian inference is Bayes’ theorem applied to parameters:</p>
<div class="math notranslate nohighlight">
\[P(\theta \mid \mathbf{X}) = \frac{P(\mathbf{X} \mid \theta) \cdot P(\theta)}{P(\mathbf{X})}\]</div>
<p>Or more intuitively:</p>
<div class="math notranslate nohighlight">
\[\underbrace{P(\theta \mid \mathbf{X})}_{\text{Posterior}} = \frac{\overbrace{P(\mathbf{X} \mid \theta)}^{\text{Likelihood}} \times \overbrace{P(\theta)}^{\text{Prior}}}{\underbrace{P(\mathbf{X})}_{\text{Evidence}}}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><strong>Prior</strong> <span class="math notranslate nohighlight">\(P(\theta)\)</span>: Our belief about <span class="math notranslate nohighlight">\(\theta\)</span> before seeing data, representing prior knowledge</p></li>
<li><p><strong>Likelihood</strong> <span class="math notranslate nohighlight">\(P(\mathbf{X} \mid \theta)\)</span>: How probable the observed data are for each parameter value</p></li>
<li><p><strong>Posterior</strong> <span class="math notranslate nohighlight">\(P(\theta \mid \mathbf{X})\)</span>: Our updated belief about <span class="math notranslate nohighlight">\(\theta\)</span> after incorporating the data</p></li>
<li><p><strong>Evidence</strong> (marginal likelihood) <span class="math notranslate nohighlight">\(P(\mathbf{X}) = \int P(\mathbf{X} \mid \theta) P(\theta) \, d\theta\)</span>: Normalizing constant ensuring posterior integrates to 1</p></li>
</ul>
<p>Since the evidence doesn’t depend on <span class="math notranslate nohighlight">\(\theta\)</span>, we often write:</p>
<div class="math notranslate nohighlight">
\[P(\theta \mid \mathbf{X}) \propto P(\mathbf{X} \mid \theta) \cdot P(\theta)\]</div>
<p>The posterior is proportional to likelihood times prior.</p>
<figure class="align-center" id="id18">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/Part1/ch1_1_bayesian_updating.png"><img alt="Three-panel visualization showing Beta(2,2) prior, binomial likelihood for 7 successes in 10 trials, and resulting Beta(9,5) posterior with 95% credible interval" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/Part1/ch1_1_bayesian_updating.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 1.6 </span><span class="caption-text"><strong>Bayesian updating with conjugate priors.</strong> Starting with a Beta(2, 2) prior (mild belief that <span class="math notranslate nohighlight">\(\theta \approx 0.5\)</span>), we observe 7 successes in 10 trials. The likelihood peaks at the MLE <span class="math notranslate nohighlight">\(\hat{\theta} = 0.7\)</span>. The posterior Beta(9, 5) is a compromise between prior and data, with a 95% credible interval of [0.39, 0.86].</span><a class="headerlink" href="#id18" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="id6">
<h4>Core Methods<a class="headerlink" href="#id6" title="Link to this heading"></a></h4>
<p><strong>A. Point Estimation</strong></p>
<p>Multiple Bayesian point estimates exist, each with different optimality properties:</p>
<ul class="simple">
<li><p><strong>Posterior mean</strong>: <span class="math notranslate nohighlight">\(\hat{\theta} = E[\theta \mid \mathbf{X}] = \int \theta \, P(\theta \mid \mathbf{X}) \, d\theta\)</span> (minimizes squared error loss)</p></li>
<li><p><strong>Posterior mode (MAP)</strong>: <span class="math notranslate nohighlight">\(\hat{\theta} = \arg\max_{\theta} P(\theta \mid \mathbf{X})\)</span> (maximizes posterior density)</p></li>
<li><p><strong>Posterior median</strong>: 50th percentile of posterior (minimizes absolute error loss)</p></li>
</ul>
<p><strong>B. Interval Estimation (Credible Intervals)</strong></p>
<p>A <span class="math notranslate nohighlight">\(100(1-\alpha)\%\)</span> <strong>credible interval</strong> (or <strong>posterior interval</strong>) is an interval <span class="math notranslate nohighlight">\([L, U]\)</span> such that:</p>
<div class="math notranslate nohighlight">
\[P(L &lt; \theta &lt; U \mid \mathbf{X}) = 1 - \alpha\]</div>
<p><strong>Interpretation</strong>: There is a <span class="math notranslate nohighlight">\(100(1-\alpha)\%\)</span> probability that <span class="math notranslate nohighlight">\(\theta\)</span> lies in this interval, given the data and prior. This is a direct probability statement about the parameter!</p>
<figure class="align-center" id="id19">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/Part1/ch1_1_confidence_vs_credible.png"><img alt="Left panel shows 25 frequentist confidence intervals with most containing true mean; right panel shows single Bayesian posterior distribution with credible interval" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/Part1/ch1_1_confidence_vs_credible.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 1.7 </span><span class="caption-text"><strong>Frequentist vs Bayesian interval interpretation.</strong> <em>Left:</em> Each of 25 samples yields a different 95% CI; approximately 95% contain the true <span class="math notranslate nohighlight">\(\mu = 5\)</span> (blue), while ~5% miss (red). The parameter is fixed; the intervals are random. <em>Right:</em> A single Bayesian analysis produces one posterior distribution; the 95% credible interval has a direct probability interpretation: “Given the data and prior, there is 95% probability <span class="math notranslate nohighlight">\(\mu\)</span> lies in this interval.”</span><a class="headerlink" href="#id19" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Common types:
* <strong>Equal-tailed interval</strong>: <span class="math notranslate nohighlight">\(\alpha/2\)</span> probability in each tail
* <strong>Highest posterior density (HPD) interval</strong>: Shortest interval containing <span class="math notranslate nohighlight">\(1-\alpha\)</span> probability mass</p>
<p><strong>C. Hypothesis Testing and Model Comparison</strong></p>
<p><strong>Posterior probabilities</strong>: Directly compute <span class="math notranslate nohighlight">\(P(H_0 \mid \mathbf{X})\)</span> and <span class="math notranslate nohighlight">\(P(H_a \mid \mathbf{X})\)</span>.</p>
<p><strong>Bayes factors</strong>: Compare evidence for competing hypotheses/models:</p>
<div class="math notranslate nohighlight">
\[\text{BF}_{10} = \frac{P(\mathbf{X} \mid H_1)}{P(\mathbf{X} \mid H_0)} = \frac{\int P(\mathbf{X} \mid \theta, H_1) P(\theta \mid H_1) \, d\theta}{\int P(\mathbf{X} \mid \theta, H_0) P(\theta \mid H_0) \, d\theta}\]</div>
<p>The Bayes factor quantifies how many times more likely the data are under <span class="math notranslate nohighlight">\(H_1\)</span> compared to <span class="math notranslate nohighlight">\(H_0\)</span>. By Bayes’ theorem:</p>
<div class="math notranslate nohighlight">
\[\underbrace{\frac{P(H_1 \mid \mathbf{X})}{P(H_0 \mid \mathbf{X})}}_{\text{Posterior odds}} = \underbrace{\text{BF}_{10}}_{\text{Bayes factor}} \times \underbrace{\frac{P(H_1)}{P(H_0)}}_{\text{Prior odds}}\]</div>
<p>Interpretation scale (Kass &amp; Raftery, 1995):
* BF &lt; 1: Evidence for <span class="math notranslate nohighlight">\(H_0\)</span>
* 1-3: Barely worth mentioning
* 3-10: Substantial evidence for <span class="math notranslate nohighlight">\(H_1\)</span>
* 10-100: Strong evidence
* &gt; 100: Decisive evidence</p>
</section>
<section id="the-role-of-priors">
<h4>The Role of Priors<a class="headerlink" href="#the-role-of-priors" title="Link to this heading"></a></h4>
<p>Choosing a prior is crucial and sometimes controversial. Common approaches:</p>
<ul class="simple">
<li><p><strong>Informative priors</strong>: Based on previous studies, expert knowledge, physical constraints, or theoretical considerations. Example: If measuring human height, prior might be <span class="math notranslate nohighlight">\(N(170, 20^2)\)</span> cm.</p></li>
<li><p><strong>Weakly informative priors</strong>: Provide gentle regularization without dominating data. Example: <span class="math notranslate nohighlight">\(N(0, 10^2)\)</span> for a standardized effect size.</p></li>
<li><p><strong>Non-informative priors</strong>: Attempt to express ignorance objectively:
- Uniform prior: <span class="math notranslate nohighlight">\(p(\theta) \propto 1\)</span> (equal belief in all values)
- Jeffreys prior: <span class="math notranslate nohighlight">\(p(\theta) \propto \sqrt{\det I(\theta)}\)</span> (invariant under reparameterization)
- Reference priors: Maximize expected information gain from data</p></li>
</ul>
<p><strong>Key insight</strong>: With large sample sizes, the likelihood dominates and different reasonable priors yield similar posteriors. Prior choice matters most with limited data.</p>
<div class="note admonition">
<p class="admonition-title">Example 💡: Bayesian Estimation of Binomial Proportion</p>
<p><strong>Given:</strong> We observe 8 successes in 10 trials. Estimate the success probability <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p><strong>Find:</strong> Posterior distribution of <span class="math notranslate nohighlight">\(p\)</span> under uniform and informative priors.</p>
<p><strong>Mathematical approach:</strong></p>
<ul class="simple">
<li><p>Likelihood: <span class="math notranslate nohighlight">\(P(y=8 \mid p, n=10) = \binom{10}{8} p^8 (1-p)^2\)</span></p></li>
<li><p>Prior 1 (uniform): <span class="math notranslate nohighlight">\(p \sim \text{Beta}(1, 1)\)</span> (equivalent to uniform on [0,1])</p></li>
<li><p>Prior 2 (informative): <span class="math notranslate nohighlight">\(p \sim \text{Beta}(5, 5)\)</span> (skeptical, centered at 0.5)</p></li>
</ul>
<p>With a Beta prior <span class="math notranslate nohighlight">\(\text{Beta}(\alpha, \beta)\)</span> and binomial likelihood, the posterior is:</p>
<div class="math notranslate nohighlight">
\[p \mid y \sim \text{Beta}(\alpha + y, \beta + n - y)\]</div>
<p><strong>Uniform prior posterior</strong>:</p>
<div class="math notranslate nohighlight">
\[p \mid y=8 \sim \text{Beta}(1+8, 1+2) = \text{Beta}(9, 3)\]</div>
<p><strong>Informative prior posterior</strong>:</p>
<div class="math notranslate nohighlight">
\[p \mid y=8 \sim \text{Beta}(5+8, 5+2) = \text{Beta}(13, 7)\]</div>
<p><strong>Python implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Data</span>
<span class="n">y</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">10</span>

<span class="c1"># Priors</span>
<span class="n">prior_uniform</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Beta(1,1) = Uniform(0,1)</span>
<span class="n">prior_inform</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>   <span class="c1"># Skeptical prior</span>

<span class="c1"># Posteriors (conjugate update)</span>
<span class="n">posterior_uniform</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>  <span class="c1"># Beta(9, 3)</span>
<span class="n">posterior_inform</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="mi">5</span> <span class="o">+</span> <span class="n">y</span><span class="p">,</span> <span class="mi">5</span> <span class="o">+</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>   <span class="c1"># Beta(13, 7)</span>

<span class="c1"># MLE for comparison</span>
<span class="n">p_mle</span> <span class="o">=</span> <span class="n">y</span> <span class="o">/</span> <span class="n">n</span>

<span class="c1"># Posterior summaries</span>
<span class="k">def</span><span class="w"> </span><span class="nf">summarize_beta</span><span class="p">(</span><span class="n">dist</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="c1"># Mode formula (α-1)/(α+β-2) valid only for α,β &gt; 1</span>
    <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">dist</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">mode</span> <span class="o">=</span> <span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">dist</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">mode</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
    <span class="n">median</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">median</span><span class="p">()</span>
    <span class="n">ci</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">interval</span><span class="p">(</span><span class="mf">0.95</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean: </span><span class="si">{</span><span class="n">mean</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mode: </span><span class="si">{</span><span class="n">mode</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Median: </span><span class="si">{</span><span class="n">median</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  95% Credible Interval: [</span><span class="si">{</span><span class="n">ci</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="n">ci</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;BAYESIAN BINOMIAL PROPORTION ESTIMATION&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Observed: </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s2"> successes in </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2"> trials&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MLE (Frequentist): </span><span class="si">{</span><span class="n">p_mle</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">mean_u</span><span class="p">,</span> <span class="n">ci_u</span> <span class="o">=</span> <span class="n">summarize_beta</span><span class="p">(</span><span class="n">posterior_uniform</span><span class="p">,</span> <span class="s2">&quot;Posterior (Uniform Prior)&quot;</span><span class="p">)</span>
<span class="n">mean_i</span><span class="p">,</span> <span class="n">ci_i</span> <span class="o">=</span> <span class="n">summarize_beta</span><span class="p">(</span><span class="n">posterior_inform</span><span class="p">,</span> <span class="s2">&quot;Posterior (Informative Prior)&quot;</span><span class="p">)</span>

<span class="c1"># Probability that p &gt; 0.5</span>
<span class="n">prob_gt_half_u</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">posterior_uniform</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">prob_gt_half_i</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">posterior_inform</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">P(p &gt; 0.5 | data, uniform prior): </span><span class="si">{</span><span class="n">prob_gt_half_u</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(p &gt; 0.5 | data, informative prior): </span><span class="si">{</span><span class="n">prob_gt_half_i</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Result:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">==================================================</span>
<span class="n">BAYESIAN</span> <span class="n">BINOMIAL</span> <span class="n">PROPORTION</span> <span class="n">ESTIMATION</span>
<span class="o">==================================================</span>
<span class="n">Observed</span><span class="p">:</span> <span class="mi">8</span> <span class="n">successes</span> <span class="ow">in</span> <span class="mi">10</span> <span class="n">trials</span>
<span class="n">MLE</span> <span class="p">(</span><span class="n">Frequentist</span><span class="p">):</span> <span class="mf">0.8000</span>

<span class="n">Posterior</span> <span class="p">(</span><span class="n">Uniform</span> <span class="n">Prior</span><span class="p">):</span>
  <span class="n">Mean</span><span class="p">:</span> <span class="mf">0.7500</span>
  <span class="n">Mode</span><span class="p">:</span> <span class="mf">0.8000</span>
  <span class="n">Median</span><span class="p">:</span> <span class="mf">0.7528</span>
  <span class="mi">95</span><span class="o">%</span> <span class="n">Credible</span> <span class="n">Interval</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.4822</span><span class="p">,</span> <span class="mf">0.9451</span><span class="p">]</span>

<span class="n">Posterior</span> <span class="p">(</span><span class="n">Informative</span> <span class="n">Prior</span><span class="p">):</span>
  <span class="n">Mean</span><span class="p">:</span> <span class="mf">0.6500</span>
  <span class="n">Mode</span><span class="p">:</span> <span class="mf">0.6667</span>
  <span class="n">Median</span><span class="p">:</span> <span class="mf">0.6508</span>
  <span class="mi">95</span><span class="o">%</span> <span class="n">Credible</span> <span class="n">Interval</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.4483</span><span class="p">,</span> <span class="mf">0.8289</span><span class="p">]</span>

<span class="n">P</span><span class="p">(</span><span class="n">p</span> <span class="o">&gt;</span> <span class="mf">0.5</span> <span class="o">|</span> <span class="n">data</span><span class="p">,</span> <span class="n">uniform</span> <span class="n">prior</span><span class="p">):</span> <span class="mf">0.9453</span>
<span class="n">P</span><span class="p">(</span><span class="n">p</span> <span class="o">&gt;</span> <span class="mf">0.5</span> <span class="o">|</span> <span class="n">data</span><span class="p">,</span> <span class="n">informative</span> <span class="n">prior</span><span class="p">):</span> <span class="mf">0.8687</span>
</pre></div>
</div>
<p><strong>Bayesian interpretation</strong>:</p>
<ul class="simple">
<li><p>With a uniform prior, the posterior mean is 0.75 and there’s a 94.5% probability that <span class="math notranslate nohighlight">\(p &gt; 0.5\)</span>.</p></li>
<li><p>With a skeptical prior centered at 0.5, the posterior mean is 0.65 (pulled toward prior) and there’s an 86.9% probability that <span class="math notranslate nohighlight">\(p &gt; 0.5\)</span>.</p></li>
<li><p>The 95% credible interval [0.4822, 0.9451] under the uniform prior means: “There is a 95% probability that the true success rate lies between 48.2% and 94.5%, given the data and prior.”</p></li>
</ul>
<p><strong>Prior influence</strong>: The informative prior “shrinks” estimates toward 0.5. With only 10 observations, prior choice substantially affects conclusions. With 100 observations (80 successes), posteriors would converge regardless of prior.</p>
</div>
</section>
<section id="strengths-and-limitations-of-bayesian-inference">
<h4>Strengths and Limitations of Bayesian Inference<a class="headerlink" href="#strengths-and-limitations-of-bayesian-inference" title="Link to this heading"></a></h4>
<p><strong>Strengths</strong>:</p>
<ul class="simple">
<li><p><strong>Direct probability statements</strong>: Can say “95% probability that :math:<a href="#id7"><span class="problematic" id="id8">`</span></a>theta in [a,b]$” and “probability hypothesis is true”</p></li>
<li><p><strong>Incorporates prior information</strong>: Formally combines expert knowledge with data</p></li>
<li><p><strong>Coherent framework</strong>: Bayes’ theorem provides unique principled way to update beliefs</p></li>
<li><p><strong>Sequential analysis</strong>: Stopping rules don’t matter (likelihood principle satisfied)</p></li>
<li><p><strong>Small sample performance</strong>: Often better than asymptotic frequentist methods with limited data</p></li>
<li><p><strong>Hierarchical modeling</strong>: Natural framework for complex multilevel models</p></li>
<li><p><strong>Handles uncertainty</strong>: Propagates uncertainty throughout analysis via probability distributions</p></li>
</ul>
<p><strong>Limitations</strong>:</p>
<ul class="simple">
<li><p><strong>Prior specification required</strong>: Must choose prior, which may be subjective or controversial</p></li>
<li><p><strong>Prior sensitivity</strong>: With limited data, conclusions can depend strongly on prior</p></li>
<li><p><strong>Computational intensity</strong>: Often requires MCMC or other sophisticated algorithms (though this improves constantly)</p></li>
<li><p><strong>Less regulatory acceptance</strong>: Some fields (clinical trials, FDA) prefer frequentist guarantees</p></li>
<li><p><strong>Interpretational commitment</strong>: Must accept probability-as-degree-of-belief framework</p></li>
</ul>
</section>
</section>
<section id="likelihood-based-inference">
<h3>Likelihood-Based Inference<a class="headerlink" href="#likelihood-based-inference" title="Link to this heading"></a></h3>
<p>The <strong>likelihood function</strong> <span class="math notranslate nohighlight">\(L(\theta \mid \mathbf{X})\)</span> plays a central role in both frequentist (MLE) and Bayesian (Bayes’ theorem) inference. Some statisticians, following Fisher and Birnbaum, argue that the likelihood itself contains all the information the data provide about parameters.</p>
<section id="the-likelihood-principle">
<h4>The Likelihood Principle<a class="headerlink" href="#the-likelihood-principle" title="Link to this heading"></a></h4>
<p><strong>Likelihood Principle</strong>: If two datasets produce proportional likelihood functions for parameter <span class="math notranslate nohighlight">\(\theta\)</span>, then they contain the same evidence about <span class="math notranslate nohighlight">\(\theta\)</span> and should lead to the same inference.</p>
<p>Formally, if <span class="math notranslate nohighlight">\(L_1(\theta \mid \mathbf{X}_1) \propto L_2(\theta \mid \mathbf{X}_2)\)</span> for all <span class="math notranslate nohighlight">\(\theta\)</span>, then <span class="math notranslate nohighlight">\(\mathbf{X}_1\)</span> and <span class="math notranslate nohighlight">\(\mathbf{X}_2\)</span> provide equivalent information about <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>This leads to focusing on <strong>likelihood ratios</strong> as the measure of evidence:</p>
<div class="math notranslate nohighlight">
\[\text{LR}(\theta_1 : \theta_2) = \frac{L(\theta_1 \mid \mathbf{X})}{L(\theta_2 \mid \mathbf{X})}\]</div>
<p>The likelihood ratio quantifies how many times more consistent the data are with <span class="math notranslate nohighlight">\(\theta_1\)</span> versus <span class="math notranslate nohighlight">\(\theta_2\)</span>.</p>
<p><strong>Key implication</strong>: Inference should depend only on the observed data through the likelihood function, not on:
* The sampling plan or stopping rule
* Data that could have been observed but weren’t
* Whether the experiment was planned or opportunistic</p>
<p><strong>A Classic Example: Negative Binomial vs. Binomial Sampling</strong></p>
<p>Consider two coin-flipping experiments that yield identical data but different sampling plans:</p>
<ul class="simple">
<li><p><strong>Experiment A</strong> (Negative Binomial): Flip until you observe <span class="math notranslate nohighlight">\(h=3\)</span> heads. It takes <span class="math notranslate nohighlight">\(t=10\)</span> total flips (7 tails).</p></li>
<li><p><strong>Experiment B</strong> (Binomial): Flip exactly <span class="math notranslate nohighlight">\(n=10\)</span> times. You observe <span class="math notranslate nohighlight">\(h=3\)</span> heads (7 tails).</p></li>
</ul>
<p>The observed data are identical: 3 heads in 10 flips.</p>
<p><strong>Likelihood functions</strong>:</p>
<p>For negative binomial sampling with <span class="math notranslate nohighlight">\(h\)</span> heads in <span class="math notranslate nohighlight">\(t\)</span> total flips:</p>
<div class="math notranslate nohighlight">
\[L_{\text{NB}}(p) = \binom{t-1}{h-1} p^h (1-p)^{t-h} \propto p^3 (1-p)^7\]</div>
<p>For binomial sampling with <span class="math notranslate nohighlight">\(h\)</span> heads in <span class="math notranslate nohighlight">\(n\)</span> flips:</p>
<div class="math notranslate nohighlight">
\[L_{\text{Bin}}(p) = \binom{n}{h} p^h (1-p)^{n-h} \propto p^3 (1-p)^7\]</div>
<p>The likelihoods are <strong>proportional</strong> (differ only by constants independent of <span class="math notranslate nohighlight">\(p\)</span>), so by the likelihood principle, they provide identical evidence about <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p><strong>Adherence</strong>:
* <strong>Bayesian inference</strong>: Automatically satisfies the likelihood principle (posterior depends only on observed data via likelihood). Both experiments yield the same posterior: <span class="math notranslate nohighlight">\(p \mid \text{data} \sim \text{Beta}(3+\alpha, 7+\beta)\)</span> for prior <span class="math notranslate nohighlight">\(\text{Beta}(\alpha, \beta)\)</span>.
* <strong>Frequentist inference</strong>: Violates the likelihood principle. P-values and confidence intervals differ because they depend on the sample space—events that didn’t occur but could have under the sampling plan.</p>
</section>
<section id="relationship-to-other-paradigms">
<h4>Relationship to Other Paradigms<a class="headerlink" href="#relationship-to-other-paradigms" title="Link to this heading"></a></h4>
<ul class="simple">
<li><p><strong>In frequentist inference</strong>: Maximum likelihood estimation (MLE) is widely used, and the likelihood ratio test is a standard method. However, inference procedures (p-values, confidence intervals) depend on the sampling distribution, not just the likelihood, violating the likelihood principle.</p></li>
<li><p><strong>In Bayesian inference</strong>: The likelihood principle is automatically satisfied—the posterior <span class="math notranslate nohighlight">\(P(\theta \mid \mathbf{X}) \propto L(\theta \mid \mathbf{X}) \cdot P(\theta)\)</span> depends on the data only through the observed likelihood.</p></li>
<li><p><strong>Pure likelihoodism</strong>: Rarely used as a complete inference framework, but likelihood ratios provide a useful measure of relative evidence that doesn’t require priors (Bayesian) or reference to hypothetical repetitions (frequentist).</p></li>
</ul>
</section>
</section>
</section>
<section id="historical-and-philosophical-debates">
<h2>Historical and Philosophical Debates<a class="headerlink" href="#historical-and-philosophical-debates" title="Link to this heading"></a></h2>
<p>The different interpretations of probability and approaches to inference have sparked intense, sometimes acrimonious debates in statistics for over a century. Understanding this history helps us appreciate modern pragmatic synthesis.</p>
<section id="the-bayesian-vs-frequentist-divide">
<h3>The Bayesian vs. Frequentist Divide<a class="headerlink" href="#the-bayesian-vs-frequentist-divide" title="Link to this heading"></a></h3>
<section id="historical-evolution">
<h4>Historical Evolution<a class="headerlink" href="#historical-evolution" title="Link to this heading"></a></h4>
<ul class="simple">
<li><p><strong>18th-19th century</strong>: Bayesian methods (called “inverse probability”) were standard. Laplace routinely used uniform priors. Bayes’ theorem (1763) and Laplace’s refinements (1812) dominated statistical thinking.</p></li>
<li><p><strong>Early 20th century</strong>: Bayesian methods fell from favor:
- Concerns about subjectivity of priors
- Lack of mathematical rigor in prior selection
- Rise of frequentist philosophy emphasizing objectivity
- Fisher, Neyman, Pearson developed alternative framework</p></li>
<li><p><strong>Mid 20th century</strong>: Frequentist dominance
- R.A. Fisher’s maximum likelihood (1912-1956)
- Neyman-Pearson hypothesis testing theory (1933)
- Frequentist methods become standard in science
- Bayesian methods relegated to niche status</p></li>
<li><p><strong>Late 20th century</strong>: Bayesian revival
- Computational revolution: MCMC algorithms (Metropolis 1953, Hastings 1970, Gelfand &amp; Smith 1990)
- Philosophical work by de Finetti, Savage, Jeffreys, Jaynes
- Success in complex modeling where frequentist methods struggle
- Machine learning adopts Bayesian methods widely</p></li>
<li><p><strong>21st century</strong>: Pragmatic coexistence
- Both paradigms widely used and respected
- Recognition that they answer different questions
- Computational tools (STAN, PyMC, JAGS) make Bayesian methods accessible
- Debate softens toward “use the right tool for the job”</p></li>
</ul>
</section>
<section id="key-points-of-contention">
<h4>Key Points of Contention<a class="headerlink" href="#key-points-of-contention" title="Link to this heading"></a></h4>
<p><strong>1. Subjectivity vs. Objectivity</strong></p>
<ul class="simple">
<li><p><strong>Frequentist view</strong>: Statistical inference should be objective and not depend on subjective beliefs. Priors introduce unscientific subjectivity.</p></li>
<li><p><strong>Bayesian response</strong>: All inference involves subjective choices (model selection, significance level, test statistic). Better to be explicit about priors than hide subjectivity elsewhere. Science proceeds by updating beliefs with evidence—Bayes’ theorem formalizes this.</p></li>
</ul>
<p><strong>2. Nature of Parameters</strong></p>
<ul class="simple">
<li><p><strong>Frequentist view</strong>: Parameters are fixed unknown constants. It’s meaningless to assign probabilities to them since they don’t vary.</p></li>
<li><p><strong>Bayesian response</strong>: Parameters are unknown, so we can represent our uncertainty probabilistically. “Fixed but unknown” is irrelevant—probability quantifies our knowledge, not physical randomness.</p></li>
</ul>
<p><strong>3. Probability Statements</strong></p>
<ul class="simple">
<li><p><strong>Frequentist</strong>: Can only make probability statements about data (given parameters), not about parameters themselves.</p></li>
<li><p><strong>Bayesian</strong>: Can make direct probability statements about anything unknown—parameters, hypotheses, future observations.</p></li>
</ul>
<p><strong>4. Prior Information</strong></p>
<ul class="simple">
<li><p><strong>Frequentist</strong>: Prior information is typically ignored or incorporated informally (e.g., choosing which hypothesis to test).</p></li>
<li><p><strong>Bayesian</strong>: Prior information is formally incorporated through the prior distribution, allowing transparent combination of knowledge sources.</p></li>
</ul>
<p><strong>5. Stopping Rules and the Likelihood Principle</strong></p>
<ul class="simple">
<li><p><strong>Frequentist</strong>: The sampling plan matters. Whether you sampled until reaching <span class="math notranslate nohighlight">\(n=100\)</span> or until observing :math:<a href="#id9"><span class="problematic" id="id10">`</span></a>k=8$ events affects p-values and confidence intervals.</p></li>
<li><p><strong>Bayesian</strong>: Stopping rules are irrelevant. Once you have the data, inference depends only on the likelihood function (likelihood principle).</p></li>
</ul>
<p><strong>Example</strong>: You flip a coin until you see 3 heads and it takes 10 flips, or you flip 10 times and see 3 heads. The data look identical, but frequentist inference differs (different sample spaces), while Bayesian inference is the same (same likelihood).</p>
</section>
</section>
<section id="modern-perspective-toward-pragmatism">
<h3>Modern Perspective: Toward Pragmatism<a class="headerlink" href="#modern-perspective-toward-pragmatism" title="Link to this heading"></a></h3>
<p>The tone of the debate has softened considerably in the 21st century. Modern statisticians increasingly recognize that:</p>
<p><strong>1. Different questions, different tools</strong></p>
<ul class="simple">
<li><p>Frequentist question: “What would happen if we used this procedure repeatedly across many hypothetical samples?”</p></li>
<li><p>Bayesian question: “What should I believe about this parameter given my prior knowledge and this evidence?”</p></li>
<li><p>Likelihood question: “What do these data tell us about the relative plausibility of different parameter values?”</p></li>
</ul>
<p>Each is legitimate; the appropriate one depends on context.</p>
<p><strong>2. Complementary strengths</strong></p>
<ul class="simple">
<li><p>Frequentist: Objective guarantees on long-run error rates, regulatory acceptance, well-understood properties</p></li>
<li><p>Bayesian: Flexibility in complex models, natural incorporation of prior information, direct probability statements</p></li>
<li><p>Practical statisticians use both depending on the problem</p></li>
</ul>
<p><strong>3. Hybrid approaches</strong></p>
<ul class="simple">
<li><p><strong>Empirical Bayes</strong>: Use data to estimate hyperparameters of the prior</p></li>
<li><p><strong>Frequentist evaluation of Bayesian methods</strong>: Check that Bayesian credible intervals have good frequentist coverage</p></li>
<li><p><strong>Robust Bayesian analysis</strong>: Examine sensitivity to prior specification</p></li>
<li><p><strong>Bayesian bootstrap</strong>: Nonparametric Bayesian resampling</p></li>
</ul>
<p><strong>4. Computational convergence</strong></p>
<p>Modern computational methods (MCMC, variational inference, bootstrap, cross-validation) have become standard tools regardless of paradigm. Computational statistics has emerged as a unifying field.</p>
<p><strong>5. Influential modern synthesis</strong></p>
<p>Andrew Gelman and other prominent Bayesian statisticians have argued that Bayesian methods should be evaluated by their frequentist properties (coverage, calibration). This represents substantial convergence in practice even if philosophical differences remain.</p>
</section>
<section id="current-usage-and-support">
<h3>Current Usage and Support<a class="headerlink" href="#current-usage-and-support" title="Link to this heading"></a></h3>
<p><strong>Frequentist Inference</strong>:</p>
<ul class="simple">
<li><p><strong>Dominance in</strong>: Medicine, biology, psychology, social sciences, regulatory contexts (FDA, EMA)</p></li>
<li><p><strong>Standard for</strong>: Clinical trials, A/B testing, quality control, most published scientific research</p></li>
<li><p><strong>Taught in</strong>: Nearly all introductory statistics courses</p></li>
<li><p><strong>Software</strong>: R (stats package), Python (scipy.stats), SAS, SPSS, Stata</p></li>
</ul>
<p><strong>Bayesian Inference</strong>:</p>
<ul class="simple">
<li><p><strong>Dominance in</strong>: Machine learning, astrophysics, econometrics (VARs), complex modeling</p></li>
<li><p><strong>Growing use in</strong>: Biostatistics, ecology, genetics, neuroscience</p></li>
<li><p><strong>Strength</strong>: Problems where frequentist solutions don’t exist or are intractable</p></li>
<li><p><strong>Software</strong>: STAN, PyMC3, JAGS, BUGS, R (rstan, brms)</p></li>
</ul>
<p><strong>Verdict</strong>: Both paradigms are here to stay. A well-trained data scientist should understand both frameworks deeply and choose appropriately based on:
* Problem structure (repeatable vs. unique, simple vs. complex)
* Available prior information
* Audience and regulatory requirements
* Computational resources
* Interpretation needs (direct probability statements vs. error rates)</p>
</section>
</section>
<section id="bringing-it-all-together">
<h2>Bringing It All Together<a class="headerlink" href="#bringing-it-all-together" title="Link to this heading"></a></h2>
<p>We’ve journeyed from the mathematical foundation of probability through philosophical interpretations to practical inference paradigms. Let’s synthesize the key insights:</p>
<p><strong>1. Mathematical Unity, Interpretational Diversity</strong></p>
<p>Kolmogorov’s axioms provide a rigorous mathematical foundation accepted by all approaches. However, profound disagreements persist about what these probabilities mean:</p>
<ul class="simple">
<li><p><strong>Objective interpretations</strong> (frequentist, propensity): Probability as a measurable feature of the world</p></li>
<li><p><strong>Epistemic interpretations</strong> (Bayesian): Probability as a quantification of uncertainty or degree of belief</p></li>
</ul>
<p><strong>2. Inference Follows Interpretation</strong></p>
<p>Your interpretation of probability determines your inference framework:</p>
<ul class="simple">
<li><p><strong>Frequentist</strong>: Parameters are fixed; evaluate procedures by long-run frequency properties</p></li>
<li><p><strong>Bayesian</strong>: Parameters are uncertain; update beliefs via Bayes’ theorem</p></li>
<li><p><strong>Likelihoodist</strong>: Focus on relative evidence via likelihood ratios</p></li>
</ul>
<p><strong>3. Different Questions, Different Answers</strong></p>
<p>These paradigms address genuinely different questions:</p>
<ul class="simple">
<li><p>Frequentist: “If I used this method repeatedly, what proportion of the time would I be correct?”</p></li>
<li><p>Bayesian: “Given my prior knowledge and this data, what should I believe?”</p></li>
<li><p>Likelihoodist: “How much more consistent are the data with one hypothesis versus another?”</p></li>
</ul>
<p>All three questions are scientifically legitimate. Context determines which is most appropriate.</p>
<p><strong>4. Practical Synthesis</strong></p>
<p>Modern practice is increasingly pragmatic and eclectic:</p>
<ul class="simple">
<li><p>Use frequentist methods for standard problems with well-established procedures</p></li>
<li><p>Use Bayesian methods for complex models, small samples, or when incorporating prior information</p></li>
<li><p>Evaluate any method by multiple criteria (internal coherence, frequentist properties, computational feasibility)</p></li>
<li><p>Recognize that perfect objectivity is impossible—all statistical inference involves choices and assumptions</p></li>
</ul>
<p><strong>5. Computational Methods Bridge Paradigms</strong></p>
<p>Simulation, resampling, and numerical optimization techniques work across paradigms:</p>
<ul class="simple">
<li><p>Monte Carlo simulation: Relevant to both frequentist sampling distributions and Bayesian posteriors</p></li>
<li><p>Bootstrap resampling: Frequentist tool that resembles Bayesian posterior sampling</p></li>
<li><p>MCMC: Core Bayesian computation method with frequentist convergence diagnostics</p></li>
<li><p>Cross-validation: Model evaluation technique used by both paradigms</p></li>
</ul>
<div class="important admonition">
<p class="admonition-title">Comparison Table 📊</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 20.0%" />
<col style="width: 40.0%" />
<col style="width: 40.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><strong>Aspect</strong></p></th>
<th class="head"><p><strong>Frequentist</strong></p></th>
<th class="head"><p><strong>Bayesian</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Parameters</strong></p></td>
<td><p>Fixed unknown constants</p></td>
<td><p>Random variables with distributions</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Probability meaning</strong></p></td>
<td><p>Long-run relative frequency</p></td>
<td><p>Degree of belief/uncertainty</p></td>
</tr>
<tr class="row-even"><td><p><strong>Inference goal</strong></p></td>
<td><p>Procedures with good error rate properties</p></td>
<td><p>Update beliefs with data</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Randomness source</strong></p></td>
<td><p>Sampling process only</p></td>
<td><p>Both sampling and parameter uncertainty</p></td>
</tr>
<tr class="row-even"><td><p><strong>Intervals</strong></p></td>
<td><p>Confidence intervals: procedure has specified coverage</p></td>
<td><p>Credible intervals: probability parameter in interval</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Interpretation</strong></p></td>
<td><p>“In 95% of repeated samples, interval contains θ”</p></td>
<td><p>“95% probability that θ ∈ [a,b] given data”</p></td>
</tr>
<tr class="row-even"><td><p><strong>Hypothesis testing</strong></p></td>
<td><p>P-values: P(data or more extreme | H₀)</p></td>
<td><p>Posterior probabilities: P(H₀ | data)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Prior information</strong></p></td>
<td><p>Not formally incorporated (informal choices remain)</p></td>
<td><p>Explicitly incorporated via prior distribution</p></td>
</tr>
<tr class="row-even"><td><p><strong>Stopping rules</strong></p></td>
<td><p>Matter (affect inference)</p></td>
<td><p>Don’t matter (likelihood principle)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Advantages</strong></p></td>
<td><p>Objective; regulatory acceptance; error guarantees</p></td>
<td><p>Direct probability statements; handles complexity; uses prior info</p></td>
</tr>
<tr class="row-even"><td><p><strong>Challenges</strong></p></td>
<td><p>No prob. statements about parameters; often misinterpreted</p></td>
<td><p>Requires priors; computationally intensive; prior sensitivity</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="looking-ahead-our-course-focus">
<h2>Looking Ahead: Our Course Focus<a class="headerlink" href="#looking-ahead-our-course-focus" title="Link to this heading"></a></h2>
<p>This course emphasizes <strong>computational methods</strong> that span multiple paradigms, with primary focus on frequentist foundations because they:</p>
<ul class="simple">
<li><p>Form the bedrock of classical statistics</p></li>
<li><p>Remain the standard in most scientific fields and regulatory contexts</p></li>
<li><p>Provide essential concepts (sampling distributions, standard errors, hypothesis tests) used everywhere</p></li>
<li><p>Offer intuitive connections to the Law of Large Numbers and Central Limit Theorem</p></li>
</ul>
<p><strong>However</strong>, we recognize and incorporate:</p>
<ul class="simple">
<li><p>Bayesian perspectives where they provide clearer intuition or better methods</p></li>
<li><p>Simulation and resampling techniques that work across paradigms</p></li>
<li><p>Computational approaches (Monte Carlo, bootstrap, cross-validation) as modern unifying tools</p></li>
<li><p>The importance of understanding multiple frameworks for sophisticated data science</p></li>
</ul>
<p><strong>Course trajectory</strong>:</p>
<ul class="simple">
<li><p><strong>Part 1 (Foundations - Chapter 1)</strong>: Probability paradigms, distributions review, Python random generation</p></li>
<li><p><strong>Part 2 (Simulation - Chapters 2-4)</strong>:
- Chapter 2: Monte Carlo, simulation methods (inverse CDF, Box-Muller, rejection sampling, variance reduction)
- Chapter 3: Parametric inference (exponential families, maximum likelihood, statistical estimators, linear models, GLMs)
- Chapter 4: Resampling methods (bootstrap, cross-validation, bias correction, model selection)</p></li>
<li><p><strong>Part 3 (Bayesian - Chapter 5)</strong>: Bayesian philosophy, priors, posteriors, MCMC (Metropolis-Hastings, Gibbs), model comparison</p></li>
<li><p><strong>Part 4 (LLMs &amp; Modern Methods - Weeks 13-15)</strong>: LLM integration in data science workflows, multimodal models, responsible AI (reliability, privacy, RAG, prompt design)</p></li>
<li><p><strong>Throughout</strong>: Computational implementation, numerical considerations, practical applications</p></li>
</ul>
<p>Our goal: Equip you with rigorous statistical foundations and practical computational skills to tackle real data science challenges, understanding the philosophical basis, assumptions, and appropriate contexts for each approach.</p>
<div class="important admonition">
<p class="admonition-title">Key Takeaways 📝</p>
<ol class="arabic simple">
<li><p><strong>Kolmogorov’s axioms</strong> provide the universal mathematical foundation—all paradigms accept them, but they don’t determine what probability means.</p></li>
<li><p><strong>Interpretation matters</strong>: Whether probability is long-run frequency or degree of belief fundamentally changes inference methods and result interpretation.</p></li>
<li><p><strong>Frequentist inference</strong> gives procedures with guaranteed long-run error rates but restricts probability statements to data, not parameters.</p></li>
<li><p><strong>Bayesian inference</strong> enables direct probability statements about parameters and hypotheses but requires specifying prior beliefs.</p></li>
<li><p><strong>No universally “correct” approach</strong>: Different paradigms answer different questions and have context-dependent strengths.</p></li>
<li><p><strong>Modern practice is pragmatic</strong>: Use the tool fitting your problem; understand assumptions and interpretations of your chosen method.</p></li>
<li><p><strong>Computational methods unify</strong>: Simulation, resampling, and optimization techniques span paradigms and form core data science skills.</p></li>
<li><p><strong>Course emphasis</strong>: Frequentist foundations with computational focus, recognizing Bayesian methods as important complementary tools.</p></li>
<li><p><strong>Outcome alignment</strong>: This chapter addresses Learning Outcome 2 (comparing Frequentist and Bayesian inference) and establishes conceptual groundwork for simulation (LO 1) and resampling methods (LO 3).</p></li>
</ol>
</div>
</section>
<section id="references-and-further-reading">
<h2>References and Further Reading<a class="headerlink" href="#references-and-further-reading" title="Link to this heading"></a></h2>
<p><strong>Foundational Works</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="kolmogorov1956" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Kolmogorov1956<span class="fn-bracket">]</span></span>
<p>Kolmogorov, A. N. (1956). <em>Foundations of the Theory of Probability</em> (N. Morrison, Trans.; 2nd ed.). Chelsea. (Original work published 1933)</p>
</div>
<div class="citation" id="fisher1925" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Fisher1925<span class="fn-bracket">]</span></span>
<p>Fisher, R. A. (1925). <em>Statistical Methods for Research Workers</em>. Oliver and Boyd.</p>
</div>
<div class="citation" id="neymanpearson1933" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>NeymanPearson1933<span class="fn-bracket">]</span></span>
<p>Neyman, J., and Pearson, E. S. (1933). On the problem of the most efficient tests of statistical hypotheses. <em>Philosophical Transactions of the Royal Society A</em>, 231(694–706), 289–337.</p>
</div>
</div>
<p><strong>Probability Interpretations</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="popper1957" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Popper1957<span class="fn-bracket">]</span></span>
<p>Popper, K. R. (1957). The propensity interpretation of the calculus of probability, and the quantum theory. In S. Körner (Ed.), <em>Observation and Interpretation</em> (pp. 65–70). Butterworths Scientific Publications.</p>
</div>
<div class="citation" id="humphreys1985" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Humphreys1985<span class="fn-bracket">]</span></span>
<p>Humphreys, P. (1985). Why propensities cannot be probabilities. <em>The Philosophical Review</em>, 94(4), 557–570.</p>
</div>
<div class="citation" id="definetti1937" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>deFinetti1937<span class="fn-bracket">]</span></span>
<p>de Finetti, B. (1937). La prévision: ses lois logiques, ses sources subjectives. <em>Annales de l’Institut Henri Poincaré</em>, 7(1), 1–68. English translation in H. E. Kyburg and H. E. Smokler (Eds.), <em>Studies in Subjective Probability</em> (1964), Wiley.</p>
</div>
<div class="citation" id="ramsey1926" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Ramsey1926<span class="fn-bracket">]</span></span>
<p>Ramsey, F. P. (1926). Truth and probability. In R. B. Braithwaite (Ed.), <em>The Foundations of Mathematics and Other Logical Essays</em> (1931), Routledge and Kegan Paul.</p>
</div>
<div class="citation" id="savage1954" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Savage1954<span class="fn-bracket">]</span></span>
<p>Savage, L. J. (1954). <em>The Foundations of Statistics</em>. John Wiley and Sons.</p>
</div>
<div class="citation" id="howie2002" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Howie2002<span class="fn-bracket">]</span></span>
<p>Howie, D. (2002). <em>Interpreting Probability: Controversies and Developments in the Early Twentieth Century</em>. Cambridge University Press.</p>
</div>
</div>
<p><strong>Bayesian Statistics</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="laplace1814" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Laplace1814<span class="fn-bracket">]</span></span>
<p>Laplace, P. S. (1814). <em>Essai philosophique sur les probabilités</em>. English translation by F. W. Truscott and F. L. Emory (1951), Dover Publications.</p>
</div>
<div class="citation" id="jeffreys1939" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Jeffreys1939<span class="fn-bracket">]</span></span>
<p>Jeffreys, H. (1939). <em>Theory of Probability</em>. Oxford University Press.</p>
</div>
<div class="citation" id="jaynes2003" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Jaynes2003<span class="fn-bracket">]</span></span>
<p>Jaynes, E. T. (2003). <em>Probability Theory: The Logic of Science</em>. Cambridge University Press.</p>
</div>
<div class="citation" id="gelmanetal2013" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>GelmanEtAl2013<span class="fn-bracket">]</span></span>
<p>Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., and Rubin, D. B. (2013). <em>Bayesian Data Analysis</em> (3rd ed.). Chapman and Hall/CRC.</p>
</div>
<div class="citation" id="robert2007" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Robert2007<span class="fn-bracket">]</span></span>
<p>Robert, C. P. (2007). <em>The Bayesian Choice: From Decision-Theoretic Foundations to Computational Implementation</em> (2nd ed.). Springer.</p>
</div>
<div class="citation" id="gelmanshalizi2013" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>GelmanShalizi2013<span class="fn-bracket">]</span></span>
<p>Gelman, A., and Shalizi, C. R. (2013). Philosophy and the practice of Bayesian statistics. <em>British Journal of Mathematical and Statistical Psychology</em>, 66(1), 8–38.</p>
</div>
<div class="citation" id="kassraftery1995" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>KassRaftery1995<span class="fn-bracket">]</span></span>
<p>Kass, R. E., and Raftery, A. E. (1995). Bayes factors. <em>Journal of the American Statistical Association</em>, 90(430), 773–795.</p>
</div>
</div>
<p><strong>Frequentist Statistics</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="mayo2018" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Mayo2018<span class="fn-bracket">]</span></span>
<p>Mayo, D. G. (2018). <em>Statistical Inference as Severe Testing: How to Get Beyond the Statistics Wars</em>. Cambridge University Press.</p>
</div>
<div class="citation" id="casellaberger2002" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CasellaBerger2002<span class="fn-bracket">]</span></span>
<p>Casella, G., and Berger, R. L. (2002). <em>Statistical Inference</em> (2nd ed.). Duxbury Press.</p>
</div>
<div class="citation" id="lehmannromano2005" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LehmannRomano2005<span class="fn-bracket">]</span></span>
<p>Lehmann, E. L., and Romano, J. P. (2005). <em>Testing Statistical Hypotheses</em> (3rd ed.). Springer.</p>
</div>
</div>
<p><strong>Likelihood-Based Inference</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="birnbaum1962" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Birnbaum1962<span class="fn-bracket">]</span></span>
<p>Birnbaum, A. (1962). On the foundations of statistical inference. <em>Journal of the American Statistical Association</em>, 57(298), 269–306.</p>
</div>
<div class="citation" id="royall1997" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Royall1997<span class="fn-bracket">]</span></span>
<p>Royall, R. (1997). <em>Statistical Evidence: A Likelihood Paradigm</em>. Chapman and Hall.</p>
</div>
</div>
<p><strong>Comprehensive Texts</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="wasserman2004" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Wasserman2004<span class="fn-bracket">]</span></span>
<p>Wasserman, L. (2004). <em>All of Statistics: A Concise Course in Statistical Inference</em>. Springer.</p>
</div>
<div class="citation" id="efronhastie2016" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>EfronHastie2016<span class="fn-bracket">]</span></span>
<p>Efron, B., and Hastie, T. (2016). <em>Computer Age Statistical Inference</em>. Cambridge University Press.</p>
</div>
</div>
<p><strong>Historical and Philosophical Perspectives</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="hacking2001" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Hacking2001<span class="fn-bracket">]</span></span>
<p>Hacking, I. (2001). <em>An Introduction to Probability and Inductive Logic</em>. Cambridge University Press.</p>
</div>
<div class="citation" id="zabell2005" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Zabell2005<span class="fn-bracket">]</span></span>
<p>Zabell, S. L. (2005). <em>Symmetry and Its Discontents: Essays on the History of Inductive Probability</em>. Cambridge University Press.</p>
</div>
<div class="citation" id="shafervovk2019" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ShaferVovk2019<span class="fn-bracket">]</span></span>
<p>Shafer, G., and Vovk, V. (2019). <em>Game-Theoretic Foundations for Probability and Finance</em>. Wiley.</p>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="1.1. Chapter 1: Statistical Paradigms and Core Concepts" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ch1.2-probability_distributions_review.html" class="btn btn-neutral float-right" title="1.1.2. Probability Distributions: Theory and Computation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>