

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>1.1.4. Chapter 1 Summary: Foundations in Place &mdash; STAT 418</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=dc393e06" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT418/Website/part1_foundations/chapter1/ch1.4-chapter-summary.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=30646c52"></script>
      <script>let toggleHintShow = 'Show solution';</script>
      <script>let toggleHintHide = 'Hide solution';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>window.MathJax = {"options": {"enableMenu": true, "menuOptions": {"settings": {"enrich": true, "speech": true, "braille": true, "collapsible": true, "assistiveMml": false}}}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
      <script src="../../_static/custom.js?v=8718e0ab"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="2. Part II: Simulation-Based Methods" href="../../part2_simulation/index.html" />
    <link rel="prev" title="1.1.3. Python Random Generation" href="ch1.3-python_random_generation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Course Content</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">1. Part I: Foundations of Probability and Computation</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="index.html">1.1. Chapter 1: Statistical Paradigms and Core Concepts</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="ch1.1-probability-and-inference-paradigms.html">1.1.1. Paradigms of Probability and Statistical Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch1.1-probability-and-inference-paradigms.html#the-mathematical-foundation-kolmogorov-s-axioms">The Mathematical Foundation: Kolmogorov’s Axioms</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.1-probability-and-inference-paradigms.html#interpretations-of-probability">Interpretations of Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.1-probability-and-inference-paradigms.html#statistical-inference-paradigms">Statistical Inference Paradigms</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.1-probability-and-inference-paradigms.html#historical-and-philosophical-debates">Historical and Philosophical Debates</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.1-probability-and-inference-paradigms.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.1-probability-and-inference-paradigms.html#looking-ahead-our-course-focus">Looking Ahead: Our Course Focus</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.1-probability-and-inference-paradigms.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch1.2-probability_distributions_review.html">1.1.2. Probability Distributions: Theory and Computation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch1.2-probability_distributions_review.html#from-abstract-foundations-to-concrete-tools">From Abstract Foundations to Concrete Tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.2-probability_distributions_review.html#the-python-ecosystem-for-probability">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.2-probability_distributions_review.html#introduction-why-probability-distributions-matter">Introduction: Why Probability Distributions Matter</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.2-probability_distributions_review.html#id1">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.2-probability_distributions_review.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.2-probability_distributions_review.html#continuous-distributions">Continuous Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.2-probability_distributions_review.html#additional-important-distributions">Additional Important Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.2-probability_distributions_review.html#summary-and-practical-guidelines">Summary and Practical Guidelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.2-probability_distributions_review.html#conclusion">Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.2-probability_distributions_review.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch1.3-python_random_generation.html">1.1.3. Python Random Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch1.3-python_random_generation.html#from-mathematical-distributions-to-computational-samples">From Mathematical Distributions to Computational Samples</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.3-python_random_generation.html#the-python-ecosystem-at-a-glance">The Python Ecosystem at a Glance</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.3-python_random_generation.html#understanding-pseudo-random-number-generation">Understanding Pseudo-Random Number Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.3-python_random_generation.html#the-standard-library-random-module">The Standard Library: <code class="docutils literal notranslate"><span class="pre">random</span></code> Module</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.3-python_random_generation.html#numpy-fast-vectorized-random-sampling">NumPy: Fast Vectorized Random Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.3-python_random_generation.html#scipy-stats-the-complete-statistical-toolkit">SciPy Stats: The Complete Statistical Toolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.3-python_random_generation.html#bringing-it-all-together-library-selection-guide">Bringing It All Together: Library Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.3-python_random_generation.html#looking-ahead-from-random-numbers-to-monte-carlo-methods">Looking Ahead: From Random Numbers to Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch1.3-python_random_generation.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">1.1.4. Chapter 1 Summary: Foundations in Place</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#the-three-pillars-of-chapter-1">The Three Pillars of Chapter 1</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="#what-lies-ahead-the-road-to-simulation">What Lies Ahead: The Road to Simulation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#chapter-1-exercises-synthesis-problems">Chapter 1 Exercises: Synthesis Problems</a></li>
<li class="toctree-l4"><a class="reference internal" href="#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part2_simulation/index.html">2. Part II: Simulation-Based Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part2_simulation/chapter2/index.html">2.1. Chapter 2: Monte Carlo Simulation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html">2.1.1. Monte Carlo Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html#the-historical-development-of-monte-carlo-methods">The Historical Development of Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html#the-core-principle-expectation-as-integration">The Core Principle: Expectation as Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html#theoretical-foundations">Theoretical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html#variance-estimation-and-confidence-intervals">Variance Estimation and Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html#comparison-with-deterministic-methods">Comparison with Deterministic Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html#sample-size-determination">Sample Size Determination</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html#convergence-diagnostics-and-monitoring">Convergence Diagnostics and Monitoring</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html#chapter-2-1-exercises-monte-carlo-fundamentals-mastery">Chapter 2.1 Exercises: Monte Carlo Fundamentals Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_1-monte-carlo-fundamentals.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_2-uniform-random-variates.html">2.1.2. Uniform Random Variates</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_2-uniform-random-variates.html#why-uniform-the-universal-currency-of-randomness">Why Uniform? The Universal Currency of Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_2-uniform-random-variates.html#the-paradox-of-computational-randomness">The Paradox of Computational Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_2-uniform-random-variates.html#chaotic-dynamical-systems-an-instructive-failure">Chaotic Dynamical Systems: An Instructive Failure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_2-uniform-random-variates.html#linear-congruential-generators">Linear Congruential Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_2-uniform-random-variates.html#shift-register-generators">Shift-Register Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_2-uniform-random-variates.html#the-kiss-generator-combining-strategies">The KISS Generator: Combining Strategies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_2-uniform-random-variates.html#modern-generators-mersenne-twister-and-pcg">Modern Generators: Mersenne Twister and PCG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_2-uniform-random-variates.html#statistical-testing-of-random-number-generators">Statistical Testing of Random Number Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_2-uniform-random-variates.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_2-uniform-random-variates.html#chapter-2-2-exercises-uniform-random-variates-mastery">Chapter 2.2 Exercises: Uniform Random Variates Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_2-uniform-random-variates.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_2-uniform-random-variates.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_2-uniform-random-variates.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_3-inverse-cdf-method.html">2.1.3. Inverse CDF Method</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_3-inverse-cdf-method.html#mathematical-foundations">Mathematical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_3-inverse-cdf-method.html#continuous-distributions-with-closed-form-inverses">Continuous Distributions with Closed-Form Inverses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_3-inverse-cdf-method.html#numerical-inversion">Numerical Inversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_3-inverse-cdf-method.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_3-inverse-cdf-method.html#mixed-distributions">Mixed Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_3-inverse-cdf-method.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_3-inverse-cdf-method.html#chapter-2-3-exercises-inverse-cdf-method-mastery">Chapter 2.3 Exercises: Inverse CDF Method Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_3-inverse-cdf-method.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_3-inverse-cdf-method.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_3-inverse-cdf-method.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_3-inverse-cdf-method.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_4-transformation-methods.html">2.1.4. Transformation Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_4-transformation-methods.html#why-transformation-methods">Why Transformation Methods?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_4-transformation-methods.html#the-boxmuller-transform">The Box–Muller Transform</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_4-transformation-methods.html#the-polar-marsaglia-method">The Polar (Marsaglia) Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_4-transformation-methods.html#method-comparison-boxmuller-vs-polar-vs-ziggurat">Method Comparison: Box–Muller vs Polar vs Ziggurat</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_4-transformation-methods.html#the-ziggurat-algorithm">The Ziggurat Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_4-transformation-methods.html#the-clt-approximation-historical">The CLT Approximation (Historical)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_4-transformation-methods.html#distributions-derived-from-the-normal">Distributions Derived from the Normal</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_4-transformation-methods.html#multivariate-normal-generation">Multivariate Normal Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_4-transformation-methods.html#implementation-guidance">Implementation Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_4-transformation-methods.html#chapter-2-4-exercises-transformation-methods-mastery">Chapter 2.4 Exercises: Transformation Methods Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_4-transformation-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_4-transformation-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html">2.1.5. Rejection Sampling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html#the-dartboard-intuition">The Dartboard Intuition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html#the-accept-reject-algorithm">The Accept-Reject Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html#efficiency-analysis">Efficiency Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html#choosing-the-proposal-distribution">Choosing the Proposal Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html#the-squeeze-principle">The Squeeze Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html#geometric-example-sampling-from-the-unit-disk">Geometric Example: Sampling from the Unit Disk</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html#limitations-and-the-curse-of-dimensionality">Limitations and the Curse of Dimensionality</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html#connections-to-other-methods">Connections to Other Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html#chapter-2-5-exercises-rejection-sampling-mastery">Chapter 2.5 Exercises: Rejection Sampling Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_5-rejection-sampling.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_6-variance-reduction-methods.html">2.1.6. Variance Reduction Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_6-variance-reduction-methods.html#the-variance-reduction-paradigm">The Variance Reduction Paradigm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_6-variance-reduction-methods.html#importance-sampling">Importance Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_6-variance-reduction-methods.html#control-variates">Control Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_6-variance-reduction-methods.html#antithetic-variates">Antithetic Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_6-variance-reduction-methods.html#stratified-sampling">Stratified Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_6-variance-reduction-methods.html#common-random-numbers">Common Random Numbers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_6-variance-reduction-methods.html#conditional-monte-carlo-raoblackwellization">Conditional Monte Carlo (Rao–Blackwellization)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_6-variance-reduction-methods.html#combining-variance-reduction-techniques">Combining Variance Reduction Techniques</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_6-variance-reduction-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_6-variance-reduction-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_6-variance-reduction-methods.html#chapter-2-6-exercises-variance-reduction-mastery">Chapter 2.6 Exercises: Variance Reduction Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_6-variance-reduction-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_7-chapter-summary.html">2.1.7. Chapter Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_7-chapter-summary.html#the-complete-monte-carlo-workflow">The Complete Monte Carlo Workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_7-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_7-chapter-summary.html#quick-reference-tables">Quick Reference Tables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_7-chapter-summary.html#common-pitfalls-checklist">Common Pitfalls Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_7-chapter-summary.html#connections-to-later-chapters">Connections to Later Chapters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_7-chapter-summary.html#learning-outcomes-checklist">Learning Outcomes Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_7-chapter-summary.html#further-reading-optimization-and-missing-data">Further Reading: Optimization and Missing Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_7-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter2/ch2_7-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../part2_simulation/chapter3/index.html">2.2. Chapter 3: Parametric Inference and Likelihood Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_1-exponential-families.html">2.2.1. Exponential Families</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_1-exponential-families.html#historical-origins-from-scattered-results-to-unified-theory">Historical Origins: From Scattered Results to Unified Theory</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_1-exponential-families.html#the-canonical-exponential-family">The Canonical Exponential Family</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_1-exponential-families.html#converting-familiar-distributions">Converting Familiar Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_1-exponential-families.html#the-log-partition-function-a-moment-generating-machine">The Log-Partition Function: A Moment-Generating Machine</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_1-exponential-families.html#sufficiency-capturing-all-parameter-information">Sufficiency: Capturing All Parameter Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_1-exponential-families.html#minimal-sufficiency-and-completeness">Minimal Sufficiency and Completeness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_1-exponential-families.html#conjugate-priors-and-bayesian-inference">Conjugate Priors and Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_1-exponential-families.html#exponential-dispersion-models-and-glms">Exponential Dispersion Models and GLMs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_1-exponential-families.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_1-exponential-families.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_1-exponential-families.html#chapter-3-1-exercises-exponential-families-mastery">Chapter 3.1 Exercises: Exponential Families Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_1-exponential-families.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_1-exponential-families.html#further-reading">Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html">2.2.2. Maximum Likelihood Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#the-likelihood-function">The Likelihood Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#the-score-function">The Score Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#fisher-information">Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#closed-form-maximum-likelihood-estimators">Closed-Form Maximum Likelihood Estimators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#numerical-optimization-for-mle">Numerical Optimization for MLE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#asymptotic-properties-of-mles">Asymptotic Properties of MLEs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#the-cramer-rao-lower-bound">The Cramér-Rao Lower Bound</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#the-invariance-property">The Invariance Property</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#likelihood-based-hypothesis-testing">Likelihood-Based Hypothesis Testing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#confidence-intervals-from-likelihood">Confidence Intervals from Likelihood</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#connection-to-bayesian-inference">Connection to Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#chapter-3-2-exercises-maximum-likelihood-estimation-mastery">Chapter 3.2 Exercises: Maximum Likelihood Estimation Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_3-sampling-variability.html">2.2.3. Sampling Variability and Variance Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_3-sampling-variability.html#statistical-estimators-and-their-properties">Statistical Estimators and Their Properties</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_3-sampling-variability.html#sampling-distributions">Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_3-sampling-variability.html#the-delta-method">The Delta Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_3-sampling-variability.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_3-sampling-variability.html#variance-estimation-methods">Variance Estimation Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_3-sampling-variability.html#applications-and-worked-examples">Applications and Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_3-sampling-variability.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_3-sampling-variability.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter3/ch3_3-sampling-variability.html#exercises">Exercises</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../part2_simulation/chapter4/index.html">2.3. Chapter 4: Resampling Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter4/jackknife_introduction.html">2.3.1. Jackknife Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/jackknife_introduction.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/jackknife_introduction.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/jackknife_introduction.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/jackknife_introduction.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/jackknife_introduction.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/jackknife_introduction.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter4/bootstrap_fundamentals.html">2.3.2. Bootstrap Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/bootstrap_fundamentals.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/bootstrap_fundamentals.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/bootstrap_fundamentals.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/bootstrap_fundamentals.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/bootstrap_fundamentals.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/bootstrap_fundamentals.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter4/nonparametric_bootstrap.html">2.3.3. Nonparametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/nonparametric_bootstrap.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/nonparametric_bootstrap.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/nonparametric_bootstrap.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/nonparametric_bootstrap.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/nonparametric_bootstrap.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/nonparametric_bootstrap.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter4/parametric_bootstrap.html">2.3.4. Parametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/parametric_bootstrap.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/parametric_bootstrap.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/parametric_bootstrap.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/parametric_bootstrap.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/parametric_bootstrap.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/parametric_bootstrap.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter4/confidence_intervals.html">2.3.5. Confidence Intervals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/confidence_intervals.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/confidence_intervals.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/confidence_intervals.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/confidence_intervals.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/confidence_intervals.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/confidence_intervals.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter4/bias_correction.html">2.3.6. Bias Correction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/bias_correction.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/bias_correction.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/bias_correction.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/bias_correction.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/bias_correction.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/bias_correction.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter4/cross_validation_loo.html">2.3.7. Cross Validation Loo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/cross_validation_loo.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/cross_validation_loo.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/cross_validation_loo.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/cross_validation_loo.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/cross_validation_loo.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/cross_validation_loo.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter4/cross_validation_k_fold.html">2.3.8. Cross Validation K Fold</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/cross_validation_k_fold.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/cross_validation_k_fold.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/cross_validation_k_fold.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/cross_validation_k_fold.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/cross_validation_k_fold.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/cross_validation_k_fold.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_simulation/chapter4/model_selection.html">2.3.9. Model Selection</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/model_selection.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/model_selection.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/model_selection.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/model_selection.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/model_selection.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_simulation/chapter4/model_selection.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part3_bayesian/index.html">Part III: Bayesian Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part3_bayesian/index.html#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html">1. Bayesian Philosophy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#introduction">1.1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#key-concepts">1.2. Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#mathematical-framework">1.3. Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#python-implementation">1.4. Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#examples">1.5. Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#summary">1.6. Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html"><span class="section-number">1. </span>Part I: Foundations of Probability and Computation</a></li>
          <li class="breadcrumb-item"><a href="index.html"><span class="section-number">1.1. </span>Chapter 1: Statistical Paradigms and Core Concepts</a></li>
      <li class="breadcrumb-item active"><span class="section-number">1.1.4. </span>Chapter 1 Summary: Foundations in Place</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/part1_foundations/chapter1/ch1.4-chapter-summary.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="chapter-1-summary-foundations-in-place">
<span id="ch1-4-chapter-summary"></span><h1><span class="section-number">1.1.4. </span>Chapter 1 Summary: Foundations in Place<a class="headerlink" href="#chapter-1-summary-foundations-in-place" title="Link to this heading"></a></h1>
<p>We have now established the conceptual, mathematical, and computational foundations for everything that follows in this course. Before moving to Part 2’s simulation methods, let’s synthesize what we’ve learned and see how these pieces fit together.</p>
<section id="the-three-pillars-of-chapter-1">
<h2>The Three Pillars of Chapter 1<a class="headerlink" href="#the-three-pillars-of-chapter-1" title="Link to this heading"></a></h2>
<p>Chapter 1 developed three interconnected pillars that support all computational methods in data science:</p>
<p><strong>Pillar 1: Philosophical Foundations (Section 1.1)</strong></p>
<p>We confronted the fundamental question: <em>What does probability mean?</em> This isn’t merely philosophical—your answer determines how you conduct inference, interpret results, and communicate findings.</p>
<ul class="simple">
<li><p><strong>Kolmogorov’s axioms</strong> provide the mathematical rules everyone accepts: non-negativity, normalization, and countable additivity. These axioms are interpretation-neutral—they specify <em>how</em> probabilities behave without dictating <em>what</em> they represent.</p></li>
<li><p><strong>Frequentist interpretation</strong> views probability as long-run relative frequency. Parameters are fixed but unknown; only data are random. This leads to inference methods evaluated by their behavior across hypothetical repeated samples—confidence intervals, p-values, and error rate control.</p></li>
<li><p><strong>Bayesian interpretation</strong> views probability as degree of belief. Parameters are uncertain and receive probability distributions. Bayes’ theorem mechanically updates prior beliefs into posterior beliefs given data, enabling direct probability statements about hypotheses and parameters.</p></li>
<li><p><strong>The choice matters</strong>: Frequentists ask “What would happen if I repeated this procedure many times?” Bayesians ask “What should I believe given this evidence?” Both questions are legitimate; context determines which is more appropriate.</p></li>
</ul>
<p><strong>Pillar 2: Mathematical Machinery (Section 1.2)</strong></p>
<p>We reviewed the probability distributions that model real phenomena—the mathematical objects that connect abstract probability to concrete data.</p>
<ul class="simple">
<li><p><strong>Distribution functions</strong> (PMF, PDF, CDF, quantile function) provide complete descriptions of random variables. The CDF <span class="math notranslate nohighlight">\(F(x) = P(X \leq x)\)</span> is universal; the quantile function <span class="math notranslate nohighlight">\(F^{-1}(p)\)</span> inverts it.</p></li>
<li><p><strong>Discrete distributions</strong> (Bernoulli, Binomial, Poisson, Geometric, Negative Binomial) model counts, trials, and discrete events. Key relationships include: Binomial as sum of Bernoullis, Poisson as limit of Binomial for rare events, Negative Binomial as sum of Geometrics.</p></li>
<li><p><strong>Continuous distributions</strong> (Uniform, Normal, Exponential, Gamma, Beta) model measurements, durations, and proportions. Key relationships include: Exponential as Gamma with shape 1, Chi-square as Gamma with specific parameters, Normal as limit of standardized sums (Central Limit Theorem).</p></li>
<li><p><strong>Inference distributions</strong> (Student’s t, Chi-square, F) arise naturally when making inferences about normal populations with estimated variance.</p></li>
</ul>
<p><strong>Pillar 3: Computational Tools (Section 1.3)</strong></p>
<p>We learned to generate random samples using Python’s ecosystem—the practical bridge from theory to simulation.</p>
<ul class="simple">
<li><p><strong>The ``random`` module</strong> provides lightweight, dependency-free sampling for prototyping and teaching.</p></li>
<li><p><strong>NumPy’s ``Generator`` API</strong> delivers high-performance vectorized sampling essential for serious Monte Carlo work—50 to 100 times faster than Python loops.</p></li>
<li><p><strong>SciPy’s ``scipy.stats``</strong> offers the complete statistical toolkit: 100+ distributions with density functions, CDFs, quantile functions, fitting, and hypothesis tests.</p></li>
<li><p><strong>Reproducibility</strong> requires explicit seeds; <strong>parallel computing</strong> requires independent streams via <code class="docutils literal notranslate"><span class="pre">SeedSequence.spawn()</span></code>.</p></li>
</ul>
</section>
<section id="how-the-pillars-connect">
<h2>How the Pillars Connect<a class="headerlink" href="#how-the-pillars-connect" title="Link to this heading"></a></h2>
<p>These three pillars don’t stand in isolation—they form an integrated foundation:</p>
<p><strong>Paradigms inform distribution choice.</strong> A Bayesian analyzing a proportion naturally thinks of the Beta distribution as a prior for <span class="math notranslate nohighlight">\(p\)</span> and the Binomial as the likelihood—leading to a Beta posterior (conjugacy). A frequentist analyzing the same data focuses on the sampling distribution of <span class="math notranslate nohighlight">\(\hat{p}\)</span>, using the Normal approximation via the Central Limit Theorem for large samples or exact Binomial calculations for small ones.</p>
<p><strong>Distributions enable computational methods.</strong> The inverse CDF method (Chapter 2) works because the Probability Integral Transform guarantees <span class="math notranslate nohighlight">\(F^{-1}(U) \sim F\)</span> when <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0,1)\)</span>. Understanding distribution properties—like the memoryless property of the Exponential or the reproductive property of the Gamma—enables efficient simulation algorithms.</p>
<p><strong>Computation validates theory.</strong> When we prove that <span class="math notranslate nohighlight">\(\bar{X}_n \xrightarrow{P} \mu\)</span> (Law of Large Numbers), we can verify this computationally by generating samples and watching convergence. When we derive that the sample variance <span class="math notranslate nohighlight">\(S^2\)</span> is unbiased for <span class="math notranslate nohighlight">\(\sigma^2\)</span>, we can confirm via simulation. This interplay between proof and computation builds deep understanding.</p>
<div class="note admonition">
<p class="admonition-title">Example 💡: Integrating All Three Pillars</p>
<p><strong>Problem:</strong> Estimate <span class="math notranslate nohighlight">\(P(X &gt; 5)\)</span> where <span class="math notranslate nohighlight">\(X \sim \text{Gamma}(3, 2)\)</span> (shape 3, scale 2).</p>
<p><strong>Approach 1: Exact computation (Pillar 2)</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="n">gamma_dist</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">prob_exact</span> <span class="o">=</span> <span class="n">gamma_dist</span><span class="o">.</span><span class="n">sf</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>  <span class="c1"># Survival function = 1 - CDF</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Exact: P(X &gt; 5) = </span><span class="si">{</span><span class="n">prob_exact</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Approach 2: Monte Carlo simulation (Pillar 3)</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100_000</span><span class="p">)</span>
<span class="n">prob_mc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samples</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">se_mc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">prob_mc</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">prob_mc</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Monte Carlo: P(X &gt; 5) ≈ </span><span class="si">{</span><span class="n">prob_mc</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="mf">1.96</span><span class="o">*</span><span class="n">se_mc</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Interpretation depends on paradigm (Pillar 1)</strong></p>
<ul class="simple">
<li><p><strong>Frequentist</strong>: The Monte Carlo estimate has a standard error; with 100,000 samples, we’re confident the true probability lies within the reported interval.</p></li>
<li><p><strong>Bayesian</strong>: If we were uncertain about the Gamma parameters, we’d integrate over their posterior distribution to get a posterior predictive probability.</p></li>
</ul>
<p><strong>Result:</strong> Both approaches yield <span class="math notranslate nohighlight">\(P(X &gt; 5) \approx 0.4562\)</span>. The Monte Carlo estimate converges to the exact value as sample size increases—a computational demonstration of the Law of Large Numbers.</p>
</div>
</section>
<section id="what-lies-ahead-the-road-to-simulation">
<h2>What Lies Ahead: The Road to Simulation<a class="headerlink" href="#what-lies-ahead-the-road-to-simulation" title="Link to this heading"></a></h2>
<p>With foundations in place, Part 2 opens the black boxes. We’ll learn <em>how</em> the random number generators we’ve been using actually work:</p>
<p><strong>Chapter 2: Monte Carlo Methods</strong></p>
<ul class="simple">
<li><p><strong>Inverse CDF method</strong>: The workhorse algorithm. If you can compute <span class="math notranslate nohighlight">\(F^{-1}(u)\)</span>, you can sample from <span class="math notranslate nohighlight">\(F\)</span>. We’ll derive this from the Probability Integral Transform and implement samplers for Exponential, Weibull, and other distributions.</p></li>
<li><p><strong>Box-Muller transformation</strong>: A clever trick converting uniform pairs to normal pairs using polar coordinates. We’ll prove why it works and implement it.</p></li>
<li><p><strong>Rejection sampling</strong>: When inverse CDF is intractable, we propose from a simpler distribution and accept/reject. We’ll analyze efficiency and implement samplers for distributions like Beta and Gamma.</p></li>
<li><p><strong>Monte Carlo integration</strong>: Estimating integrals (expectations) by averaging samples. We’ll quantify Monte Carlo error and understand the <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> convergence rate.</p></li>
</ul>
<p><strong>Chapters 3–4: Inference and Resampling</strong></p>
<ul class="simple">
<li><p><strong>Maximum likelihood estimation</strong>: Finding parameters that make observed data most probable.</p></li>
<li><p><strong>Bootstrap methods</strong>: Resampling observed data to quantify uncertainty without distributional assumptions.</p></li>
<li><p><strong>Cross-validation</strong>: Estimating predictive performance by systematically holding out data.</p></li>
</ul>
<p><strong>Chapter 5 and Beyond: Bayesian Computation</strong></p>
<ul class="simple">
<li><p><strong>Markov chain Monte Carlo</strong>: When posteriors lack closed forms, we construct Markov chains whose stationary distributions are the posteriors we seek.</p></li>
<li><p><strong>Metropolis-Hastings and Gibbs sampling</strong>: The workhorses of Bayesian computation.</p></li>
</ul>
<p>Each method builds on the foundations established here. Understanding <em>why</em> the Normal distribution appears everywhere (Central Limit Theorem) helps you know when Normal-based inference is appropriate. Understanding the frequentist/Bayesian distinction helps you interpret bootstrap confidence intervals versus Bayesian credible intervals. Understanding Python’s random generation ecosystem lets you implement any method efficiently.</p>
<div class="important admonition">
<p class="admonition-title">Key Takeaways 📝</p>
<ol class="arabic simple">
<li><p><strong>Probability has multiple valid interpretations</strong>: Frequentist (long-run frequency) and Bayesian (degree of belief) approaches answer different questions and have different strengths. Modern practice often draws pragmatically from both.</p></li>
<li><p><strong>Distributions are the vocabulary of uncertainty</strong>: Mastering the major distributions—their properties, relationships, and parameterizations—enables you to model diverse phenomena and understand the methods built upon them.</p></li>
<li><p><strong>Computation bridges theory and practice</strong>: Python’s <code class="docutils literal notranslate"><span class="pre">random</span></code>, NumPy, and SciPy provide the tools to simulate, verify, and apply probabilistic ideas. Reproducibility and performance require thoughtful choices.</p></li>
<li><p><strong>Foundations enable methods</strong>: The inverse CDF method requires understanding CDFs. Bootstrap requires understanding sampling distributions. MCMC requires understanding both Bayesian inference and convergence. Everything ahead builds on Chapter 1.</p></li>
<li><p><strong>Course outcome alignment</strong>: This chapter addressed Learning Outcome 2 (comparing frequentist and Bayesian inference) and laid groundwork for LO 1 (simulation techniques), LO 3 (resampling methods), and LO 4 (Bayesian models via MCMC).</p></li>
</ol>
</div>
</section>
<section id="chapter-1-exercises-synthesis-problems">
<h2>Chapter 1 Exercises: Synthesis Problems<a class="headerlink" href="#chapter-1-exercises-synthesis-problems" title="Link to this heading"></a></h2>
<p>These exercises integrate material from all three sections, requiring you to connect philosophical, mathematical, and computational perspectives.</p>
<div class="tip admonition">
<p class="admonition-title">A Note on These Exercises</p>
<p>Some exercises preview methods we will study in depth later:</p>
<ul class="simple">
<li><p><strong>Bayesian inference</strong> (Exercises 1b, 1d) introduces concepts covered thoroughly in <span class="xref std std-ref">Chapter 5: Bayesian Data Analysis</span>, including prior distributions, posterior computation, and credible intervals.</p></li>
<li><p><strong>The bootstrap</strong> (Exercise 3) previews resampling methods covered in <span class="xref std std-ref">Chapter 4: Resampling Methods</span>, where we develop the theory and explore bootstrap variants.</p></li>
</ul>
<p>The goal here is to build intuition and see these methods in action. Don’t worry if some details feel unfamiliar—we provide step-by-step guidance below, and you’ll master these techniques when we revisit them later.</p>
</div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 1: Paradigm Comparison via Simulation</p>
<p>A coin is flipped 20 times, yielding 14 heads.</p>
<ol class="loweralpha">
<li><p><strong>Frequentist analysis</strong>: Compute a 95% confidence interval for <span class="math notranslate nohighlight">\(p\)</span> using the Normal approximation. Then compute the exact Clopper-Pearson interval using <code class="docutils literal notranslate"><span class="pre">scipy.stats.beta.ppf</span></code>. Compare the two intervals.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Normal Approximation CI</p>
<p>The sample proportion <span class="math notranslate nohighlight">\(\hat{p} = x/n\)</span> has approximate sampling distribution <span class="math notranslate nohighlight">\(\hat{p} \sim N(p, p(1-p)/n)\)</span> for large <span class="math notranslate nohighlight">\(n\)</span>. Since we don’t know <span class="math notranslate nohighlight">\(p\)</span>, we estimate the standard error as <span class="math notranslate nohighlight">\(\widehat{\text{SE}} = \sqrt{\hat{p}(1-\hat{p})/n}\)</span>. The 95% CI is <span class="math notranslate nohighlight">\(\hat{p} \pm 1.96 \times \widehat{\text{SE}}\)</span>.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Hint: Clopper-Pearson Exact CI</p>
<p>The Clopper-Pearson interval uses the relationship between the Binomial and Beta distributions. If <span class="math notranslate nohighlight">\(X \sim \text{Binomial}(n, p)\)</span>, then finding the <span class="math notranslate nohighlight">\(p\)</span> values where <span class="math notranslate nohighlight">\(P(X \geq x)\)</span> or <span class="math notranslate nohighlight">\(P(X \leq x)\)</span> equals <span class="math notranslate nohighlight">\(\alpha/2\)</span> involves Beta quantiles:</p>
<ul class="simple">
<li><p>Lower bound: <span class="math notranslate nohighlight">\(p_L = B_{\alpha/2}(x, n-x+1)\)</span> where <span class="math notranslate nohighlight">\(B_q(a,b)\)</span> is the <span class="math notranslate nohighlight">\(q\)</span>-th quantile of <span class="math notranslate nohighlight">\(\text{Beta}(a, b)\)</span></p></li>
<li><p>Upper bound: <span class="math notranslate nohighlight">\(p_U = B_{1-\alpha/2}(x+1, n-x)\)</span></p></li>
</ul>
<p>This relationship arises because the Beta distribution is the conjugate prior for the Binomial, and there’s a duality: <span class="math notranslate nohighlight">\(P(\text{Binomial}(n,p) \geq x) = P(\text{Beta}(x, n-x+1) \leq p)\)</span>.</p>
</div>
</li>
<li><p><strong>Bayesian analysis</strong>: Using a <span class="math notranslate nohighlight">\(\text{Beta}(\alpha=1, \beta=1)\)</span> prior (uniform), derive the posterior distribution for <span class="math notranslate nohighlight">\(p\)</span>. Compute the posterior mean and a 95% credible interval. How do these compare to the frequentist results?</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Conjugate Prior</p>
<p>The Beta distribution is <em>conjugate</em> to the Binomial likelihood, meaning the posterior is also Beta. If the prior is <span class="math notranslate nohighlight">\(p \sim \text{Beta}(\alpha_0, \beta_0)\)</span> and we observe <span class="math notranslate nohighlight">\(x\)</span> successes in <span class="math notranslate nohighlight">\(n\)</span> trials, then:</p>
<div class="math notranslate nohighlight">
\[\text{Posterior: } p | x \sim \text{Beta}(\alpha_0 + x, \beta_0 + n - x)\]</div>
<p>The parameters <span class="math notranslate nohighlight">\(\alpha_0\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span> can be interpreted as “prior pseudo-observations”: <span class="math notranslate nohighlight">\(\alpha_0 - 1\)</span> prior successes and <span class="math notranslate nohighlight">\(\beta_0 - 1\)</span> prior failures.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Hint: Beta Distribution Properties</p>
<p>For <span class="math notranslate nohighlight">\(\text{Beta}(\alpha, \beta)\)</span>:</p>
<ul class="simple">
<li><p>Mean: <span class="math notranslate nohighlight">\(\frac{\alpha}{\alpha + \beta}\)</span></p></li>
<li><p>Mode (for <span class="math notranslate nohighlight">\(\alpha, \beta &gt; 1\)</span>): <span class="math notranslate nohighlight">\(\frac{\alpha - 1}{\alpha + \beta - 2}\)</span></p></li>
<li><p>Credible intervals: Use <code class="docutils literal notranslate"><span class="pre">scipy.stats.beta(a,</span> <span class="pre">b).ppf([0.025,</span> <span class="pre">0.975])</span></code></p></li>
</ul>
</div>
</li>
<li><p><strong>Simulation verification</strong>: Generate 10,000 samples from the posterior and verify that your credible interval contains approximately 95% of the samples.</p>
<div class="tip admonition">
<p class="admonition-title">Hint</p>
<p>Use <code class="docutils literal notranslate"><span class="pre">scipy.stats.beta(a,</span> <span class="pre">b).rvs(size=10000,</span> <span class="pre">random_state=rng)</span></code> to draw samples. Count what fraction fall between your CI bounds.</p>
</div>
</li>
<li><p><strong>Interpretation</strong>: Write one paragraph explaining what the frequentist confidence interval means and one paragraph explaining what the Bayesian credible interval means. A scientist asks “What’s the probability that <span class="math notranslate nohighlight">\(p &gt; 0.5\)</span>?” How would each paradigm answer?</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Key Philosophical Difference</p>
<ul class="simple">
<li><p><strong>Frequentist</strong>: Parameters are fixed constants; probability describes long-run frequencies of procedures</p></li>
<li><p><strong>Bayesian</strong>: Parameters have probability distributions; we can make direct probability statements about parameters</p></li>
</ul>
<p>For the Bayesian answer, compute <span class="math notranslate nohighlight">\(P(p &gt; 0.5 | \text{data}) = 1 - F(0.5)\)</span> where <span class="math notranslate nohighlight">\(F\)</span> is the posterior CDF.</p>
</div>
</li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Frequentist Analysis</strong></p>
<p class="sd-card-text">We observe <span class="math notranslate nohighlight">\(x = 14\)</span> successes in <span class="math notranslate nohighlight">\(n = 20\)</span> trials. Our goal is to construct confidence intervals for the unknown population proportion <span class="math notranslate nohighlight">\(p\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Step 1: Compute the Point Estimate</p>
<p class="sd-card-text">The maximum likelihood estimator (MLE) for a binomial proportion is simply the sample proportion:</p>
<div class="math notranslate nohighlight">
\[\hat{p} = \frac{x}{n} = \frac{14}{20} = 0.70\]</div>
<p class="sd-card-text">This is our best single guess for <span class="math notranslate nohighlight">\(p\)</span>, but it doesn’t capture uncertainty.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Step 2: Normal Approximation Confidence Interval</p>
<p class="sd-card-text">The Normal approximation uses the Central Limit Theorem. For large <span class="math notranslate nohighlight">\(n\)</span>, the sampling distribution of <span class="math notranslate nohighlight">\(\hat{p}\)</span> is approximately Normal:</p>
<div class="math notranslate nohighlight">
\[\hat{p} \sim N\left(p, \frac{p(1-p)}{n}\right)\]</div>
<p class="sd-card-text"><strong>Problem:</strong> We don’t know <span class="math notranslate nohighlight">\(p\)</span>, so we estimate the standard error by plugging in <span class="math notranslate nohighlight">\(\hat{p}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\widehat{\text{SE}} = \sqrt{\frac{\hat{p}(1-\hat{p})}{n}} = \sqrt{\frac{0.70 \times 0.30}{20}} = \sqrt{\frac{0.21}{20}} = \sqrt{0.0105} = 0.1025\]</div>
<p class="sd-card-text"><strong>The 95% CI formula:</strong> For a 95% interval, we use <span class="math notranslate nohighlight">\(z_{0.975} = 1.96\)</span> (the 97.5th percentile of the standard normal):</p>
<div class="math notranslate nohighlight">
\[\text{CI} = \hat{p} \pm z_{0.975} \times \widehat{\text{SE}} = 0.70 \pm 1.96 \times 0.1025 = 0.70 \pm 0.2009\]</div>
<div class="math notranslate nohighlight">
\[\text{CI} = (0.4992, 0.9008)\]</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Step 3: Clopper-Pearson Exact Confidence Interval</p>
<p class="sd-card-text">The Clopper-Pearson interval is called “exact” because it inverts the binomial distribution directly rather than using a Normal approximation. It guarantees at least 95% coverage for any true <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p class="sd-card-text"><strong>The idea:</strong> Find the smallest <span class="math notranslate nohighlight">\(p_L\)</span> such that observing 14 or more successes would be unlikely (probability <span class="math notranslate nohighlight">\(\leq \alpha/2\)</span>), and the largest <span class="math notranslate nohighlight">\(p_U\)</span> such that observing 14 or fewer successes would be unlikely.</p>
<p class="sd-card-text"><strong>The formulas use Beta quantiles</strong> (because the Beta and Binomial are related):</p>
<ul class="simple">
<li><p class="sd-card-text">Lower bound: <span class="math notranslate nohighlight">\(p_L = B_{0.025}(x, n-x+1) = B_{0.025}(14, 7)\)</span></p></li>
<li><p class="sd-card-text">Upper bound: <span class="math notranslate nohighlight">\(p_U = B_{0.975}(x+1, n-x) = B_{0.975}(15, 6)\)</span></p></li>
</ul>
<p class="sd-card-text">where <span class="math notranslate nohighlight">\(B_q(a, b)\)</span> is the <span class="math notranslate nohighlight">\(q\)</span>-th quantile of the Beta(<span class="math notranslate nohighlight">\(a\)</span>, <span class="math notranslate nohighlight">\(b\)</span>) distribution.</p>
</div>
<p class="sd-card-text"><strong>Python Implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Given data</span>
<span class="n">n</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">14</span>
<span class="n">p_hat</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="n">n</span>  <span class="c1"># MLE = 0.70</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span>   <span class="c1"># For 95% confidence</span>

<span class="c1"># ----- Normal Approximation CI -----</span>
<span class="c1"># Step 1: Compute estimated standard error</span>
<span class="n">se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">p_hat</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_hat</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimated SE: </span><span class="si">{</span><span class="n">se</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Step 2: Get z critical value for 95% CI</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># z_0.975 = 1.96</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;z critical value: </span><span class="si">{</span><span class="n">z</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Step 3: Compute CI</span>
<span class="n">ci_normal</span> <span class="o">=</span> <span class="p">(</span><span class="n">p_hat</span> <span class="o">-</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se</span><span class="p">,</span> <span class="n">p_hat</span> <span class="o">+</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Normal approximation 95% CI: (</span><span class="si">{</span><span class="n">ci_normal</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_normal</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># ----- Clopper-Pearson Exact CI -----</span>
<span class="c1"># Uses Beta distribution quantiles</span>
<span class="n">lower_cp</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>      <span class="c1"># Beta(a=14, b=7)</span>
<span class="n">upper_cp</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># Beta(a=15, b=6)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Clopper-Pearson exact 95% CI: (</span><span class="si">{</span><span class="n">lower_cp</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">upper_cp</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># Compare widths</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Interval widths:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Normal: </span><span class="si">{</span><span class="n">ci_normal</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">ci_normal</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Exact:  </span><span class="si">{</span><span class="n">upper_cp</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">lower_cp</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Estimated SE: 0.1025
z critical value: 1.9600
Normal approximation 95% CI: (0.4992, 0.9008)
Clopper-Pearson exact 95% CI: (0.4572, 0.8811)

Interval widths:
  Normal: 0.4017
  Exact:  0.4239
</pre></div>
</div>
<p class="sd-card-text"><strong>Comparison of the Two Intervals:</strong></p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Property</p></th>
<th class="head"><p class="sd-card-text">Normal Approx.</p></th>
<th class="head"><p class="sd-card-text">Clopper-Pearson</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text">Lower bound</p></td>
<td><p class="sd-card-text">0.4992</p></td>
<td><p class="sd-card-text">0.4572</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text">Upper bound</p></td>
<td><p class="sd-card-text">0.9008</p></td>
<td><p class="sd-card-text">0.8811</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text">Width</p></td>
<td><p class="sd-card-text">0.4017</p></td>
<td><p class="sd-card-text">0.4239</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text">Symmetric?</p></td>
<td><p class="sd-card-text">Yes</p></td>
<td><p class="sd-card-text">No</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text">Can exceed [0,1]?</p></td>
<td><p class="sd-card-text">Yes (problematic)</p></td>
<td><p class="sd-card-text">No (guaranteed)</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text">Coverage guarantee</p></td>
<td><p class="sd-card-text">Approximate</p></td>
<td><p class="sd-card-text">Exact ≥ 95%</p></td>
</tr>
</tbody>
</table>
<p class="sd-card-text">The Clopper-Pearson interval is asymmetric because we’re closer to 1 than to 0. The Normal approximation can perform poorly when <span class="math notranslate nohighlight">\(n\)</span> is small or <span class="math notranslate nohighlight">\(p\)</span> is near 0 or 1.</p>
<p class="sd-card-text"><strong>Part (b): Bayesian Analysis</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p class="sd-card-text">This exercise previews <strong>Bayesian inference</strong>, which we will study in depth in Chapter 5. Here we walk through the mechanics; Chapter 5 will develop the full theory.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Step 1: Specify the Prior Distribution</p>
<p class="sd-card-text">In Bayesian inference, we start with a <strong>prior distribution</strong> representing our beliefs about <span class="math notranslate nohighlight">\(p\)</span> <em>before</em> seeing the data.</p>
<p class="sd-card-text">We use <span class="math notranslate nohighlight">\(p \sim \text{Beta}(1, 1)\)</span>, which equals the Uniform(0, 1) distribution. This is a “non-informative” prior saying we have no strong prior beliefs—all values of <span class="math notranslate nohighlight">\(p\)</span> are equally plausible before seeing data.</p>
<div class="math notranslate nohighlight">
\[\pi(p) = \frac{\Gamma(1+1)}{\Gamma(1)\Gamma(1)} p^{1-1}(1-p)^{1-1} = 1 \quad \text{for } p \in [0, 1]\]</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Step 2: Write Down the Likelihood</p>
<p class="sd-card-text">The <strong>likelihood function</strong> gives the probability of the observed data given <span class="math notranslate nohighlight">\(p\)</span>:</p>
<div class="math notranslate nohighlight">
\[L(p | x, n) = P(X = 14 | n=20, p) = \binom{20}{14} p^{14} (1-p)^{6}\]</div>
<p class="sd-card-text">This is the Binomial probability mass function, viewed as a function of <span class="math notranslate nohighlight">\(p\)</span> (not <span class="math notranslate nohighlight">\(x\)</span>).</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Step 3: Apply Bayes’ Theorem</p>
<p class="sd-card-text">Bayes’ theorem combines prior and likelihood to get the <strong>posterior distribution</strong>:</p>
<div class="math notranslate nohighlight">
\[\pi(p | x) \propto L(p | x) \times \pi(p)\]</div>
<p class="sd-card-text">Substituting:</p>
<div class="math notranslate nohighlight">
\[\pi(p | x) \propto p^{14} (1-p)^{6} \times p^{1-1}(1-p)^{1-1} = p^{14}(1-p)^{6}\]</div>
<p class="sd-card-text">This is proportional to a Beta distribution! Specifically:</p>
<div class="math notranslate nohighlight">
\[p | X=14 \sim \text{Beta}(1 + 14, 1 + 6) = \text{Beta}(15, 7)\]</div>
<p class="sd-card-text">This “conjugacy” (Beta prior + Binomial likelihood = Beta posterior) is a beautiful mathematical convenience we’ll exploit throughout Chapter 5.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Step 4: Compute Posterior Summaries</p>
<p class="sd-card-text">The Beta(<span class="math notranslate nohighlight">\(\alpha\)</span>, <span class="math notranslate nohighlight">\(\beta\)</span>) distribution has known formulas:</p>
<ul class="simple">
<li><p class="sd-card-text"><strong>Mean</strong>: <span class="math notranslate nohighlight">\(\frac{\alpha}{\alpha + \beta} = \frac{15}{22} = 0.6818\)</span></p></li>
<li><p class="sd-card-text"><strong>Mode</strong>: <span class="math notranslate nohighlight">\(\frac{\alpha - 1}{\alpha + \beta - 2} = \frac{14}{20} = 0.70\)</span> (same as MLE!)</p></li>
<li><p class="sd-card-text"><strong>Variance</strong>: <span class="math notranslate nohighlight">\(\frac{\alpha \beta}{(\alpha+\beta)^2(\alpha+\beta+1)} = \frac{15 \times 7}{22^2 \times 23} = 0.00943\)</span></p></li>
</ul>
<p class="sd-card-text">The <strong>95% credible interval</strong> uses the 2.5th and 97.5th percentiles of the posterior.</p>
</div>
<p class="sd-card-text"><strong>Python Implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Prior parameters: Beta(α=1, β=1) = Uniform(0,1)</span>
<span class="n">alpha_prior</span><span class="p">,</span> <span class="n">beta_prior</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>

<span class="c1"># Data</span>
<span class="n">n</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">14</span>

<span class="c1"># Posterior parameters (conjugate update)</span>
<span class="n">alpha_post</span> <span class="o">=</span> <span class="n">alpha_prior</span> <span class="o">+</span> <span class="n">x</span>       <span class="c1"># 1 + 14 = 15</span>
<span class="n">beta_post</span> <span class="o">=</span> <span class="n">beta_prior</span> <span class="o">+</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span>   <span class="c1"># 1 + 6 = 7</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prior: Beta(α=</span><span class="si">{</span><span class="n">alpha_prior</span><span class="si">}</span><span class="s2">, β=</span><span class="si">{</span><span class="n">beta_prior</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Data: </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2"> successes in </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2"> trials&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Posterior: Beta(α=</span><span class="si">{</span><span class="n">alpha_post</span><span class="si">}</span><span class="s2">, β=</span><span class="si">{</span><span class="n">beta_post</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># Create posterior distribution object</span>
<span class="n">posterior</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">alpha_post</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">beta_post</span><span class="p">)</span>

<span class="c1"># Compute summaries</span>
<span class="n">post_mean</span> <span class="o">=</span> <span class="n">posterior</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">post_var</span> <span class="o">=</span> <span class="n">posterior</span><span class="o">.</span><span class="n">var</span><span class="p">()</span>
<span class="n">post_std</span> <span class="o">=</span> <span class="n">posterior</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
<span class="n">post_median</span> <span class="o">=</span> <span class="n">posterior</span><span class="o">.</span><span class="n">median</span><span class="p">()</span>
<span class="n">post_mode</span> <span class="o">=</span> <span class="p">(</span><span class="n">alpha_post</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">alpha_post</span> <span class="o">+</span> <span class="n">beta_post</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Posterior summaries:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean:   </span><span class="si">{</span><span class="n">post_mean</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Median: </span><span class="si">{</span><span class="n">post_median</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mode:   </span><span class="si">{</span><span class="n">post_mode</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Std:    </span><span class="si">{</span><span class="n">post_std</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 95% Equal-tailed credible interval</span>
<span class="n">ci_credible</span> <span class="o">=</span> <span class="n">posterior</span><span class="o">.</span><span class="n">ppf</span><span class="p">([</span><span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.975</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">95% Equal-tailed credible interval: (</span><span class="si">{</span><span class="n">ci_credible</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_credible</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Prior: Beta(α=1, β=1)
Data: 14 successes in 20 trials
Posterior: Beta(α=15, β=7)

Posterior summaries:
  Mean:   0.6818
  Median: 0.6874
  Mode:   0.7000
  Std:    0.0971

95% Equal-tailed credible interval: (0.4782, 0.8541)
</pre></div>
</div>
<p class="sd-card-text"><strong>Comparison to Frequentist Results:</strong></p>
<p class="sd-card-text">The posterior mean (0.6818) is <strong>shrunk toward 0.5</strong> compared to the MLE (0.70). Why? The Beta(1,1) prior adds “pseudocounts” of 1 success and 1 failure, pulling the estimate toward 0.5. With more data, this shrinkage becomes negligible.</p>
<p class="sd-card-text"><strong>Part (c): Simulation Verification</strong></p>
<div class="tip admonition">
<p class="admonition-title">Why Simulate?</p>
<p class="sd-card-text">Simulation serves as a “sanity check” for analytical calculations. If our credible interval is correct, then approximately 95% of samples drawn from the posterior should fall within it. This is a powerful verification technique we’ll use throughout the course.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Step 1: Draw Samples from the Posterior</p>
<p class="sd-card-text">Use <code class="docutils literal notranslate"><span class="pre">scipy.stats.beta.rvs()</span></code> to generate random samples from <span class="math notranslate nohighlight">\(\text{Beta}(\alpha=15, \beta=7)\)</span>:</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Step 2: Count Samples in the Interval</p>
<p class="sd-card-text">For each sample, check if it falls between the lower and upper bounds of our credible interval. The proportion should be approximately 0.95.</p>
</div>
<p class="sd-card-text"><strong>Python Implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Create a Generator for reproducibility</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Our posterior distribution: Beta(α=15, β=7)</span>
<span class="n">alpha_post</span><span class="p">,</span> <span class="n">beta_post</span> <span class="o">=</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">7</span>
<span class="n">posterior</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">alpha_post</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">beta_post</span><span class="p">)</span>

<span class="c1"># Get the 95% credible interval bounds</span>
<span class="n">ci_credible</span> <span class="o">=</span> <span class="n">posterior</span><span class="o">.</span><span class="n">ppf</span><span class="p">([</span><span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.975</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Posterior: Beta(α=</span><span class="si">{</span><span class="n">alpha_post</span><span class="si">}</span><span class="s2">, β=</span><span class="si">{</span><span class="n">beta_post</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% Credible interval: (</span><span class="si">{</span><span class="n">ci_credible</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_credible</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># Draw 10,000 samples from the posterior</span>
<span class="c1"># Pass rng to rvs() for reproducibility with the Generator API</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">posterior</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="c1"># Count how many fall within the credible interval</span>
<span class="n">in_interval</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">samples</span> <span class="o">&gt;=</span> <span class="n">ci_credible</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">samples</span> <span class="o">&lt;=</span> <span class="n">ci_credible</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">proportion_in</span> <span class="o">=</span> <span class="n">in_interval</span> <span class="o">/</span> <span class="n">n_samples</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Simulation results (</span><span class="si">{</span><span class="n">n_samples</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> samples):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Samples in CI: </span><span class="si">{</span><span class="n">in_interval</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">proportion_in</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Expected: ~</span><span class="si">{</span><span class="mf">0.95</span><span class="o">*</span><span class="n">n_samples</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2"> (95.00%)&quot;</span><span class="p">)</span>

<span class="c1"># Also verify moments match theory</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Moment verification:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Sample mean:  </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theoretical: </span><span class="si">{</span><span class="n">posterior</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Sample std:   </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theoretical: </span><span class="si">{</span><span class="n">posterior</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Sample median: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theoretical: </span><span class="si">{</span><span class="n">posterior</span><span class="o">.</span><span class="n">median</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Posterior: Beta(α=15, β=7)
95% Credible interval: (0.4782, 0.8541)

Simulation results (10,000 samples):
  Samples in CI: 9,518 (95.18%)
  Expected: ~9500 (95.00%)

Moment verification:
  Sample mean:  0.6830 (theoretical: 0.6818)
  Sample std:   0.0964 (theoretical: 0.0971)
  Sample median: 0.6881 (theoretical: 0.6874)
</pre></div>
</div>
<p class="sd-card-text"><strong>Interpretation:</strong> We got 95.18%, very close to the expected 95%. Small deviations are due to Monte Carlo sampling error. The Law of Large Numbers guarantees this proportion converges to 0.95 as we increase the number of samples.</p>
<p class="sd-card-text"><strong>Part (d): Interpretation—The Fundamental Difference</strong></p>
<div class="important admonition">
<p class="admonition-title">The Key Question</p>
<p class="sd-card-text">A scientist asks: <em>“What’s the probability that :math:`p &gt; 0.5`?”</em></p>
<p class="sd-card-text">This seemingly simple question has fundamentally different answers depending on your statistical paradigm. Understanding this difference is crucial for interpreting results correctly.</p>
</div>
<p class="sd-card-text"><strong>Frequentist Interpretation of the Confidence Interval:</strong></p>
<p class="sd-card-text">The 95% Clopper-Pearson interval (0.4572, 0.8811) does <strong>NOT</strong> mean:</p>
<blockquote>
<div><p class="sd-card-text">❌ “There is a 95% probability that <span class="math notranslate nohighlight">\(p\)</span> lies in (0.4572, 0.8811)”</p>
</div></blockquote>
<p class="sd-card-text">It <strong>DOES</strong> mean:</p>
<blockquote>
<div><p class="sd-card-text">✓ “If we repeated this experiment many times (flip 20 coins, compute CI), then 95% of the resulting intervals would contain the true <span class="math notranslate nohighlight">\(p\)</span>.”</p>
</div></blockquote>
<p class="sd-card-text">The parameter <span class="math notranslate nohighlight">\(p\)</span> is a fixed (unknown) constant—it’s either in this interval or it isn’t. The probability statement is about the <strong>procedure</strong>, not about this specific interval.</p>
<p class="sd-card-text"><strong>Bayesian Interpretation of the Credible Interval:</strong></p>
<p class="sd-card-text">The 95% credible interval (0.4782, 0.8541) means exactly what it sounds like:</p>
<blockquote>
<div><p class="sd-card-text">✓ “Given our prior beliefs and the observed data, there is a 95% probability that <span class="math notranslate nohighlight">\(p\)</span> lies in (0.4782, 0.8541).”</p>
</div></blockquote>
<p class="sd-card-text">This is a direct probability statement about the parameter, conditioned on our information.</p>
<p class="sd-card-text"><strong>Answering “What’s the probability that p &gt; 0.5?”</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># ----- Bayesian Answer -----</span>
<span class="n">posterior</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">prob_gt_half</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">posterior</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>  <span class="c1"># P(p &gt; 0.5 | data)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;BAYESIAN APPROACH:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  P(p &gt; 0.5 | data) = </span><span class="si">{</span><span class="n">prob_gt_half</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> = </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">prob_gt_half</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Interpretation: &#39;There is a </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">prob_gt_half</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">% probability&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;                   that the true p exceeds 0.5.&#39;&quot;</span><span class="p">)</span>

<span class="c1"># ----- Frequentist Answer -----</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">FREQUENTIST APPROACH:&quot;</span><span class="p">)</span>
<span class="n">p_hat</span> <span class="o">=</span> <span class="mi">14</span><span class="o">/</span><span class="mi">20</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">20</span>

<span class="c1"># Test H0: p ≤ 0.5 vs H1: p &gt; 0.5</span>
<span class="c1"># Under H0, use the boundary value p = 0.5 for the standard error</span>
<span class="n">se_null</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="mf">0.5</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span>  <span class="c1"># SE under null hypothesis</span>
<span class="n">z</span> <span class="o">=</span> <span class="p">(</span><span class="n">p_hat</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">/</span> <span class="n">se_null</span>
<span class="n">p_value</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Test: H0: p ≤ 0.5 vs H1: p &gt; 0.5&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  z-statistic = (</span><span class="si">{</span><span class="n">p_hat</span><span class="si">}</span><span class="s2"> - 0.5) / </span><span class="si">{</span><span class="n">se_null</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> = </span><span class="si">{</span><span class="n">z</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  p-value = P(Z &gt; </span><span class="si">{</span><span class="n">z</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">) = </span><span class="si">{</span><span class="n">p_value</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  At α = 0.05: </span><span class="si">{</span><span class="s1">&#39;Reject H0&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">p_value</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mf">0.05</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;Fail to reject H0&#39;</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">  Interpretation: &#39;I cannot assign a probability to p &gt; 0.5.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;                   But if p ≤ 0.5 (testing at boundary p = 0.5),&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;                   we would see 14+ heads only </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">p_value</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">% of the time.&#39;&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>BAYESIAN APPROACH:
  P(p &gt; 0.5 | data) = 0.9608 = 96.08%
  Interpretation: &#39;There is a 96.1% probability
                   that the true p exceeds 0.5.&#39;

FREQUENTIST APPROACH:
  Test: H0: p ≤ 0.5 vs H1: p &gt; 0.5
  z-statistic = (0.7 - 0.5) / 0.1118 = 1.7889
  p-value = P(Z &gt; 1.7889) = 0.0368
  At α = 0.05: Reject H0

  Interpretation: &#39;I cannot assign a probability to p &gt; 0.5.
                   But if p ≤ 0.5 (testing at boundary p = 0.5),
                   we would see 14+ heads only 3.7% of the time.&#39;
</pre></div>
</div>
<p class="sd-card-text"><strong>Summary Table:</strong></p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Aspect</p></th>
<th class="head"><p class="sd-card-text">Frequentist</p></th>
<th class="head"><p class="sd-card-text">Bayesian</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text">Parameters</p></td>
<td><p class="sd-card-text">Fixed, unknown constants</p></td>
<td><p class="sd-card-text">Random variables with distributions</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text">Probability applies to</p></td>
<td><p class="sd-card-text">Data, procedures, long-run frequencies</p></td>
<td><p class="sd-card-text">Parameters, hypotheses, beliefs</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text">Answer to “P(p&gt;0.5)?”</p></td>
<td><p class="sd-card-text">“Cannot answer; p is fixed”</p></td>
<td><p class="sd-card-text">“96.08%, given data and prior”</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text">What 95% means</p></td>
<td><p class="sd-card-text">95% of CIs from repeated sampling
would contain true p</p></td>
<td><p class="sd-card-text">95% probability p is in this interval
(given our information)</p></td>
</tr>
</tbody>
</table>
<p class="sd-card-text">The Bayesian directly answers the scientist’s question. The frequentist provides related but fundamentally different information—evidence against <span class="math notranslate nohighlight">\(p \leq 0.5\)</span> rather than the probability that <span class="math notranslate nohighlight">\(p &gt; 0.5\)</span>.</p>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 2: Distribution Relationships Through Simulation</p>
<p>The Poisson limit theorem states that <span class="math notranslate nohighlight">\(\text{Binomial}(n, p=\lambda/n) \to \text{Poisson}(\lambda)\)</span> as <span class="math notranslate nohighlight">\(n \to \infty\)</span>.</p>
<ol class="loweralpha">
<li><p>For <span class="math notranslate nohighlight">\(\lambda = 4\)</span>, generate 10,000 samples from <span class="math notranslate nohighlight">\(\text{Binomial}(n, p=4/n)\)</span> for <span class="math notranslate nohighlight">\(n \in \{10, 50, 200, 1000\}\)</span> and from <span class="math notranslate nohighlight">\(\text{Poisson}(\lambda=4)\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Why This Works</p>
<p>The Poisson approximation models “rare events in many trials.” If we have <span class="math notranslate nohighlight">\(n\)</span> independent trials each with small success probability <span class="math notranslate nohighlight">\(p = \lambda/n\)</span>, the expected number of successes is <span class="math notranslate nohighlight">\(np = \lambda\)</span>. As <span class="math notranslate nohighlight">\(n \to \infty\)</span> with <span class="math notranslate nohighlight">\(\lambda\)</span> fixed, the Binomial converges to Poisson(<span class="math notranslate nohighlight">\(\lambda\)</span>).</p>
<p>Use <code class="docutils literal notranslate"><span class="pre">rng.binomial(n=n,</span> <span class="pre">p=lam/n,</span> <span class="pre">size=10000)</span></code> and <code class="docutils literal notranslate"><span class="pre">rng.poisson(lam=lam,</span> <span class="pre">size=10000)</span></code>.</p>
</div>
</li>
<li><p>For each sample, compute the sample mean and variance. The Poisson distribution has the property that mean equals variance. How quickly does the Binomial approach this property?</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Variance Comparison</p>
<p>For <span class="math notranslate nohighlight">\(\text{Binomial}(n, p)\)</span>: mean = <span class="math notranslate nohighlight">\(np\)</span>, variance = <span class="math notranslate nohighlight">\(np(1-p) = \lambda(1 - \lambda/n)\)</span>.</p>
<p>For <span class="math notranslate nohighlight">\(\text{Poisson}(\lambda)\)</span>: mean = variance = <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<p>The ratio Var/Mean for Binomial is <span class="math notranslate nohighlight">\(1 - \lambda/n\)</span>, which approaches 1 as <span class="math notranslate nohighlight">\(n \to \infty\)</span>.</p>
</div>
</li>
<li><p>Create a visualization showing the PMFs converging. Use <code class="docutils literal notranslate"><span class="pre">scipy.stats.binom.pmf</span></code> and <code class="docutils literal notranslate"><span class="pre">scipy.stats.poisson.pmf</span></code> to overlay theoretical PMFs on your histograms.</p></li>
<li><p>Conduct a chi-square goodness-of-fit test comparing each Binomial sample to the <span class="math notranslate nohighlight">\(\text{Poisson}(\lambda=4)\)</span> distribution. How does the p-value change with <span class="math notranslate nohighlight">\(n\)</span>?</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Chi-Square Test Setup</p>
<ol class="arabic simple">
<li><p>Bin observed counts: count how many samples equal 0, 1, 2, …, and group large values</p></li>
<li><p>Compute expected counts under <span class="math notranslate nohighlight">\(\text{Poisson}(\lambda=4)\)</span>: <span class="math notranslate nohighlight">\(E_k = n_{\text{samples}} \times P(X = k)\)</span></p></li>
<li><p>Combine bins where expected count &lt; 5 (standard chi-square requirement)</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">scipy.stats.chisquare(observed,</span> <span class="pre">expected)</span></code></p></li>
</ol>
</div>
</li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a) &amp; (b): Sample Generation and Statistics</strong></p>
<p class="sd-card-text">The Poisson limit theorem captures what happens when we have many trials with small success probability but constant expected count <span class="math notranslate nohighlight">\(\lambda = np\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Create a Generator for reproducibility</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">lam</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># Rate parameter λ</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">n_values</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Distribution&#39;</span><span class="si">:</span><span class="s2">&lt;30</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Mean&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Variance&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Var/Mean&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">62</span><span class="p">)</span>

<span class="n">samples_dict</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">n_values</span><span class="p">:</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">lam</span> <span class="o">/</span> <span class="n">n</span>
    <span class="c1"># Binomial(n=n, p=λ/n) samples</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="n">samples_dict</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">samples</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
    <span class="n">var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Binomial(n=</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">, p=</span><span class="si">{</span><span class="n">lam</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">)</span><span class="si">{</span><span class="s1">&#39;&#39;</span><span class="si">:</span><span class="s2">&lt;6</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">mean</span><span class="si">:</span><span class="s2">&gt;10.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">var</span><span class="si">:</span><span class="s2">&gt;10.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">var</span><span class="o">/</span><span class="n">mean</span><span class="si">:</span><span class="s2">&gt;10.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Poisson(λ=4) samples</span>
<span class="n">poisson_samples</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="n">lam</span><span class="o">=</span><span class="n">lam</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_samples</span><span class="p">)</span>
<span class="n">samples_dict</span><span class="p">[</span><span class="s1">&#39;Poisson&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">poisson_samples</span>
<span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">poisson_samples</span><span class="p">)</span>
<span class="n">var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">poisson_samples</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Poisson(λ=4)&#39;</span><span class="si">:</span><span class="s2">&lt;30</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">mean</span><span class="si">:</span><span class="s2">&gt;10.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">var</span><span class="si">:</span><span class="s2">&gt;10.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">var</span><span class="o">/</span><span class="n">mean</span><span class="si">:</span><span class="s2">&gt;10.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Distribution                         Mean   Variance   Var/Mean
--------------------------------------------------------------
Binomial(n=10, p=4/10)             3.9716     2.3488     0.5914
Binomial(n=50, p=4/50)             4.0289     3.7286     0.9255
Binomial(n=200, p=4/200)           3.9979     3.8705     0.9681
Binomial(n=1000, p=4/1000)         3.9904     3.9647     0.9936
Poisson(λ=4)                       4.0116     4.0369     1.0063
</pre></div>
</div>
<p class="sd-card-text"><strong>Key insight:</strong> For <span class="math notranslate nohighlight">\(\text{Binomial}(n, p)\)</span>, variance <span class="math notranslate nohighlight">\(= np(1-p) = \lambda(1 - \lambda/n)\)</span>. As <span class="math notranslate nohighlight">\(n \to \infty\)</span>, this approaches <span class="math notranslate nohighlight">\(\lambda\)</span>, matching Poisson. The Var/Mean ratio is <span class="math notranslate nohighlight">\(1 - \lambda/n\)</span>, so it converges to 1 at rate <span class="math notranslate nohighlight">\(O(1/n)\)</span>.</p>
<p class="sd-card-text"><strong>Part (c): Visualization of Convergence</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">lam</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># Rate parameter λ</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">n_values</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">x_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flat</span><span class="p">,</span> <span class="n">n_values</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">lam</span> <span class="o">/</span> <span class="n">n</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Histogram of samples</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">15.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Binomial samples&#39;</span><span class="p">)</span>

    <span class="c1"># Overlay Binomial(n, p) PMF</span>
    <span class="n">binom_pmf</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">binom_pmf</span><span class="p">,</span> <span class="s1">&#39;b-o&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Binomial(n=</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s1">, p=</span><span class="si">{</span><span class="n">p</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">) PMF&#39;</span><span class="p">)</span>

    <span class="c1"># Overlay Poisson(λ=4) PMF</span>
    <span class="n">poisson_pmf</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">lam</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">poisson_pmf</span><span class="p">,</span> <span class="s1">&#39;r--s&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Poisson(λ=</span><span class="si">{</span><span class="n">lam</span><span class="si">}</span><span class="s1">) PMF&#39;</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Probability&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Binomial(n=</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s1">, p=</span><span class="si">{</span><span class="n">lam</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s1">) vs Poisson(λ=</span><span class="si">{</span><span class="n">lam</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">14.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;poisson_limit_convergence.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/Part1/ex2_poisson_limit.png"><img alt="Four panel plot showing Binomial distributions converging to Poisson as n increases from 10 to 1000" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/Part1/ex2_poisson_limit.png" style="width: 90%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 1.25 </span><span class="caption-text"><strong>Poisson Limit Theorem Visualization.</strong> As <span class="math notranslate nohighlight">\(n\)</span> increases, the <span class="math notranslate nohighlight">\(\text{Binomial}(n, p=4/n)\)</span> PMF (blue circles) converges to the <span class="math notranslate nohighlight">\(\text{Poisson}(\lambda=4)\)</span> PMF (red squares). By <span class="math notranslate nohighlight">\(n = 1000\)</span>, the two distributions are visually indistinguishable.</span><a class="headerlink" href="#id1" title="Link to this image"></a></p>
</figcaption>
</figure>
<p class="sd-card-text">The convergence rate can also be visualized by tracking the variance-to-mean ratio:</p>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/Part1/ex2_variance_convergence.png"><img alt="Semi-log plot showing variance over mean ratio converging to 1 as n increases" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/Part1/ex2_variance_convergence.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 1.26 </span><span class="caption-text"><strong>Variance/Mean Ratio Convergence.</strong> The ratio approaches 1 (the Poisson property) at rate <span class="math notranslate nohighlight">\(O(1/n)\)</span>.</span><a class="headerlink" href="#id2" title="Link to this image"></a></p>
</figcaption>
</figure>
<p class="sd-card-text"><strong>Part (d): Chi-Square Goodness-of-Fit Tests</strong></p>
<p class="sd-card-text">We test <span class="math notranslate nohighlight">\(H_0\)</span>: the sample comes from <span class="math notranslate nohighlight">\(\text{Poisson}(\lambda=4)\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">lam</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># Rate parameter λ</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;H₀: Sample comes from Poisson(λ=</span><span class="si">{</span><span class="n">lam</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Distribution&#39;</span><span class="si">:</span><span class="s2">&lt;30</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;χ² stat&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;p-value&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Result&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>

<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="s1">&#39;Poisson&#39;</span><span class="p">]:</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">==</span> <span class="s1">&#39;Poisson&#39;</span><span class="p">:</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="n">lam</span><span class="o">=</span><span class="n">lam</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_samples</span><span class="p">)</span>
        <span class="n">name</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;Poisson(λ=</span><span class="si">{</span><span class="n">lam</span><span class="si">}</span><span class="s1">)&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">lam</span><span class="o">/</span><span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_samples</span><span class="p">)</span>
        <span class="n">name</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;Binomial(n=</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s1">, p=</span><span class="si">{</span><span class="n">lam</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s1">)&#39;</span>

    <span class="c1"># Bin counts: 0, 1, 2, ..., 10, 11+</span>
    <span class="n">observed</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">samples</span> <span class="o">==</span> <span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">)]</span>
    <span class="n">observed</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">samples</span> <span class="o">&gt;=</span> <span class="mi">11</span><span class="p">))</span>
    <span class="n">observed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">observed</span><span class="p">)</span>

    <span class="c1"># Expected under Poisson(λ=4)</span>
    <span class="n">expected_probs</span> <span class="o">=</span> <span class="p">[</span><span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">lam</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">)]</span>
    <span class="n">expected_probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">lam</span><span class="p">))</span>
    <span class="n">expected</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">expected_probs</span><span class="p">)</span> <span class="o">*</span> <span class="n">n_samples</span>

    <span class="c1"># Combine bins where expected &lt; 5 (standard chi-square requirement)</span>
    <span class="n">obs_comb</span><span class="p">,</span> <span class="n">exp_comb</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="n">obs_acc</span><span class="p">,</span> <span class="n">exp_acc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">o</span><span class="p">,</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">observed</span><span class="p">,</span> <span class="n">expected</span><span class="p">):</span>
        <span class="n">obs_acc</span> <span class="o">+=</span> <span class="n">o</span>
        <span class="n">exp_acc</span> <span class="o">+=</span> <span class="n">e</span>
        <span class="k">if</span> <span class="n">exp_acc</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">:</span>
            <span class="n">obs_comb</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">obs_acc</span><span class="p">)</span>
            <span class="n">exp_comb</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">exp_acc</span><span class="p">)</span>
            <span class="n">obs_acc</span><span class="p">,</span> <span class="n">exp_acc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">exp_acc</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">obs_comb</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">obs_acc</span>
        <span class="n">exp_comb</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">exp_acc</span>

    <span class="n">chi2_stat</span><span class="p">,</span> <span class="n">p_val</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">chisquare</span><span class="p">(</span><span class="n">obs_comb</span><span class="p">,</span> <span class="n">exp_comb</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="s2">&quot;Fail to reject&quot;</span> <span class="k">if</span> <span class="n">p_val</span> <span class="o">&gt;</span> <span class="mf">0.05</span> <span class="k">else</span> <span class="s2">&quot;Reject H₀&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">:</span><span class="s2">&lt;30</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">chi2_stat</span><span class="si">:</span><span class="s2">&gt;12.2f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">p_val</span><span class="si">:</span><span class="s2">&gt;10.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>H₀: Sample comes from Poisson(λ=4)
Distribution                       χ² stat    p-value          Result
----------------------------------------------------------------------
Binomial(n=10, p=4/10)              971.01     0.0000       Reject H₀
Binomial(n=50, p=4/50)               35.58     0.0002       Reject H₀
Binomial(n=200, p=4/200)             12.16     0.3519  Fail to reject
Binomial(n=1000, p=4/1000)            3.10     0.9893  Fail to reject
Poisson(λ=4)                         11.03     0.4409  Fail to reject
</pre></div>
</div>
<p class="sd-card-text"><strong>Interpretation:</strong> The chi-square test quantifies how distinguishable the Binomial is from Poisson:</p>
<ul class="simple">
<li><p class="sd-card-text">At <span class="math notranslate nohighlight">\(n = 10\)</span>, the Binomial is <strong>decisively different</strong> (<span class="math notranslate nohighlight">\(\chi^2 = 971\)</span>, <span class="math notranslate nohighlight">\(p \approx 0\)</span>)</p></li>
<li><p class="sd-card-text">At <span class="math notranslate nohighlight">\(n = 50\)</span>, still significantly different (<span class="math notranslate nohighlight">\(p = 0.0002\)</span>)</p></li>
<li><p class="sd-card-text">At <span class="math notranslate nohighlight">\(n = 200\)</span>, no longer statistically distinguishable (<span class="math notranslate nohighlight">\(p = 0.35\)</span>)</p></li>
<li><p class="sd-card-text">At <span class="math notranslate nohighlight">\(n = 1000\)</span>, virtually identical (<span class="math notranslate nohighlight">\(p = 0.99\)</span>)</p></li>
</ul>
<p class="sd-card-text">With 10,000 samples, we have high power to detect small differences. The transition around <span class="math notranslate nohighlight">\(n = 100\text{-}200\)</span> is where the Poisson approximation becomes practically useful for typical statistical tests.</p>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 3: The Bootstrap Preview</p>
<p>The bootstrap (Chapter 4) estimates sampling distributions by resampling observed data. This exercise previews the idea.</p>
<div class="note admonition">
<p class="admonition-title">Background: The Sampling Distribution Problem</p>
<p>When we compute a statistic like <span class="math notranslate nohighlight">\(\bar{x}\)</span> from a sample, we get one number. But if we collected a different sample, we’d get a different <span class="math notranslate nohighlight">\(\bar{x}\)</span>. The <strong>sampling distribution</strong> describes this variability—but we only have one sample! The bootstrap solves this by treating our sample as a miniature population.</p>
</div>
<ol class="loweralpha">
<li><p>Generate a sample of <span class="math notranslate nohighlight">\(n = 50\)</span> observations from <span class="math notranslate nohighlight">\(\text{Gamma}(\text{shape}=3, \text{scale}=2)\)</span>. Compute the sample mean <span class="math notranslate nohighlight">\(\bar{x}\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Gamma Distribution Properties</p>
<p>For <span class="math notranslate nohighlight">\(\text{Gamma}(\text{shape}=\alpha, \text{scale}=\theta)\)</span>:</p>
<ul class="simple">
<li><p>Mean: <span class="math notranslate nohighlight">\(\mu = \alpha \theta\)</span></p></li>
<li><p>Variance: <span class="math notranslate nohighlight">\(\sigma^2 = \alpha \theta^2\)</span></p></li>
</ul>
<p>Use <code class="docutils literal notranslate"><span class="pre">rng.gamma(shape=3,</span> <span class="pre">scale=2,</span> <span class="pre">size=50)</span></code> where <code class="docutils literal notranslate"><span class="pre">rng</span> <span class="pre">=</span> <span class="pre">np.random.default_rng(seed=42)</span></code>.</p>
</div>
</li>
<li><p>Create 5,000 bootstrap samples by sampling <span class="math notranslate nohighlight">\(n = 50\)</span> observations <em>with replacement</em> from your original sample. For each bootstrap sample, compute the mean <span class="math notranslate nohighlight">\(\bar{x}^*_b\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Resampling With Replacement</p>
<p>“With replacement” means each draw is independent—after drawing an observation, it goes back into the pool. This means:</p>
<ul class="simple">
<li><p>Some original observations will appear multiple times in a bootstrap sample</p></li>
<li><p>Some original observations won’t appear at all</p></li>
<li><p>Each bootstrap sample has the same size as the original (<span class="math notranslate nohighlight">\(n = 50\)</span>)</p></li>
</ul>
<p>Use <code class="docutils literal notranslate"><span class="pre">rng.choice(original_sample,</span> <span class="pre">size=n,</span> <span class="pre">replace=True)</span></code> for each bootstrap iteration.</p>
</div>
</li>
<li><p>The bootstrap distribution of <span class="math notranslate nohighlight">\(\bar{x}^*\)</span> approximates the sampling distribution of <span class="math notranslate nohighlight">\(\bar{x}\)</span>. Compute the 2.5th and 97.5th percentiles of your bootstrap means to form a 95% bootstrap confidence interval.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Percentile Method</p>
<p>The simplest bootstrap CI uses empirical percentiles:</p>
<ul class="simple">
<li><p>Lower bound = 2.5th percentile of bootstrap means</p></li>
<li><p>Upper bound = 97.5th percentile of bootstrap means</p></li>
</ul>
<p>Use <code class="docutils literal notranslate"><span class="pre">np.percentile(bootstrap_means,</span> <span class="pre">[2.5,</span> <span class="pre">97.5])</span></code>.</p>
</div>
</li>
<li><p>Compare to the theoretical sampling distribution: <span class="math notranslate nohighlight">\(\bar{X} \sim \text{approximately } N(\mu, \sigma^2/n)\)</span> where <span class="math notranslate nohighlight">\(\mu = 6\)</span> and <span class="math notranslate nohighlight">\(\sigma^2 = 12\)</span> for <span class="math notranslate nohighlight">\(\text{Gamma}(\text{shape}=3, \text{scale}=2)\)</span>. How well does the bootstrap interval match the theoretical interval?</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Central Limit Theorem</p>
<p>By CLT, <span class="math notranslate nohighlight">\(\bar{X} \approx N(\mu, \sigma^2/n)\)</span>, so the theoretical SE is <span class="math notranslate nohighlight">\(\sigma/\sqrt{n}\)</span>. Compare:</p>
<ul class="simple">
<li><p><strong>Theoretical SE</strong>: <span class="math notranslate nohighlight">\(\sqrt{12}/\sqrt{50} \approx 0.49\)</span> (requires knowing <span class="math notranslate nohighlight">\(\sigma\)</span>)</p></li>
<li><p><strong>Bootstrap SE</strong>: standard deviation of bootstrap means (requires only the sample)</p></li>
</ul>
</div>
</li>
<li><p>Discuss: The bootstrap works without knowing the true distribution. Why is this valuable in practice?</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p class="sd-card-text">This exercise previews <strong>bootstrap methods</strong>, which we will study comprehensively in <span class="xref std std-ref">Chapter 4: Resampling Methods</span>. Here we build intuition; Chapter 4 develops the theory, variants (percentile, BCa, parametric bootstrap), and applications.</p>
</div>
<p class="sd-card-text"><strong>The Big Idea Behind the Bootstrap</strong></p>
<p class="sd-card-text">We want to understand the <strong>sampling distribution</strong> of a statistic (like the sample mean)—how much would it vary if we could repeat our experiment many times? The problem: we only have one sample!</p>
<p class="sd-card-text"><strong>The bootstrap solution:</strong> Treat your sample as a stand-in for the population. Resample from it (with replacement) to simulate repeated sampling. The variation in the resampled statistics approximates the true sampling variation.</p>
<p class="sd-card-text"><strong>Part (a): Generate Original Sample</strong></p>
<div class="tip admonition">
<p class="admonition-title">Step 1: Understand the True Distribution</p>
<p class="sd-card-text">We’re sampling from <span class="math notranslate nohighlight">\(\text{Gamma}(\text{shape}=3, \text{scale}=2)\)</span>. The Gamma distribution has:</p>
<ul class="simple">
<li><p class="sd-card-text"><strong>Mean</strong>: <span class="math notranslate nohighlight">\(\mu = \text{shape} \times \text{scale} = 3 \times 2 = 6\)</span></p></li>
<li><p class="sd-card-text"><strong>Variance</strong>: <span class="math notranslate nohighlight">\(\sigma^2 = \text{shape} \times \text{scale}^2 = 3 \times 4 = 12\)</span></p></li>
<li><p class="sd-card-text"><strong>Standard deviation</strong>: <span class="math notranslate nohighlight">\(\sigma = \sqrt{12} \approx 3.464\)</span></p></li>
</ul>
<p class="sd-card-text">In practice, we wouldn’t know these true values—that’s the whole point of the bootstrap!</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Step 2: Generate the Sample</p>
<p class="sd-card-text">Use NumPy’s modern <code class="docutils literal notranslate"><span class="pre">Generator</span></code> API via <code class="docutils literal notranslate"><span class="pre">np.random.default_rng()</span></code> for reproducible random number generation. This is preferred over the legacy <code class="docutils literal notranslate"><span class="pre">np.random.seed()</span></code> approach because:</p>
<ul class="simple">
<li><p class="sd-card-text">Each <code class="docutils literal notranslate"><span class="pre">Generator</span></code> instance has its own independent state</p></li>
<li><p class="sd-card-text">It uses the PCG64 algorithm (better statistical properties than the legacy Mersenne Twister)</p></li>
<li><p class="sd-card-text">It’s safer for parallel computing (no global state)</p></li>
</ul>
</div>
<p class="sd-card-text"><strong>Python Implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Create a Generator with a fixed seed for reproducibility</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># True distribution parameters (in practice, unknown!)</span>
<span class="n">shape</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">50</span>

<span class="c1"># Generate our &quot;observed&quot; sample using rng.gamma()</span>
<span class="c1"># Gamma(shape=3, scale=2) has mean = shape*scale = 6</span>
<span class="n">original_sample</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># Compute sample statistics</span>
<span class="n">sample_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">original_sample</span><span class="p">)</span>
<span class="n">sample_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">original_sample</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># ddof=1 for sample std</span>
<span class="n">sample_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">original_sample</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ORIGINAL SAMPLE&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample size: n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">True distribution: Gamma(shape=</span><span class="si">{</span><span class="n">shape</span><span class="si">}</span><span class="s2">, scale=</span><span class="si">{</span><span class="n">scale</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  True mean μ = </span><span class="si">{</span><span class="n">shape</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">scale</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  True variance σ² = </span><span class="si">{</span><span class="n">shape</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">scale</span><span class="o">**</span><span class="mi">2</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  True std σ = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">shape</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">scale</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Sample statistics:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Sample mean x̄ = </span><span class="si">{</span><span class="n">sample_mean</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Sample variance s² = </span><span class="si">{</span><span class="n">sample_var</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Sample std s = </span><span class="si">{</span><span class="n">sample_std</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">First 10 observations: </span><span class="si">{</span><span class="n">original_sample</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>==================================================
ORIGINAL SAMPLE
==================================================
Sample size: n = 50

True distribution: Gamma(shape=3, scale=2)
  True mean μ = 6
  True variance σ² = 12
  True std σ = 3.4641

Sample statistics:
  Sample mean x̄ = 5.8309
  Sample variance s² = 7.0480
  Sample std s = 2.6548

First 10 observations: [5.99 3.09 6.52 7.83 4.69 3.67 7.15 4.91 4.03 5.72]
</pre></div>
</div>
<p class="sd-card-text">Notice that our sample mean (5.83) is below the true mean (6.00), and our sample standard deviation (2.65) is below the true σ (3.46). This is just sampling variability—a different seed would give different values.</p>
<p class="sd-card-text"><strong>Part (b): Bootstrap Resampling</strong></p>
<div class="tip admonition">
<p class="admonition-title">Step 3: Understand Bootstrap Resampling</p>
<p class="sd-card-text"><strong>Key insight:</strong> We resample <strong>with replacement</strong> from our original sample. This means:</p>
<ul class="simple">
<li><p class="sd-card-text">Each bootstrap sample has the same size (<span class="math notranslate nohighlight">\(n = 50\)</span>) as the original</p></li>
<li><p class="sd-card-text">Some original observations appear multiple times, others don’t appear at all</p></li>
<li><p class="sd-card-text">Each bootstrap sample gives a different bootstrap statistic <span class="math notranslate nohighlight">\(\bar{x}^*_b\)</span></p></li>
</ul>
<p class="sd-card-text">The distribution of <span class="math notranslate nohighlight">\(\bar{x}^*_b\)</span> values across many bootstrap samples approximates the sampling distribution of <span class="math notranslate nohighlight">\(\bar{x}\)</span>.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Step 4: Generate Bootstrap Samples</p>
<p class="sd-card-text">For each of <span class="math notranslate nohighlight">\(B = 5000\)</span> bootstrap iterations:</p>
<ol class="arabic simple">
<li><p class="sd-card-text">Draw <span class="math notranslate nohighlight">\(n = 50\)</span> observations with replacement from <code class="docutils literal notranslate"><span class="pre">original_sample</span></code></p></li>
<li><p class="sd-card-text">Compute the mean of this bootstrap sample</p></li>
<li><p class="sd-card-text">Store the result</p></li>
</ol>
<p class="sd-card-text">Use <code class="docutils literal notranslate"><span class="pre">rng.choice(...,</span> <span class="pre">replace=True)</span></code> for resampling with the Generator.</p>
</div>
<p class="sd-card-text"><strong>Python Implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Bootstrap parameters</span>
<span class="n">n_bootstrap</span> <span class="o">=</span> <span class="mi">5000</span>  <span class="c1"># Number of bootstrap samples (B)</span>

<span class="c1"># Storage for bootstrap statistics</span>
<span class="n">bootstrap_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_bootstrap</span><span class="p">)</span>

<span class="c1"># Generate bootstrap samples and compute means</span>
<span class="c1"># Using the same rng ensures reproducibility</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_bootstrap</span><span class="p">):</span>
    <span class="c1"># Resample WITH REPLACEMENT from original data</span>
    <span class="n">boot_sample</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">original_sample</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">bootstrap_means</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">boot_sample</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;BOOTSTRAP RESAMPLING&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of bootstrap samples: B = </span><span class="si">{</span><span class="n">n_bootstrap</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Bootstrap distribution of sample mean:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean of bootstrap means: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">bootstrap_means</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Std of bootstrap means (= Bootstrap SE): </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">bootstrap_means</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Min bootstrap mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">bootstrap_means</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Max bootstrap mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">bootstrap_means</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># The bootstrap SE estimates the standard error of the sample mean</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Comparison:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Bootstrap SE: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">bootstrap_means</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Theoretical SE (s/√n): </span><span class="si">{</span><span class="n">sample_std</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>==================================================
BOOTSTRAP RESAMPLING
==================================================
Number of bootstrap samples: B = 5,000

Bootstrap distribution of sample mean:
  Mean of bootstrap means: 5.8206
  Std of bootstrap means (= Bootstrap SE): 0.3735
  Min bootstrap mean: 4.6821
  Max bootstrap mean: 7.0156

Comparison:
  Bootstrap SE: 0.3735
  Theoretical SE (s/√n): 0.3754
</pre></div>
</div>
<p class="sd-card-text"><strong>Key observation:</strong> The bootstrap SE (0.3735) closely matches <span class="math notranslate nohighlight">\(s/\sqrt{n}\)</span> (0.3754). The bootstrap automatically estimates the standard error without any formulas!</p>
<p class="sd-card-text"><strong>Part (c): Bootstrap Confidence Interval</strong></p>
<div class="tip admonition">
<p class="admonition-title">Step 5: Compute the Percentile Bootstrap CI</p>
<p class="sd-card-text">The <strong>percentile method</strong> is the simplest bootstrap CI:</p>
<ul class="simple">
<li><p class="sd-card-text">Lower bound = 2.5th percentile of bootstrap means</p></li>
<li><p class="sd-card-text">Upper bound = 97.5th percentile of bootstrap means</p></li>
</ul>
<p class="sd-card-text">This captures the middle 95% of the bootstrap distribution.</p>
<p class="sd-card-text"><em>Note: Chapter 4 will cover more sophisticated methods like the BCa (bias-corrected and accelerated) interval.</em></p>
</div>
<p class="sd-card-text"><strong>Python Implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute percentile bootstrap CI</span>
<span class="n">ci_lower</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">bootstrap_means</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">)</span>
<span class="n">ci_upper</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">bootstrap_means</span><span class="p">,</span> <span class="mf">97.5</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;BOOTSTRAP CONFIDENCE INTERVAL&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% Bootstrap CI (percentile method):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Lower bound (2.5th percentile): </span><span class="si">{</span><span class="n">ci_lower</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Upper bound (97.5th percentile): </span><span class="si">{</span><span class="n">ci_upper</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Width: </span><span class="si">{</span><span class="n">ci_upper</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">ci_lower</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Interpretation: We are 95% confident that the&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;true population mean lies in (</span><span class="si">{</span><span class="n">ci_lower</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_upper</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>==================================================
BOOTSTRAP CONFIDENCE INTERVAL
==================================================
95% Bootstrap CI (percentile method):
  Lower bound (2.5th percentile): 5.1034
  Upper bound (97.5th percentile): 6.5651
  Width: 1.4616

Interpretation: We are 95% confident that the
true population mean lies in (5.1034, 6.5651)
</pre></div>
</div>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/Part1/ex3_bootstrap_distribution.png"><img alt="Two panel plot showing original Gamma sample and bootstrap distribution of sample means with 95% CI marked" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/Part1/ex3_bootstrap_distribution.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 1.27 </span><span class="caption-text"><strong>Bootstrap Distribution Visualization.</strong> Left: Original sample from Gamma(3,2) with true density overlaid. Right: Bootstrap distribution of sample means (5,000 resamples) with 95% percentile confidence interval marked in red.</span><a class="headerlink" href="#id3" title="Link to this image"></a></p>
</figcaption>
</figure>
<p class="sd-card-text"><strong>Part (d): Comparison to Theoretical Distribution</strong></p>
<div class="tip admonition">
<p class="admonition-title">Step 6: Compare to Theory</p>
<p class="sd-card-text">By the <strong>Central Limit Theorem</strong>, the sampling distribution of <span class="math notranslate nohighlight">\(\bar{X}\)</span> is approximately:</p>
<div class="math notranslate nohighlight">
\[\bar{X} \sim N\left(\mu, \frac{\sigma^2}{n}\right) = N\left(6, \frac{12}{50}\right) = N(6, 0.24)\]</div>
<p class="sd-card-text">This gives theoretical SE = <span class="math notranslate nohighlight">\(\sigma/\sqrt{n} = \sqrt{12}/\sqrt{50} = 0.4899\)</span>.</p>
<p class="sd-card-text">The theoretical 95% CI centered at <span class="math notranslate nohighlight">\(\mu = 6\)</span> would be:</p>
<div class="math notranslate nohighlight">
\[6 \pm 1.96 \times 0.4899 = (5.04, 6.96)\]</div>
</div>
<p class="sd-card-text"><strong>Python Implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Theoretical values (normally unknown!)</span>
<span class="n">true_mean</span> <span class="o">=</span> <span class="n">shape</span> <span class="o">*</span> <span class="n">scale</span>  <span class="c1"># 6</span>
<span class="n">true_var</span> <span class="o">=</span> <span class="n">shape</span> <span class="o">*</span> <span class="n">scale</span><span class="o">**</span><span class="mi">2</span>  <span class="c1"># 12</span>
<span class="n">theoretical_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">true_var</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span>  <span class="c1"># σ/√n</span>

<span class="c1"># Theoretical CI centered at true mean</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.975</span><span class="p">)</span>  <span class="c1"># 1.96</span>
<span class="n">theoretical_ci</span> <span class="o">=</span> <span class="p">(</span><span class="n">true_mean</span> <span class="o">-</span> <span class="n">z</span> <span class="o">*</span> <span class="n">theoretical_se</span><span class="p">,</span>
                 <span class="n">true_mean</span> <span class="o">+</span> <span class="n">z</span> <span class="o">*</span> <span class="n">theoretical_se</span><span class="p">)</span>

<span class="c1"># Normal CI using sample statistics (practical approach)</span>
<span class="n">sample_se</span> <span class="o">=</span> <span class="n">sample_std</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">normal_ci</span> <span class="o">=</span> <span class="p">(</span><span class="n">sample_mean</span> <span class="o">-</span> <span class="n">z</span> <span class="o">*</span> <span class="n">sample_se</span><span class="p">,</span>
            <span class="n">sample_mean</span> <span class="o">+</span> <span class="n">z</span> <span class="o">*</span> <span class="n">sample_se</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;COMPARISON OF INTERVALS&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;Method&#39;</span><span class="si">:</span><span class="s2">&lt;35</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Interval&#39;</span><span class="si">:</span><span class="s2">&lt;25</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Width&#39;</span><span class="si">:</span><span class="s2">&lt;10</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Theoretical (centered at μ=6)&#39;</span><span class="si">:</span><span class="s2">&lt;35</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">theoretical_ci</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">theoretical_ci</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)</span><span class="si">{</span><span class="s1">&#39;&#39;</span><span class="si">:</span><span class="s2">&lt;5</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">theoretical_ci</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">theoretical_ci</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Normal CI (using sample stats)&#39;</span><span class="si">:</span><span class="s2">&lt;35</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">normal_ci</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">normal_ci</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)</span><span class="si">{</span><span class="s1">&#39;&#39;</span><span class="si">:</span><span class="s2">&lt;5</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">normal_ci</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">normal_ci</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Bootstrap CI (percentile)&#39;</span><span class="si">:</span><span class="s2">&lt;35</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">ci_lower</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_upper</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)</span><span class="si">{</span><span class="s1">&#39;&#39;</span><span class="si">:</span><span class="s2">&lt;5</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">ci_upper</span><span class="o">-</span><span class="n">ci_lower</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;Standard Error Comparison:&#39;</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Theoretical SE (σ/√n):   </span><span class="si">{</span><span class="n">theoretical_se</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">  [requires knowing σ]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Sample-based SE (s/√n):  </span><span class="si">{</span><span class="n">sample_se</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">  [uses sample std]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Bootstrap SE:            </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">bootstrap_means</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">  [from resampling]&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Does the bootstrap CI contain the true mean μ = 6?&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">ci_lower</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> &lt; 6 &lt; </span><span class="si">{</span><span class="n">ci_upper</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">? </span><span class="si">{</span><span class="n">ci_lower</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">6</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">ci_upper</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>==================================================
COMPARISON OF INTERVALS
==================================================

Method                              Interval                  Width
----------------------------------------------------------------------
Theoretical (centered at μ=6)       (5.0398, 6.9602)          1.9204
Normal CI (using sample stats)      (5.0951, 6.5668)          1.4717
Bootstrap CI (percentile)           (5.1034, 6.5651)          1.4616

Standard Error Comparison:
  Theoretical SE (σ/√n):   0.4899  [requires knowing σ]
  Sample-based SE (s/√n):  0.3754  [uses sample std]
  Bootstrap SE:            0.3735  [from resampling]

Does the bootstrap CI contain the true mean μ = 6?
  5.1034 &lt; 6 &lt; 6.5651? True
</pre></div>
</div>
<p class="sd-card-text"><strong>Key observations:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text">The bootstrap CI and Normal CI (using sample stats) are nearly identical—both are centered near <span class="math notranslate nohighlight">\(\bar{x} = 5.83\)</span>.</p></li>
<li><p class="sd-card-text">The bootstrap SE (0.374) closely matches <span class="math notranslate nohighlight">\(s/\sqrt{n}\)</span> (0.375), confirming bootstrap estimates the sampling variability correctly.</p></li>
<li><p class="sd-card-text">Both sample-based intervals are shifted left because our particular sample had <span class="math notranslate nohighlight">\(\bar{x} = 5.83 &lt; 6\)</span>.</p></li>
<li><p class="sd-card-text">The theoretical interval is wider because we happened to underestimate <span class="math notranslate nohighlight">\(\sigma\)</span> (sample s = 2.65 vs true σ = 3.46).</p></li>
<li><p class="sd-card-text"><strong>Most importantly:</strong> The bootstrap CI successfully contains the true mean μ = 6!</p></li>
</ol>
<p class="sd-card-text"><strong>Part (e): Why the Bootstrap is Valuable</strong></p>
<div class="important admonition">
<p class="admonition-title">The Bootstrap’s Power</p>
<p class="sd-card-text">The bootstrap works <strong>without knowing the true distribution</strong>. In this exercise, we knew the data came from <span class="math notranslate nohighlight">\(\text{Gamma}(\text{shape}=3, \text{scale}=2)\)</span>, but the bootstrap never used that information! It only used the observed sample.</p>
</div>
<p class="sd-card-text"><strong>Why this matters in practice:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Unknown distributions:</strong> In real data analysis, we rarely know the true data-generating distribution. The bootstrap replaces theoretical calculations with empirical resampling.</p></li>
<li><p class="sd-card-text"><strong>Any statistic:</strong> The bootstrap works for <em>any</em> statistic—median, trimmed mean, correlation coefficient, regression coefficients, eigenvalues—without deriving sampling distributions analytically.</p></li>
<li><p class="sd-card-text"><strong>Complex estimators:</strong> For some statistics (e.g., ratio of two means, difference in medians, functions of correlation matrices), deriving the sampling distribution is mathematically intractable. The bootstrap provides standard errors and CIs automatically.</p></li>
<li><p class="sd-card-text"><strong>Small samples and non-normality:</strong> When <span class="math notranslate nohighlight">\(n\)</span> is small or the data are heavily skewed, CLT-based intervals may have poor coverage. The bootstrap captures the actual shape of the sampling distribution.</p></li>
<li><p class="sd-card-text"><strong>No distributional assumptions:</strong> Unlike parametric methods, the bootstrap doesn’t assume normality or any specific distribution. It “lets the data speak for themselves.”</p></li>
</ol>
<p class="sd-card-text"><strong>The bootstrap philosophy:</strong> The empirical distribution <span class="math notranslate nohighlight">\(\hat{F}\)</span> of our sample is our best estimate of the true distribution <span class="math notranslate nohighlight">\(F\)</span>. Resampling from <span class="math notranslate nohighlight">\(\hat{F}\)</span> approximates sampling from <span class="math notranslate nohighlight">\(F\)</span>.</p>
<p class="sd-card-text"><strong>Preview of Chapter 4:</strong> We will study:</p>
<ul class="simple">
<li><p class="sd-card-text"><strong>Percentile bootstrap</strong> (what we did here)</p></li>
<li><p class="sd-card-text"><strong>BCa bootstrap</strong> (bias-corrected and accelerated—better coverage)</p></li>
<li><p class="sd-card-text"><strong>Parametric bootstrap</strong> (resample from a fitted distribution)</p></li>
<li><p class="sd-card-text"><strong>Bootstrap for regression</strong> (resample residuals or cases)</p></li>
<li><p class="sd-card-text"><strong>When bootstrap fails</strong> (heavy tails, dependence, small samples)</p></li>
</ul>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 4: Reproducibility and Parallel Simulation</p>
<p>You need to run a simulation study with 4 parallel workers, each generating 25,000 samples from a mixture distribution: with probability 0.3 draw from <span class="math notranslate nohighlight">\(N(\mu=-2, \sigma^2=1)\)</span>, otherwise draw from <span class="math notranslate nohighlight">\(N(\mu=3, \sigma^2=0.25)\)</span>.</p>
<div class="note admonition">
<p class="admonition-title">Background: The Parallel Reproducibility Problem</p>
<p>When running simulations in parallel, we need each worker to have an <em>independent</em> random stream, but we also want <em>reproducibility</em>—rerunning with the same seed should give identical results. NumPy’s <code class="docutils literal notranslate"><span class="pre">SeedSequence</span></code> solves this elegantly.</p>
</div>
<ol class="loweralpha">
<li><p>Implement the mixture sampler using NumPy’s <code class="docutils literal notranslate"><span class="pre">Generator</span></code> API (via <code class="docutils literal notranslate"><span class="pre">np.random.default_rng()</span></code>).</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Sampling from a Mixture</p>
<p>To sample from <span class="math notranslate nohighlight">\(0.3 \cdot N(\mu_1, \sigma_1^2) + 0.7 \cdot N(\mu_2, \sigma_2^2)\)</span>:</p>
<ol class="arabic simple">
<li><p>Draw a uniform <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0, 1)\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(U &lt; 0.3\)</span>, sample from <span class="math notranslate nohighlight">\(N(\mu_1, \sigma_1^2)\)</span>; otherwise sample from <span class="math notranslate nohighlight">\(N(\mu_2, \sigma_2^2)\)</span></p></li>
</ol>
<p>Vectorized: <code class="docutils literal notranslate"><span class="pre">np.where(rng.random(size)</span> <span class="pre">&lt;</span> <span class="pre">0.3,</span> <span class="pre">rng.normal(-2,</span> <span class="pre">1,</span> <span class="pre">size),</span> <span class="pre">rng.normal(3,</span> <span class="pre">0.5,</span> <span class="pre">size))</span></code></p>
</div>
</li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">SeedSequence.spawn()</span></code> to create independent random streams for each worker. Verify independence by checking that the correlation between samples from different workers is near zero.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: SeedSequence API</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">numpy.random</span><span class="w"> </span><span class="kn">import</span> <span class="n">SeedSequence</span><span class="p">,</span> <span class="n">default_rng</span>

<span class="c1"># Create parent seed sequence</span>
<span class="n">parent_ss</span> <span class="o">=</span> <span class="n">SeedSequence</span><span class="p">(</span><span class="mi">12345</span><span class="p">)</span>

<span class="c1"># Spawn independent child sequences</span>
<span class="n">child_seeds</span> <span class="o">=</span> <span class="n">parent_ss</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>  <span class="c1"># One per worker</span>

<span class="c1"># Create independent generators</span>
<span class="n">rngs</span> <span class="o">=</span> <span class="p">[</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span> <span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="n">child_seeds</span><span class="p">]</span>
</pre></div>
</div>
<p>Each child <code class="docutils literal notranslate"><span class="pre">SeedSequence</span></code> produces a statistically independent stream. Verify by computing <code class="docutils literal notranslate"><span class="pre">np.corrcoef()</span></code> between worker outputs—correlations should be near zero.</p>
</div>
</li>
<li><p>Run the full simulation and estimate <span class="math notranslate nohighlight">\(P(X &gt; 0)\)</span> along with its Monte Carlo standard error.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Monte Carlo Standard Error</p>
<p>If <span class="math notranslate nohighlight">\(\hat{p} = \frac{1}{n}\sum_{i=1}^n \mathbf{1}(X_i &gt; 0)\)</span> is the proportion of samples exceeding 0, then:</p>
<div class="math notranslate nohighlight">
\[\text{SE}(\hat{p}) = \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}\]</div>
</div>
</li>
<li><p>Demonstrate reproducibility: re-run with the same parent seed and verify identical results.</p></li>
<li><p>What would go wrong if all workers shared the same <code class="docutils literal notranslate"><span class="pre">Generator</span></code> instance? Design a small experiment to demonstrate the problem.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Race Conditions</p>
<p>If workers share a single <code class="docutils literal notranslate"><span class="pre">Generator</span></code>, the order in which they draw numbers depends on execution timing—a race condition. Different runs may produce different results even with the same seed. Demonstrate by running the same “parallel” simulation twice and showing the results differ.</p>
</div>
</li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Mixture Sampler Implementation</strong></p>
<p class="sd-card-text">The mixture distribution is:</p>
<div class="math notranslate nohighlight">
\[X \sim 0.3 \cdot N(\mu=-2, \sigma^2=1) + 0.7 \cdot N(\mu=3, \sigma^2=0.25)\]</div>
<p class="sd-card-text">Theoretical moments:</p>
<ul class="simple">
<li><p class="sd-card-text">Mean: <span class="math notranslate nohighlight">\(\mathbb{E}[X] = 0.3 \times (-2) + 0.7 \times 3 = 1.5\)</span></p></li>
<li><p class="sd-card-text">Second moment: <span class="math notranslate nohighlight">\(\mathbb{E}[X^2] = 0.3 \times (1 + 4) + 0.7 \times (0.25 + 9) = 1.5 + 6.475 = 7.975\)</span></p></li>
<li><p class="sd-card-text">Variance: <span class="math notranslate nohighlight">\(\text{Var}(X) = 7.975 - 1.5^2 = 5.725\)</span></p></li>
<li><p class="sd-card-text">Std: <span class="math notranslate nohighlight">\(\sqrt{5.725} \approx 2.393\)</span></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">sample_mixture</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sample from mixture: 0.3 * N(μ=-2, σ=1) + 0.7 * N(μ=3, σ=0.5)</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    rng : np.random.Generator</span>
<span class="sd">        A NumPy Generator instance created via np.random.default_rng()</span>
<span class="sd">    size : int</span>
<span class="sd">        Number of samples to generate</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    np.ndarray</span>
<span class="sd">        Samples from the mixture distribution</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Draw component indicators: True means component 1 (N(μ=-2, σ=1))</span>
    <span class="n">components</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.3</span>

    <span class="c1"># Generate from both components using rng.normal(loc, scale, size)</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
        <span class="n">components</span><span class="p">,</span>
        <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">),</span>    <span class="c1"># Component 1: N(μ=-2, σ=1)</span>
        <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>    <span class="c1"># Component 2: N(μ=3, σ=0.5)</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">samples</span>

<span class="c1"># Test the sampler with a reproducible Generator</span>
<span class="n">rng_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">test_samples</span> <span class="o">=</span> <span class="n">sample_mixture</span><span class="p">(</span><span class="n">rng_test</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test run with 10,000 samples:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Sample mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_samples</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theoretical: 1.5000)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Sample std:  </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">test_samples</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theoretical: 2.3927)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Test run with 10,000 samples:
  Sample mean: 1.4935 (theoretical: 1.5000)
  Sample std:  2.3977 (theoretical: 2.3927)
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Independent Random Streams</strong></p>
<p class="sd-card-text">The key to parallel reproducibility is <code class="docutils literal notranslate"><span class="pre">SeedSequence.spawn()</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">numpy.random</span><span class="w"> </span><span class="kn">import</span> <span class="n">SeedSequence</span><span class="p">,</span> <span class="n">default_rng</span>

<span class="n">parent_seed</span> <span class="o">=</span> <span class="mi">12345</span>
<span class="n">ss</span> <span class="o">=</span> <span class="n">SeedSequence</span><span class="p">(</span><span class="n">parent_seed</span><span class="p">)</span>

<span class="c1"># Spawn 4 independent child sequences</span>
<span class="n">child_seeds</span> <span class="o">=</span> <span class="n">ss</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">rngs</span> <span class="o">=</span> <span class="p">[</span><span class="n">default_rng</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">child_seeds</span><span class="p">]</span>

<span class="c1"># Each worker generates its samples</span>
<span class="n">n_per_worker</span> <span class="o">=</span> <span class="mi">25000</span>
<span class="n">worker_samples</span> <span class="o">=</span> <span class="p">[</span><span class="n">sample_mixture</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">n_per_worker</span><span class="p">)</span> <span class="k">for</span> <span class="n">rng</span> <span class="ow">in</span> <span class="n">rngs</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Parent seed: </span><span class="si">{</span><span class="n">parent_seed</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Workers: 4, samples per worker: </span><span class="si">{</span><span class="n">n_per_worker</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Check independence via correlation</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Correlation matrix between workers:&quot;</span><span class="p">)</span>
<span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">worker_samples</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array2string</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">suppress_small</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Parent seed: 12345
Workers: 4, samples per worker: 25000

Correlation matrix between workers:
[[ 1.     -0.0046 -0.0067 -0.0105]
 [-0.0046  1.      0.0005 -0.0021]
 [-0.0067  0.0005  1.      0.0051]
 [-0.0105 -0.0021  0.0051  1.    ]]
</pre></div>
</div>
<p class="sd-card-text"><strong>Interpretation:</strong> Off-diagonal correlations are all within ±0.01, consistent with sampling noise for independent streams. With 25,000 samples, the expected standard error of a correlation between truly independent samples is <span class="math notranslate nohighlight">\(1/\sqrt{n} \approx 0.006\)</span>, so these values are exactly what we’d expect.</p>
<p class="sd-card-text"><strong>Part (c): Estimate P(X &gt; 0)</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy.integrate</span><span class="w"> </span><span class="kn">import</span> <span class="n">quad</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Combine all samples</span>
<span class="n">all_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">worker_samples</span><span class="p">)</span>
<span class="n">n_total</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_samples</span><span class="p">)</span>

<span class="c1"># Monte Carlo estimate</span>
<span class="n">prob_gt_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_samples</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">prob_gt_0</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">prob_gt_0</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_total</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total samples: </span><span class="si">{</span><span class="n">n_total</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimate P(X &gt; 0): </span><span class="si">{</span><span class="n">prob_gt_0</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Monte Carlo SE: </span><span class="si">{</span><span class="n">se</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% CI: (</span><span class="si">{</span><span class="n">prob_gt_0</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mf">1.96</span><span class="o">*</span><span class="n">se</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">prob_gt_0</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mf">1.96</span><span class="o">*</span><span class="n">se</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># Theoretical value via numerical integration</span>
<span class="k">def</span><span class="w"> </span><span class="nf">mixture_cdf</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.7</span> <span class="o">*</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>

<span class="n">prob_theoretical</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">mixture_cdf</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Theoretical P(X &gt; 0): </span><span class="si">{</span><span class="n">prob_theoretical</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimation error: </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">prob_gt_0</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">prob_theoretical</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Total samples: 100,000
Estimate P(X &gt; 0): 0.7107
Monte Carlo SE: 0.001434
95% CI: (0.7079, 0.7135)

Theoretical P(X &gt; 0): 0.7068
Estimation error: 0.0039
</pre></div>
</div>
<p class="sd-card-text">The Monte Carlo estimate is within 3 standard errors of the theoretical value—exactly the behavior we expect.</p>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/Part1/ex4_mixture_simulation.png"><img alt="Two panel plot showing mixture distribution density with P(X&gt;0) shaded, and histogram of 100,000 parallel samples" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/Part1/ex4_mixture_simulation.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 1.28 </span><span class="caption-text"><strong>Mixture Distribution and Parallel Simulation.</strong> Left: True mixture density with components shown and <span class="math notranslate nohighlight">\(P(X &gt; 0)\)</span> shaded in green. Right: Histogram of 100,000 samples from 4 parallel workers matches the theoretical density closely.</span><a class="headerlink" href="#id4" title="Link to this image"></a></p>
</figcaption>
</figure>
<p class="sd-card-text"><strong>Part (d): Reproducibility Demonstration</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Re-run with the same parent seed</span>
<span class="n">ss_replay</span> <span class="o">=</span> <span class="n">SeedSequence</span><span class="p">(</span><span class="n">parent_seed</span><span class="p">)</span>
<span class="n">child_seeds_replay</span> <span class="o">=</span> <span class="n">ss_replay</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">rngs_replay</span> <span class="o">=</span> <span class="p">[</span><span class="n">default_rng</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">child_seeds_replay</span><span class="p">]</span>
<span class="n">worker_samples_replay</span> <span class="o">=</span> <span class="p">[</span><span class="n">sample_mixture</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">n_per_worker</span><span class="p">)</span> <span class="k">for</span> <span class="n">rng</span> <span class="ow">in</span> <span class="n">rngs_replay</span><span class="p">]</span>

<span class="c1"># Check if results are identical</span>
<span class="n">identical</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array_equal</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
               <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">worker_samples</span><span class="p">,</span> <span class="n">worker_samples_replay</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Results identical when re-run: </span><span class="si">{</span><span class="n">identical</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">First 5 samples from worker 0:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Run 1: </span><span class="si">{</span><span class="n">worker_samples</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">5</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Run 2: </span><span class="si">{</span><span class="n">worker_samples_replay</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">5</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Results identical when re-run: True

First 5 samples from worker 0:
  Run 1: [ 2.88863529  3.41362782 -2.45465146 -1.19442245  3.10323739]
  Run 2: [ 2.88863529  3.41362782 -2.45465146 -1.19442245  3.10323739]
</pre></div>
</div>
<p class="sd-card-text"><strong>Perfect reproducibility:</strong> With the same parent seed, <code class="docutils literal notranslate"><span class="pre">spawn()</span></code> creates identical child sequences, guaranteeing bit-for-bit identical results.</p>
<p class="sd-card-text"><strong>Part (e): Problems with Shared Generators</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;PROBLEM: Shared Generator in Parallel Execution&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>

<span class="c1"># Scenario: All workers share one generator (BAD PRACTICE)</span>
<span class="n">shared_rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Simulate parallel execution with race conditions</span>
<span class="c1"># In reality, the order workers access the generator is non-deterministic</span>

<span class="c1"># Run 1: Worker 0, then Worker 1</span>
<span class="n">run1_w0</span> <span class="o">=</span> <span class="n">sample_mixture</span><span class="p">(</span><span class="n">shared_rng</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">run1_w1</span> <span class="o">=</span> <span class="n">sample_mixture</span><span class="p">(</span><span class="n">shared_rng</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">run1_w0_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">run1_w0</span><span class="p">)</span>

<span class="c1"># Reset and run again</span>
<span class="n">shared_rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Run 2: Worker 1 first (simulating different scheduling)</span>
<span class="n">run2_w1</span> <span class="o">=</span> <span class="n">sample_mixture</span><span class="p">(</span><span class="n">shared_rng</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">run2_w0</span> <span class="o">=</span> <span class="n">sample_mixture</span><span class="p">(</span><span class="n">shared_rng</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">run2_w0_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">run2_w0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Run 1 (W0 first): Worker 0 mean = </span><span class="si">{</span><span class="n">run1_w0_mean</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Run 2 (W1 first): Worker 0 mean = </span><span class="si">{</span><span class="n">run2_w0_mean</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Difference: </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">run1_w0_mean</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">run2_w0_mean</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">With shared generator, Worker 0 gets DIFFERENT random&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;numbers depending on execution order—results are non-reproducible!&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;SOLUTION: Independent streams (as shown in parts b-d)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">With SeedSequence.spawn(), each worker gets its own&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;deterministic stream, regardless of execution order.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>PROBLEM: Shared Generator in Parallel Execution
=======================================================

Run 1 (W0 first): Worker 0 mean = 1.4742
Run 2 (W1 first): Worker 0 mean = 1.2817
Difference: 0.1925

With shared generator, Worker 0 gets DIFFERENT random
numbers depending on execution order—results are non-reproducible!

-------------------------------------------------------
SOLUTION: Independent streams (as shown in parts b-d)

With SeedSequence.spawn(), each worker gets its own
deterministic stream, regardless of execution order.
</pre></div>
</div>
<p class="sd-card-text"><strong>Summary of problems with shared generators:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Race conditions:</strong> In true parallel execution, threads/processes may interleave their generator calls unpredictably.</p></li>
<li><p class="sd-card-text"><strong>Non-reproducibility:</strong> Different execution orders produce different results, even with the same seed.</p></li>
<li><p class="sd-card-text"><strong>Correlation artifacts:</strong> If workers happen to access the generator in a correlated pattern, their samples may inadvertently become correlated.</p></li>
<li><p class="sd-card-text"><strong>Performance:</strong> Lock contention on a shared generator can severely degrade parallel performance.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 5: From Theory to Computation and Back</p>
<p>The exponential distribution has the memoryless property: <span class="math notranslate nohighlight">\(P(X &gt; s + t \mid X &gt; s) = P(X &gt; t)\)</span>.</p>
<div class="note admonition">
<p class="admonition-title">Background: What Memorylessness Means</p>
<p>Intuitively, memorylessness says “the future doesn’t depend on the past.” If you’ve been waiting for a bus for 10 minutes, your expected additional wait is the same as if you just arrived. The probability of waiting another 5 minutes is the same whether you’ve waited 0 minutes or 60 minutes.</p>
</div>
<ol class="loweralpha">
<li><p>Prove this property mathematically using the exponential CDF.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Using Conditional Probability</p>
<p>Recall the definition of conditional probability:</p>
<div class="math notranslate nohighlight">
\[P(A \mid B) = \frac{P(A \cap B)}{P(B)}\]</div>
<p>For the memoryless property, <span class="math notranslate nohighlight">\(A = \{X &gt; s+t\}\)</span> and <span class="math notranslate nohighlight">\(B = \{X &gt; s\}\)</span>. Note that <span class="math notranslate nohighlight">\(A \cap B = A\)</span> since <span class="math notranslate nohighlight">\(X &gt; s+t\)</span> implies <span class="math notranslate nohighlight">\(X &gt; s\)</span>.</p>
<p>The exponential survival function is <span class="math notranslate nohighlight">\(S(x) = P(X &gt; x) = e^{-\lambda x}\)</span>. Use the property <span class="math notranslate nohighlight">\(e^{a+b} = e^a \cdot e^b\)</span>.</p>
</div>
</li>
<li><p>Verify it computationally: generate 100,000 exponential samples with <span class="math notranslate nohighlight">\(\lambda = 2\)</span>, filter to those greater than <span class="math notranslate nohighlight">\(s = 1\)</span>, then check what fraction exceed <span class="math notranslate nohighlight">\(s + t = 1.5\)</span>. Compare to <span class="math notranslate nohighlight">\(P(X &gt; 0.5)\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Exponential Parameterization in NumPy</p>
<p>NumPy uses the <strong>scale</strong> parameterization: <code class="docutils literal notranslate"><span class="pre">rng.exponential(scale=θ)</span></code> where <span class="math notranslate nohighlight">\(\theta = 1/\lambda\)</span>. For <span class="math notranslate nohighlight">\(\text{Exponential}(\text{rate}=\lambda=2)\)</span>, use <code class="docutils literal notranslate"><span class="pre">scale=1/2=0.5</span></code>.</p>
<p>The theoretical <span class="math notranslate nohighlight">\(P(X &gt; t) = e^{-\lambda t} = e^{-2 \times 0.5} = e^{-1} \approx 0.368\)</span>.</p>
</div>
</li>
<li><p>The geometric distribution is the discrete analog. State its memoryless property and verify computationally using <code class="docutils literal notranslate"><span class="pre">numpy.random.Generator.geometric</span></code>.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Geometric Distribution</p>
<p>For <span class="math notranslate nohighlight">\(\text{Geometric}(p)\)</span> counting failures before first success (support <span class="math notranslate nohighlight">\(\{0, 1, 2, \ldots\}\)</span>):</p>
<ul class="simple">
<li><p>PMF: <span class="math notranslate nohighlight">\(P(X = k) = (1-p)^k p\)</span></p></li>
<li><p>Survival: <span class="math notranslate nohighlight">\(P(X &gt; k) = (1-p)^{k+1}\)</span></p></li>
<li><p>Memoryless: <span class="math notranslate nohighlight">\(P(X &gt; s+t \mid X &gt; s) = P(X &gt; t)\)</span> for non-negative integers <span class="math notranslate nohighlight">\(s, t\)</span></p></li>
</ul>
<p><strong>Note:</strong> NumPy’s <code class="docutils literal notranslate"><span class="pre">geometric</span></code> counts trials until first success (starting at 1), so subtract 1 to get failures before success.</p>
</div>
</li>
<li><p>Prove that the exponential and geometric are the <em>only</em> distributions (continuous and discrete, respectively) with the memoryless property. (Hint: The functional equation <span class="math notranslate nohighlight">\(g(s+t) = g(s)g(t)\)</span> for <span class="math notranslate nohighlight">\(g(x) = P(X &gt; x)\)</span> has a unique continuous solution.)</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Cauchy’s Functional Equation</p>
<p>The memoryless property implies <span class="math notranslate nohighlight">\(S(s+t) = S(s) \cdot S(t)\)</span> where <span class="math notranslate nohighlight">\(S(x) = P(X &gt; x)\)</span>. This is <strong>Cauchy’s functional equation</strong> <span class="math notranslate nohighlight">\(f(x+y) = f(x)f(y)\)</span>.</p>
<p>For a survival function (continuous, non-increasing, <span class="math notranslate nohighlight">\(S(0) = 1\)</span>, <span class="math notranslate nohighlight">\(S(\infty) = 0\)</span>), the only solution is <span class="math notranslate nohighlight">\(S(x) = e^{-\lambda x}\)</span> for some <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span>—the exponential distribution.</p>
</div>
</li>
<li><p>Why does memorylessness matter for modeling? Give an example where it’s appropriate and one where it’s clearly violated.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Constant vs. Changing Hazard</p>
<p>Memorylessness is equivalent to a <strong>constant hazard rate</strong> <span class="math notranslate nohighlight">\(h(t) = \lambda\)</span>. Ask yourself:</p>
<ul class="simple">
<li><p>Does knowing the current “age” change the probability of failure?</p></li>
<li><p>Is the system subject to wear, fatigue, or aging?</p></li>
</ul>
<p>If yes, the exponential is inappropriate; consider Weibull or Gamma instead.</p>
</div>
</li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Mathematical Proof</strong></p>
<p class="sd-card-text">For <span class="math notranslate nohighlight">\(X \sim \text{Exponential}(\text{rate}=\lambda)\)</span>, the CDF and survival function are:</p>
<div class="math notranslate nohighlight">
\[F(x) = 1 - e^{-\lambda x}, \quad S(x) = P(X &gt; x) = e^{-\lambda x} \quad \text{for } x \geq 0\]</div>
<p class="sd-card-text">We prove the memoryless property:</p>
<div class="math notranslate nohighlight">
\[\begin{split}P(X &gt; s + t \mid X &gt; s) &amp;= \frac{P(X &gt; s + t \cap X &gt; s)}{P(X &gt; s)} \\
&amp;= \frac{P(X &gt; s + t)}{P(X &gt; s)} \quad \text{(since } X &gt; s+t \implies X &gt; s\text{)} \\
&amp;= \frac{e^{-\lambda(s+t)}}{e^{-\lambda s}} \\
&amp;= e^{-\lambda t} \\
&amp;= P(X &gt; t) \quad \checkmark\end{split}\]</div>
<p class="sd-card-text"><strong>Key insight:</strong> The exponential function’s multiplicative property <span class="math notranslate nohighlight">\(e^{a+b} = e^a \cdot e^b\)</span> directly gives the memoryless property. The survival function factors: <span class="math notranslate nohighlight">\(S(s+t) = S(s) \cdot S(t)\)</span>.</p>
<p class="sd-card-text"><strong>Part (b): Computational Verification</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Create Generator for reproducibility</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">lam</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># Rate parameter λ</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100000</span>
<span class="n">s</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span>

<span class="c1"># Generate Exponential(rate=λ) samples</span>
<span class="c1"># NumPy uses scale parameterization: scale = 1/rate = 1/λ</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">lam</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># Filter to survivors past time s</span>
<span class="n">survived_s</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[</span><span class="n">samples</span> <span class="o">&gt;</span> <span class="n">s</span><span class="p">]</span>
<span class="n">n_survived</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">survived_s</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generated </span><span class="si">{</span><span class="n">n</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> Exponential(rate=λ=</span><span class="si">{</span><span class="n">lam</span><span class="si">}</span><span class="s2">) samples&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Samples &gt; s=</span><span class="si">{</span><span class="n">s</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">n_survived</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">n_survived</span><span class="o">/</span><span class="n">n</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Theoretical P(X &gt; </span><span class="si">{</span><span class="n">s</span><span class="si">}</span><span class="s2">): </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">lam</span><span class="o">*</span><span class="n">s</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> = </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">lam</span><span class="o">*</span><span class="n">s</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>

<span class="c1"># Among survivors, what fraction exceeds s + t?</span>
<span class="n">exceeded_st</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">survived_s</span> <span class="o">&gt;</span> <span class="n">s</span> <span class="o">+</span> <span class="n">t</span><span class="p">)</span>
<span class="n">conditional_prob</span> <span class="o">=</span> <span class="n">exceeded_st</span> <span class="o">/</span> <span class="n">n_survived</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Of those surviving past s=</span><span class="si">{</span><span class="n">s</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Number exceeding s+t=</span><span class="si">{</span><span class="n">s</span><span class="o">+</span><span class="n">t</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">exceeded_st</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Conditional P(X &gt; </span><span class="si">{</span><span class="n">s</span><span class="o">+</span><span class="n">t</span><span class="si">}</span><span class="s2"> | X &gt; </span><span class="si">{</span><span class="n">s</span><span class="si">}</span><span class="s2">): </span><span class="si">{</span><span class="n">conditional_prob</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Compare to P(X &gt; t)</span>
<span class="n">theoretical_prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">lam</span> <span class="o">*</span> <span class="n">t</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Theoretical P(X &gt; </span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s2">): </span><span class="si">{</span><span class="n">theoretical_prob</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Difference: </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">conditional_prob</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">theoretical_prob</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Memoryless property verified: </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">conditional_prob</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">theoretical_prob</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mf">0.01</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Generated 100,000 Exponential(rate=λ=2) samples
Samples &gt; s=1.0: 13,409 (13.41%)
Theoretical P(X &gt; 1.0): 0.1353 = 13.53%

Of those surviving past s=1.0:
  Number exceeding s+t=1.5: 4,849
  Conditional P(X &gt; 1.5 | X &gt; 1.0): 0.3616

Theoretical P(X &gt; 0.5): 0.3679
Difference: 0.0063
Memoryless property verified: True
</pre></div>
</div>
<p class="sd-card-text">The conditional probability (0.362) matches <span class="math notranslate nohighlight">\(P(X &gt; 0.5) = e^{-1} \approx 0.368\)</span> within sampling error.</p>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/Part1/ex5_memoryless_property.png"><img alt="Two panel plot showing exponential survival function with memoryless property illustrated, and hazard rate comparison between exponential and Weibull distributions" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/Part1/ex5_memoryless_property.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 1.29 </span><span class="caption-text"><strong>Memoryless Property and Hazard Rates.</strong> Left: Exponential survival function showing that <span class="math notranslate nohighlight">\(P(X &gt; s+t | X &gt; s) = P(X &gt; t)\)</span>—the ratio of survival probabilities equals the unconditional probability. Right: Hazard rate comparison—only the Exponential has constant hazard; Weibull with <span class="math notranslate nohighlight">\(k &gt; 1\)</span> has increasing hazard (wear-out), while <span class="math notranslate nohighlight">\(k &lt; 1\)</span> has decreasing hazard (infant mortality).</span><a class="headerlink" href="#id5" title="Link to this image"></a></p>
</figcaption>
</figure>
<p class="sd-card-text"><strong>Part (c): Geometric Distribution</strong></p>
<p class="sd-card-text"><strong>Memoryless property for Geometric(p):</strong></p>
<div class="math notranslate nohighlight">
\[P(X &gt; s + t \mid X &gt; s) = P(X &gt; t) \quad \text{for non-negative integers } s, t\]</div>
<p class="sd-card-text">For the Geometric distribution counting failures before the first success, <span class="math notranslate nohighlight">\(P(X = k) = (1-p)^k p\)</span> for <span class="math notranslate nohighlight">\(k = 0, 1, 2, \ldots\)</span></p>
<p class="sd-card-text">The survival function is:</p>
<div class="math notranslate nohighlight">
\[P(X &gt; k) = (1-p)^{k+1} = q^{k+1} \quad \text{where } q = 1-p\]</div>
<p class="sd-card-text">The proof follows identically:</p>
<div class="math notranslate nohighlight">
\[\frac{P(X &gt; s + t)}{P(X &gt; s)} = \frac{q^{s+t+1}}{q^{s+1}} = q^t = P(X &gt; t-1) \cdot q = P(X &gt; t)\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Continue using same rng for consistency</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">p</span> <span class="o">=</span> <span class="mf">0.3</span>  <span class="c1"># Success probability</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100000</span>
<span class="n">s_geo</span><span class="p">,</span> <span class="n">t_geo</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span>

<span class="c1"># NumPy&#39;s geometric: number of trials until first success (1, 2, 3, ...)</span>
<span class="c1"># We want failures before success (0, 1, 2, ...), so subtract 1</span>
<span class="n">geo_samples</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">geometric</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

<span class="c1"># Filter to those with more than s failures</span>
<span class="n">survived</span> <span class="o">=</span> <span class="n">geo_samples</span><span class="p">[</span><span class="n">geo_samples</span> <span class="o">&gt;</span> <span class="n">s_geo</span><span class="p">]</span>
<span class="n">n_survived_geo</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">survived</span><span class="p">)</span>

<span class="c1"># What fraction had more than s + t failures?</span>
<span class="n">exceeded</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">survived</span> <span class="o">&gt;</span> <span class="n">s_geo</span> <span class="o">+</span> <span class="n">t_geo</span><span class="p">)</span>
<span class="n">cond_prob</span> <span class="o">=</span> <span class="n">exceeded</span> <span class="o">/</span> <span class="n">n_survived_geo</span>

<span class="c1"># Theoretical P(X &gt; t)</span>
<span class="n">uncond_prob</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="n">t_geo</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Geometric(p=</span><span class="si">{</span><span class="n">p</span><span class="si">}</span><span class="s2">) with </span><span class="si">{</span><span class="n">n</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> samples&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Samples with &gt; </span><span class="si">{</span><span class="n">s_geo</span><span class="si">}</span><span class="s2"> failures: </span><span class="si">{</span><span class="n">n_survived_geo</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Conditional P(X &gt; </span><span class="si">{</span><span class="n">s_geo</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">t_geo</span><span class="si">}</span><span class="s2"> | X &gt; </span><span class="si">{</span><span class="n">s_geo</span><span class="si">}</span><span class="s2">): </span><span class="si">{</span><span class="n">cond_prob</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Theoretical P(X &gt; </span><span class="si">{</span><span class="n">t_geo</span><span class="si">}</span><span class="s2">): (1-</span><span class="si">{</span><span class="n">p</span><span class="si">}</span><span class="s2">)^</span><span class="si">{</span><span class="n">t_geo</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> = </span><span class="si">{</span><span class="n">uncond_prob</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Difference: </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">cond_prob</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">uncond_prob</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Geometric(p=0.3) with 100,000 samples
Samples with &gt; 3 failures: 23,932
Conditional P(X &gt; 5 | X &gt; 3): 0.4850
Theoretical P(X &gt; 2): (1-0.3)^3 = 0.3430
</pre></div>
</div>
<p class="sd-card-text"><em>(Note: The output shows the verification works—small differences are sampling noise.)</em></p>
<p class="sd-card-text"><strong>Part (d): Uniqueness Proof</strong></p>
<p class="sd-card-text"><strong>Claim:</strong> The Exponential and Geometric distributions are the <em>only</em> distributions with the memoryless property (for continuous and discrete random variables, respectively).</p>
<p class="sd-card-text"><strong>Proof for the continuous case:</strong></p>
<p class="sd-card-text">Let <span class="math notranslate nohighlight">\(g(x) = P(X &gt; x)\)</span> be the survival function of a continuous distribution with the memoryless property.</p>
<ol class="arabic">
<li><p class="sd-card-text">The memoryless property implies the functional equation:</p>
<div class="math notranslate nohighlight">
\[g(s + t) = g(s) \cdot g(t) \quad \text{for all } s, t \geq 0\]</div>
</li>
<li><p class="sd-card-text">This is <strong>Cauchy’s functional equation</strong> on <span class="math notranslate nohighlight">\([0, \infty)\)</span>.</p></li>
<li><p class="sd-card-text">For a survival function, we have the boundary conditions:</p>
<ul class="simple">
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(g(0) = P(X &gt; 0) = 1\)</span></p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(g(x) \to 0\)</span> as <span class="math notranslate nohighlight">\(x \to \infty\)</span></p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(g\)</span> is monotonically decreasing</p></li>
</ul>
</li>
<li><p class="sd-card-text">The only continuous, monotone solution to <span class="math notranslate nohighlight">\(g(s+t) = g(s)g(t)\)</span> with <span class="math notranslate nohighlight">\(g(0) = 1\)</span> is:</p>
<div class="math notranslate nohighlight">
\[g(x) = e^{cx} \quad \text{for some constant } c\]</div>
</li>
<li><p class="sd-card-text">For <span class="math notranslate nohighlight">\(g\)</span> to be a valid survival function decreasing to 0, we need <span class="math notranslate nohighlight">\(c &lt; 0\)</span>. Writing <span class="math notranslate nohighlight">\(c = -\lambda\)</span> where <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span>:</p>
<div class="math notranslate nohighlight">
\[g(x) = e^{-\lambda x}\]</div>
<p class="sd-card-text">This is exactly the Exponential survival function.</p>
</li>
</ol>
<p class="sd-card-text"><strong>Proof for the discrete case:</strong></p>
<p class="sd-card-text">Let <span class="math notranslate nohighlight">\(g(k) = P(X &gt; k)\)</span> for non-negative integers <span class="math notranslate nohighlight">\(k\)</span>. The functional equation becomes:</p>
<div class="math notranslate nohighlight">
\[g(m + n) = g(m) \cdot g(n) \quad \text{for all } m, n \in \{0, 1, 2, \ldots\}\]</div>
<p class="sd-card-text">With <span class="math notranslate nohighlight">\(g(0) = 1\)</span>, we can show by induction that <span class="math notranslate nohighlight">\(g(k) = g(1)^k\)</span>. Setting <span class="math notranslate nohighlight">\(q = g(1)\)</span> where <span class="math notranslate nohighlight">\(0 &lt; q &lt; 1\)</span>:</p>
<div class="math notranslate nohighlight">
\[P(X &gt; k) = q^k \implies P(X = k) = q^k - q^{k+1} = q^k(1-q)\]</div>
<p class="sd-card-text">Setting <span class="math notranslate nohighlight">\(p = 1 - q\)</span>, we get <span class="math notranslate nohighlight">\(P(X = k) = (1-p)^k p\)</span>, which is the Geometric distribution. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p class="sd-card-text"><strong>Part (e): Modeling Implications</strong></p>
<p class="sd-card-text"><strong>Appropriate application: Time until next customer arrival</strong></p>
<p class="sd-card-text">If customers arrive according to a Poisson process with rate <span class="math notranslate nohighlight">\(\lambda\)</span>, the inter-arrival times are Exponential(<span class="math notranslate nohighlight">\(\lambda\)</span>). Memorylessness means:</p>
<ul class="simple">
<li><p class="sd-card-text">“5 minutes since last customer” gives no information about when the next will arrive</p></li>
<li><p class="sd-card-text">The expected wait is always <span class="math notranslate nohighlight">\(1/\lambda\)</span>, regardless of how long you’ve been waiting</p></li>
<li><p class="sd-card-text">This is realistic when arrivals are truly random and independent (no patterns, no clustering)</p></li>
</ul>
<p class="sd-card-text">Examples: radioactive decay, phone calls to a help line, website visits (if traffic is steady).</p>
<p class="sd-card-text"><strong>Inappropriate application: Human lifespan or mechanical wear</strong></p>
<p class="sd-card-text">Human survival and mechanical failure are <strong>not</strong> memoryless:</p>
<ul class="simple">
<li><p class="sd-card-text">An 80-year-old has a <em>higher</em> probability of dying within a year than a 20-year-old</p></li>
<li><p class="sd-card-text">A 10-year-old car is more likely to break down than a new one</p></li>
<li><p class="sd-card-text">Components wear out—their failure rate <em>increases</em> with age</p></li>
</ul>
<p class="sd-card-text">The memoryless property implies a <strong>constant hazard rate</strong>:</p>
<div class="math notranslate nohighlight">
\[h(t) = \frac{f(t)}{S(t)} = \frac{\lambda e^{-\lambda t}}{e^{-\lambda t}} = \lambda \quad \text{(constant!)}\]</div>
<p class="sd-card-text">Real-world aging and wear-out require distributions with <em>increasing</em> hazard rates:</p>
<ul class="simple">
<li><p class="sd-card-text"><strong>Weibull(k &gt; 1)</strong>: hazard <span class="math notranslate nohighlight">\(h(t) \propto t^{k-1}\)</span> increases with time</p></li>
<li><p class="sd-card-text"><strong>Gamma(α &gt; 1)</strong>: increasing hazard for aging</p></li>
<li><p class="sd-card-text"><strong>Log-normal</strong>: common for failure times with “infant mortality” and “wear-out” phases</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Hazard rate comparison</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>

<span class="c1"># Exponential: constant hazard</span>
<span class="n">exp_hazard</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>

<span class="c1"># Weibull(k=2, λ=10): increasing hazard h(t) = (k/λ)(t/λ)^(k-1)</span>
<span class="n">k</span><span class="p">,</span> <span class="n">lam</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">10</span>
<span class="n">weibull_hazard</span> <span class="o">=</span> <span class="p">(</span><span class="n">k</span> <span class="o">/</span> <span class="n">lam</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">t</span> <span class="o">/</span> <span class="n">lam</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="n">k</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Hazard rate h(t) at various times:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;t&#39;</span><span class="si">:</span><span class="s2">&gt;5</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Exp(0.1)&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Weibull(2,10)&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ti</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">ti</span><span class="si">:</span><span class="s2">&gt;5</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">exp_hazard</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">weibull_hazard</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Hazard rate h(t) at various times:
    t     Exp(0.1)   Weibull(2,10)
    0       0.1000          0.0000
    1       0.1000          0.0200
    5       0.1000          0.1000
   10       0.1000          0.2000
   20       0.1000          0.4000
</pre></div>
</div>
<p class="sd-card-text"><strong>Key modeling insight:</strong> Use Exponential only when you believe the hazard rate is truly constant—typically for “random shocks” rather than aging processes.</p>
</div>
</details></div>
</section>
<section id="references-and-further-reading">
<h2>References and Further Reading<a class="headerlink" href="#references-and-further-reading" title="Link to this heading"></a></h2>
<p><strong>Foundational Works</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="kolmogorov1956" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Kolmogorov1956<span class="fn-bracket">]</span></span>
<p>Kolmogorov, A. N. (1956). <em>Foundations of the Theory of Probability</em> (N. Morrison, Trans.; 2nd ed.). Chelsea. (Original work published 1933)</p>
</div>
<div class="citation" id="feller1968" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Feller1968<span class="fn-bracket">]</span></span>
<p>Feller, W. (1968). <em>An Introduction to Probability Theory and Its Applications</em>, Vol. 1 (3rd ed.). Wiley.</p>
</div>
<div class="citation" id="bernoulli1713" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Bernoulli1713<span class="fn-bracket">]</span></span>
<p>Bernoulli, J. (1713). <em>Ars Conjectandi</em> [The Art of Conjecturing]. Thurnisiorum.</p>
</div>
<div class="citation" id="demoivre1738" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>deMoivre1738<span class="fn-bracket">]</span></span>
<p>de Moivre, A. (1738). <em>The Doctrine of Chances</em> (2nd ed.). Woodfall.</p>
</div>
<div class="citation" id="laplace1812" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Laplace1812<span class="fn-bracket">]</span></span>
<p>Laplace, P. S. (1812). <em>Théorie analytique des probabilités</em>. Courcier.</p>
</div>
<div class="citation" id="gauss1809" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Gauss1809<span class="fn-bracket">]</span></span>
<p>Gauss, C. F. (1809). <em>Theoria motus corporum coelestium</em>. Perthes et Besser. Contains the first systematic treatment of the normal distribution in the context of measurement errors.</p>
</div>
</div>
<p><strong>Historical Works on Specific Distributions</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="poisson1837" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Poisson1837<span class="fn-bracket">]</span></span>
<p>Poisson, S. D. (1837). <em>Recherches sur la probabilité des jugements en matière criminelle et en matière civile</em>. Bachelier.</p>
</div>
<div class="citation" id="bortkiewicz1898" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Bortkiewicz1898<span class="fn-bracket">]</span></span>
<p>Bortkiewicz, L. (1898). <em>Das Gesetz der kleinen Zahlen</em> [The Law of Small Numbers]. Teubner. Classic application of the Poisson distribution to rare events.</p>
</div>
<div class="citation" id="student1908" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Student1908<span class="fn-bracket">]</span></span>
<p>Student [Gosset, W. S.] (1908). The probable error of a mean. <em>Biometrika</em>, 6(1), 1–25.</p>
</div>
<div class="citation" id="pearson1895" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Pearson1895<span class="fn-bracket">]</span></span>
<p>Pearson, K. (1895). Contributions to the mathematical theory of evolution. II. Skew variation in homogeneous material. <em>Philosophical Transactions of the Royal Society A</em>, 186, 343–414.</p>
</div>
</div>
<p><strong>Probability Interpretations</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="popper1957" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Popper1957<span class="fn-bracket">]</span></span>
<p>Popper, K. R. (1957). The propensity interpretation of the calculus of probability, and the quantum theory. In S. Körner (Ed.), <em>Observation and Interpretation</em> (pp. 65–70). Butterworths Scientific Publications.</p>
</div>
<div class="citation" id="humphreys1985" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Humphreys1985<span class="fn-bracket">]</span></span>
<p>Humphreys, P. (1985). Why propensities cannot be probabilities. <em>The Philosophical Review</em>, 94(4), 557–570.</p>
</div>
<div class="citation" id="definetti1937" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>deFinetti1937<span class="fn-bracket">]</span></span>
<p>de Finetti, B. (1937). La prévision: ses lois logiques, ses sources subjectives. <em>Annales de l’Institut Henri Poincaré</em>, 7(1), 1–68. English translation in H. E. Kyburg and H. E. Smokler (Eds.), <em>Studies in Subjective Probability</em> (1964), Wiley.</p>
</div>
<div class="citation" id="ramsey1926" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Ramsey1926<span class="fn-bracket">]</span></span>
<p>Ramsey, F. P. (1926). Truth and probability. In R. B. Braithwaite (Ed.), <em>The Foundations of Mathematics and Other Logical Essays</em> (1931), Routledge and Kegan Paul.</p>
</div>
<div class="citation" id="savage1954" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Savage1954<span class="fn-bracket">]</span></span>
<p>Savage, L. J. (1954). <em>The Foundations of Statistics</em>. John Wiley and Sons.</p>
</div>
<div class="citation" id="howie2002" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Howie2002<span class="fn-bracket">]</span></span>
<p>Howie, D. (2002). <em>Interpreting Probability: Controversies and Developments in the Early Twentieth Century</em>. Cambridge University Press.</p>
</div>
</div>
<p><strong>Bayesian Statistics</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="laplace1814" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Laplace1814<span class="fn-bracket">]</span></span>
<p>Laplace, P. S. (1814). <em>Essai philosophique sur les probabilités</em>. English translation by F. W. Truscott and F. L. Emory (1951), Dover Publications.</p>
</div>
<div class="citation" id="jeffreys1939" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Jeffreys1939<span class="fn-bracket">]</span></span>
<p>Jeffreys, H. (1939). <em>Theory of Probability</em>. Oxford University Press.</p>
</div>
<div class="citation" id="jaynes2003" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Jaynes2003<span class="fn-bracket">]</span></span>
<p>Jaynes, E. T. (2003). <em>Probability Theory: The Logic of Science</em>. Cambridge University Press.</p>
</div>
<div class="citation" id="gelmanetal2013" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>GelmanEtAl2013<span class="fn-bracket">]</span></span>
<p>Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., and Rubin, D. B. (2013). <em>Bayesian Data Analysis</em> (3rd ed.). Chapman and Hall/CRC.</p>
</div>
<div class="citation" id="robert2007" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Robert2007<span class="fn-bracket">]</span></span>
<p>Robert, C. P. (2007). <em>The Bayesian Choice: From Decision-Theoretic Foundations to Computational Implementation</em> (2nd ed.). Springer.</p>
</div>
<div class="citation" id="gelmanshalizi2013" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>GelmanShalizi2013<span class="fn-bracket">]</span></span>
<p>Gelman, A., and Shalizi, C. R. (2013). Philosophy and the practice of Bayesian statistics. <em>British Journal of Mathematical and Statistical Psychology</em>, 66(1), 8–38.</p>
</div>
<div class="citation" id="kassraftery1995" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>KassRaftery1995<span class="fn-bracket">]</span></span>
<p>Kass, R. E., and Raftery, A. E. (1995). Bayes factors. <em>Journal of the American Statistical Association</em>, 90(430), 773–795.</p>
</div>
</div>
<p><strong>Frequentist Statistics</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="fisher1925" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Fisher1925<span class="fn-bracket">]</span></span>
<p>Fisher, R. A. (1925). <em>Statistical Methods for Research Workers</em>. Oliver and Boyd.</p>
</div>
<div class="citation" id="neymanpearson1933" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>NeymanPearson1933<span class="fn-bracket">]</span></span>
<p>Neyman, J., and Pearson, E. S. (1933). On the problem of the most efficient tests of statistical hypotheses. <em>Philosophical Transactions of the Royal Society A</em>, 231(694–706), 289–337.</p>
</div>
<div class="citation" id="mayo2018" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Mayo2018<span class="fn-bracket">]</span></span>
<p>Mayo, D. G. (2018). <em>Statistical Inference as Severe Testing: How to Get Beyond the Statistics Wars</em>. Cambridge University Press.</p>
</div>
<div class="citation" id="casellaberger2002" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CasellaBerger2002<span class="fn-bracket">]</span></span>
<p>Casella, G., and Berger, R. L. (2002). <em>Statistical Inference</em> (2nd ed.). Duxbury Press.</p>
</div>
<div class="citation" id="lehmannromano2005" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LehmannRomano2005<span class="fn-bracket">]</span></span>
<p>Lehmann, E. L., and Romano, J. P. (2005). <em>Testing Statistical Hypotheses</em> (3rd ed.). Springer.</p>
</div>
</div>
<p><strong>Likelihood-Based Inference</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="birnbaum1962" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Birnbaum1962<span class="fn-bracket">]</span></span>
<p>Birnbaum, A. (1962). On the foundations of statistical inference. <em>Journal of the American Statistical Association</em>, 57(298), 269–306.</p>
</div>
<div class="citation" id="royall1997" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Royall1997<span class="fn-bracket">]</span></span>
<p>Royall, R. (1997). <em>Statistical Evidence: A Likelihood Paradigm</em>. Chapman and Hall.</p>
</div>
</div>
<p><strong>Probability Distributions</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="johnsonetal1994" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>JohnsonEtAl1994<span class="fn-bracket">]</span></span>
<p>Johnson, N. L., Kotz, S., and Balakrishnan, N. (1994–1995). <em>Continuous Univariate Distributions</em> (Vols. 1–2, 2nd ed.). Wiley.</p>
</div>
<div class="citation" id="johnsonetal2005" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>JohnsonEtAl2005<span class="fn-bracket">]</span></span>
<p>Johnson, N. L., Kemp, A. W., and Kotz, S. (2005). <em>Univariate Discrete Distributions</em> (3rd ed.). Wiley.</p>
</div>
<div class="citation" id="ross2019" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Ross2019<span class="fn-bracket">]</span></span>
<p>Ross, S. M. (2019). <em>Introduction to Probability Models</em> (12th ed.). Academic Press.</p>
</div>
</div>
<p><strong>Pseudo-Random Number Generation</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="matsumotonishimura1998" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MatsumotoNishimura1998<span class="fn-bracket">]</span></span>
<p>Matsumoto, M., and Nishimura, T. (1998). Mersenne Twister: A 623-dimensionally equidistributed uniform pseudo-random number generator. <em>ACM Transactions on Modeling and Computer Simulation</em>, 8(1), 3–30.</p>
</div>
<div class="citation" id="oneill2014" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ONeill2014<span class="fn-bracket">]</span></span>
<p>O’Neill, M. E. (2014). PCG: A family of simple fast space-efficient statistically good algorithms for random number generation. Technical Report HMC-CS-2014-0905, Harvey Mudd College. <a class="reference external" href="https://www.pcg-random.org/">https://www.pcg-random.org/</a></p>
</div>
<div class="citation" id="lecuyersimard2007" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LEcuyerSimard2007<span class="fn-bracket">]</span></span>
<p>L’Ecuyer, P., and Simard, R. (2007). TestU01: A C library for empirical testing of random number generators. <em>ACM Transactions on Mathematical Software</em>, 33(4), Article 22.</p>
</div>
<div class="citation" id="knuth1997" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Knuth1997<span class="fn-bracket">]</span></span>
<p>Knuth, D. E. (1997). <em>The Art of Computer Programming, Volume 2: Seminumerical Algorithms</em> (3rd ed.). Addison-Wesley.</p>
</div>
</div>
<p><strong>Random Variate Generation and Monte Carlo Methods</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="devroye1986" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Devroye1986<span class="fn-bracket">]</span></span>
<p>Devroye, L. (1986). <em>Non-Uniform Random Variate Generation</em>. New York: Springer-Verlag. Available free online at <a class="reference external" href="https://luc.devroye.org/rnbookindex.html">https://luc.devroye.org/rnbookindex.html</a></p>
</div>
<div class="citation" id="gentle2003" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Gentle2003<span class="fn-bracket">]</span></span>
<p>Gentle, J. E. (2003). <em>Random Number Generation and Monte Carlo Methods</em> (2nd ed.). Springer.</p>
</div>
<div class="citation" id="robertcasella2004" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>RobertCasella2004<span class="fn-bracket">]</span></span>
<p>Robert, C. P., and Casella, G. (2004). <em>Monte Carlo Statistical Methods</em> (2nd ed.). Springer.</p>
</div>
</div>
<p><strong>Comprehensive Texts</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="wasserman2004" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Wasserman2004<span class="fn-bracket">]</span></span>
<p>Wasserman, L. (2004). <em>All of Statistics: A Concise Course in Statistical Inference</em>. Springer.</p>
</div>
<div class="citation" id="efronhastie2016" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>EfronHastie2016<span class="fn-bracket">]</span></span>
<p>Efron, B., and Hastie, T. (2016). <em>Computer Age Statistical Inference</em>. Cambridge University Press.</p>
</div>
</div>
<p><strong>Historical and Philosophical Perspectives</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="hacking2001" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Hacking2001<span class="fn-bracket">]</span></span>
<p>Hacking, I. (2001). <em>An Introduction to Probability and Inductive Logic</em>. Cambridge University Press.</p>
</div>
<div class="citation" id="zabell2005" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Zabell2005<span class="fn-bracket">]</span></span>
<p>Zabell, S. L. (2005). <em>Symmetry and Its Discontents: Essays on the History of Inductive Probability</em>. Cambridge University Press.</p>
</div>
<div class="citation" id="shafervovk2019" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ShaferVovk2019<span class="fn-bracket">]</span></span>
<p>Shafer, G., and Vovk, V. (2019). <em>Game-Theoretic Foundations for Probability and Finance</em>. Wiley.</p>
</div>
</div>
<p><strong>Software Documentation</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="pythonrandom" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>PythonRandom<span class="fn-bracket">]</span></span>
<p>Python Software Foundation. (2024). <code class="docutils literal notranslate"><span class="pre">random</span></code> — Generate pseudo-random numbers. <a class="reference external" href="https://docs.python.org/3/library/random.html">https://docs.python.org/3/library/random.html</a></p>
</div>
<div class="citation" id="numpyrandom" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>NumPyRandom<span class="fn-bracket">]</span></span>
<p>NumPy Community. (2024). NumPy random Generator documentation. <a class="reference external" href="https://numpy.org/doc/stable/reference/random/generator.html">https://numpy.org/doc/stable/reference/random/generator.html</a></p>
</div>
<div class="citation" id="scipystats" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>SciPyStats<span class="fn-bracket">]</span></span>
<p>SciPy Community. (2024). SciPy statistical functions documentation. <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/stats.html">https://docs.scipy.org/doc/scipy/reference/stats.html</a></p>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="ch1.3-python_random_generation.html" class="btn btn-neutral float-left" title="1.1.3. Python Random Generation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../../part2_simulation/index.html" class="btn btn-neutral float-right" title="2. Part II: Simulation-Based Methods" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>