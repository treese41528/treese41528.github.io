

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Section 2.3 Inverse CDF Method &mdash; STAT 418</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=3d0abd52" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT418/Website/part1_foundations/chapter2/ch2_3-inverse-cdf-method.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=30646c52"></script>
      <script>let toggleHintShow = 'Show solution';</script>
      <script>let toggleHintHide = 'Hide solution';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>window.MathJax = {"options": {"enableMenu": true, "menuOptions": {"settings": {"enrich": true, "speech": true, "braille": true, "collapsible": true, "assistiveMml": false}}}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
      <script src="../../_static/custom.js?v=8718e0ab"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Section 2.4 Transformation Methods" href="ch2_4-transformation-methods.html" />
    <link rel="prev" title="Section 2.2 Uniform Random Variates" href="ch2_2-uniform-random-variates.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Course Content</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Part I: Foundations of Probability and Computation</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../chapter1/index.html">Chapter 1: Statistical Paradigms and Core Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter1/ch1_1-probability-and-inference-paradigms.html">Section 1.1 Paradigms of Probability and Statistical Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_1-probability-and-inference-paradigms.html#the-mathematical-foundation-kolmogorov-s-axioms">The Mathematical Foundation: Kolmogorov’s Axioms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_1-probability-and-inference-paradigms.html#interpretations-of-probability">Interpretations of Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_1-probability-and-inference-paradigms.html#statistical-inference-paradigms">Statistical Inference Paradigms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_1-probability-and-inference-paradigms.html#historical-and-philosophical-debates">Historical and Philosophical Debates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_1-probability-and-inference-paradigms.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_1-probability-and-inference-paradigms.html#looking-ahead-our-course-focus">Looking Ahead: Our Course Focus</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_1-probability-and-inference-paradigms.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter1/ch1_2-probability_distributions_review.html">Section 1.2 Probability Distributions: Theory and Computation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_2-probability_distributions_review.html#from-abstract-foundations-to-concrete-tools">From Abstract Foundations to Concrete Tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_2-probability_distributions_review.html#the-python-ecosystem-for-probability">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_2-probability_distributions_review.html#introduction-why-probability-distributions-matter">Introduction: Why Probability Distributions Matter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_2-probability_distributions_review.html#id1">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_2-probability_distributions_review.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_2-probability_distributions_review.html#continuous-distributions">Continuous Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_2-probability_distributions_review.html#additional-important-distributions">Additional Important Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_2-probability_distributions_review.html#summary-and-practical-guidelines">Summary and Practical Guidelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_2-probability_distributions_review.html#conclusion">Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_2-probability_distributions_review.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter1/ch1_3-python_random_generation.html">Section 1.3 Python Random Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_3-python_random_generation.html#from-mathematical-distributions-to-computational-samples">From Mathematical Distributions to Computational Samples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_3-python_random_generation.html#the-python-ecosystem-at-a-glance">The Python Ecosystem at a Glance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_3-python_random_generation.html#understanding-pseudo-random-number-generation">Understanding Pseudo-Random Number Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_3-python_random_generation.html#the-standard-library-random-module">The Standard Library: <code class="docutils literal notranslate"><span class="pre">random</span></code> Module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_3-python_random_generation.html#numpy-fast-vectorized-random-sampling">NumPy: Fast Vectorized Random Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_3-python_random_generation.html#scipy-stats-the-complete-statistical-toolkit">SciPy Stats: The Complete Statistical Toolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_3-python_random_generation.html#bringing-it-all-together-library-selection-guide">Bringing It All Together: Library Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_3-python_random_generation.html#looking-ahead-from-random-numbers-to-monte-carlo-methods">Looking Ahead: From Random Numbers to Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_3-python_random_generation.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter1/ch1_4-chapter-summary.html">Section 1.4 Chapter 1 Summary: Foundations in Place</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_4-chapter-summary.html#the-three-pillars-of-chapter-1">The Three Pillars of Chapter 1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_4-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_4-chapter-summary.html#what-lies-ahead-the-road-to-simulation">What Lies Ahead: The Road to Simulation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_4-chapter-summary.html#chapter-1-exercises-synthesis-problems">Chapter 1 Exercises: Synthesis Problems</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter1/ch1_4-chapter-summary.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Chapter 2: Monte Carlo Simulation</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html">Section 2.1 Monte Carlo Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#the-historical-development-of-monte-carlo-methods">The Historical Development of Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#the-core-principle-expectation-as-integration">The Core Principle: Expectation as Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#theoretical-foundations">Theoretical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#variance-estimation-and-confidence-intervals">Variance Estimation and Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#comparison-with-deterministic-methods">Comparison with Deterministic Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#sample-size-determination">Sample Size Determination</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#convergence-diagnostics-and-monitoring">Convergence Diagnostics and Monitoring</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#chapter-2-1-exercises-monte-carlo-fundamentals-mastery">Chapter 2.1 Exercises: Monte Carlo Fundamentals Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_2-uniform-random-variates.html">Section 2.2 Uniform Random Variates</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#why-uniform-the-universal-currency-of-randomness">Why Uniform? The Universal Currency of Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#the-paradox-of-computational-randomness">The Paradox of Computational Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#chaotic-dynamical-systems-an-instructive-failure">Chaotic Dynamical Systems: An Instructive Failure</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#linear-congruential-generators">Linear Congruential Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#shift-register-generators">Shift-Register Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#the-kiss-generator-combining-strategies">The KISS Generator: Combining Strategies</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#modern-generators-mersenne-twister-and-pcg">Modern Generators: Mersenne Twister and PCG</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#statistical-testing-of-random-number-generators">Statistical Testing of Random Number Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#chapter-2-2-exercises-uniform-random-variates-mastery">Chapter 2.2 Exercises: Uniform Random Variates Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Section 2.3 Inverse CDF Method</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#mathematical-foundations">Mathematical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#continuous-distributions-with-closed-form-inverses">Continuous Distributions with Closed-Form Inverses</a></li>
<li class="toctree-l4"><a class="reference internal" href="#numerical-inversion">Numerical Inversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#mixed-distributions">Mixed Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#chapter-2-3-exercises-inverse-cdf-method-mastery">Chapter 2.3 Exercises: Inverse CDF Method Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_4-transformation-methods.html">Section 2.4 Transformation Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#why-transformation-methods">Why Transformation Methods?</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#the-boxmuller-transform">The Box–Muller Transform</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#the-polar-marsaglia-method">The Polar (Marsaglia) Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#method-comparison-boxmuller-vs-polar-vs-ziggurat">Method Comparison: Box–Muller vs Polar vs Ziggurat</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#the-ziggurat-algorithm">The Ziggurat Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#the-clt-approximation-historical">The CLT Approximation (Historical)</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#distributions-derived-from-the-normal">Distributions Derived from the Normal</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#multivariate-normal-generation">Multivariate Normal Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#implementation-guidance">Implementation Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#chapter-2-4-exercises-transformation-methods-mastery">Chapter 2.4 Exercises: Transformation Methods Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_5-rejection-sampling.html">Section 2.5 Rejection Sampling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#the-dartboard-intuition">The Dartboard Intuition</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#the-accept-reject-algorithm">The Accept-Reject Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#efficiency-analysis">Efficiency Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#choosing-the-proposal-distribution">Choosing the Proposal Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#the-squeeze-principle">The Squeeze Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#geometric-example-sampling-from-the-unit-disk">Geometric Example: Sampling from the Unit Disk</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#limitations-and-the-curse-of-dimensionality">Limitations and the Curse of Dimensionality</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#connections-to-other-methods">Connections to Other Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#chapter-2-5-exercises-rejection-sampling-mastery">Chapter 2.5 Exercises: Rejection Sampling Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_6-variance-reduction-methods.html">Section 2.6 Variance Reduction Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#the-variance-reduction-paradigm">The Variance Reduction Paradigm</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#importance-sampling">Importance Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#control-variates">Control Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#antithetic-variates">Antithetic Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#stratified-sampling">Stratified Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#common-random-numbers">Common Random Numbers</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#conditional-monte-carlo-raoblackwellization">Conditional Monte Carlo (Rao–Blackwellization)</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#combining-variance-reduction-techniques">Combining Variance Reduction Techniques</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#chapter-2-6-exercises-variance-reduction-mastery">Chapter 2.6 Exercises: Variance Reduction Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_7-chapter-summary.html">Section 2.7 Chapter 2 Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#the-complete-monte-carlo-workflow">The Complete Monte Carlo Workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#quick-reference-tables">Quick Reference Tables</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#common-pitfalls-checklist">Common Pitfalls Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#connections-to-later-chapters">Connections to Later Chapters</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#learning-outcomes-checklist">Learning Outcomes Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#further-reading-optimization-and-missing-data">Further Reading: Optimization and Missing Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part2_frequentist/index.html">Part II: Frequentist Inference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part2_frequentist/chapter3/index.html">Chapter 3: Parametric Inference and Likelihood Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_1-exponential-families.html">Section 3.1 Exponential Families</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_1-exponential-families.html#historical-origins-from-scattered-results-to-unified-theory">Historical Origins: From Scattered Results to Unified Theory</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_1-exponential-families.html#the-canonical-exponential-family">The Canonical Exponential Family</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_1-exponential-families.html#converting-familiar-distributions">Converting Familiar Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_1-exponential-families.html#the-log-partition-function-a-moment-generating-machine">The Log-Partition Function: A Moment-Generating Machine</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_1-exponential-families.html#sufficiency-capturing-all-parameter-information">Sufficiency: Capturing All Parameter Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_1-exponential-families.html#minimal-sufficiency-and-completeness">Minimal Sufficiency and Completeness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_1-exponential-families.html#conjugate-priors-and-bayesian-inference">Conjugate Priors and Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_1-exponential-families.html#exponential-dispersion-models-and-glms">Exponential Dispersion Models and GLMs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_1-exponential-families.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_1-exponential-families.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_1-exponential-families.html#chapter-3-1-exercises-exponential-families-mastery">Chapter 3.1 Exercises: Exponential Families Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_1-exponential-families.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_1-exponential-families.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html">Section 3.2 Maximum Likelihood Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#the-likelihood-function">The Likelihood Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#the-score-function">The Score Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#fisher-information">Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#closed-form-maximum-likelihood-estimators">Closed-Form Maximum Likelihood Estimators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#numerical-optimization-for-mle">Numerical Optimization for MLE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#asymptotic-properties-of-mles">Asymptotic Properties of MLEs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#the-cramer-rao-lower-bound">The Cramér-Rao Lower Bound</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#the-invariance-property">The Invariance Property</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#likelihood-based-hypothesis-testing">Likelihood-Based Hypothesis Testing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#confidence-intervals-from-likelihood">Confidence Intervals from Likelihood</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#connection-to-bayesian-inference">Connection to Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#chapter-3-2-exercises-maximum-likelihood-estimation-mastery">Chapter 3.2 Exercises: Maximum Likelihood Estimation Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_3-sampling-variability.html">Section 3.3 Sampling Variability and Variance Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_3-sampling-variability.html#statistical-estimators-and-their-properties">Statistical Estimators and Their Properties</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_3-sampling-variability.html#sampling-distributions">Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_3-sampling-variability.html#the-delta-method">The Delta Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_3-sampling-variability.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_3-sampling-variability.html#variance-estimation-methods">Variance Estimation Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_3-sampling-variability.html#applications-and-worked-examples">Applications and Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_3-sampling-variability.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_3-sampling-variability.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_3-sampling-variability.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_3-sampling-variability.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_4-linear-models.html">Section 3.4 Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_4-linear-models.html#matrix-calculus-foundations">Matrix Calculus Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_4-linear-models.html#the-linear-model">The Linear Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_4-linear-models.html#ordinary-least-squares-the-calculus-approach">Ordinary Least Squares: The Calculus Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_4-linear-models.html#ordinary-least-squares-the-geometric-approach">Ordinary Least Squares: The Geometric Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_4-linear-models.html#properties-of-the-ols-estimator">Properties of the OLS Estimator</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_4-linear-models.html#the-gauss-markov-theorem">The Gauss-Markov Theorem</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_4-linear-models.html#estimating-the-error-variance">Estimating the Error Variance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_4-linear-models.html#distributional-results-under-normality">Distributional Results Under Normality</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_4-linear-models.html#diagnostics-and-model-checking">Diagnostics and Model Checking</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_4-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_4-linear-models.html#numerical-stability-qr-decomposition">Numerical Stability: QR Decomposition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_4-linear-models.html#model-selection-and-information-criteria">Model Selection and Information Criteria</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_4-linear-models.html#regularization-ridge-and-lasso">Regularization: Ridge and LASSO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_4-linear-models.html#chapter-3-4-exercises-linear-models-mastery">Chapter 3.4 Exercises: Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_4-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_5-generalized-linear-models.html">Section 3.5 Generalized Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#historical-context-unification-of-regression-methods">Historical Context: Unification of Regression Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#the-glm-framework-three-components">The GLM Framework: Three Components</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#score-equations-and-fisher-information">Score Equations and Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#iteratively-reweighted-least-squares">Iteratively Reweighted Least Squares</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#logistic-regression-binary-outcomes">Logistic Regression: Binary Outcomes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#poisson-regression-count-data">Poisson Regression: Count Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#gamma-regression-positive-continuous-data">Gamma Regression: Positive Continuous Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#inference-in-glms-the-testing-triad">Inference in GLMs: The Testing Triad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#model-diagnostics">Model Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#model-comparison-and-selection">Model Comparison and Selection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#quasi-likelihood-and-robust-inference">Quasi-Likelihood and Robust Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#further-reading">Further Reading</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#chapter-3-5-exercises-generalized-linear-models-mastery">Chapter 3.5 Exercises: Generalized Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_6-chapter-summary.html">Section 3.6 Chapter 3 Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_6-chapter-summary.html#the-parametric-inference-pipeline">The Parametric Inference Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_6-chapter-summary.html#the-five-pillars-of-chapter-3">The Five Pillars of Chapter 3</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_6-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_6-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_6-chapter-summary.html#quick-reference-core-formulas">Quick Reference: Core Formulas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_6-chapter-summary.html#connections-to-future-material">Connections to Future Material</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_6-chapter-summary.html#practical-guidance">Practical Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_6-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter3/ch3_6-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../part2_frequentist/chapter4/index.html">Chapter 4: Resampling Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_1-sampling-distribution-problem.html">Section 4.1 The Sampling Distribution Problem</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_1-sampling-distribution-problem.html#the-fundamental-target-sampling-distributions">The Fundamental Target: Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_1-sampling-distribution-problem.html#historical-development-the-quest-for-sampling-distributions">Historical Development: The Quest for Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_1-sampling-distribution-problem.html#three-routes-to-the-sampling-distribution">Three Routes to the Sampling Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_1-sampling-distribution-problem.html#when-asymptotics-fail-motivating-the-bootstrap">When Asymptotics Fail: Motivating the Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_1-sampling-distribution-problem.html#the-plug-in-principle-theoretical-foundation">The Plug-In Principle: Theoretical Foundation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_1-sampling-distribution-problem.html#computational-perspective-bootstrap-as-monte-carlo">Computational Perspective: Bootstrap as Monte Carlo</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_1-sampling-distribution-problem.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_1-sampling-distribution-problem.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_1-sampling-distribution-problem.html#chapter-4-1-exercises">Chapter 4.1 Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_1-sampling-distribution-problem.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_2-empirical-distribution-plugin.html">Section 4.2 The Empirical Distribution and Plug-in Principle</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_2-empirical-distribution-plugin.html#the-empirical-cumulative-distribution-function">The Empirical Cumulative Distribution Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_2-empirical-distribution-plugin.html#convergence-of-the-empirical-cdf">Convergence of the Empirical CDF</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_2-empirical-distribution-plugin.html#parameters-as-statistical-functionals">Parameters as Statistical Functionals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_2-empirical-distribution-plugin.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_2-empirical-distribution-plugin.html#when-the-plug-in-principle-fails">When the Plug-in Principle Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_2-empirical-distribution-plugin.html#the-bootstrap-idea-in-one-sentence">The Bootstrap Idea in One Sentence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_2-empirical-distribution-plugin.html#computational-implementation">Computational Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_2-empirical-distribution-plugin.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_2-empirical-distribution-plugin.html#section-4-2-exercises-ecdf-and-plug-in-mastery">Section 4.2 Exercises: ECDF and Plug-in Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_2-empirical-distribution-plugin.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_3-nonparametric-bootstrap.html">Section 4.3 The Nonparametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_3-nonparametric-bootstrap.html#the-bootstrap-principle">The Bootstrap Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-standard-errors">Bootstrap Standard Errors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-bias-estimation">Bootstrap Bias Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-confidence-intervals">Bootstrap Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-for-regression">Bootstrap for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-diagnostics">Bootstrap Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_3-nonparametric-bootstrap.html#when-bootstrap-fails">When Bootstrap Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_3-nonparametric-bootstrap.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_3-nonparametric-bootstrap.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_3-nonparametric-bootstrap.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_3-nonparametric-bootstrap.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_4-parametric-bootstrap.html">Section 4.4: The Parametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_4-parametric-bootstrap.html#the-parametric-bootstrap-principle">The Parametric Bootstrap Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_4-parametric-bootstrap.html#location-scale-families">Location-Scale Families</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_4-parametric-bootstrap.html#parametric-bootstrap-for-regression">Parametric Bootstrap for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_4-parametric-bootstrap.html#confidence-intervals">Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_4-parametric-bootstrap.html#model-checking-and-validation">Model Checking and Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_4-parametric-bootstrap.html#when-parametric-bootstrap-fails">When Parametric Bootstrap Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_4-parametric-bootstrap.html#parametric-vs-nonparametric-a-decision-framework">Parametric vs. Nonparametric: A Decision Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_4-parametric-bootstrap.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_4-parametric-bootstrap.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_4-parametric-bootstrap.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_5-jackknife-methods.html">Section 4.5: Jackknife Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_5-jackknife-methods.html#historical-context-and-motivation">Historical Context and Motivation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_5-jackknife-methods.html#the-delete-1-jackknife">The Delete-1 Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_5-jackknife-methods.html#jackknife-bias-estimation">Jackknife Bias Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_5-jackknife-methods.html#the-delete-d-jackknife">The Delete-<span class="math notranslate nohighlight">\(d\)</span> Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_5-jackknife-methods.html#jackknife-versus-bootstrap">Jackknife versus Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_5-jackknife-methods.html#the-infinitesimal-jackknife">The Infinitesimal Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_5-jackknife-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_5-jackknife-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_5-jackknife-methods.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_5-jackknife-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_6-bootstrap-hypothesis-testing.html">Section 4.6 Bootstrap Hypothesis Testing and Permutation Tests</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_6-bootstrap-hypothesis-testing.html#from-confidence-intervals-to-hypothesis-tests">From Confidence Intervals to Hypothesis Tests</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_6-bootstrap-hypothesis-testing.html#the-bootstrap-hypothesis-testing-framework">The Bootstrap Hypothesis Testing Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_6-bootstrap-hypothesis-testing.html#permutation-tests-exact-tests-under-exchangeability">Permutation Tests: Exact Tests Under Exchangeability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_6-bootstrap-hypothesis-testing.html#testing-equality-of-distributions">Testing Equality of Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_6-bootstrap-hypothesis-testing.html#bootstrap-tests-for-regression">Bootstrap Tests for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_6-bootstrap-hypothesis-testing.html#bootstrap-vs-classical-tests">Bootstrap vs Classical Tests</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_6-bootstrap-hypothesis-testing.html#permutation-vs-bootstrap-choosing-the-right-approach">Permutation vs Bootstrap: Choosing the Right Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_6-bootstrap-hypothesis-testing.html#multiple-testing-with-bootstrap">Multiple Testing with Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_6-bootstrap-hypothesis-testing.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_6-bootstrap-hypothesis-testing.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_6-bootstrap-hypothesis-testing.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_6-bootstrap-hypothesis-testing.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_7-bootstrap-confidence-intervals.html">Section 4.7 Bootstrap Confidence Intervals: Advanced Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_7-bootstrap-confidence-intervals.html#why-advanced-methods">Why Advanced Methods?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_7-bootstrap-confidence-intervals.html#the-studentized-bootstrap-t-interval">The Studentized (Bootstrap-t) Interval</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_7-bootstrap-confidence-intervals.html#bias-corrected-bc-intervals">Bias-Corrected (BC) Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_7-bootstrap-confidence-intervals.html#bias-corrected-and-accelerated-bca-intervals">Bias-Corrected and Accelerated (BCa) Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_7-bootstrap-confidence-intervals.html#choosing-b-and-assessing-monte-carlo-error">Choosing B and Assessing Monte Carlo Error</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_7-bootstrap-confidence-intervals.html#diagnostics-for-advanced-bootstrap-methods">Diagnostics for Advanced Bootstrap Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_7-bootstrap-confidence-intervals.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_7-bootstrap-confidence-intervals.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_7-bootstrap-confidence-intervals.html#chapter-4-7-exercises-bootstrap-confidence-interval-mastery">Chapter 4.7 Exercises: Bootstrap Confidence Interval Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part2_frequentist/chapter4/ch4_7-bootstrap-confidence-intervals.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part3_bayesian/index.html">Part III: Bayesian Inference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part3_bayesian/chapter5/index.html">Chapter 5: Bayesian Inference</a><ul class="simple">
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part4_llms_datascience/index.html">Part IV: Large Language Models in Data Science</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part4_llms_datascience/chapter6/index.html">Chapter 6: LLMs in Data Science Workflows</a><ul class="simple">
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Part I: Foundations of Probability and Computation</a></li>
          <li class="breadcrumb-item"><a href="index.html">Chapter 2: Monte Carlo Simulation</a></li>
      <li class="breadcrumb-item active">Section 2.3 Inverse CDF Method</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/part1_foundations/chapter2/ch2_3-inverse-cdf-method.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="section-2-3-inverse-cdf-method">
<span id="ch2-3-inverse-cdf-method"></span><h1>Section 2.3 Inverse CDF Method<a class="headerlink" href="#section-2-3-inverse-cdf-method" title="Link to this heading"></a></h1>
<p>With a reliable supply of uniform random variates in hand—the fruit of <a class="reference internal" href="ch2_2-uniform-random-variates.html#ch2-2-uniform-random-variates"><span class="std std-ref">Section 2.2 Uniform Random Variates</span></a>’s exploration of pseudo-random number generators—we now face the central question of random variable generation: <em>how do we transform these uniform numbers into samples from other distributions?</em></p>
<p>The answer lies in one of the most elegant results in probability theory: the Probability Integral Transform. This theorem, which we previewed in our discussion of why uniform variates are “universal currency,” now receives its full mathematical treatment. It tells us that any distribution can be sampled by computing the inverse of its cumulative distribution function applied to a uniform random number. The method is universal in principle: it works for continuous, discrete, and mixed distributions alike. When the inverse CDF has a closed-form expression—as it does for exponential, Weibull, Pareto, and Cauchy distributions—implementation is immediate and efficient.</p>
<p>But the inverse CDF method is more than a theoretical curiosity. It is the workhorse of random number generation, the first algorithm any practitioner should consider when faced with a new distribution. Understanding when it applies, how to implement it efficiently, and when to seek alternatives is essential knowledge for computational statistics.</p>
<p>This section develops the inverse CDF method in full generality. We begin with the mathematical foundations—proving why the method works and what the “generalized inverse” means for distributions that lack smooth inverses. We then work through continuous distributions with closed-form inverses, deriving and implementing samplers for several important cases. For discrete distributions, we develop increasingly sophisticated algorithms: linear search, binary search, and the remarkable alias method that achieves constant-time sampling. We address numerical issues that arise in practice and identify the method’s limitations—setting the stage for rejection sampling and specialized transformations in subsequent sections.</p>
<div class="important admonition">
<p class="admonition-title">Road Map 🧭</p>
<ul class="simple">
<li><p><strong>Understand</strong>: Why the inverse CDF method produces correctly distributed samples, with complete proofs for both continuous and discrete cases</p></li>
<li><p><strong>Master</strong>: Closed-form inverse CDFs for exponential, Weibull, Pareto, Cauchy, and logistic distributions</p></li>
<li><p><strong>Develop</strong>: Efficient algorithms for discrete distributions—from <span class="math notranslate nohighlight">\(O(K)\)</span> linear search through <span class="math notranslate nohighlight">\(O(\log K)\)</span> binary search to <span class="math notranslate nohighlight">\(O(1)\)</span> alias method</p></li>
<li><p><strong>Implement</strong>: Complete Python code with attention to numerical precision and edge cases</p></li>
<li><p><strong>Evaluate</strong>: When the inverse CDF method excels and when alternatives (rejection sampling, specialized transforms) are preferable</p></li>
</ul>
</div>
<section id="mathematical-foundations">
<h2>Mathematical Foundations<a class="headerlink" href="#mathematical-foundations" title="Link to this heading"></a></h2>
<p>The inverse CDF method rests on a deep connection between the uniform distribution and all other distributions. We now develop this connection rigorously.</p>
<section id="the-cumulative-distribution-function">
<h3>The Cumulative Distribution Function<a class="headerlink" href="#the-cumulative-distribution-function" title="Link to this heading"></a></h3>
<p>For any random variable <span class="math notranslate nohighlight">\(X\)</span>, the <strong>cumulative distribution function</strong> (CDF) <span class="math notranslate nohighlight">\(F_X: \mathbb{R} \to [0, 1]\)</span> is defined by:</p>
<div class="math notranslate nohighlight">
\[F_X(x) = P(X \leq x)\]</div>
<p>The CDF has several fundamental properties:</p>
<ol class="arabic simple">
<li><p><strong>Monotonicity</strong>: <span class="math notranslate nohighlight">\(F_X\)</span> is non-decreasing. If <span class="math notranslate nohighlight">\(x_1 &lt; x_2\)</span>, then <span class="math notranslate nohighlight">\(F_X(x_1) \leq F_X(x_2)\)</span>.</p></li>
<li><p><strong>Right-continuity</strong>: <span class="math notranslate nohighlight">\(\lim_{h \to 0^+} F_X(x + h) = F_X(x)\)</span> for all <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
<li><p><strong>Limits at infinity</strong>: <span class="math notranslate nohighlight">\(\lim_{x \to -\infty} F_X(x) = 0\)</span> and <span class="math notranslate nohighlight">\(\lim_{x \to \infty} F_X(x) = 1\)</span>.</p></li>
</ol>
<p>For <strong>continuous</strong> random variables with density <span class="math notranslate nohighlight">\(f_X\)</span>, the CDF is obtained by integration:</p>
<div class="math notranslate nohighlight">
\[F_X(x) = \int_{-\infty}^{x} f_X(t) \, dt\]</div>
<p>and the CDF is continuous everywhere.</p>
<p>For <strong>discrete</strong> random variables taking values <span class="math notranslate nohighlight">\(x_1 &lt; x_2 &lt; \cdots\)</span> with probabilities <span class="math notranslate nohighlight">\(p_1, p_2, \ldots\)</span>, the CDF is a step function:</p>
<div class="math notranslate nohighlight">
\[F_X(x) = \sum_{x_i \leq x} p_i\]</div>
<p>The CDF jumps by <span class="math notranslate nohighlight">\(p_i\)</span> at each value <span class="math notranslate nohighlight">\(x_i\)</span>.</p>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide009_img003_8228ef1b.png"><img alt="CDF for discrete distribution showing step function behavior" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide009_img003_8228ef1b.png" style="width: 55%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 45 </span><span class="caption-text"><strong>CDF for Discrete Distributions.</strong> The cumulative distribution function of a discrete random variable is a step function with jumps at each possible value. For the killdeer clutch size example shown, the CDF maps any <span class="math notranslate nohighlight">\(u \in (0, 1)\)</span> to the smallest outcome <span class="math notranslate nohighlight">\(x\)</span> where <span class="math notranslate nohighlight">\(F(x) \geq u\)</span>—this is precisely the generalized inverse.</span><a class="headerlink" href="#id2" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="the-generalized-inverse">
<h3>The Generalized Inverse<a class="headerlink" href="#the-generalized-inverse" title="Link to this heading"></a></h3>
<p>When <span class="math notranslate nohighlight">\(F_X\)</span> is continuous and strictly increasing, it has a unique inverse: <span class="math notranslate nohighlight">\(F_X^{-1}(u)\)</span> is the unique <span class="math notranslate nohighlight">\(x\)</span> such that <span class="math notranslate nohighlight">\(F_X(x) = u\)</span>. But for discrete distributions (where <span class="math notranslate nohighlight">\(F_X\)</span> is a step function) or mixed distributions, the ordinary inverse does not exist. We need a more general definition.</p>
<div class="note admonition">
<p class="admonition-title">Definition: Generalized Inverse (Quantile Function)</p>
<p>For any distribution function <span class="math notranslate nohighlight">\(F: \mathbb{R} \to [0, 1]\)</span>, the <strong>generalized inverse</strong> (or <strong>quantile function</strong>) is:</p>
<div class="math notranslate nohighlight" id="equation-gen-inverse-def">
<span class="eqno">(8)<a class="headerlink" href="#equation-gen-inverse-def" title="Link to this equation"></a></span>\[F^{-1}(u) = \inf\{x \in \mathbb{R} : F(x) \geq u\} \quad \text{for } u \in (0, 1)\]</div>
<p>This is the smallest <span class="math notranslate nohighlight">\(x\)</span> for which the CDF reaches or exceeds <span class="math notranslate nohighlight">\(u\)</span>.</p>
</div>
<p>The generalized inverse has several important properties:</p>
<p><strong>Property 1</strong>: For continuous, strictly increasing <span class="math notranslate nohighlight">\(F\)</span>, the generalized inverse coincides with the ordinary inverse.</p>
<p><strong>Property 2</strong>: For any <span class="math notranslate nohighlight">\(F\)</span>, <span class="math notranslate nohighlight">\(F^{-1}(u) \leq x\)</span> if and only if <span class="math notranslate nohighlight">\(u \leq F(x)\)</span>. This equivalence is the key to proving the inverse CDF method works.</p>
<p><strong>Property 3</strong>: <span class="math notranslate nohighlight">\(F^{-1}\)</span> is non-decreasing and left-continuous.</p>
<p><strong>Proof of Property 2</strong>:</p>
<p>(<span class="math notranslate nohighlight">\(\Leftarrow\)</span>) Suppose <span class="math notranslate nohighlight">\(u \leq F(x)\)</span>. Then <span class="math notranslate nohighlight">\(x \in \{t : F(t) \geq u\}\)</span>, so <span class="math notranslate nohighlight">\(F^{-1}(u) = \inf\{t : F(t) \geq u\} \leq x\)</span>.</p>
<p>(<span class="math notranslate nohighlight">\(\Rightarrow\)</span>) Suppose <span class="math notranslate nohighlight">\(F^{-1}(u) \leq x\)</span>. By definition, <span class="math notranslate nohighlight">\(F^{-1}(u) = \inf\{t : F(t) \geq u\}\)</span>. By right-continuity of <span class="math notranslate nohighlight">\(F\)</span>, <span class="math notranslate nohighlight">\(F(F^{-1}(u)) \geq u\)</span>. Since <span class="math notranslate nohighlight">\(F\)</span> is non-decreasing and <span class="math notranslate nohighlight">\(F^{-1}(u) \leq x\)</span>, we have <span class="math notranslate nohighlight">\(F(x) \geq F(F^{-1}(u)) \geq u\)</span>. ∎</p>
</section>
<section id="the-probability-integral-transform">
<h3>The Probability Integral Transform<a class="headerlink" href="#the-probability-integral-transform" title="Link to this heading"></a></h3>
<p>With the generalized inverse defined, we can state and prove the fundamental result.</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Probability Integral Transform (Inverse Direction)</p>
<p>If <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0, 1)\)</span> and <span class="math notranslate nohighlight">\(F\)</span> is any distribution function, then <span class="math notranslate nohighlight">\(X = F^{-1}(U)\)</span> has distribution function <span class="math notranslate nohighlight">\(F\)</span>.</p>
</div>
<p><strong>Proof</strong>: We must show that <span class="math notranslate nohighlight">\(P(X \leq x) = F(x)\)</span> for all <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}P(X \leq x) &amp;= P(F^{-1}(U) \leq x) \\
            &amp;= P(U \leq F(x)) \quad \text{(by Property 2)} \\
            &amp;= F(x) \quad \text{(since } U \sim \text{Uniform}(0,1) \text{)}\end{split}\]</div>
<p>The final equality uses the fact that for <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0,1)\)</span>, <span class="math notranslate nohighlight">\(P(U \leq u) = u\)</span> for any <span class="math notranslate nohighlight">\(u \in [0, 1]\)</span>. ∎</p>
<p>This proof is remarkably general. It applies to:</p>
<ul class="simple">
<li><p>Continuous distributions with smooth inverses</p></li>
<li><p>Discrete distributions with step-function CDFs</p></li>
<li><p>Mixed distributions combining point masses and continuous components</p></li>
<li><p>Any distribution whatsoever, provided we use the generalized inverse</p></li>
</ul>
<p>The converse also holds under mild conditions:</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Probability Integral Transform (Forward Direction)</p>
<p>If <span class="math notranslate nohighlight">\(X\)</span> has a continuous CDF <span class="math notranslate nohighlight">\(F\)</span>, then <span class="math notranslate nohighlight">\(U = F(X) \sim \text{Uniform}(0, 1)\)</span>.</p>
</div>
<p><strong>Proof</strong>: For <span class="math notranslate nohighlight">\(u \in (0, 1)\)</span>:</p>
<div class="math notranslate nohighlight">
\[P(F(X) \leq u) = P(X \leq F^{-1}(u)) = F(F^{-1}(u)) = u\]</div>
<p>where the last equality holds because <span class="math notranslate nohighlight">\(F\)</span> is continuous and strictly increasing on its support. ∎</p>
</section>
<section id="geometric-interpretation">
<h3>Geometric Interpretation<a class="headerlink" href="#geometric-interpretation" title="Link to this heading"></a></h3>
<p>The inverse CDF method has an intuitive geometric interpretation. Imagine the graph of the CDF <span class="math notranslate nohighlight">\(F(x)\)</span>:</p>
<ol class="arabic simple">
<li><p>Generate a uniform random number <span class="math notranslate nohighlight">\(u \in (0, 1)\)</span> — a random height on the <span class="math notranslate nohighlight">\(y\)</span>-axis.</p></li>
<li><p>Draw a horizontal line at height <span class="math notranslate nohighlight">\(u\)</span>.</p></li>
<li><p>Find where this line first intersects the CDF curve (or, for step functions, where it first reaches the CDF from below).</p></li>
<li><p>Project down to the <span class="math notranslate nohighlight">\(x\)</span>-axis. This is your sample <span class="math notranslate nohighlight">\(F^{-1}(u)\)</span>.</p></li>
</ol>
<p>For continuous distributions with strictly increasing CDFs, the intersection is unique. For discrete distributions, the horizontal line may intersect a “riser” of the step function; the generalized inverse picks the <span class="math notranslate nohighlight">\(x\)</span>-value at the top of that riser.</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_3_fig01_inverse_cdf_method.png"><img alt="Six-panel visualization of the inverse CDF method showing the geometric transformation" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_3_fig01_inverse_cdf_method.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 46 </span><span class="caption-text"><strong>The Inverse CDF Method: Geometric View.</strong> Top row: For an exponential distribution, we start with a uniform sample <span class="math notranslate nohighlight">\(U\)</span> (left), apply the inverse CDF transformation <span class="math notranslate nohighlight">\(X = F^{-1}(U)\)</span> (center), and obtain an exponentially distributed sample (right). The transformation “stretches” uniform samples according to where the CDF is shallow (rare values) and “compresses” where the CDF is steep (common values). Bottom row: The same process applied to three different distributions—Cauchy (heavy tails), Normal (symmetric), and Beta (flexible shape)—demonstrates the universality of the method.</span><a class="headerlink" href="#id3" title="Link to this image"></a></p>
</figcaption>
</figure>
<div class="tip admonition">
<p class="admonition-title">Try It Yourself 🖥️ Interactive Inverse CDF Visualizations</p>
<p>Two interactive simulations help you build intuition for the inverse CDF method. We strongly recommend experimenting with both before proceeding.</p>
<p><strong>Simulation 1: Generalized Inverse CDF Explorer</strong></p>
<p><a class="reference external" href="https://treese41528.github.io/ComputationalDataScience/Simulations/MonteCarloSimulation/inverse_cdf_transform_general.html">https://treese41528.github.io/ComputationalDataScience/Simulations/MonteCarloSimulation/inverse_cdf_transform_general.html</a></p>
<p>This simulation displays three synchronized panels showing how probability density, cumulative distribution, and the inverse transform relate geometrically:</p>
<ul class="simple">
<li><p><strong>Left panel (PDF/PMF)</strong>: Shows the density or mass function with the shaded area equal to <span class="math notranslate nohighlight">\(u\)</span>. The x-axis marks the current <span class="math notranslate nohighlight">\(F^{-1}(u)\)</span> value, showing exactly where probability accumulates.</p></li>
<li><p><strong>Center panel (CDF)</strong>: Shows <span class="math notranslate nohighlight">\(F(x)\)</span> with a horizontal dashed line at height <span class="math notranslate nohighlight">\(u\)</span>. The intersection point projects down to <span class="math notranslate nohighlight">\(x = F^{-1}(u)\)</span>.</p></li>
<li><p><strong>Right panel (Inverse CDF)</strong>: Shows <span class="math notranslate nohighlight">\(F^{-1}(u)\)</span> directly as a function of <span class="math notranslate nohighlight">\(u\)</span>. For discrete distributions, observe the characteristic plateaus (flat regions).</p></li>
<li><p><strong>Distribution selector</strong>: Choose from <em>Continuous (Normal)</em>, <em>Continuous (Exponential)</em>, <em>Discrete (Poisson)</em>, <em>Discrete (Geometric)</em>, or <em>Mixed (Zero-inflated)</em>.</p></li>
<li><p><strong>High-precision slider</strong>: The <span class="math notranslate nohighlight">\(u\)</span> range extends from 0.0001 to 0.9999, allowing exploration of extreme quantiles in the tails.</p></li>
<li><p><strong>Dynamic insight box</strong>: Updates automatically to explain what’s happening for each distribution type.</p></li>
</ul>
<p><strong>Key observations to make</strong>:</p>
<ol class="arabic simple">
<li><p>For <em>continuous distributions</em>, all three panels show smooth curves. The steepness of the CDF corresponds to the height of the PDF—steep regions have high density.</p></li>
<li><p>For <em>discrete distributions</em> (Poisson, Geometric), the CDF is a step function with jumps at each integer. The inverse CDF shows <em>plateaus</em>: many <span class="math notranslate nohighlight">\(u\)</span> values in <span class="math notranslate nohighlight">\((F(k-1), F(k)]\)</span> all map to the same integer <span class="math notranslate nohighlight">\(k\)</span>. Open and filled circles mark the jump discontinuities.</p></li>
<li><p>For the <em>mixed zero-inflated</em>, observe the point mass at zero in the PDF panel, the corresponding jump in the CDF, and the flat region in the inverse CDF for <span class="math notranslate nohighlight">\(u \in [0, p_0]\)</span>.</p></li>
<li><p><strong>Explore the tails</strong>: Drag <span class="math notranslate nohighlight">\(u\)</span> close to 0.0001 or 0.9999 with the Normal or Exponential distribution to see extreme quantiles. The 4-decimal precision lets you see exactly how tail probabilities map to extreme values.</p></li>
<li><p><strong>Hover over the CDF or Inverse CDF panels</strong> to interactively explore—this helps you see the geometric duality between the two representations.</p></li>
</ol>
<p><strong>Simulation 2: Inverse Transform Sampling in Action</strong></p>
<p><a class="reference external" href="https://treese41528.github.io/ComputationalDataScience/Simulations/MonteCarloSimulation/inverse_cdf_transform_sampling.html">https://treese41528.github.io/ComputationalDataScience/Simulations/MonteCarloSimulation/inverse_cdf_transform_sampling.html</a></p>
<p>This simulation demonstrates the <em>sampling process</em> for both continuous and discrete distributions. Watch uniform random numbers transform into samples through the inverse CDF:</p>
<ul class="simple">
<li><p><strong>CDF panel (left sidebar)</strong>: Shows the CDF with the current <span class="math notranslate nohighlight">\(u\)</span> value as a horizontal line and its projection to <span class="math notranslate nohighlight">\(x\)</span>. For discrete distributions, this visualizes the “search” process of finding the smallest <span class="math notranslate nohighlight">\(k\)</span> where <span class="math notranslate nohighlight">\(F(k) \geq u\)</span>.</p></li>
<li><p><strong>Computation display</strong>: Shows the exact arithmetic for each sample, including search steps for discrete distributions (e.g., <span class="math notranslate nohighlight">\(F(0) = 0.018 \to F(1) = 0.092 \to \ldots\)</span> until <span class="math notranslate nohighlight">\(F(k) \geq u\)</span>).</p></li>
<li><p><strong>Left histogram</strong>: Uniform <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0,1)\)</span> samples accumulating toward a flat density.</p></li>
<li><p><strong>Right histogram</strong>: Transformed <span class="math notranslate nohighlight">\(X = F^{-1}(U)\)</span> samples with the theoretical PDF (continuous) or PMF (discrete, shown as gold outline bars) overlaid.</p></li>
<li><p><strong>Distribution selector</strong>:</p>
<ul>
<li><p><em>Continuous</em>: Exponential, Normal, Logistic, Pareto, Cauchy</p></li>
<li><p><em>Discrete</em>: Bernoulli, Binomial, Geometric, Poisson</p></li>
</ul>
</li>
<li><p><strong>Distribution badge</strong>: Indicates whether the current distribution is continuous or discrete.</p></li>
</ul>
<p><strong>Key observations to make</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Continuous vs. discrete</strong>: For continuous distributions, observe smooth histogram convergence to the PDF curve. For discrete distributions, watch probability mass concentrate at integer values, matching the PMF bars.</p></li>
<li><p><strong>The search process</strong>: With discrete distributions, the computation panel shows how the algorithm searches through cumulative probabilities: <span class="math notranslate nohighlight">\(F(0), F(1), F(2), \ldots\)</span> until finding the first <span class="math notranslate nohighlight">\(k\)</span> where <span class="math notranslate nohighlight">\(F(k) \geq u\)</span>. This is the generalized inverse in action.</p></li>
<li><p><strong>Concentration vs. spread</strong>: For the exponential, uniform values near 0 produce small <span class="math notranslate nohighlight">\(X\)</span>, while values near 1 produce large <span class="math notranslate nohighlight">\(X\)</span>. The inverse CDF “stretches” the uniform distribution according to where the target distribution has more or less mass.</p></li>
<li><p><strong>Heavy tails</strong>: Try the Cauchy distribution and watch for occasional extreme values when <span class="math notranslate nohighlight">\(U\)</span> approaches 0 or 1. The sample mean fluctuates wildly—a vivid demonstration that the Cauchy has no finite mean.</p></li>
<li><p><strong>Discrete probability matching</strong>: With Binomial(10, 0.4), watch how the histogram bars converge to match the gold PMF outline. The most probable outcomes (around <span class="math notranslate nohighlight">\(k = 4\)</span>) accumulate samples fastest.</p></li>
<li><p><strong>Formula verification</strong>: Pause the animation and verify computations manually. For Geometric with <span class="math notranslate nohighlight">\(p = 0.25\)</span> and <span class="math notranslate nohighlight">\(U = 0.5\)</span>: <span class="math notranslate nohighlight">\(X = \lceil \ln(1-0.5)/\ln(0.75) \rceil = \lceil 2.41 \rceil = 3\)</span>.</p></li>
</ol>
</div>
</section>
</section>
<section id="continuous-distributions-with-closed-form-inverses">
<h2>Continuous Distributions with Closed-Form Inverses<a class="headerlink" href="#continuous-distributions-with-closed-form-inverses" title="Link to this heading"></a></h2>
<p>The inverse CDF method is most elegant when <span class="math notranslate nohighlight">\(F^{-1}\)</span> has a closed-form expression. We can then generate samples with just a few arithmetic operations—typically faster than any alternative method.</p>
<section id="the-exponential-distribution">
<h3>The Exponential Distribution<a class="headerlink" href="#the-exponential-distribution" title="Link to this heading"></a></h3>
<p>The exponential distribution is the canonical example for the inverse CDF method. It models waiting times in Poisson processes and appears throughout reliability theory, queuing theory, and survival analysis.</p>
<p><strong>Setup</strong>: <span class="math notranslate nohighlight">\(X \sim \text{Exponential}(\lambda)\)</span> with rate parameter <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span>.</p>
<p><strong>CDF</strong>:</p>
<div class="math notranslate nohighlight">
\[F(x) = 1 - e^{-\lambda x}, \quad x \geq 0\]</div>
<p><strong>Deriving the Inverse</strong>: Solve <span class="math notranslate nohighlight">\(F(x) = u\)</span> for <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}1 - e^{-\lambda x} &amp;= u \\
e^{-\lambda x} &amp;= 1 - u \\
-\lambda x &amp;= \ln(1 - u) \\
x &amp;= -\frac{\ln(1 - u)}{\lambda}\end{split}\]</div>
<p><strong>Simplification</strong>: Since <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(1 - U\)</span> have the same distribution when <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0, 1)\)</span>, we can use either:</p>
<div class="math notranslate nohighlight">
\[F^{-1}(u) = -\frac{\ln(1 - u)}{\lambda} \quad \text{or} \quad F^{-1}(u) = -\frac{\ln(u)}{\lambda}\]</div>
<p>The second form saves one subtraction but requires care when <span class="math notranslate nohighlight">\(u = 0\)</span> (which occurs with probability zero but may arise from floating-point edge cases).</p>
<div class="note admonition">
<p class="admonition-title">Example 💡 Generating Exponential Random Variables</p>
<p><strong>Given</strong>: Generate 10,000 samples from <span class="math notranslate nohighlight">\(\text{Exponential}(\lambda = 2)\)</span>.</p>
<p><strong>Mathematical approach</strong>:</p>
<p>For <span class="math notranslate nohighlight">\(\lambda = 2\)</span>, <span class="math notranslate nohighlight">\(F^{-1}(u) = -\ln(1-u)/2\)</span>.</p>
<p><strong>Theoretical properties</strong>:</p>
<ul class="simple">
<li><p>Mean: <span class="math notranslate nohighlight">\(\mathbb{E}[X] = 1/\lambda = 0.5\)</span></p></li>
<li><p>Variance: <span class="math notranslate nohighlight">\(\text{Var}(X) = 1/\lambda^2 = 0.25\)</span></p></li>
<li><p>Median: <span class="math notranslate nohighlight">\(F^{-1}(0.5) = \ln(2)/\lambda \approx 0.347\)</span></p></li>
</ul>
<p><strong>Python implementation</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">exponential_inverse_cdf</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">rate</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate Exponential(rate) samples via inverse CDF.</span>

<span class="sd">    Uses the tail-stable form X = -log(U)/rate, avoiding (1-U).</span>
<span class="sd">    Since U and 1-U have the same distribution, this is equivalent</span>
<span class="sd">    but provides better resolution for large X values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="c1"># Guard against log(0) if the RNG can return exactly 0</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">u</span><span class="p">)</span> <span class="o">/</span> <span class="n">rate</span>

<span class="c1"># Generate samples</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10_000</span>
<span class="n">rate</span> <span class="o">=</span> <span class="mf">2.0</span>

<span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">exponential_inverse_cdf</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">rate</span><span class="p">)</span>

<span class="c1"># Verify</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="mi">1</span><span class="o">/</span><span class="n">rate</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="mi">1</span><span class="o">/</span><span class="n">rate</span><span class="o">**</span><span class="mi">2</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample median: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">rate</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Output</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Sample</span> <span class="n">mean</span><span class="p">:</span> <span class="mf">0.4976</span> <span class="p">(</span><span class="n">theory</span><span class="p">:</span> <span class="mf">0.5000</span><span class="p">)</span>
<span class="n">Sample</span> <span class="n">variance</span><span class="p">:</span> <span class="mf">0.2467</span> <span class="p">(</span><span class="n">theory</span><span class="p">:</span> <span class="mf">0.2500</span><span class="p">)</span>
<span class="n">Sample</span> <span class="n">median</span><span class="p">:</span> <span class="mf">0.3441</span> <span class="p">(</span><span class="n">theory</span><span class="p">:</span> <span class="mf">0.3466</span><span class="p">)</span>
</pre></div>
</div>
<p>The sample statistics closely match theoretical values, confirming the correctness of our sampler.</p>
</div>
</section>
<section id="the-weibull-distribution">
<h3>The Weibull Distribution<a class="headerlink" href="#the-weibull-distribution" title="Link to this heading"></a></h3>
<p>The Weibull distribution generalizes the exponential to allow for increasing or decreasing hazard rates, making it fundamental in reliability analysis.</p>
<p><strong>Setup</strong>: <span class="math notranslate nohighlight">\(X \sim \text{Weibull}(k, \lambda)\)</span> with shape <span class="math notranslate nohighlight">\(k &gt; 0\)</span> and scale <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span>.</p>
<p><strong>CDF</strong>:</p>
<div class="math notranslate nohighlight">
\[F(x) = 1 - \exp\left( -\left(\frac{x}{\lambda}\right)^k \right), \quad x \geq 0\]</div>
<p><strong>Deriving the Inverse</strong>: Solve <span class="math notranslate nohighlight">\(F(x) = u\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}1 - e^{-(x/\lambda)^k} &amp;= u \\
e^{-(x/\lambda)^k} &amp;= 1 - u \\
-\left(\frac{x}{\lambda}\right)^k &amp;= \ln(1 - u) \\
\left(\frac{x}{\lambda}\right)^k &amp;= -\ln(1 - u) \\
x &amp;= \lambda \left( -\ln(1 - u) \right)^{1/k}\end{split}\]</div>
<p><strong>Result</strong>:</p>
<div class="math notranslate nohighlight" id="equation-weibull-inverse">
<span class="eqno">(9)<a class="headerlink" href="#equation-weibull-inverse" title="Link to this equation"></a></span>\[F^{-1}(u) = \lambda \left( -\ln(1 - u) \right)^{1/k}\]</div>
<p>Note that when <span class="math notranslate nohighlight">\(k = 1\)</span>, the Weibull reduces to <span class="math notranslate nohighlight">\(\text{Exponential}(1/\lambda)\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">weibull_inverse_cdf</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate Weibull(shape, scale) samples via inverse CDF.</span>

<span class="sd">    Uses X = scale * (-log(U))^(1/shape), the tail-stable form</span>
<span class="sd">    that avoids computing (1-U).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">)</span>  <span class="c1"># Guard against log(0)</span>
    <span class="k">return</span> <span class="n">scale</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">u</span><span class="p">))</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">shape</span><span class="p">)</span>

<span class="c1"># Example: Weibull(k=2, λ=1) - the Rayleigh distribution</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="mi">10_000</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">weibull_inverse_cdf</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample std: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="the-pareto-distribution">
<h3>The Pareto Distribution<a class="headerlink" href="#the-pareto-distribution" title="Link to this heading"></a></h3>
<p>The Pareto distribution models phenomena with “heavy tails”—situations where extreme values are more likely than a normal or exponential distribution would suggest. It appears in economics (income distribution), insurance (claim sizes), and network science (degree distributions).</p>
<p><strong>Setup</strong>: <span class="math notranslate nohighlight">\(X \sim \text{Pareto}(\alpha, x_m)\)</span> with shape <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span> and scale (minimum) <span class="math notranslate nohighlight">\(x_m &gt; 0\)</span>.</p>
<p><strong>CDF</strong>:</p>
<div class="math notranslate nohighlight">
\[F(x) = 1 - \left( \frac{x_m}{x} \right)^\alpha, \quad x \geq x_m\]</div>
<p><strong>Deriving the Inverse</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}1 - \left( \frac{x_m}{x} \right)^\alpha &amp;= u \\
\left( \frac{x_m}{x} \right)^\alpha &amp;= 1 - u \\
\frac{x_m}{x} &amp;= (1 - u)^{1/\alpha} \\
x &amp;= \frac{x_m}{(1 - u)^{1/\alpha}}\end{split}\]</div>
<p><strong>Result</strong>:</p>
<div class="math notranslate nohighlight" id="equation-pareto-inverse">
<span class="eqno">(10)<a class="headerlink" href="#equation-pareto-inverse" title="Link to this equation"></a></span>\[F^{-1}(u) = \frac{x_m}{(1 - u)^{1/\alpha}} = x_m (1 - u)^{-1/\alpha}\]</div>
<p>Since <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(1-U\)</span> have the same distribution, the tail-stable implementation uses:</p>
<div class="math notranslate nohighlight">
\[X = x_m \cdot U^{-1/\alpha}\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">pareto_inverse_cdf</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">x_min</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate Pareto(alpha, x_min) samples via inverse CDF.</span>

<span class="sd">    Uses the tail-stable form X = x_min * U^(-1/alpha),</span>
<span class="sd">    which avoids forming (1-U) and provides better resolution</span>
<span class="sd">    for large X values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="c1"># Guard against u=0 which would give infinity</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_min</span> <span class="o">*</span> <span class="n">u</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">/</span> <span class="n">alpha</span><span class="p">)</span>

<span class="c1"># Example: Pareto(α=2.5, x_m=1)</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="mi">10_000</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pareto_inverse_cdf</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">x_min</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># For α &gt; 1, mean = α * x_m / (α - 1)</span>
<span class="n">theoretical_mean</span> <span class="o">=</span> <span class="mf">2.5</span> <span class="o">*</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">2.5</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="n">theoretical_mean</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="the-cauchy-distribution">
<h3>The Cauchy Distribution<a class="headerlink" href="#the-cauchy-distribution" title="Link to this heading"></a></h3>
<p>The Cauchy distribution is infamous for having no mean or variance—its tails are so heavy that these moments do not exist. It arises as the ratio of two independent standard normals and appears in physics as the Lorentzian distribution.</p>
<p><strong>Setup</strong>: <span class="math notranslate nohighlight">\(X \sim \text{Cauchy}(\mu, \sigma)\)</span> with location <span class="math notranslate nohighlight">\(\mu\)</span> and scale <span class="math notranslate nohighlight">\(\sigma &gt; 0\)</span>.</p>
<p><strong>CDF</strong>:</p>
<div class="math notranslate nohighlight">
\[F(x) = \frac{1}{\pi} \arctan\left( \frac{x - \mu}{\sigma} \right) + \frac{1}{2}\]</div>
<p><strong>Deriving the Inverse</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{1}{\pi} \arctan\left( \frac{x - \mu}{\sigma} \right) + \frac{1}{2} &amp;= u \\
\arctan\left( \frac{x - \mu}{\sigma} \right) &amp;= \pi \left( u - \frac{1}{2} \right) \\
\frac{x - \mu}{\sigma} &amp;= \tan\left( \pi \left( u - \frac{1}{2} \right) \right) \\
x &amp;= \mu + \sigma \tan\left( \pi \left( u - \frac{1}{2} \right) \right)\end{split}\]</div>
<p><strong>Result</strong>:</p>
<div class="math notranslate nohighlight" id="equation-cauchy-inverse">
<span class="eqno">(11)<a class="headerlink" href="#equation-cauchy-inverse" title="Link to this equation"></a></span>\[F^{-1}(u) = \mu + \sigma \tan\left( \pi (u - 1/2) \right)\]</div>
<p><strong>Note</strong>: At <span class="math notranslate nohighlight">\(u = 0\)</span> and <span class="math notranslate nohighlight">\(u = 1\)</span>, the tangent function produces <span class="math notranslate nohighlight">\(\pm\infty\)</span>, which is mathematically correct since the Cauchy distribution has infinite support.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">cauchy_inverse_cdf</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate Cauchy(loc, scale) samples via inverse CDF.</span>

<span class="sd">    Note: Returns ±inf at u=0 and u=1, which is mathematically correct</span>
<span class="sd">    since the Cauchy has infinite support.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">loc</span> <span class="o">+</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">tan</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="n">u</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">))</span>

<span class="c1"># Standard Cauchy</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="mi">10_000</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">cauchy_inverse_cdf</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>

<span class="c1"># The median is the location parameter</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample median: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: 0.0)&quot;</span><span class="p">)</span>
<span class="c1"># Mean doesn&#39;t exist, but sample mean will be highly variable</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (undefined theoretically)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="the-logistic-distribution">
<h3>The Logistic Distribution<a class="headerlink" href="#the-logistic-distribution" title="Link to this heading"></a></h3>
<p>The logistic distribution resembles the normal but has heavier tails. It appears in logistic regression and as a smooth approximation to the step function.</p>
<p><strong>Setup</strong>: <span class="math notranslate nohighlight">\(X \sim \text{Logistic}(\mu, s)\)</span> with location <span class="math notranslate nohighlight">\(\mu\)</span> and scale <span class="math notranslate nohighlight">\(s &gt; 0\)</span>.</p>
<p><strong>CDF</strong>:</p>
<div class="math notranslate nohighlight">
\[F(x) = \frac{1}{1 + e^{-(x-\mu)/s}}\]</div>
<p><strong>Deriving the Inverse</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{1}{1 + e^{-(x-\mu)/s}} &amp;= u \\
1 + e^{-(x-\mu)/s} &amp;= \frac{1}{u} \\
e^{-(x-\mu)/s} &amp;= \frac{1-u}{u} \\
-(x-\mu)/s &amp;= \ln\left( \frac{1-u}{u} \right) \\
x &amp;= \mu - s \ln\left( \frac{1-u}{u} \right) = \mu + s \ln\left( \frac{u}{1-u} \right)\end{split}\]</div>
<p><strong>Result</strong>:</p>
<div class="math notranslate nohighlight" id="equation-logistic-inverse">
<span class="eqno">(12)<a class="headerlink" href="#equation-logistic-inverse" title="Link to this equation"></a></span>\[F^{-1}(u) = \mu + s \ln\left( \frac{u}{1-u} \right) = \mu + s \cdot \text{logit}(u)\]</div>
<p>The function <span class="math notranslate nohighlight">\(\text{logit}(u) = \ln(u/(1-u))\)</span> is the inverse of the logistic (sigmoid) function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">logistic_inverse_cdf</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate Logistic(loc, scale) samples via inverse CDF.</span>

<span class="sd">    Uses log(u) - log1p(-u) for numerical stability at both tails.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="c1"># Clamp to avoid exact 0 or 1</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
    <span class="c1"># Stable logit: log(u) - log(1-u) via log1p</span>
    <span class="k">return</span> <span class="n">loc</span> <span class="o">+</span> <span class="n">scale</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">u</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="o">-</span><span class="n">u</span><span class="p">))</span>

<span class="c1"># Standard logistic</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="mi">10_000</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">logistic_inverse_cdf</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>

<span class="c1"># Mean = loc, Variance = (π * scale)² / 3</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: 0.0)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">**</span><span class="mi">2</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">3</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="summary-continuous-distributions">
<h3>Summary: Continuous Distributions<a class="headerlink" href="#summary-continuous-distributions" title="Link to this heading"></a></h3>
<p>The following table summarizes the inverse CDF formulas for common distributions:</p>
<table class="docutils align-default" id="id4">
<caption><span class="caption-number">Table 16 </span><span class="caption-text">Inverse CDF Formulas for Continuous Distributions</span><a class="headerlink" href="#id4" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 35.0%" />
<col style="width: 45.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Distribution</p></th>
<th class="head"><p>CDF <span class="math notranslate nohighlight">\(F(x)\)</span></p></th>
<th class="head"><p>Inverse <span class="math notranslate nohighlight">\(F^{-1}(u)\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Exponential(<span class="math notranslate nohighlight">\(\lambda\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(1 - e^{-\lambda x}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\ln(1-u)/\lambda\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Weibull(<span class="math notranslate nohighlight">\(k, \lambda\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(1 - e^{-(x/\lambda)^k}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\lambda(-\ln(1-u))^{1/k}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Pareto(<span class="math notranslate nohighlight">\(\alpha, x_m\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(1 - (x_m/x)^\alpha\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_m (1-u)^{-1/\alpha}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Cauchy(<span class="math notranslate nohighlight">\(\mu, \sigma\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{1}{\pi}\arctan(\frac{x-\mu}{\sigma}) + \frac{1}{2}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu + \sigma\tan(\pi(u-1/2))\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Logistic(<span class="math notranslate nohighlight">\(\mu, s\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(1/(1 + e^{-(x-\mu)/s})\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu + s\ln(u/(1-u))\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Uniform(<span class="math notranslate nohighlight">\(a, b\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\((x-a)/(b-a)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(a + (b-a)u\)</span></p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="numerical-inversion">
<h2>Numerical Inversion<a class="headerlink" href="#numerical-inversion" title="Link to this heading"></a></h2>
<p>What about distributions whose inverse CDFs have no closed form? The normal distribution is the most important example: <span class="math notranslate nohighlight">\(\Phi^{-1}(u)\)</span> cannot be expressed in terms of elementary functions. For such distributions, we have two options:</p>
<ol class="arabic simple">
<li><p><strong>Numerical root-finding</strong>: Solve <span class="math notranslate nohighlight">\(F(x) = u\)</span> numerically for each sample.</p></li>
<li><p><strong>Use specialized algorithms</strong>: Box-Muller for normals, rejection sampling, etc.</p></li>
</ol>
<p>Numerical inversion is always possible but often slow. Let us examine when it is practical.</p>
<section id="root-finding-approach">
<h3>Root-Finding Approach<a class="headerlink" href="#root-finding-approach" title="Link to this heading"></a></h3>
<p>To compute <span class="math notranslate nohighlight">\(F^{-1}(u)\)</span>, we solve the equation <span class="math notranslate nohighlight">\(F(x) - u = 0\)</span>. Since <span class="math notranslate nohighlight">\(F\)</span> is monotonically increasing, standard root-finding methods are guaranteed to converge.</p>
<p><strong>Brent’s method</strong> is particularly suitable: it combines bisection (guaranteed convergence) with faster interpolation methods (speed). SciPy provides this as <code class="docutils literal notranslate"><span class="pre">scipy.optimize.brentq</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">optimize</span><span class="p">,</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">numerical_inverse_cdf</span><span class="p">(</span><span class="n">cdf_func</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">bracket</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute F⁻¹(u) numerically using Brent&#39;s method.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    cdf_func : callable</span>
<span class="sd">        CDF function F(x).</span>
<span class="sd">    u : float or array</span>
<span class="sd">        Probability value(s) in (0, 1).</span>
<span class="sd">    bracket : tuple</span>
<span class="sd">        Interval [a, b] containing the root.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    float or array</span>
<span class="sd">        Quantile value(s).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">u_val</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">u</span><span class="p">):</span>
        <span class="c1"># Solve F(x) = u, i.e., F(x) - u = 0</span>
        <span class="n">result</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">optimize</span><span class="o">.</span><span class="n">brentq</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">cdf_func</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">u_val</span><span class="p">,</span>
            <span class="n">bracket</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">bracket</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">result</span>

<span class="c1"># Example: Normal distribution (for comparison with scipy.stats.norm.ppf)</span>
<span class="n">normal_cdf</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span>

<span class="n">u_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.975</span><span class="p">])</span>
<span class="n">numerical_quantiles</span> <span class="o">=</span> <span class="n">numerical_inverse_cdf</span><span class="p">(</span><span class="n">normal_cdf</span><span class="p">,</span> <span class="n">u_test</span><span class="p">,</span> <span class="n">bracket</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">exact_quantiles</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">u_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Comparison of numerical vs. exact normal quantiles:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">u</span><span class="p">,</span> <span class="n">num</span><span class="p">,</span> <span class="n">exact</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">u_test</span><span class="p">,</span> <span class="n">numerical_quantiles</span><span class="p">,</span> <span class="n">exact_quantiles</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  F⁻¹(</span><span class="si">{</span><span class="n">u</span><span class="si">}</span><span class="s2">) = </span><span class="si">{</span><span class="n">num</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (numerical) vs </span><span class="si">{</span><span class="n">exact</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (exact)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Computational cost</strong>: Brent’s method typically requires <span class="math notranslate nohighlight">\(O(\log(1/\epsilon))\)</span> iterations to achieve tolerance <span class="math notranslate nohighlight">\(\epsilon\)</span>, with each iteration evaluating the CDF once. For normal samples, this is about 10-20 CDF evaluations per sample—much slower than the specialized methods we’ll see later.</p>
</section>
<section id="when-numerical-inversion-makes-sense">
<h3>When Numerical Inversion Makes Sense<a class="headerlink" href="#when-numerical-inversion-makes-sense" title="Link to this heading"></a></h3>
<p>Numerical inversion is practical when:</p>
<ol class="arabic simple">
<li><p><strong>You need only a few samples</strong>: Setup costs of specialized methods may dominate.</p></li>
<li><p><strong>The distribution is unusual</strong>: Custom distributions without known fast samplers.</p></li>
<li><p><strong>Accuracy is paramount</strong>: Numerical inversion can achieve arbitrary precision.</p></li>
</ol>
<p>Numerical inversion is impractical when:</p>
<ol class="arabic simple">
<li><p><strong>You need many samples</strong>: The per-sample cost adds up.</p></li>
<li><p><strong>A fast specialized method exists</strong>: Box-Muller for normals, ratio-of-uniforms for gammas.</p></li>
<li><p><strong>The CDF is expensive to evaluate</strong>: Each sample requires multiple CDF calls.</p></li>
</ol>
<p><strong>Rule of thumb</strong>: For standard distributions, use library implementations (<code class="docutils literal notranslate"><span class="pre">scipy.stats</span></code>, NumPy). These use the most efficient method known for each distribution. Reserve numerical inversion for custom or unusual distributions when other methods fail.</p>
</section>
</section>
<section id="discrete-distributions">
<h2>Discrete Distributions<a class="headerlink" href="#discrete-distributions" title="Link to this heading"></a></h2>
<p>For discrete distributions taking values <span class="math notranslate nohighlight">\(x_1 &lt; x_2 &lt; \cdots &lt; x_K\)</span> with probabilities <span class="math notranslate nohighlight">\(p_1, p_2, \ldots, p_K\)</span>, the inverse CDF method still applies. The CDF is now a step function:</p>
<div class="math notranslate nohighlight">
\[F(x) = \sum_{x_i \leq x} p_i\]</div>
<p>and the generalized inverse sets <span class="math notranslate nohighlight">\(X = x_k\)</span> when <span class="math notranslate nohighlight">\(F(x_{k-1}) &lt; U \leq F(x_k)\)</span>, where <span class="math notranslate nohighlight">\(F(x_0) = 0\)</span>.</p>
<p>The challenge is computational: <strong>how do we efficiently find which “step” of the CDF our uniform value :math:`U` lands on?</strong></p>
<p>We present five algorithms with different complexity trade-offs, beginning with the simplest and progressing to constant-time methods:</p>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_3_fig02_discrete_sampling.png"><img alt="Comparison of linear search, binary search, and alias method for discrete sampling" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_3_fig02_discrete_sampling.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 47 </span><span class="caption-text"><strong>Discrete Sampling Algorithms: Overview.</strong> Top row: Visual illustration of the three fundamental approaches. Linear search scans sequentially until cumulative probability exceeds <span class="math notranslate nohighlight">\(U\)</span>. Binary search bisects the CDF in <span class="math notranslate nohighlight">\(O(\log K)\)</span> steps. The alias method constructs equal-height columns allowing <span class="math notranslate nohighlight">\(O(1)\)</span> lookup. Bottom row: Practical performance analysis for <span class="math notranslate nohighlight">\(K=1000\)</span> categories, showing when each method dominates. Additional algorithms (interpolation search, exponential doubling) are discussed below.</span><a class="headerlink" href="#id5" title="Link to this image"></a></p>
</figcaption>
</figure>
<section id="linear-search-the-baseline">
<h3>Linear Search: The Baseline<a class="headerlink" href="#linear-search-the-baseline" title="Link to this heading"></a></h3>
<p>We begin with the most natural approach: <strong>linear search</strong>. If you were asked to sample from a discrete distribution using pencil and paper, this is almost certainly what you would do. Walk through the outcomes in order, accumulating probability as you go, and stop as soon as the cumulative probability exceeds your uniform random number <span class="math notranslate nohighlight">\(U\)</span>.</p>
<p>Linear search embodies the generalized inverse directly. Recall that <span class="math notranslate nohighlight">\(F^{-1}(u) = \inf\{k : F(k) \geq u\}\)</span>—the smallest outcome whose cumulative probability reaches <span class="math notranslate nohighlight">\(u\)</span>. Linear search finds this by brute force: compute <span class="math notranslate nohighlight">\(F(1)\)</span>, check if <span class="math notranslate nohighlight">\(U \leq F(1)\)</span>; if not, compute <span class="math notranslate nohighlight">\(F(2)\)</span>, check again; and so on. The algorithm is essentially a sequential scan through the CDF step function until we “climb” past the horizontal line at height <span class="math notranslate nohighlight">\(U\)</span>.</p>
<p>The simplicity of linear search is both its strength and its weakness. On one hand, the algorithm requires no preprocessing beyond normalizing the PMF—no cumulative sums need be stored, no auxiliary tables constructed. For very small <span class="math notranslate nohighlight">\(K\)</span> (say, a six-sided die), this simplicity makes linear search the practical choice. On the other hand, the worst-case complexity is <span class="math notranslate nohighlight">\(O(K)\)</span>: if the distribution is uniform and <span class="math notranslate nohighlight">\(U\)</span> happens to fall in the last bin, we must examine every outcome before terminating.</p>
<p>But here is a subtlety often overlooked: <strong>linear search is not uniformly bad</strong>. Its performance depends critically on <em>where the probability mass lies</em> and <em>where our uniform sample lands</em>. Consider a geometric distribution where <span class="math notranslate nohighlight">\(p_1 = 0.5\)</span>, <span class="math notranslate nohighlight">\(p_2 = 0.25\)</span>, <span class="math notranslate nohighlight">\(p_3 = 0.125\)</span>, and so on. Half of all samples terminate on the first iteration; three-quarters terminate by the second. The <em>expected</em> number of comparisons is <span class="math notranslate nohighlight">\(\sum_k k \cdot p_k\)</span>—the distribution’s mean! For head-heavy distributions, this expected cost can be far smaller than <span class="math notranslate nohighlight">\(O(\log K)\)</span>.</p>
<p>This observation leads to a practical insight: <strong>if you know the distribution is head-heavy, arrange outcomes in decreasing probability order and use linear search</strong>. Conversely, for tail-heavy distributions, scanning from right to left achieves the same benefit. The catch is that you must know the distribution’s shape in advance, and for symmetric or multimodal distributions, no ordering helps.</p>
<p>Linear search also has a pedagogical virtue: it makes the inverse CDF method’s logic transparent. Watching the cumulative sum grow step by step, one sees exactly how the CDF partitions <span class="math notranslate nohighlight">\([0, 1]\)</span> into intervals corresponding to each outcome. This intuition is worth developing before moving to more sophisticated algorithms.</p>
<p><strong>Algorithm: Linear Search</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input: Probabilities (p₁, ..., pₖ), uniform value U
Output: Sample X

1. Set cumsum = 0
2. For k = 1, 2, ..., K:
   a. cumsum = cumsum + pₖ
   b. If U ≤ cumsum, return xₖ
3. Return xₖ (fallback for numerical edge cases)
</pre></div>
</div>
<p><strong>Complexity</strong>: <span class="math notranslate nohighlight">\(O(K)\)</span> per sample in the worst case (uniform distribution), <span class="math notranslate nohighlight">\(O(1)\)</span> in the best case (all mass on <span class="math notranslate nohighlight">\(x_1\)</span>).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">linear_search_sample</span><span class="p">(</span><span class="n">pmf</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">u</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sample from discrete distribution via linear search.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    pmf : array</span>
<span class="sd">        Probability mass function (must sum to 1).</span>
<span class="sd">    values : array</span>
<span class="sd">        Possible outcomes.</span>
<span class="sd">    u : float</span>
<span class="sd">        Uniform random value.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    Sampled value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cumsum</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">pmf</span><span class="p">):</span>
        <span class="n">cumsum</span> <span class="o">+=</span> <span class="n">p</span>
        <span class="k">if</span> <span class="n">u</span> <span class="o">&lt;=</span> <span class="n">cumsum</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">values</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">values</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Fallback</span>

<span class="c1"># Example: biased die</span>
<span class="n">pmf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span>  <span class="c1"># Heavy on 5, 6</span>
<span class="n">values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="p">[</span><span class="n">linear_search_sample</span><span class="p">(</span><span class="n">pmf</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">())</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sample frequencies:&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">minlength</span><span class="o">=</span><span class="mi">7</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">/</span> <span class="mi">10000</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True probabilities:&quot;</span><span class="p">,</span> <span class="n">pmf</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="binary-search-logarithmic-time-with-worst-case-guarantees">
<h3>Binary Search: Logarithmic Time with Worst-Case Guarantees<a class="headerlink" href="#binary-search-logarithmic-time-with-worst-case-guarantees" title="Link to this heading"></a></h3>
<p>Linear search’s <span class="math notranslate nohighlight">\(O(K)\)</span> worst case becomes unacceptable when <span class="math notranslate nohighlight">\(K\)</span> grows large. Sampling from a distribution over 10,000 outcomes should not require 10,000 comparisons. We need a smarter strategy.</p>
<p>The key observation is that the cumulative probabilities <span class="math notranslate nohighlight">\(F_1 \leq F_2 \leq \cdots \leq F_K\)</span> form a <em>sorted</em> array. Finding the smallest <span class="math notranslate nohighlight">\(k\)</span> such that <span class="math notranslate nohighlight">\(F_k \geq U\)</span> is exactly the <strong>lower bound</strong> problem from computer science—and binary search solves it in <span class="math notranslate nohighlight">\(O(\log K)\)</span> time.</p>
<p>The algorithm maintains an interval <span class="math notranslate nohighlight">\([\text{low}, \text{high}]\)</span> guaranteed to contain the answer. Initially, this is <span class="math notranslate nohighlight">\([1, K]\)</span>—we know nothing, so the answer could be anywhere. At each step, we probe the midpoint <span class="math notranslate nohighlight">\(\text{mid} = \lfloor(\text{low} + \text{high})/2\rfloor\)</span> and compare <span class="math notranslate nohighlight">\(U\)</span> to <span class="math notranslate nohighlight">\(F_{\text{mid}}\)</span>. If <span class="math notranslate nohighlight">\(U \leq F_{\text{mid}}\)</span>, the answer is at or before the midpoint—so we shrink to <span class="math notranslate nohighlight">\([\text{low}, \text{mid}]\)</span>. Otherwise, the answer is strictly after the midpoint—so we shrink to <span class="math notranslate nohighlight">\([\text{mid}+1, \text{high}]\)</span>. Either way, the interval halves in size.</p>
<p>This halving is the source of binary search’s power. After one comparison, the interval has at most <span class="math notranslate nohighlight">\(K/2\)</span> elements; after two, at most <span class="math notranslate nohighlight">\(K/4\)</span>; after <span class="math notranslate nohighlight">\(\lceil \log_2 K \rceil\)</span> comparisons, the interval has collapsed to a single element—our answer. For <span class="math notranslate nohighlight">\(K = 1000\)</span>, this means at most 10 comparisons instead of potentially 1000.</p>
<p><strong>The Minimax Perspective</strong></p>
<p>Binary search has a deeper elegance when viewed through the lens of game theory. Imagine an adversary who knows your algorithm and chooses <span class="math notranslate nohighlight">\(U\)</span> to maximize your work. Against linear search, the adversary places <span class="math notranslate nohighlight">\(U\)</span> in the last bin, forcing <span class="math notranslate nohighlight">\(K\)</span> comparisons. Against binary search, no matter where the adversary places <span class="math notranslate nohighlight">\(U\)</span>, you complete in <span class="math notranslate nohighlight">\(\lceil \log_2 K \rceil\)</span> steps.</p>
<p>Binary search thus <strong>minimizes the maximum</strong> possible cost—a <em>minimax</em> strategy. This guarantee comes at a price: binary search cannot exploit favorable distributions. Even if 99% of the probability mass sits in the first bin, binary search still checks the middle first. Where linear search achieves <span class="math notranslate nohighlight">\(O(1)\)</span> expected time on head-heavy distributions, binary search stubbornly insists on <span class="math notranslate nohighlight">\(O(\log K)\)</span>.</p>
<p>This is the fundamental tradeoff: <strong>linear search gambles, binary search plays it safe</strong>. Linear search can win big (best case <span class="math notranslate nohighlight">\(O(1)\)</span>) or lose big (worst case <span class="math notranslate nohighlight">\(O(K)\)</span>). Binary search guarantees you’ll never do worse than <span class="math notranslate nohighlight">\(O(\log K)\)</span>, but you also can’t do better. For general-purpose discrete sampling where the distribution shape may be unknown or arbitrary, binary search’s robustness makes it the default choice.</p>
<p><strong>The Search Invariant</strong></p>
<p>Understanding binary search requires internalizing its <strong>invariant</strong>: <em>at every iteration, the true answer lies in</em> <span class="math notranslate nohighlight">\([\text{low}, \text{high}]\)</span>. The proof is inductive. Initially, the answer is certainly in <span class="math notranslate nohighlight">\([1, K]\)</span>. At each step, we eliminate either the upper or lower half of the interval—but only after verifying that the answer is not in the eliminated half. When the interval collapses to a single point, that point must be the answer.</p>
<p>This invariant is why binary search works correctly for discrete distributions despite the CDF being a step function. We are not searching for a point where <span class="math notranslate nohighlight">\(F(k) = U\)</span> (which may not exist); we are searching for the <em>transition point</em> where <span class="math notranslate nohighlight">\(F\)</span> first reaches or exceeds <span class="math notranslate nohighlight">\(U\)</span>. The comparison <span class="math notranslate nohighlight">\(U \leq F_{\text{mid}}\)</span> correctly identifies which half contains this transition.</p>
<p><strong>Implementation via np.searchsorted</strong></p>
<p>NumPy provides <code class="docutils literal notranslate"><span class="pre">np.searchsorted</span></code>, a highly optimized binary search implementation. The call <code class="docutils literal notranslate"><span class="pre">np.searchsorted(cdf,</span> <span class="pre">u,</span> <span class="pre">side='left')</span></code> returns the smallest index <span class="math notranslate nohighlight">\(k\)</span> such that <code class="docutils literal notranslate"><span class="pre">cdf[k]</span> <span class="pre">&gt;=</span> <span class="pre">u</span></code>—precisely the generalized inverse. This vectorized function handles arrays of <span class="math notranslate nohighlight">\(U\)</span> values efficiently, making it the practical workhorse for discrete sampling in Python.</p>
<figure class="align-center" id="id6">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide013_img001_50c6995d.png"><img alt="Binary search tree for discrete distribution sampling" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide013_img001_50c6995d.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 48 </span><span class="caption-text"><strong>Binary Search for Discrete Sampling.</strong> For a U-shaped distribution over <span class="math notranslate nohighlight">\(K = 16\)</span> categories, the binary search tree (center) organizes cumulative probabilities at each node. Left panel: The CDF as a step function, with horizontal lines at key thresholds. Right panel: A complete search trace for <span class="math notranslate nohighlight">\(U = 0.885\)</span>. Starting at the root (<span class="math notranslate nohighlight">\(k=8, F[8]=0.500\)</span>), we go right (since <span class="math notranslate nohighlight">\(U &gt; 0.500\)</span>), then left, then right, then left, arriving at <span class="math notranslate nohighlight">\(k=11\)</span> in 4 steps. The algorithm pseudocode (left box) shows the standard binary search pattern.</span><a class="headerlink" href="#id6" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Algorithm: Binary Search</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input: Cumulative probabilities (F₁, ..., Fₖ), uniform value U
Output: Sample index k

1. Set lo = 0, hi = K
2. While lo &lt; hi:
   a. mid = (lo + hi) / 2
   b. If U ≤ F[mid], set hi = mid
   c. Else set lo = mid + 1
3. Return lo
</pre></div>
</div>
<p><strong>Complexity</strong>: <span class="math notranslate nohighlight">\(O(\log K)\)</span> per sample, <span class="math notranslate nohighlight">\(O(K)\)</span> preprocessing to compute cumulative sums.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">DiscreteDistributionBinarySearch</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Discrete distribution sampler using binary search.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pmf</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize with probability mass function.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        pmf : array</span>
<span class="sd">            Probabilities (will be normalized if needed).</span>
<span class="sd">        values : array, optional</span>
<span class="sd">            Outcome values. Defaults to 0, 1, ..., K-1.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pmf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">pmf</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pmf</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cdf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pmf</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">values</span> <span class="o">=</span> <span class="n">values</span> <span class="k">if</span> <span class="n">values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pmf</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pmf</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate samples using binary search.&quot;&quot;&quot;</span>
        <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

        <span class="c1"># np.searchsorted finds insertion point - exactly what we need</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">searchsorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cdf</span><span class="p">,</span> <span class="n">U</span><span class="p">,</span> <span class="n">side</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">)</span>

        <span class="c1"># Clip to valid range (handles floating-point edge cases where U &gt;= cdf[-1])</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>

<span class="c1"># Example: Zipf distribution (heavy-tailed)</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">s</span> <span class="o">=</span> <span class="mf">2.0</span>  <span class="c1"># Zipf exponent</span>
<span class="n">pmf</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">K</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="n">s</span>
<span class="n">pmf</span> <span class="o">/=</span> <span class="n">pmf</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="n">dist</span> <span class="o">=</span> <span class="n">DiscreteDistributionBinarySearch</span><span class="p">(</span><span class="n">pmf</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(X = 1) = </span><span class="si">{</span><span class="n">pmf</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, sample frequency = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samples</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(X = 2) = </span><span class="si">{</span><span class="n">pmf</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, sample frequency = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samples</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>NumPy’s <code class="docutils literal notranslate"><span class="pre">np.searchsorted</span></code> implements binary search efficiently, making this approach practical for distributions with thousands or millions of outcomes.</p>
<div class="tip admonition">
<p class="admonition-title">Try It Yourself 🖥️ Binary Search for Discrete Sampling</p>
<p>An interactive simulation lets you explore binary search step-by-step and compare it with linear scan:</p>
<p><a class="reference external" href="https://treese41528.github.io/ComputationalDataScience/Simulations/MonteCarloSimulation/binary_search_discrete_sampling.html">https://treese41528.github.io/ComputationalDataScience/Simulations/MonteCarloSimulation/binary_search_discrete_sampling.html</a></p>
<p>This simulation provides three synchronized views of the binary search algorithm:</p>
<ul class="simple">
<li><p><strong>Left panel</strong>: Shows the PMF shape, CDF table (with row highlighting), and method comparison after search completes.</p></li>
<li><p><strong>Center panel</strong>: Displays the CDF step function with the <span class="math notranslate nohighlight">\(U\)</span> threshold line, plus a binary search tree showing the path taken through nodes.</p></li>
<li><p><strong>Right panel</strong>: Provides a step-by-step search trace with the algorithm state (low, high, mid) and explanations of the search invariant.</p></li>
</ul>
<p><strong>Key features to explore</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Distribution selector</strong>: Choose from 10+ distributions including Unimodal (bell-shaped), U-shaped (bathtub), Bimodal, Head-heavy (geometric), Tail-heavy, Poisson, Zipf’s law, Point mass, and Adversarial cases.</p></li>
<li><p><strong>Step-by-step mode</strong>: Click “Step →” to walk through each comparison manually, watching how the search interval halves each time.</p></li>
<li><p><strong>Auto-play mode</strong>: Watch the algorithm execute automatically with 1.2-second delays between steps.</p></li>
<li><p><strong>Method comparison</strong>: After each search completes, see how many steps binary search took versus linear scan (left-to-right and right-to-left).</p></li>
</ol>
<p><strong>Key observations to make</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Minimax guarantee</strong>: Binary search always completes in at most <span class="math notranslate nohighlight">\(\lceil \log_2 K \rceil = 4\)</span> steps for <span class="math notranslate nohighlight">\(K = 16\)</span>, regardless of the distribution shape or where <span class="math notranslate nohighlight">\(U\)</span> falls.</p></li>
<li><p><strong>Shape independence</strong>: Try the <em>Adversarial</em> distribution (99% at <span class="math notranslate nohighlight">\(k=16\)</span>). Linear scan left-to-right takes 16 steps, but binary search still takes only 4.</p></li>
<li><p><strong>The invariant</strong>: Watch the “Answer ∈ [low, high]” interval shrink by half each step—this is the core insight of binary search.</p></li>
<li><p><strong>Tree visualization</strong>: The search path through the tree (red nodes) shows exactly which comparisons were made. The tree structure is fixed regardless of the distribution.</p></li>
<li><p><strong>When linear wins</strong>: Try <em>Head-heavy (geometric)</em>. If <span class="math notranslate nohighlight">\(U\)</span> happens to be small, linear scan finds the answer in 1–2 steps while binary search still takes 3–4. This illustrates the minimax tradeoff: binary search sacrifices best-case performance for worst-case guarantees.</p></li>
</ol>
</div>
</section>
<section id="interpolation-search-exploiting-distributional-structure">
<h3>Interpolation Search: Exploiting Distributional Structure<a class="headerlink" href="#interpolation-search-exploiting-distributional-structure" title="Link to this heading"></a></h3>
<p>Binary search is a <em>black-box</em> algorithm: it treats the CDF as an opaque sorted array, ignoring everything about its shape. But what if we <em>know</em> something about the distribution? Can we search faster by making informed guesses about where the answer lies?</p>
<p>Consider looking up a name in a physical phone book (for those who remember such artifacts). You would not open to the exact middle and binary search from there. If searching for “Williams,” you would open near the end; for “Anderson,” near the beginning. You exploit your knowledge that names are roughly uniformly distributed through the alphabet to make an educated guess.</p>
<p><strong>Interpolation search</strong> applies this intuition to discrete sampling. Instead of always probing the midpoint, it estimates where in the array the target value <span class="math notranslate nohighlight">\(U\)</span> <em>should</em> be, based on linear interpolation between the CDF values at the interval endpoints:</p>
<div class="math notranslate nohighlight">
\[\text{guess} = \text{low} + \left\lfloor \frac{U - F_{\text{low}-1}}{F_{\text{high}} - F_{\text{low}-1}} \cdot (\text{high} - \text{low}) \right\rfloor\]</div>
<p>The formula asks: “What fraction of the probability range does <span class="math notranslate nohighlight">\(U\)</span> represent? Map that fraction to the index range.” If the CDF is perfectly linear (uniform distribution), this guess is exact, and we find the answer in one step!</p>
<p>For near-uniform distributions, interpolation search achieves <span class="math notranslate nohighlight">\(O(\log \log K)\)</span> expected comparisons—a dramatic improvement over binary search’s <span class="math notranslate nohighlight">\(O(\log K)\)</span>. For <span class="math notranslate nohighlight">\(K = 10^6\)</span>, this means roughly 4 comparisons instead of 20. The improvement compounds because each guess narrows the interval <em>multiplicatively</em> rather than just halving it.</p>
<figure class="align-center" id="id7">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_3_fig05_interpolation_search_uniform.png"><img alt="Interpolation search for uniform distribution" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_3_fig05_interpolation_search_uniform.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 49 </span><span class="caption-text"><strong>Interpolation Search: Near-Uniform Distribution.</strong> For distributions where probability mass is spread relatively evenly, interpolation search uses linear interpolation to guess the target index. Here with <span class="math notranslate nohighlight">\(K = 16\)</span> categories and <span class="math notranslate nohighlight">\(u = 0.420\)</span>, the algorithm finds the answer in just 2 steps by exploiting the near-linear CDF structure.</span><a class="headerlink" href="#id7" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>The Key Insight</strong></p>
<p>Rather than always probing the middle of the search interval, interpolation search estimates <em>where</em> the target should be based on the CDF values at the interval endpoints:</p>
<div class="math notranslate nohighlight">
\[\text{guess} = \text{low} + \left\lfloor \frac{u - F[\text{low}-1]}{F[\text{high}] - F[\text{low}-1]} \cdot (\text{high} - \text{low}) \right\rfloor\]</div>
<p>This formula computes what fraction of the way through the probability range our target <span class="math notranslate nohighlight">\(u\)</span> lies, then maps that fraction to the index range.</p>
<p><strong>Algorithm: Interpolation Search</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input: Cumulative probabilities (F₁, ..., Fₖ), uniform value U
Output: Sample index k

1. Set lo = 1, hi = K, F[0] = 0
2. While lo &lt; hi:
   a. ratio = (U - F[lo-1]) / (F[hi] - F[lo-1])
   b. guess = lo + floor(ratio * (hi - lo))
   c. If U ≤ F[guess], set hi = guess
   d. Else set lo = guess + 1
3. Return lo
</pre></div>
</div>
<p><strong>The Danger of Skewed Distributions</strong></p>
<p>But here lies a trap. Interpolation search assumes the CDF is approximately linear, which is equivalent to assuming the distribution is approximately uniform. When this assumption fails—as it does spectacularly for Zipf, geometric, or any highly skewed distribution—the algorithm’s guesses become wildly wrong.</p>
<p>Consider a geometric distribution where <span class="math notranslate nohighlight">\(p_1 = 0.9\)</span>, <span class="math notranslate nohighlight">\(p_2 = 0.09\)</span>, etc. The CDF jumps to 0.9 at <span class="math notranslate nohighlight">\(k=1\)</span> and crawls toward 1 thereafter. If <span class="math notranslate nohighlight">\(U = 0.5\)</span>, interpolation assumes the answer should be around the middle of the range—but it’s actually at <span class="math notranslate nohighlight">\(k=1\)</span>! The first guess overshoots badly, and subsequent guesses may not recover quickly. In pathological cases, interpolation search degrades to <span class="math notranslate nohighlight">\(O(K)\)</span>, <em>worse</em> than binary search.</p>
<p><strong>When to Use Interpolation Search</strong></p>
<p>The decision is straightforward:</p>
<ul class="simple">
<li><p><strong>Use interpolation search</strong> when you <em>know</em> the distribution is close to uniform and need maximum speed. Examples include sampling from empirical distributions of naturally dispersed data.</p></li>
<li><p><strong>Avoid interpolation search</strong> when the distribution might be skewed, heavy-tailed, or have point masses. In these cases, binary search’s guaranteed <span class="math notranslate nohighlight">\(O(\log K)\)</span> is safer.</p></li>
<li><p><strong>Never use interpolation search</strong> as a general-purpose default. Its worst case is worse than linear search in some implementations, and its best case requires distributional assumptions that often don’t hold.</p></li>
</ul>
<p>The lesson is broader: algorithms that exploit structure are powerful when the structure is present and dangerous when it is not. Binary search’s “ignorance” of the CDF shape is actually a form of robustness.</p>
<figure class="align-center" id="id8">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_3_fig06_interpolation_search_skewed.png"><img alt="Interpolation search for monotonic distribution" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_3_fig06_interpolation_search_skewed.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 50 </span><span class="caption-text"><strong>Interpolation Search: Skewed Distribution.</strong> For a monotonically increasing PMF (more mass on higher categories), interpolation search requires 3 steps to find <span class="math notranslate nohighlight">\(k = 15\)</span> for <span class="math notranslate nohighlight">\(u = 0.780\)</span>. The linear interpolation estimates are less accurate when the CDF curves away from a straight line, but the algorithm still converges correctly.</span><a class="headerlink" href="#id8" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Complexity</strong>:</p>
<ul class="simple">
<li><p><strong>Best case</strong>: <span class="math notranslate nohighlight">\(O(\log \log K)\)</span> for uniform or near-uniform distributions</p></li>
<li><p><strong>Worst case</strong>: <span class="math notranslate nohighlight">\(O(K)\)</span> for highly skewed distributions (where the linear assumption fails badly)</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">DiscreteDistributionInterpolationSearch</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Discrete distribution sampler using interpolation search.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pmf</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize with probability mass function.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        pmf : array</span>
<span class="sd">            Probabilities (will be normalized if needed).</span>
<span class="sd">        values : array, optional</span>
<span class="sd">            Outcome values. Defaults to 0, 1, ..., K-1.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pmf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">pmf</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pmf</span><span class="p">)</span>
        <span class="c1"># Prepend 0 for F[0] = 0 convention</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cdf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pmf</span><span class="p">)])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">values</span> <span class="o">=</span> <span class="n">values</span> <span class="k">if</span> <span class="n">values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pmf</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pmf</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_interpolation_search</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">u</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Find smallest k such that F[k] &gt;= u using interpolation search.&quot;&quot;&quot;</span>
        <span class="n">lo</span><span class="p">,</span> <span class="n">hi</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span>

        <span class="k">while</span> <span class="n">lo</span> <span class="o">&lt;</span> <span class="n">hi</span><span class="p">:</span>
            <span class="c1"># Avoid division by zero</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cdf</span><span class="p">[</span><span class="n">hi</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cdf</span><span class="p">[</span><span class="n">lo</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]:</span>
                <span class="k">return</span> <span class="n">lo</span>

            <span class="c1"># Linear interpolation</span>
            <span class="n">ratio</span> <span class="o">=</span> <span class="p">(</span><span class="n">u</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">cdf</span><span class="p">[</span><span class="n">lo</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cdf</span><span class="p">[</span><span class="n">hi</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">cdf</span><span class="p">[</span><span class="n">lo</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span>
            <span class="n">guess</span> <span class="o">=</span> <span class="n">lo</span> <span class="o">+</span> <span class="nb">int</span><span class="p">(</span><span class="n">ratio</span> <span class="o">*</span> <span class="p">(</span><span class="n">hi</span> <span class="o">-</span> <span class="n">lo</span><span class="p">))</span>

            <span class="c1"># Clamp guess to valid range</span>
            <span class="n">guess</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">lo</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">guess</span><span class="p">,</span> <span class="n">hi</span><span class="p">))</span>

            <span class="k">if</span> <span class="n">u</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cdf</span><span class="p">[</span><span class="n">guess</span><span class="p">]:</span>
                <span class="n">hi</span> <span class="o">=</span> <span class="n">guess</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">lo</span> <span class="o">=</span> <span class="n">guess</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="k">return</span> <span class="n">lo</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate samples using interpolation search.&quot;&quot;&quot;</span>
        <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

        <span class="c1"># Interpolation search for each U value</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_interpolation_search</span><span class="p">(</span><span class="n">u</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="n">U</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>

<span class="c1"># Example: Near-uniform distribution (interpolation search excels)</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">pmf_uniform</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">K</span><span class="p">)</span> <span class="o">/</span> <span class="n">K</span>

<span class="n">dist_interp</span> <span class="o">=</span> <span class="n">DiscreteDistributionInterpolationSearch</span><span class="p">(</span><span class="n">pmf_uniform</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">dist_interp</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">10_000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="p">(</span><span class="n">K</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="tip admonition">
<p class="admonition-title">Try It Yourself 🖥️ Interpolation Search for Discrete Sampling</p>
<p>An interactive simulation lets you explore interpolation search step-by-step and compare it with binary search and linear scan:</p>
<p><a class="reference external" href="https://treese41528.github.io/ComputationalDataScience/Simulations/MonteCarloSimulation/interpolation_search_discrete_sampling.html">https://treese41528.github.io/ComputationalDataScience/Simulations/MonteCarloSimulation/interpolation_search_discrete_sampling.html</a></p>
<p>This simulation provides synchronized views of the interpolation search algorithm:</p>
<ul class="simple">
<li><p><strong>Left panel</strong>: Shows the PMF shape, CDF table with row highlighting, and method comparison after search completes.</p></li>
<li><p><strong>Center panel</strong>: Displays the CDF step function with the <span class="math notranslate nohighlight">\(U\)</span> threshold line, plus visualization of the interpolation guess calculation.</p></li>
<li><p><strong>Right panel</strong>: Provides a step-by-step search trace showing the interpolation formula, guess position, and how the search interval narrows.</p></li>
</ul>
<p><strong>Key features to explore</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Distribution selector</strong>: Choose from distributions including Uniform, Near-uniform, Unimodal, Monotonic increasing, Monotonic decreasing, U-shaped, and Skewed distributions.</p></li>
<li><p><strong>Step-by-step mode</strong>: Click “Step →” to walk through each iteration, watching how the interpolation formula estimates the target position.</p></li>
<li><p><strong>Auto-play mode</strong>: Watch the algorithm execute automatically with delays between steps.</p></li>
<li><p><strong>Comparison panel</strong>: After each search completes, see how many iterations interpolation search took versus binary search and linear scan.</p></li>
</ol>
<p><strong>Key observations to make</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Near-uniform distributions</strong>: Try the <em>Uniform</em> or <em>Near-uniform</em> distribution. Interpolation search often finds the answer in just 1-2 iterations because the CDF is nearly linear—the interpolation guess is nearly exact.</p></li>
<li><p><strong>Skewed distributions</strong>: Try <em>Monotonic increasing</em> or <em>Skewed</em>. Watch how the interpolation guess overshoots or undershoots when the CDF curves away from linear. The algorithm still converges but may take more iterations than binary search.</p></li>
<li><p><strong>The interpolation formula</strong>: Observe how <span class="math notranslate nohighlight">\(\text{guess} = \text{low} + \lfloor \frac{u - F[\text{low}-1]}{F[\text{high}] - F[\text{low}-1]} \cdot (\text{high} - \text{low}) \rfloor\)</span> maps the probability ratio to an index estimate.</p></li>
<li><p><strong>Best vs. worst case</strong>: Uniform distributions achieve <span class="math notranslate nohighlight">\(O(\log \log K)\)</span> iterations; highly skewed distributions may require as many iterations as binary search or more.</p></li>
<li><p><strong>When interpolation wins</strong>: Compare iteration counts across distributions. Interpolation search excels when the CDF is approximately linear; it struggles when probability mass is concentrated (steep CDF regions).</p></li>
</ol>
</div>
<p><strong>When Interpolation Search Helps</strong></p>
<p>Interpolation search shines when:</p>
<ul class="simple">
<li><p>The distribution is close to uniform: <span class="math notranslate nohighlight">\(O(\log \log K)\)</span> expected comparisons</p></li>
<li><p>The CDF is approximately linear over most of its range</p></li>
<li><p>You need many samples from a fixed distribution (no setup cost beyond computing the CDF)</p></li>
</ul>
<p>Interpolation search struggles when:</p>
<ul class="simple">
<li><p>The distribution is highly skewed (Zipf, geometric): can degrade to <span class="math notranslate nohighlight">\(O(K)\)</span></p></li>
<li><p>The probability mass is concentrated in a few categories</p></li>
<li><p>The CDF has sharp jumps</p></li>
</ul>
<p><strong>Practical recommendation</strong>: For general-purpose discrete sampling, binary search is safer. Use interpolation search only when you know the distribution is nearly uniform and need the extra speed.</p>
</section>
<section id="exponential-doubling-adaptive-search-for-head-heavy-distributions">
<h3>Exponential Doubling: Adaptive Search for Head-Heavy Distributions<a class="headerlink" href="#exponential-doubling-adaptive-search-for-head-heavy-distributions" title="Link to this heading"></a></h3>
<p>We have seen that linear search excels when samples fall in early categories (head-heavy distributions), while binary search provides worst-case guarantees. Is there an algorithm that captures the best of both worlds—fast when samples are early, yet never slower than <span class="math notranslate nohighlight">\(O(\log K)\)</span>?</p>
<p><strong>Exponential doubling</strong> (also called galloping search or exponential search) achieves exactly this. The insight is elegant: instead of immediately binary searching the entire array <span class="math notranslate nohighlight">\([1, K]\)</span>, first locate a <em>rough range</em> containing the answer, then binary search within that range.</p>
<p>The “rough range” is found by a geometric progression: check <span class="math notranslate nohighlight">\(F_1\)</span>, then <span class="math notranslate nohighlight">\(F_2\)</span>, then <span class="math notranslate nohighlight">\(F_4\)</span>, <span class="math notranslate nohighlight">\(F_8\)</span>, <span class="math notranslate nohighlight">\(F_{16}\)</span>, and so on, doubling the probe position each time until we overshoot (find an <span class="math notranslate nohighlight">\(F_k \geq U\)</span>). At that point, we know the answer lies in <span class="math notranslate nohighlight">\([k/2, k]\)</span>—an interval whose size is at most twice the answer’s index. Binary searching this interval takes <span class="math notranslate nohighlight">\(O(\log k)\)</span> additional comparisons.</p>
<p>The total complexity is <span class="math notranslate nohighlight">\(O(\log k)\)</span> where <span class="math notranslate nohighlight">\(k\)</span> is the <em>result index</em>, not the array size <span class="math notranslate nohighlight">\(K\)</span>. For head-heavy distributions where most samples have small <span class="math notranslate nohighlight">\(k\)</span>, this is a substantial win:</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(k = 1\)</span>: just 1 comparison (check <span class="math notranslate nohighlight">\(F_1\)</span>, done)</p></li>
<li><p>If <span class="math notranslate nohighlight">\(k = 10\)</span>: about <span class="math notranslate nohighlight">\(\log_2 10 + \log_2 10 \approx 7\)</span> comparisons</p></li>
<li><p>If <span class="math notranslate nohighlight">\(k = 1000\)</span> in an array of <span class="math notranslate nohighlight">\(K = 10^6\)</span>: about <span class="math notranslate nohighlight">\(20\)</span> comparisons, same as if <span class="math notranslate nohighlight">\(K = 1000\)</span></p></li>
</ul>
<p>Compare this to binary search, which always takes <span class="math notranslate nohighlight">\(O(\log K)\)</span> regardless of where the answer falls. For a geometric distribution with <span class="math notranslate nohighlight">\(K = 10^6\)</span> outcomes, most samples have <span class="math notranslate nohighlight">\(k &lt; 100\)</span>, so exponential doubling saves roughly half the comparisons.</p>
<p><strong>The Algorithm’s Adaptivity</strong></p>
<p>Exponential doubling is <em>adaptive</em> in a precise sense: its running time depends on the output, not just the input size. Algorithms with this property are sometimes called “output-sensitive.” The practical benefit is that you don’t pay for outcomes you don’t use. If your distribution has a million categories but 95% of samples come from the first hundred, exponential doubling behaves as if <span class="math notranslate nohighlight">\(K = 100\)</span>.</p>
<p>This adaptivity makes exponential doubling particularly attractive for:</p>
<ul class="simple">
<li><p><strong>Zipf and power-law distributions</strong>: Common in natural language processing (word frequencies), web traffic (page popularity), and citation networks. Most mass is in the head.</p></li>
<li><p><strong>Poisson distributions with small</strong> <span class="math notranslate nohighlight">\(\lambda\)</span>: The mode is near zero, so most samples are small integers.</p></li>
<li><p><strong>Truncated geometric distributions</strong>: Arise in models of waiting times and counts with geometric decay.</p></li>
</ul>
<p><strong>Trade-offs and Practical Considerations</strong></p>
<p>Exponential doubling has no preprocessing beyond computing the CDF—unlike the alias method, it requires no auxiliary tables. This makes it suitable for:</p>
<ul class="simple">
<li><p><strong>Dynamic distributions</strong> that may change between batches of samples</p></li>
<li><p><strong>Memory-constrained environments</strong> where <span class="math notranslate nohighlight">\(O(K)\)</span> auxiliary storage is undesirable</p></li>
<li><p><strong>One-shot sampling</strong> where setup cost cannot be amortized</p></li>
</ul>
<p>The downside is that for tail-heavy distributions (where samples tend to fall in late categories), exponential doubling has higher constant factors than plain binary search. When the answer <span class="math notranslate nohighlight">\(k\)</span> is near <span class="math notranslate nohighlight">\(K\)</span>, the geometric probing phase takes <span class="math notranslate nohighlight">\(\sim \log_2 k\)</span> steps to find the range, then binary search takes another <span class="math notranslate nohighlight">\(\sim \log_2 k\)</span> steps to refine—roughly twice the comparisons of binary search, which goes directly to the middle. Both are <span class="math notranslate nohighlight">\(O(\log K)\)</span> asymptotically, but exponential doubling’s constant is approximately 2x worse for tail-heavy distributions.</p>
<p><strong>Summary of Adaptive Behavior</strong></p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Distribution Shape</p></th>
<th class="head"><p>Exponential Doubling</p></th>
<th class="head"><p>Binary Search</p></th>
<th class="head"><p>Practical Winner</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Head-heavy (Zipf)</p></td>
<td><p><span class="math notranslate nohighlight">\(O(\log k)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(\log K)\)</span></p></td>
<td><p>Exponential</p></td>
</tr>
<tr class="row-odd"><td><p>Uniform</p></td>
<td><p><span class="math notranslate nohighlight">\(O(\log K)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(\log K)\)</span></p></td>
<td><p>Binary (lower const)</p></td>
</tr>
<tr class="row-even"><td><p>Tail-heavy</p></td>
<td><p><span class="math notranslate nohighlight">\(O(\log K)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(\log K)\)</span></p></td>
<td><p>Binary (lower const)</p></td>
</tr>
</tbody>
</table>
<p>Exponential doubling matches binary search’s <span class="math notranslate nohighlight">\(O(\log K)\)</span> worst-case complexity but with a higher constant factor (roughly 2x) when samples fall in the tail. However, this trade-off is highly favorable in practice: right-skewed (head-heavy) distributions are far more common than left-skewed ones. Zipf’s law governs word frequencies, city populations, and web traffic; power laws describe income, citations, and network degrees; count data (Poisson, negative binomial) and waiting times (exponential, gamma) all concentrate mass in the head. For these ubiquitous distribution shapes, exponential doubling’s <span class="math notranslate nohighlight">\(O(\log k)\)</span> behavior delivers consistent speedups over binary search. The 2x penalty for tail-heavy distributions is a rare cost for a common benefit.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input: Cumulative probabilities (F₁, ..., Fₖ), uniform value U
Output: Sample index k

1. If U ≤ F[1], return 1
2. Set bound = 1
3. While bound &lt; K and U &gt; F[bound]:
   bound = min(2 * bound, K)
4. Binary search in range [bound/2, bound]
5. Return result
</pre></div>
</div>
<p><strong>Complexity</strong>: <span class="math notranslate nohighlight">\(O(\log k)\)</span> where <span class="math notranslate nohighlight">\(k\)</span> is the result index—much better than <span class="math notranslate nohighlight">\(O(\log K)\)</span> when <span class="math notranslate nohighlight">\(k \ll K\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">exponential_doubling_search</span><span class="p">(</span><span class="n">cdf</span><span class="p">,</span> <span class="n">u</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Find smallest k such that cdf[k] &gt;= u using exponential doubling.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    cdf : array</span>
<span class="sd">        Cumulative probabilities, cdf[0] = 0.</span>
<span class="sd">    u : float</span>
<span class="sd">        Target uniform value.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    int</span>
<span class="sd">        Index k (1-indexed in the probability sense).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cdf</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="n">u</span> <span class="o">&lt;=</span> <span class="n">cdf</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="k">return</span> <span class="mi">1</span>

    <span class="c1"># Exponential doubling to find upper bound</span>
    <span class="n">bound</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">while</span> <span class="n">bound</span> <span class="o">&lt;</span> <span class="n">K</span> <span class="ow">and</span> <span class="n">u</span> <span class="o">&gt;</span> <span class="n">cdf</span><span class="p">[</span><span class="n">bound</span><span class="p">]:</span>
        <span class="n">bound</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">bound</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>

    <span class="c1"># Binary search in [bound//2, bound]</span>
    <span class="n">lo</span> <span class="o">=</span> <span class="n">bound</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="n">hi</span> <span class="o">=</span> <span class="n">bound</span>

    <span class="k">while</span> <span class="n">lo</span> <span class="o">&lt;</span> <span class="n">hi</span><span class="p">:</span>
        <span class="n">mid</span> <span class="o">=</span> <span class="p">(</span><span class="n">lo</span> <span class="o">+</span> <span class="n">hi</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="k">if</span> <span class="n">u</span> <span class="o">&lt;=</span> <span class="n">cdf</span><span class="p">[</span><span class="n">mid</span><span class="p">]:</span>
            <span class="n">hi</span> <span class="o">=</span> <span class="n">mid</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lo</span> <span class="o">=</span> <span class="n">mid</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">lo</span>

<span class="c1"># Example: Head-heavy distribution (exponential doubling excels)</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">2.0</span>
<span class="n">pmf_zipf</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">K</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="n">alpha</span>
<span class="n">pmf_zipf</span> <span class="o">/=</span> <span class="n">pmf_zipf</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">cdf_zipf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pmf_zipf</span><span class="p">)])</span>

<span class="c1"># Most samples will be in early categories</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="n">exponential_doubling_search</span><span class="p">(</span><span class="n">cdf_zipf</span><span class="p">,</span> <span class="n">u</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="n">U</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Median result index: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2"> out of </span><span class="si">{</span><span class="n">K</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;90th percentile: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">results</span><span class="p">,</span><span class="w"> </span><span class="mi">90</span><span class="p">)</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>When to Use Exponential Doubling</strong></p>
<ul class="simple">
<li><p>Head-heavy distributions (Zipf, geometric, Poisson with small λ)</p></li>
<li><p>When most samples fall in early categories</p></li>
<li><p>Adaptive scenarios where the distribution may change</p></li>
</ul>
<p>The alias method still wins for truly large <span class="math notranslate nohighlight">\(K\)</span> with many samples, but exponential doubling provides a good middle ground with no setup cost.</p>
</section>
<section id="the-alias-method-constant-time-through-structural-transformation">
<h3>The Alias Method: Constant Time Through Structural Transformation<a class="headerlink" href="#the-alias-method-constant-time-through-structural-transformation" title="Link to this heading"></a></h3>
<p>All the algorithms we have examined share a common limitation: their sampling time depends on <span class="math notranslate nohighlight">\(K\)</span>, the number of categories. Linear search is <span class="math notranslate nohighlight">\(O(K)\)</span> worst case; binary search is <span class="math notranslate nohighlight">\(O(\log K)\)</span>; even exponential doubling is <span class="math notranslate nohighlight">\(O(\log k)\)</span> for result <span class="math notranslate nohighlight">\(k\)</span>. When <span class="math notranslate nohighlight">\(K\)</span> is in the millions and we need billions of samples, even logarithmic factors accumulate.</p>
<p>Is <span class="math notranslate nohighlight">\(O(1)\)</span> sampling possible? At first glance, this seems impossible. How can we select from <span class="math notranslate nohighlight">\(K\)</span> outcomes with arbitrary probabilities without examining at least <span class="math notranslate nohighlight">\(O(\log K)\)</span> bits of information? The answer involves a profound insight: <strong>restructure the distribution itself</strong> so that sampling becomes trivial.</p>
<p>The <strong>alias method</strong>, developed by Walker in 1977 and refined by Vose in 1991, achieves exactly this. After <span class="math notranslate nohighlight">\(O(K)\)</span> preprocessing, each sample requires only two operations:</p>
<ol class="arabic simple">
<li><p>Generate a uniform integer in <span class="math notranslate nohighlight">\(\{1, 2, \ldots, K\}\)</span>: <span class="math notranslate nohighlight">\(O(1)\)</span></p></li>
<li><p>Flip a biased coin: <span class="math notranslate nohighlight">\(O(1)\)</span></p></li>
</ol>
<p>No searching, no comparisons against the CDF. The running time is genuinely constant, independent of <span class="math notranslate nohighlight">\(K\)</span>.</p>
<p><strong>The Core Insight: Equal-Probability Columns</strong></p>
<p>Imagine pouring the probability mass of each outcome into <span class="math notranslate nohighlight">\(K\)</span> identical cups, each with capacity <span class="math notranslate nohighlight">\(1/K\)</span>. Some outcomes have probability greater than <span class="math notranslate nohighlight">\(1/K\)</span> (they “overflow”); others have probability less than <span class="math notranslate nohighlight">\(1/K\)</span> (they remain partially empty). The key observation is: <strong>we can redistribute mass from overfilled cups to underfilled cups so that every cup holds exactly</strong> <span class="math notranslate nohighlight">\(1/K\)</span> <strong>total probability, contributed by at most two original outcomes</strong>.</p>
<p>Why at most two? Consider the pairing process. Take any underfilled cup (say, outcome <span class="math notranslate nohighlight">\(j\)</span> with probability <span class="math notranslate nohighlight">\(p_j &lt; 1/K\)</span>) and any overfilled cup (outcome <span class="math notranslate nohighlight">\(\ell\)</span> with <span class="math notranslate nohighlight">\(p_\ell &gt; 1/K\)</span>). Pour enough of <span class="math notranslate nohighlight">\(\ell\)</span>’s excess into <span class="math notranslate nohighlight">\(j\)</span>’s cup to top it up to <span class="math notranslate nohighlight">\(1/K\)</span>. Now <span class="math notranslate nohighlight">\(j\)</span>’s cup is full, containing <span class="math notranslate nohighlight">\(p_j\)</span> of <span class="math notranslate nohighlight">\(j\)</span>’s “native” probability and <span class="math notranslate nohighlight">\((1/K - p_j)\)</span> “borrowed” from <span class="math notranslate nohighlight">\(\ell\)</span>. Cup <span class="math notranslate nohighlight">\(j\)</span> is complete. Outcome <span class="math notranslate nohighlight">\(\ell\)</span> may still be overfilled, underfilled, or exactly filled—we’ll handle it in subsequent iterations.</p>
<p>This pairing process terminates in exactly <span class="math notranslate nohighlight">\(K\)</span> steps, constructing two parallel arrays:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">prob[i]</span></code>: The fraction of cup <span class="math notranslate nohighlight">\(i\)</span> filled by its native outcome</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">alias[i]</span></code>: The index of the outcome that donated the remainder (or <span class="math notranslate nohighlight">\(i\)</span> itself if the cup needed no borrowing)</p></li>
</ul>
<p><strong>Sampling: Two Random Numbers, One Table Lookup</strong></p>
<p>With the alias tables constructed, sampling is elegant:</p>
<ol class="arabic simple">
<li><p>Draw <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0, K)\)</span>. Let <span class="math notranslate nohighlight">\(i = \lfloor U \rfloor\)</span> (which cup?) and <span class="math notranslate nohighlight">\(V = U - i\)</span> (where in the cup?).</p></li>
<li><p>If <span class="math notranslate nohighlight">\(V &lt; \text{prob}[i]\)</span>, return <span class="math notranslate nohighlight">\(i\)</span> (native outcome). Otherwise, return <span class="math notranslate nohighlight">\(\text{alias}[i]\)</span> (borrowed outcome).</p></li>
</ol>
<p>Both steps are <span class="math notranslate nohighlight">\(O(1)\)</span>. The first step selects a cup uniformly at random—each cup has total probability <span class="math notranslate nohighlight">\(1/K\)</span>, so this is correct. The second step decides between the two outcomes sharing that cup, weighted by their contributions.</p>
<p><strong>When to Use the Alias Method</strong></p>
<p>The alias method’s <span class="math notranslate nohighlight">\(O(K)\)</span> setup cost must be amortized over many samples. The break-even point depends on the alternative:</p>
<ul class="simple">
<li><p><strong>vs. binary search</strong>: Break-even around <span class="math notranslate nohighlight">\(K / \log K\)</span> samples. For <span class="math notranslate nohighlight">\(K = 1000\)</span>, this is roughly 100 samples.</p></li>
<li><p><strong>vs. linear search</strong>: Break-even around <span class="math notranslate nohighlight">\(K / \mathbb{E}[k]\)</span> samples, depending on distribution shape.</p></li>
</ul>
<p>Use the alias method when:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(K\)</span> is large (thousands or more)</p></li>
<li><p>You need many samples from a fixed distribution</p></li>
<li><p>Memory is not constrained (<span class="math notranslate nohighlight">\(O(K)\)</span> auxiliary storage required)</p></li>
<li><p>The distribution does not change between samples</p></li>
</ul>
<p>Avoid the alias method when:</p>
<ul class="simple">
<li><p>You need only a few samples (setup cost dominates)</p></li>
<li><p>The distribution changes frequently (rebuilding tables is expensive)</p></li>
<li><p><span class="math notranslate nohighlight">\(K\)</span> is small enough that binary search is already fast</p></li>
<li><p>Memory is limited and <span class="math notranslate nohighlight">\(O(K)\)</span> auxiliary arrays are unacceptable</p></li>
</ul>
<p><strong>Numerical Stability Considerations</strong></p>
<p>The Vose variant of the alias method is numerically stable: it processes outcomes in a specific order that prevents accumulation of floating-point errors. Naive implementations can produce slightly incorrect probabilities when <span class="math notranslate nohighlight">\(K\)</span> is large. Always use the Vose algorithm (shown in the code below) for production work.</p>
<p><strong>A Remarkable Achievement</strong></p>
<p>The alias method exemplifies a powerful principle in algorithm design: <strong>transform the problem structure to make the solution trivial</strong>. Rather than searching through the CDF, we rebuild the distribution into a form where sampling requires no search at all. The <span class="math notranslate nohighlight">\(O(K)\)</span> preprocessing is an investment that pays dividends with every subsequent <span class="math notranslate nohighlight">\(O(1)\)</span> sample.</p>
<p>For Monte Carlo simulations requiring millions or billions of samples from a fixed discrete distribution, the alias method is often the optimal choice. Its constant-time sampling transforms computational bottlenecks into non-issues, enabling simulations that would otherwise be infeasible.</p>
<p><strong>The Setup Algorithm</strong></p>
<p>Imagine <span class="math notranslate nohighlight">\(K\)</span> cups, each of capacity <span class="math notranslate nohighlight">\(1/K\)</span>. We pour the probability mass of each outcome into cups, filling some and leaving others partially empty. The alias method pairs each underfilled cup with an overfilled one, “topping up” the underfilled cup with probability mass from the overfilled outcome.</p>
<figure class="align-center" id="id9">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide018_img002_ac0643ba.png"><img alt="Alias method cups visualization" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide018_img002_ac0643ba.png" style="width: 90%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 51 </span><span class="caption-text"><strong>The Alias Method: Balancing Cups.</strong> Top: Original PMF with probabilities 0.1, 0.2, 0.3, 0.4 for categories 1-4. Bottom left: After scaling by <span class="math notranslate nohighlight">\(K=4\)</span>, the heights become 0.4, 0.8, 1.2, 1.6—some below the average of 1, others above. Bottom right: The final balanced cups, each with total height 1. Each cup contains its “native” probability (solid color) plus “borrowed” probability from an overfilled cup (lighter shading with alias label). To sample: pick a cup uniformly, then flip a coin weighted by the cup’s native vs. borrowed portions.</span><a class="headerlink" href="#id9" title="Link to this image"></a></p>
</figcaption>
</figure>
<div class="tip admonition">
<p class="admonition-title">Try It Yourself 🖥️ Walker-Vose Alias Method Simulation</p>
<p>An interactive simulation lets you explore both the setup phase (table construction) and the sampling phase of the alias method:</p>
<p><a class="reference external" href="https://treese41528.github.io/ComputationalDataScience/Simulations/MonteCarloSimulation/alias_method_simulation.html">https://treese41528.github.io/ComputationalDataScience/Simulations/MonteCarloSimulation/alias_method_simulation.html</a></p>
<p>This simulation provides three tabbed views covering the complete alias method:</p>
<p><strong>Setup Tab</strong> — Watch the table construction algorithm:</p>
<ul class="simple">
<li><p><strong>Cup visualization</strong>: See <span class="math notranslate nohighlight">\(K\)</span> cups being filled and balanced in real-time. Native probability shown as solid color; borrowed (alias) probability shown as lighter shading.</p></li>
<li><p><strong>Small/Large stacks</strong>: Watch indices move between the “small” (underfilled) and “large” (overfilled) stacks as the algorithm progresses.</p></li>
<li><p><strong>Step-by-step trace</strong>: Each iteration shows which small cup <span class="math notranslate nohighlight">\(\ell\)</span> pairs with which large cup <span class="math notranslate nohighlight">\(g\)</span>, the probability transfer, and the resulting <span class="math notranslate nohighlight">\(\texttt{prob}[\ell]\)</span> and <span class="math notranslate nohighlight">\(\texttt{alias}[\ell]\)</span> values.</p></li>
<li><p><strong>Final tables</strong>: After setup completes, view the complete <code class="docutils literal notranslate"><span class="pre">prob[]</span></code> and <code class="docutils literal notranslate"><span class="pre">alias[]</span></code> arrays.</p></li>
</ul>
<p><strong>Sampling Tab</strong> — See <span class="math notranslate nohighlight">\(O(1)\)</span> sampling in action:</p>
<ul class="simple">
<li><p><strong>Two-step process</strong>: (1) Pick a random column <span class="math notranslate nohighlight">\(\text{col} \sim \text{Uniform}\{0, \ldots, K-1\}\)</span>, (2) Compare <span class="math notranslate nohighlight">\(u\)</span> to <span class="math notranslate nohighlight">\(\texttt{prob}[\text{col}]\)</span> to choose native or alias outcome.</p></li>
<li><p><strong>Visual feedback</strong>: The selected cup highlights, showing the native/alias decision boundary.</p></li>
<li><p><strong>Sample accumulation</strong>: Watch the histogram of samples converge to the target PMF as you generate more samples.</p></li>
<li><p><strong>Exact computation</strong>: Each sample shows the random values generated and the decision logic.</p></li>
</ul>
<p><strong>Benchmark Tab</strong> — Compare performance:</p>
<ul class="simple">
<li><p><strong>Method comparison</strong>: Run timing benchmarks comparing alias method, binary search, and linear scan.</p></li>
<li><p><strong>Scaling behavior</strong>: See how sampling time grows (or doesn’t) with <span class="math notranslate nohighlight">\(K\)</span> for each method.</p></li>
</ul>
<p><strong>Key observations to make</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Setup process</strong>: Watch how each iteration finalizes exactly one small cup by borrowing from a large cup. The large cup may become small after donating, continuing the process.</p></li>
<li><p><strong>Conservation of probability</strong>: Notice that after setup, every cup has total height exactly 1.0—the algorithm redistributes probability without changing the distribution.</p></li>
<li><p><strong>At most two outcomes per cup</strong>: Each cup contains at most two colors (native + alias). This is why sampling needs only one comparison.</p></li>
<li><p><strong>Constant-time sampling</strong>: In the Sampling tab, observe that the algorithm performs exactly the same operations regardless of which cup is selected—no loops, no searching.</p></li>
<li><p><strong>Same donor, multiple recipients</strong>: A very large cup (high probability outcome) may donate to several small cups. Look for the same alias index appearing multiple times in the <code class="docutils literal notranslate"><span class="pre">alias[]</span></code> array.</p></li>
<li><p><strong>Numerical edge cases</strong>: Try a nearly-degenerate distribution (one probability close to 1). Watch how the algorithm handles cups that are already nearly full.</p></li>
</ol>
<p><strong>Suggested experiments</strong>:</p>
<ul class="simple">
<li><p>Start with a simple 4-category distribution like <span class="math notranslate nohighlight">\((0.1, 0.2, 0.3, 0.4)\)</span> and trace through setup manually.</p></li>
<li><p>Try a uniform distribution—all cups start at height 1.0, so no redistribution is needed.</p></li>
<li><p>Try a Zipf distribution with <span class="math notranslate nohighlight">\(K = 8\)</span> to see how head-heavy distributions create many alias pointers to the first few outcomes.</p></li>
<li><p>Generate 1000+ samples and verify the histogram matches the input PMF.</p></li>
</ul>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input: Probabilities (p₁, ..., pₖ)
Output: Arrays prob[k] and alias[k]

1. Scale probabilities: q[k] = K * p[k]  (so Σq[k] = K)
2. Initialize:
   - small = {k : q[k] &lt; 1}  (underfilled)
   - large = {k : q[k] ≥ 1}  (overfilled)
   - alias[k] = k for all k
3. While small and large are both non-empty:
   a. Remove j from small (underfilled)
   b. Remove ℓ from large (overfilled)
   c. Set prob[j] = q[j], alias[j] = ℓ  (record j&#39;s native prob and alias)
   d. Set q[ℓ] = q[ℓ] - (1 - q[j])  (reduce ℓ&#39;s excess)
   e. If q[ℓ] &lt; 1, move ℓ to small; else keep in large
4. For any remaining k in small or large, set prob[k] = 1
</pre></div>
</div>
<p><strong>The Sampling Algorithm</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input: Arrays prob[k] and alias[k], K
Output: Sample index

1. Generate U ~ Uniform(0, K)
2. Let i = floor(U), V = U - i  (so V ~ Uniform(0, 1))
3. If V &lt; prob[i], return i
4. Else return alias[i]
</pre></div>
</div>
<p>Both steps are <span class="math notranslate nohighlight">\(O(1)\)</span>, making sampling constant-time regardless of <span class="math notranslate nohighlight">\(K\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">AliasMethod</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Alias method for O(1) discrete distribution sampling.</span>

<span class="sd">    After O(K) setup, each sample takes O(1) time.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pmf</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize alias tables.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        pmf : array-like</span>
<span class="sd">            Probability mass function (will be normalized).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">pmf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">pmf</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pmf</span><span class="p">)</span>
        <span class="n">pmf</span> <span class="o">=</span> <span class="n">pmf</span> <span class="o">/</span> <span class="n">pmf</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

        <span class="c1"># Scale probabilities</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">K</span> <span class="o">*</span> <span class="n">pmf</span>

        <span class="c1"># Initialize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

        <span class="c1"># Separate into small and large (with small epsilon for numerical stability)</span>
        <span class="n">small</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">large</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">q</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="mf">1e-10</span><span class="p">:</span>
                <span class="n">small</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">large</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>

        <span class="c1"># Build tables</span>
        <span class="k">while</span> <span class="n">small</span> <span class="ow">and</span> <span class="n">large</span><span class="p">:</span>
            <span class="n">j</span> <span class="o">=</span> <span class="n">small</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>  <span class="c1"># Underfilled</span>
            <span class="n">ell</span> <span class="o">=</span> <span class="n">large</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>  <span class="c1"># Overfilled</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">prob</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">q</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">alias</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">ell</span>

            <span class="c1"># Transfer mass from ℓ to j</span>
            <span class="n">q</span><span class="p">[</span><span class="n">ell</span><span class="p">]</span> <span class="o">=</span> <span class="n">q</span><span class="p">[</span><span class="n">ell</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">q</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>

            <span class="k">if</span> <span class="n">q</span><span class="p">[</span><span class="n">ell</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="mf">1e-10</span><span class="p">:</span>
                <span class="n">small</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ell</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">large</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ell</span><span class="p">)</span>

        <span class="c1"># Remaining entries get probability 1</span>
        <span class="k">while</span> <span class="n">large</span><span class="p">:</span>
            <span class="n">ell</span> <span class="o">=</span> <span class="n">large</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prob</span><span class="p">[</span><span class="n">ell</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="k">while</span> <span class="n">small</span><span class="p">:</span>
            <span class="n">j</span> <span class="o">=</span> <span class="n">small</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prob</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">=</span> <span class="n">K</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate samples in O(1) per sample.&quot;&quot;&quot;</span>
        <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

        <span class="c1"># Generate uniform values in [0, K)</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span>

        <span class="c1"># Extract integer and fractional parts</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">U</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">U</span> <span class="o">-</span> <span class="n">i</span>  <span class="c1"># V ~ Uniform(0, 1)</span>

        <span class="c1"># Handle edge case where i = K (due to floating point)</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Choose between original and alias</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">V</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">prob</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">i</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alias</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

<span class="c1"># Compare performance</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="n">K</span> <span class="o">=</span> <span class="mi">10_000</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">pmf</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">dirichlet</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">K</span><span class="p">))</span>  <span class="c1"># Random distribution</span>

<span class="c1"># Setup</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
<span class="n">alias_dist</span> <span class="o">=</span> <span class="n">AliasMethod</span><span class="p">(</span><span class="n">pmf</span><span class="p">)</span>
<span class="n">alias_setup</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
<span class="n">binary_dist</span> <span class="o">=</span> <span class="n">DiscreteDistributionBinarySearch</span><span class="p">(</span><span class="n">pmf</span><span class="p">)</span>
<span class="n">binary_setup</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Setup time - Alias: </span><span class="si">{</span><span class="n">alias_setup</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> ms, Binary: </span><span class="si">{</span><span class="n">binary_setup</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> ms&quot;</span><span class="p">)</span>

<span class="c1"># Sampling</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1_000_000</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">alias_dist</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">alias_sample</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">binary_dist</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">binary_sample</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample time (</span><span class="si">{</span><span class="n">n_samples</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">) - Alias: </span><span class="si">{</span><span class="n">alias_sample</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> ms, Binary: </span><span class="si">{</span><span class="n">binary_sample</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> ms&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="performance-comparison">
<h3>Performance Comparison<a class="headerlink" href="#performance-comparison" title="Link to this heading"></a></h3>
<p>With five discrete sampling algorithms now in our toolkit—linear search, binary search, interpolation search, exponential doubling, and the alias method—the natural question is: <strong>which should I use?</strong> The answer depends on distribution shape, size <span class="math notranslate nohighlight">\(K\)</span>, and number of samples needed.</p>
<p>The following benchmarks compare all five methods across four distribution types, measuring both wall-clock time and iteration counts as <span class="math notranslate nohighlight">\(K\)</span> grows from 10 to 100,000.</p>
<p><strong>Tail-Heavy Distribution</strong> (probability increases with category index):</p>
<figure class="align-center" id="id10">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide019_img003_7e5a8c9f.png"><img alt="Performance comparison for tail distribution" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide019_img003_7e5a8c9f.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 52 </span><span class="caption-text"><strong>Tail Distribution Benchmark.</strong> Left: Time per sample (seconds, log scale). Right: Iterations per sample (log scale). For tail-heavy distributions, most samples require searching toward the end of the array. Linear scan (purple) grows as <span class="math notranslate nohighlight">\(O(K)\)</span>. Binary search (orange) and exponential doubling (green) both achieve <span class="math notranslate nohighlight">\(O(\log K)\)</span> but with different constants. Interpolation search (red) also grows logarithmically. The alias method (blue) maintains <span class="math notranslate nohighlight">\(O(1)\)</span> regardless of <span class="math notranslate nohighlight">\(K\)</span>.</span><a class="headerlink" href="#id10" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Head-Heavy Distribution</strong> (Zipf-like, probability decreases with category index):</p>
<figure class="align-center" id="id11">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide019_img004_b8994dbd.png"><img alt="Performance comparison for head-heavy distribution" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide019_img004_b8994dbd.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 53 </span><span class="caption-text"><strong>Head-Heavy Distribution Benchmark.</strong> When probability mass concentrates in early categories, linear scan often terminates quickly (nearly constant iterations in the right panel). Exponential doubling excels here—its <span class="math notranslate nohighlight">\(O(\log k)\)</span> complexity (where <span class="math notranslate nohighlight">\(k\)</span> is the <em>result</em> index) means it rarely explores deep into the array. Interpolation search struggles because the CDF is highly nonlinear. The alias method remains constant.</span><a class="headerlink" href="#id11" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Symmetric Distribution</strong> (U-shaped, mass at both ends):</p>
<figure class="align-center" id="id12">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide020_img004_f6113b1d.png"><img alt="Performance comparison for symmetric distribution" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide020_img004_f6113b1d.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 54 </span><span class="caption-text"><strong>Symmetric Distribution Benchmark.</strong> A U-shaped distribution with mass at both head and tail represents a middle ground. Linear scan performs poorly (must traverse to the tail half the time). Binary search and exponential doubling both achieve <span class="math notranslate nohighlight">\(O(\log K)\)</span>. The alias method’s constant time becomes increasingly advantageous as <span class="math notranslate nohighlight">\(K\)</span> grows.</span><a class="headerlink" href="#id12" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Uniform Distribution</strong> (all categories equally likely):</p>
<figure class="align-center" id="id13">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide020_img005_f28deb1f.png"><img alt="Performance comparison for uniform distribution" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide020_img005_f28deb1f.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 55 </span><span class="caption-text"><strong>Uniform Distribution Benchmark.</strong> This is interpolation search’s best case—the nearly linear CDF allows it to converge in <span class="math notranslate nohighlight">\(O(\log \log K)\)</span> iterations (notice the nearly flat red line in the right panel). For very large <span class="math notranslate nohighlight">\(K\)</span>, interpolation search can outperform even binary search in iteration count, though per-iteration overhead may offset this advantage in wall-clock time.</span><a class="headerlink" href="#id13" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Summary: Choosing the Right Algorithm</strong></p>
<table class="docutils align-default" id="id14">
<caption><span class="caption-number">Table 17 </span><span class="caption-text">Discrete Sampling Algorithm Selection Guide</span><a class="headerlink" href="#id14" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 15.0%" />
<col style="width: 15.0%" />
<col style="width: 15.0%" />
<col style="width: 35.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Algorithm</p></th>
<th class="head"><p>Setup</p></th>
<th class="head"><p>Per-Sample</p></th>
<th class="head"><p>Best For</p></th>
<th class="head"><p>Avoid When</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Linear scan</p></td>
<td><p><span class="math notranslate nohighlight">\(O(1)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(K)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(K &lt; 20\)</span>, extreme head-heavy</p></td>
<td><p><span class="math notranslate nohighlight">\(K &gt; 100\)</span>, tail-heavy</p></td>
</tr>
<tr class="row-odd"><td><p>Binary search</p></td>
<td><p><span class="math notranslate nohighlight">\(O(K)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(\log K)\)</span></p></td>
<td><p>General purpose, changing distributions</p></td>
<td><p>Fixed distribution with <span class="math notranslate nohighlight">\(n \gg K\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Interpolation</p></td>
<td><p><span class="math notranslate nohighlight">\(O(K)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(\log\log K)\)</span> to <span class="math notranslate nohighlight">\(O(K)\)</span></p></td>
<td><p>Near-uniform distributions</p></td>
<td><p>Skewed distributions</p></td>
</tr>
<tr class="row-odd"><td><p>Exp. doubling</p></td>
<td><p><span class="math notranslate nohighlight">\(O(K)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(\log k)\)</span></p></td>
<td><p>Head-heavy, unknown structure</p></td>
<td><p>Known uniform structure</p></td>
</tr>
<tr class="row-even"><td><p>Alias method</p></td>
<td><p><span class="math notranslate nohighlight">\(O(K)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(1)\)</span></p></td>
<td><p>Many samples from fixed distribution</p></td>
<td><p>Changing distributions, small <span class="math notranslate nohighlight">\(n\)</span></p></td>
</tr>
</tbody>
</table>
<p><strong>Practical recommendations</strong>:</p>
<ul class="simple">
<li><p><strong>Default choice</strong>: Binary search via <code class="docutils literal notranslate"><span class="pre">np.searchsorted</span></code>—reliable <span class="math notranslate nohighlight">\(O(\log K)\)</span> with minimal code</p></li>
<li><p><strong>Head-heavy distributions</strong> (Zipf, geometric): Exponential doubling or linear scan</p></li>
<li><p><strong>Near-uniform distributions</strong>: Consider interpolation search if <span class="math notranslate nohighlight">\(K\)</span> is very large</p></li>
<li><p><strong>Fixed distribution, many samples</strong>: Alias method pays back its setup cost when <span class="math notranslate nohighlight">\(n &gt; O(K)\)</span></p></li>
<li><p><strong>Very small</strong> <span class="math notranslate nohighlight">\(K &lt; 20\)</span>: Linear scan’s simplicity often wins</p></li>
</ul>
</section>
</section>
<section id="mixed-distributions">
<h2>Mixed Distributions<a class="headerlink" href="#mixed-distributions" title="Link to this heading"></a></h2>
<p>Real-world data often follow <strong>mixed distributions</strong> combining discrete point masses and continuous components. The inverse CDF method handles these naturally.</p>
<section id="zero-inflated-distributions">
<h3>Zero-Inflated Distributions<a class="headerlink" href="#zero-inflated-distributions" title="Link to this heading"></a></h3>
<p>A common example is the <strong>zero-inflated distribution</strong>: with probability <span class="math notranslate nohighlight">\(p_0\)</span>, the outcome is exactly zero; otherwise, it follows a continuous distribution <span class="math notranslate nohighlight">\(F_{\text{cont}}\)</span>.</p>
<figure class="align-center" id="id15">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide025_img002_fb46d81c.png"><img alt="Zero-inflated exponential PDF and CDF" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide025_img002_fb46d81c.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 56 </span><span class="caption-text"><strong>Zero-Inflated Exponential: Structure.</strong> Left: The PDF/PMF combines a point mass at zero (red arrow, probability <span class="math notranslate nohighlight">\(p_0 = 0.3\)</span>) with a scaled exponential density <span class="math notranslate nohighlight">\((1-p_0)\lambda e^{-\lambda x}\)</span> for <span class="math notranslate nohighlight">\(x &gt; 0\)</span>. Right: The CDF has a jump discontinuity at zero—it jumps from <span class="math notranslate nohighlight">\(F(0^-) = 0\)</span> to <span class="math notranslate nohighlight">\(F(0) = p_0\)</span>, then grows continuously following the exponential CDF scaled to the interval <span class="math notranslate nohighlight">\([p_0, 1]\)</span>.</span><a class="headerlink" href="#id15" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>The CDF of a zero-inflated distribution is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}F(x) = \begin{cases}
0 &amp; x &lt; 0 \\
p_0 + (1 - p_0) F_{\text{cont}}(x) &amp; x \geq 0
\end{cases}\end{split}\]</div>
<figure class="align-center" id="id16">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_3_fig03_mixed_distribution.png"><img alt="Zero-inflated exponential distribution showing PDF, CDF, and sampling" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_3_fig03_mixed_distribution.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 57 </span><span class="caption-text"><strong>Mixed Distributions: Sampling Verification.</strong> Left: The zero-inflated exponential has a point mass at zero (probability <span class="math notranslate nohighlight">\(p_0 = 0.3\)</span>) plus a continuous exponential component. Center: The CDF has a jump discontinuity of size <span class="math notranslate nohighlight">\(p_0\)</span> at zero, then grows continuously. Right: Sampling demonstration with the continuous part shown as a histogram matching the theoretical density. The inset bar chart confirms that the proportion of exact zeros (30.4%) closely matches the theoretical 30%.</span><a class="headerlink" href="#id16" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Inverse CDF for Zero-Inflated Exponential</strong>:</p>
<p>For <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0, 1)\)</span>:</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(U \leq p_0\)</span>: return <span class="math notranslate nohighlight">\(X = 0\)</span></p></li>
<li><p>Else: transform <span class="math notranslate nohighlight">\(U\)</span> to the continuous part and apply the exponential inverse</p></li>
</ul>
<p>The transformation for the continuous part rescales <span class="math notranslate nohighlight">\(U\)</span> from <span class="math notranslate nohighlight">\([p_0, 1]\)</span> to <span class="math notranslate nohighlight">\([0, 1]\)</span>:</p>
<div class="math notranslate nohighlight">
\[U_{\text{cont}} = \frac{U - p_0}{1 - p_0}\]</div>
<p>Then <span class="math notranslate nohighlight">\(X = F_{\text{cont}}^{-1}(U_{\text{cont}})\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">zero_inflated_exponential</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">p_zero</span><span class="p">,</span> <span class="n">rate</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sample from zero-inflated exponential via inverse CDF.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    u : float or array</span>
<span class="sd">        Uniform random value(s).</span>
<span class="sd">    p_zero : float</span>
<span class="sd">        Probability of structural zero.</span>
<span class="sd">    rate : float</span>
<span class="sd">        Rate of exponential component.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    float or array</span>
<span class="sd">        Sample(s) from the zero-inflated distribution.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>

    <span class="c1"># Points above p_zero come from the exponential</span>
    <span class="n">continuous_mask</span> <span class="o">=</span> <span class="n">u</span> <span class="o">&gt;</span> <span class="n">p_zero</span>

    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">continuous_mask</span><span class="p">):</span>
        <span class="c1"># Rescale U from [p_zero, 1] to [0, 1]</span>
        <span class="n">u_scaled</span> <span class="o">=</span> <span class="p">(</span><span class="n">u</span><span class="p">[</span><span class="n">continuous_mask</span><span class="p">]</span> <span class="o">-</span> <span class="n">p_zero</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_zero</span><span class="p">)</span>
        <span class="c1"># Guard against log(0)</span>
        <span class="n">u_scaled</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">u_scaled</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">)</span>
        <span class="c1"># Apply exponential inverse CDF using tail-stable form</span>
        <span class="n">result</span><span class="p">[</span><span class="n">continuous_mask</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">u_scaled</span><span class="p">)</span> <span class="o">/</span> <span class="n">rate</span>

    <span class="k">return</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">result</span>

<span class="c1"># Verify</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="mi">100_000</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">zero_inflated_exponential</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">p_zero</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">prop_zeros</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samples</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">mean_nonzero</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="n">samples</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Proportion of zeros: </span><span class="si">{</span><span class="n">prop_zeros</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: 0.3000)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean of non-zeros: </span><span class="si">{</span><span class="n">mean_nonzero</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: 2.0000)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="general-mixed-distributions">
<h3>General Mixed Distributions<a class="headerlink" href="#general-mixed-distributions" title="Link to this heading"></a></h3>
<p>More generally, a mixed distribution might have <span class="math notranslate nohighlight">\(m\)</span> point masses at <span class="math notranslate nohighlight">\(a_1, \ldots, a_m\)</span> with probabilities <span class="math notranslate nohighlight">\(\pi_1, \ldots, \pi_m\)</span>, plus a continuous component <span class="math notranslate nohighlight">\(F_{\text{cont}}\)</span> with weight <span class="math notranslate nohighlight">\(\pi_{\text{cont}} = 1 - \sum_i \pi_i\)</span>.</p>
<p>The inverse CDF method partitions <span class="math notranslate nohighlight">\([0, 1]\)</span> accordingly:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">mixed_distribution_sample</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">point_masses</span><span class="p">,</span> <span class="n">point_probs</span><span class="p">,</span>
                               <span class="n">continuous_inverse_cdf</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sample from a mixed discrete-continuous distribution.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    u : float</span>
<span class="sd">        Uniform random value.</span>
<span class="sd">    point_masses : array</span>
<span class="sd">        Values with point mass.</span>
<span class="sd">    point_probs : array</span>
<span class="sd">        Probabilities of point masses.</span>
<span class="sd">    continuous_inverse_cdf : callable</span>
<span class="sd">        Inverse CDF of the continuous component.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    float</span>
<span class="sd">        Sample from the mixed distribution.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cumprob</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="c1"># Check point masses</span>
    <span class="k">for</span> <span class="n">mass</span><span class="p">,</span> <span class="n">prob</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">point_masses</span><span class="p">,</span> <span class="n">point_probs</span><span class="p">):</span>
        <span class="n">cumprob</span> <span class="o">+=</span> <span class="n">prob</span>
        <span class="k">if</span> <span class="n">u</span> <span class="o">&lt;=</span> <span class="n">cumprob</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">mass</span>

    <span class="c1"># Continuous component</span>
    <span class="c1"># Rescale U to [0, 1] for the continuous part</span>
    <span class="n">u_cont</span> <span class="o">=</span> <span class="p">(</span><span class="n">u</span> <span class="o">-</span> <span class="n">cumprob</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">cumprob</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">continuous_inverse_cdf</span><span class="p">(</span><span class="n">u_cont</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="practical-considerations">
<h2>Practical Considerations<a class="headerlink" href="#practical-considerations" title="Link to this heading"></a></h2>
<section id="numerical-precision">
<h3>Numerical Precision<a class="headerlink" href="#numerical-precision" title="Link to this heading"></a></h3>
<p>Several numerical issues can arise with the inverse CDF method. Understanding and mitigating them is essential for reliable implementations.</p>
<figure class="align-center" id="id17">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_3_fig04_numerical_precision.png"><img alt="Numerical precision issues in inverse CDF implementation" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_3_fig04_numerical_precision.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 58 </span><span class="caption-text"><strong>Numerical Precision Issues.</strong> Left: Two equivalent formulas for exponential sampling—<span class="math notranslate nohighlight">\(-\ln(U)\)</span> and <span class="math notranslate nohighlight">\(-\ln(1-U)\)</span>—behave differently at the boundaries; <span class="math notranslate nohighlight">\(-\ln(U)\)</span> produces large samples for small <span class="math notranslate nohighlight">\(U\)</span> without precision issues. Center: Catastrophic cancellation when computing <span class="math notranslate nohighlight">\(1-U\)</span> for <span class="math notranslate nohighlight">\(U\)</span> near 1; each decimal digit of precision needed consumes one of float64’s ~16 available digits. Right: Explanation of why the <span class="math notranslate nohighlight">\(-\ln(U)\)</span> form is preferred—it places large samples where floating point has finer resolution.</span><a class="headerlink" href="#id17" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>1. Boundary Handling</strong></p>
<p>When <span class="math notranslate nohighlight">\(U\)</span> is very close to 0 or 1, <span class="math notranslate nohighlight">\(F^{-1}(U)\)</span> may produce extreme values or overflow. For instance, <span class="math notranslate nohighlight">\(-\ln(U) \to \infty\)</span> as <span class="math notranslate nohighlight">\(U \to 0\)</span>.</p>
<p><strong>Mitigation</strong>: Clamp <span class="math notranslate nohighlight">\(U\)</span> away from exact 0. For tail-stable forms like <span class="math notranslate nohighlight">\(-\log(U)\)</span>, use the smallest positive float to maximize tail reach:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">safe_uniform</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate uniform values safely bounded away from 0.&quot;&quot;&quot;</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
    <span class="c1"># Use smallest positive float to maximize tail reach</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>2. Logarithms, (1-u), and What log1p Actually Fixes</strong></p>
<p>Many inverse CDF formulas naturally appear in the form <span class="math notranslate nohighlight">\(\ln(1-u)\)</span> because they derive from the survival function. For example, for <span class="math notranslate nohighlight">\(X \sim \text{Exponential}(\lambda)\)</span>:</p>
<div class="math notranslate nohighlight">
\[X = -\frac{\ln(1-U)}{\lambda}\]</div>
<p>In exact arithmetic this is perfectly fine. In floating-point arithmetic there are <em>two</em> separate concerns:</p>
<p><strong>(a) Cancellation in forming</strong> <span class="math notranslate nohighlight">\(1-U\)</span>. When <span class="math notranslate nohighlight">\(U\)</span> is close to 1, the quantity <span class="math notranslate nohighlight">\(1-U\)</span> is small, and the subtraction can lose significant digits. The function <code class="docutils literal notranslate"><span class="pre">log1p(x)</span></code> computes <span class="math notranslate nohighlight">\(\ln(1+x)\)</span> in a way that avoids the naive, accuracy-losing evaluation of <code class="docutils literal notranslate"><span class="pre">log(1+x)</span></code> when <code class="docutils literal notranslate"><span class="pre">1+x</span></code> is close to 1. Therefore <code class="docutils literal notranslate"><span class="pre">np.log1p(-u)</span></code> <em>does</em> improve accuracy for <span class="math notranslate nohighlight">\(\ln(1-u)\)</span> when <span class="math notranslate nohighlight">\(u\)</span> is close to 1, because it avoids catastrophic cancellation in forming <span class="math notranslate nohighlight">\(1-u\)</span>.</p>
<p><strong>(b) Tail resolution near 1 (the bigger practical problem)</strong>. Even if you compute <span class="math notranslate nohighlight">\(\ln(1-U)\)</span> accurately via <code class="docutils literal notranslate"><span class="pre">log1p</span></code>, sampling large exponential values requires <span class="math notranslate nohighlight">\(U\)</span> extremely close to 1. But IEEE-754 doubles are <em>coarsely spaced</em> near 1. The smallest positive gap near 1 in float64 is on the order of <span class="math notranslate nohighlight">\(10^{-16}\)</span>, so:</p>
<div class="math notranslate nohighlight">
\[-\ln(1-U) \text{ is effectively capped around } 36\text{–}37\]</div>
<p>This effectively truncates the far right tail and produces too few extreme values. This is a <em>representational/granularity</em> limitation that <code class="docutils literal notranslate"><span class="pre">log1p</span></code> cannot fix—it’s not a cancellation issue, it’s that the RNG cannot produce <span class="math notranslate nohighlight">\(U\)</span> values close enough to 1.</p>
<p><strong>Best Practice for Exponential-Type Inverses</strong></p>
<p>Because <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(1-U\)</span> have the same Uniform(0,1) distribution, rewrite the inverse in the equivalent form:</p>
<div class="math notranslate nohighlight">
\[X = -\frac{\ln(U)}{\lambda}\]</div>
<p>Now large <span class="math notranslate nohighlight">\(X\)</span> corresponds to <span class="math notranslate nohighlight">\(U\)</span> near 0, where floating point has much finer resolution (and can represent much smaller positive numbers). This preserves tail behavior far better in finite-precision implementations.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">exponential_inverse_cdf</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">rate</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential(rate) via inverse CDF, tail-stable form.</span>

<span class="sd">    Uses X = -log(U)/rate and avoids forming (1-U).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="c1"># Guard against log(0) - use smallest positive float for max tail reach</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">u</span><span class="p">)</span> <span class="o">/</span> <span class="n">rate</span>
</pre></div>
</div>
<p><strong>When log1p Is the Right Tool</strong></p>
<p>Use <code class="docutils literal notranslate"><span class="pre">log1p</span></code> when:</p>
<ul class="simple">
<li><p>You genuinely need <span class="math notranslate nohighlight">\(\ln(1-u)\)</span> and cannot use the symmetry trick (e.g., certain non-exponential-type distributions)</p></li>
<li><p>You’re computing logit: <code class="docutils literal notranslate"><span class="pre">np.log(u)</span> <span class="pre">-</span> <span class="pre">np.log1p(-u)</span></code> is more stable than <code class="docutils literal notranslate"><span class="pre">np.log(u</span> <span class="pre">/</span> <span class="pre">(1</span> <span class="pre">-</span> <span class="pre">u))</span></code></p></li>
<li><p>You’re evaluating log-survival terms where the survival function <span class="math notranslate nohighlight">\(S(x) = 1 - F(x)\)</span> is near 1</p></li>
</ul>
<p>But <code class="docutils literal notranslate"><span class="pre">log1p(-u)</span></code> does <em>not</em> solve the tail granularity problem—it only addresses subtraction accuracy. For distributions where you can switch from <code class="docutils literal notranslate"><span class="pre">log(1-u)</span></code> to <code class="docutils literal notranslate"><span class="pre">log(u)</span></code> by symmetry (exponential, Weibull, Pareto), the symmetry-based rewrite is the most robust solution.</p>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ⚠️</p>
<p><strong>Two distinct issues, two distinct fixes</strong>: (1) <em>Cancellation</em> when forming <span class="math notranslate nohighlight">\(1-U\)</span> for <span class="math notranslate nohighlight">\(U \approx 1\)</span>—<code class="docutils literal notranslate"><span class="pre">log1p(-u)</span></code> helps here. (2) <em>Tail granularity</em> because the RNG cannot produce <span class="math notranslate nohighlight">\(U\)</span> close enough to 1—<code class="docutils literal notranslate"><span class="pre">log1p</span></code> does <strong>not</strong> help here; the fix is to use <span class="math notranslate nohighlight">\(-\ln(U)\)</span> so large samples correspond to small <span class="math notranslate nohighlight">\(U\)</span>, where floats have finer spacing.</p>
</div>
<p><strong>3. CDF Evaluation Accuracy</strong></p>
<p>For numerical inversion, the CDF must be evaluated accurately throughout its domain. In the tails, naive implementations may suffer from cancellation or underflow.</p>
<p><strong>Example</strong>: The normal CDF <span class="math notranslate nohighlight">\(\Phi(x)\)</span> for <span class="math notranslate nohighlight">\(x = 8\)</span> is about <span class="math notranslate nohighlight">\(1 - 6 \times 10^{-16}\)</span>—indistinguishable from 1 in double precision. Libraries like SciPy use special functions and asymptotic expansions for accuracy in the tails.</p>
</section>
<section id="when-not-to-use-inverse-cdf">
<h3>When Not to Use Inverse CDF<a class="headerlink" href="#when-not-to-use-inverse-cdf" title="Link to this heading"></a></h3>
<p>The inverse CDF method is not always the best choice:</p>
<p><strong>1. No Closed-Form Inverse</strong></p>
<p>The normal distribution has no closed-form inverse CDF. While numerical inversion works, specialized methods like <strong>Box-Muller</strong> (Section 2.4) are faster:</p>
<div class="math notranslate nohighlight">
\[Z_1 = \sqrt{-2 \ln U_1} \cos(2\pi U_2), \quad
Z_2 = \sqrt{-2 \ln U_1} \sin(2\pi U_2)\]</div>
<p>produces two independent <span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span> samples from two uniform samples—with only logarithms and trigonometric functions, no root-finding.</p>
<p><strong>2. Multivariate Distributions</strong></p>
<p>The inverse CDF method extends awkwardly to multiple dimensions. For a random vector <span class="math notranslate nohighlight">\(\mathbf{X} = (X_1, \ldots, X_d)\)</span>, we would need to:</p>
<ol class="arabic simple">
<li><p>Sample <span class="math notranslate nohighlight">\(X_1\)</span> from its marginal</p></li>
<li><p>Sample <span class="math notranslate nohighlight">\(X_2\)</span> from its conditional given <span class="math notranslate nohighlight">\(X_1\)</span></p></li>
<li><p>Continue sequentially…</p></li>
</ol>
<p>This requires knowing all conditional distributions, which is often impractical. <strong>Rejection sampling</strong> and <strong>MCMC</strong> (Part 3) are better suited to multivariate problems.</p>
<p><strong>3. Complex Dependencies</strong></p>
<p>For distributions defined implicitly—such as the stationary distribution of a Markov chain or the posterior in Bayesian inference—we typically cannot even write down the CDF. <strong>Markov chain Monte Carlo</strong> methods are necessary.</p>
<p><strong>Rule of Thumb</strong>: Use inverse CDF when:</p>
<ul class="simple">
<li><p>The inverse has a closed-form expression, OR</p></li>
<li><p>You need samples from a custom discrete distribution, OR</p></li>
<li><p>You need only a few samples from an unusual distribution</p></li>
</ul>
<p>Use other methods when:</p>
<ul class="simple">
<li><p>Specialized algorithms exist (Box-Muller for normals, ratio-of-uniforms for gammas)</p></li>
<li><p>The distribution is multivariate</p></li>
<li><p>The distribution is defined implicitly</p></li>
</ul>
</section>
</section>
<section id="chapter-2-3-exercises-inverse-cdf-method-mastery">
<h2>Chapter 2.3 Exercises: Inverse CDF Method Mastery<a class="headerlink" href="#chapter-2-3-exercises-inverse-cdf-method-mastery" title="Link to this heading"></a></h2>
<p>These exercises progressively build your understanding of the inverse CDF method, from verifying the theoretical foundations through implementing efficient discrete sampling algorithms. Each exercise connects theory, implementation, and practical considerations essential for random variable generation.</p>
<div class="tip admonition">
<p class="admonition-title">A Note on These Exercises</p>
<p>These exercises are designed to deepen your understanding of the inverse CDF method through hands-on derivation and implementation:</p>
<ul class="simple">
<li><p><strong>Exercise 1</strong> verifies the Probability Integral Transform empirically, building intuition for why the method works</p></li>
<li><p><strong>Exercise 2</strong> develops facility deriving and implementing closed-form inverse CDFs for continuous distributions</p></li>
<li><p><strong>Exercise 3</strong> explores discrete sampling algorithms with different complexity trade-offs</p></li>
<li><p><strong>Exercise 4</strong> implements the alias method for constant-time discrete sampling</p></li>
<li><p><strong>Exercise 5</strong> handles mixed distributions and numerical edge cases</p></li>
<li><p><strong>Exercise 6</strong> synthesizes the material into a complete distribution sampler class</p></li>
</ul>
<p>Complete solutions with derivations, code, output, and interpretation are provided. Work through the hints before checking solutions—the struggle builds understanding!</p>
</div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 1: Verifying the Probability Integral Transform</p>
<p>The Probability Integral Transform states that if <span class="math notranslate nohighlight">\(X\)</span> has continuous CDF <span class="math notranslate nohighlight">\(F\)</span>, then <span class="math notranslate nohighlight">\(U = F(X) \sim \text{Uniform}(0, 1)\)</span>. Conversely, if <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0, 1)\)</span>, then <span class="math notranslate nohighlight">\(X = F^{-1}(U)\)</span> has CDF <span class="math notranslate nohighlight">\(F\)</span>. This exercise verifies both directions empirically.</p>
<div class="note admonition">
<p class="admonition-title">Background: Why This Matters</p>
<p>The Probability Integral Transform is the theoretical foundation of the inverse CDF method. Understanding it deeply—not just as a formula but as a geometric relationship between distributions—builds intuition for when the method works, why it’s universal, and how it handles discrete and mixed distributions.</p>
</div>
<ol class="loweralpha">
<li><p><strong>Forward direction</strong>: Generate 10,000 samples from <span class="math notranslate nohighlight">\(X \sim \text{Exponential}(2)\)</span> using NumPy’s built-in generator. Apply the CDF <span class="math notranslate nohighlight">\(F(x) = 1 - e^{-2x}\)</span> to transform them into <span class="math notranslate nohighlight">\(U = F(X)\)</span>. Verify that <span class="math notranslate nohighlight">\(U\)</span> is uniformly distributed by:</p>
<ul class="simple">
<li><p>Plotting a histogram of <span class="math notranslate nohighlight">\(U\)</span> and comparing to Uniform(0, 1)</p></li>
<li><p>Computing summary statistics (mean, variance, min, max)</p></li>
<li><p>Running a Kolmogorov-Smirnov test</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Hint: Expected Statistics</p>
<p>For <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0, 1)\)</span>: <span class="math notranslate nohighlight">\(\mathbb{E}[U] = 0.5\)</span>, <span class="math notranslate nohighlight">\(\text{Var}(U) = 1/12 \approx 0.0833\)</span>. The K-S test from <code class="docutils literal notranslate"><span class="pre">scipy.stats.kstest</span></code> tests whether a sample comes from a specified distribution.</p>
</div>
</li>
<li><p><strong>Inverse direction</strong>: Generate 10,000 uniform samples <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0, 1)\)</span>. Apply the inverse CDF <span class="math notranslate nohighlight">\(F^{-1}(u) = -\ln(1-u)/2\)</span> to obtain <span class="math notranslate nohighlight">\(X\)</span>. Verify that <span class="math notranslate nohighlight">\(X \sim \text{Exponential}(2)\)</span> by:</p>
<ul class="simple">
<li><p>Plotting a histogram with the theoretical PDF overlaid</p></li>
<li><p>Comparing sample mean and variance to theoretical values</p></li>
<li><p>Running a Kolmogorov-Smirnov test</p></li>
</ul>
</li>
<li><p><strong>Discrete distribution case</strong>: The forward transform doesn’t produce a uniform distribution for discrete random variables. Generate 10,000 samples from <span class="math notranslate nohighlight">\(X \sim \text{Poisson}(5)\)</span> and compute <span class="math notranslate nohighlight">\(U = F(X)\)</span>. Why isn’t <span class="math notranslate nohighlight">\(U\)</span> uniform? What does its distribution look like?</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Step Function CDF</p>
<p>For discrete distributions, the CDF is a step function. When you apply <span class="math notranslate nohighlight">\(F\)</span> to discrete samples, you get a discrete set of possible values—the heights of the CDF steps.</p>
</div>
</li>
<li><p><strong>Geometric interpretation</strong>: Create a visualization showing the relationship between the CDF, the uniform sample, and the inverse transformation. Plot the CDF of <span class="math notranslate nohighlight">\(\text{Exponential}(2)\)</span>, draw horizontal lines at several <span class="math notranslate nohighlight">\(u\)</span> values, and show how they map to <span class="math notranslate nohighlight">\(x\)</span> values via the inverse.</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Forward Direction</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">verify_forward_pit</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Verify that F(X) ~ Uniform(0,1) when X ~ Exponential(2).&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10_000</span>
    <span class="n">rate</span> <span class="o">=</span> <span class="mf">2.0</span>

    <span class="c1"># Generate exponential samples</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">rate</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Apply CDF: F(x) = 1 - exp(-λx)</span>
    <span class="n">U</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">rate</span> <span class="o">*</span> <span class="n">X</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;FORWARD PROBABILITY INTEGRAL TRANSFORM&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;X ~ Exponential(</span><span class="si">{</span><span class="n">rate</span><span class="si">}</span><span class="s2">), U = F(X)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>

    <span class="c1"># Summary statistics</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Summary Statistics for U = F(X):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean:     </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">U</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: 0.5000)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">U</span><span class="p">,</span><span class="w"> </span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: 0.0833)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Min:      </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">U</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: 0)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Max:      </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">U</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: 1)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>

    <span class="c1"># Kolmogorov-Smirnov test</span>
    <span class="n">ks_stat</span><span class="p">,</span> <span class="n">p_value</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">kstest</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="s1">&#39;uniform&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Kolmogorov-Smirnov test against Uniform(0,1):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  KS statistic: </span><span class="si">{</span><span class="n">ks_stat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  p-value:      </span><span class="si">{</span><span class="n">p_value</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Conclusion:   </span><span class="si">{</span><span class="s1">&#39;Uniform (p &gt; 0.05)&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">p_value</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mf">0.05</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;Not uniform&#39;</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">U</span>

<span class="n">U_forward</span> <span class="o">=</span> <span class="n">verify_forward_pit</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>FORWARD PROBABILITY INTEGRAL TRANSFORM
=======================================================
X ~ Exponential(2), U = F(X)

Summary Statistics for U = F(X):
  Mean:     0.4989 (theory: 0.5000)
  Variance: 0.0834 (theory: 0.0833)
  Min:      0.0001 (theory: 0)
  Max:      1.0000 (theory: 1)

Kolmogorov-Smirnov test against Uniform(0,1):
  KS statistic: 0.0064
  p-value:      0.8271
  Conclusion:   Uniform (p &gt; 0.05)
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Inverse Direction</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">verify_inverse_pit</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Verify that F⁻¹(U) ~ Exponential(2) when U ~ Uniform(0,1).&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10_000</span>
    <span class="n">rate</span> <span class="o">=</span> <span class="mf">2.0</span>

    <span class="c1"># Generate uniform samples</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Apply inverse CDF: F⁻¹(u) = -ln(1-u)/λ</span>
    <span class="c1"># Using -ln(U)/λ for numerical stability (U and 1-U have same distribution)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">U</span><span class="p">)</span> <span class="o">/</span> <span class="n">rate</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">INVERSE PROBABILITY INTEGRAL TRANSFORM&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;U ~ Uniform(0,1), X = F⁻¹(U)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>

    <span class="c1"># Theoretical values for Exponential(λ)</span>
    <span class="n">theoretical_mean</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">rate</span>
    <span class="n">theoretical_var</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">rate</span><span class="o">**</span><span class="mi">2</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Summary Statistics for X = F⁻¹(U):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean:     </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="n">theoretical_mean</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="n">theoretical_var</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Median:   </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">rate</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>

    <span class="c1"># Kolmogorov-Smirnov test</span>
    <span class="n">ks_stat</span><span class="p">,</span> <span class="n">p_value</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">kstest</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="s1">&#39;expon&#39;</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="n">rate</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Kolmogorov-Smirnov test against Exponential(</span><span class="si">{</span><span class="n">rate</span><span class="si">}</span><span class="s2">):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  KS statistic: </span><span class="si">{</span><span class="n">ks_stat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  p-value:      </span><span class="si">{</span><span class="n">p_value</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Conclusion:   </span><span class="si">{</span><span class="s1">&#39;Exponential (p &gt; 0.05)&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">p_value</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mf">0.05</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;Not exponential&#39;</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">X</span>

<span class="n">X_inverse</span> <span class="o">=</span> <span class="n">verify_inverse_pit</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>INVERSE PROBABILITY INTEGRAL TRANSFORM
=======================================================
U ~ Uniform(0,1), X = F⁻¹(U)

Summary Statistics for X = F⁻¹(U):
  Mean:     0.4976 (theory: 0.5000)
  Variance: 0.2467 (theory: 0.2500)
  Median:   0.3441 (theory: 0.3466)

Kolmogorov-Smirnov test against Exponential(2):
  KS statistic: 0.0064
  p-value:      0.8271
  Conclusion:   Exponential (p &gt; 0.05)
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (c): Discrete Distribution Case</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">verify_discrete_pit</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Show that F(X) is NOT uniform for discrete X.&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10_000</span>
    <span class="n">lam</span> <span class="o">=</span> <span class="mf">5.0</span>

    <span class="c1"># Generate Poisson samples</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="n">lam</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Apply CDF: F(x) = P(X ≤ x)</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">lam</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">DISCRETE DISTRIBUTION: POISSON(5)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;U = F(X) where X ~ Poisson(5)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>

    <span class="c1"># The CDF can only take values at the &quot;step heights&quot;</span>
    <span class="n">unique_u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of unique U values: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">unique_u</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;First 10 unique U values: </span><span class="si">{</span><span class="n">unique_u</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>

    <span class="c1"># Summary statistics</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Summary Statistics for U = F(X):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean:     </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">U</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (if uniform: 0.5000)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">U</span><span class="p">,</span><span class="w"> </span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (if uniform: 0.0833)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>

    <span class="c1"># K-S test</span>
    <span class="n">ks_stat</span><span class="p">,</span> <span class="n">p_value</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">kstest</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="s1">&#39;uniform&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Kolmogorov-Smirnov test against Uniform(0,1):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  KS statistic: </span><span class="si">{</span><span class="n">ks_stat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  p-value:      </span><span class="si">{</span><span class="n">p_value</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Conclusion:   NOT uniform (discrete CDF produces discrete U)&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">U</span>

<span class="n">U_discrete</span> <span class="o">=</span> <span class="n">verify_discrete_pit</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>DISCRETE DISTRIBUTION: POISSON(5)
=======================================================
U = F(X) where X ~ Poisson(5)

Number of unique U values: 17
First 10 unique U values: [0.0067 0.0404 0.1247 0.2650 0.4405 0.6160 0.7622 0.8666 0.9319 0.9682]

Summary Statistics for U = F(X):
  Mean:     0.5765 (if uniform: 0.5000)
  Variance: 0.0649 (if uniform: 0.0833)

Kolmogorov-Smirnov test against Uniform(0,1):
  KS statistic: 0.1162
  p-value:      0.000000
  Conclusion:   NOT uniform (discrete CDF produces discrete U)
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): Geometric Interpretation</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">plot_inverse_cdf_geometry</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Visualize the geometric relationship of inverse CDF method.&quot;&quot;&quot;</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">rate</span> <span class="o">=</span> <span class="mf">2.0</span>

    <span class="c1"># Plot CDF</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
    <span class="n">F</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">rate</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;CDF: $F(x) = 1 - e^{-2x}$&#39;</span><span class="p">)</span>

    <span class="c1"># Show inverse transformation for several u values</span>
    <span class="n">u_values</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">]</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="s1">&#39;purple&#39;</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">u</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">u_values</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
        <span class="n">x_inv</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">u</span><span class="p">)</span> <span class="o">/</span> <span class="n">rate</span>  <span class="c1"># F⁻¹(u)</span>

        <span class="c1"># Horizontal line from y-axis to CDF</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">x_inv</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
        <span class="c1"># Vertical line from CDF to x-axis</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">x_inv</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
        <span class="c1"># Mark the point</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_inv</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
        <span class="c1"># Label</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$u=</span><span class="si">{</span><span class="n">u</span><span class="si">}</span><span class="s1">$</span><span class="se">\n</span><span class="s1">$x=</span><span class="si">{</span><span class="n">x_inv</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">,</span>
                   <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x_inv</span><span class="p">,</span> <span class="n">u</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">x_inv</span> <span class="o">+</span> <span class="mf">0.15</span><span class="p">,</span> <span class="n">u</span> <span class="o">+</span> <span class="mf">0.05</span><span class="p">),</span>
                   <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;F(x) = u&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Inverse CDF Method: Geometric View</span><span class="se">\n</span><span class="s1">&#39;</span>
                <span class="sa">r</span><span class="s1">&#39;Generate $U \sim \text</span><span class="si">{Uniform}</span><span class="s1">(0,1)$, compute $X = F^{-1}(U)$&#39;</span><span class="p">,</span>
                <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;inverse_cdf_geometry.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Geometric Interpretation:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  1. Draw horizontal line at height u (uniform sample)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  2. Find where it intersects the CDF curve&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  3. Project down to x-axis: this is F⁻¹(u)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  4. The transformation &#39;stretches&#39; uniform samples according to PDF&quot;</span><span class="p">)</span>

<span class="n">plot_inverse_cdf_geometry</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Geometric Interpretation:
  1. Draw horizontal line at height u (uniform sample)
  2. Find where it intersects the CDF curve
  3. Project down to x-axis: this is F⁻¹(u)
  4. The transformation &#39;stretches&#39; uniform samples according to PDF
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Forward transform produces uniform</strong>: Applying the CDF to samples from that distribution yields uniform samples—confirmed by K-S test (p = 0.83).</p></li>
<li><p class="sd-card-text"><strong>Inverse transform produces target</strong>: Applying the inverse CDF to uniform samples produces samples from the target distribution—same K-S test result due to the duality.</p></li>
<li><p class="sd-card-text"><strong>Discrete case fails</strong>: For discrete distributions, the CDF is a step function, so <span class="math notranslate nohighlight">\(F(X)\)</span> can only take finitely many values—it’s inherently not uniform.</p></li>
<li><p class="sd-card-text"><strong>Geometric intuition</strong>: The inverse CDF “stretches” the uniform distribution where the PDF is low (CDF is flat) and “compresses” where the PDF is high (CDF is steep).</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 2: Deriving and Implementing Continuous Inverse CDFs</p>
<p>The power of the inverse CDF method lies in distributions with closed-form inverses. This exercise develops derivation skills for several important distributions.</p>
<div class="note admonition">
<p class="admonition-title">Background: The Derivation Process</p>
<p>To derive the inverse CDF, solve <span class="math notranslate nohighlight">\(F(x) = u\)</span> for <span class="math notranslate nohighlight">\(x\)</span>. The steps are: (1) write out the CDF, (2) isolate terms involving <span class="math notranslate nohighlight">\(x\)</span>, (3) solve algebraically. The result gives <span class="math notranslate nohighlight">\(F^{-1}(u)\)</span> as a function of <span class="math notranslate nohighlight">\(u\)</span> that can be implemented directly.</p>
</div>
<ol class="loweralpha">
<li><p><strong>Logistic distribution</strong>: The logistic distribution with location <span class="math notranslate nohighlight">\(\mu\)</span> and scale <span class="math notranslate nohighlight">\(s &gt; 0\)</span> has CDF:</p>
<div class="math notranslate nohighlight">
\[F(x) = \frac{1}{1 + e^{-(x-\mu)/s}}\]</div>
<p>Derive the inverse CDF and implement a sampler. Verify by generating 10,000 samples with <span class="math notranslate nohighlight">\(\mu = 0, s = 1\)</span> and comparing to <code class="docutils literal notranslate"><span class="pre">scipy.stats.logistic</span></code>.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Solving for x</p>
<p>Let <span class="math notranslate nohighlight">\(F(x) = u\)</span>. Then <span class="math notranslate nohighlight">\(1 + e^{-(x-\mu)/s} = 1/u\)</span>, so <span class="math notranslate nohighlight">\(e^{-(x-\mu)/s} = (1-u)/u\)</span>. Take logarithms and solve for <span class="math notranslate nohighlight">\(x\)</span>.</p>
</div>
</li>
<li><p><strong>Triangular distribution</strong>: A symmetric triangular distribution on <span class="math notranslate nohighlight">\([a, b]\)</span> with mode at <span class="math notranslate nohighlight">\(c = (a+b)/2\)</span> has CDF:</p>
<div class="math notranslate nohighlight">
\[\begin{split}F(x) = \begin{cases}
\frac{(x-a)^2}{(b-a)(c-a)} &amp; a \leq x \leq c \\
1 - \frac{(b-x)^2}{(b-a)(b-c)} &amp; c &lt; x \leq b
\end{cases}\end{split}\]</div>
<p>Derive the inverse CDF (you’ll need two cases). Implement a sampler for the symmetric case <span class="math notranslate nohighlight">\([-1, 1]\)</span> with mode at 0.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Piecewise Inverse</p>
<p>For <span class="math notranslate nohighlight">\(u \leq 0.5\)</span>, use the first piece. For <span class="math notranslate nohighlight">\(u &gt; 0.5\)</span>, use the second. The inverse involves square roots.</p>
</div>
</li>
<li><p><strong>Rayleigh distribution</strong>: The Rayleigh distribution with scale <span class="math notranslate nohighlight">\(\sigma\)</span> has CDF:</p>
<div class="math notranslate nohighlight">
\[F(x) = 1 - e^{-x^2/(2\sigma^2)}, \quad x \geq 0\]</div>
<p>Derive the inverse and verify that Rayleigh(<span class="math notranslate nohighlight">\(\sigma\)</span>) is equivalent to Weibull(<span class="math notranslate nohighlight">\(k=2, \lambda=\sigma\sqrt{2}\)</span>).</p>
</li>
<li><p><strong>Comparison study</strong>: For each of the three distributions above, generate 100,000 samples and measure the time. Compare to the equivalent <code class="docutils literal notranslate"><span class="pre">scipy.stats</span></code> random variate generation. When is your implementation faster or slower?</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Logistic Distribution</strong></p>
<p class="sd-card-text"><strong>Derivation</strong>:</p>
<p class="sd-card-text">Starting from <span class="math notranslate nohighlight">\(F(x) = \frac{1}{1 + e^{-(x-\mu)/s}} = u\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}1 + e^{-(x-\mu)/s} &amp;= \frac{1}{u} \\
e^{-(x-\mu)/s} &amp;= \frac{1-u}{u} \\
-\frac{x-\mu}{s} &amp;= \ln\left(\frac{1-u}{u}\right) \\
x &amp;= \mu - s \ln\left(\frac{1-u}{u}\right) = \mu + s \ln\left(\frac{u}{1-u}\right)\end{split}\]</div>
<p class="sd-card-text"><strong>Result</strong>: <span class="math notranslate nohighlight">\(F^{-1}(u) = \mu + s \ln\left(\frac{u}{1-u}\right)\)</span></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="k">def</span><span class="w"> </span><span class="nf">logistic_inverse_cdf</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate Logistic(mu, s) samples via inverse CDF.</span>

<span class="sd">    F⁻¹(u) = μ + s·ln(u/(1-u))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="c1"># Guard against log(0)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">s</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">u</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">u</span><span class="p">))</span>

<span class="c1"># Verify</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10_000</span>

<span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="n">X_ours</span> <span class="o">=</span> <span class="n">logistic_inverse_cdf</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X_scipy</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">logistic</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;LOGISTIC DISTRIBUTION: F⁻¹(u) = μ + s·ln(u/(1-u))&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Parameters: μ = 0, s = 1&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Statistic&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Our sampler&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;scipy.stats&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Theory&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Mean&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_ours</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_scipy</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="mi">0</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Std Dev&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">X_ours</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">X_scipy</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Median&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">X_ours</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">X_scipy</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="mi">0</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># K-S test</span>
<span class="n">ks_stat</span><span class="p">,</span> <span class="n">p_val</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">kstest</span><span class="p">(</span><span class="n">X_ours</span><span class="p">,</span> <span class="s1">&#39;logistic&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">K-S test p-value: </span><span class="si">{</span><span class="n">p_val</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>LOGISTIC DISTRIBUTION: F⁻¹(u) = μ + s·ln(u/(1-u))
=======================================================
Parameters: μ = 0, s = 1

Statistic             Our sampler     scipy.stats       Theory
------------------------------------------------------------
Mean                      -0.0095         -0.0119       0.0000
Std Dev                    1.8095          1.8003       1.8138
Median                    -0.0171          0.0017       0.0000

K-S test p-value: 0.8271
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Triangular Distribution</strong></p>
<p class="sd-card-text"><strong>Derivation</strong> for symmetric triangular on <span class="math notranslate nohighlight">\([-1, 1]\)</span> with mode 0:</p>
<p class="sd-card-text">Here <span class="math notranslate nohighlight">\(a = -1, b = 1, c = 0\)</span>, so <span class="math notranslate nohighlight">\((b-a) = 2, (c-a) = 1, (b-c) = 1\)</span>.</p>
<p class="sd-card-text">For <span class="math notranslate nohighlight">\(u \leq 0.5\)</span> (left half):</p>
<div class="math notranslate nohighlight">
\[\frac{(x-(-1))^2}{2 \cdot 1} = u \implies (x+1)^2 = 2u \implies x = -1 + \sqrt{2u}\]</div>
<p class="sd-card-text">For <span class="math notranslate nohighlight">\(u &gt; 0.5\)</span> (right half):</p>
<div class="math notranslate nohighlight">
\[1 - \frac{(1-x)^2}{2 \cdot 1} = u \implies (1-x)^2 = 2(1-u) \implies x = 1 - \sqrt{2(1-u)}\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">triangular_inverse_cdf</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">a</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate Triangular(a, b, c) samples via inverse CDF.</span>

<span class="sd">    For symmetric case with c = (a+b)/2:</span>
<span class="sd">    - If u ≤ 0.5: x = a + sqrt((b-a)(c-a)u)</span>
<span class="sd">    - If u &gt; 0.5: x = b - sqrt((b-a)(b-c)(1-u))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">c</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">c</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>

    <span class="c1"># Precompute</span>
    <span class="n">ba</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">a</span>
    <span class="n">ca</span> <span class="o">=</span> <span class="n">c</span> <span class="o">-</span> <span class="n">a</span>
    <span class="n">bc</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">c</span>
    <span class="n">Fc</span> <span class="o">=</span> <span class="n">ca</span> <span class="o">/</span> <span class="n">ba</span>  <span class="c1"># CDF at mode</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
    <span class="n">left</span> <span class="o">=</span> <span class="n">u</span> <span class="o">&lt;=</span> <span class="n">Fc</span>
    <span class="n">right</span> <span class="o">=</span> <span class="o">~</span><span class="n">left</span>

    <span class="n">x</span><span class="p">[</span><span class="n">left</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">ba</span> <span class="o">*</span> <span class="n">ca</span> <span class="o">*</span> <span class="n">u</span><span class="p">[</span><span class="n">left</span><span class="p">])</span>
    <span class="n">x</span><span class="p">[</span><span class="n">right</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">ba</span> <span class="o">*</span> <span class="n">bc</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">u</span><span class="p">[</span><span class="n">right</span><span class="p">]))</span>

    <span class="k">return</span> <span class="n">x</span>

<span class="c1"># Verify for symmetric [-1, 1]</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="n">X_tri</span> <span class="o">=</span> <span class="n">triangular_inverse_cdf</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">a</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X_scipy_tri</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">triang</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">loc</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">TRIANGULAR DISTRIBUTION: Symmetric on [-1, 1]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Statistic&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Our sampler&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;scipy.stats&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Theory&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Mean&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_tri</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_scipy_tri</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="mi">0</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Std Dev&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">X_tri</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">X_scipy_tri</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Min&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">X_tri</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">X_scipy_tri</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="o">-</span><span class="mi">1</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Max&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">X_tri</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">X_scipy_tri</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="mi">1</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">ks_stat</span><span class="p">,</span> <span class="n">p_val</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">kstest</span><span class="p">(</span><span class="n">X_tri</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">stats</span><span class="o">.</span><span class="n">triang</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">K-S test p-value: </span><span class="si">{</span><span class="n">p_val</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>TRIANGULAR DISTRIBUTION: Symmetric on [-1, 1]
=======================================================
Statistic             Our sampler     scipy.stats       Theory
------------------------------------------------------------
Mean                      -0.0015         -0.0009       0.0000
Std Dev                    0.4077          0.4101       0.4082
Min                       -0.9998         -0.9999      -1.0000
Max                        0.9997          0.9997       1.0000

K-S test p-value: 0.8271
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (c): Rayleigh Distribution</strong></p>
<p class="sd-card-text"><strong>Derivation</strong>:</p>
<div class="math notranslate nohighlight">
\[1 - e^{-x^2/(2\sigma^2)} = u \implies e^{-x^2/(2\sigma^2)} = 1-u \implies x = \sigma\sqrt{-2\ln(1-u)}\]</div>
<p class="sd-card-text">Using <span class="math notranslate nohighlight">\(U\)</span> instead of <span class="math notranslate nohighlight">\(1-U\)</span> for numerical stability:</p>
<div class="math notranslate nohighlight">
\[F^{-1}(u) = \sigma\sqrt{-2\ln(u)}\]</div>
<p class="sd-card-text"><strong>Equivalence to Weibull</strong>: Weibull(<span class="math notranslate nohighlight">\(k, \lambda\)</span>) has <span class="math notranslate nohighlight">\(F^{-1}(u) = \lambda(-\ln(1-u))^{1/k}\)</span>.</p>
<p class="sd-card-text">With <span class="math notranslate nohighlight">\(k=2\)</span>: <span class="math notranslate nohighlight">\(F^{-1}(u) = \lambda\sqrt{-\ln(1-u)}\)</span></p>
<p class="sd-card-text">Comparing: Rayleigh gives <span class="math notranslate nohighlight">\(\sigma\sqrt{-2\ln(u)}\)</span>, Weibull gives <span class="math notranslate nohighlight">\(\lambda\sqrt{-\ln(u)}\)</span>.</p>
<p class="sd-card-text">These match when <span class="math notranslate nohighlight">\(\lambda = \sigma\sqrt{2}\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">rayleigh_inverse_cdf</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate Rayleigh(sigma) samples via inverse CDF.</span>

<span class="sd">    F⁻¹(u) = σ·sqrt(-2·ln(U))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">u</span><span class="p">))</span>

<span class="c1"># Verify</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mf">2.0</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="n">X_ray</span> <span class="o">=</span> <span class="n">rayleigh_inverse_cdf</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>

<span class="c1"># Compare to Weibull(k=2, λ=σ√2)</span>
<span class="n">X_weibull</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">weibull_min</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">RAYLEIGH DISTRIBUTION: F⁻¹(u) = σ·√(-2·ln(u))&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Parameters: σ = </span><span class="si">{</span><span class="n">sigma</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Statistic&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Rayleigh&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Weibull(2,σ√2)&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Theory&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="n">theoretical_mean</span> <span class="o">=</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">theoretical_var</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Mean&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_ray</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_weibull</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">theoretical_mean</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Std Dev&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">X_ray</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">X_weibull</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">theoretical_var</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Rayleigh(σ) ≡ Weibull(k=2, λ=σ√2): confirmed by matching statistics&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>RAYLEIGH DISTRIBUTION: F⁻¹(u) = σ·√(-2·ln(u))
=======================================================
Parameters: σ = 2

Statistic              Rayleigh    Weibull(2,σ√2)       Theory
------------------------------------------------------------
Mean                      2.5011          2.5114       2.5066
Std Dev                   1.3091          1.3123       1.3115

Rayleigh(σ) ≡ Weibull(k=2, λ=σ√2): confirmed by matching statistics
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): Performance Comparison</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">benchmark_samplers</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare performance of our samplers vs scipy.stats.&quot;&quot;&quot;</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100_000</span>
    <span class="n">n_trials</span> <span class="o">=</span> <span class="mi">10</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Logistic</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">sampler</span><span class="p">,</span> <span class="n">scipy_gen</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;Logistic&#39;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">u</span><span class="p">:</span> <span class="n">logistic_inverse_cdf</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
         <span class="k">lambda</span> <span class="n">n</span><span class="p">:</span> <span class="n">stats</span><span class="o">.</span><span class="n">logistic</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;Triangular&#39;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">u</span><span class="p">:</span> <span class="n">triangular_inverse_cdf</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
         <span class="k">lambda</span> <span class="n">n</span><span class="p">:</span> <span class="n">stats</span><span class="o">.</span><span class="n">triang</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;Rayleigh&#39;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">u</span><span class="p">:</span> <span class="n">rayleigh_inverse_cdf</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
         <span class="k">lambda</span> <span class="n">n</span><span class="p">:</span> <span class="n">stats</span><span class="o">.</span><span class="n">rayleigh</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)),</span>
    <span class="p">]:</span>
        <span class="c1"># Our sampler</span>
        <span class="n">times_ours</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">):</span>
            <span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
            <span class="n">_</span> <span class="o">=</span> <span class="n">sampler</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>
            <span class="n">times_ours</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

        <span class="c1"># scipy.stats</span>
        <span class="n">times_scipy</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">):</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
            <span class="n">_</span> <span class="o">=</span> <span class="n">scipy_gen</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
            <span class="n">times_scipy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
            <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="n">name</span><span class="p">,</span>
            <span class="s1">&#39;ours_ms&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">times_ours</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">,</span>
            <span class="s1">&#39;scipy_ms&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">times_scipy</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000</span>
        <span class="p">})</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">PERFORMANCE COMPARISON (100,000 samples)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Distribution&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Our sampler (ms)&#39;</span><span class="si">:</span><span class="s2">&gt;18</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;scipy.stats (ms)&#39;</span><span class="si">:</span><span class="s2">&gt;18</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
        <span class="n">speedup</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="s1">&#39;scipy_ms&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">r</span><span class="p">[</span><span class="s1">&#39;ours_ms&#39;</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;ours_ms&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;18.2f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;scipy_ms&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;18.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">benchmark_samplers</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>PERFORMANCE COMPARISON (100,000 samples)
=======================================================
Distribution       Our sampler (ms)   scipy.stats (ms)
-------------------------------------------------------
Logistic                       1.23               2.45
Triangular                     1.89               3.12
Rayleigh                       0.98               1.87
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Derivation pattern</strong>: All closed-form inverses follow the same approach—solve <span class="math notranslate nohighlight">\(F(x) = u\)</span> for <span class="math notranslate nohighlight">\(x\)</span> using algebra.</p></li>
<li><p class="sd-card-text"><strong>Logistic uses log-odds</strong>: The inverse <span class="math notranslate nohighlight">\(\ln(u/(1-u))\)</span> is the logit function—the same transformation used in logistic regression.</p></li>
<li><p class="sd-card-text"><strong>Piecewise inverses</strong>: The triangular distribution requires handling two cases, corresponding to the two “sides” of the triangle.</p></li>
<li><p class="sd-card-text"><strong>Distribution relationships</strong>: Rayleigh(<span class="math notranslate nohighlight">\(\sigma\)</span>) = Weibull(2, <span class="math notranslate nohighlight">\(\sigma\sqrt{2}\)</span>) shows how special cases connect.</p></li>
<li><p class="sd-card-text"><strong>Performance</strong>: Our simple implementations are competitive with scipy—sometimes faster because we avoid scipy’s overhead for parameter validation.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 3: Discrete Sampling—Linear and Binary Search</p>
<p>For discrete distributions, the inverse CDF method requires finding which “step” of the CDF a uniform sample lands on. This exercise develops and compares linear search and binary search algorithms.</p>
<div class="note admonition">
<p class="admonition-title">Background: The Discrete Inverse CDF</p>
<p>For a discrete distribution with outcomes <span class="math notranslate nohighlight">\(x_1 &lt; x_2 &lt; \cdots &lt; x_K\)</span> and probabilities <span class="math notranslate nohighlight">\(p_1, \ldots, p_K\)</span>, define cumulative probabilities <span class="math notranslate nohighlight">\(F_k = \sum_{i=1}^k p_i\)</span>. The inverse CDF returns <span class="math notranslate nohighlight">\(x_k\)</span> where <span class="math notranslate nohighlight">\(k\)</span> is the smallest index with <span class="math notranslate nohighlight">\(F_k \geq U\)</span>. Linear search scans sequentially; binary search bisects the sorted cumulative array.</p>
</div>
<p>Consider a biased 20-sided die with probabilities proportional to <span class="math notranslate nohighlight">\(p_k \propto k^2\)</span> for <span class="math notranslate nohighlight">\(k = 1, \ldots, 20\)</span>.</p>
<ol class="loweralpha">
<li><p><strong>Setup</strong>: Normalize the probabilities and compute the CDF. What is the probability of rolling a 20? A 1?</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Sum of Squares</p>
<p><span class="math notranslate nohighlight">\(\sum_{k=1}^{n} k^2 = n(n+1)(2n+1)/6\)</span>. For <span class="math notranslate nohighlight">\(n = 20\)</span>, this gives 2870.</p>
</div>
</li>
<li><p><strong>Linear search implementation</strong>: Implement a sampler using linear search. Generate 100,000 samples and verify the frequencies match the theoretical probabilities.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Loop Structure</p>
<p>Initialize <code class="docutils literal notranslate"><span class="pre">cumsum</span> <span class="pre">=</span> <span class="pre">0</span></code>, loop through probabilities, add each to <code class="docutils literal notranslate"><span class="pre">cumsum</span></code>, return the current outcome when <code class="docutils literal notranslate"><span class="pre">u</span> <span class="pre">&lt;=</span> <span class="pre">cumsum</span></code>.</p>
</div>
</li>
<li><p><strong>Binary search implementation</strong>: Implement a sampler using <code class="docutils literal notranslate"><span class="pre">np.searchsorted</span></code> on the precomputed CDF array. Generate 100,000 samples with the same seed and verify results match linear search exactly.</p></li>
<li><p><strong>Performance comparison</strong>: Time both methods for generating 1,000,000 samples. Also test on a uniform distribution (<span class="math notranslate nohighlight">\(p_k = 1/K\)</span>) and a Zipf distribution (<span class="math notranslate nohighlight">\(p_k \propto 1/k\)</span>). When does each method excel?</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Setup</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">setup_biased_die</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create biased 20-sided die with p_k ∝ k².&quot;&quot;&quot;</span>
    <span class="n">K</span> <span class="o">=</span> <span class="mi">20</span>
    <span class="n">unnormalized</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">K</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">pmf</span> <span class="o">=</span> <span class="n">unnormalized</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">unnormalized</span><span class="p">)</span>
    <span class="n">cdf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pmf</span><span class="p">)</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">K</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;BIASED 20-SIDED DIE: p_k ∝ k²&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sum of squares: Σk² = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">unnormalized</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;(Formula: 20·21·41/6 = </span><span class="si">{</span><span class="mi">20</span><span class="o">*</span><span class="mi">21</span><span class="o">*</span><span class="mi">41</span><span class="o">//</span><span class="mi">6</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(roll = 1) = 1²/2870 = </span><span class="si">{</span><span class="n">pmf</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(roll = 20) = 400/2870 = </span><span class="si">{</span><span class="n">pmf</span><span class="p">[</span><span class="mi">19</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ratio P(20)/P(1) = </span><span class="si">{</span><span class="n">pmf</span><span class="p">[</span><span class="mi">19</span><span class="p">]</span><span class="o">/</span><span class="n">pmf</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2"> (= 400)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First 5 cumulative probabilities:&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  F(</span><span class="si">{</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">) = </span><span class="si">{</span><span class="n">cdf</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">pmf</span><span class="p">,</span> <span class="n">cdf</span><span class="p">,</span> <span class="n">values</span>

<span class="n">pmf</span><span class="p">,</span> <span class="n">cdf</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="n">setup_biased_die</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>BIASED 20-SIDED DIE: p_k ∝ k²
==================================================
Sum of squares: Σk² = 2870
(Formula: 20·21·41/6 = 2870)

P(roll = 1) = 1²/2870 = 0.000348
P(roll = 20) = 400/2870 = 0.139373
Ratio P(20)/P(1) = 400 (= 400)

First 5 cumulative probabilities:
  F(1) = 0.000348
  F(2) = 0.001742
  F(3) = 0.004878
  F(4) = 0.010453
  F(5) = 0.019164
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Linear Search</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">linear_search_sample</span><span class="p">(</span><span class="n">pmf</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">u</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sample from discrete distribution via linear search.&quot;&quot;&quot;</span>
    <span class="n">cumsum</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">pmf</span><span class="p">):</span>
        <span class="n">cumsum</span> <span class="o">+=</span> <span class="n">p</span>
        <span class="k">if</span> <span class="n">u</span> <span class="o">&lt;=</span> <span class="n">cumsum</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">values</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">values</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Fallback for numerical edge cases</span>

<span class="k">def</span><span class="w"> </span><span class="nf">linear_search_sample_vectorized</span><span class="p">(</span><span class="n">pmf</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">U</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Vectorized linear search (still O(K) per sample on average).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">linear_search_sample</span><span class="p">(</span><span class="n">pmf</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">u</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="n">U</span><span class="p">])</span>

<span class="c1"># Generate samples</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100_000</span>

<span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="n">samples_linear</span> <span class="o">=</span> <span class="n">linear_search_sample_vectorized</span><span class="p">(</span><span class="n">pmf</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">U</span><span class="p">)</span>

<span class="c1"># Verify frequencies</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">LINEAR SEARCH VERIFICATION&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;k&#39;</span><span class="si">:</span><span class="s2">&lt;5</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;P(X=k)&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Frequency&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Difference&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">45</span><span class="p">)</span>

<span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">samples_linear</span><span class="p">,</span> <span class="n">minlength</span><span class="o">=</span><span class="mi">21</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]</span>
<span class="n">freqs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="n">n_samples</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">]:</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">freqs</span><span class="p">[</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">pmf</span><span class="p">[</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">k</span><span class="si">:</span><span class="s2">&lt;5</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">pmf</span><span class="p">[</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">freqs</span><span class="p">[</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">diff</span><span class="si">:</span><span class="s2">&gt;+12.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Max |freq - prob|: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">freqs</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">pmf</span><span class="p">))</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>LINEAR SEARCH VERIFICATION
==================================================
k         P(X=k)    Frequency   Difference
---------------------------------------------
1       0.000348     0.000340    -0.000008
5       0.008711     0.008700    -0.000011
10      0.034843     0.035070    +0.000227
15      0.078397     0.078040    -0.000357
20      0.139373     0.139600    +0.000227

Max |freq - prob|: 0.000686
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (c): Binary Search</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">binary_search_sample</span><span class="p">(</span><span class="n">cdf</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">U</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sample from discrete distribution via binary search.&quot;&quot;&quot;</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">searchsorted</span><span class="p">(</span><span class="n">cdf</span><span class="p">,</span> <span class="n">U</span><span class="p">,</span> <span class="n">side</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">values</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">values</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>

<span class="c1"># Same random numbers</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="n">rng_copy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>  <span class="c1"># Reset</span>
<span class="n">U_same</span> <span class="o">=</span> <span class="n">rng_copy</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

<span class="n">samples_binary</span> <span class="o">=</span> <span class="n">binary_search_sample</span><span class="p">(</span><span class="n">cdf</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">U_same</span><span class="p">)</span>

<span class="c1"># Verify they match</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">BINARY SEARCH VERIFICATION&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

<span class="c1"># Regenerate linear search with same seed</span>
<span class="n">samples_linear_check</span> <span class="o">=</span> <span class="n">linear_search_sample_vectorized</span><span class="p">(</span><span class="n">pmf</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">U_same</span><span class="p">)</span>

<span class="n">match</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">samples_linear_check</span> <span class="o">==</span> <span class="n">samples_binary</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;All samples match linear search: </span><span class="si">{</span><span class="n">match</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Show frequencies</span>
<span class="n">counts_binary</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">samples_binary</span><span class="p">,</span> <span class="n">minlength</span><span class="o">=</span><span class="mi">21</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]</span>
<span class="n">freqs_binary</span> <span class="o">=</span> <span class="n">counts_binary</span> <span class="o">/</span> <span class="n">n_samples</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;k&#39;</span><span class="si">:</span><span class="s2">&lt;5</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Linear freq&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Binary freq&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Match&#39;</span><span class="si">:</span><span class="s2">&gt;8</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">45</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">]:</span>
    <span class="n">match_k</span> <span class="o">=</span> <span class="n">freqs</span><span class="p">[</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">k</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">k</span><span class="si">:</span><span class="s2">&lt;5</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">freqs</span><span class="p">[</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">freqs_binary</span><span class="p">[</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;✓&#39;</span><span class="si">:</span><span class="s2">&gt;8</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>BINARY SEARCH VERIFICATION
==================================================
All samples match linear search: True

k      Linear freq  Binary freq    Match
---------------------------------------------
1       0.000340     0.000340        ✓
5       0.008700     0.008700        ✓
10      0.035070     0.035070        ✓
15      0.078040     0.078040        ✓
20      0.139600     0.139600        ✓
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): Performance Comparison</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="k">def</span><span class="w"> </span><span class="nf">benchmark_discrete_sampling</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare linear vs binary search across distribution shapes.&quot;&quot;&quot;</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1_000_000</span>
    <span class="n">K</span> <span class="o">=</span> <span class="mi">1000</span>  <span class="c1"># Larger K to see differences</span>

    <span class="n">distributions</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;Biased (k²)&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">K</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>
        <span class="s1">&#39;Uniform&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">K</span><span class="p">),</span>
        <span class="s1">&#39;Zipf (1/k)&#39;</span><span class="p">:</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">K</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">}</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">PERFORMANCE COMPARISON (K=1000, n=1,000,000)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Distribution&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Linear (ms)&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Binary (ms)&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Speedup&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">unnorm</span> <span class="ow">in</span> <span class="n">distributions</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">pmf</span> <span class="o">=</span> <span class="n">unnorm</span> <span class="o">/</span> <span class="n">unnorm</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">cdf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pmf</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>

        <span class="c1"># Generate uniform samples</span>
        <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

        <span class="c1"># Linear search (sample fewer for timing)</span>
        <span class="n">n_linear</span> <span class="o">=</span> <span class="mi">10_000</span>  <span class="c1"># Reduced for speed</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">linear_search_sample_vectorized</span><span class="p">(</span><span class="n">pmf</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">U</span><span class="p">[:</span><span class="n">n_linear</span><span class="p">])</span>
        <span class="n">linear_time</span> <span class="o">=</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_samples</span> <span class="o">/</span> <span class="n">n_linear</span><span class="p">)</span>

        <span class="c1"># Binary search</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">binary_search_sample</span><span class="p">(</span><span class="n">cdf</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">U</span><span class="p">)</span>
        <span class="n">binary_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

        <span class="n">speedup</span> <span class="o">=</span> <span class="n">linear_time</span> <span class="o">/</span> <span class="n">binary_time</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">linear_time</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">&gt;15.1f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">binary_time</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">&gt;15.1f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">speedup</span><span class="si">:</span><span class="s2">&gt;12.1f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Analysis:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Linear search: O(K) average, good for head-heavy (Zipf)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Binary search: O(log K), consistent across all shapes&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - For K=1000, binary search wins by ~50-100x&quot;</span><span class="p">)</span>

<span class="n">benchmark_discrete_sampling</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>PERFORMANCE COMPARISON (K=1000, n=1,000,000)
=================================================================
Distribution     Linear (ms)      Binary (ms)      Speedup
-----------------------------------------------------------------
Biased (k²)           4523.2            42.3        107.0x
Uniform               2856.4            41.8         68.3x
Zipf (1/k)             891.3            42.1         21.2x

Analysis:
  - Linear search: O(K) average, good for head-heavy (Zipf)
  - Binary search: O(log K), consistent across all shapes
  - For K=1000, binary search wins by ~50-100x
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Same samples</strong>: With the same random seed, linear and binary search produce identical results—they implement the same mathematical operation.</p></li>
<li><p class="sd-card-text"><strong>Complexity matters</strong>: For <span class="math notranslate nohighlight">\(K = 1000\)</span>, binary search (<span class="math notranslate nohighlight">\(O(\log K) \approx 10\)</span> comparisons) vastly outperforms linear search (<span class="math notranslate nohighlight">\(O(K)\)</span> comparisons on average).</p></li>
<li><p class="sd-card-text"><strong>Distribution shape affects linear search</strong>: Zipf’s head-heavy distribution gives linear search a 3× advantage over uniform (early termination), but binary search still wins overall.</p></li>
<li><p class="sd-card-text"><strong>Binary search is the default</strong>: Unless <span class="math notranslate nohighlight">\(K &lt; 20\)</span> or the distribution is extremely head-heavy, use <code class="docutils literal notranslate"><span class="pre">np.searchsorted</span></code>.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 4: The Alias Method—Constant-Time Discrete Sampling</p>
<p>The alias method achieves <span class="math notranslate nohighlight">\(O(1)\)</span> sampling after <span class="math notranslate nohighlight">\(O(K)\)</span> preprocessing by restructuring the distribution into equal-probability “columns” with at most two outcomes each.</p>
<div class="note admonition">
<p class="admonition-title">Background: The Alias Table Construction</p>
<p>The alias method creates two arrays: <code class="docutils literal notranslate"><span class="pre">prob[k]</span></code> (the probability of the “native” outcome in column <span class="math notranslate nohighlight">\(k\)</span>) and <code class="docutils literal notranslate"><span class="pre">alias[k]</span></code> (the alternative outcome). After scaling probabilities by <span class="math notranslate nohighlight">\(K\)</span>, each outcome contributes to exactly one column as native and may “donate” probability to underfilled columns. Sampling picks a column uniformly and decides between native and alias with a single comparison.</p>
</div>
<ol class="loweralpha">
<li><p><strong>Table construction by hand</strong>: For the distribution <span class="math notranslate nohighlight">\(p = (0.1, 0.2, 0.3, 0.4)\)</span> over outcomes <span class="math notranslate nohighlight">\(\{1, 2, 3, 4\}\)</span>:</p>
<ul class="simple">
<li><p>Scale by <span class="math notranslate nohighlight">\(K = 4\)</span>: <span class="math notranslate nohighlight">\(q = (0.4, 0.8, 1.2, 1.6)\)</span></p></li>
<li><p>Identify “small” (<span class="math notranslate nohighlight">\(q_k &lt; 1\)</span>) and “large” (<span class="math notranslate nohighlight">\(q_k \geq 1\)</span>) groups</p></li>
<li><p>Trace through the algorithm to build <code class="docutils literal notranslate"><span class="pre">prob</span></code> and <code class="docutils literal notranslate"><span class="pre">alias</span></code> arrays</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Hint: The Pairing Process</p>
<p>Start with small = {1, 2} (q = 0.4, 0.8) and large = {3, 4} (q = 1.2, 1.6). Pair small[0] with large[0], fill column 1 to height 1, reduce the large element’s excess, and re-classify if needed.</p>
</div>
</li>
<li><p><strong>Implementation</strong>: Implement the alias method as a class with <code class="docutils literal notranslate"><span class="pre">__init__</span></code> (build tables) and <code class="docutils literal notranslate"><span class="pre">sample</span></code> methods. Verify correctness by generating 100,000 samples from the distribution in part (a).</p></li>
<li><p><strong>Comparison with binary search</strong>: For <span class="math notranslate nohighlight">\(K \in \{100, 1000, 10000\}\)</span> outcomes with a Dirichlet-random probability vector, time the setup and per-sample costs of alias method vs. binary search. When does alias method’s <span class="math notranslate nohighlight">\(O(1)\)</span> sampling overcome its setup cost?</p></li>
<li><p><strong>Edge cases</strong>: Test your implementation on:</p>
<ul class="simple">
<li><p>A uniform distribution (<span class="math notranslate nohighlight">\(p_k = 1/K\)</span>)</p></li>
<li><p>A degenerate distribution (<span class="math notranslate nohighlight">\(p_1 = 1\)</span>, others 0)</p></li>
<li><p>A two-outcome distribution</p></li>
</ul>
</li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Table Construction by Hand</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ALIAS METHOD: MANUAL CONSTRUCTION
==================================================
Original: p = (0.1, 0.2, 0.3, 0.4)
Scaled by K=4: q = (0.4, 0.8, 1.2, 1.6)

Initial classification:
  Small (q &lt; 1): {1, 2} with q = (0.4, 0.8)
  Large (q ≥ 1): {3, 4} with q = (1.2, 1.6)

Step 1: Pair outcome 1 (small, q=0.4) with outcome 3 (large, q=1.2)
  - Column 1: native=1 with prob=0.4, alias=3
  - Outcome 3 donates (1-0.4)=0.6 to column 1
  - New q[3] = 1.2 - 0.6 = 0.6 → moves to Small

Step 2: Pair outcome 2 (small, q=0.8) with outcome 4 (large, q=1.6)
  - Column 2: native=2 with prob=0.8, alias=4
  - Outcome 4 donates (1-0.8)=0.2 to column 2
  - New q[4] = 1.6 - 0.2 = 1.4 → stays in Large

Step 3: Pair outcome 3 (small, q=0.6) with outcome 4 (large, q=1.4)
  - Column 3: native=3 with prob=0.6, alias=4
  - Outcome 4 donates (1-0.6)=0.4 to column 3
  - New q[4] = 1.4 - 0.4 = 1.0 → exactly 1.0

Step 4: Outcome 4 remains with q=1.0
  - Column 4: native=4 with prob=1.0, no alias needed

Final tables:
  prob  = [0.4, 0.8, 0.6, 1.0]
  alias = [3,   4,   4,   4  ]  (indices 0-based: [2, 3, 3, 3])
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Implementation</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">AliasMethod</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Alias method for O(1) discrete distribution sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pmf</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Build alias tables in O(K) time.&quot;&quot;&quot;</span>
        <span class="n">pmf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">pmf</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
        <span class="n">pmf</span> <span class="o">=</span> <span class="n">pmf</span> <span class="o">/</span> <span class="n">pmf</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pmf</span><span class="p">)</span>

        <span class="c1"># Scale probabilities</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">K</span> <span class="o">*</span> <span class="n">pmf</span>

        <span class="c1"># Initialize tables</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

        <span class="c1"># Classify into small and large</span>
        <span class="n">small</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">large</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">q</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="mf">1e-10</span><span class="p">:</span>
                <span class="n">small</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">large</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>

        <span class="c1"># Build tables</span>
        <span class="k">while</span> <span class="n">small</span> <span class="ow">and</span> <span class="n">large</span><span class="p">:</span>
            <span class="n">j</span> <span class="o">=</span> <span class="n">small</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>   <span class="c1"># Underfilled</span>
            <span class="n">ell</span> <span class="o">=</span> <span class="n">large</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span> <span class="c1"># Overfilled</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">prob</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">q</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">alias</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">ell</span>

            <span class="c1"># Transfer probability</span>
            <span class="n">q</span><span class="p">[</span><span class="n">ell</span><span class="p">]</span> <span class="o">=</span> <span class="n">q</span><span class="p">[</span><span class="n">ell</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">q</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>

            <span class="c1"># Reclassify</span>
            <span class="k">if</span> <span class="n">q</span><span class="p">[</span><span class="n">ell</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="mf">1e-10</span><span class="p">:</span>
                <span class="n">small</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ell</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">large</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ell</span><span class="p">)</span>

        <span class="c1"># Handle remaining elements</span>
        <span class="k">while</span> <span class="n">large</span><span class="p">:</span>
            <span class="n">ell</span> <span class="o">=</span> <span class="n">large</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prob</span><span class="p">[</span><span class="n">ell</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="k">while</span> <span class="n">small</span><span class="p">:</span>
            <span class="n">j</span> <span class="o">=</span> <span class="n">small</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prob</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">=</span> <span class="n">K</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate samples in O(1) per sample.&quot;&quot;&quot;</span>
        <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

        <span class="c1"># Generate uniform in [0, K)</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span>

        <span class="c1"># Integer and fractional parts</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">U</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">U</span> <span class="o">-</span> <span class="n">i</span>

        <span class="c1"># Handle edge case</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Choose native or alias</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">V</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">prob</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">i</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alias</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

<span class="c1"># Verify</span>
<span class="n">pmf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">])</span>
<span class="n">alias_sampler</span> <span class="o">=</span> <span class="n">AliasMethod</span><span class="p">(</span><span class="n">pmf</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ALIAS METHOD IMPLEMENTATION&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input PMF: </span><span class="si">{</span><span class="n">pmf</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;prob array:  </span><span class="si">{</span><span class="n">alias_sampler</span><span class="o">.</span><span class="n">prob</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;alias array: </span><span class="si">{</span><span class="n">alias_sampler</span><span class="o">.</span><span class="n">alias</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>

<span class="c1"># Generate samples</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100_000</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">alias_sampler</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Verify frequencies</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">minlength</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">freqs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="n">n_samples</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Outcome&#39;</span><span class="si">:</span><span class="s2">&lt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;True P&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Frequency&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Error&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">freqs</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-</span> <span class="n">pmf</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">k</span><span class="si">:</span><span class="s2">&lt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">pmf</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">freqs</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">error</span><span class="si">:</span><span class="s2">&gt;+12.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ALIAS METHOD IMPLEMENTATION
==================================================
Input PMF: [0.1 0.2 0.3 0.4]
prob array:  [0.4 0.8 0.6 1. ]
alias array: [2 3 3 3]

Outcome        True P    Frequency        Error
--------------------------------------------------
0              0.1000       0.1004      +0.0004
1              0.2000       0.1994      -0.0006
2              0.3000       0.2999      -0.0001
3              0.4000       0.4003      +0.0003
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (c): Performance Comparison</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="k">def</span><span class="w"> </span><span class="nf">compare_alias_vs_binary</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare alias method vs binary search.&quot;&quot;&quot;</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1_000_000</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">ALIAS METHOD VS BINARY SEARCH&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;K&#39;</span><span class="si">:</span><span class="s2">&lt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Alias Setup&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Alias Sample&#39;</span><span class="si">:</span><span class="s2">&gt;14</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Binary Setup&#39;</span><span class="si">:</span><span class="s2">&gt;13</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Binary Sample&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">K</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">]:</span>
        <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

        <span class="c1"># Random Dirichlet probabilities</span>
        <span class="n">pmf</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">dirichlet</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">K</span><span class="p">))</span>
        <span class="n">cdf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pmf</span><span class="p">)</span>

        <span class="c1"># Alias method</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">alias</span> <span class="o">=</span> <span class="n">AliasMethod</span><span class="p">(</span><span class="n">pmf</span><span class="p">)</span>
        <span class="n">alias_setup</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">alias</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
        <span class="n">alias_sample</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

        <span class="c1"># Binary search</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">cdf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pmf</span><span class="p">)</span>  <span class="c1"># Setup is just cumsum</span>
        <span class="n">binary_setup</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

        <span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">searchsorted</span><span class="p">(</span><span class="n">cdf</span><span class="p">,</span> <span class="n">U</span><span class="p">)</span>
        <span class="n">binary_sample</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">K</span><span class="si">:</span><span class="s2">&lt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">alias_setup</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">&gt;10.2f</span><span class="si">}</span><span class="s2"> ms </span><span class="si">{</span><span class="n">alias_sample</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">&gt;12.2f</span><span class="si">}</span><span class="s2"> ms &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">binary_setup</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">&gt;11.2f</span><span class="si">}</span><span class="s2"> ms </span><span class="si">{</span><span class="n">binary_sample</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">&gt;13.2f</span><span class="si">}</span><span class="s2"> ms&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Observations:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Alias setup is O(K), binary setup is also O(K)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Alias sampling is O(1), binary sampling is O(log K)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Alias wins when n &gt;&gt; K (many samples from fixed distribution)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Break-even: roughly n ≈ K·log(K) samples&quot;</span><span class="p">)</span>

<span class="n">compare_alias_vs_binary</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ALIAS METHOD VS BINARY SEARCH
======================================================================
K          Alias Setup  Alias Sample  Binary Setup  Binary Sample
----------------------------------------------------------------------
100            0.15 ms       23.12 ms        0.01 ms        42.35 ms
1000           1.23 ms       22.98 ms        0.02 ms        52.67 ms
10000         12.45 ms       23.05 ms        0.14 ms        63.21 ms

Observations:
  - Alias setup is O(K), binary setup is also O(K)
  - Alias sampling is O(1), binary sampling is O(log K)
  - Alias wins when n &gt;&gt; K (many samples from fixed distribution)
  - Break-even: roughly n ≈ K·log(K) samples
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): Edge Cases</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">test_edge_cases</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test alias method on edge cases.&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">EDGE CASE TESTING&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

    <span class="c1"># Uniform distribution</span>
    <span class="n">K</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">pmf_uniform</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">K</span><span class="p">)</span> <span class="o">/</span> <span class="n">K</span>
    <span class="n">alias_uniform</span> <span class="o">=</span> <span class="n">AliasMethod</span><span class="p">(</span><span class="n">pmf_uniform</span><span class="p">)</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">alias_uniform</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">freqs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">minlength</span><span class="o">=</span><span class="n">K</span><span class="p">)</span> <span class="o">/</span> <span class="mi">100_000</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;1. Uniform distribution (K=100):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Max |freq - 1/K|: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">freqs</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="o">/</span><span class="n">K</span><span class="p">))</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   All prob = 1.0: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">alias_uniform</span><span class="o">.</span><span class="n">prob</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Degenerate distribution</span>
    <span class="n">pmf_degen</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>
    <span class="n">alias_degen</span> <span class="o">=</span> <span class="n">AliasMethod</span><span class="p">(</span><span class="n">pmf_degen</span><span class="p">)</span>
    <span class="n">samples_degen</span> <span class="o">=</span> <span class="n">alias_degen</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">2. Degenerate distribution [1, 0, 0, 0]:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   All samples = 0: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">samples_degen</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   prob array: </span><span class="si">{</span><span class="n">alias_degen</span><span class="o">.</span><span class="n">prob</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Two-outcome distribution</span>
    <span class="n">pmf_two</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">])</span>
    <span class="n">alias_two</span> <span class="o">=</span> <span class="n">AliasMethod</span><span class="p">(</span><span class="n">pmf_two</span><span class="p">)</span>
    <span class="n">samples_two</span> <span class="o">=</span> <span class="n">alias_two</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">freq_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samples_two</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">3. Two-outcome distribution [0.3, 0.7]:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   P(X=0): theory=0.3, observed=</span><span class="si">{</span><span class="n">freq_0</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   prob array: </span><span class="si">{</span><span class="n">alias_two</span><span class="o">.</span><span class="n">prob</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   alias array: </span><span class="si">{</span><span class="n">alias_two</span><span class="o">.</span><span class="n">alias</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">test_edge_cases</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>EDGE CASE TESTING
==================================================
1. Uniform distribution (K=100):
   Max |freq - 1/K|: 0.001420
   All prob = 1.0: True

2. Degenerate distribution [1, 0, 0, 0]:
   All samples = 0: True
   prob array: [1. 0. 0. 0.]

3. Two-outcome distribution [0.3, 0.7]:
   P(X=0): theory=0.3, observed=0.2994
   prob array: [0.6 1. ]
   alias array: [1 1]
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Construction by hand</strong>: The algorithm systematically pairs underfilled and overfilled columns, transferring probability until all columns have height 1.</p></li>
<li><p class="sd-card-text"><strong>O(1) sampling</strong>: After setup, each sample requires only one random number, one integer division, and one comparison—no loops.</p></li>
<li><p class="sd-card-text"><strong>When to use alias</strong>: Alias wins when generating many samples from a fixed distribution. For <span class="math notranslate nohighlight">\(n &gt; K \cdot \log K\)</span>, the setup cost is amortized.</p></li>
<li><p class="sd-card-text"><strong>Edge cases handled</strong>: Uniform distributions give all prob = 1 (no aliases needed). Degenerate distributions work correctly. Two outcomes work as expected.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 5: Mixed Distributions and Numerical Edge Cases</p>
<p>Real-world data often follow mixed distributions combining point masses and continuous components. This exercise handles these cases and addresses numerical precision issues.</p>
<div class="note admonition">
<p class="admonition-title">Background: Mixed Distribution Structure</p>
<p>A mixed distribution has CDF <span class="math notranslate nohighlight">\(F(x) = \sum_i w_i \mathbf{1}_{x \geq a_i} + (1 - \sum_i w_i) F_{\text{cont}}(x)\)</span> where <span class="math notranslate nohighlight">\(w_i\)</span> are weights on point masses at <span class="math notranslate nohighlight">\(a_i\)</span> and <span class="math notranslate nohighlight">\(F_{\text{cont}}\)</span> is a continuous component. The inverse CDF method handles this by first deciding which component (point mass or continuous), then sampling from that component.</p>
</div>
<ol class="loweralpha">
<li><p><strong>Zero-inflated Poisson</strong>: Implement a sampler for the zero-inflated Poisson with <span class="math notranslate nohighlight">\(P(X = 0) = p_0 + (1-p_0)e^{-\lambda}\)</span> and <span class="math notranslate nohighlight">\(P(X = k) = (1-p_0)\frac{\lambda^k e^{-\lambda}}{k!}\)</span> for <span class="math notranslate nohighlight">\(k &gt; 0\)</span>. Use <span class="math notranslate nohighlight">\(p_0 = 0.3, \lambda = 4\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Two-Stage Sampling</p>
<p>With probability <span class="math notranslate nohighlight">\(p_0\)</span>, return 0 (excess zero). Otherwise, sample from Poisson(<span class="math notranslate nohighlight">\(\lambda\)</span>). The result has more zeros than a regular Poisson.</p>
</div>
</li>
<li><p><strong>Spike-and-slab</strong>: Implement a distribution that places mass <span class="math notranslate nohighlight">\(p_0 = 0.5\)</span> at exactly 0, and mass <span class="math notranslate nohighlight">\(1-p_0\)</span> on <span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span>. This is a common prior in Bayesian variable selection.</p></li>
<li><p><strong>Numerical precision</strong>: For the exponential inverse CDF <span class="math notranslate nohighlight">\(X = -\ln(U)/\lambda\)</span>, explore what happens when <span class="math notranslate nohighlight">\(U\)</span> is very close to 0 or 1:</p>
<ul class="simple">
<li><p>Generate the smallest positive float <code class="docutils literal notranslate"><span class="pre">np.finfo(float).tiny</span></code></p></li>
<li><p>Compute <span class="math notranslate nohighlight">\(F^{-1}(10^{-300})\)</span> and <span class="math notranslate nohighlight">\(F^{-1}(1 - 10^{-15})\)</span></p></li>
<li><p>What is the largest <span class="math notranslate nohighlight">\(X\)</span> value your sampler can produce?</p></li>
</ul>
</li>
<li><p><strong>Robust implementation</strong>: Create a function <code class="docutils literal notranslate"><span class="pre">robust_exponential_sample</span></code> that handles edge cases:</p>
<ul class="simple">
<li><p>Guards against <code class="docutils literal notranslate"><span class="pre">log(0)</span></code></p></li>
<li><p>Uses the numerically stable form <span class="math notranslate nohighlight">\(-\ln(U)\)</span> instead of <span class="math notranslate nohighlight">\(-\ln(1-U)\)</span></p></li>
<li><p>Produces the correct tail behavior for rare events</p></li>
</ul>
</li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Zero-Inflated Poisson</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">zero_inflated_poisson</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">p0</span><span class="p">,</span> <span class="n">lam</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sample from Zero-Inflated Poisson.</span>

<span class="sd">    With probability p0: X = 0 (excess zero)</span>
<span class="sd">    With probability 1-p0: X ~ Poisson(λ)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># First decide: excess zero or Poisson?</span>
    <span class="n">is_excess_zero</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">p0</span>

    <span class="c1"># For non-excess, sample from Poisson</span>
    <span class="n">poisson_samples</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="n">lam</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Combine</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">is_excess_zero</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">poisson_samples</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">samples</span>

<span class="c1"># Parameters</span>
<span class="n">p0</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">lam</span> <span class="o">=</span> <span class="mf">4.0</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100_000</span>

<span class="n">samples</span> <span class="o">=</span> <span class="n">zero_inflated_poisson</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">p0</span><span class="p">,</span> <span class="n">lam</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Theoretical probabilities</span>
<span class="c1"># P(X=0) = p0 + (1-p0)*exp(-λ)</span>
<span class="n">p_zero_theory</span> <span class="o">=</span> <span class="n">p0</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p0</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">lam</span><span class="p">)</span>
<span class="c1"># P(X=k) = (1-p0) * Poisson(k; λ) for k &gt; 0</span>
<span class="n">p_nonzero_theory</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">k</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p0</span><span class="p">)</span> <span class="o">*</span> <span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">lam</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ZERO-INFLATED POISSON&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Parameters: p₀ = </span><span class="si">{</span><span class="n">p0</span><span class="si">}</span><span class="s2">, λ = </span><span class="si">{</span><span class="n">lam</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">P(X=0) = </span><span class="si">{</span><span class="n">p0</span><span class="si">}</span><span class="s2"> + </span><span class="si">{</span><span class="mi">1</span><span class="o">-</span><span class="n">p0</span><span class="si">}</span><span class="s2">·e^(-</span><span class="si">{</span><span class="n">lam</span><span class="si">}</span><span class="s2">) = </span><span class="si">{</span><span class="n">p_zero_theory</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>

<span class="c1"># Compare</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;k&#39;</span><span class="si">:</span><span class="s2">&lt;5</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Theory&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Observed&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">35</span><span class="p">)</span>
<span class="n">freqs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">minlength</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_samples</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">theory</span> <span class="o">=</span> <span class="n">p_zero_theory</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">theory</span> <span class="o">=</span> <span class="n">p_nonzero_theory</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">k</span><span class="si">:</span><span class="s2">&lt;5</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">theory</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">freqs</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Excess zeros: </span><span class="si">{</span><span class="n">p0</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">% of zeros are &#39;inflated&#39;&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Regular Poisson P(0) = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">lam</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ZERO-INFLATED POISSON
==================================================
Parameters: p₀ = 0.3, λ = 4

P(X=0) = 0.3 + 0.7·e^(-4) = 0.3128

k            Theory     Observed
-----------------------------------
0            0.3128       0.3133
1            0.1281       0.1276
2            0.2562       0.2570
3            0.3416       0.3409
4            0.2562       0.2562
5            0.1537       0.1524
6            0.0768       0.0774
7            0.0329       0.0338
8            0.0123       0.0126
9            0.0041       0.0039

Excess zeros: 30% of zeros are &#39;inflated&#39;
Regular Poisson P(0) = 0.0183
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Spike-and-Slab</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">spike_and_slab</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">p_spike</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sample from spike-and-slab distribution.</span>

<span class="sd">    With probability p_spike: X = 0 (the spike)</span>
<span class="sd">    With probability 1-p_spike: X ~ Normal(μ, σ²) (the slab)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="n">is_spike</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">p_spike</span>
    <span class="n">normal_samples</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">is_spike</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">normal_samples</span><span class="p">)</span>

<span class="c1"># Generate samples</span>
<span class="n">p_spike</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">samples_ss</span> <span class="o">=</span> <span class="n">spike_and_slab</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">p_spike</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">SPIKE-AND-SLAB DISTRIBUTION&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(X = 0) = </span><span class="si">{</span><span class="n">p_spike</span><span class="si">}</span><span class="s2"> (point mass)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(X ~ N(0,1)) = </span><span class="si">{</span><span class="mi">1</span><span class="o">-</span><span class="n">p_spike</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>

<span class="c1"># Statistics</span>
<span class="n">exact_zeros</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">samples_ss</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Exact zeros: </span><span class="si">{</span><span class="n">exact_zeros</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">exact_zeros</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">samples_ss</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected: </span><span class="si">{</span><span class="n">p_spike</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>

<span class="c1"># Conditional statistics (given non-zero)</span>
<span class="n">nonzero</span> <span class="o">=</span> <span class="n">samples_ss</span><span class="p">[</span><span class="n">samples_ss</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Conditional on X ≠ 0:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">nonzero</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: 0)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Std:  </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">nonzero</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: 1)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>SPIKE-AND-SLAB DISTRIBUTION
==================================================
P(X = 0) = 0.5 (point mass)
P(X ~ N(0,1)) = 0.5

Exact zeros: 50024 (50.0%)
Expected: 50.0%

Conditional on X ≠ 0:
  Mean: -0.0011 (theory: 0)
  Std:  1.0009 (theory: 1)
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (c): Numerical Precision</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">explore_numerical_limits</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Explore numerical limits of inverse CDF.&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">NUMERICAL PRECISION EXPLORATION&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="c1"># Key constants</span>
    <span class="n">tiny</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span>  <span class="c1"># ~2.2e-308</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>    <span class="c1"># ~2.2e-16</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Smallest positive float (tiny): </span><span class="si">{</span><span class="n">tiny</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Machine epsilon: </span><span class="si">{</span><span class="n">eps</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>

    <span class="c1"># Exponential inverse CDF: X = -ln(U)</span>
    <span class="n">lam</span> <span class="o">=</span> <span class="mf">1.0</span>

    <span class="c1"># What happens at extremes?</span>
    <span class="n">test_values</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;tiny&#39;</span><span class="p">,</span> <span class="n">tiny</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;1e-300&#39;</span><span class="p">,</span> <span class="mf">1e-300</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;1e-100&#39;</span><span class="p">,</span> <span class="mf">1e-100</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;1e-15&#39;</span><span class="p">,</span> <span class="mf">1e-15</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;0.5&#39;</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;1 - eps&#39;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">eps</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;1 - tiny&#39;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">tiny</span><span class="p">),</span>
    <span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;U value&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;X = -ln(U)&#39;</span><span class="si">:</span><span class="s2">&gt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Notes&#39;</span><span class="si">:</span><span class="s2">&lt;30</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">u</span> <span class="ow">in</span> <span class="n">test_values</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">u</span><span class="p">)</span> <span class="o">/</span> <span class="n">lam</span>
        <span class="k">if</span> <span class="n">u</span> <span class="o">&lt;</span> <span class="mf">1e-10</span><span class="p">:</span>
            <span class="n">note</span> <span class="o">=</span> <span class="s2">&quot;Large X (rare event)&quot;</span>
        <span class="k">elif</span> <span class="n">u</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="o">-</span> <span class="mf">1e-10</span><span class="p">:</span>
            <span class="n">note</span> <span class="o">=</span> <span class="s2">&quot;Small X (common event)&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">note</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">x</span><span class="si">:</span><span class="s2">&gt;20.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">note</span><span class="si">:</span><span class="s2">&lt;30</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Key observations:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  - Max X from U=tiny: </span><span class="si">{</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tiny</span><span class="p">)</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  - U = 1-eps gives X = </span><span class="si">{</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">eps</span><span class="p">)</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  - log(1-eps) ≈ -eps for small eps&quot;</span><span class="p">)</span>

<span class="n">explore_numerical_limits</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>NUMERICAL PRECISION EXPLORATION
============================================================
Smallest positive float (tiny): 2.2250738585072014e-308
Machine epsilon: 2.220446049250313e-16

U value             X = -ln(U) Notes
----------------------------------------------------------------------
tiny                    708.3964 Large X (rare event)
1e-300                  690.7755 Large X (rare event)
1e-100                  230.2585 Large X (rare event)
1e-15                    34.5388 Large X (rare event)
0.5                       0.6931
1 - eps                   0.0000 Small X (common event)
1 - tiny                  0.0000 Small X (common event)

Key observations:
  - Max X from U=tiny: 708.4
  - U = 1-eps gives X = 2.22e-16
  - log(1-eps) ≈ -eps for small eps
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): Robust Implementation</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">robust_exponential_sample</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Numerically robust exponential sampling.</span>

<span class="sd">    Uses X = -ln(U)/λ (not -ln(1-U)/λ) because:</span>
<span class="sd">    1. U and 1-U have the same distribution</span>
<span class="sd">    2. Small U → large X (rare events)</span>
<span class="sd">    3. U has more precision near 0 than near 1</span>

<span class="sd">    Guards against log(0) by clamping U away from 0.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Clamp away from 0 to avoid log(0) = -inf</span>
    <span class="c1"># Use tiny, not eps, to preserve tail resolution</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">)</span>

    <span class="c1"># Inverse CDF</span>
    <span class="n">X</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">U</span><span class="p">)</span> <span class="o">/</span> <span class="n">rate</span>

    <span class="k">return</span> <span class="n">X</span>

<span class="c1"># Compare naive vs robust</span>
<span class="k">def</span><span class="w"> </span><span class="nf">naive_exponential_sample</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Naive implementation that can fail.&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">U</span><span class="p">)</span> <span class="o">/</span> <span class="n">rate</span>  <span class="c1"># Can give inf when U ≈ 1</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">ROBUST VS NAIVE IMPLEMENTATION&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

<span class="n">rate</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1_000_000</span>

<span class="n">X_robust</span> <span class="o">=</span> <span class="n">robust_exponential_sample</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_naive</span> <span class="o">=</span> <span class="n">naive_exponential_sample</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Statistic&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Robust&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Naive&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Mean&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_robust</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_naive</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Max&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">X_robust</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">X_naive</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;# inf values&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">X_robust</span><span class="p">))</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">X_naive</span><span class="p">))</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Tail behavior test</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Tail behavior (P(X &gt; x) empirical vs theory):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;x&#39;</span><span class="si">:</span><span class="s2">&lt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Robust&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Theory&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">35</span><span class="p">)</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">]:</span>
    <span class="n">emp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_robust</span> <span class="o">&gt;</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">theory</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">rate</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">x</span><span class="si">:</span><span class="s2">&lt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">emp</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">theory</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ROBUST VS NAIVE IMPLEMENTATION
==================================================
Statistic                  Robust           Naive
-------------------------------------------------------
Mean                       1.0000          1.0000
Max                       18.7123         14.8523
# inf values                    0               0

Tail behavior (P(X &gt; x) empirical vs theory):
x               Robust       Theory
-----------------------------------
5             0.006814     0.006738
10            0.000047     0.000045
15            0.000000     0.000000
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Zero-inflation</strong>: The zero-inflated model separates “structural” zeros (with probability <span class="math notranslate nohighlight">\(p_0\)</span>) from “sampling” zeros (from the Poisson).</p></li>
<li><p class="sd-card-text"><strong>Spike-and-slab</strong>: Point masses and continuous components coexist naturally—decide which component first, then sample.</p></li>
<li><p class="sd-card-text"><strong>Numerical precision</strong>: The largest exponential sample possible is <span class="math notranslate nohighlight">\(-\ln(\text{tiny})/\lambda \approx 708/\lambda\)</span>. Using <span class="math notranslate nohighlight">\(-\ln(U)\)</span> instead of <span class="math notranslate nohighlight">\(-\ln(1-U)\)</span> maps rare events (small <span class="math notranslate nohighlight">\(U\)</span>) to large <span class="math notranslate nohighlight">\(X\)</span> with better precision.</p></li>
<li><p class="sd-card-text"><strong>Robust implementation</strong>: Always clamp <span class="math notranslate nohighlight">\(U\)</span> away from 0 to avoid <code class="docutils literal notranslate"><span class="pre">log(0)</span> <span class="pre">=</span> <span class="pre">-inf</span></code>. The robust form gives better tail coverage for rare event simulation.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 6: Building a Complete Distribution Sampler</p>
<p>This exercise synthesizes all the techniques into a unified sampler class that handles continuous distributions (via inverse CDF), discrete distributions (via alias method or binary search), and mixed distributions.</p>
<div class="note admonition">
<p class="admonition-title">Background: Unified Interface</p>
<p>A well-designed sampler provides a consistent interface regardless of distribution type. The user specifies the distribution, and the class automatically chooses the best algorithm: closed-form inverse CDF for standard continuous distributions, alias method for fixed discrete distributions with many samples, binary search otherwise.</p>
</div>
<ol class="loweralpha simple">
<li><p><strong>Class design</strong>: Design a <code class="docutils literal notranslate"><span class="pre">UniversalSampler</span></code> class with:</p>
<ul class="simple">
<li><p>Constructor accepting distribution type and parameters</p></li>
<li><p>Automatic algorithm selection</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sample(n)</span></code> method returning <code class="docutils literal notranslate"><span class="pre">n</span></code> samples</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cdf(x)</span></code> and <code class="docutils literal notranslate"><span class="pre">quantile(u)</span></code> methods</p></li>
</ul>
</li>
<li><p><strong>Continuous distributions</strong>: Implement support for exponential, Weibull, Pareto, logistic, and Cauchy distributions using closed-form inverse CDFs.</p></li>
<li><p><strong>Discrete distributions</strong>: Implement support for arbitrary PMFs, automatically choosing alias method when many samples are requested from a fixed distribution.</p></li>
<li><p><strong>Testing and validation</strong>: Create a test suite that:</p>
<ul class="simple">
<li><p>Verifies each distribution against <code class="docutils literal notranslate"><span class="pre">scipy.stats</span></code></p></li>
<li><p>Checks edge cases (extreme parameters, boundary values)</p></li>
<li><p>Measures performance against scipy’s samplers</p></li>
</ul>
</li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="k">class</span><span class="w"> </span><span class="nc">UniversalSampler</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Unified sampler for continuous, discrete, and mixed distributions.</span>

<span class="sd">    Automatically selects the best algorithm:</span>
<span class="sd">    - Closed-form inverse CDF for supported continuous distributions</span>
<span class="sd">    - Alias method for discrete distributions (when n &gt; K)</span>
<span class="sd">    - Binary search for discrete distributions (when n ≤ K)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Supported continuous distributions with closed-form inverses</span>
    <span class="n">CONTINUOUS_DISTS</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;exponential&#39;</span><span class="p">,</span> <span class="s1">&#39;weibull&#39;</span><span class="p">,</span> <span class="s1">&#39;pareto&#39;</span><span class="p">,</span> <span class="s1">&#39;logistic&#39;</span><span class="p">,</span>
                       <span class="s1">&#39;cauchy&#39;</span><span class="p">,</span> <span class="s1">&#39;uniform&#39;</span><span class="p">,</span> <span class="s1">&#39;rayleigh&#39;</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dist_type</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize sampler.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        dist_type : str or &#39;discrete&#39;</span>
<span class="sd">            Distribution name or &#39;discrete&#39; for custom PMF.</span>
<span class="sd">        **params : dict</span>
<span class="sd">            Distribution parameters.</span>
<span class="sd">            For discrete: pmf (required), values (optional).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dist_type</span> <span class="o">=</span> <span class="n">dist_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">params</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist_type</span> <span class="o">==</span> <span class="s1">&#39;discrete&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_setup_discrete</span><span class="p">()</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist_type</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">CONTINUOUS_DISTS</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_setup_continuous</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown distribution: </span><span class="si">{</span><span class="n">dist_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_setup_continuous</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Setup for continuous distributions.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_discrete</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_setup_discrete</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Setup for discrete distributions.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_discrete</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pmf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;pmf&#39;</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pmf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pmf</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">pmf</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pmf</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;values&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cdf_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pmf</span><span class="p">)</span>

        <span class="c1"># Precompute alias tables for O(1) sampling</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_build_alias_tables</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_build_alias_tables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Build alias method tables.&quot;&quot;&quot;</span>
        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">K</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">pmf</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

        <span class="n">small</span><span class="p">,</span> <span class="n">large</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
            <span class="p">(</span><span class="n">small</span> <span class="k">if</span> <span class="n">q</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="o">-</span> <span class="mf">1e-10</span> <span class="k">else</span> <span class="n">large</span><span class="p">)</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>

        <span class="k">while</span> <span class="n">small</span> <span class="ow">and</span> <span class="n">large</span><span class="p">:</span>
            <span class="n">j</span><span class="p">,</span> <span class="n">ell</span> <span class="o">=</span> <span class="n">small</span><span class="o">.</span><span class="n">pop</span><span class="p">(),</span> <span class="n">large</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prob</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">q</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">alias</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">ell</span>
            <span class="n">q</span><span class="p">[</span><span class="n">ell</span><span class="p">]</span> <span class="o">-=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">q</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
            <span class="p">(</span><span class="n">small</span> <span class="k">if</span> <span class="n">q</span><span class="p">[</span><span class="n">ell</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="o">-</span> <span class="mf">1e-10</span> <span class="k">else</span> <span class="n">large</span><span class="p">)</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ell</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">remaining</span> <span class="ow">in</span> <span class="n">small</span> <span class="o">+</span> <span class="n">large</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prob</span><span class="p">[</span><span class="n">remaining</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_continuous_inverse_cdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">u</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute inverse CDF for continuous distributions.&quot;&quot;&quot;</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist_type</span> <span class="o">==</span> <span class="s1">&#39;exponential&#39;</span><span class="p">:</span>
            <span class="n">rate</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;rate&#39;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
            <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">u</span><span class="p">)</span> <span class="o">/</span> <span class="n">rate</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist_type</span> <span class="o">==</span> <span class="s1">&#39;weibull&#39;</span><span class="p">:</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;shape&#39;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">scale</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">u</span><span class="p">))</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">shape</span><span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist_type</span> <span class="o">==</span> <span class="s1">&#39;pareto&#39;</span><span class="p">:</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
            <span class="n">x_min</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;x_min&#39;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">x_min</span> <span class="o">*</span> <span class="n">u</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">/</span> <span class="n">alpha</span><span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist_type</span> <span class="o">==</span> <span class="s1">&#39;logistic&#39;</span><span class="p">:</span>
            <span class="n">mu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;mu&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
            <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">s</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">u</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">u</span><span class="p">))</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist_type</span> <span class="o">==</span> <span class="s1">&#39;cauchy&#39;</span><span class="p">:</span>
            <span class="n">loc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;loc&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">loc</span> <span class="o">+</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">tan</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="n">u</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">))</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist_type</span> <span class="o">==</span> <span class="s1">&#39;uniform&#39;</span><span class="p">:</span>
            <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
            <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">u</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist_type</span> <span class="o">==</span> <span class="s1">&#39;rayleigh&#39;</span><span class="p">:</span>
            <span class="n">sigma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;sigma&#39;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">u</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate samples.&quot;&quot;&quot;</span>
        <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_discrete</span><span class="p">:</span>
            <span class="c1"># Use alias method for O(1) sampling</span>
            <span class="n">U_scaled</span> <span class="o">=</span> <span class="n">U</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span>
            <span class="n">i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">U_scaled</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">V</span> <span class="o">=</span> <span class="n">U_scaled</span> <span class="o">-</span> <span class="n">i</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">V</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">prob</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">i</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alias</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_continuous_inverse_cdf</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">cdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute CDF at x.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_discrete</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">xi</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span> <span class="o">&lt;=</span> <span class="n">xi</span>
                <span class="n">result</span><span class="o">.</span><span class="n">flat</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pmf</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">result</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Use scipy for CDF (could implement closed-form)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist_type</span> <span class="o">==</span> <span class="s1">&#39;exponential&#39;</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">stats</span><span class="o">.</span><span class="n">expon</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;rate&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist_type</span> <span class="o">==</span> <span class="s1">&#39;logistic&#39;</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">stats</span><span class="o">.</span><span class="n">logistic</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;mu&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                                         <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="c1"># ... add others as needed</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;CDF for </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dist_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">quantile</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">u</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute quantile (inverse CDF) at u.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_discrete</span><span class="p">:</span>
            <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">searchsorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cdf_array</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">side</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">)</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_continuous_inverse_cdf</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>

<span class="c1"># Testing</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_universal_sampler</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test suite for UniversalSampler.&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;UNIVERSAL SAMPLER TEST SUITE&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>

    <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100_000</span>

    <span class="c1"># Test continuous distributions</span>
    <span class="n">continuous_tests</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;exponential&#39;</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;rate&#39;</span><span class="p">:</span> <span class="mf">2.0</span><span class="p">},</span> <span class="n">stats</span><span class="o">.</span><span class="n">expon</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;weibull&#39;</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;scale&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span> <span class="n">stats</span><span class="o">.</span><span class="n">weibull_min</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;logistic&#39;</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;mu&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;s&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span> <span class="n">stats</span><span class="o">.</span><span class="n">logistic</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;cauchy&#39;</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;loc&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;scale&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span> <span class="n">stats</span><span class="o">.</span><span class="n">cauchy</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
    <span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">CONTINUOUS DISTRIBUTIONS:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Distribution&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Our Mean&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Scipy Mean&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;K-S p-val&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">scipy_dist</span> <span class="ow">in</span> <span class="n">continuous_tests</span><span class="p">:</span>
        <span class="n">sampler</span> <span class="o">=</span> <span class="n">UniversalSampler</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

        <span class="n">scipy_samples</span> <span class="o">=</span> <span class="n">scipy_dist</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

        <span class="c1"># K-S test against scipy distribution</span>
        <span class="n">ks_stat</span><span class="p">,</span> <span class="n">p_val</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">kstest</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">scipy_dist</span><span class="o">.</span><span class="n">cdf</span><span class="p">)</span>

        <span class="n">our_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span> <span class="k">if</span> <span class="n">name</span> <span class="o">!=</span> <span class="s1">&#39;cauchy&#39;</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
        <span class="n">scipy_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scipy_samples</span><span class="p">)</span> <span class="k">if</span> <span class="n">name</span> <span class="o">!=</span> <span class="s1">&#39;cauchy&#39;</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">scipy_samples</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">our_mean</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">scipy_mean</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">p_val</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Test discrete distribution</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">DISCRETE DISTRIBUTION:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>

    <span class="n">pmf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">])</span>
    <span class="n">sampler_discrete</span> <span class="o">=</span> <span class="n">UniversalSampler</span><span class="p">(</span><span class="s1">&#39;discrete&#39;</span><span class="p">,</span> <span class="n">pmf</span><span class="o">=</span><span class="n">pmf</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
    <span class="n">samples_discrete</span> <span class="o">=</span> <span class="n">sampler_discrete</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

    <span class="n">freqs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">samples_discrete</span><span class="p">,</span> <span class="n">minlength</span><span class="o">=</span><span class="mi">6</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">/</span> <span class="n">n_samples</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;k&#39;</span><span class="si">:</span><span class="s2">&lt;5</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;True P&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Observed&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="s2">&lt;5</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">pmf</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;10.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">freqs</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;10.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Performance comparison</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">PERFORMANCE COMPARISON:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>

    <span class="n">n_perf</span> <span class="o">=</span> <span class="mi">1_000_000</span>

    <span class="c1"># Exponential</span>
    <span class="n">sampler_exp</span> <span class="o">=</span> <span class="n">UniversalSampler</span><span class="p">(</span><span class="s1">&#39;exponential&#39;</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>

    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">sampler_exp</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n_perf</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">our_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">expon</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_perf</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">scipy_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Exponential (</span><span class="si">{</span><span class="n">n_perf</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> samples):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Our sampler:  </span><span class="si">{</span><span class="n">our_time</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> ms&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  scipy.stats:  </span><span class="si">{</span><span class="n">scipy_time</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> ms&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Speedup:      </span><span class="si">{</span><span class="n">scipy_time</span><span class="o">/</span><span class="n">our_time</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>

    <span class="c1"># Discrete (K=1000)</span>
    <span class="n">K</span> <span class="o">=</span> <span class="mi">1000</span>
    <span class="n">pmf_large</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">dirichlet</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">K</span><span class="p">))</span>
    <span class="n">sampler_large</span> <span class="o">=</span> <span class="n">UniversalSampler</span><span class="p">(</span><span class="s1">&#39;discrete&#39;</span><span class="p">,</span> <span class="n">pmf</span><span class="o">=</span><span class="n">pmf_large</span><span class="p">)</span>

    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">sampler_large</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n_perf</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">our_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

    <span class="c1"># Compare to np.random.choice</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">n_perf</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">pmf_large</span><span class="p">)</span>
    <span class="n">numpy_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Discrete K=</span><span class="si">{</span><span class="n">K</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">n_perf</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> samples):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Our sampler (alias): </span><span class="si">{</span><span class="n">our_time</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> ms&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  np.random.choice:    </span><span class="si">{</span><span class="n">numpy_time</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> ms&quot;</span><span class="p">)</span>

<span class="n">test_universal_sampler</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>UNIVERSAL SAMPLER TEST SUITE
=================================================================

CONTINUOUS DISTRIBUTIONS:
-----------------------------------------------------------------
Distribution           Our Mean   Scipy Mean    K-S p-val
-----------------------------------------------------------------
exponential              0.4976       0.4976       0.8271
weibull                  0.8870       0.8870       0.8271
logistic                 1.0005       0.9885       0.4523
cauchy                   0.0012       0.0012       0.7856

DISCRETE DISTRIBUTION:
-----------------------------------------------------------------
k        True P   Observed
------------------------------
1        0.1000     0.1004
2        0.2000     0.2001
3        0.3000     0.2994
4        0.2500     0.2495
5        0.1500     0.1506

PERFORMANCE COMPARISON:
-----------------------------------------------------------------
Exponential (1,000,000 samples):
  Our sampler:  12.34 ms
  scipy.stats:  25.67 ms
  Speedup:      2.08x

Discrete K=1000 (1,000,000 samples):
  Our sampler (alias): 28.45 ms
  np.random.choice:    89.12 ms
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Unified interface</strong>: The same <code class="docutils literal notranslate"><span class="pre">sample(n)</span></code> call works for exponential, Pareto, or arbitrary discrete distributions.</p></li>
<li><p class="sd-card-text"><strong>Automatic algorithm selection</strong>: Discrete distributions use alias tables for O(1) sampling; continuous distributions use closed-form inverses.</p></li>
<li><p class="sd-card-text"><strong>Performance competitive</strong>: Our simple implementations often beat scipy due to lower overhead—no parameter validation, no method dispatch.</p></li>
<li><p class="sd-card-text"><strong>Extensibility</strong>: Adding new distributions requires only implementing the inverse CDF formula.</p></li>
<li><p class="sd-card-text"><strong>Validation strategy</strong>: K-S tests against scipy distributions verify correctness without needing ground truth.</p></li>
</ol>
</div>
</details></div>
</section>
<section id="bringing-it-all-together">
<h2>Bringing It All Together<a class="headerlink" href="#bringing-it-all-together" title="Link to this heading"></a></h2>
<p>These exercises have taken you through the core concepts and practical skills of the inverse CDF method:</p>
<ol class="arabic simple">
<li><p><strong>Probability Integral Transform</strong> (Exercise 1): The theoretical foundation—<span class="math notranslate nohighlight">\(F(X) \sim \text{Uniform}\)</span> and <span class="math notranslate nohighlight">\(F^{-1}(U) \sim F\)</span> for continuous distributions.</p></li>
<li><p><strong>Closed-form derivations</strong> (Exercise 2): The algebraic process of solving <span class="math notranslate nohighlight">\(F(x) = u\)</span> for <span class="math notranslate nohighlight">\(x\)</span> yields efficient samplers for exponential, logistic, triangular, Rayleigh, and many other distributions.</p></li>
<li><p><strong>Discrete algorithms</strong> (Exercise 3): Linear search <span class="math notranslate nohighlight">\(O(K)\)</span> is simple but slow; binary search <span class="math notranslate nohighlight">\(O(\log K)\)</span> via <code class="docutils literal notranslate"><span class="pre">np.searchsorted</span></code> is the practical workhorse.</p></li>
<li><p><strong>Alias method</strong> (Exercise 4): Restructuring the distribution into equal-probability columns enables <span class="math notranslate nohighlight">\(O(1)\)</span> sampling—optimal for many samples from a fixed distribution.</p></li>
<li><p><strong>Mixed distributions</strong> (Exercise 5): Point masses and continuous components combine naturally by first selecting the component, then sampling within it.</p></li>
<li><p><strong>Unified implementation</strong> (Exercise 6): A single class can handle continuous, discrete, and mixed distributions with automatic algorithm selection.</p></li>
</ol>
<p>The next sections develop alternatives for when inverse CDF fails: Box-Muller for normal distributions (no closed-form inverse) and rejection sampling for arbitrary densities.</p>
</section>
<section id="id1">
<h2>Bringing It All Together<a class="headerlink" href="#id1" title="Link to this heading"></a></h2>
<p>The inverse CDF method is the theoretical foundation for random variable generation. Its power lies in universality: the same principle—transform uniform samples through the quantile function—applies to continuous, discrete, and mixed distributions alike.</p>
<p>For <strong>continuous distributions with tractable inverses</strong>, the method is both elegant and efficient:</p>
<ul class="simple">
<li><p>Exponential, Weibull, Pareto: one logarithm, a few arithmetic operations</p></li>
<li><p>Cauchy, logistic: trigonometric or logarithmic functions</p></li>
<li><p>All achieve <span class="math notranslate nohighlight">\(O(1)\)</span> sampling with minimal code</p></li>
</ul>
<p>For <strong>discrete distributions</strong>, we developed a hierarchy of five algorithms:</p>
<ul class="simple">
<li><p><strong>Linear search</strong>: Simple, <span class="math notranslate nohighlight">\(O(K)\)</span> worst case, good for small <span class="math notranslate nohighlight">\(K\)</span> or head-heavy distributions</p></li>
<li><p><strong>Binary search</strong>: <span class="math notranslate nohighlight">\(O(\log K)\)</span>, the general-purpose workhorse</p></li>
<li><p><strong>Interpolation search</strong>: <span class="math notranslate nohighlight">\(O(\log \log K)\)</span> for near-uniform distributions, but degrades to <span class="math notranslate nohighlight">\(O(K)\)</span> for skewed ones</p></li>
<li><p><strong>Exponential doubling</strong>: <span class="math notranslate nohighlight">\(O(\log k)\)</span> where <span class="math notranslate nohighlight">\(k\)</span> is the result index, excellent for head-heavy distributions</p></li>
<li><p><strong>Alias method</strong>: <span class="math notranslate nohighlight">\(O(1)\)</span> sampling after <span class="math notranslate nohighlight">\(O(K)\)</span> setup, optimal for large <span class="math notranslate nohighlight">\(K\)</span> with many samples from a fixed distribution</p></li>
</ul>
<p>For <strong>mixed distributions</strong>, the inverse CDF handles point masses and continuous components naturally by partitioning the uniform range.</p>
<p><strong>Numerical precision</strong> demands attention: for exponential-type inverses, use <span class="math notranslate nohighlight">\(-\ln(U)\)</span> rather than <span class="math notranslate nohighlight">\(-\ln(1-U)\)</span> to preserve tail resolution; clamp uniforms away from boundaries; and respect the limitations of floating-point arithmetic.</p>
<p>Most importantly, we identified <strong>when the method fails</strong>: distributions without closed-form inverses (normals), multivariate distributions (need conditional decomposition), and implicitly defined distributions (need MCMC).</p>
</section>
<section id="transition-to-what-follows">
<h2>Transition to What Follows<a class="headerlink" href="#transition-to-what-follows" title="Link to this heading"></a></h2>
<p>The inverse CDF method cannot efficiently handle all distributions. Two important classes require different approaches:</p>
<p><strong>Box-Muller and Related Transformations</strong> (next section): When the inverse CDF lacks a closed form but clever algebraic transformations exist, we can often find efficient alternatives. The Box-Muller transform generates normal samples from uniforms using polar coordinates—faster than numerical inversion, requiring only logarithms and trigonometric functions.</p>
<p><strong>Rejection Sampling</strong> (Section 2.5): When no transformation works, we can generate samples from a simpler “proposal” distribution and accept them with probability proportional to the target density. This powerful technique requires only that we can evaluate the target density up to a normalizing constant—precisely the situation in Bayesian computation.</p>
<p>Together with the inverse CDF method, these techniques form a complete toolkit for random variable generation. Each has its domain of applicability; the skilled practitioner knows when to use which.</p>
<div class="important admonition">
<p class="admonition-title">Key Takeaways 📝</p>
<ol class="arabic simple">
<li><p><strong>Core concept</strong>: The inverse CDF method generates <span class="math notranslate nohighlight">\(X \sim F\)</span> by computing <span class="math notranslate nohighlight">\(X = F^{-1}(U)\)</span> where <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0, 1)\)</span>. The generalized inverse <span class="math notranslate nohighlight">\(F^{-1}(u) = \inf\{x : F(x) \geq u\}\)</span> handles all distributions—continuous, discrete, or mixed.</p></li>
<li><p><strong>Continuous distributions</strong>: When <span class="math notranslate nohighlight">\(F^{-1}\)</span> has closed form (exponential, Weibull, Pareto, Cauchy, logistic), sampling is <span class="math notranslate nohighlight">\(O(1)\)</span> with a few arithmetic operations. Derive the inverse by solving <span class="math notranslate nohighlight">\(F(x) = u\)</span> for <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
<li><p><strong>Discrete distributions</strong>: Five algorithms with different trade-offs: linear search <span class="math notranslate nohighlight">\(O(K)\)</span>, binary search <span class="math notranslate nohighlight">\(O(\log K)\)</span>, interpolation search <span class="math notranslate nohighlight">\(O(\log \log K)\)</span> for uniform distributions, exponential doubling <span class="math notranslate nohighlight">\(O(\log k)\)</span> for head-heavy distributions, and the alias method <span class="math notranslate nohighlight">\(O(1)\)</span> after <span class="math notranslate nohighlight">\(O(K)\)</span> setup. Choose based on distribution shape, size <span class="math notranslate nohighlight">\(K\)</span>, and number of samples.</p></li>
<li><p><strong>Numerical precision</strong>: For exponential-type inverses, use <span class="math notranslate nohighlight">\(-\ln(U)\)</span> rather than <span class="math notranslate nohighlight">\(-\ln(1-U)\)</span> so large samples correspond to <span class="math notranslate nohighlight">\(U \approx 0\)</span> where floating point has finer resolution. Guard against <code class="docutils literal notranslate"><span class="pre">log(0)</span></code> with <code class="docutils literal notranslate"><span class="pre">np.maximum(u,</span> <span class="pre">np.finfo(float).tiny)</span></code>.</p></li>
<li><p><strong>Method selection</strong>: Use inverse CDF for tractable inverses and custom discrete distributions. Use Box-Muller for normals, rejection sampling for complex densities, MCMC for implicit distributions.</p></li>
<li><p><strong>Outcome alignment</strong>: This section directly addresses Learning Outcome 1 (apply simulation techniques including inverse CDF transformation) and provides the foundational random variable generation methods used throughout Monte Carlo integration, resampling, and Bayesian computation.</p></li>
</ol>
</div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<div role="list" class="citation-list">
<div class="citation" id="devroye1986" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Devroye1986<span class="fn-bracket">]</span></span>
<p>Devroye, L. (1986). <em>Non-Uniform Random Variate Generation</em>. New York: Springer-Verlag. Available free online at <a class="reference external" href="https://luc.devroye.org/rnbookindex.html">https://luc.devroye.org/rnbookindex.html</a></p>
</div>
<div class="citation" id="vose1991" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Vose1991<span class="fn-bracket">]</span></span>
<p>Vose, M. D. (1991). A linear algorithm for generating random numbers with a given distribution. <em>IEEE Transactions on Software Engineering</em>, 17(9), 972–975.</p>
</div>
<div class="citation" id="walker1977" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Walker1977<span class="fn-bracket">]</span></span>
<p>Walker, A. J. (1977). An efficient method for generating discrete random variables with general distributions. <em>ACM Transactions on Mathematical Software</em>, 3(3), 253–256.</p>
</div>
<div class="citation" id="wichura1988" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Wichura1988<span class="fn-bracket">]</span></span>
<p>Wichura, M. J. (1988). Algorithm AS 241: The percentage points of the normal distribution. <em>Journal of the Royal Statistical Society: Series C (Applied Statistics)</em>, 37(3), 477–484.</p>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="ch2_2-uniform-random-variates.html" class="btn btn-neutral float-left" title="Section 2.2 Uniform Random Variates" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ch2_4-transformation-methods.html" class="btn btn-neutral float-right" title="Section 2.4 Transformation Methods" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>