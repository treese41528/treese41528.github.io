

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>1.1. Monte Carlo Fundamentals &mdash; STAT 418</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=5af2d4ad" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT418/Website/part2_simulation/chapter2/ch2.1-monte-carlo-fundamentals.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script>window.MathJax = {"options": {"enableMenu": true, "menuOptions": {"settings": {"enrich": true, "speech": true, "braille": true, "collapsible": true, "assistiveMml": false}}}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
      <script src="../../_static/custom.js?v=d2113767"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="1.2. Uniform Random Variates" href="ch2.2-uniform-random-variates.html" />
    <link rel="prev" title="1. Chapter 2: Monte Carlo Simulation" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Course Content</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../part1_foundations/index.html">1. Part I: Foundations of Probability and Computation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part1_foundations/chapter1/index.html">1.1. Chapter 1: Statistical Paradigms and Core Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html">1.1.1. Paradigms of Probability and Statistical Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#the-mathematical-foundation-kolmogorov-s-axioms">The Mathematical Foundation: Kolmogorov‚Äôs Axioms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#interpretations-of-probability">Interpretations of Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#statistical-inference-paradigms">Statistical Inference Paradigms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#historical-and-philosophical-debates">Historical and Philosophical Debates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#looking-ahead-our-course-focus">Looking Ahead: Our Course Focus</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html">1.1.2. Probability Distributions: Theory and Computation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#from-abstract-foundations-to-concrete-tools">From Abstract Foundations to Concrete Tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#the-python-ecosystem-for-probability">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#introduction-why-probability-distributions-matter">Introduction: Why Probability Distributions Matter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#id1">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#continuous-distributions">Continuous Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#additional-important-distributions">Additional Important Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#summary-and-practical-guidelines">Summary and Practical Guidelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html">1.1.3. Python Random Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#from-mathematical-distributions-to-computational-samples">From Mathematical Distributions to Computational Samples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-python-ecosystem-at-a-glance">The Python Ecosystem at a Glance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#understanding-pseudo-random-number-generation">Understanding Pseudo-Random Number Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-standard-library-random-module">The Standard Library: <code class="docutils literal notranslate"><span class="pre">random</span></code> Module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#numpy-fast-vectorized-random-sampling">NumPy: Fast Vectorized Random Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#scipy-stats-the-complete-statistical-toolkit">SciPy Stats: The Complete Statistical Toolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#bringing-it-all-together-library-selection-guide">Bringing It All Together: Library Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#looking-ahead-from-random-numbers-to-monte-carlo-methods">Looking Ahead: From Random Numbers to Monte Carlo Methods</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Part II: Simulation-Based Methods</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="index.html">1. Chapter 2: Monte Carlo Simulation</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">1.1. Monte Carlo Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#the-historical-development-of-monte-carlo-methods">1.1.1. The Historical Development of Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-core-principle-expectation-as-integration">1.1.2. The Core Principle: Expectation as Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="#theoretical-foundations">1.1.3. Theoretical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#variance-estimation-and-confidence-intervals">1.1.4. Variance Estimation and Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="#worked-examples">1.1.5. Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="#comparison-with-deterministic-methods">1.1.6. Comparison with Deterministic Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sample-size-determination">1.1.7. Sample Size Determination</a></li>
<li class="toctree-l4"><a class="reference internal" href="#convergence-diagnostics-and-monitoring">1.1.8. Convergence Diagnostics and Monitoring</a></li>
<li class="toctree-l4"><a class="reference internal" href="#practical-considerations">1.1.9. Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bringing-it-all-together">1.1.10. Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="#transition-to-what-follows">1.1.11. Transition to What Follows</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2.2-uniform-random-variates.html">1.2. Uniform Random Variates</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2.2-uniform-random-variates.html#the-probability-integral-transform">1.2.1. The Probability Integral Transform</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.2-uniform-random-variates.html#the-paradox-of-computational-randomness">1.2.2. The Paradox of Computational Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.2-uniform-random-variates.html#chaotic-dynamical-systems-an-instructive-failure">1.2.3. Chaotic Dynamical Systems: An Instructive Failure</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.2-uniform-random-variates.html#linear-congruential-generators">1.2.4. Linear Congruential Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.2-uniform-random-variates.html#shift-register-generators">1.2.5. Shift-Register Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.2-uniform-random-variates.html#the-kiss-generator-combining-strategies">1.2.6. The KISS Generator: Combining Strategies</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.2-uniform-random-variates.html#modern-generators-mersenne-twister-and-pcg">1.2.7. Modern Generators: Mersenne Twister and PCG</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.2-uniform-random-variates.html#statistical-testing-of-random-number-generators">1.2.8. Statistical Testing of Random Number Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.2-uniform-random-variates.html#practical-considerations">1.2.9. Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.2-uniform-random-variates.html#bringing-it-all-together">1.2.10. Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.2-uniform-random-variates.html#transition-to-what-follows">1.2.11. Transition to What Follows</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2.3-inverse-cdf-method.html">1.3. Inverse CDF Method</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2.3-inverse-cdf-method.html#mathematical-foundations">1.3.1. Mathematical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.3-inverse-cdf-method.html#continuous-distributions-with-closed-form-inverses">1.3.2. Continuous Distributions with Closed-Form Inverses</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.3-inverse-cdf-method.html#numerical-inversion">1.3.3. Numerical Inversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.3-inverse-cdf-method.html#discrete-distributions">1.3.4. Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.3-inverse-cdf-method.html#mixed-distributions">1.3.5. Mixed Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.3-inverse-cdf-method.html#practical-considerations">1.3.6. Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.3-inverse-cdf-method.html#bringing-it-all-together">1.3.7. Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.3-inverse-cdf-method.html#transition-to-what-follows">1.3.8. Transition to What Follows</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter3/index.html">2. Chapter 3: Frequentist Statistical Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/sampling_variability.html">2.1. Sampling Variability</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#introduction">2.1.1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#key-concepts">2.1.2. Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#mathematical-framework">2.1.3. Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#python-implementation">2.1.4. Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#examples">2.1.5. Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#summary">2.1.6. Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/statistical_estimators.html">2.2. Statistical Estimators</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#introduction">2.2.1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#key-concepts">2.2.2. Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#mathematical-framework">2.2.3. Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#python-implementation">2.2.4. Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#examples">2.2.5. Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#summary">2.2.6. Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/plugin_methods.html">2.3. Plugin Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#introduction">2.3.1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#key-concepts">2.3.2. Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#mathematical-framework">2.3.3. Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#python-implementation">2.3.4. Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#examples">2.3.5. Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#summary">2.3.6. Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/parametric_inference.html">2.4. Parametric Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#introduction">2.4.1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#key-concepts">2.4.2. Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#mathematical-framework">2.4.3. Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#python-implementation">2.4.4. Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#examples">2.4.5. Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#summary">2.4.6. Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/exponential_families.html">2.5. Exponential Families</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#introduction">2.5.1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#key-concepts">2.5.2. Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#mathematical-framework">2.5.3. Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#python-implementation">2.5.4. Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#examples">2.5.5. Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#summary">2.5.6. Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/maximum_likelihood.html">2.6. Maximum Likelihood</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#introduction">2.6.1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#key-concepts">2.6.2. Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#mathematical-framework">2.6.3. Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#python-implementation">2.6.4. Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#examples">2.6.5. Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#summary">2.6.6. Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/linear_models.html">2.7. Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#introduction">2.7.1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#key-concepts">2.7.2. Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#mathematical-framework">2.7.3. Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#python-implementation">2.7.4. Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#examples">2.7.5. Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#summary">2.7.6. Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/generalized_linear_models.html">2.8. Generalized Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#introduction">2.8.1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#key-concepts">2.8.2. Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#mathematical-framework">2.8.3. Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#python-implementation">2.8.4. Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#examples">2.8.5. Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#summary">2.8.6. Summary</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter4/index.html">3. Chapter 4: Resampling Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/jackknife_introduction.html">3.1. Jackknife Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#introduction">3.1.1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#key-concepts">3.1.2. Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#mathematical-framework">3.1.3. Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#python-implementation">3.1.4. Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#examples">3.1.5. Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#summary">3.1.6. Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html">3.2. Bootstrap Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#introduction">3.2.1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#key-concepts">3.2.2. Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#mathematical-framework">3.2.3. Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#python-implementation">3.2.4. Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#examples">3.2.5. Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#summary">3.2.6. Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html">3.3. Nonparametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#introduction">3.3.1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#key-concepts">3.3.2. Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#mathematical-framework">3.3.3. Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#python-implementation">3.3.4. Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#examples">3.3.5. Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#summary">3.3.6. Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/parametric_bootstrap.html">3.4. Parametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#introduction">3.4.1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#key-concepts">3.4.2. Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#mathematical-framework">3.4.3. Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#python-implementation">3.4.4. Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#examples">3.4.5. Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#summary">3.4.6. Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/confidence_intervals.html">3.5. Confidence Intervals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#introduction">3.5.1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#key-concepts">3.5.2. Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#mathematical-framework">3.5.3. Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#python-implementation">3.5.4. Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#examples">3.5.5. Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#summary">3.5.6. Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/bias_correction.html">3.6. Bias Correction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#introduction">3.6.1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#key-concepts">3.6.2. Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#mathematical-framework">3.6.3. Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#python-implementation">3.6.4. Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#examples">3.6.5. Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#summary">3.6.6. Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/cross_validation_loo.html">3.7. Cross Validation Loo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#introduction">3.7.1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#key-concepts">3.7.2. Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#mathematical-framework">3.7.3. Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#python-implementation">3.7.4. Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#examples">3.7.5. Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#summary">3.7.6. Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html">3.8. Cross Validation K Fold</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#introduction">3.8.1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#key-concepts">3.8.2. Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#mathematical-framework">3.8.3. Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#python-implementation">3.8.4. Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#examples">3.8.5. Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#summary">3.8.6. Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/model_selection.html">3.9. Model Selection</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#introduction">3.9.1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#key-concepts">3.9.2. Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#mathematical-framework">3.9.3. Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#python-implementation">3.9.4. Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#examples">3.9.5. Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#summary">3.9.6. Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part3_bayesian/index.html">2. Part III: Bayesian Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part3_bayesian/index.html#overview">2.1. Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html">2.1.1. Bayesian Philosophy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Part II: Simulation-Based Methods</a></li>
          <li class="breadcrumb-item"><a href="index.html"><span class="section-number">1. </span>Chapter 2: Monte Carlo Simulation</a></li>
      <li class="breadcrumb-item active"><span class="section-number">1.1. </span>Monte Carlo Fundamentals</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/part2_simulation/chapter2/ch2.1-monte-carlo-fundamentals.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="monte-carlo-fundamentals">
<span id="ch2-1-monte-carlo-fundamentals"></span><h1><span class="section-number">1.1. </span>Monte Carlo Fundamentals<a class="headerlink" href="#monte-carlo-fundamentals" title="Link to this heading">ÔÉÅ</a></h1>
<p>In the spring of 1946, the mathematician Stanislaw Ulam was recovering from a near-fatal case of viral encephalitis at his home in Los Angeles. To pass the time during his convalescence, he played countless games of solitaire‚Äîand found himself wondering: what is the probability of winning a game of Canfield solitaire? The combinatorics were hopelessly complex. There were too many possible configurations, too many branching paths through a game, to enumerate them all. But Ulam realized something profound: he didn‚Äôt need to enumerate every possibility. He could simply <em>play</em> a hundred games and count how many he won.</p>
<p>This insight‚Äîthat we can estimate probabilities by running experiments rather than computing them analytically‚Äîwas not new. The Comte de Buffon had used a similar approach in 1777 to estimate œÄ by dropping needles onto a lined floor. But Ulam saw something that Buffon could not have imagined: the recently completed ENIAC computer could ‚Äúplay‚Äù millions of such games, transforming a parlor trick into a serious computational method. Within weeks, Ulam had discussed the idea with his colleague John von Neumann, and the two began developing what would become one of the most powerful computational frameworks in all of science.</p>
<p>They needed a code name for this method, which they were applying to classified problems in nuclear weapons design at Los Alamos. Nicholas Metropolis suggested ‚ÄúMonte Carlo,‚Äù after the famous casino in Monaco where Ulam‚Äôs uncle had a gambling habit. The name stuck, and with it, a new era in computational science began.</p>
<p>This chapter introduces Monte Carlo methods‚Äîa family of algorithms that use random sampling to solve problems that would otherwise be intractable. We will see how randomness, properly harnessed, becomes a precision instrument for computing integrals, estimating probabilities, and approximating quantities that resist analytical attack. The ideas are simple, but their power is immense: Monte Carlo methods now pervade physics, finance, machine learning, and statistics, anywhere that high-dimensional integration or complex probability calculations arise.</p>
<div class="important admonition">
<p class="admonition-title">Road Map üß≠</p>
<ul class="simple">
<li><p><strong>Understand</strong>: The fundamental principle that Monte Carlo integration estimates integrals as expectations of random samples, and why this works via the Law of Large Numbers and Central Limit Theorem</p></li>
<li><p><strong>Develop</strong>: Deep intuition for the <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> convergence rate‚Äîwhat it means, why it arises, and its remarkable dimension-independence</p></li>
<li><p><strong>Implement</strong>: Complete Python code for Monte Carlo estimation with variance quantification, confidence intervals, and convergence diagnostics</p></li>
<li><p><strong>Evaluate</strong>: When Monte Carlo methods outperform deterministic alternatives, and how to assess estimation quality in practice</p></li>
<li><p><strong>Connect</strong>: How Monte Carlo integration motivates the random variable generation techniques of subsequent sections</p></li>
</ul>
</div>
<section id="the-historical-development-of-monte-carlo-methods">
<h2><span class="section-number">1.1.1. </span>The Historical Development of Monte Carlo Methods<a class="headerlink" href="#the-historical-development-of-monte-carlo-methods" title="Link to this heading">ÔÉÅ</a></h2>
<p>Before diving into the mathematics, it is worth understanding how Monte Carlo methods emerged and evolved. This history illuminates why the methods work, what problems motivated their development, and why they remain central to computational science today.</p>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide003_img002_8c3660db.png"><img alt="Timeline showing evolution of Monte Carlo methods from Buffon's Needle (1777) to modern neural-enhanced methods (2020s)" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide003_img002_8c3660db.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 1.25 </span><span class="caption-text"><strong>Historical Evolution of Monte Carlo Methods.</strong> This timeline traces 250 years of algorithmic innovation, from Buffon‚Äôs needle experiment in 1777 through the founding contributions of Ulam and von Neumann in the 1940s, the development of MCMC methods, resampling techniques, and modern neural-enhanced approaches. Each innovation opened new classes of problems to computational attack.</span><a class="headerlink" href="#id1" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<section id="buffon-s-needle-the-first-monte-carlo-experiment">
<h3><span class="section-number">1.1.1.1. </span>Buffon‚Äôs Needle: The First Monte Carlo Experiment<a class="headerlink" href="#buffon-s-needle-the-first-monte-carlo-experiment" title="Link to this heading">ÔÉÅ</a></h3>
<p>In 1777, Georges-Louis Leclerc, Comte de Buffon, posed a deceptively simple question: suppose we have a floor made of parallel wooden planks, each of width <span class="math notranslate nohighlight">\(d\)</span>, and we drop a needle of length <span class="math notranslate nohighlight">\(\ell \leq d\)</span> onto this floor. What is the probability that the needle crosses one of the cracks between planks?</p>
<p>To answer this, Buffon introduced what we would now recognize as a probabilistic model. Let <span class="math notranslate nohighlight">\(\theta\)</span> denote the angle between the needle and the direction of the planks, uniformly distributed on <span class="math notranslate nohighlight">\([0, \pi)\)</span>. Let <span class="math notranslate nohighlight">\(y\)</span> denote the distance from the needle‚Äôs center to the nearest crack, uniformly distributed on <span class="math notranslate nohighlight">\([0, d/2]\)</span>. The needle crosses a crack if and only if the vertical projection of half the needle exceeds the distance to the crack‚Äîthat is, if <span class="math notranslate nohighlight">\(y \leq \frac{\ell}{2} \sin\theta\)</span>.</p>
<p>The probability of crossing is therefore:</p>
<div class="math notranslate nohighlight">
\[P(\text{crossing}) = \frac{1}{\pi} \cdot \frac{2}{d} \int_0^{\pi} \int_0^{(\ell/2)\sin\theta} dy \, d\theta\]</div>
<p>Evaluating the inner integral yields <span class="math notranslate nohighlight">\(\frac{\ell}{2}\sin\theta\)</span>, and the outer integral gives <span class="math notranslate nohighlight">\(\int_0^{\pi} \sin\theta \, d\theta = 2\)</span>. Thus:</p>
<div class="math notranslate nohighlight">
\[P(\text{crossing}) = \frac{1}{\pi} \cdot \frac{2}{d} \cdot \frac{\ell}{2} \cdot 2 = \frac{2\ell}{\pi d}\]</div>
<p>This elegant result has a remarkable consequence. If we drop <span class="math notranslate nohighlight">\(n\)</span> needles and observe that <span class="math notranslate nohighlight">\(k\)</span> of them cross a crack, then our estimate of the crossing probability is <span class="math notranslate nohighlight">\(\hat{p} = k/n\)</span>. Rearranging Buffon‚Äôs formula:</p>
<div class="math notranslate nohighlight">
\[\pi = \frac{2\ell}{d \cdot P(\text{crossing})} \approx \frac{2\ell n}{d k}\]</div>
<p>We can estimate <span class="math notranslate nohighlight">\(\pi\)</span> by throwing needles!</p>
<p>This is a Monte Carlo method avant la lettre: we use random experiments to estimate a deterministic quantity. Of course, Buffon lacked computers, and actually throwing thousands of needles by hand is tedious. In 1901, the Italian mathematician Mario Lazzarini claimed to have obtained <span class="math notranslate nohighlight">\(\pi \approx 3.1415929\)</span> by throwing a needle 3,408 times‚Äîsuspiciously close to the correct value of <span class="math notranslate nohighlight">\(355/113\)</span>. Most historians believe Lazzarini fudged his data, but the underlying principle was sound.</p>
<div class="tip admonition">
<p class="admonition-title">Try It Yourself üñ•Ô∏è Buffon‚Äôs Needle Simulation</p>
<p>Experience Buffon‚Äôs experiment interactively:</p>
<p><strong>Interactive Simulation</strong>: <a class="reference external" href="https://treese41528.github.io/ComputationalDataScience/Simulations/MonteCarloSimulation/buffons_needle_simulation.html">https://treese41528.github.io/ComputationalDataScience/Simulations/MonteCarloSimulation/buffons_needle_simulation.html</a></p>
<p>Watch how the <span class="math notranslate nohighlight">\(\pi\)</span> estimate fluctuates wildly with few needles, then gradually stabilizes as you accumulate thousands of throws. This is the Law of Large Numbers in action‚Äîa theme we will return to throughout this chapter.</p>
</div>
</section>
<section id="fermi-s-envelope-calculations">
<h3><span class="section-number">1.1.1.2. </span>Fermi‚Äôs Envelope Calculations<a class="headerlink" href="#fermi-s-envelope-calculations" title="Link to this heading">ÔÉÅ</a></h3>
<p>The physicist Enrico Fermi was famous for his ability to estimate quantities that seemed impossibly difficult to calculate. How many piano tuners are there in Chicago? How much energy is released in a nuclear explosion? Fermi would break these problems into pieces, estimate each piece roughly, and multiply‚Äîoften achieving answers accurate to within an order of magnitude.</p>
<p>Less well known is that Fermi also pioneered proto-Monte Carlo methods. In the 1930s, working on neutron diffusion problems in Rome, Fermi developed a mechanical device‚Äîessentially a specialized slide rule‚Äîthat could generate random numbers to simulate neutron paths through matter. He used these simulations to estimate quantities like neutron absorption cross-sections, which were too complex to compute analytically.</p>
<p>This work remained largely unpublished, but it anticipated the key insight of Monte Carlo: when a deterministic calculation is intractable, a stochastic simulation may succeed. Fermi‚Äôs physical random number generator was crude, but the principle was the same one that Ulam and von Neumann would later implement on electronic computers.</p>
</section>
<section id="the-manhattan-project-and-eniac">
<h3><span class="section-number">1.1.1.3. </span>The Manhattan Project and ENIAC<a class="headerlink" href="#the-manhattan-project-and-eniac" title="Link to this heading">ÔÉÅ</a></h3>
<p>The development of nuclear weapons during World War II created an urgent need for computational methods. The behavior of neutrons in a nuclear reaction‚Äîhow they scatter, slow down, and trigger fission‚Äîdepends on complex integrals over energy and angle that resist analytical solution. The physicists at Los Alamos needed numbers, not theorems.</p>
<p>It was in this context that Ulam‚Äôs solitaire insight proved transformative. Ulam and von Neumann realized that the same principle‚Äîestimate a complicated quantity by averaging over random samples‚Äîcould be applied to neutron transport. Instead of integrating over all possible neutron paths analytically (impossible), they could simulate thousands of individual neutrons, tracking each one as it scattered and absorbed through the weapon‚Äôs core.</p>
<p>Von Neumann took the lead in implementing these ideas on ENIAC, one of the first general-purpose electronic computers. ENIAC could perform about 5,000 operations per second‚Äîglacially slow by modern standards, but revolutionary in 1946. Von Neumann and his team programmed ENIAC to simulate neutron histories, and the results helped validate the design of thermonuclear weapons.</p>
<p>The ‚ÄúMonte Carlo method‚Äù was formally introduced to the broader scientific community in a 1949 paper by Metropolis and Ulam, though much of the early work remained classified for decades. The name, coined by Metropolis, captured both the element of chance central to the method and the slightly disreputable excitement of gambling‚Äîa fitting tribute to Ulam‚Äôs card-playing origins.</p>
</section>
<section id="why-monte-carlo-changed-everything">
<h3><span class="section-number">1.1.1.4. </span>Why ‚ÄúMonte Carlo‚Äù Changed Everything<a class="headerlink" href="#why-monte-carlo-changed-everything" title="Link to this heading">ÔÉÅ</a></h3>
<p>The Monte Carlo revolution was not merely about having faster computers. It represented a conceptual breakthrough: <em>randomness is a computational resource</em>. By embracing uncertainty rather than fighting it, Monte Carlo methods could attack problems that deterministic methods could not touch.</p>
<p>Consider the challenge of computing a 100-dimensional integral. Deterministic quadrature methods‚Äîthe trapezoidal rule, Simpson‚Äôs rule, Gaussian quadrature‚Äîall suffer from the ‚Äúcurse of dimensionality.‚Äù If we use <span class="math notranslate nohighlight">\(n\)</span> points per dimension, we need <span class="math notranslate nohighlight">\(n^{100}\)</span> total evaluations. Even with <span class="math notranslate nohighlight">\(n = 2\)</span>, this exceeds <span class="math notranslate nohighlight">\(10^{30}\)</span>‚Äîmore function evaluations than atoms in a human body.</p>
<p>Monte Carlo methods sidestep this curse entirely. As we will see, the error in a Monte Carlo estimate depends only on the number of samples and the variance of the integrand, not on the dimension of the space. A 100-dimensional integral is no harder than a one-dimensional integral, at least in terms of convergence rate. This dimension-independence is the source of Monte Carlo‚Äôs power.</p>
</section>
</section>
<section id="the-core-principle-expectation-as-integration">
<h2><span class="section-number">1.1.2. </span>The Core Principle: Expectation as Integration<a class="headerlink" href="#the-core-principle-expectation-as-integration" title="Link to this heading">ÔÉÅ</a></h2>
<p>We now turn to the mathematical foundations of Monte Carlo integration. The key insight is simple but profound: any integral can be rewritten as an expected value, and expected values can be estimated by averaging samples.</p>
<section id="from-integrals-to-expectations">
<h3><span class="section-number">1.1.2.1. </span>From Integrals to Expectations<a class="headerlink" href="#from-integrals-to-expectations" title="Link to this heading">ÔÉÅ</a></h3>
<p>Consider a general integral of the form:</p>
<div class="math notranslate nohighlight" id="equation-general-integral">
<span class="eqno">(1.1)<a class="headerlink" href="#equation-general-integral" title="Link to this equation">ÔÉÅ</a></span>\[I = \int_{\mathcal{X}} h(x) \, dx\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{X} \subseteq \mathbb{R}^d\)</span> is the domain of integration and <span class="math notranslate nohighlight">\(h: \mathcal{X} \to \mathbb{R}\)</span> is the function we wish to integrate. At first glance, this seems like a problem for calculus, not probability. But watch what happens when we introduce a probability density.</p>
<p>Let <span class="math notranslate nohighlight">\(f(x)\)</span> be any probability density function on <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>‚Äîthat is, <span class="math notranslate nohighlight">\(f(x) \geq 0\)</span> everywhere and <span class="math notranslate nohighlight">\(\int_{\mathcal{X}} f(x) \, dx = 1\)</span>. We can rewrite our integral as:</p>
<div class="math notranslate nohighlight" id="equation-importance-rewrite">
<span class="eqno">(1.2)<a class="headerlink" href="#equation-importance-rewrite" title="Link to this equation">ÔÉÅ</a></span>\[I = \int_{\mathcal{X}} h(x) \, dx = \int_{\mathcal{X}} \frac{h(x)}{f(x)} f(x) \, dx = \mathbb{E}_f\left[ \frac{h(X)}{f(X)} \right]\]</div>
<p>where the expectation is taken over a random variable <span class="math notranslate nohighlight">\(X\)</span> with density <span class="math notranslate nohighlight">\(f\)</span>. We have transformed an integral into an expected value!</p>
<p>The simplest choice is the uniform density on <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. If <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> has finite volume <span class="math notranslate nohighlight">\(V = \int_{\mathcal{X}} dx\)</span>, then <span class="math notranslate nohighlight">\(f(x) = 1/V\)</span> is a valid density, and:</p>
<div class="math notranslate nohighlight">
\[I = V \cdot \mathbb{E}_{\text{Uniform}(\mathcal{X})}[h(X)]\]</div>
<p>For example, to compute <span class="math notranslate nohighlight">\(\int_0^1 e^{-x^2} dx\)</span>, we write:</p>
<div class="math notranslate nohighlight">
\[\int_0^1 e^{-x^2} dx = \mathbb{E}[e^{-U^2}] \quad \text{where } U \sim \text{Uniform}(0, 1)\]</div>
<p>This rewriting is always possible. But why is it useful?</p>
</section>
<section id="the-monte-carlo-estimator">
<h3><span class="section-number">1.1.2.2. </span>The Monte Carlo Estimator<a class="headerlink" href="#the-monte-carlo-estimator" title="Link to this heading">ÔÉÅ</a></h3>
<p>The power of the expectation formulation becomes clear when we recall the Law of Large Numbers. If <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> are independent and identically distributed (iid) with <span class="math notranslate nohighlight">\(\mathbb{E}[X_i] = \mu\)</span>, then the sample mean converges to the true mean:</p>
<div class="math notranslate nohighlight">
\[\bar{X}_n = \frac{1}{n} \sum_{i=1}^{n} X_i \xrightarrow{\text{a.s.}} \mu \quad \text{as } n \to \infty\]</div>
<p>Applied to our integral:</p>
<div class="definition admonition">
<p class="admonition-title">Definition: Monte Carlo Estimator</p>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> be iid samples from a density <span class="math notranslate nohighlight">\(f\)</span> on <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. The <strong>Monte Carlo estimator</strong> of the integral <span class="math notranslate nohighlight">\(I = \int_{\mathcal{X}} h(x) f(x) \, dx = \mathbb{E}_f[h(X)]\)</span> is:</p>
<div class="math notranslate nohighlight" id="equation-mc-estimator-def">
<span class="eqno">(1.3)<a class="headerlink" href="#equation-mc-estimator-def" title="Link to this equation">ÔÉÅ</a></span>\[\hat{I}_n = \frac{1}{n} \sum_{i=1}^{n} h(X_i)\]</div>
<p>More generally, for <span class="math notranslate nohighlight">\(I = \int_{\mathcal{X}} g(x) \, dx\)</span> where we sample from density <span class="math notranslate nohighlight">\(f\)</span>:</p>
<div class="math notranslate nohighlight">
\[\hat{I}_n = \frac{1}{n} \sum_{i=1}^{n} \frac{g(X_i)}{f(X_i)}\]</div>
</div>
<p>The Monte Carlo method is disarmingly simple: draw random samples, evaluate the function at each sample, and average the results. No derivatives, no quadrature weights, no mesh generation‚Äîjust sampling and averaging.</p>
<p>But this simplicity conceals depth. The choice of sampling density <span class="math notranslate nohighlight">\(f\)</span> is entirely up to us, and different choices lead to dramatically different performance. We will explore this in the section on importance sampling; for now, we focus on the ‚Äúnaive‚Äù case where <span class="math notranslate nohighlight">\(f\)</span> matches the density of the integrand or is uniform on the domain.</p>
</section>
<section id="a-first-example-estimating-pi">
<h3><span class="section-number">1.1.2.3. </span>A First Example: Estimating <span class="math notranslate nohighlight">\(\pi\)</span><a class="headerlink" href="#a-first-example-estimating-pi" title="Link to this heading">ÔÉÅ</a></h3>
<p>Let us return to the problem of estimating <span class="math notranslate nohighlight">\(\pi\)</span>, now with Monte Carlo machinery. Consider the integral:</p>
<div class="math notranslate nohighlight">
\[\pi = \int_{-1}^{1} \int_{-1}^{1} \mathbf{1}_{x^2 + y^2 \leq 1} \, dx \, dy\]</div>
<p>This is the area of the unit disk. Rewriting as an expectation:</p>
<div class="math notranslate nohighlight">
\[\pi = 4 \cdot \mathbb{E}[\mathbf{1}_{X^2 + Y^2 \leq 1}] \quad \text{where } (X, Y) \sim \text{Uniform}([-1,1]^2)\]</div>
<p>The factor of 4 accounts for the area of the square <span class="math notranslate nohighlight">\([-1,1]^2\)</span>. The Monte Carlo estimator is:</p>
<div class="math notranslate nohighlight">
\[\hat{\pi}_n = \frac{4}{n} \sum_{i=1}^{n} \mathbf{1}_{X_i^2 + Y_i^2 \leq 1}\]</div>
<p>That is: generate <span class="math notranslate nohighlight">\(n\)</span> uniform points in the square, count how many fall inside the unit circle, and multiply by 4.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">estimate_pi_monte_carlo</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Estimate œÄ using Monte Carlo integration.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of random points to generate.</span>
<span class="sd">    seed : int, optional</span>
<span class="sd">        Random seed for reproducibility.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        Contains estimate, standard_error, and confidence interval.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Generate uniform points in [-1, 1]¬≤</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Count points inside unit circle</span>
    <span class="n">inside</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span>

    <span class="c1"># Monte Carlo estimate</span>
    <span class="n">p_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">inside</span><span class="p">)</span>
    <span class="n">pi_hat</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">p_hat</span>

    <span class="c1"># Standard error (indicator has Bernoulli variance p(1-p))</span>
    <span class="n">se_p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">p_hat</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_hat</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="n">se_pi</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">se_p</span>

    <span class="c1"># 95% confidence interval</span>
    <span class="n">ci</span> <span class="o">=</span> <span class="p">(</span><span class="n">pi_hat</span> <span class="o">-</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">se_pi</span><span class="p">,</span> <span class="n">pi_hat</span> <span class="o">+</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">se_pi</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;estimate&#39;</span><span class="p">:</span> <span class="n">pi_hat</span><span class="p">,</span>
        <span class="s1">&#39;standard_error&#39;</span><span class="p">:</span> <span class="n">se_pi</span><span class="p">,</span>
        <span class="s1">&#39;ci_95&#39;</span><span class="p">:</span> <span class="n">ci</span><span class="p">,</span>
        <span class="s1">&#39;n_samples&#39;</span><span class="p">:</span> <span class="n">n_samples</span>
    <span class="p">}</span>

<span class="c1"># Run the estimation</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">estimate_pi_monte_carlo</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;œÄ estimate: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;estimate&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True œÄ:     </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error:      </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;estimate&#39;</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Std Error:  </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;standard_error&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% CI:     (</span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci_95&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci_95&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Running this code with 100,000 samples yields an estimate around 3.143 with standard error about 0.005. The true value <span class="math notranslate nohighlight">\(\pi \approx 3.14159\)</span> lies comfortably within the confidence interval. With a million samples, the error shrinks by a factor of <span class="math notranslate nohighlight">\(\sqrt{10} \approx 3.16\)</span>, and with ten million, by another factor of <span class="math notranslate nohighlight">\(\sqrt{10}\)</span>.</p>
<p>This is Monte Carlo at its most basic: evaluate a simple function at random points and average. But even this toy example illustrates the key features of the method‚Äîease of implementation, probabilistic error bounds, and graceful scaling with sample size.</p>
</section>
</section>
<section id="theoretical-foundations">
<h2><span class="section-number">1.1.3. </span>Theoretical Foundations<a class="headerlink" href="#theoretical-foundations" title="Link to this heading">ÔÉÅ</a></h2>
<p>Why does the Monte Carlo method work? What determines the rate of convergence? These questions have precise mathematical answers rooted in classical probability theory.</p>
<section id="the-law-of-large-numbers">
<h3><span class="section-number">1.1.3.1. </span>The Law of Large Numbers<a class="headerlink" href="#the-law-of-large-numbers" title="Link to this heading">ÔÉÅ</a></h3>
<p>The Law of Large Numbers (LLN) is the foundational result guaranteeing that Monte Carlo estimators converge to the true value. There are several versions; we state the strongest form.</p>
<div class="theorem admonition">
<p class="admonition-title">Theorem: Strong Law of Large Numbers</p>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots\)</span> be independent and identically distributed random variables with <span class="math notranslate nohighlight">\(\mathbb{E}[|X_1|] &lt; \infty\)</span>. Then:</p>
<div class="math notranslate nohighlight">
\[\bar{X}_n = \frac{1}{n} \sum_{i=1}^{n} X_i \xrightarrow{\text{a.s.}} \mathbb{E}[X_1] \quad \text{as } n \to \infty\]</div>
<p>The notation <span class="math notranslate nohighlight">\(\xrightarrow{\text{a.s.}}\)</span> denotes <em>almost sure convergence</em>: the probability that the sequence converges is exactly 1.</p>
</div>
<p>For Monte Carlo integration, we apply this theorem with <span class="math notranslate nohighlight">\(X_i = h(X_i)\)</span> where the <span class="math notranslate nohighlight">\(X_i\)</span> are iid from density <span class="math notranslate nohighlight">\(f\)</span>. The condition <span class="math notranslate nohighlight">\(\mathbb{E}[|h(X)|] &lt; \infty\)</span> ensures that the integral we are estimating actually exists.</p>
<p>The LLN tells us that <span class="math notranslate nohighlight">\(\hat{I}_n \to I\)</span> with probability 1. No matter how complex the integrand, no matter how high the dimension, the Monte Carlo estimator will eventually get arbitrarily close to the true value. This is an extraordinarily powerful guarantee.</p>
<p>But the LLN is silent on <em>how fast</em> convergence occurs. For that, we need the Central Limit Theorem.</p>
</section>
<section id="the-central-limit-theorem-and-the-o-n-1-2-rate">
<h3><span class="section-number">1.1.3.2. </span>The Central Limit Theorem and the <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> Rate<a class="headerlink" href="#the-central-limit-theorem-and-the-o-n-1-2-rate" title="Link to this heading">ÔÉÅ</a></h3>
<p>The Central Limit Theorem (CLT) is the workhorse result for quantifying Monte Carlo error.</p>
<div class="theorem admonition">
<p class="admonition-title">Theorem: Central Limit Theorem</p>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots\)</span> be iid with mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2 &lt; \infty\)</span>. Then:</p>
<div class="math notranslate nohighlight">
\[\sqrt{n} \left( \bar{X}_n - \mu \right) \xrightarrow{d} \mathcal{N}(0, \sigma^2)\]</div>
<p>where <span class="math notranslate nohighlight">\(\xrightarrow{d}\)</span> denotes convergence in distribution. Equivalently, for large <span class="math notranslate nohighlight">\(n\)</span>:</p>
<div class="math notranslate nohighlight">
\[\bar{X}_n \stackrel{\cdot}{\sim} \mathcal{N}\left( \mu, \frac{\sigma^2}{n} \right)\]</div>
</div>
<p>Applied to Monte Carlo integration:</p>
<div class="math notranslate nohighlight">
\[\hat{I}_n \stackrel{\cdot}{\sim} \mathcal{N}\left( I, \frac{\sigma^2}{n} \right)\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma^2 = \text{Var}_f[h(X)] = \mathbb{E}_f[(h(X) - I)^2]\)</span> is the variance of the integrand under the sampling distribution.</p>
<p>The <strong>standard error</strong> of the Monte Carlo estimator is:</p>
<div class="math notranslate nohighlight" id="equation-mc-standard-error">
<span class="eqno">(1.4)<a class="headerlink" href="#equation-mc-standard-error" title="Link to this equation">ÔÉÅ</a></span>\[\text{SE}(\hat{I}_n) = \frac{\sigma}{\sqrt{n}}\]</div>
<p>This is the celebrated <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> convergence rate. To reduce the error by a factor of 10, we need 100 times as many samples. To gain one decimal place of accuracy, we need 100 times the computational effort.</p>
<p>This rate may seem slow‚Äîand compared to some deterministic methods, it is. But the rate is <em>independent of dimension</em>. Whether we are integrating over <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> or <span class="math notranslate nohighlight">\(\mathbb{R}^{1000}\)</span>, the error decreases as <span class="math notranslate nohighlight">\(1/\sqrt{n}\)</span>. This dimension-independence is the source of Monte Carlo‚Äôs power in high-dimensional problems.</p>
<div class="note admonition">
<p class="admonition-title">Example üí° Understanding the Square Root Law</p>
<p><strong>Scenario</strong>: You estimate an integral with 1,000 samples and get a standard error of 0.1. Your boss needs the error reduced to 0.01.</p>
<p><strong>Analysis</strong>: The standard error scales as <span class="math notranslate nohighlight">\(1/\sqrt{n}\)</span>. To reduce error by a factor of 10, you need <span class="math notranslate nohighlight">\(n\)</span> to increase by a factor of <span class="math notranslate nohighlight">\(10^2 = 100\)</span>.</p>
<p><strong>Conclusion</strong>: You need <span class="math notranslate nohighlight">\(1000 \times 100 = 100,000\)</span> samples.</p>
<p>This quadratic penalty is the price of Monte Carlo‚Äôs simplicity. In low dimensions, deterministic methods often achieve polynomial convergence rates like <span class="math notranslate nohighlight">\(O(n^{-2})\)</span> or better, making them far more efficient. But in high dimensions, Monte Carlo‚Äôs dimension-independent <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> beats any polynomial rate that degrades with dimension.</p>
</div>
</section>
<section id="why-the-square-root">
<h3><span class="section-number">1.1.3.3. </span>Why the Square Root?<a class="headerlink" href="#why-the-square-root" title="Link to this heading">ÔÉÅ</a></h3>
<p>The <span class="math notranslate nohighlight">\(1/\sqrt{n}\)</span> rate may seem mysterious, but it has a simple explanation rooted in the behavior of sums of random variables.</p>
<p>Consider <span class="math notranslate nohighlight">\(n\)</span> iid random variables <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span>, each with variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>. Their sum has variance:</p>
<div class="math notranslate nohighlight">
\[\text{Var}\left( \sum_{i=1}^n X_i \right) = \sum_{i=1}^n \text{Var}(X_i) = n\sigma^2\]</div>
<p>The variance of the sum grows linearly with <span class="math notranslate nohighlight">\(n\)</span>. But when we take the mean, we divide by <span class="math notranslate nohighlight">\(n\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{Var}\left( \frac{1}{n} \sum_{i=1}^n X_i \right) = \frac{1}{n^2} \cdot n\sigma^2 = \frac{\sigma^2}{n}\]</div>
<p>The standard deviation is the square root of variance, giving <span class="math notranslate nohighlight">\(\sigma/\sqrt{n}\)</span>.</p>
<p>This behavior is fundamental to averages of random quantities. Each additional sample adds information, but with diminishing returns: the first sample reduces uncertainty enormously; the millionth sample contributes almost nothing. This is why the square root appears.</p>
</section>
</section>
<section id="variance-estimation-and-confidence-intervals">
<h2><span class="section-number">1.1.4. </span>Variance Estimation and Confidence Intervals<a class="headerlink" href="#variance-estimation-and-confidence-intervals" title="Link to this heading">ÔÉÅ</a></h2>
<p>The CLT tells us that <span class="math notranslate nohighlight">\(\hat{I}_n\)</span> is approximately normal with known variance <span class="math notranslate nohighlight">\(\sigma^2/n\)</span>. But we rarely know <span class="math notranslate nohighlight">\(\sigma^2\)</span>‚Äîit depends on the integrand and the sampling distribution. We must estimate it from the same samples we use to estimate <span class="math notranslate nohighlight">\(I\)</span>.</p>
<section id="the-sample-variance">
<h3><span class="section-number">1.1.4.1. </span>The Sample Variance<a class="headerlink" href="#the-sample-variance" title="Link to this heading">ÔÉÅ</a></h3>
<p>The natural estimator of <span class="math notranslate nohighlight">\(\sigma^2 = \text{Var}[h(X)]\)</span> is the sample variance:</p>
<div class="math notranslate nohighlight" id="equation-sample-var">
<span class="eqno">(1.5)<a class="headerlink" href="#equation-sample-var" title="Link to this equation">ÔÉÅ</a></span>\[\hat{\sigma}^2_n = \frac{1}{n-1} \sum_{i=1}^{n} \left( h(X_i) - \hat{I}_n \right)^2\]</div>
<p>The divisor <span class="math notranslate nohighlight">\(n-1\)</span> (rather than <span class="math notranslate nohighlight">\(n\)</span>) makes this estimator unbiased: <span class="math notranslate nohighlight">\(\mathbb{E}[\hat{\sigma}^2_n] = \sigma^2\)</span>. This is known as Bessel‚Äôs correction.</p>
<p>By the Law of Large Numbers, <span class="math notranslate nohighlight">\(\hat{\sigma}^2_n \to \sigma^2\)</span> almost surely as <span class="math notranslate nohighlight">\(n \to \infty\)</span>. Combined with the CLT, this gives us a practical way to construct confidence intervals.</p>
</section>
<section id="constructing-confidence-intervals">
<h3><span class="section-number">1.1.4.2. </span>Constructing Confidence Intervals<a class="headerlink" href="#constructing-confidence-intervals" title="Link to this heading">ÔÉÅ</a></h3>
<p>An asymptotic <span class="math notranslate nohighlight">\((1-\alpha)\)</span> confidence interval for <span class="math notranslate nohighlight">\(I\)</span> is:</p>
<div class="math notranslate nohighlight" id="equation-mc-ci-full">
<span class="eqno">(1.6)<a class="headerlink" href="#equation-mc-ci-full" title="Link to this equation">ÔÉÅ</a></span>\[\left[ \hat{I}_n - z_{\alpha/2} \frac{\hat{\sigma}_n}{\sqrt{n}}, \quad \hat{I}_n + z_{\alpha/2} \frac{\hat{\sigma}_n}{\sqrt{n}} \right]\]</div>
<p>where <span class="math notranslate nohighlight">\(z_{\alpha/2} = \Phi^{-1}(1 - \alpha/2)\)</span> is the standard normal quantile. For common confidence levels:</p>
<ul class="simple">
<li><p>90% CI: <span class="math notranslate nohighlight">\(z_{0.05} \approx 1.645\)</span></p></li>
<li><p>95% CI: <span class="math notranslate nohighlight">\(z_{0.025} \approx 1.960\)</span></p></li>
<li><p>99% CI: <span class="math notranslate nohighlight">\(z_{0.005} \approx 2.576\)</span></p></li>
</ul>
<p>The interval has the interpretation: in repeated sampling, approximately <span class="math notranslate nohighlight">\((1-\alpha) \times 100\%\)</span> of such intervals will contain the true value <span class="math notranslate nohighlight">\(I\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">monte_carlo_integrate</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">sampler</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">confidence</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Monte Carlo integration with uncertainty quantification.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    h : callable</span>
<span class="sd">        Function to integrate. Must accept array input.</span>
<span class="sd">    sampler : callable</span>
<span class="sd">        Function that returns n samples from the target distribution.</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of Monte Carlo samples.</span>
<span class="sd">    confidence : float</span>
<span class="sd">        Confidence level for interval (default 0.95).</span>
<span class="sd">    seed : int, optional</span>
<span class="sd">        Random seed for reproducibility.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        Contains estimate, std_error, confidence interval, and diagnostics.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Generate samples and evaluate function</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">sampler</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="n">h_values</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

    <span class="c1"># Point estimate</span>
    <span class="n">estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_values</span><span class="p">)</span>

    <span class="c1"># Variance estimation (Bessel&#39;s correction)</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">h_values</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">std_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">variance</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Confidence interval</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">confidence</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">ci_lower</span> <span class="o">=</span> <span class="n">estimate</span> <span class="o">-</span> <span class="n">z</span> <span class="o">*</span> <span class="n">std_error</span>
    <span class="n">ci_upper</span> <span class="o">=</span> <span class="n">estimate</span> <span class="o">+</span> <span class="n">z</span> <span class="o">*</span> <span class="n">std_error</span>

    <span class="c1"># Effective sample size (for future variance reduction comparisons)</span>
    <span class="n">ess</span> <span class="o">=</span> <span class="n">n_samples</span>  <span class="c1"># For standard MC, ESS = n</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;estimate&#39;</span><span class="p">:</span> <span class="n">estimate</span><span class="p">,</span>
        <span class="s1">&#39;std_error&#39;</span><span class="p">:</span> <span class="n">std_error</span><span class="p">,</span>
        <span class="s1">&#39;variance&#39;</span><span class="p">:</span> <span class="n">variance</span><span class="p">,</span>
        <span class="s1">&#39;ci&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">ci_lower</span><span class="p">,</span> <span class="n">ci_upper</span><span class="p">),</span>
        <span class="s1">&#39;confidence&#39;</span><span class="p">:</span> <span class="n">confidence</span><span class="p">,</span>
        <span class="s1">&#39;n_samples&#39;</span><span class="p">:</span> <span class="n">n_samples</span><span class="p">,</span>
        <span class="s1">&#39;ess&#39;</span><span class="p">:</span> <span class="n">ess</span><span class="p">,</span>
        <span class="s1">&#39;h_values&#39;</span><span class="p">:</span> <span class="n">h_values</span>
    <span class="p">}</span>
</pre></div>
</div>
</section>
<section id="numerical-stability-welford-s-algorithm">
<h3><span class="section-number">1.1.4.3. </span>Numerical Stability: Welford‚Äôs Algorithm<a class="headerlink" href="#numerical-stability-welford-s-algorithm" title="Link to this heading">ÔÉÅ</a></h3>
<p>Computing the sample variance naively using <span class="math notranslate nohighlight">\(\frac{1}{n-1}(\sum h_i^2 - n\bar{h}^2)\)</span> can suffer catastrophic cancellation when the mean is large compared to the standard deviation. The two terms <span class="math notranslate nohighlight">\(\sum h_i^2\)</span> and <span class="math notranslate nohighlight">\(n\bar{h}^2\)</span> may be nearly equal, and their difference may lose many significant digits.</p>
<p>Welford‚Äôs algorithm computes the mean and variance in a single pass with guaranteed numerical stability:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">welford_online</span><span class="p">(</span><span class="n">values</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute mean and variance using Welford&#39;s online algorithm.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    values : iterable</span>
<span class="sd">        Stream of values (can be a generator).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    mean : float</span>
<span class="sd">        Sample mean.</span>
<span class="sd">    variance : float</span>
<span class="sd">        Sample variance (with Bessel&#39;s correction).</span>
<span class="sd">    n : int</span>
<span class="sd">        Number of values processed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">M2</span> <span class="o">=</span> <span class="mf">0.0</span>  <span class="c1"># Sum of squared deviations from current mean</span>

    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">values</span><span class="p">:</span>
        <span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">mean</span>
        <span class="n">mean</span> <span class="o">+=</span> <span class="n">delta</span> <span class="o">/</span> <span class="n">n</span>
        <span class="n">delta2</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">mean</span>  <span class="c1"># Updated mean</span>
        <span class="n">M2</span> <span class="o">+=</span> <span class="n">delta</span> <span class="o">*</span> <span class="n">delta2</span>

    <span class="k">if</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;nan&#39;</span><span class="p">),</span> <span class="n">n</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="n">M2</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">n</span>

<span class="c1"># Example: large mean, small variance</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">large_mean_data</span> <span class="o">=</span> <span class="mf">1e8</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span>

<span class="c1"># Naive method (may have precision issues)</span>
<span class="n">naive_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">large_mean_data</span><span class="p">)</span>
<span class="n">naive_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">large_mean_data</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="mi">9999</span> <span class="o">-</span> <span class="mi">10000</span> <span class="o">*</span> <span class="n">naive_mean</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mi">9999</span>

<span class="c1"># Welford&#39;s method (stable)</span>
<span class="n">welford_mean</span><span class="p">,</span> <span class="n">welford_var</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">welford_online</span><span class="p">(</span><span class="n">large_mean_data</span><span class="p">)</span>

<span class="c1"># NumPy&#39;s implementation (also stable)</span>
<span class="n">numpy_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">large_mean_data</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Naive variance:   </span><span class="si">{</span><span class="n">naive_var</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Welford variance: </span><span class="si">{</span><span class="n">welford_var</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;NumPy variance:   </span><span class="si">{</span><span class="n">numpy_var</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>In practice, NumPy‚Äôs <code class="docutils literal notranslate"><span class="pre">np.var</span></code> uses numerically stable algorithms, so you rarely need to implement Welford‚Äôs algorithm yourself. But understanding why stability matters is important for debugging unexpected results.</p>
</section>
</section>
<section id="worked-examples">
<h2><span class="section-number">1.1.5. </span>Worked Examples<a class="headerlink" href="#worked-examples" title="Link to this heading">ÔÉÅ</a></h2>
<p>We now work through several examples of increasing complexity, illustrating the breadth of Monte Carlo applications.</p>
<section id="example-1-the-gaussian-integral">
<h3><span class="section-number">1.1.5.1. </span>Example 1: The Gaussian Integral<a class="headerlink" href="#example-1-the-gaussian-integral" title="Link to this heading">ÔÉÅ</a></h3>
<p>The integral <span class="math notranslate nohighlight">\(\int_{-\infty}^{\infty} e^{-x^2} dx = \sqrt{\pi}\)</span> is famous for being impossible to evaluate in closed form using elementary antiderivatives, yet having a beautiful exact answer. Let us estimate it via Monte Carlo.</p>
<p><strong>Challenge</strong>: The domain is infinite, so we cannot sample uniformly.</p>
<p><strong>Solution</strong>: Recognize the integrand as proportional to a Gaussian density. If <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(0, 1/\sqrt{2})\)</span>, then:</p>
<div class="math notranslate nohighlight">
\[f(x) = \sqrt{\frac{1}{\pi}} e^{-x^2}\]</div>
<p>This is a valid probability density (it integrates to 1). We can write:</p>
<div class="math notranslate nohighlight">
\[\int_{-\infty}^{\infty} e^{-x^2} dx = \int_{-\infty}^{\infty} \frac{e^{-x^2}}{f(x)} f(x) dx = \sqrt{\pi} \int_{-\infty}^{\infty} f(x) dx = \sqrt{\pi}\]</div>
<p>This derivation shows the integral equals <span class="math notranslate nohighlight">\(\sqrt{\pi}\)</span> exactly, but let‚Äôs also verify by Monte Carlo.</p>
<p><strong>Alternative approach</strong>: Sample from a distribution that covers the domain and reweight:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">gaussian_integral_mc</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Estimate ‚à´ exp(-x¬≤) dx via Monte Carlo.</span>

<span class="sd">    We sample from N(0, 1) and use importance sampling:</span>
<span class="sd">    ‚à´ exp(-x¬≤) dx = ‚à´ [exp(-x¬≤) / œÜ(x)] œÜ(x) dx</span>
<span class="sd">    where œÜ(x) is the standard normal density.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Sample from N(0, 1)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Integrand: exp(-x¬≤)</span>
    <span class="c1"># Sampling density: œÜ(x) = exp(-x¬≤/2) / sqrt(2œÄ)</span>
    <span class="c1"># Ratio: exp(-x¬≤) / œÜ(x) = sqrt(2œÄ) * exp(-x¬≤/2)</span>

    <span class="n">h_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">X</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>

    <span class="n">estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_values</span><span class="p">)</span>
    <span class="n">std_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">h_values</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">estimate</span><span class="p">,</span> <span class="n">std_error</span>

<span class="n">est</span><span class="p">,</span> <span class="n">se</span> <span class="o">=</span> <span class="n">gaussian_integral_mc</span><span class="p">(</span><span class="mi">100_000</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimate: </span><span class="si">{</span><span class="n">est</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> ¬± </span><span class="si">{</span><span class="n">se</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True value: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The estimate will be very close to <span class="math notranslate nohighlight">\(\sqrt{\pi} \approx 1.7725\)</span>.</p>
</section>
<section id="example-2-probability-of-a-rare-event">
<h3><span class="section-number">1.1.5.2. </span>Example 2: Probability of a Rare Event<a class="headerlink" href="#example-2-probability-of-a-rare-event" title="Link to this heading">ÔÉÅ</a></h3>
<p>Suppose <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(0, 1)\)</span>. What is <span class="math notranslate nohighlight">\(P(X &gt; 4)\)</span>?</p>
<p>From standard normal tables, <span class="math notranslate nohighlight">\(P(X &gt; 4) = 1 - \Phi(4) \approx 3.167 \times 10^{-5}\)</span>. This is a rare event‚Äîonly about 3 in 100,000 standard normal draws exceed 4.</p>
<p><strong>Naive Monte Carlo</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100_000</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">p_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span> <span class="o">&gt;</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">p_hat</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_hat</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimate: </span><span class="si">{</span><span class="n">p_hat</span><span class="si">:</span><span class="s2">.6e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Std Error: </span><span class="si">{</span><span class="n">se</span><span class="si">:</span><span class="s2">.6e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True value: </span><span class="si">{</span><span class="mi">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="si">:</span><span class="s2">.6e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>With 100,000 samples, we might observe only 3-4 exceedances, giving a highly variable estimate. The relative error (standard error divided by estimate) is enormous.</p>
<p><strong>Problem</strong>: To estimate a probability <span class="math notranslate nohighlight">\(p\)</span>, the standard error is <span class="math notranslate nohighlight">\(\sqrt{p(1-p)/n} \approx \sqrt{p/n}\)</span> for small <span class="math notranslate nohighlight">\(p\)</span>. The relative error is <span class="math notranslate nohighlight">\(\sqrt{(1-p)/(np)} \approx 1/\sqrt{np}\)</span>. To achieve 10% relative error for <span class="math notranslate nohighlight">\(p = 10^{-5}\)</span>, we need <span class="math notranslate nohighlight">\(n \approx 100/p = 10^7\)</span> samples.</p>
<p>This motivates <strong>importance sampling</strong> (covered in a later section), which generates samples preferentially in the region of interest. For now, the lesson is that naive Monte Carlo struggles with rare events.</p>
</section>
<section id="example-3-a-high-dimensional-integral">
<h3><span class="section-number">1.1.5.3. </span>Example 3: A High-Dimensional Integral<a class="headerlink" href="#example-3-a-high-dimensional-integral" title="Link to this heading">ÔÉÅ</a></h3>
<p>Consider the integral:</p>
<div class="math notranslate nohighlight">
\[I = \int_{[0,1]^d} \prod_{j=1}^{d} \left( 1 + \frac{x_j}{d} \right) dx_1 \cdots dx_d\]</div>
<p>For large <span class="math notranslate nohighlight">\(d\)</span>, this integral is analytically tractable:</p>
<div class="math notranslate nohighlight">
\[I = \prod_{j=1}^{d} \int_0^1 \left( 1 + \frac{x_j}{d} \right) dx_j = \left( 1 + \frac{1}{2d} \right)^d \xrightarrow{d \to \infty} \sqrt{e}\]</div>
<p>Let us estimate this integral in <span class="math notranslate nohighlight">\(d = 100\)</span> dimensions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">high_dim_integrand</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute product integrand: ‚àè(1 + x_j/d).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : ndarray of shape (n_samples, d)</span>
<span class="sd">        Sample points in [0,1]^d.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ndarray of shape (n_samples,)</span>
<span class="sd">        Function values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">X</span> <span class="o">/</span> <span class="n">d</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">mc_high_dim</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Monte Carlo integration in d dimensions.&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Uniform samples in [0,1]^d</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>

    <span class="c1"># Evaluate integrand</span>
    <span class="n">h_values</span> <span class="o">=</span> <span class="n">high_dim_integrand</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="n">estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_values</span><span class="p">)</span>
    <span class="n">std_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">h_values</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># True value (for comparison)</span>
    <span class="n">true_value</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">/</span><span class="n">d</span><span class="p">)</span><span class="o">**</span><span class="n">d</span>

    <span class="k">return</span> <span class="n">estimate</span><span class="p">,</span> <span class="n">std_error</span><span class="p">,</span> <span class="n">true_value</span>

<span class="c1"># Estimate in 100 dimensions</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">est</span><span class="p">,</span> <span class="n">se</span><span class="p">,</span> <span class="n">truth</span> <span class="o">=</span> <span class="n">mc_high_dim</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="mi">100_000</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;d = </span><span class="si">{</span><span class="n">d</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimate: </span><span class="si">{</span><span class="n">est</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> ¬± </span><span class="si">{</span><span class="n">se</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True value: </span><span class="si">{</span><span class="n">truth</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error: </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">est</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">truth</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The Monte Carlo estimate converges as <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> regardless of <span class="math notranslate nohighlight">\(d\)</span>. Try increasing <span class="math notranslate nohighlight">\(d\)</span> to 1000 or 10,000‚Äîthe convergence rate remains unchanged.</p>
</section>
<section id="example-4-bayesian-posterior-mean">
<h3><span class="section-number">1.1.5.4. </span>Example 4: Bayesian Posterior Mean<a class="headerlink" href="#example-4-bayesian-posterior-mean" title="Link to this heading">ÔÉÅ</a></h3>
<p>Bayesian inference often requires computing posterior expectations:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[\theta | \text{data}] = \int \theta \cdot \pi(\theta | \text{data}) \, d\theta\]</div>
<p>When the posterior <span class="math notranslate nohighlight">\(\pi(\theta | \text{data})\)</span> is available (perhaps up to a normalizing constant), Monte Carlo integration applies directly‚Äîif we can sample from the posterior. This is the motivation for Markov chain Monte Carlo methods in Part 3.</p>
<p>As a simple example, suppose we observe <span class="math notranslate nohighlight">\(x = 7\)</span> successes in <span class="math notranslate nohighlight">\(n = 10\)</span> Bernoulli trials with unknown success probability <span class="math notranslate nohighlight">\(\theta\)</span>. With a <span class="math notranslate nohighlight">\(\text{Beta}(1, 1)\)</span> (uniform) prior, the posterior is <span class="math notranslate nohighlight">\(\text{Beta}(8, 4)\)</span>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Posterior is Beta(8, 4)</span>
<span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span>
<span class="n">posterior</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>

<span class="c1"># True posterior mean</span>
<span class="n">true_mean</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">/</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="p">)</span>

<span class="c1"># Monte Carlo estimate</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10_000</span>
<span class="n">theta_samples</span> <span class="o">=</span> <span class="n">posterior</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

<span class="n">mc_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">theta_samples</span><span class="p">)</span>
<span class="n">mc_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">theta_samples</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True posterior mean: </span><span class="si">{</span><span class="n">true_mean</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MC estimate: </span><span class="si">{</span><span class="n">mc_mean</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> ¬± </span><span class="si">{</span><span class="n">mc_se</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># We can also estimate posterior quantiles, variance, etc.</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Posterior median (MC): </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">theta_samples</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Posterior median (exact): </span><span class="si">{</span><span class="n">posterior</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="example-5-the-normal-cdf">
<h3><span class="section-number">1.1.5.5. </span>Example 5: The Normal CDF<a class="headerlink" href="#example-5-the-normal-cdf" title="Link to this heading">ÔÉÅ</a></h3>
<p>The cumulative distribution function of the standard normal, <span class="math notranslate nohighlight">\(\Phi(t) = P(Z \leq t)\)</span> for <span class="math notranslate nohighlight">\(Z \sim \mathcal{N}(0, 1)\)</span>, has no closed-form expression. Yet it is one of the most important functions in statistics. Let us estimate <span class="math notranslate nohighlight">\(\Phi(1.96)\)</span>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">estimate_normal_cdf</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Estimate Œ¶(t) = P(Z ‚â§ t) for Z ~ N(0,1).&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Indicator function: 1 if Z ‚â§ t, else 0</span>
    <span class="n">indicators</span> <span class="o">=</span> <span class="n">Z</span> <span class="o">&lt;=</span> <span class="n">t</span>

    <span class="n">p_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">indicators</span><span class="p">)</span>
    <span class="n">se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">p_hat</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_hat</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">p_hat</span><span class="p">,</span> <span class="n">se</span>

<span class="n">t</span> <span class="o">=</span> <span class="mf">1.96</span>
<span class="n">est</span><span class="p">,</span> <span class="n">se</span> <span class="o">=</span> <span class="n">estimate_normal_cdf</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="mi">1_000_000</span><span class="p">)</span>
<span class="n">true_val</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Œ¶(</span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s2">) estimate: </span><span class="si">{</span><span class="n">est</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> ¬± </span><span class="si">{</span><span class="n">se</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Œ¶(</span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s2">) true:     </span><span class="si">{</span><span class="n">true_val</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>With one million samples, the estimate is accurate to about four decimal places. For extreme quantiles (e.g., <span class="math notranslate nohighlight">\(t = 5\)</span>), the probability is so small that accurate estimation requires importance sampling.</p>
</section>
</section>
<section id="comparison-with-deterministic-methods">
<h2><span class="section-number">1.1.6. </span>Comparison with Deterministic Methods<a class="headerlink" href="#comparison-with-deterministic-methods" title="Link to this heading">ÔÉÅ</a></h2>
<p>Monte Carlo integration is not the only way to compute integrals numerically. Deterministic quadrature methods‚Äîthe trapezoidal rule, Simpson‚Äôs rule, Gaussian quadrature‚Äîhave been studied for centuries and, in low dimensions, often outperform Monte Carlo. Understanding when to use which approach is essential for the computational practitioner.</p>
<section id="one-dimensional-quadrature">
<h3><span class="section-number">1.1.6.1. </span>One-Dimensional Quadrature<a class="headerlink" href="#one-dimensional-quadrature" title="Link to this heading">ÔÉÅ</a></h3>
<p>For a one-dimensional integral <span class="math notranslate nohighlight">\(\int_a^b f(x) dx\)</span>, deterministic methods exploit the smoothness of <span class="math notranslate nohighlight">\(f\)</span> to achieve rapid convergence.</p>
<p><strong>Trapezoidal Rule</strong>: Approximate the integrand by piecewise linear functions. With <span class="math notranslate nohighlight">\(n\)</span> equally spaced points, the error is <span class="math notranslate nohighlight">\(O(n^{-2})\)</span> for twice-differentiable <span class="math notranslate nohighlight">\(f\)</span>.</p>
<p><strong>Simpson‚Äôs Rule</strong>: Approximate by piecewise quadratics. Error is <span class="math notranslate nohighlight">\(O(n^{-4})\)</span> for sufficiently smooth <span class="math notranslate nohighlight">\(f\)</span>.</p>
<p><strong>Gaussian Quadrature</strong>: Choose evaluation points and weights optimally. With <span class="math notranslate nohighlight">\(n\)</span> points, Gaussian quadrature integrates polynomials of degree <span class="math notranslate nohighlight">\(2n-1\)</span> exactly. For analytic functions, convergence can be exponentially fast.</p>
<p>Compare these to Monte Carlo‚Äôs <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span>. In one dimension, Monte Carlo loses badly:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">integrate</span>

<span class="c1"># Integrand: exp(-x¬≤) on [0, 2]</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># True value (via error function)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.special</span><span class="w"> </span><span class="kn">import</span> <span class="n">erf</span>
<span class="n">true_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">erf</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Monte Carlo</span>
<span class="k">def</span><span class="w"> </span><span class="nf">mc_estimate</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># Multiply by interval length</span>

<span class="c1"># Trapezoidal rule</span>
<span class="k">def</span><span class="w"> </span><span class="nf">trap_estimate</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">trapz</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># Simpson&#39;s rule</span>
<span class="k">def</span><span class="w"> </span><span class="nf">simp_estimate</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">integrate</span><span class="o">.</span><span class="n">simpson</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True value: </span><span class="si">{</span><span class="n">true_value</span><span class="si">:</span><span class="s2">.10f</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]:</span>
    <span class="n">mc</span> <span class="o">=</span> <span class="n">mc_estimate</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">tr</span> <span class="o">=</span> <span class="n">trap_estimate</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">si</span> <span class="o">=</span> <span class="n">simp_estimate</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Monte Carlo: </span><span class="si">{</span><span class="n">mc</span><span class="si">:</span><span class="s2">.10f</span><span class="si">}</span><span class="s2">  (error </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">mc</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">true_value</span><span class="p">)</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Trapezoidal: </span><span class="si">{</span><span class="n">tr</span><span class="si">:</span><span class="s2">.10f</span><span class="si">}</span><span class="s2">  (error </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">tr</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">true_value</span><span class="p">)</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Simpson:     </span><span class="si">{</span><span class="n">si</span><span class="si">:</span><span class="s2">.10f</span><span class="si">}</span><span class="s2">  (error </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">si</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">true_value</span><span class="p">)</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
<p>With <span class="math notranslate nohighlight">\(n = 1000\)</span> points, Simpson‚Äôs rule achieves machine precision while Monte Carlo‚Äôs error is around <span class="math notranslate nohighlight">\(10^{-2}\)</span>. There is no contest.</p>
</section>
<section id="the-curse-of-dimensionality">
<h3><span class="section-number">1.1.6.2. </span>The Curse of Dimensionality<a class="headerlink" href="#the-curse-of-dimensionality" title="Link to this heading">ÔÉÅ</a></h3>
<p>The situation reverses dramatically in high dimensions. Consider integrating over <span class="math notranslate nohighlight">\([0, 1]^d\)</span>. A deterministic method using a grid with <span class="math notranslate nohighlight">\(m\)</span> points per dimension requires <span class="math notranslate nohighlight">\(m^d\)</span> total evaluations.</p>
<p>For Simpson‚Äôs rule with error <span class="math notranslate nohighlight">\(O(h^4) = O(m^{-4})\)</span>, the total error is <span class="math notranslate nohighlight">\(O(m^{-4})\)</span> but the cost is <span class="math notranslate nohighlight">\(m^d\)</span>. If we want error <span class="math notranslate nohighlight">\(\epsilon\)</span>, we need <span class="math notranslate nohighlight">\(m \sim \epsilon^{-1/4}\)</span>, giving cost <span class="math notranslate nohighlight">\(\sim \epsilon^{-d/4}\)</span>.</p>
<p>For Monte Carlo with error <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span>, achieving error <span class="math notranslate nohighlight">\(\epsilon\)</span> requires <span class="math notranslate nohighlight">\(n \sim \epsilon^{-2}\)</span> samples, independent of <span class="math notranslate nohighlight">\(d\)</span>.</p>
<p>The crossover occurs when <span class="math notranslate nohighlight">\(\epsilon^{-d/4} = \epsilon^{-2}\)</span>, i.e., <span class="math notranslate nohighlight">\(d = 8\)</span>. For <span class="math notranslate nohighlight">\(d &gt; 8\)</span>, Monte Carlo requires fewer function evaluations than Simpson‚Äôs rule to achieve the same accuracy‚Äîand the advantage grows exponentially with dimension.</p>
<p>This analysis ignores constants, which can favor either method in specific cases. But the fundamental message is robust: <strong>in high dimensions, Monte Carlo wins</strong>.</p>
<table class="docutils align-default" id="id2">
<caption><span class="caption-number">Table 1.15 </span><span class="caption-text">Comparison of Integration Methods</span><a class="headerlink" href="#id2" title="Link to this table">ÔÉÅ</a></caption>
<colgroup>
<col style="width: 25.0%" />
<col style="width: 25.0%" />
<col style="width: 25.0%" />
<col style="width: 25.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>1D Convergence</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(d\)</span>-D Convergence</p></th>
<th class="head"><p>Best For</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Monte Carlo</p></td>
<td><p><span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span></p></td>
<td><p>High dimensions, complex domains</p></td>
</tr>
<tr class="row-odd"><td><p>Trapezoidal</p></td>
<td><p><span class="math notranslate nohighlight">\(O(n^{-2})\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(n^{-2/d})\)</span></p></td>
<td><p>Low-dim smooth functions</p></td>
</tr>
<tr class="row-even"><td><p>Simpson</p></td>
<td><p><span class="math notranslate nohighlight">\(O(n^{-4})\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(n^{-4/d})\)</span></p></td>
<td><p>Low-dim very smooth functions</p></td>
</tr>
<tr class="row-odd"><td><p>Gaussian</p></td>
<td><p><span class="math notranslate nohighlight">\(O(e^{-cn})\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(e^{-cn^{1/d}})\)</span></p></td>
<td><p>Low-dim analytic functions</p></td>
</tr>
<tr class="row-even"><td><p>Quasi-Monte Carlo</p></td>
<td><p><span class="math notranslate nohighlight">\(O(n^{-1}(\log n)^d)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(n^{-1}(\log n)^d)\)</span></p></td>
<td><p>Moderate dimensions, smooth functions</p></td>
</tr>
</tbody>
</table>
</section>
<section id="quasi-monte-carlo-methods">
<h3><span class="section-number">1.1.6.3. </span>Quasi-Monte Carlo Methods<a class="headerlink" href="#quasi-monte-carlo-methods" title="Link to this heading">ÔÉÅ</a></h3>
<p>A middle ground between deterministic quadrature and Monte Carlo is provided by <strong>quasi-Monte Carlo</strong> (QMC) methods. Instead of random samples, QMC uses carefully constructed <strong>low-discrepancy sequences</strong>‚Äîdeterministic sequences that fill space more uniformly than random points.</p>
<p>Famous examples include Halton sequences, Sobol sequences, and lattice rules. Under smoothness conditions on the integrand, QMC achieves convergence rates of <span class="math notranslate nohighlight">\(O(n^{-1} (\log n)^d)\)</span>, faster than Monte Carlo‚Äôs <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> but with a dependence on dimension.</p>
<p>QMC is increasingly popular in computational finance and computer graphics. However, it requires more care: the sequences must match the problem structure, variance estimation is trickier, and the smoothness assumptions may fail. For general-purpose integration, especially with non-smooth or high-variance integrands, standard Monte Carlo remains the most robust choice.</p>
</section>
</section>
<section id="sample-size-determination">
<h2><span class="section-number">1.1.7. </span>Sample Size Determination<a class="headerlink" href="#sample-size-determination" title="Link to this heading">ÔÉÅ</a></h2>
<p>A critical practical question is: <strong>how many samples do I need?</strong> The answer depends on the desired precision, the variance of the integrand, and the acceptable probability of error.</p>
<section id="the-sample-size-formula">
<h3><span class="section-number">1.1.7.1. </span>The Sample Size Formula<a class="headerlink" href="#the-sample-size-formula" title="Link to this heading">ÔÉÅ</a></h3>
<p>From the CLT, the Monte Carlo estimator is approximately:</p>
<div class="math notranslate nohighlight">
\[\hat{I}_n \sim \mathcal{N}\left( I, \frac{\sigma^2}{n} \right)\]</div>
<p>To achieve a standard error of <span class="math notranslate nohighlight">\(\epsilon\)</span>, we need:</p>
<div class="math notranslate nohighlight">
\[\frac{\sigma}{\sqrt{n}} \leq \epsilon \implies n \geq \frac{\sigma^2}{\epsilon^2}\]</div>
<p>For a 95% confidence interval of half-width <span class="math notranslate nohighlight">\(\epsilon\)</span>, we need:</p>
<div class="math notranslate nohighlight">
\[1.96 \cdot \frac{\sigma}{\sqrt{n}} \leq \epsilon \implies n \geq \left( \frac{1.96 \cdot \sigma}{\epsilon} \right)^2 \approx \frac{4\sigma^2}{\epsilon^2}\]</div>
<p>The factor of 4 (approximately <span class="math notranslate nohighlight">\(1.96^2\)</span>) accounts for the confidence level.</p>
</section>
<section id="practical-sample-size-determination">
<h3><span class="section-number">1.1.7.2. </span>Practical Sample Size Determination<a class="headerlink" href="#practical-sample-size-determination" title="Link to this heading">ÔÉÅ</a></h3>
<p>In practice, <span class="math notranslate nohighlight">\(\sigma\)</span> is unknown. A common approach is:</p>
<ol class="arabic simple">
<li><p><strong>Pilot study</strong>: Run a small simulation (e.g., 1,000 samples) to estimate <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span>.</p></li>
<li><p><strong>Compute required :math:`n`</strong>: Use <span class="math notranslate nohighlight">\(n = 4\hat{\sigma}^2 / \epsilon^2\)</span> for 95% confidence.</p></li>
<li><p><strong>Run full simulation</strong>: Generate <span class="math notranslate nohighlight">\(n\)</span> samples (possibly in addition to the pilot).</p></li>
<li><p><strong>Verify</strong>: Check that the final standard error meets requirements.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">determine_sample_size</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">sampler</span><span class="p">,</span> <span class="n">target_se</span><span class="p">,</span> <span class="n">pilot_n</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                           <span class="n">confidence</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determine required sample size for target standard error.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    h : callable</span>
<span class="sd">        Function to integrate.</span>
<span class="sd">    sampler : callable</span>
<span class="sd">        Function returning samples from target distribution.</span>
<span class="sd">    target_se : float</span>
<span class="sd">        Desired standard error.</span>
<span class="sd">    pilot_n : int</span>
<span class="sd">        Pilot sample size for variance estimation.</span>
<span class="sd">    confidence : float</span>
<span class="sd">        Confidence level for interval.</span>
<span class="sd">    seed : int</span>
<span class="sd">        Random seed.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        Contains required_n, estimated_variance, pilot results.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Pilot study</span>
    <span class="n">pilot_samples</span> <span class="o">=</span> <span class="n">sampler</span><span class="p">(</span><span class="n">pilot_n</span><span class="p">)</span>
    <span class="n">pilot_h</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">pilot_samples</span><span class="p">)</span>
    <span class="n">sigma_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">pilot_h</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Required sample size</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">confidence</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">required_n</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">z</span> <span class="o">*</span> <span class="n">sigma_hat</span> <span class="o">/</span> <span class="n">target_se</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;required_n&#39;</span><span class="p">:</span> <span class="n">required_n</span><span class="p">,</span>
        <span class="s1">&#39;estimated_sigma&#39;</span><span class="p">:</span> <span class="n">sigma_hat</span><span class="p">,</span>
        <span class="s1">&#39;pilot_n&#39;</span><span class="p">:</span> <span class="n">pilot_n</span><span class="p">,</span>
        <span class="s1">&#39;pilot_estimate&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">pilot_h</span><span class="p">),</span>
        <span class="s1">&#39;pilot_se&#39;</span><span class="p">:</span> <span class="n">sigma_hat</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">pilot_n</span><span class="p">)</span>
    <span class="p">}</span>

<span class="c1"># Example: Estimate E[X¬≤] for X ~ N(0,1) with SE ‚â§ 0.01</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">determine_sample_size</span><span class="p">(</span>
    <span class="n">h</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">sampler</span><span class="o">=</span><span class="k">lambda</span> <span class="n">n</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n</span><span class="p">),</span>
    <span class="n">target_se</span><span class="o">=</span><span class="mf">0.01</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimated œÉ: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;estimated_sigma&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Required n for SE ‚â§ 0.01: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;required_n&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>For estimating <span class="math notranslate nohighlight">\(\mathbb{E}[X^2]\)</span> where <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(0,1)\)</span>, the variance is <span class="math notranslate nohighlight">\(\text{Var}(X^2) = \mathbb{E}[X^4] - (\mathbb{E}[X^2])^2 = 3 - 1 = 2\)</span>. To achieve standard error 0.01, we need <span class="math notranslate nohighlight">\(n \geq (1.96)^2 \times 2 / (0.01)^2 \approx 77,000\)</span> samples.</p>
</section>
</section>
<section id="convergence-diagnostics-and-monitoring">
<h2><span class="section-number">1.1.8. </span>Convergence Diagnostics and Monitoring<a class="headerlink" href="#convergence-diagnostics-and-monitoring" title="Link to this heading">ÔÉÅ</a></h2>
<p>Beyond computing point estimates and confidence intervals, it is important to monitor the convergence of Monte Carlo simulations. Visual diagnostics can reveal problems‚Äîheavy tails, multimodality, slow mixing‚Äîthat summary statistics might miss.</p>
<section id="running-mean-plots">
<h3><span class="section-number">1.1.8.1. </span>Running Mean Plots<a class="headerlink" href="#running-mean-plots" title="Link to this heading">ÔÉÅ</a></h3>
<p>The most basic diagnostic is a plot of the running mean:</p>
<div class="math notranslate nohighlight">
\[\hat{I}_k = \frac{1}{k} \sum_{i=1}^{k} h(X_i) \quad \text{for } k = 1, 2, \ldots, n\]</div>
<p>If the simulation is converging properly, this plot should:</p>
<ol class="arabic simple">
<li><p>Fluctuate widely for small <span class="math notranslate nohighlight">\(k\)</span></p></li>
<li><p>Stabilize and approach a horizontal asymptote as <span class="math notranslate nohighlight">\(k\)</span> grows</p></li>
<li><p>Have diminishing fluctuations proportional to <span class="math notranslate nohighlight">\(1/\sqrt{k}\)</span></p></li>
</ol>
</section>
<section id="standard-error-decay">
<h3><span class="section-number">1.1.8.2. </span>Standard Error Decay<a class="headerlink" href="#standard-error-decay" title="Link to this heading">ÔÉÅ</a></h3>
<p>The standard error should decrease as <span class="math notranslate nohighlight">\(1/\sqrt{n}\)</span>. A log-log plot of standard error versus sample size should have slope <span class="math notranslate nohighlight">\(-1/2\)</span>. Deviations suggest:</p>
<ul class="simple">
<li><p><strong>Steeper slope</strong>: Variance is decreasing (possibly a problem with the estimator)</p></li>
<li><p><strong>Shallower slope</strong>: Correlation in samples, infinite variance, or other issues</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">convergence_diagnostics</span><span class="p">(</span><span class="n">h_values</span><span class="p">,</span> <span class="n">true_value</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create comprehensive convergence diagnostics.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    h_values : array</span>
<span class="sd">        Sequence of h(X_i) values.</span>
<span class="sd">    true_value : float, optional</span>
<span class="sd">        True integral value (if known).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    fig : matplotlib Figure</span>
<span class="sd">        Diagnostic plots.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">h_values</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Running statistics</span>
    <span class="n">cumsum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">h_values</span><span class="p">)</span>
    <span class="n">running_mean</span> <span class="o">=</span> <span class="n">cumsum</span> <span class="o">/</span> <span class="n">indices</span>

    <span class="c1"># Running variance (using parallel algorithm for stability)</span>
    <span class="n">running_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">running_var</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">M2</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">h_values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">h_values</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-</span> <span class="n">mean</span>
        <span class="n">mean</span> <span class="o">+=</span> <span class="n">delta</span> <span class="o">/</span> <span class="p">(</span><span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">delta2</span> <span class="o">=</span> <span class="n">h_values</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-</span> <span class="n">mean</span>
        <span class="n">M2</span> <span class="o">+=</span> <span class="n">delta</span> <span class="o">*</span> <span class="n">delta2</span>
        <span class="n">running_var</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">M2</span> <span class="o">/</span> <span class="n">k</span> <span class="k">if</span> <span class="n">k</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="n">running_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">running_var</span> <span class="o">/</span> <span class="n">indices</span><span class="p">)</span>

    <span class="c1"># Create figure</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

    <span class="c1"># Plot 1: Running mean</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">running_mean</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">indices</span><span class="p">[</span><span class="mi">100</span><span class="p">:],</span>
                    <span class="n">running_mean</span><span class="p">[</span><span class="mi">100</span><span class="p">:]</span> <span class="o">-</span> <span class="mf">1.96</span><span class="o">*</span><span class="n">running_se</span><span class="p">[</span><span class="mi">100</span><span class="p">:],</span>
                    <span class="n">running_mean</span><span class="p">[</span><span class="mi">100</span><span class="p">:]</span> <span class="o">+</span> <span class="mf">1.96</span><span class="o">*</span><span class="n">running_se</span><span class="p">[</span><span class="mi">100</span><span class="p">:],</span>
                    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">true_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">true_value</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span>
                   <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;True value = </span><span class="si">{</span><span class="n">true_value</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Sample size&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Running mean&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Running Mean with 95% Confidence Band&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>

    <span class="c1"># Plot 2: Standard error decay</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">se_plot_idx</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="mi">99</span><span class="p">:]</span>  <span class="c1"># Start after 100 samples</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">se_plot_idx</span><span class="p">,</span> <span class="n">running_se</span><span class="p">[</span><span class="mi">99</span><span class="p">:],</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
              <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Observed SE&#39;</span><span class="p">)</span>
    <span class="c1"># Reference line with slope -1/2</span>
    <span class="n">ref_se</span> <span class="o">=</span> <span class="n">running_se</span><span class="p">[</span><span class="mi">99</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">100</span> <span class="o">/</span> <span class="n">se_plot_idx</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">se_plot_idx</span><span class="p">,</span> <span class="n">ref_se</span><span class="p">,</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
              <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$O(n^{-1/2})$ reference&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Sample size&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Standard Error&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Standard Error Decay&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="c1"># Plot 3: Histogram of h-values</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">h_values</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_values</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
               <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Mean = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_values</span><span class="p">)</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;h(X)&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Density&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Distribution of h(X) Values&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="c1"># Plot 4: Autocorrelation (to detect dependence)</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">max_lag</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">n</span> <span class="o">//</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">autocorr</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">h_values</span><span class="p">[:</span><span class="o">-</span><span class="n">k</span><span class="p">],</span> <span class="n">h_values</span><span class="p">[</span><span class="n">k</span><span class="p">:])[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_lag</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_lag</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">autocorr</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mf">1.96</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="o">-</span><span class="mf">1.96</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Lag&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Autocorrelation&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Autocorrelation (should be ~0 for iid samples)&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">fig</span>

<span class="c1"># Example usage</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="mi">50000</span><span class="p">)</span>
<span class="n">h_values</span> <span class="o">=</span> <span class="n">X</span><span class="o">**</span><span class="mi">2</span>  <span class="c1"># E[X¬≤] = 1</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">convergence_diagnostics</span><span class="p">(</span><span class="n">h_values</span><span class="p">,</span> <span class="n">true_value</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="pathological-cases">
<h3><span class="section-number">1.1.8.3. </span>Pathological Cases<a class="headerlink" href="#pathological-cases" title="Link to this heading">ÔÉÅ</a></h3>
<p>Several situations can cause Monte Carlo to behave unexpectedly:</p>
<p><strong>Heavy tails</strong>: If <span class="math notranslate nohighlight">\(\text{Var}[h(X)] = \infty\)</span>, the CLT does not apply. The estimator still converges (by LLN, if <span class="math notranslate nohighlight">\(\mathbb{E}[|h(X)|] &lt; \infty\)</span>), but standard error estimates are meaningless. Example: estimating <span class="math notranslate nohighlight">\(\mathbb{E}[1/U]\)</span> where <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0,1)\)</span>.</p>
<p><strong>Multimodality</strong>: If the integrand has isolated peaks that the sampling distribution rarely visits, estimates may be biased until enough samples hit every mode. This is particularly problematic in high dimensions.</p>
<p><strong>Rare events</strong>: Estimating <span class="math notranslate nohighlight">\(P(A)\)</span> for rare events <span class="math notranslate nohighlight">\(A\)</span> requires approximately <span class="math notranslate nohighlight">\(10/P(A)\)</span> samples just to see the event happen a few times. The relative error is enormous unless importance sampling is used.</p>
<p><strong>Dependent samples</strong>: If samples are correlated (as in MCMC), the effective sample size is smaller than the nominal sample size, and standard error formulas underestimate uncertainty.</p>
</section>
</section>
<section id="practical-considerations">
<h2><span class="section-number">1.1.9. </span>Practical Considerations<a class="headerlink" href="#practical-considerations" title="Link to this heading">ÔÉÅ</a></h2>
<p>Before concluding, we collect several practical points for implementing Monte Carlo methods effectively.</p>
<section id="when-to-use-monte-carlo">
<h3><span class="section-number">1.1.9.1. </span>When to Use Monte Carlo<a class="headerlink" href="#when-to-use-monte-carlo" title="Link to this heading">ÔÉÅ</a></h3>
<p>Monte Carlo integration is the method of choice when:</p>
<ol class="arabic simple">
<li><p><strong>Dimension is high</strong> (<span class="math notranslate nohighlight">\(d \gtrsim 5\)</span>): The curse of dimensionality kills deterministic methods.</p></li>
<li><p><strong>Domain is complex</strong>: Irregular regions, constraints, and complex boundaries are natural for Monte Carlo but problematic for quadrature.</p></li>
<li><p><strong>Integrand is non-smooth</strong>: Monte Carlo doesn‚Äôt require derivatives or smoothness.</p></li>
<li><p><strong>Sampling is easy</strong>: If we can easily generate samples from the target distribution, Monte Carlo is straightforward to implement.</p></li>
</ol>
<p>Monte Carlo is a poor choice when:</p>
<ol class="arabic simple">
<li><p><strong>Dimension is very low</strong> (<span class="math notranslate nohighlight">\(d \leq 3\)</span>) and the integrand is smooth: Use Gaussian quadrature.</p></li>
<li><p><strong>High precision is required</strong> with smooth integrands: Deterministic methods converge faster.</p></li>
<li><p><strong>Sampling is expensive</strong>: Each Monte Carlo sample requires a new function evaluation; quadrature methods can achieve more with fewer evaluations for smooth functions.</p></li>
</ol>
</section>
<section id="reproducibility">
<h3><span class="section-number">1.1.9.2. </span>Reproducibility<a class="headerlink" href="#reproducibility" title="Link to this heading">ÔÉÅ</a></h3>
<p>Always set random seeds and document them. Monte Carlo results should be reproducible.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Good practice</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">my_monte_carlo_analysis</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Document the seed in the function signature.&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="c1"># ... analysis ...</span>
    <span class="k">return</span> <span class="n">results</span>

<span class="c1"># Call with explicit seed</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">my_monte_carlo_analysis</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="computational-efficiency">
<h3><span class="section-number">1.1.9.3. </span>Computational Efficiency<a class="headerlink" href="#computational-efficiency" title="Link to this heading">ÔÉÅ</a></h3>
<ul class="simple">
<li><p><strong>Vectorize</strong>: Use NumPy operations on arrays, not Python loops.</p></li>
<li><p><strong>Generate samples in batches</strong>: <code class="docutils literal notranslate"><span class="pre">rng.random(100_000)</span></code> is faster than 100,000 calls to <code class="docutils literal notranslate"><span class="pre">rng.random()</span></code>.</p></li>
<li><p><strong>Parallelize when possible</strong>: For embarrassingly parallel problems, distribute samples across cores.</p></li>
</ul>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ‚ö†Ô∏è</p>
<p><strong>Reporting estimates without uncertainty</strong>: A Monte Carlo estimate without a standard error or confidence interval is nearly meaningless. Always report <span class="math notranslate nohighlight">\(\hat{I}_n \pm z_{\alpha/2} \cdot \hat{\sigma}_n / \sqrt{n}\)</span>.</p>
<p><strong>Bad</strong>: ‚ÄúThe integral is 3.14159.‚Äù</p>
<p><strong>Good</strong>: ‚ÄúThe integral is 3.142 ¬± 0.005 (95% CI: [3.132, 3.152]) based on 100,000 samples.‚Äù</p>
</div>
</section>
</section>
<section id="bringing-it-all-together">
<h2><span class="section-number">1.1.10. </span>Bringing It All Together<a class="headerlink" href="#bringing-it-all-together" title="Link to this heading">ÔÉÅ</a></h2>
<p>Monte Carlo integration transforms the ancient problem of computing integrals into the modern practice of sampling and averaging. The method‚Äôs power derives from three pillars:</p>
<ol class="arabic simple">
<li><p><strong>Universality</strong>: Any integral can be written as an expected value, and expected values can be estimated by sample means.</p></li>
<li><p><strong>The Law of Large Numbers</strong>: Sample means converge to population means, guaranteeing that Monte Carlo estimators approach the true value.</p></li>
<li><p><strong>The Central Limit Theorem</strong>: The error decreases as <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span>, providing both a convergence rate and a framework for uncertainty quantification via confidence intervals.</p></li>
</ol>
<p>The <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> rate is simultaneously the method‚Äôs weakness and its strength. In low dimensions, deterministic quadrature methods converge faster. But in high dimensions, where deterministic methods suffer the curse of dimensionality, Monte Carlo‚Äôs dimension-independent rate makes it the only viable option.</p>
<p>The examples in this section‚Äîestimating <span class="math notranslate nohighlight">\(\pi\)</span>, computing Gaussian integrals, evaluating posterior means, tackling high-dimensional problems‚Äîillustrate the breadth of Monte Carlo applications. The diagnostic tools‚Äîrunning mean plots, standard error decay, autocorrelation checks‚Äîequip you to assess whether your simulations are converging properly.</p>
</section>
<section id="transition-to-what-follows">
<h2><span class="section-number">1.1.11. </span>Transition to What Follows<a class="headerlink" href="#transition-to-what-follows" title="Link to this heading">ÔÉÅ</a></h2>
<p>With the foundations of Monte Carlo integration in place, we face a critical question: <em>where do the random samples come from?</em></p>
<p>Throughout this section, we have assumed that generating samples from the target distribution‚Äîuniform on <span class="math notranslate nohighlight">\([0, 1]^d\)</span>, standard normal, a posterior distribution‚Äîis straightforward. But this assumption hides a mountain of computational machinery.</p>
<p>The <strong>next section</strong> (<a class="reference internal" href="ch2.2-uniform-random-variates.html#ch2-2-uniform-random-variates"><span class="std std-ref">Uniform Random Variates</span></a>) addresses the most fundamental case: generating uniform random numbers. We will see that computers, being deterministic machines, cannot produce ‚Äútrue‚Äù randomness‚Äîonly pseudo-random sequences that pass stringent statistical tests. Understanding how these sequences are generated, what can go wrong, and how to ensure reproducibility is essential for any serious practitioner.</p>
<p>Following that, the <strong>inverse CDF method</strong> (<a class="reference internal" href="ch2.3-inverse-cdf-method.html#ch2-3-inverse-cdf-method"><span class="std std-ref">Inverse CDF Method</span></a>) shows how to transform uniform random numbers into samples from other distributions. If we can compute the inverse cumulative distribution function <span class="math notranslate nohighlight">\(F^{-1}(u)\)</span>, we can generate samples from <span class="math notranslate nohighlight">\(F\)</span> by applying <span class="math notranslate nohighlight">\(F^{-1}\)</span> to uniform random numbers. This elegant technique works for many important distributions‚Äîexponential, Weibull, Cauchy‚Äîbut fails when <span class="math notranslate nohighlight">\(F^{-1}\)</span> has no closed form.</p>
<p>For distributions like the normal, where the inverse CDF is not available analytically, <strong>specialized transformations</strong> like Box-Muller offer efficient alternatives. And for truly complex distributions‚Äîposteriors in Bayesian inference, for instance‚Äî<strong>rejection sampling</strong> and eventually <strong>Markov chain Monte Carlo</strong> (Part 3) provide the tools to generate the samples that Monte Carlo integration requires.</p>
<p>The story of Monte Carlo methods is thus a story of two interlocking challenges: using random samples to estimate integrals (this section), and generating the random samples in the first place (the sections to come). Master both, and you hold a computational toolkit of extraordinary power.</p>
<div class="important admonition">
<p class="admonition-title">Key Takeaways üìù</p>
<ol class="arabic simple">
<li><p><strong>Core concept</strong>: Monte Carlo integration estimates integrals by rewriting them as expectations and averaging random samples. The Law of Large Numbers guarantees convergence; the Central Limit Theorem provides the <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> rate.</p></li>
<li><p><strong>Historical insight</strong>: Monte Carlo emerged from the Manhattan Project, where Ulam and von Neumann needed to compute neutron transport integrals that resisted analytical attack. The method turned randomness from a nuisance into a computational tool.</p></li>
<li><p><strong>Dimension independence</strong>: The <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> rate does not depend on the dimension of the integration domain. This is why Monte Carlo dominates in high-dimensional problems where deterministic methods fail.</p></li>
<li><p><strong>Practical application</strong>: Always report standard errors and confidence intervals. Use pilot studies to estimate variance for sample size planning. Monitor convergence visually with running mean plots and standard error decay.</p></li>
<li><p><strong>Method selection</strong>: In 1‚Äì3 dimensions with smooth integrands, use deterministic quadrature. In high dimensions or with complex domains, use Monte Carlo. Consider quasi-Monte Carlo for moderate dimensions with smooth functions.</p></li>
<li><p><strong>Outcome alignment</strong>: This section directly addresses Learning Outcome 1 (apply simulation techniques for Monte Carlo integration) and provides the conceptual foundation for all subsequent simulation methods, including variance reduction, importance sampling, and MCMC.</p></li>
</ol>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="1. Chapter 2: Monte Carlo Simulation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ch2.2-uniform-random-variates.html" class="btn btn-neutral float-right" title="1.2. Uniform Random Variates" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>