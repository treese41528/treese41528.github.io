

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>2.3.1. The Sampling Distribution Problem &mdash; STAT 418</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=dc393e06" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT418/Website/part2_simulation/chapter4/ch4_1-sampling-distribution-problem.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=30646c52"></script>
      <script>let toggleHintShow = 'Show solution';</script>
      <script>let toggleHintHide = 'Hide solution';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>window.MathJax = {"options": {"enableMenu": true, "menuOptions": {"settings": {"enrich": true, "speech": true, "braille": true, "collapsible": true, "assistiveMml": false}}}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
      <script src="../../_static/custom.js?v=8718e0ab"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="3. Part III: Bayesian Methods" href="../../part3_bayesian/index.html" />
    <link rel="prev" title="2.3. Chapter 4: Resampling Methods" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Course Content</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../part1_foundations/index.html">1. Part I: Foundations of Probability and Computation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part1_foundations/chapter1/index.html">1.1. Chapter 1: Statistical Paradigms and Core Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html">1.1.1. Paradigms of Probability and Statistical Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#the-mathematical-foundation-kolmogorov-s-axioms">The Mathematical Foundation: Kolmogorov’s Axioms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#interpretations-of-probability">Interpretations of Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#statistical-inference-paradigms">Statistical Inference Paradigms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#historical-and-philosophical-debates">Historical and Philosophical Debates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#looking-ahead-our-course-focus">Looking Ahead: Our Course Focus</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html">1.1.2. Probability Distributions: Theory and Computation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#from-abstract-foundations-to-concrete-tools">From Abstract Foundations to Concrete Tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#the-python-ecosystem-for-probability">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#introduction-why-probability-distributions-matter">Introduction: Why Probability Distributions Matter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#id1">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#continuous-distributions">Continuous Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#additional-important-distributions">Additional Important Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#summary-and-practical-guidelines">Summary and Practical Guidelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#conclusion">Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html">1.1.3. Python Random Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#from-mathematical-distributions-to-computational-samples">From Mathematical Distributions to Computational Samples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-python-ecosystem-at-a-glance">The Python Ecosystem at a Glance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#understanding-pseudo-random-number-generation">Understanding Pseudo-Random Number Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-standard-library-random-module">The Standard Library: <code class="docutils literal notranslate"><span class="pre">random</span></code> Module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#numpy-fast-vectorized-random-sampling">NumPy: Fast Vectorized Random Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#scipy-stats-the-complete-statistical-toolkit">SciPy Stats: The Complete Statistical Toolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#bringing-it-all-together-library-selection-guide">Bringing It All Together: Library Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#looking-ahead-from-random-numbers-to-monte-carlo-methods">Looking Ahead: From Random Numbers to Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html">1.1.4. Chapter 1 Summary: Foundations in Place</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#the-three-pillars-of-chapter-1">The Three Pillars of Chapter 1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#what-lies-ahead-the-road-to-simulation">What Lies Ahead: The Road to Simulation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#chapter-1-exercises-synthesis-problems">Chapter 1 Exercises: Synthesis Problems</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">2. Part II: Simulation-Based Methods</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../chapter2/index.html">2.1. Chapter 2: Monte Carlo Simulation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html">2.1.1. Monte Carlo Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#the-historical-development-of-monte-carlo-methods">The Historical Development of Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#the-core-principle-expectation-as-integration">The Core Principle: Expectation as Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#theoretical-foundations">Theoretical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#variance-estimation-and-confidence-intervals">Variance Estimation and Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#comparison-with-deterministic-methods">Comparison with Deterministic Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#sample-size-determination">Sample Size Determination</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#convergence-diagnostics-and-monitoring">Convergence Diagnostics and Monitoring</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#chapter-2-1-exercises-monte-carlo-fundamentals-mastery">Chapter 2.1 Exercises: Monte Carlo Fundamentals Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html">2.1.2. Uniform Random Variates</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#why-uniform-the-universal-currency-of-randomness">Why Uniform? The Universal Currency of Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#the-paradox-of-computational-randomness">The Paradox of Computational Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#chaotic-dynamical-systems-an-instructive-failure">Chaotic Dynamical Systems: An Instructive Failure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#linear-congruential-generators">Linear Congruential Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#shift-register-generators">Shift-Register Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#the-kiss-generator-combining-strategies">The KISS Generator: Combining Strategies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#modern-generators-mersenne-twister-and-pcg">Modern Generators: Mersenne Twister and PCG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#statistical-testing-of-random-number-generators">Statistical Testing of Random Number Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#chapter-2-2-exercises-uniform-random-variates-mastery">Chapter 2.2 Exercises: Uniform Random Variates Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html">2.1.3. Inverse CDF Method</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#mathematical-foundations">Mathematical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#continuous-distributions-with-closed-form-inverses">Continuous Distributions with Closed-Form Inverses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#numerical-inversion">Numerical Inversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#mixed-distributions">Mixed Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#chapter-2-3-exercises-inverse-cdf-method-mastery">Chapter 2.3 Exercises: Inverse CDF Method Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html">2.1.4. Transformation Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#why-transformation-methods">Why Transformation Methods?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-boxmuller-transform">The Box–Muller Transform</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-polar-marsaglia-method">The Polar (Marsaglia) Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#method-comparison-boxmuller-vs-polar-vs-ziggurat">Method Comparison: Box–Muller vs Polar vs Ziggurat</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-ziggurat-algorithm">The Ziggurat Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-clt-approximation-historical">The CLT Approximation (Historical)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#distributions-derived-from-the-normal">Distributions Derived from the Normal</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#multivariate-normal-generation">Multivariate Normal Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#implementation-guidance">Implementation Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#chapter-2-4-exercises-transformation-methods-mastery">Chapter 2.4 Exercises: Transformation Methods Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html">2.1.5. Rejection Sampling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-dartboard-intuition">The Dartboard Intuition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-accept-reject-algorithm">The Accept-Reject Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#efficiency-analysis">Efficiency Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#choosing-the-proposal-distribution">Choosing the Proposal Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-squeeze-principle">The Squeeze Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#geometric-example-sampling-from-the-unit-disk">Geometric Example: Sampling from the Unit Disk</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#limitations-and-the-curse-of-dimensionality">Limitations and the Curse of Dimensionality</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#connections-to-other-methods">Connections to Other Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#chapter-2-5-exercises-rejection-sampling-mastery">Chapter 2.5 Exercises: Rejection Sampling Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html">2.1.6. Variance Reduction Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#the-variance-reduction-paradigm">The Variance Reduction Paradigm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#importance-sampling">Importance Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#control-variates">Control Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#antithetic-variates">Antithetic Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#stratified-sampling">Stratified Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#common-random-numbers">Common Random Numbers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#conditional-monte-carlo-raoblackwellization">Conditional Monte Carlo (Rao–Blackwellization)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#combining-variance-reduction-techniques">Combining Variance Reduction Techniques</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#chapter-2-6-exercises-variance-reduction-mastery">Chapter 2.6 Exercises: Variance Reduction Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html">2.1.7. Chapter Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#the-complete-monte-carlo-workflow">The Complete Monte Carlo Workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#quick-reference-tables">Quick Reference Tables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#common-pitfalls-checklist">Common Pitfalls Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#connections-to-later-chapters">Connections to Later Chapters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#learning-outcomes-checklist">Learning Outcomes Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#further-reading-optimization-and-missing-data">Further Reading: Optimization and Missing Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter3/index.html">2.2. Chapter 3: Parametric Inference and Likelihood Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html">2.2.1. Exponential Families</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#historical-origins-from-scattered-results-to-unified-theory">Historical Origins: From Scattered Results to Unified Theory</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#the-canonical-exponential-family">The Canonical Exponential Family</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#converting-familiar-distributions">Converting Familiar Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#the-log-partition-function-a-moment-generating-machine">The Log-Partition Function: A Moment-Generating Machine</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#sufficiency-capturing-all-parameter-information">Sufficiency: Capturing All Parameter Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#minimal-sufficiency-and-completeness">Minimal Sufficiency and Completeness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#conjugate-priors-and-bayesian-inference">Conjugate Priors and Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#exponential-dispersion-models-and-glms">Exponential Dispersion Models and GLMs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#chapter-3-1-exercises-exponential-families-mastery">Chapter 3.1 Exercises: Exponential Families Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html">2.2.2. Maximum Likelihood Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-likelihood-function">The Likelihood Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-score-function">The Score Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#fisher-information">Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#closed-form-maximum-likelihood-estimators">Closed-Form Maximum Likelihood Estimators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#numerical-optimization-for-mle">Numerical Optimization for MLE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#asymptotic-properties-of-mles">Asymptotic Properties of MLEs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-cramer-rao-lower-bound">The Cramér-Rao Lower Bound</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-invariance-property">The Invariance Property</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#likelihood-based-hypothesis-testing">Likelihood-Based Hypothesis Testing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#confidence-intervals-from-likelihood">Confidence Intervals from Likelihood</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#connection-to-bayesian-inference">Connection to Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#chapter-3-2-exercises-maximum-likelihood-estimation-mastery">Chapter 3.2 Exercises: Maximum Likelihood Estimation Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html">2.2.3. Sampling Variability and Variance Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#statistical-estimators-and-their-properties">Statistical Estimators and Their Properties</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#sampling-distributions">Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#the-delta-method">The Delta Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#variance-estimation-methods">Variance Estimation Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#applications-and-worked-examples">Applications and Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html">2.2.4. Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#matrix-calculus-foundations">Matrix Calculus Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#the-linear-model">The Linear Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#ordinary-least-squares-the-calculus-approach">Ordinary Least Squares: The Calculus Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#ordinary-least-squares-the-geometric-approach">Ordinary Least Squares: The Geometric Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#properties-of-the-ols-estimator">Properties of the OLS Estimator</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#the-gauss-markov-theorem">The Gauss-Markov Theorem</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#estimating-the-error-variance">Estimating the Error Variance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#distributional-results-under-normality">Distributional Results Under Normality</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#diagnostics-and-model-checking">Diagnostics and Model Checking</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#numerical-stability-qr-decomposition">Numerical Stability: QR Decomposition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#model-selection-and-information-criteria">Model Selection and Information Criteria</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#regularization-ridge-and-lasso">Regularization: Ridge and LASSO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#chapter-3-4-exercises-linear-models-mastery">Chapter 3.4 Exercises: Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html">2.2.5. Generalized Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#historical-context-unification-of-regression-methods">Historical Context: Unification of Regression Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#the-glm-framework-three-components">The GLM Framework: Three Components</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#score-equations-and-fisher-information">Score Equations and Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#iteratively-reweighted-least-squares">Iteratively Reweighted Least Squares</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#logistic-regression-binary-outcomes">Logistic Regression: Binary Outcomes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#poisson-regression-count-data">Poisson Regression: Count Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#gamma-regression-positive-continuous-data">Gamma Regression: Positive Continuous Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#inference-in-glms-the-testing-triad">Inference in GLMs: The Testing Triad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#model-diagnostics">Model Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#model-comparison-and-selection">Model Comparison and Selection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#quasi-likelihood-and-robust-inference">Quasi-Likelihood and Robust Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#further-reading">Further Reading</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#chapter-3-5-exercises-generalized-linear-models-mastery">Chapter 3.5 Exercises: Generalized Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html">2.2.6. Chapter Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#the-parametric-inference-pipeline">The Parametric Inference Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#the-five-pillars-of-chapter-3">The Five Pillars of Chapter 3</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#quick-reference-core-formulas">Quick Reference: Core Formulas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#connections-to-future-material">Connections to Future Material</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#practical-guidance">Practical Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">2.3. Chapter 4: Resampling Methods</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">2.3.1. The Sampling Distribution Problem</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#the-fundamental-target-sampling-distributions">The Fundamental Target: Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#historical-development-the-quest-for-sampling-distributions">Historical Development: The Quest for Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#three-routes-to-the-sampling-distribution">Three Routes to the Sampling Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="#when-asymptotics-fail-motivating-the-bootstrap">When Asymptotics Fail: Motivating the Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-plug-in-principle-theoretical-foundation">The Plug-In Principle: Theoretical Foundation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#computational-perspective-bootstrap-as-monte-carlo">Computational Perspective: Bootstrap as Monte Carlo</a></li>
<li class="toctree-l4"><a class="reference internal" href="#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="#chapter-4-1-exercises">Chapter 4.1 Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part3_bayesian/index.html">3. Part III: Bayesian Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part3_bayesian/index.html#overview">3.1. Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html">3.1.1. Bayesian Philosophy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html"><span class="section-number">2. </span>Part II: Simulation-Based Methods</a></li>
          <li class="breadcrumb-item"><a href="index.html"><span class="section-number">2.3. </span>Chapter 4: Resampling Methods</a></li>
      <li class="breadcrumb-item active"><span class="section-number">2.3.1. </span>The Sampling Distribution Problem</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/part2_simulation/chapter4/ch4_1-sampling-distribution-problem.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="the-sampling-distribution-problem">
<span id="ch4-1-sampling-distribution-problem"></span><h1><span class="section-number">2.3.1. </span>The Sampling Distribution Problem<a class="headerlink" href="#the-sampling-distribution-problem" title="Link to this heading"></a></h1>
<p>In <a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#ch3-1-exponential-families"><span class="std std-ref">Chapter 3</span></a>, we developed the machinery of parametric inference: exponential families provide a unified structure for probability models, maximum likelihood yields point estimates with elegant theoretical properties, and Fisher information quantifies the precision of those estimates. Given data <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span> from a known parametric family <span class="math notranslate nohighlight">\(\{F_\theta : \theta \in \Theta\}\)</span>, we can compute the MLE <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>, derive its asymptotic distribution via the Central Limit Theorem, and construct confidence intervals using the inverse Fisher information. This framework is powerful, elegant, and—under appropriate regularity conditions—optimal.</p>
<p>But what happens when we cannot trust the parametric model? What if the sample size is too small for asymptotics to be reliable? What if the statistic of interest is a median or correlation coefficient whose sampling distribution has no closed form? What if we want inference for a complex quantity like the ratio of two means, where the delta method approximation may be poor? In these situations—which arise constantly in practice—we confront a fundamental challenge: <strong>we need to know the sampling distribution of our estimator, but probability theory alone cannot tell us what it is</strong>.</p>
<p>This challenge motivates the entire apparatus of resampling methods. The key insight, due to Bradley Efron in 1979, is deceptively simple: if we cannot derive the sampling distribution analytically or trust asymptotic approximations, we can <em>estimate</em> it by treating the observed sample as a proxy for the population and resampling from it repeatedly. This chapter develops this idea systematically, but first we must be precise about what we are trying to approximate and why it matters so fundamentally to statistical inference.</p>
<p>The present section establishes the conceptual and mathematical foundations for everything that follows. We begin by defining the sampling distribution with full mathematical rigor, then examine the historical development of methods for approximating it. We compare three canonical routes—analytic derivation, parametric Monte Carlo, and the bootstrap—understanding when each is appropriate. We identify specific scenarios where classical asymptotic methods fail, motivating the need for resampling. Finally, we introduce the plug-in principle as the theoretical foundation for bootstrap inference, preparing for the rigorous development in <span class="xref std std-ref">Section 4.2</span>.</p>
<div class="important admonition">
<p class="admonition-title">Road Map 🧭</p>
<ul class="simple">
<li><p><strong>Define</strong>: The sampling distribution <span class="math notranslate nohighlight">\(G\)</span> of a statistic and prove it is the fundamental target of uncertainty quantification</p></li>
<li><p><strong>Derive</strong>: Explicit formulas connecting bias, variance, MSE, confidence intervals, and p-values to <span class="math notranslate nohighlight">\(G\)</span></p></li>
<li><p><strong>Distinguish</strong>: Three routes to approximating <span class="math notranslate nohighlight">\(G\)</span>—analytic, parametric Monte Carlo, and bootstrap—with formal analysis of each</p></li>
<li><p><strong>Analyze</strong>: When classical asymptotics are adequate versus when they fail, with explicit examples and theoretical justification</p></li>
<li><p><strong>Establish</strong>: The plug-in principle as the conceptual and mathematical foundation for bootstrap methods</p></li>
</ul>
</div>
<section id="the-fundamental-target-sampling-distributions">
<h2>The Fundamental Target: Sampling Distributions<a class="headerlink" href="#the-fundamental-target-sampling-distributions" title="Link to this heading"></a></h2>
<p>Every method for quantifying uncertainty—standard errors, confidence intervals, hypothesis tests, prediction intervals—derives from a single mathematical object: the <strong>sampling distribution</strong> of the statistic. Before developing computational methods to approximate it, we must understand precisely what this object is, why it matters, and how all uncertainty quantification flows from it.</p>
<section id="formal-definition-and-setup">
<h3>Formal Definition and Setup<a class="headerlink" href="#formal-definition-and-setup" title="Link to this heading"></a></h3>
<p>Consider the fundamental statistical setup. We observe data <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> drawn independently and identically distributed (iid) from some unknown population distribution <span class="math notranslate nohighlight">\(F\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-iid-setup">
<span class="eqno">(2.133)<a class="headerlink" href="#equation-iid-setup" title="Link to this equation"></a></span>\[X_1, X_2, \ldots, X_n \stackrel{\text{iid}}{\sim} F\]</div>
<p>where <span class="math notranslate nohighlight">\(F: \mathbb{R} \to [0, 1]\)</span> is the cumulative distribution function (CDF) of the population. We compute a statistic—a function of the data:</p>
<div class="math notranslate nohighlight" id="equation-statistic-def">
<span class="eqno">(2.134)<a class="headerlink" href="#equation-statistic-def" title="Link to this equation"></a></span>\[\hat{\theta} = T(X_1, X_2, \ldots, X_n)\]</div>
<p>where <span class="math notranslate nohighlight">\(T: \mathbb{R}^n \to \mathbb{R}\)</span> is a known, measurable function. The statistic <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is intended to estimate some population quantity <span class="math notranslate nohighlight">\(\theta = \tau(F)\)</span>—a <strong>functional</strong> of the distribution <span class="math notranslate nohighlight">\(F\)</span>. Examples include:</p>
<ul class="simple">
<li><p>The population mean: <span class="math notranslate nohighlight">\(\tau(F) = \int x \, dF(x) = \mathbb{E}_F[X]\)</span></p></li>
<li><p>The population variance: <span class="math notranslate nohighlight">\(\tau(F) = \int (x - \mu)^2 \, dF(x) = \text{Var}_F(X)\)</span></p></li>
<li><p>The population median: <span class="math notranslate nohighlight">\(\tau(F) = F^{-1}(0.5)\)</span></p></li>
<li><p>The population correlation: <span class="math notranslate nohighlight">\(\tau(F) = \text{Corr}_F(X, Y)\)</span></p></li>
</ul>
<p>Here is the crucial observation that underlies all of statistical inference:</p>
<div class="note admonition">
<p class="admonition-title">Fundamental Observation</p>
<p><strong>The statistic</strong> <span class="math notranslate nohighlight">\(\hat{\theta} = T(X_1, \ldots, X_n)\)</span> <strong>is a random variable.</strong></p>
<p>Its value depends on which particular sample <span class="math notranslate nohighlight">\((X_1, \ldots, X_n)\)</span> we happen to observe. Different samples from the same population <span class="math notranslate nohighlight">\(F\)</span> yield different values of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>. The probability distribution governing this variability is the <strong>sampling distribution</strong>.</p>
</div>
<div class="note admonition">
<p class="admonition-title">Definition: Sampling Distribution</p>
<p>Let <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} F\)</span> and let <span class="math notranslate nohighlight">\(\hat{\theta} = T(X_1, \ldots, X_n)\)</span> be a statistic. The <strong>sampling distribution</strong> of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is the probability distribution of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> induced by the randomness in <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span>.</p>
<p>Formally, its cumulative distribution function is:</p>
<div class="math notranslate nohighlight" id="equation-sampling-cdf">
<span class="eqno">(2.135)<a class="headerlink" href="#equation-sampling-cdf" title="Link to this equation"></a></span>\[G(t) = G_F(t) = P_F\{\hat{\theta} \leq t\} = P_F\{T(X_1, \ldots, X_n) \leq t\}\]</div>
<p>where the subscript <span class="math notranslate nohighlight">\(F\)</span> emphasizes that the probability is computed under the assumption that <span class="math notranslate nohighlight">\(X_i \sim F\)</span>.</p>
</div>
<p>The sampling distribution <span class="math notranslate nohighlight">\(G\)</span> is a <strong>theoretical construct</strong>: it describes what would happen if we could repeat the entire data-collection process infinitely many times, each time drawing a fresh sample of size <span class="math notranslate nohighlight">\(n\)</span> from <span class="math notranslate nohighlight">\(F\)</span> and computing <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>. In reality, we observe only <strong>one sample</strong>—a single realization <span class="math notranslate nohighlight">\((x_1, \ldots, x_n)\)</span> and a single value <span class="math notranslate nohighlight">\(\hat{\theta} = T(x_1, \ldots, x_n)\)</span>.</p>
<p>The fundamental challenge of statistical inference is to learn about the distribution <span class="math notranslate nohighlight">\(G\)</span> from this single realization. Everything we want to know about the uncertainty of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>—its standard error, whether it is biased, how confident we should be in conclusions drawn from it—is encoded in <span class="math notranslate nohighlight">\(G\)</span>.</p>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_1_fig01_sampling_distribution_concept.png"><img alt="Conceptual diagram showing population F generating many samples, each yielding an estimate, with the collection of estimates forming the sampling distribution G" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_1_fig01_sampling_distribution_concept.png" style="width: 90%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.94 </span><span class="caption-text"><strong>Figure 4.1.1</strong>: The Sampling Distribution Concept. The unknown population <span class="math notranslate nohighlight">\(F\)</span> generates infinitely many possible samples of size <span class="math notranslate nohighlight">\(n\)</span>. Each sample yields an estimate <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>. The distribution of these estimates across all possible samples is the sampling distribution <span class="math notranslate nohighlight">\(G\)</span>. In practice, we observe only one sample and must estimate <span class="math notranslate nohighlight">\(G\)</span> from it.</span><a class="headerlink" href="#id1" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="everything-flows-from-g-a-formal-development">
<h3>Everything Flows from G: A Formal Development<a class="headerlink" href="#everything-flows-from-g-a-formal-development" title="Link to this heading"></a></h3>
<p>Every aspect of uncertainty quantification is a <strong>functional of the sampling distribution</strong> <span class="math notranslate nohighlight">\(G\)</span>. This section develops these connections rigorously, showing that <span class="math notranslate nohighlight">\(G\)</span> is indeed the fundamental target.</p>
<p><strong>Bias</strong></p>
<p>Bias measures systematic error—the tendency of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> to overestimate or underestimate <span class="math notranslate nohighlight">\(\theta\)</span> on average across repeated samples.</p>
<div class="note admonition">
<p class="admonition-title">Definition: Bias</p>
<p>The <strong>bias</strong> of an estimator <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> for parameter <span class="math notranslate nohighlight">\(\theta\)</span> is:</p>
<div class="math notranslate nohighlight" id="equation-bias-def">
<span class="eqno">(2.136)<a class="headerlink" href="#equation-bias-def" title="Link to this equation"></a></span>\[\text{Bias}_F(\hat{\theta}) = \mathbb{E}_F[\hat{\theta}] - \theta = \int_{-\infty}^{\infty} t \, dG(t) - \theta\]</div>
<p>An estimator is <strong>unbiased</strong> if <span class="math notranslate nohighlight">\(\text{Bias}_F(\hat{\theta}) = 0\)</span> for all <span class="math notranslate nohighlight">\(F\)</span> in the model.</p>
</div>
<p>The expectation <span class="math notranslate nohighlight">\(\mathbb{E}_F[\hat{\theta}]\)</span> is the mean of the sampling distribution <span class="math notranslate nohighlight">\(G\)</span>. If <span class="math notranslate nohighlight">\(G\)</span> has density <span class="math notranslate nohighlight">\(g\)</span>, then:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}_F[\hat{\theta}] = \int_{-\infty}^{\infty} t \, g(t) \, dt\]</div>
<p>Bias tells us whether our estimator is “aimed correctly.” An unbiased estimator hits the true parameter on average, though any individual estimate may be above or below <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p><strong>Variance and Standard Error</strong></p>
<p>Variance quantifies the spread of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> around its mean across repeated samples—how much <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> fluctuates from sample to sample.</p>
<div class="note admonition">
<p class="admonition-title">Definition: Variance and Standard Error</p>
<p>The <strong>variance</strong> of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is:</p>
<div class="math notranslate nohighlight" id="equation-var-def">
<span class="eqno">(2.137)<a class="headerlink" href="#equation-var-def" title="Link to this equation"></a></span>\[\text{Var}_F(\hat{\theta}) = \mathbb{E}_F\left[(\hat{\theta} - \mathbb{E}_F[\hat{\theta}])^2\right] = \int_{-\infty}^{\infty} (t - \mathbb{E}_F[\hat{\theta}])^2 \, dG(t)\]</div>
<p>The <strong>standard error</strong> is <span class="math notranslate nohighlight">\(\text{SE}(\hat{\theta}) = \sqrt{\text{Var}_F(\hat{\theta})}\)</span>.</p>
</div>
<p>Standard error is the standard deviation of the sampling distribution. It measures the typical magnitude of deviation of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> from its expected value.</p>
<p><strong>Mean Squared Error</strong></p>
<p>Mean squared error (MSE) combines bias and variance into a single measure of overall estimator quality.</p>
<div class="note admonition">
<p class="admonition-title">Definition: Mean Squared Error</p>
<p>The <strong>mean squared error</strong> of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> for estimating <span class="math notranslate nohighlight">\(\theta\)</span> is:</p>
<div class="math notranslate nohighlight" id="equation-mse-def">
<span class="eqno">(2.138)<a class="headerlink" href="#equation-mse-def" title="Link to this equation"></a></span>\[\text{MSE}_F(\hat{\theta}) = \mathbb{E}_F\left[(\hat{\theta} - \theta)^2\right]\]</div>
</div>
<div class="note admonition">
<p class="admonition-title">Theorem: Bias-Variance Decomposition</p>
<div class="math notranslate nohighlight" id="equation-bias-variance-decomp">
<span class="eqno">(2.139)<a class="headerlink" href="#equation-bias-variance-decomp" title="Link to this equation"></a></span>\[\text{MSE}_F(\hat{\theta}) = \text{Bias}_F(\hat{\theta})^2 + \text{Var}_F(\hat{\theta})\]</div>
<p><strong>Proof</strong>: Let <span class="math notranslate nohighlight">\(\mu = \mathbb{E}_F[\hat{\theta}]\)</span> and <span class="math notranslate nohighlight">\(b = \mu - \theta\)</span> (the bias). Then:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{MSE} &amp;= \mathbb{E}_F[(\hat{\theta} - \theta)^2] \\
&amp;= \mathbb{E}_F[(\hat{\theta} - \mu + \mu - \theta)^2] \\
&amp;= \mathbb{E}_F[(\hat{\theta} - \mu)^2] + 2(\mu - \theta)\mathbb{E}_F[\hat{\theta} - \mu] + (\mu - \theta)^2 \\
&amp;= \text{Var}_F(\hat{\theta}) + 0 + b^2 \\
&amp;= \text{Var}_F(\hat{\theta}) + \text{Bias}_F(\hat{\theta})^2\end{split}\]</div>
<p>where we used <span class="math notranslate nohighlight">\(\mathbb{E}_F[\hat{\theta} - \mu] = 0\)</span>. ∎</p>
</div>
<p>This decomposition reveals a fundamental tradeoff:</p>
<ul class="simple">
<li><p><strong>Low bias, high variance</strong>: The estimator is centered correctly but imprecise</p></li>
<li><p><strong>High bias, low variance</strong>: The estimator is precise but systematically wrong</p></li>
<li><p><strong>Optimal</strong>: Balance bias and variance to minimize MSE</p></li>
</ul>
<p><strong>Confidence Intervals</strong></p>
<p>A confidence interval is a random interval <span class="math notranslate nohighlight">\([L, U]\)</span> constructed from the data such that:</p>
<div class="math notranslate nohighlight" id="equation-ci-coverage-def">
<span class="eqno">(2.140)<a class="headerlink" href="#equation-ci-coverage-def" title="Link to this equation"></a></span>\[P_F\{L \leq \theta \leq U\} \geq 1 - \alpha\]</div>
<p>The probability here is over the sampling distribution. Different samples yield different intervals <span class="math notranslate nohighlight">\([L, U]\)</span>—some contain <span class="math notranslate nohighlight">\(\theta\)</span>, some do not. The coverage probability is the proportion of intervals that contain <span class="math notranslate nohighlight">\(\theta\)</span> across all possible samples.</p>
<div class="note admonition">
<p class="admonition-title">Key Insight: CI Coverage is About G</p>
<p>The coverage probability <span class="math notranslate nohighlight">\(P_F\{L \leq \theta \leq U\}\)</span> is computed with respect to the sampling distribution of <span class="math notranslate nohighlight">\((L, U)\)</span>. Since <span class="math notranslate nohighlight">\(L = L(X_1, \ldots, X_n)\)</span> and <span class="math notranslate nohighlight">\(U = U(X_1, \ldots, X_n)\)</span> are functions of the data, they have a joint sampling distribution. The coverage probability is a functional of this distribution.</p>
<p>To construct valid confidence intervals, we need to know or approximate the sampling distribution of our pivotal quantity.</p>
</div>
<p><strong>Hypothesis Test p-values</strong></p>
<p>A p-value is the probability, under the null hypothesis, of observing a test statistic as extreme as or more extreme than what was observed:</p>
<div class="math notranslate nohighlight" id="equation-pvalue-def">
<span class="eqno">(2.141)<a class="headerlink" href="#equation-pvalue-def" title="Link to this equation"></a></span>\[p = P_{F_0}\{T(X_1, \ldots, X_n) \geq T_{\text{obs}}\}\]</div>
<p>where <span class="math notranslate nohighlight">\(F_0\)</span> is the null distribution and <span class="math notranslate nohighlight">\(T_{\text{obs}}\)</span> is the observed test statistic.</p>
<p>This probability is computed with respect to the sampling distribution of <span class="math notranslate nohighlight">\(T\)</span> under <span class="math notranslate nohighlight">\(F_0\)</span>. To compute p-values, we need to know <span class="math notranslate nohighlight">\(G_{F_0}\)</span>.</p>
</section>
<section id="the-sampling-distribution-in-closed-form-rare-but-illuminating">
<h3>The Sampling Distribution in Closed Form: Rare but Illuminating<a class="headerlink" href="#the-sampling-distribution-in-closed-form-rare-but-illuminating" title="Link to this heading"></a></h3>
<p>In a few special cases, we can derive the sampling distribution <span class="math notranslate nohighlight">\(G\)</span> exactly. These cases are illuminating because they show what we’re trying to approximate in general.</p>
<p><strong>Case 1: Sample Mean from Normal Population</strong></p>
<p>Suppose <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} N(\mu, \sigma^2)\)</span> and <span class="math notranslate nohighlight">\(\hat{\theta} = \bar{X} = \frac{1}{n}\sum_{i=1}^n X_i\)</span>.</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Exact Sampling Distribution of Sample Mean</p>
<p>If <span class="math notranslate nohighlight">\(X_i \stackrel{\text{iid}}{\sim} N(\mu, \sigma^2)\)</span>, then:</p>
<div class="math notranslate nohighlight" id="equation-xbar-exact">
<span class="eqno">(2.142)<a class="headerlink" href="#equation-xbar-exact" title="Link to this equation"></a></span>\[\bar{X} \sim N\left(\mu, \frac{\sigma^2}{n}\right)\]</div>
<p><strong>Proof</strong>: Since <span class="math notranslate nohighlight">\(\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i\)</span> is a linear combination of independent normals, it is normal. Its mean is:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[\bar{X}] = \frac{1}{n}\sum_{i=1}^n \mathbb{E}[X_i] = \frac{1}{n} \cdot n\mu = \mu\]</div>
<p>Its variance is:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\bar{X}) = \frac{1}{n^2}\sum_{i=1}^n \text{Var}(X_i) = \frac{1}{n^2} \cdot n\sigma^2 = \frac{\sigma^2}{n}\]</div>
<p>∎</p>
</div>
<p>This is one of the rare cases where we know <span class="math notranslate nohighlight">\(G\)</span> exactly. From this, everything follows immediately:</p>
<ul class="simple">
<li><p>Bias: <span class="math notranslate nohighlight">\(\mathbb{E}[\bar{X}] - \mu = 0\)</span> (unbiased)</p></li>
<li><p>Standard error: <span class="math notranslate nohighlight">\(\text{SE}(\bar{X}) = \sigma/\sqrt{n}\)</span></p></li>
<li><p>95% CI (if <span class="math notranslate nohighlight">\(\sigma\)</span> known): <span class="math notranslate nohighlight">\(\bar{X} \pm 1.96 \sigma/\sqrt{n}\)</span></p></li>
<li><p>95% CI (if <span class="math notranslate nohighlight">\(\sigma\)</span> unknown): <span class="math notranslate nohighlight">\(\bar{X} \pm t_{n-1, 0.975} \cdot s/\sqrt{n}\)</span> where <span class="math notranslate nohighlight">\(s\)</span> is the sample standard deviation</p></li>
</ul>
<p><strong>Case 2: Sample Variance from Normal Population</strong></p>
<p>For <span class="math notranslate nohighlight">\(S^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X})^2\)</span>:</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Distribution of Sample Variance</p>
<p>If <span class="math notranslate nohighlight">\(X_i \stackrel{\text{iid}}{\sim} N(\mu, \sigma^2)\)</span>, then:</p>
<div class="math notranslate nohighlight" id="equation-s2-exact">
<span class="eqno">(2.143)<a class="headerlink" href="#equation-s2-exact" title="Link to this equation"></a></span>\[\frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n-1}\]</div>
<p>and <span class="math notranslate nohighlight">\(S^2\)</span> is independent of <span class="math notranslate nohighlight">\(\bar{X}\)</span>.</p>
</div>
<p>This result, combined with the distribution of <span class="math notranslate nohighlight">\(\bar{X}\)</span>, gives us the t-distribution:</p>
<div class="math notranslate nohighlight">
\[\frac{\bar{X} - \mu}{S/\sqrt{n}} \sim t_{n-1}\]</div>
<p><strong>Why These Results Are Exceptional</strong></p>
<p>These closed-form results are exceptional, not typical. They rely on:</p>
<ol class="arabic simple">
<li><p><strong>Specific distributional assumptions</strong> (normality)</p></li>
<li><p><strong>Specific statistics</strong> (means, variances, linear combinations)</p></li>
<li><p><strong>Mathematical properties</strong> (closure of normals under linear combinations, Cochran’s theorem for independence)</p></li>
</ol>
<p>For most statistics and most populations, no closed-form sampling distribution exists.</p>
<div class="note admonition">
<p class="admonition-title">Example 💡 Exact Inference When G Is Known</p>
<p><strong>Given</strong>: <span class="math notranslate nohighlight">\(X_1, \ldots, X_{16} \stackrel{\text{iid}}{\sim} N(100, 225)\)</span> (IQ scores with <span class="math notranslate nohighlight">\(\mu = 100\)</span>, <span class="math notranslate nohighlight">\(\sigma = 15\)</span>).</p>
<p><strong>Find</strong>: The sampling distribution of <span class="math notranslate nohighlight">\(\bar{X}\)</span> and a 95% confidence interval for <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p><strong>Solution</strong>:</p>
<p>Since <span class="math notranslate nohighlight">\(\bar{X} \sim N(\mu, \sigma^2/n) = N(100, 225/16) = N(100, 14.0625)\)</span>:</p>
<ul class="simple">
<li><p>Standard error: <span class="math notranslate nohighlight">\(\text{SE} = 15/\sqrt{16} = 3.75\)</span></p></li>
<li><p>95% CI (σ known): <span class="math notranslate nohighlight">\(100 \pm 1.96 \times 3.75 = [92.65, 107.35]\)</span></p></li>
</ul>
<p><strong>Python verification</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Parameters</span>
<span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">16</span>
<span class="n">se_exact</span> <span class="o">=</span> <span class="n">sigma</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># Exact sampling distribution</span>
<span class="n">sampling_dist</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">se_exact</span><span class="p">)</span>

<span class="c1"># Confidence interval quantiles</span>
<span class="n">ci_lower</span> <span class="o">=</span> <span class="n">sampling_dist</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.025</span><span class="p">)</span>  <span class="c1"># 2.5th percentile</span>
<span class="n">ci_upper</span> <span class="o">=</span> <span class="n">sampling_dist</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.975</span><span class="p">)</span>  <span class="c1"># 97.5th percentile</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sampling distribution: N(</span><span class="si">{</span><span class="n">mu</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">se_exact</span><span class="o">**</span><span class="mi">2</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Standard Error: </span><span class="si">{</span><span class="n">se_exact</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% CI: [</span><span class="si">{</span><span class="n">ci_lower</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_upper</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>

<span class="c1"># Verify by simulation</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_sim</span> <span class="o">=</span> <span class="mi">100000</span>
<span class="n">sample_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">)])</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Simulation verification (</span><span class="si">{</span><span class="n">n_sim</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> samples):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean of X̄: </span><span class="si">{</span><span class="n">sample_means</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="n">mu</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;SD of X̄: </span><span class="si">{</span><span class="n">sample_means</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="n">se_exact</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Sampling distribution: N(100, 14.0625)
Standard Error: 3.7500
95% CI: [92.65, 107.35]

Simulation verification (100,000 samples):
Mean of X̄: 100.01 (theory: 100)
SD of X̄: 3.7502 (theory: 3.7500)
</pre></div>
</div>
<p>This is the ideal scenario: the sampling distribution is known exactly, and all uncertainty quantification follows directly from probability theory.</p>
</div>
</section>
</section>
<section id="historical-development-the-quest-for-sampling-distributions">
<h2>Historical Development: The Quest for Sampling Distributions<a class="headerlink" href="#historical-development-the-quest-for-sampling-distributions" title="Link to this heading"></a></h2>
<p>The search for sampling distributions has shaped the history of statistics. Understanding this development illuminates why resampling methods represent such a profound conceptual advance.</p>
<section id="the-classical-era-exact-distributions-19001930">
<h3>The Classical Era: Exact Distributions (1900–1930)<a class="headerlink" href="#the-classical-era-exact-distributions-19001930" title="Link to this heading"></a></h3>
<p>The early 20th century saw remarkable mathematical achievements in deriving exact sampling distributions for specific statistics under specific distributional assumptions.</p>
<p><strong>Pearson’s Chi-Square (1900)</strong></p>
<p>Karl Pearson derived the chi-square distribution for goodness-of-fit testing:</p>
<div class="math notranslate nohighlight">
\[\chi^2 = \sum_{i=1}^k \frac{(O_i - E_i)^2}{E_i} \xrightarrow{d} \chi^2_{k-1}\]</div>
<p>under the null hypothesis, where <span class="math notranslate nohighlight">\(O_i\)</span> are observed counts and <span class="math notranslate nohighlight">\(E_i\)</span> are expected counts.</p>
<p><strong>Student’s t-Distribution (1908)</strong></p>
<p>William Sealy Gosset, publishing under the pseudonym “Student,” derived the exact distribution of:</p>
<div class="math notranslate nohighlight" id="equation-t-statistic">
<span class="eqno">(2.144)<a class="headerlink" href="#equation-t-statistic" title="Link to this equation"></a></span>\[T = \frac{\bar{X} - \mu}{S/\sqrt{n}} \sim t_{n-1}\]</div>
<p>when <span class="math notranslate nohighlight">\(X_i \sim N(\mu, \sigma^2)\)</span>. This was a breakthrough because it freed the confidence interval from requiring known <span class="math notranslate nohighlight">\(\sigma\)</span>. The derivation relied heavily on the normal assumption and the independence of <span class="math notranslate nohighlight">\(\bar{X}\)</span> and <span class="math notranslate nohighlight">\(S^2\)</span> (Cochran’s theorem).</p>
<p><strong>Fisher’s F-Distribution (1920s)</strong></p>
<p>Ronald Fisher derived the distribution of the ratio of independent chi-square variables:</p>
<div class="math notranslate nohighlight">
\[F = \frac{\chi^2_m / m}{\chi^2_n / n} \sim F_{m,n}\]</div>
<p>This enabled analysis of variance (ANOVA) and the testing of variance ratios—again, under normality.</p>
<p><strong>The Limitation of Exact Methods</strong></p>
<p>These exact results required:</p>
<ul class="simple">
<li><p>Specific distributional assumptions (usually normality)</p></li>
<li><p>Specific statistics (means, variances, linear combinations)</p></li>
<li><p>Mathematical tractability (closure properties, independence results)</p></li>
</ul>
<p>For a sample median, a correlation coefficient, or a ratio of means, no closed-form sampling distribution was available. The “distribution-free” methods that emerged (rank tests, etc.) addressed some cases but were limited in scope.</p>
</section>
<section id="the-asymptotic-era-central-limit-theorem-19301970">
<h3>The Asymptotic Era: Central Limit Theorem (1930–1970)<a class="headerlink" href="#the-asymptotic-era-central-limit-theorem-19301970" title="Link to this heading"></a></h3>
<p>The Central Limit Theorem liberated statistical inference from strict distributional assumptions, at least for large samples. This represented a fundamental shift from exact to approximate inference.</p>
<p><strong>The Classical CLT</strong></p>
<div class="note admonition">
<p class="admonition-title">Theorem: Central Limit Theorem</p>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> be iid with <span class="math notranslate nohighlight">\(\mathbb{E}[X_i] = \mu\)</span> and <span class="math notranslate nohighlight">\(\text{Var}(X_i) = \sigma^2 &lt; \infty\)</span>. Then:</p>
<div class="math notranslate nohighlight" id="equation-clt">
<span class="eqno">(2.145)<a class="headerlink" href="#equation-clt" title="Link to this equation"></a></span>\[\frac{\sqrt{n}(\bar{X} - \mu)}{\sigma} \xrightarrow{d} N(0, 1) \quad \text{as } n \to \infty\]</div>
<p>or equivalently:</p>
<div class="math notranslate nohighlight">
\[\sqrt{n}(\bar{X} - \mu) \xrightarrow{d} N(0, \sigma^2)\]</div>
</div>
<p>This result holds <strong>regardless of the shape of</strong> <span class="math notranslate nohighlight">\(F\)</span>, provided <span class="math notranslate nohighlight">\(F\)</span> has finite variance. The sampling distribution of <span class="math notranslate nohighlight">\(\bar{X}\)</span> is approximately normal for large <span class="math notranslate nohighlight">\(n\)</span>, no matter how non-normal the population.</p>
<p><strong>The Delta Method</strong></p>
<p>For smooth functions <span class="math notranslate nohighlight">\(g\)</span>, the CLT extends via Taylor expansion:</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Delta Method</p>
<p>If <span class="math notranslate nohighlight">\(\sqrt{n}(\hat{\theta} - \theta) \xrightarrow{d} N(0, \sigma^2)\)</span> and <span class="math notranslate nohighlight">\(g\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(\theta\)</span> with <span class="math notranslate nohighlight">\(g'(\theta) \neq 0\)</span>, then:</p>
<div class="math notranslate nohighlight" id="equation-delta-method">
<span class="eqno">(2.146)<a class="headerlink" href="#equation-delta-method" title="Link to this equation"></a></span>\[\sqrt{n}(g(\hat{\theta}) - g(\theta)) \xrightarrow{d} N(0, [g'(\theta)]^2 \sigma^2)\]</div>
<p><strong>Proof sketch</strong>: By Taylor expansion around <span class="math notranslate nohighlight">\(\theta\)</span>:</p>
<div class="math notranslate nohighlight">
\[g(\hat{\theta}) - g(\theta) \approx g'(\theta)(\hat{\theta} - \theta)\]</div>
<p>Multiplying by <span class="math notranslate nohighlight">\(\sqrt{n}\)</span> and applying Slutsky’s theorem gives the result. ∎</p>
</div>
<p>This enabled approximate inference for transformed quantities: log means, square roots, ratios (via multivariate delta method).</p>
<p><strong>Asymptotic Theory for MLEs</strong></p>
<p>Maximum likelihood estimators have elegant asymptotic properties:</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Asymptotic Normality of MLE</p>
<p>Under regularity conditions (see <a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#ch3-2-maximum-likelihood-estimation"><span class="std std-ref">Section 3.2</span></a>):</p>
<div class="math notranslate nohighlight" id="equation-mle-asymptotic">
<span class="eqno">(2.147)<a class="headerlink" href="#equation-mle-asymptotic" title="Link to this equation"></a></span>\[\sqrt{n}(\hat{\theta}_{\text{MLE}} - \theta_0) \xrightarrow{d} N(0, I_1(\theta_0)^{-1})\]</div>
<p>where <span class="math notranslate nohighlight">\(I_1(\theta)\)</span> is the Fisher information per observation.</p>
</div>
<p>This provides a universal recipe for inference: compute the MLE, estimate Fisher information, and use the normal approximation.</p>
<p><strong>The Limitation of Asymptotic Methods</strong></p>
<p>Asymptotic results describe behavior as <span class="math notranslate nohighlight">\(n \to \infty\)</span>, but we always have finite <span class="math notranslate nohighlight">\(n\)</span>. Critical questions include:</p>
<ul class="simple">
<li><p><strong>How large must</strong> <span class="math notranslate nohighlight">\(n\)</span> <strong>be?</strong> There is no universal answer. The required <span class="math notranslate nohighlight">\(n\)</span> depends on:
- The population distribution <span class="math notranslate nohighlight">\(F\)</span> (skewness, kurtosis, tail behavior)
- The statistic <span class="math notranslate nohighlight">\(T\)</span> (smooth vs. non-smooth)
- The target of inference (center vs. tails of <span class="math notranslate nohighlight">\(G\)</span>)</p></li>
<li><p><strong>What about non-smooth statistics?</strong> The median, quantiles, and other non-smooth statistics have slower convergence or require different asymptotic theory.</p></li>
<li><p><strong>What about boundary parameters?</strong> When <span class="math notranslate nohighlight">\(\theta\)</span> is near the boundary of the parameter space, standard asymptotic theory fails.</p></li>
</ul>
</section>
<section id="the-computational-era-resampling-1979present">
<h3>The Computational Era: Resampling (1979–present)<a class="headerlink" href="#the-computational-era-resampling-1979present" title="Link to this heading"></a></h3>
<p>Bradley Efron’s 1979 paper “Bootstrap Methods: Another Look at the Jackknife” inaugurated a new paradigm that fundamentally changed statistical practice.</p>
<p><strong>The Key Insight</strong></p>
<p>Efron’s insight was deceptively simple:</p>
<div class="note admonition">
<p class="admonition-title">The Bootstrap Idea</p>
<p>If we cannot derive <span class="math notranslate nohighlight">\(G_F\)</span> analytically or trust asymptotic approximations, we can <strong>estimate</strong> <span class="math notranslate nohighlight">\(G_F\)</span> by Monte Carlo simulation—but using the empirical distribution <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> as a stand-in for the unknown <span class="math notranslate nohighlight">\(F\)</span>.</p>
</div>
<p>The bootstrap does not derive <span class="math notranslate nohighlight">\(G\)</span>; it <strong>simulates</strong> an approximation to <span class="math notranslate nohighlight">\(G\)</span> by treating <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> as if it were the true population.</p>
<p><strong>Why This Works (Informally)</strong></p>
<ol class="arabic simple">
<li><p>The empirical distribution <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> converges to <span class="math notranslate nohighlight">\(F\)</span> (Glivenko-Cantelli theorem)</p></li>
<li><p>For “nice” functionals <span class="math notranslate nohighlight">\(T\)</span>, if <span class="math notranslate nohighlight">\(\hat{F}_n \approx F\)</span>, then <span class="math notranslate nohighlight">\(G_{\hat{F}_n} \approx G_F\)</span></p></li>
<li><p>We can compute <span class="math notranslate nohighlight">\(G_{\hat{F}_n}\)</span> by Monte Carlo: resample from <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> (i.e., sample with replacement from the data)</p></li>
</ol>
<p><strong>The Conceptual Shift</strong></p>
<p>The shift was profound:</p>
<ul class="simple">
<li><p><strong>Classical question</strong>: “What is the exact or asymptotic form of <span class="math notranslate nohighlight">\(G_F\)</span>?”</p></li>
<li><p><strong>Bootstrap question</strong>: “What would <span class="math notranslate nohighlight">\(G\)</span> look like if <span class="math notranslate nohighlight">\(F = \hat{F}_n\)</span>?”</p></li>
</ul>
<p>The second question can be answered by simulation for virtually any statistic, without requiring distributional assumptions or asymptotic theory.</p>
<p><strong>Historical Impact</strong></p>
<p>The bootstrap arrived at the right moment:</p>
<ul class="simple">
<li><p>Computational power was becoming widely available (1970s–1980s)</p></li>
<li><p>Statisticians recognized the limitations of asymptotic methods</p></li>
<li><p>Applied problems increasingly involved complex statistics</p></li>
</ul>
<p>Today, bootstrap methods are standard tools in virtually every area of applied statistics.</p>
</section>
</section>
<section id="three-routes-to-the-sampling-distribution">
<h2>Three Routes to the Sampling Distribution<a class="headerlink" href="#three-routes-to-the-sampling-distribution" title="Link to this heading"></a></h2>
<p>We now systematically compare the three canonical approaches to approximating the sampling distribution <span class="math notranslate nohighlight">\(G\)</span>. Each has distinct mathematical foundations, computational requirements, and domains of applicability.</p>
<section id="route-1-analytic-asymptotic-derivation">
<h3>Route 1: Analytic/Asymptotic Derivation<a class="headerlink" href="#route-1-analytic-asymptotic-derivation" title="Link to this heading"></a></h3>
<p><strong>Mathematical Foundation</strong></p>
<p>The analytic approach uses probability theory—exact distributional results, the Central Limit Theorem, delta method, and likelihood theory—to derive <span class="math notranslate nohighlight">\(G\)</span> or its large-sample approximation.</p>
<p><strong>Exact Results</strong> (when available):</p>
<ul class="simple">
<li><p>Normal population, sample mean: <span class="math notranslate nohighlight">\(\bar{X} \sim N(\mu, \sigma^2/n)\)</span></p></li>
<li><p>Normal population, sample variance: <span class="math notranslate nohighlight">\((n-1)S^2/\sigma^2 \sim \chi^2_{n-1}\)</span></p></li>
<li><p>Two normals, variance ratio: <span class="math notranslate nohighlight">\(F = S_1^2/S_2^2 \sim F_{n_1-1, n_2-1}\)</span> (under equal variance null)</p></li>
</ul>
<p><strong>Asymptotic Results</strong> (for large <span class="math notranslate nohighlight">\(n\)</span>):</p>
<ul class="simple">
<li><p>Sample mean (any <span class="math notranslate nohighlight">\(F\)</span> with finite variance): <span class="math notranslate nohighlight">\(\sqrt{n}(\bar{X} - \mu) \xrightarrow{d} N(0, \sigma^2)\)</span></p></li>
<li><p>MLE (under regularity): <span class="math notranslate nohighlight">\(\sqrt{n}(\hat{\theta} - \theta_0) \xrightarrow{d} N(0, I_1(\theta_0)^{-1})\)</span></p></li>
<li><p>Sample correlation: <span class="math notranslate nohighlight">\(\sqrt{n}(\hat{\rho} - \rho) \xrightarrow{d} N(0, (1-\rho^2)^2)\)</span> for bivariate normal</p></li>
</ul>
<p><strong>Practical Implementation</strong></p>
<p>For the sample mean:</p>
<div class="math notranslate nohighlight">
\[\hat{\theta} = \bar{X}, \quad \widehat{\text{SE}} = \frac{s}{\sqrt{n}}, \quad \text{CI: } \bar{X} \pm t_{n-1, 1-\alpha/2} \cdot \frac{s}{\sqrt{n}}\]</div>
<p>For MLE <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\widehat{\text{SE}} = \sqrt{[\hat{I}(\hat{\theta})]^{-1}}, \quad \text{CI: } \hat{\theta} \pm z_{1-\alpha/2} \cdot \widehat{\text{SE}}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{I}(\hat{\theta})\)</span> is the observed or expected Fisher information.</p>
<p><strong>Strengths</strong></p>
<ul class="simple">
<li><p><strong>Computational efficiency</strong>: Closed-form formulas are instantaneous</p></li>
<li><p><strong>No Monte Carlo variability</strong>: Results are deterministic given the data</p></li>
<li><p><strong>Theoretical optimality</strong>: Under correct assumptions, often achieves optimal efficiency</p></li>
<li><p><strong>Deep insight</strong>: Formulas reveal how SE depends on <span class="math notranslate nohighlight">\(n\)</span>, <span class="math notranslate nohighlight">\(\sigma\)</span>, design, etc.</p></li>
</ul>
<p><strong>Weaknesses</strong></p>
<ul class="simple">
<li><p><strong>Requires strong assumptions</strong> that may not hold in practice</p></li>
<li><p><strong>Limited to specific statistics</strong>: Works for smooth functions of moments, not for medians, quantiles, etc.</p></li>
<li><p><strong>Asymptotic approximations may be poor</strong> for finite <span class="math notranslate nohighlight">\(n\)</span>, especially with skewed or heavy-tailed data</p></li>
<li><p><strong>Fails near boundaries</strong>: Standard theory breaks down when <span class="math notranslate nohighlight">\(\theta\)</span> is near the boundary of the parameter space</p></li>
</ul>
<table class="docutils align-default" id="id2">
<caption><span class="caption-number">Table 2.23 </span><span class="caption-text">Statistics with Tractable Analytic/Asymptotic Sampling Distributions</span><a class="headerlink" href="#id2" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 25.0%" />
<col style="width: 35.0%" />
<col style="width: 40.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Statistic</p></th>
<th class="head"><p>Sampling Distribution</p></th>
<th class="head"><p>Conditions</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Sample mean <span class="math notranslate nohighlight">\(\bar{X}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(N(\mu, \sigma^2/n)\)</span> exactly or asymptotically</p></td>
<td><p>Normal <span class="math notranslate nohighlight">\(F\)</span> (exact) or finite variance (asymptotic)</p></td>
</tr>
<tr class="row-odd"><td><p>Sample variance <span class="math notranslate nohighlight">\(S^2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{\sigma^2}{n-1} \chi^2_{n-1}\)</span></p></td>
<td><p>Normal <span class="math notranslate nohighlight">\(F\)</span> (exact only)</p></td>
</tr>
<tr class="row-even"><td><p>Sample proportion <span class="math notranslate nohighlight">\(\hat{p}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(N(p, p(1-p)/n)\)</span> asymptotically</p></td>
<td><p><span class="math notranslate nohighlight">\(np\)</span> and <span class="math notranslate nohighlight">\(n(1-p)\)</span> large</p></td>
</tr>
<tr class="row-odd"><td><p>MLE <span class="math notranslate nohighlight">\(\hat{\theta}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(N(\theta_0, I_n(\theta_0)^{-1})\)</span> asymptotically</p></td>
<td><p>Regularity conditions, large <span class="math notranslate nohighlight">\(n\)</span></p></td>
</tr>
<tr class="row-even"><td><p>OLS coefficient <span class="math notranslate nohighlight">\(\hat{\beta}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(N(\beta, \sigma^2(\mathbf{X}'\mathbf{X})^{-1})\)</span> exactly</p></td>
<td><p>Normal errors, fixed <span class="math notranslate nohighlight">\(\mathbf{X}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Pearson correlation <span class="math notranslate nohighlight">\(\hat{\rho}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(N(\rho, (1-\rho^2)^2/n)\)</span> asymptotically</p></td>
<td><p>Bivariate normal, large <span class="math notranslate nohighlight">\(n\)</span></p></td>
</tr>
</tbody>
</table>
</section>
<section id="route-2-parametric-monte-carlo-parametric-bootstrap">
<h3>Route 2: Parametric Monte Carlo (Parametric Bootstrap)<a class="headerlink" href="#route-2-parametric-monte-carlo-parametric-bootstrap" title="Link to this heading"></a></h3>
<p><strong>Mathematical Foundation</strong></p>
<p>Assume the population belongs to a parametric family: <span class="math notranslate nohighlight">\(F \in \{F_\eta : \eta \in \mathcal{H}\}\)</span>. Estimate the parameter <span class="math notranslate nohighlight">\(\hat{\eta}\)</span> from data, then simulate many datasets from <span class="math notranslate nohighlight">\(F_{\hat{\eta}}\)</span>, computing the statistic for each.</p>
<p><strong>The Algorithm</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Algorithm: Parametric Bootstrap

Input: Data x = (x_1, ..., x_n), parametric family {F_eta}, statistic T, replications B
Output: Monte Carlo approximation to G

1. Fit model: eta_hat = estimate from x (e.g., MLE)
2. For b = 1, 2, ..., B:
   a. Generate x*^(b) = (x_1*^(b), ..., x_n*^(b)) from F_{eta_hat}
   b. Compute theta_hat*^(b) = T(x*^(b))
3. Return {theta_hat*_1, ..., theta_hat*_B} as Monte Carlo sample from G_{F_{eta_hat}}

Use the bootstrap distribution for:
- Standard error: SE_boot = sd{theta_hat*_1, ..., theta_hat*_B}
- Bias estimate: bias_boot = mean{theta_hat*} - theta_hat
- Confidence intervals: various methods
</pre></div>
</div>
<p><strong>Mathematical Justification</strong></p>
<p>The parametric bootstrap approximates:</p>
<div class="math notranslate nohighlight">
\[G_F(t) = P_F\{T(X_1, \ldots, X_n) \leq t\} \approx G_{F_{\hat{\eta}}}(t) = P_{F_{\hat{\eta}}}\{T(X_1^*, \ldots, X_n^*) \leq t\}\]</div>
<p>If the parametric model is correct (i.e., <span class="math notranslate nohighlight">\(F = F_{\eta_0}\)</span> for some <span class="math notranslate nohighlight">\(\eta_0\)</span>) and <span class="math notranslate nohighlight">\(\hat{\eta}\)</span> is consistent, then <span class="math notranslate nohighlight">\(G_{F_{\hat{\eta}}}\)</span> converges to <span class="math notranslate nohighlight">\(G_F\)</span>.</p>
<p><strong>Strengths</strong></p>
<ul class="simple">
<li><p><strong>Applicable to complex statistics</strong>: Works for any <span class="math notranslate nohighlight">\(T\)</span> that can be computed</p></li>
<li><p><strong>Can generate values outside observed data range</strong>: Important for tail inference</p></li>
<li><p><strong>Efficient when model is correct</strong>: Uses parametric structure for precision</p></li>
<li><p><strong>Flexible</strong>: Can incorporate complex model features (random effects, dependencies)</p></li>
</ul>
<p><strong>Weaknesses</strong></p>
<ul class="simple">
<li><p><strong>Requires specifying a parametric model</strong>: Need to choose <span class="math notranslate nohighlight">\(F_\eta\)</span></p></li>
<li><p><strong>Sensitive to model misspecification</strong>: If <span class="math notranslate nohighlight">\(F \notin \{F_\eta\}\)</span>, results may be biased</p></li>
<li><p><strong>Two sources of error</strong>: Model error and Monte Carlo error</p></li>
<li><p><strong>More computationally intensive</strong> than analytic methods</p></li>
</ul>
<p><strong>Example: Parametric Bootstrap for Exponential Rate</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">parametric_bootstrap_exponential</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">B</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parametric bootstrap for exponential rate parameter.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : array</span>
<span class="sd">        Observed data (assumed exponential)</span>
<span class="sd">    B : int</span>
<span class="sd">        Number of bootstrap replicates</span>
<span class="sd">    seed : int</span>
<span class="sd">        Random seed</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict with rate_hat, se_boot, ci_percentile, boot_dist</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># MLE for exponential rate</span>
    <span class="n">rate_hat</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="c1"># Parametric bootstrap: sample from Exp(rate_hat)</span>
    <span class="n">rate_boot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
        <span class="n">x_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">rate_hat</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
        <span class="n">rate_boot</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">x_star</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="n">se_boot</span> <span class="o">=</span> <span class="n">rate_boot</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ci_percentile</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">rate_boot</span><span class="p">,</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">97.5</span><span class="p">])</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;rate_hat&#39;</span><span class="p">:</span> <span class="n">rate_hat</span><span class="p">,</span>
        <span class="s1">&#39;se_boot&#39;</span><span class="p">:</span> <span class="n">se_boot</span><span class="p">,</span>
        <span class="s1">&#39;ci_percentile&#39;</span><span class="p">:</span> <span class="n">ci_percentile</span><span class="p">,</span>
        <span class="s1">&#39;boot_dist&#39;</span><span class="p">:</span> <span class="n">rate_boot</span>
    <span class="p">}</span>

<span class="c1"># Example usage</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">true_rate</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">true_rate</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">parametric_bootstrap_exponential</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">B</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True rate: </span><span class="si">{</span><span class="n">true_rate</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MLE: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;rate_hat&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bootstrap SE: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;se_boot&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% CI: [</span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci_percentile&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci_percentile&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="route-3-nonparametric-bootstrap">
<h3>Route 3: Nonparametric Bootstrap<a class="headerlink" href="#route-3-nonparametric-bootstrap" title="Link to this heading"></a></h3>
<p><strong>Mathematical Foundation</strong></p>
<p>Replace the unknown <span class="math notranslate nohighlight">\(F\)</span> with the empirical distribution <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>, which places mass <span class="math notranslate nohighlight">\(1/n\)</span> at each observed data point:</p>
<div class="math notranslate nohighlight" id="equation-ecdf-def">
<span class="eqno">(2.148)<a class="headerlink" href="#equation-ecdf-def" title="Link to this equation"></a></span>\[\hat{F}_n(x) = \frac{1}{n} \sum_{i=1}^n \mathbf{1}\{X_i \leq x\}\]</div>
<p>Simulate datasets by resampling with replacement from <span class="math notranslate nohighlight">\(\{X_1, \ldots, X_n\}\)</span>.</p>
<p><strong>The Algorithm</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Algorithm: Nonparametric Bootstrap

Input: Data x = (x_1, ..., x_n), statistic T, replications B
Output: Bootstrap distribution {theta_hat*_1, ..., theta_hat*_B}

1. Compute point estimate: theta_hat = T(x)
2. For b = 1, 2, ..., B:
   a. Generate x*^(b) by sampling n values from {x_1, ..., x_n} with replacement
   b. Compute theta_hat*^(b) = T(x*^(b))
3. Return {theta_hat*_1, ..., theta_hat*_B}
</pre></div>
</div>
<p><strong>Mathematical Justification</strong></p>
<p>The nonparametric bootstrap approximates:</p>
<div class="math notranslate nohighlight" id="equation-bootstrap-approx">
<span class="eqno">(2.149)<a class="headerlink" href="#equation-bootstrap-approx" title="Link to this equation"></a></span>\[G_F(t) = P_F\{T(X_1, \ldots, X_n) \leq t\} \approx G_{\hat{F}_n}(t) = P_{\hat{F}_n}\{T(X_1^*, \ldots, X_n^*) \leq t\}\]</div>
<p>The justification rests on:</p>
<ol class="arabic simple">
<li><p><strong>Consistency of</strong> <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>: By Glivenko-Cantelli, <span class="math notranslate nohighlight">\(\|\hat{F}_n - F\|_\infty \to 0\)</span> almost surely</p></li>
<li><p><strong>Continuity of the functional</strong>: For “smooth” functionals, <span class="math notranslate nohighlight">\(G_{\hat{F}_n} \to G_F\)</span> when <span class="math notranslate nohighlight">\(\hat{F}_n \to F\)</span></p></li>
</ol>
<p>We develop this rigorously in <span class="xref std std-ref">Section 4.2</span>.</p>
<p><strong>Strengths</strong></p>
<ul class="simple">
<li><p><strong>No parametric model required</strong>: Completely nonparametric</p></li>
<li><p><strong>Automatic adaptation</strong>: Captures skewness, heavy tails, heterogeneity in the data</p></li>
<li><p><strong>Universal applicability</strong>: Works for essentially any statistic <span class="math notranslate nohighlight">\(T\)</span></p></li>
<li><p><strong>Captures finite-sample behavior</strong>: Reflects actual data characteristics, not asymptotic idealizations</p></li>
<li><p><strong>Easy to implement</strong>: Requires only the ability to compute <span class="math notranslate nohighlight">\(T\)</span></p></li>
</ul>
<p><strong>Weaknesses</strong></p>
<ul class="simple">
<li><p><strong>Cannot generate values outside the observed data range</strong>: Bootstrap samples are limited to observed values</p></li>
<li><p><strong>May fail for extreme-value statistics</strong>: Max, min, high quantiles have non-regular behavior</p></li>
<li><p><strong>Requires modification for dependent or designed data</strong>: Standard bootstrap assumes iid</p></li>
<li><p><strong>Computationally intensive</strong>: Requires <span class="math notranslate nohighlight">\(B\)</span> evaluations of <span class="math notranslate nohighlight">\(T\)</span></p></li>
</ul>
<p><strong>Implementation</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span>

<span class="k">def</span><span class="w"> </span><span class="nf">bootstrap_distribution</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">statistic</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="nb">float</span><span class="p">],</span>
    <span class="n">B</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
    <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate the bootstrap distribution of a statistic.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : np.ndarray</span>
<span class="sd">        Original data sample of shape (n,) or (n, p)</span>
<span class="sd">    statistic : callable</span>
<span class="sd">        Function that computes the statistic from a sample.</span>
<span class="sd">        Should accept an array and return a scalar.</span>
<span class="sd">    B : int</span>
<span class="sd">        Number of bootstrap replicates (default 10000)</span>
<span class="sd">    seed : int, optional</span>
<span class="sd">        Random seed for reproducibility</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    np.ndarray</span>
<span class="sd">        Bootstrap replicates of shape (B,)</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; rng = np.random.default_rng(42)</span>
<span class="sd">    &gt;&gt;&gt; x = rng.normal(0, 1, 50)</span>
<span class="sd">    &gt;&gt;&gt; boot_means = bootstrap_distribution(x, np.mean, B=5000, seed=42)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;Bootstrap SE: {boot_means.std(ddof=1):.4f}&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">theta_boot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
        <span class="c1"># Resample with replacement</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">x_star</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
        <span class="n">theta_boot</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">statistic</span><span class="p">(</span><span class="n">x_star</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">theta_boot</span>


<span class="k">def</span><span class="w"> </span><span class="nf">bootstrap_inference</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">statistic</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="nb">float</span><span class="p">],</span>
    <span class="n">B</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
    <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
    <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Complete bootstrap inference for a statistic.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : np.ndarray</span>
<span class="sd">        Original data</span>
<span class="sd">    statistic : callable</span>
<span class="sd">        Statistic function</span>
<span class="sd">    B : int</span>
<span class="sd">        Number of bootstrap replicates</span>
<span class="sd">    alpha : float</span>
<span class="sd">        Significance level for confidence interval</span>
<span class="sd">    seed : int</span>
<span class="sd">        Random seed</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict with keys:</span>
<span class="sd">        &#39;theta_hat&#39;: point estimate</span>
<span class="sd">        &#39;se_boot&#39;: bootstrap standard error</span>
<span class="sd">        &#39;bias_boot&#39;: bootstrap bias estimate</span>
<span class="sd">        &#39;ci_percentile&#39;: percentile confidence interval</span>
<span class="sd">        &#39;ci_basic&#39;: basic (pivotal) confidence interval</span>
<span class="sd">        &#39;boot_dist&#39;: bootstrap distribution</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">theta_hat</span> <span class="o">=</span> <span class="n">statistic</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">boot_dist</span> <span class="o">=</span> <span class="n">bootstrap_distribution</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">statistic</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>

    <span class="n">se_boot</span> <span class="o">=</span> <span class="n">boot_dist</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">bias_boot</span> <span class="o">=</span> <span class="n">boot_dist</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">-</span> <span class="n">theta_hat</span>

    <span class="c1"># Percentile interval</span>
    <span class="n">ci_percentile</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">boot_dist</span><span class="p">,</span> <span class="p">[</span><span class="mi">100</span><span class="o">*</span><span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">)])</span>

    <span class="c1"># Basic (pivotal) interval</span>
    <span class="n">q_lower</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">boot_dist</span><span class="p">,</span> <span class="mi">100</span><span class="o">*</span><span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">q_upper</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">boot_dist</span><span class="p">,</span> <span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">ci_basic</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">theta_hat</span> <span class="o">-</span> <span class="n">q_upper</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">theta_hat</span> <span class="o">-</span> <span class="n">q_lower</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;theta_hat&#39;</span><span class="p">:</span> <span class="n">theta_hat</span><span class="p">,</span>
        <span class="s1">&#39;se_boot&#39;</span><span class="p">:</span> <span class="n">se_boot</span><span class="p">,</span>
        <span class="s1">&#39;bias_boot&#39;</span><span class="p">:</span> <span class="n">bias_boot</span><span class="p">,</span>
        <span class="s1">&#39;ci_percentile&#39;</span><span class="p">:</span> <span class="n">ci_percentile</span><span class="p">,</span>
        <span class="s1">&#39;ci_basic&#39;</span><span class="p">:</span> <span class="n">ci_basic</span><span class="p">,</span>
        <span class="s1">&#39;boot_dist&#39;</span><span class="p">:</span> <span class="n">boot_dist</span>
    <span class="p">}</span>
</pre></div>
</div>
</section>
<section id="formal-comparison-of-the-three-routes">
<h3>Formal Comparison of the Three Routes<a class="headerlink" href="#formal-comparison-of-the-three-routes" title="Link to this heading"></a></h3>
<table class="docutils align-default" id="id3">
<caption><span class="caption-number">Table 2.24 </span><span class="caption-text">Comprehensive Comparison of Approaches to the Sampling Distribution</span><a class="headerlink" href="#id3" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 18.0%" />
<col style="width: 27.0%" />
<col style="width: 27.0%" />
<col style="width: 28.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Aspect</p></th>
<th class="head"><p>Analytic/Asymptotic</p></th>
<th class="head"><p>Parametric Monte Carlo</p></th>
<th class="head"><p>Nonparametric Bootstrap</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Population assumption</strong></p></td>
<td><p>Known form or CLT applies</p></td>
<td><p><span class="math notranslate nohighlight">\(F \in \{F_\eta\}\)</span> (parametric family)</p></td>
<td><p>None (uses <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>What is approximated</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(G_F\)</span> exactly or asymptotically</p></td>
<td><p><span class="math notranslate nohighlight">\(G_{F_{\hat{\eta}}}\)</span> via simulation</p></td>
<td><p><span class="math notranslate nohighlight">\(G_{\hat{F}_n}\)</span> via resampling</p></td>
</tr>
<tr class="row-even"><td><p><strong>Key assumption</strong></p></td>
<td><p>Distributional form or regularity</p></td>
<td><p>Parametric model is correct</p></td>
<td><p><span class="math notranslate nohighlight">\(\hat{F}_n \approx F\)</span> (large <span class="math notranslate nohighlight">\(n\)</span>)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Tail behavior</strong></p></td>
<td><p>Theory-driven (extrapolates)</p></td>
<td><p>Model-driven (extrapolates)</p></td>
<td><p>Data-limited (no extrapolation)</p></td>
</tr>
<tr class="row-even"><td><p><strong>Smooth statistics</strong></p></td>
<td><p>Often works well</p></td>
<td><p>Works well if model fits</p></td>
<td><p>Works well</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Non-smooth statistics</strong></p></td>
<td><p>Often fails or complex</p></td>
<td><p>Works if model fits</p></td>
<td><p>Usually works</p></td>
</tr>
<tr class="row-even"><td><p><strong>Small</strong> <span class="math notranslate nohighlight">\(n\)</span></p></td>
<td><p>Asymptotics unreliable</p></td>
<td><p>Depends on model adequacy</p></td>
<td><p>May be unstable</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Computational cost</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(O(1)\)</span> (formula)</p></td>
<td><p><span class="math notranslate nohighlight">\(O(B \cdot C(n))\)</span> where <span class="math notranslate nohighlight">\(C(n)\)</span> = cost of <span class="math notranslate nohighlight">\(T\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(B \cdot C(n))\)</span></p></td>
</tr>
<tr class="row-even"><td><p><strong>MC variability</strong></p></td>
<td><p>None</p></td>
<td><p>Yes (reducible by increasing <span class="math notranslate nohighlight">\(B\)</span>)</p></td>
<td><p>Yes (reducible by increasing <span class="math notranslate nohighlight">\(B\)</span>)</p></td>
</tr>
</tbody>
</table>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_1_fig09_method_comparison.png"><img alt="Side-by-side comparison of analytic, parametric bootstrap, and nonparametric bootstrap confidence intervals for different scenarios" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_1_fig09_method_comparison.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.95 </span><span class="caption-text"><strong>Figure 4.1.9</strong>: Method Comparison Across Scenarios. Each panel shows 95% confidence intervals from three methods for a different inference problem. Panel A: Sample mean from normal data—all methods agree. Panel B: Sample median from skewed data—bootstrap intervals are appropriately asymmetric while analytic intervals are symmetric. Panel C: Ratio of means—bootstrap captures the right-skewed distribution that the delta method misses. The bootstrap adapts automatically to the structure of each problem.</span><a class="headerlink" href="#id4" title="Link to this image"></a></p>
</figcaption>
</figure>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_1_fig02_three_routes.png"><img alt="Flowchart showing three routes from population F to sampling distribution G" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_1_fig02_three_routes.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.96 </span><span class="caption-text"><strong>Figure 4.1.2</strong>: Three Routes to the Sampling Distribution. The analytic route derives <span class="math notranslate nohighlight">\(G\)</span> using probability theory—fast but assumption-heavy. Parametric Monte Carlo simulates from a fitted model—flexible but model-dependent. The bootstrap simulates from the empirical distribution—model-free but data-limited.</span><a class="headerlink" href="#id5" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="when-asymptotics-fail-motivating-the-bootstrap">
<h2>When Asymptotics Fail: Motivating the Bootstrap<a class="headerlink" href="#when-asymptotics-fail-motivating-the-bootstrap" title="Link to this heading"></a></h2>
<p>The bootstrap becomes essential when analytic and asymptotic methods fail or are unreliable. This section examines specific scenarios where classical methods break down, providing mathematical analysis and empirical demonstrations.</p>
<section id="scenario-1-non-smooth-statistics">
<h3>Scenario 1: Non-Smooth Statistics<a class="headerlink" href="#scenario-1-non-smooth-statistics" title="Link to this heading"></a></h3>
<p>Consider estimating the population median <span class="math notranslate nohighlight">\(m = F^{-1}(0.5)\)</span> using the sample median <span class="math notranslate nohighlight">\(\hat{m}\)</span>.</p>
<p><strong>The Asymptotic Theory</strong></p>
<p>The sample median has asymptotic distribution:</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Asymptotic Distribution of Sample Median</p>
<p>If <span class="math notranslate nohighlight">\(F\)</span> has a density <span class="math notranslate nohighlight">\(f\)</span> that is positive and continuous at <span class="math notranslate nohighlight">\(m = F^{-1}(0.5)\)</span>, then:</p>
<div class="math notranslate nohighlight" id="equation-median-asymptotic">
<span class="eqno">(2.150)<a class="headerlink" href="#equation-median-asymptotic" title="Link to this equation"></a></span>\[\sqrt{n}(\hat{m} - m) \xrightarrow{d} N\left(0, \frac{1}{4[f(m)]^2}\right)\]</div>
<p>The asymptotic standard error is:</p>
<div class="math notranslate nohighlight">
\[\text{SE}_{\text{asymp}}(\hat{m}) = \frac{1}{2f(m)\sqrt{n}}\]</div>
</div>
<p><strong>The Problem</strong></p>
<p>This formula requires knowing <span class="math notranslate nohighlight">\(f(m)\)</span>—the density evaluated at the median—which is unknown and difficult to estimate reliably:</p>
<ol class="arabic simple">
<li><p><strong>Density estimation is hard</strong>: Requires choosing a bandwidth, is sensitive to outliers, and has slow convergence rates</p></li>
<li><p><strong>Evaluation at</strong> <span class="math notranslate nohighlight">\(m\)</span> <strong>is even harder</strong>: We must estimate <span class="math notranslate nohighlight">\(f\)</span> at a specific point (the median), compounding estimation error</p></li>
<li><p><strong>The formula is sensitive</strong>: Small errors in <span class="math notranslate nohighlight">\(\hat{f}(m)\)</span> lead to large errors in <span class="math notranslate nohighlight">\(\widehat{\text{SE}}\)</span> because <span class="math notranslate nohighlight">\(f(m)\)</span> appears in the denominator</p></li>
</ol>
<p><strong>Bootstrap Solution</strong></p>
<p>The bootstrap sidesteps density estimation entirely:</p>
<ol class="arabic simple">
<li><p>Resample with replacement from the data</p></li>
<li><p>Compute the sample median for each resample</p></li>
<li><p>Use the empirical standard deviation of bootstrap medians as the SE estimate</p></li>
</ol>
<p>No density estimation required!</p>
<div class="note admonition">
<p class="admonition-title">Example 💡 Median Inference: Asymptotic vs. Bootstrap</p>
<p><strong>Setup</strong>: <span class="math notranslate nohighlight">\(n = 50\)</span> observations from a lognormal distribution (right-skewed).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">gaussian_kde</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># True population parameters</span>
<span class="n">mu_log</span><span class="p">,</span> <span class="n">sigma_log</span> <span class="o">=</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.8</span>
<span class="n">true_median</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">mu_log</span><span class="p">)</span>  <span class="c1"># Median of lognormal = exp(mu)</span>
<span class="n">true_density_at_median</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">lognorm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">true_median</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">sigma_log</span><span class="p">,</span>
                                            <span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">mu_log</span><span class="p">))</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">50</span>

<span class="c1"># Generate sample</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">lognormal</span><span class="p">(</span><span class="n">mu_log</span><span class="p">,</span> <span class="n">sigma_log</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">median_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MEDIAN INFERENCE: ASYMPTOTIC VS BOOTSTRAP&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">True median: </span><span class="si">{</span><span class="n">true_median</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample median: </span><span class="si">{</span><span class="n">median_hat</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True f(m): </span><span class="si">{</span><span class="n">true_density_at_median</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Asymptotic SE: requires density estimation</span>
<span class="c1"># Method 1: Kernel density estimation</span>
<span class="n">kde</span> <span class="o">=</span> <span class="n">gaussian_kde</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">f_hat_kde</span> <span class="o">=</span> <span class="n">kde</span><span class="p">(</span><span class="n">median_hat</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">se_asymp_kde</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">f_hat_kde</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>

<span class="c1"># Method 2: Using true density (oracle - not available in practice)</span>
<span class="n">se_asymp_oracle</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">true_density_at_median</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Asymptotic SE (KDE): </span><span class="si">{</span><span class="n">se_asymp_kde</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Asymptotic SE (oracle): </span><span class="si">{</span><span class="n">se_asymp_oracle</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Bootstrap SE</span>
<span class="n">B</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">median_boot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
                         <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">)])</span>
<span class="n">se_boot</span> <span class="o">=</span> <span class="n">median_boot</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bootstrap SE: </span><span class="si">{</span><span class="n">se_boot</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Confidence intervals</span>
<span class="n">z_crit</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.975</span><span class="p">)</span>
<span class="n">ci_asymp</span> <span class="o">=</span> <span class="p">(</span><span class="n">median_hat</span> <span class="o">-</span> <span class="n">z_crit</span><span class="o">*</span><span class="n">se_asymp_kde</span><span class="p">,</span> <span class="n">median_hat</span> <span class="o">+</span> <span class="n">z_crit</span><span class="o">*</span><span class="n">se_asymp_kde</span><span class="p">)</span>
<span class="n">ci_boot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">median_boot</span><span class="p">,</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">97.5</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">95% CI (asymptotic): [</span><span class="si">{</span><span class="n">ci_asymp</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_asymp</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% CI (bootstrap):  [</span><span class="si">{</span><span class="n">ci_boot</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_boot</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>

<span class="c1"># Check if bootstrap captures skewness</span>
<span class="n">boot_skewness</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">skew</span><span class="p">(</span><span class="n">median_boot</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Bootstrap distribution skewness: </span><span class="si">{</span><span class="n">boot_skewness</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;(Positive skew reflects the right-skewed population)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>============================================================
MEDIAN INFERENCE: ASYMPTOTIC VS BOOTSTRAP
============================================================

True median: 4.482
Sample median: 4.668
True f(m): 0.1265

Asymptotic SE (KDE): 0.6177
Asymptotic SE (oracle): 0.5597
Bootstrap SE: 0.6834

95% CI (asymptotic): [3.457, 5.878]
95% CI (bootstrap):  [3.551, 6.276]

Bootstrap distribution skewness: 0.634
(Positive skew reflects the right-skewed population)
</pre></div>
</div>
<p><strong>Key observations</strong>:</p>
<ol class="arabic simple">
<li><p>The KDE-based SE differs from the oracle SE by about 10%—density estimation introduces substantial uncertainty</p></li>
<li><p>The bootstrap SE is slightly larger, reflecting additional uncertainty the asymptotic formula misses</p></li>
<li><p>The bootstrap distribution is right-skewed (skewness = 0.63), capturing the population’s skewness that the symmetric normal CI ignores</p></li>
</ol>
</div>
<figure class="align-center" id="id6">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_1_fig03_median_inference.png"><img alt="Comparison of asymptotic and bootstrap inference for the sample median, showing bootstrap distribution with skewness versus symmetric normal approximation" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_1_fig03_median_inference.png" style="width: 90%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.97 </span><span class="caption-text"><strong>Figure 4.1.3</strong>: Median Inference—Asymptotic vs. Bootstrap. Left: The bootstrap distribution of the sample median captures the right-skewness inherited from the lognormal population. Right: The asymptotic normal approximation (red curve) assumes symmetry. The bootstrap percentile interval (shaded) is appropriately asymmetric, while the asymptotic interval (dashed lines) is symmetric around the estimate.</span><a class="headerlink" href="#id6" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="scenario-2-ratios-of-random-variables">
<h3>Scenario 2: Ratios of Random Variables<a class="headerlink" href="#scenario-2-ratios-of-random-variables" title="Link to this heading"></a></h3>
<p>Consider estimating the ratio of two population means: <span class="math notranslate nohighlight">\(\theta = \mu_X / \mu_Y\)</span>.</p>
<p><strong>The Delta Method Approach</strong></p>
<p>For independent samples, the delta method gives:</p>
<div class="math notranslate nohighlight">
\[\hat{\theta} = \frac{\bar{X}}{\bar{Y}}, \quad
\text{Var}(\hat{\theta}) \approx \hat{\theta}^2 \left(\frac{\sigma_X^2}{n_X \mu_X^2} + \frac{\sigma_Y^2}{n_Y \mu_Y^2}\right)\]</div>
<p>For paired data <span class="math notranslate nohighlight">\((X_i, Y_i)\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-ratio-delta-variance">
<span class="eqno">(2.151)<a class="headerlink" href="#equation-ratio-delta-variance" title="Link to this equation"></a></span>\[\text{Var}(\hat{\theta}) \approx \frac{1}{n\mu_Y^2}\left(\sigma_X^2 + \theta^2\sigma_Y^2 - 2\theta\sigma_{XY}\right)\]</div>
<p><strong>The Problems</strong></p>
<ol class="arabic simple">
<li><p><strong>Denominator variability</strong>: When <span class="math notranslate nohighlight">\(\bar{Y}\)</span> is variable (small <span class="math notranslate nohighlight">\(n_Y\)</span>, large <span class="math notranslate nohighlight">\(\sigma_Y\)</span>), <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> can be very unstable</p></li>
<li><p><strong>Near-zero denominators</strong>: If <span class="math notranslate nohighlight">\(\bar{Y} \approx 0\)</span> in some samples, the ratio explodes</p></li>
<li><p><strong>Non-normality</strong>: The ratio of normals is not normal (it follows a Cauchy-like distribution when means are near zero)</p></li>
<li><p><strong>Skewness</strong>: Even when well-behaved, ratios are typically right-skewed; symmetric CIs are inappropriate</p></li>
</ol>
<p><strong>Bootstrap Solution</strong></p>
<p>Resample paired observations <span class="math notranslate nohighlight">\((X_i, Y_i)\)</span>, compute <span class="math notranslate nohighlight">\(\bar{X}^*/\bar{Y}^*\)</span> for each resample, and use the bootstrap distribution directly. This automatically captures:</p>
<ul class="simple">
<li><p>The joint variability of numerator and denominator</p></li>
<li><p>The skewness of the ratio distribution</p></li>
<li><p>The instability when <span class="math notranslate nohighlight">\(\bar{Y}\)</span> is near zero</p></li>
</ul>
<figure class="align-center" id="id7">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_1_fig04_ratio_inference.png"><img alt="Bootstrap distribution of ratio estimator showing right-skewed distribution compared to symmetric delta method approximation" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_1_fig04_ratio_inference.png" style="width: 90%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.98 </span><span class="caption-text"><strong>Figure 4.1.4</strong>: Ratio Inference—Delta Method vs. Bootstrap. The bootstrap distribution (histogram) reveals the inherent right-skewness of the ratio <span class="math notranslate nohighlight">\(\bar{X}/\bar{Y}\)</span>. The delta method normal approximation (red curve) incorrectly assumes symmetry. When denominator variability is substantial, the delta method can severely underestimate the probability of extreme values.</span><a class="headerlink" href="#id7" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="scenario-3-small-samples-from-non-normal-populations">
<h3>Scenario 3: Small Samples from Non-Normal Populations<a class="headerlink" href="#scenario-3-small-samples-from-non-normal-populations" title="Link to this heading"></a></h3>
<p>Asymptotic theory describes behavior as <span class="math notranslate nohighlight">\(n \to \infty\)</span>. For small <span class="math notranslate nohighlight">\(n\)</span>, the CLT approximation may be poor, especially for:</p>
<ul class="simple">
<li><p><strong>Skewed populations</strong>: The symmetric normal approximation is inappropriate</p></li>
<li><p><strong>Heavy-tailed populations</strong>: The sample mean has higher variance than the CLT suggests</p></li>
<li><p><strong>Discrete populations</strong>: The approximation can be noticeably discrete for small <span class="math notranslate nohighlight">\(n\)</span></p></li>
</ul>
<p><strong>Theoretical Background: Berry-Esseen Bound</strong></p>
<p>The Berry-Esseen theorem quantifies the rate of convergence in the CLT:</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Berry-Esseen Bound</p>
<p>If <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span> are iid with <span class="math notranslate nohighlight">\(\mathbb{E}[X] = \mu\)</span>, <span class="math notranslate nohighlight">\(\text{Var}(X) = \sigma^2\)</span>, and <span class="math notranslate nohighlight">\(\mathbb{E}[|X - \mu|^3] = \rho &lt; \infty\)</span>, then:</p>
<div class="math notranslate nohighlight" id="equation-berry-esseen">
<span class="eqno">(2.152)<a class="headerlink" href="#equation-berry-esseen" title="Link to this equation"></a></span>\[\sup_t \left| P\left\{\frac{\sqrt{n}(\bar{X} - \mu)}{\sigma} \leq t\right\} - \Phi(t) \right| \leq \frac{C \cdot \rho}{\sigma^3 \sqrt{n}}\]</div>
<p>where <span class="math notranslate nohighlight">\(C &lt; 0.4748\)</span> (Shevtsova, 2011) and <span class="math notranslate nohighlight">\(\Phi\)</span> is the standard normal CDF.</p>
</div>
<p>The bound shows that convergence is <span class="math notranslate nohighlight">\(O(1/\sqrt{n})\)</span> and is slower when the population has high skewness (large <span class="math notranslate nohighlight">\(\rho/\sigma^3\)</span>).</p>
<p><strong>Simulation Study: Coverage of t-Intervals</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">coverage_study</span><span class="p">(</span><span class="n">distribution</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">n_values</span><span class="p">,</span> <span class="n">n_sim</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Study coverage of t-intervals for various sample sizes.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">n_values</span><span class="p">:</span>
        <span class="n">covers</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">true_mean</span> <span class="o">=</span> <span class="n">distribution</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">**</span><span class="n">params</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
            <span class="n">sample</span> <span class="o">=</span> <span class="n">distribution</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">rng</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
            <span class="n">xbar</span> <span class="o">=</span> <span class="n">sample</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
            <span class="n">se</span> <span class="o">=</span> <span class="n">sample</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
            <span class="n">t_crit</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">ci_lower</span> <span class="o">=</span> <span class="n">xbar</span> <span class="o">-</span> <span class="n">t_crit</span> <span class="o">*</span> <span class="n">se</span>
            <span class="n">ci_upper</span> <span class="o">=</span> <span class="n">xbar</span> <span class="o">+</span> <span class="n">t_crit</span> <span class="o">*</span> <span class="n">se</span>

            <span class="k">if</span> <span class="n">ci_lower</span> <span class="o">&lt;=</span> <span class="n">true_mean</span> <span class="o">&lt;=</span> <span class="n">ci_upper</span><span class="p">:</span>
                <span class="n">covers</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">results</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">covers</span> <span class="o">/</span> <span class="n">n_sim</span>

    <span class="k">return</span> <span class="n">results</span>

<span class="c1"># Test on exponential (skewed) and t_3 (heavy-tailed)</span>
<span class="n">n_values</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;T-INTERVAL COVERAGE BY SAMPLE SIZE&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Nominal coverage: 95%&quot;</span><span class="p">)</span>

<span class="c1"># Exponential(1): skewness = 2</span>
<span class="n">exp_coverage</span> <span class="o">=</span> <span class="n">coverage_study</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">expon</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;scale&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span> <span class="n">n_values</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Exponential(1) [skewness = 2]:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">cov</span> <span class="ow">in</span> <span class="n">exp_coverage</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">flag</span> <span class="o">=</span> <span class="s2">&quot; ⚠&quot;</span> <span class="k">if</span> <span class="n">cov</span> <span class="o">&lt;</span> <span class="mf">0.93</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  n = </span><span class="si">{</span><span class="n">n</span><span class="si">:</span><span class="s2">3d</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">cov</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}{</span><span class="n">flag</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Interpretation</strong>: For skewed (exponential) and heavy-tailed (t_3) populations, t-intervals undercover for small <span class="math notranslate nohighlight">\(n\)</span>. Coverage approaches 95% only for <span class="math notranslate nohighlight">\(n \geq 50-100\)</span>. Normal data achieves correct coverage even for <span class="math notranslate nohighlight">\(n = 10\)</span>.</p>
<figure class="align-center" id="id8">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_1_fig05_small_sample_heavy_tails.png"><img alt="Coverage probability of t-intervals across sample sizes for normal, exponential, and t(3) distributions showing undercoverage for non-normal populations at small n" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_1_fig05_small_sample_heavy_tails.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.99 </span><span class="caption-text"><strong>Figure 4.1.5</strong>: T-Interval Coverage Depends on Population Shape. Coverage probability (y-axis) versus sample size (x-axis) for three populations. The horizontal line marks nominal 95% coverage. Normal populations achieve correct coverage even at <span class="math notranslate nohighlight">\(n = 10\)</span>. Exponential (skewed) and <span class="math notranslate nohighlight">\(t_3\)</span> (heavy-tailed) populations show substantial undercoverage for <span class="math notranslate nohighlight">\(n &lt; 50\)</span>, with coverage gradually improving toward the nominal level as <span class="math notranslate nohighlight">\(n\)</span> increases.</span><a class="headerlink" href="#id8" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="scenario-4-boundary-and-extreme-value-statistics">
<h3>Scenario 4: Boundary and Extreme-Value Statistics<a class="headerlink" href="#scenario-4-boundary-and-extreme-value-statistics" title="Link to this heading"></a></h3>
<p>When parameters lie on or near the boundary of the parameter space, or when the statistic involves extreme values, standard asymptotic theory fails.</p>
<p><strong>Example: Sample Maximum</strong></p>
<p>For <span class="math notranslate nohighlight">\(\hat{\theta} = X_{(n)} = \max(X_1, \ldots, X_n)\)</span>:</p>
<div class="warning admonition">
<p class="admonition-title">The Maximum Is Non-Regular</p>
<p>The sample maximum has a <strong>non-standard</strong> asymptotic distribution. Under appropriate conditions:</p>
<div class="math notranslate nohighlight">
\[a_n(X_{(n)} - b_n) \xrightarrow{d} G\]</div>
<p>where <span class="math notranslate nohighlight">\(G\)</span> is a generalized extreme value (GEV) distribution—not normal! The normalizing constants <span class="math notranslate nohighlight">\(a_n, b_n\)</span> depend on the tail behavior of <span class="math notranslate nohighlight">\(F\)</span>.</p>
<p>The CLT does not apply to extreme-value statistics.</p>
</div>
<p><strong>Bootstrap Considerations for Extremes</strong></p>
<p>The nonparametric bootstrap can also struggle with extreme-value statistics because:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\hat{F}_n\)</span> cannot generate values outside the observed range</p></li>
<li><p>The bootstrap maximum <span class="math notranslate nohighlight">\(X_{(n)}^*\)</span> has a point mass at <span class="math notranslate nohighlight">\(X_{(n)}\)</span> (probability that the largest observation is resampled at least once)</p></li>
</ol>
<p>For extreme-value inference, specialized methods (parametric bootstrap with GEV family, subsampling, or m-out-of-n bootstrap) may be needed.</p>
</section>
<section id="a-decision-framework">
<h3>A Decision Framework<a class="headerlink" href="#a-decision-framework" title="Link to this heading"></a></h3>
<p>Given the analysis above, when should each approach be used?</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>START: Need SE or CI for theta_hat = T(X_1,...,X_n)

|
+--&gt; Is T a smooth function of sample moments?
|   |
|   +--&gt; YES: Is n large AND population well-behaved?
|   |   |
|   |   +--&gt; YES: Use ANALYTIC methods (CLT, delta method)
|   |   |
|   |   +--&gt; NO (small n, heavy tails): Use BOOTSTRAP
|   |
|   +--&gt; NO: T is non-smooth (median, quantile, etc.)
|       |
|       +--&gt; Use BOOTSTRAP
|
+--&gt; Is there a credible parametric model?
|   |
|   +--&gt; YES: Use PARAMETRIC BOOTSTRAP (good for tails)
|   |
|   +--&gt; NO: Use NONPARAMETRIC BOOTSTRAP
|
+--&gt; Are observations dependent?
|   |
|   +--&gt; Clustered: CLUSTER BOOTSTRAP
|   +--&gt; Time series: BLOCK BOOTSTRAP
|   +--&gt; Stratified: STRATIFIED BOOTSTRAP
|
+--&gt; Is theta_hat an extreme-value statistic?
    |
    +--&gt; Standard bootstrap may fail: Consider specialized methods
</pre></div>
</div>
<figure class="align-center" id="id9">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_1_fig10_decision_flowchart.png"><img alt="Decision flowchart for choosing between analytic, parametric bootstrap, and nonparametric bootstrap methods based on statistic type, sample size, and data structure" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_1_fig10_decision_flowchart.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.100 </span><span class="caption-text"><strong>Figure 4.1.10</strong>: Decision Framework for Choosing an Inference Method. This flowchart guides practitioners through key questions: Is the statistic smooth? Is <span class="math notranslate nohighlight">\(n\)</span> large? Is there a credible parametric model? Are observations independent? The answers determine whether analytic methods suffice or whether bootstrap (parametric or nonparametric) is needed, and which bootstrap variant is appropriate.</span><a class="headerlink" href="#id9" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="the-plug-in-principle-theoretical-foundation">
<h2>The Plug-In Principle: Theoretical Foundation<a class="headerlink" href="#the-plug-in-principle-theoretical-foundation" title="Link to this heading"></a></h2>
<p>The bootstrap is not a black box or ad-hoc method—it rests on a principled idea called the <strong>plug-in principle</strong>. Understanding this principle clarifies why the bootstrap works and when it might fail.</p>
<section id="statistical-functionals">
<h3>Statistical Functionals<a class="headerlink" href="#statistical-functionals" title="Link to this heading"></a></h3>
<p>Most population quantities of interest can be expressed as <strong>functionals</strong>—functions that take a probability distribution as input and return a number.</p>
<div class="note admonition">
<p class="admonition-title">Definition: Statistical Functional</p>
<p>A <strong>statistical functional</strong> is a map <span class="math notranslate nohighlight">\(T: \mathcal{F} \to \mathbb{R}\)</span> that assigns a real number to each distribution in some class <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>.</p>
<p>The <strong>parameter</strong> <span class="math notranslate nohighlight">\(\theta\)</span> is then <span class="math notranslate nohighlight">\(\theta = T(F)\)</span>.</p>
</div>
<p><strong>Examples of Statistical Functionals</strong>:</p>
<table class="docutils align-default" id="id10">
<caption><span class="caption-number">Table 2.25 </span><span class="caption-text">Common Parameters as Functionals</span><a class="headerlink" href="#id10" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 40.0%" />
<col style="width: 40.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Functional Notation</p></th>
<th class="head"><p>Definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Mean</p></td>
<td><p><span class="math notranslate nohighlight">\(T(F) = \int x \, dF(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbb{E}_F[X]\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Variance</p></td>
<td><p><span class="math notranslate nohighlight">\(T(F) = \int (x - \mu_F)^2 \, dF(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\text{Var}_F(X)\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Median</p></td>
<td><p><span class="math notranslate nohighlight">\(T(F) = F^{-1}(0.5)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\inf\{x : F(x) \geq 0.5\}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Correlation</p></td>
<td><p><span class="math notranslate nohighlight">\(T(F) = \frac{\text{Cov}_F(X,Y)}{\text{SD}_F(X) \cdot \text{SD}_F(Y)}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\text{Corr}_F(X, Y)\)</span></p></td>
</tr>
</tbody>
</table>
</section>
<section id="the-plug-in-estimator">
<h3>The Plug-In Estimator<a class="headerlink" href="#the-plug-in-estimator" title="Link to this heading"></a></h3>
<p>The <strong>plug-in principle</strong> provides a general recipe for estimation:</p>
<div class="note admonition">
<p class="admonition-title">The Plug-In Principle (Parameter Level)</p>
<p>To estimate a functional <span class="math notranslate nohighlight">\(\theta = T(F)\)</span>, substitute the empirical distribution <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> for <span class="math notranslate nohighlight">\(F\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-plugin-parameter">
<span class="eqno">(2.153)<a class="headerlink" href="#equation-plugin-parameter" title="Link to this equation"></a></span>\[\hat{\theta} = T(\hat{F}_n)\]</div>
</div>
<p>The empirical distribution function is:</p>
<div class="math notranslate nohighlight" id="equation-ecdf-formal">
<span class="eqno">(2.154)<a class="headerlink" href="#equation-ecdf-formal" title="Link to this equation"></a></span>\[\hat{F}_n(x) = \frac{1}{n} \sum_{i=1}^n \mathbf{1}\{X_i \leq x\}\]</div>
<p>This is the CDF of a discrete distribution placing mass <span class="math notranslate nohighlight">\(1/n\)</span> on each observed data point.</p>
<p><strong>Verifying Plug-In for Common Statistics</strong>:</p>
<p><strong>Sample mean as plug-in estimator of population mean</strong>:</p>
<div class="math notranslate nohighlight">
\[T(F) = \int x \, dF(x) \quad \Rightarrow \quad T(\hat{F}_n) = \int x \, d\hat{F}_n(x) = \frac{1}{n}\sum_{i=1}^n x_i = \bar{x}\]</div>
<p><strong>Sample variance as plug-in estimator</strong> (with divisor <span class="math notranslate nohighlight">\(n\)</span>):</p>
<div class="math notranslate nohighlight">
\[T(F) = \int (x - \mu_F)^2 \, dF(x) \quad \Rightarrow \quad T(\hat{F}_n) = \frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2\]</div>
</section>
<section id="extending-to-uncertainty-quantification">
<h3>Extending to Uncertainty Quantification<a class="headerlink" href="#extending-to-uncertainty-quantification" title="Link to this heading"></a></h3>
<p>The key insight of the bootstrap is to extend the plug-in principle from <strong>parameters</strong> to <strong>sampling distributions</strong>:</p>
<div class="note admonition">
<p class="admonition-title">The Plug-In Principle (Distribution Level)</p>
<p><strong>Parameter plug-in</strong>: The parameter <span class="math notranslate nohighlight">\(\theta = T(F)\)</span> is estimated by <span class="math notranslate nohighlight">\(\hat{\theta} = T(\hat{F}_n)\)</span>.</p>
<p><strong>Uncertainty plug-in</strong>: The sampling distribution <span class="math notranslate nohighlight">\(G_F\)</span> of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is estimated by <span class="math notranslate nohighlight">\(G_{\hat{F}_n}\)</span>.</p>
<p>Since we cannot draw new samples from the unknown <span class="math notranslate nohighlight">\(F\)</span>, we draw samples from <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>—which means sampling with replacement from the observed data.</p>
</div>
<p>The relationship is:</p>
<div class="math notranslate nohighlight">
\[\underbrace{G_{\hat{F}_n}}_{\text{bootstrap distribution}} \text{ approximates } \underbrace{G_F}_{\text{true sampling distribution}}\]</div>
<p>just as</p>
<div class="math notranslate nohighlight">
\[\underbrace{\hat{F}_n}_{\text{empirical distribution}} \text{ approximates } \underbrace{F}_{\text{true population}}\]</div>
<figure class="align-center" id="id11">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_1_fig06_plugin_principle.png"><img alt="Diagram illustrating the plug-in principle at both parameter and distribution levels, showing F to theta and F_hat to theta_hat parallels" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_1_fig06_plugin_principle.png" style="width: 85%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.101 </span><span class="caption-text"><strong>Figure 4.1.6</strong>: The Plug-In Principle at Two Levels. Top row: Parameter estimation—the unknown <span class="math notranslate nohighlight">\(F\)</span> yields parameter <span class="math notranslate nohighlight">\(\theta = T(F)\)</span>, which we estimate by <span class="math notranslate nohighlight">\(\hat{\theta} = T(\hat{F}_n)\)</span> using the empirical distribution. Bottom row: Uncertainty estimation—the unknown sampling distribution <span class="math notranslate nohighlight">\(G_F\)</span> is approximated by <span class="math notranslate nohighlight">\(G_{\hat{F}_n}\)</span>, obtained by resampling from <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>. Both levels rest on the same principle: substitute <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> for <span class="math notranslate nohighlight">\(F\)</span>.</span><a class="headerlink" href="#id11" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Why This Should Work (Informal Argument)</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\hat{F}_n \to F\)</span> as <span class="math notranslate nohighlight">\(n \to \infty\)</span> (Glivenko-Cantelli theorem)</p></li>
<li><p>If <span class="math notranslate nohighlight">\(G\)</span> depends “continuously” on <span class="math notranslate nohighlight">\(F\)</span>, then <span class="math notranslate nohighlight">\(G_{\hat{F}_n} \to G_F\)</span></p></li>
<li><p>We can compute <span class="math notranslate nohighlight">\(G_{\hat{F}_n}\)</span> by Monte Carlo (resampling from <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>)</p></li>
</ol>
<p>The formal development of when this works is the subject of <span class="xref std std-ref">Section 4.2</span>.</p>
</section>
<section id="theoretical-justification-preview">
<h3>Theoretical Justification: Preview<a class="headerlink" href="#theoretical-justification-preview" title="Link to this heading"></a></h3>
<p>The rigorous justification for the bootstrap involves several deep results from probability theory, which we develop fully in Section 4.2. Here we preview the key ideas.</p>
<p><strong>Glivenko-Cantelli Theorem</strong>: The empirical distribution converges uniformly to the true distribution.</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Glivenko-Cantelli (Preview)</p>
<p>With probability 1:</p>
<div class="math notranslate nohighlight" id="equation-gc-preview">
<span class="eqno">(2.155)<a class="headerlink" href="#equation-gc-preview" title="Link to this equation"></a></span>\[\|\hat{F}_n - F\|_\infty = \sup_x \left| \hat{F}_n(x) - F(x) \right| \xrightarrow{a.s.} 0 \quad \text{as } n \to \infty\]</div>
</div>
<p><strong>Bootstrap Consistency</strong>: Under regularity conditions, for a broad class of statistics:</p>
<div class="math notranslate nohighlight">
\[\sup_t |G_{\hat{F}_n}(t) - G_F(t)| \xrightarrow{P} 0\]</div>
<p>The bootstrap distribution converges to the true sampling distribution in probability.</p>
<p><strong>When the Bootstrap Fails</strong></p>
<p>The bootstrap can fail when:</p>
<ol class="arabic simple">
<li><p><strong>Sample size too small</strong>: <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> is too coarse an approximation to <span class="math notranslate nohighlight">\(F\)</span></p></li>
<li><p><strong>Functional not continuous</strong>: Extreme-value statistics, for example</p></li>
<li><p><strong>Tail dependence</strong>: Bootstrap cannot generate values outside observed range</p></li>
<li><p><strong>Dependence structure</strong>: Standard bootstrap assumes iid</p></li>
</ol>
</section>
</section>
<section id="computational-perspective-bootstrap-as-monte-carlo">
<h2>Computational Perspective: Bootstrap as Monte Carlo<a class="headerlink" href="#computational-perspective-bootstrap-as-monte-carlo" title="Link to this heading"></a></h2>
<p>From a computational standpoint, the bootstrap is simply <strong>Monte Carlo integration</strong> where the target distribution is <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> rather than some theoretical <span class="math notranslate nohighlight">\(F\)</span>.</p>
<figure class="align-center" id="id12">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_1_fig07_bootstrap_algorithm.png"><img alt="Step-by-step visualization of the bootstrap algorithm showing original sample, resampling with replacement, computing statistics, and building the bootstrap distribution" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_1_fig07_bootstrap_algorithm.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.102 </span><span class="caption-text"><strong>Figure 4.1.7</strong>: The Bootstrap Algorithm Visualized. Starting from the original sample (left), we repeatedly resample with replacement to create bootstrap samples (middle). For each bootstrap sample, we compute the statistic of interest. After <span class="math notranslate nohighlight">\(B\)</span> replications, the collection of bootstrap statistics forms the bootstrap distribution (right), which approximates the true sampling distribution <span class="math notranslate nohighlight">\(G\)</span>.</span><a class="headerlink" href="#id12" title="Link to this image"></a></p>
</figcaption>
</figure>
<section id="two-layers-of-randomness">
<h3>Two Layers of Randomness<a class="headerlink" href="#two-layers-of-randomness" title="Link to this heading"></a></h3>
<p>A crucial point for understanding and reporting bootstrap results: the bootstrap involves <strong>two distinct sources of randomness</strong>:</p>
<div class="note admonition">
<p class="admonition-title">Two Sources of Uncertainty</p>
<p><strong>1. Statistical Uncertainty</strong> (what we want to estimate)</p>
<ul class="simple">
<li><p><strong>Source</strong>: Finite sample size <span class="math notranslate nohighlight">\(n\)</span>; different samples from <span class="math notranslate nohighlight">\(F\)</span> give different <span class="math notranslate nohighlight">\(\hat{\theta}\)</span></p></li>
<li><p><strong>Quantified by</strong>: The true standard error <span class="math notranslate nohighlight">\(\text{SE}(\hat{\theta}) = \sqrt{\text{Var}_F(\hat{\theta})}\)</span></p></li>
<li><p><strong>Controlled by</strong>: Collecting more data (larger <span class="math notranslate nohighlight">\(n\)</span>)</p></li>
<li><p><strong>Magnitude</strong>: Typically <span class="math notranslate nohighlight">\(O(1/\sqrt{n})\)</span></p></li>
</ul>
<p><strong>2. Monte Carlo Uncertainty</strong> (computational noise)</p>
<ul class="simple">
<li><p><strong>Source</strong>: Finite number of bootstrap replicates <span class="math notranslate nohighlight">\(B\)</span></p></li>
<li><p><strong>Quantified by</strong>: The Monte Carlo standard error of <span class="math notranslate nohighlight">\(\widehat{\text{SE}}_{\text{boot}}\)</span></p></li>
<li><p><strong>Controlled by</strong>: Increasing <span class="math notranslate nohighlight">\(B\)</span></p></li>
<li><p><strong>Magnitude</strong>: Typically <span class="math notranslate nohighlight">\(O(1/\sqrt{B})\)</span></p></li>
</ul>
</div>
<p><strong>Monte Carlo Error of the SE Estimate</strong></p>
<p>The bootstrap SE estimate has its own variability:</p>
<div class="math notranslate nohighlight" id="equation-mc-error-se">
<span class="eqno">(2.156)<a class="headerlink" href="#equation-mc-error-se" title="Link to this equation"></a></span>\[\text{SE}(\widehat{\text{SE}}_{\text{boot}}) \approx \frac{\widehat{\text{SE}}_{\text{boot}}}{\sqrt{2(B-1)}}\]</div>
<p>For <span class="math notranslate nohighlight">\(B = 1000\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{Coefficient of Variation} = \frac{1}{\sqrt{2 \times 999}} \approx 2.2\%\]</div>
<p>For <span class="math notranslate nohighlight">\(B = 10000\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{Coefficient of Variation} = \frac{1}{\sqrt{2 \times 9999}} \approx 0.7\%\]</div>
<figure class="align-center" id="id13">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_1_fig08_two_layers_randomness.png"><img alt="Diagram distinguishing statistical uncertainty from finite n versus Monte Carlo uncertainty from finite B, showing how each is controlled" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_1_fig08_two_layers_randomness.png" style="width: 90%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.103 </span><span class="caption-text"><strong>Figure 4.1.8</strong>: Two Layers of Randomness in Bootstrap Inference. Statistical uncertainty (left) arises from observing a finite sample <span class="math notranslate nohighlight">\(n\)</span> from the population—this is what we want to estimate. Monte Carlo uncertainty (right) arises from using finite bootstrap replicates <span class="math notranslate nohighlight">\(B\)</span>—this is computational noise we can reduce. Increasing <span class="math notranslate nohighlight">\(B\)</span> reduces Monte Carlo error but cannot reduce statistical uncertainty; only collecting more data (larger <span class="math notranslate nohighlight">\(n\)</span>) addresses statistical uncertainty.</span><a class="headerlink" href="#id13" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Choosing B</strong></p>
<p>Guidelines based on the target:</p>
<table class="docutils align-default" id="id14">
<caption><span class="caption-number">Table 2.26 </span><span class="caption-text">Guidelines for Choosing B</span><a class="headerlink" href="#id14" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 35.0%" />
<col style="width: 20.0%" />
<col style="width: 45.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Target</p></th>
<th class="head"><p>Recommended B</p></th>
<th class="head"><p>Rationale</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Standard error estimation</p></td>
<td><p>200–1,000</p></td>
<td><p>SE estimate has low MC variance</p></td>
</tr>
<tr class="row-odd"><td><p>Percentile CI (95%)</p></td>
<td><p>1,000–2,000</p></td>
<td><p>Need accurate 2.5th and 97.5th percentiles</p></td>
</tr>
<tr class="row-even"><td><p>Percentile CI (99%)</p></td>
<td><p>5,000–10,000</p></td>
<td><p>More extreme percentiles need more samples</p></td>
</tr>
<tr class="row-odd"><td><p>BCa intervals</p></td>
<td><p>2,000–10,000</p></td>
<td><p>Acceleration constant estimation</p></td>
</tr>
<tr class="row-even"><td><p>Publication-quality results</p></td>
<td><p>10,000+</p></td>
<td><p>Minimize MC variability in reported values</p></td>
</tr>
</tbody>
</table>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ⚠️ Confusing the Two Sources of Uncertainty</p>
<p><strong>Misconception</strong>: “I ran the bootstrap with <span class="math notranslate nohighlight">\(B = 100,000\)</span> replicates, so my standard error must be very accurate.”</p>
<p><strong>Reality</strong>: Large <span class="math notranslate nohighlight">\(B\)</span> reduces <strong>Monte Carlo uncertainty</strong> but does not reduce <strong>statistical uncertainty</strong>. If <span class="math notranslate nohighlight">\(n\)</span> is small and <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> is a poor approximation to <span class="math notranslate nohighlight">\(F\)</span>, the bootstrap estimate of SE may be biased regardless of <span class="math notranslate nohighlight">\(B\)</span>.</p>
<p><strong>Rule of Thumb</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(B\)</span> controls Monte Carlo precision (computational noise)</p></li>
<li><p><span class="math notranslate nohighlight">\(n\)</span> controls statistical accuracy (how well <span class="math notranslate nohighlight">\(\hat{F}_n \approx F\)</span>)</p></li>
<li><p>Increasing <span class="math notranslate nohighlight">\(B\)</span> beyond ~10,000 rarely helps; collecting more data (larger <span class="math notranslate nohighlight">\(n\)</span>) does</p></li>
</ul>
</div>
</section>
</section>
<section id="practical-considerations">
<h2>Practical Considerations<a class="headerlink" href="#practical-considerations" title="Link to this heading"></a></h2>
<p>Before implementing bootstrap methods, practitioners should understand several practical issues that affect reliability and interpretation.</p>
<section id="sample-size-requirements">
<h3>Sample Size Requirements<a class="headerlink" href="#sample-size-requirements" title="Link to this heading"></a></h3>
<p>The bootstrap assumes <span class="math notranslate nohighlight">\(\hat{F}_n \approx F\)</span>, which requires <span class="math notranslate nohighlight">\(n\)</span> to be large enough that the sample adequately represents the population.</p>
<p><strong>General Guidelines</strong>:</p>
<table class="docutils align-default" id="id15">
<caption><span class="caption-number">Table 2.27 </span><span class="caption-text">Minimum Sample Sizes for Bootstrap</span><a class="headerlink" href="#id15" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 40.0%" />
<col style="width: 20.0%" />
<col style="width: 40.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Statistic Type</p></th>
<th class="head"><p>Minimum n</p></th>
<th class="head"><p>Notes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Smooth (mean, regression)</p></td>
<td><p>15–20</p></td>
<td><p>Works well for most distributions</p></td>
</tr>
<tr class="row-odd"><td><p>Quantiles (median, quartiles)</p></td>
<td><p>30–50</p></td>
<td><p>More for extreme quantiles</p></td>
</tr>
<tr class="row-even"><td><p>Correlation, covariance</p></td>
<td><p>25–30</p></td>
<td><p>Need bivariate structure captured</p></td>
</tr>
<tr class="row-odd"><td><p>Ratios</p></td>
<td><p>30+</p></td>
<td><p>Denominator variability matters</p></td>
</tr>
<tr class="row-even"><td><p>Multivariate (p parameters)</p></td>
<td><p>5p–10p</p></td>
<td><p>Rule of thumb: several times # parameters</p></td>
</tr>
</tbody>
</table>
</section>
<section id="when-the-bootstrap-may-fail">
<h3>When the Bootstrap May Fail<a class="headerlink" href="#when-the-bootstrap-may-fail" title="Link to this heading"></a></h3>
<p>The bootstrap is not universally applicable. Key failure modes include:</p>
<p><strong>1. Extreme-Value Statistics</strong></p>
<p>The sample maximum <span class="math notranslate nohighlight">\(X_{(n)}\)</span> or minimum <span class="math notranslate nohighlight">\(X_{(1)}\)</span> have non-regular behavior. Bootstrap distribution places positive probability mass at the observed extremes.</p>
<p><strong>2. Boundary Parameters</strong></p>
<p>When <span class="math notranslate nohighlight">\(\theta\)</span> is near or at the boundary of parameter space (proportions near 0 or 1, variance near 0, correlation near ±1).</p>
<p><strong>3. Heavy Tails with Infinite Variance</strong></p>
<p>If <span class="math notranslate nohighlight">\(\text{Var}_F(X) = \infty\)</span> (e.g., Cauchy distribution), the bootstrap for <span class="math notranslate nohighlight">\(\bar{X}\)</span> may not converge correctly.</p>
<p><strong>4. Dependent Data</strong></p>
<p>The standard bootstrap assumes iid observations. For time series, clustered data, or spatial data, specialized bootstrap methods are required.</p>
</section>
</section>
<section id="bringing-it-all-together">
<h2>Bringing It All Together<a class="headerlink" href="#bringing-it-all-together" title="Link to this heading"></a></h2>
<p>This section has established the conceptual and mathematical foundations for resampling methods. The key developments are:</p>
<p><strong>The Fundamental Target</strong></p>
<p>The sampling distribution <span class="math notranslate nohighlight">\(G(t) = P_F\{\hat{\theta} \leq t\}\)</span> is the fundamental target of uncertainty quantification. Everything we want to know about estimator uncertainty—standard errors, confidence intervals, p-values, bias—is a functional of <span class="math notranslate nohighlight">\(G\)</span>.</p>
<p><strong>Three Routes to G</strong></p>
<p>We identified three canonical approaches:</p>
<ol class="arabic simple">
<li><p><strong>Analytic/Asymptotic</strong>: Derive <span class="math notranslate nohighlight">\(G\)</span> using probability theory. Fast and assumption-rich; works well for smooth statistics, large samples, correct model specification.</p></li>
<li><p><strong>Parametric Monte Carlo</strong>: Simulate from a fitted parametric model <span class="math notranslate nohighlight">\(F_{\hat{\eta}}\)</span>. Flexible and can extrapolate into tails. Carries the risk of model misspecification.</p></li>
<li><p><strong>Nonparametric Bootstrap</strong>: Simulate from the empirical distribution <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>. Model-free, automatic, broadly applicable. Cannot extrapolate beyond observed data.</p></li>
</ol>
<p><strong>When Classical Methods Fail</strong></p>
<p>We documented scenarios where asymptotics are unreliable: non-smooth statistics, ratios, small samples from non-normal populations, and extreme-value statistics.</p>
<p><strong>The Plug-In Principle</strong></p>
<p>The bootstrap rests on extending plug-in estimation from parameters to distributions:</p>
<ul class="simple">
<li><p>Parameter level: <span class="math notranslate nohighlight">\(\theta = T(F) \to \hat{\theta} = T(\hat{F}_n)\)</span></p></li>
<li><p>Distribution level: <span class="math notranslate nohighlight">\(G_F \to G_{\hat{F}_n}\)</span></p></li>
</ul>
<p><strong>Looking Ahead</strong></p>
<p><span class="xref std std-ref">Section 4.2</span> develops the empirical distribution and plug-in principle rigorously, proving Glivenko-Cantelli and the DKW inequality. Subsequent sections develop confidence interval methods, diagnostics, and specialized variants for regression, dependent data, and hypothesis testing.</p>
<div class="important admonition">
<p class="admonition-title">Key Takeaways 📝</p>
<ol class="arabic simple">
<li><p><strong>Core concept</strong>: The sampling distribution <span class="math notranslate nohighlight">\(G(t) = P_F\{\hat{\theta} \leq t\}\)</span> is the fundamental target of all uncertainty quantification. Standard errors, confidence intervals, p-values, and bias are all functionals of <span class="math notranslate nohighlight">\(G\)</span>.</p></li>
<li><p><strong>Three routes</strong>: Analytic methods derive <span class="math notranslate nohighlight">\(G\)</span> from theory (fast but assumption-heavy); parametric Monte Carlo simulates from a fitted model (flexible but model-dependent); bootstrap simulates from <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> (model-free but data-limited).</p></li>
<li><p><strong>When asymptotics fail</strong>: Non-smooth statistics (median), ratios, small samples from non-normal populations, and boundary parameters all cause classical asymptotic methods to perform poorly. The bootstrap provides a robust alternative.</p></li>
<li><p><strong>Plug-in principle</strong>: The bootstrap extends plug-in estimation from parameters (<span class="math notranslate nohighlight">\(\theta = T(F) \to T(\hat{F}_n)\)</span>) to sampling distributions (<span class="math notranslate nohighlight">\(G_F \to G_{\hat{F}_n}\)</span>). This is justified by the consistency of <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> for <span class="math notranslate nohighlight">\(F\)</span>.</p></li>
<li><p><strong>Two sources of uncertainty</strong>: Statistical uncertainty (from finite <span class="math notranslate nohighlight">\(n\)</span>) is what we want to estimate. Monte Carlo uncertainty (from finite <span class="math notranslate nohighlight">\(B\)</span>) is computational noise we can reduce by increasing <span class="math notranslate nohighlight">\(B\)</span>.</p></li>
<li><p><strong>Course outcome alignment</strong>: This section addresses Learning Outcome 1 (simulation techniques) by framing the bootstrap as Monte Carlo on <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>, and Learning Outcome 3 (resampling for variability and CIs) by establishing the conceptual foundation for all resampling inference.</p></li>
</ol>
</div>
</section>
<section id="chapter-4-1-exercises">
<h2>Chapter 4.1 Exercises<a class="headerlink" href="#chapter-4-1-exercises" title="Link to this heading"></a></h2>
<p>These exercises develop your understanding of the sampling distribution problem and the foundations of resampling methods.</p>
<div class="tip admonition">
<p class="admonition-title">Exercise 1: Fundamentals of Sampling Distributions</p>
<ol class="loweralpha simple">
<li><p>Explain why the sampling distribution <span class="math notranslate nohighlight">\(G\)</span> is called “the fundamental target of uncertainty quantification.” Identify which aspects of inference derive from <span class="math notranslate nohighlight">\(G\)</span>.</p></li>
<li><p>Suppose <span class="math notranslate nohighlight">\(X_1, \ldots, X_{25} \stackrel{\text{iid}}{\sim} \text{Exp}(1)\)</span>. Let <span class="math notranslate nohighlight">\(\hat{\theta} = \bar{X}\)</span>. What are the exact mean and variance of the sampling distribution of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>?</p></li>
<li><p>A colleague claims “the standard error is a property of the sample.” Correct this misconception.</p></li>
<li><p>If we observe <span class="math notranslate nohighlight">\(\bar{x} = 1.15\)</span> from our sample of 25, explain what “95% coverage probability” means for a confidence interval.</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a)</strong>: The sampling distribution <span class="math notranslate nohighlight">\(G(t) = P_F\{\hat{\theta} \leq t\}\)</span> is fundamental because all uncertainty measures derive from it:</p>
<ul class="simple">
<li><p class="sd-card-text">Bias: <span class="math notranslate nohighlight">\(\text{Bias}(\hat{\theta}) = \mathbb{E}_G[\hat{\theta}] - \theta\)</span></p></li>
<li><p class="sd-card-text">Variance/SE: <span class="math notranslate nohighlight">\(\text{Var}(\hat{\theta}) = \text{Var}_G[\hat{\theta}]\)</span></p></li>
<li><p class="sd-card-text">Confidence intervals: Coverage computed with respect to <span class="math notranslate nohighlight">\(G\)</span></p></li>
<li><p class="sd-card-text">P-values: Tail probabilities under <span class="math notranslate nohighlight">\(G\)</span> (or <span class="math notranslate nohighlight">\(G\)</span> under null)</p></li>
</ul>
<p class="sd-card-text"><strong>Part (b)</strong>: For <span class="math notranslate nohighlight">\(X_i \stackrel{\text{iid}}{\sim} \text{Exp}(1)\)</span> with mean 1 and variance 1:</p>
<ul class="simple">
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(\mathbb{E}[\bar{X}] = 1\)</span></p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(\text{Var}(\bar{X}) = 1/25 = 0.04\)</span></p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(\text{SE}(\bar{X}) = 0.2\)</span></p></li>
</ul>
<p class="sd-card-text">The sampling distribution is Gamma(25, 1/25), right-skewed.</p>
<p class="sd-card-text"><strong>Part (c)</strong>: The standard error is the standard deviation of the <strong>sampling distribution</strong>, not of the sample. It quantifies variability of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> across hypothetical repeated samples. We can only estimate SE from a single sample.</p>
<p class="sd-card-text"><strong>Part (d)</strong>: The 95% coverage means: if we repeated the sampling process infinitely many times, each time computing an interval <span class="math notranslate nohighlight">\([L, U]\)</span>, then 95% of these intervals would contain the true <span class="math notranslate nohighlight">\(\theta = 1\)</span>. The probability is over the sampling distribution of <span class="math notranslate nohighlight">\((L, U)\)</span>, not over <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
</div>
</details></div>
<div class="tip admonition">
<p class="admonition-title">Exercise 2: Comparing the Three Routes</p>
<p>For each scenario, identify which route to the sampling distribution would be most appropriate:</p>
<ol class="loweralpha simple">
<li><p>Estimating the mean of a normal population with <span class="math notranslate nohighlight">\(n = 100\)</span>, <span class="math notranslate nohighlight">\(\sigma\)</span> unknown</p></li>
<li><p>Estimating the sample median from <span class="math notranslate nohighlight">\(n = 50\)</span> observations from an unknown heavy-tailed distribution</p></li>
<li><p>Estimating the ratio <span class="math notranslate nohighlight">\(\mu_X / \mu_Y\)</span> from paired data with <span class="math notranslate nohighlight">\(n = 40\)</span></p></li>
<li><p>Estimating the 99th percentile of a lognormal distribution from <span class="math notranslate nohighlight">\(n = 500\)</span></p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a)</strong>: <strong>Analytic (t-interval)</strong> — Large <span class="math notranslate nohighlight">\(n\)</span>, known family, t-distribution is exact/near-exact.</p>
<p class="sd-card-text"><strong>Part (b)</strong>: <strong>Nonparametric Bootstrap</strong> — Median is non-smooth; asymptotic SE requires density estimation; bootstrap avoids this.</p>
<p class="sd-card-text"><strong>Part (c)</strong>: <strong>Nonparametric Bootstrap</strong> — Ratios have skewed, non-normal distributions; delta method may be poor.</p>
<p class="sd-card-text"><strong>Part (d)</strong>: <strong>Parametric Bootstrap</strong> — Extreme quantiles (99th) benefit from parametric extrapolation; nonparametric bootstrap cannot generate values beyond observed max.</p>
</div>
</details></div>
<div class="tip admonition">
<p class="admonition-title">Exercise 3: Monte Carlo vs. Statistical Uncertainty</p>
<p>A colleague says “I computed a bootstrap SE with <span class="math notranslate nohighlight">\(B = 50,000\)</span> replicates, so it must be very precise.”</p>
<ol class="loweralpha simple">
<li><p>Is this reasoning correct? Explain the distinction between Monte Carlo error and statistical uncertainty.</p></li>
<li><p>For <span class="math notranslate nohighlight">\(B = 50,000\)</span>, what is the approximate Monte Carlo coefficient of variation of the SE estimate?</p></li>
<li><p>If <span class="math notranslate nohighlight">\(n = 20\)</span> and the true SE is 0.5, would increasing <span class="math notranslate nohighlight">\(B\)</span> from 10,000 to 100,000 meaningfully improve inference?</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a)</strong>: Partially correct. Large <span class="math notranslate nohighlight">\(B\)</span> reduces <strong>Monte Carlo uncertainty</strong> (computational noise). It does NOT reduce <strong>statistical uncertainty</strong> (from finite <span class="math notranslate nohighlight">\(n\)</span>). If <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> is a poor approximation to <span class="math notranslate nohighlight">\(F\)</span>, the bootstrap may be biased regardless of <span class="math notranslate nohighlight">\(B\)</span>.</p>
<p class="sd-card-text"><strong>Part (b)</strong>:</p>
<div class="math notranslate nohighlight">
\[\text{CV} \approx \frac{1}{\sqrt{2B}} = \frac{1}{\sqrt{100,000}} \approx 0.3\%\]</div>
<p class="sd-card-text">Monte Carlo error is negligible.</p>
<p class="sd-card-text"><strong>Part (c)</strong>: <strong>No</strong>. At <span class="math notranslate nohighlight">\(B = 10,000\)</span>, CV ≈ 0.7%. At <span class="math notranslate nohighlight">\(B = 100,000\)</span>, CV ≈ 0.2%. Both are negligible compared to statistical uncertainty from <span class="math notranslate nohighlight">\(n = 20\)</span>. To improve inference, collect more data.</p>
</div>
</details></div>
<div class="tip admonition">
<p class="admonition-title">Exercise 4: Plug-In Estimators</p>
<ol class="loweralpha simple">
<li><p>Show that the sample mean <span class="math notranslate nohighlight">\(\bar{X}\)</span> is the plug-in estimator of <span class="math notranslate nohighlight">\(\mu = \int x \, dF(x)\)</span>.</p></li>
<li><p>Show that the plug-in estimator of variance has divisor <span class="math notranslate nohighlight">\(n\)</span>, not <span class="math notranslate nohighlight">\(n-1\)</span>.</p></li>
<li><p>Explain why plug-in estimators are generally biased. Give an example.</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a)</strong>:</p>
<div class="math notranslate nohighlight">
\[T(\hat{F}_n) = \int x \, d\hat{F}_n(x) = \sum_{i=1}^n x_i \cdot \frac{1}{n} = \bar{x}\]</div>
<p class="sd-card-text"><strong>Part (b)</strong>:</p>
<div class="math notranslate nohighlight">
\[T(\hat{F}_n) = \int (x - \bar{x})^2 \, d\hat{F}_n(x) = \frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2\]</div>
<p class="sd-card-text">This is the MLE, not the unbiased estimator.</p>
<p class="sd-card-text"><strong>Part (c)</strong>: Plug-in estimators substitute <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> for <span class="math notranslate nohighlight">\(F\)</span>. For nonlinear functionals, Jensen’s inequality implies <span class="math notranslate nohighlight">\(\mathbb{E}[T(\hat{F}_n)] \neq T(F)\)</span>.</p>
<p class="sd-card-text">Example: Plug-in variance has <span class="math notranslate nohighlight">\(\mathbb{E}[\hat{\sigma}^2] = \frac{n-1}{n}\sigma^2\)</span>, biased by <span class="math notranslate nohighlight">\(-\sigma^2/n\)</span>.</p>
</div>
</details></div>
<div class="tip admonition">
<p class="admonition-title">Exercise 5: Coverage Simulation</p>
<p>Conduct a simulation comparing t-interval and bootstrap percentile CI coverage for the mean.</p>
<ol class="loweralpha simple">
<li><p>Test on: Exponential(1), t(3), and Normal(0,1)</p></li>
<li><p>Use sample sizes <span class="math notranslate nohighlight">\(n \in \{10, 25, 50, 100\}\)</span></p></li>
<li><p>For which distributions and sample sizes does the t-interval undercover?</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">coverage_study</span><span class="p">(</span><span class="n">dist</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">n_values</span><span class="p">,</span> <span class="n">n_sim</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span> <span class="n">B</span><span class="o">=</span><span class="mi">2000</span><span class="p">):</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">n_values</span><span class="p">:</span>
        <span class="n">t_covers</span> <span class="o">=</span> <span class="n">boot_covers</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">true_mean</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">**</span><span class="n">params</span><span class="p">)</span>
        <span class="n">t_crit</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
            <span class="n">sample</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">rng</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
            <span class="n">xbar</span> <span class="o">=</span> <span class="n">sample</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
            <span class="n">se</span> <span class="o">=</span> <span class="n">sample</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

            <span class="c1"># t-interval</span>
            <span class="k">if</span> <span class="n">xbar</span> <span class="o">-</span> <span class="n">t_crit</span><span class="o">*</span><span class="n">se</span> <span class="o">&lt;=</span> <span class="n">true_mean</span> <span class="o">&lt;=</span> <span class="n">xbar</span> <span class="o">+</span> <span class="n">t_crit</span><span class="o">*</span><span class="n">se</span><span class="p">:</span>
                <span class="n">t_covers</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># Bootstrap</span>
            <span class="n">boot_means</span> <span class="o">=</span> <span class="p">[</span><span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
                          <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">)]</span>
            <span class="n">ci</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">boot_means</span><span class="p">,</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">97.5</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">ci</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">true_mean</span> <span class="o">&lt;=</span> <span class="n">ci</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                <span class="n">boot_covers</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">results</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">t_covers</span><span class="o">/</span><span class="n">n_sim</span><span class="p">,</span> <span class="n">boot_covers</span><span class="o">/</span><span class="n">n_sim</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">results</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Results</strong>: Exponential and t(3) undercover for small <span class="math notranslate nohighlight">\(n\)</span> (below 92-93%). Normal achieves ~95% for all <span class="math notranslate nohighlight">\(n\)</span>. Both methods perform similarly for the mean—bootstrap advantages emerge for non-smooth statistics.</p>
</div>
</details></div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<p><strong>Foundational Bootstrap Papers</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="efron1979" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Efron1979<span class="fn-bracket">]</span></span>
<p>Efron, B. (1979). Bootstrap methods: Another look at the jackknife. <em>The Annals of Statistics</em>, 7(1), 1–26.</p>
</div>
<div class="citation" id="efrontibshirani1993" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>EfronTibshirani1993<span class="fn-bracket">]</span></span>
<p>Efron, B., and Tibshirani, R. J. (1993). <em>An Introduction to the Bootstrap</em>. Chapman &amp; Hall/CRC.</p>
</div>
</div>
<p><strong>Theoretical Foundations</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="hall1992" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Hall1992<span class="fn-bracket">]</span></span>
<p>Hall, P. (1992). <em>The Bootstrap and Edgeworth Expansion</em>. Springer.</p>
</div>
<div class="citation" id="vandervaart1998" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>VanDerVaart1998<span class="fn-bracket">]</span></span>
<p>van der Vaart, A. W. (1998). <em>Asymptotic Statistics</em>. Cambridge University Press.</p>
</div>
</div>
<p><strong>Convergence Results</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="glivenko1933" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Glivenko1933<span class="fn-bracket">]</span></span>
<p>Glivenko, V. (1933). Sulla determinazione empirica delle leggi di probabilità. <em>Giornale dell’Istituto Italiano degli Attuari</em>, 4, 92–99.</p>
</div>
<div class="citation" id="dvoretzky1956" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Dvoretzky1956<span class="fn-bracket">]</span></span>
<p>Dvoretzky, A., Kiefer, J., and Wolfowitz, J. (1956). Asymptotic minimax character of the sample distribution function. <em>The Annals of Mathematical Statistics</em>, 27(3), 642–669.</p>
</div>
</div>
<p><strong>Modern References</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="efronhastie2016" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>EfronHastie2016<span class="fn-bracket">]</span></span>
<p>Efron, B., and Hastie, T. (2016). <em>Computer Age Statistical Inference</em>. Cambridge University Press.</p>
</div>
<div class="citation" id="davison1997" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Davison1997<span class="fn-bracket">]</span></span>
<p>Davison, A. C., and Hinkley, D. V. (1997). <em>Bootstrap Methods and their Application</em>. Cambridge University Press.</p>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="2.3. Chapter 4: Resampling Methods" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../../part3_bayesian/index.html" class="btn btn-neutral float-right" title="3. Part III: Bayesian Methods" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>