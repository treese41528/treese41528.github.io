

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Section 4.5: Jackknife Methods &mdash; STAT 418</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=3d0abd52" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT418/Website/part2_simulation/chapter4/ch4_5-jackknife-methods.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=30646c52"></script>
      <script>let toggleHintShow = 'Show solution';</script>
      <script>let toggleHintHide = 'Hide solution';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>window.MathJax = {"options": {"enableMenu": true, "menuOptions": {"settings": {"enrich": true, "speech": true, "braille": true, "collapsible": true, "assistiveMml": false}}}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
      <script src="../../_static/custom.js?v=8718e0ab"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Part III: Bayesian Methods" href="../../part3_bayesian/index.html" />
    <link rel="prev" title="Section 4.4: The Parametric Bootstrap" href="ch4_4-parametric-bootstrap.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Course Content</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../part1_foundations/index.html">Part I: Foundations of Probability and Computation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part1_foundations/chapter1/index.html">Chapter 1: Statistical Paradigms and Core Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html">Paradigms of Probability and Statistical Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#the-mathematical-foundation-kolmogorov-s-axioms">The Mathematical Foundation: Kolmogorov’s Axioms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#interpretations-of-probability">Interpretations of Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#statistical-inference-paradigms">Statistical Inference Paradigms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#historical-and-philosophical-debates">Historical and Philosophical Debates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#looking-ahead-our-course-focus">Looking Ahead: Our Course Focus</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html">Probability Distributions: Theory and Computation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#from-abstract-foundations-to-concrete-tools">From Abstract Foundations to Concrete Tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#the-python-ecosystem-for-probability">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#introduction-why-probability-distributions-matter">Introduction: Why Probability Distributions Matter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#id1">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#continuous-distributions">Continuous Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#additional-important-distributions">Additional Important Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#summary-and-practical-guidelines">Summary and Practical Guidelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#conclusion">Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html">Python Random Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#from-mathematical-distributions-to-computational-samples">From Mathematical Distributions to Computational Samples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-python-ecosystem-at-a-glance">The Python Ecosystem at a Glance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#understanding-pseudo-random-number-generation">Understanding Pseudo-Random Number Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-standard-library-random-module">The Standard Library: <code class="docutils literal notranslate"><span class="pre">random</span></code> Module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#numpy-fast-vectorized-random-sampling">NumPy: Fast Vectorized Random Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#scipy-stats-the-complete-statistical-toolkit">SciPy Stats: The Complete Statistical Toolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#bringing-it-all-together-library-selection-guide">Bringing It All Together: Library Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#looking-ahead-from-random-numbers-to-monte-carlo-methods">Looking Ahead: From Random Numbers to Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html">Chapter 1 Summary: Foundations in Place</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#the-three-pillars-of-chapter-1">The Three Pillars of Chapter 1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#what-lies-ahead-the-road-to-simulation">What Lies Ahead: The Road to Simulation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#chapter-1-exercises-synthesis-problems">Chapter 1 Exercises: Synthesis Problems</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Part II: Simulation-Based Methods</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../chapter2/index.html">Chapter 2: Monte Carlo Simulation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html">Monte Carlo Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#the-historical-development-of-monte-carlo-methods">The Historical Development of Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#the-core-principle-expectation-as-integration">The Core Principle: Expectation as Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#theoretical-foundations">Theoretical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#variance-estimation-and-confidence-intervals">Variance Estimation and Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#comparison-with-deterministic-methods">Comparison with Deterministic Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#sample-size-determination">Sample Size Determination</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#convergence-diagnostics-and-monitoring">Convergence Diagnostics and Monitoring</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#chapter-2-1-exercises-monte-carlo-fundamentals-mastery">Chapter 2.1 Exercises: Monte Carlo Fundamentals Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html">Uniform Random Variates</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#why-uniform-the-universal-currency-of-randomness">Why Uniform? The Universal Currency of Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#the-paradox-of-computational-randomness">The Paradox of Computational Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#chaotic-dynamical-systems-an-instructive-failure">Chaotic Dynamical Systems: An Instructive Failure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#linear-congruential-generators">Linear Congruential Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#shift-register-generators">Shift-Register Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#the-kiss-generator-combining-strategies">The KISS Generator: Combining Strategies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#modern-generators-mersenne-twister-and-pcg">Modern Generators: Mersenne Twister and PCG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#statistical-testing-of-random-number-generators">Statistical Testing of Random Number Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#chapter-2-2-exercises-uniform-random-variates-mastery">Chapter 2.2 Exercises: Uniform Random Variates Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html">Inverse CDF Method</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#mathematical-foundations">Mathematical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#continuous-distributions-with-closed-form-inverses">Continuous Distributions with Closed-Form Inverses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#numerical-inversion">Numerical Inversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#mixed-distributions">Mixed Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#chapter-2-3-exercises-inverse-cdf-method-mastery">Chapter 2.3 Exercises: Inverse CDF Method Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html">Transformation Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#why-transformation-methods">Why Transformation Methods?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-boxmuller-transform">The Box–Muller Transform</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-polar-marsaglia-method">The Polar (Marsaglia) Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#method-comparison-boxmuller-vs-polar-vs-ziggurat">Method Comparison: Box–Muller vs Polar vs Ziggurat</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-ziggurat-algorithm">The Ziggurat Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-clt-approximation-historical">The CLT Approximation (Historical)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#distributions-derived-from-the-normal">Distributions Derived from the Normal</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#multivariate-normal-generation">Multivariate Normal Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#implementation-guidance">Implementation Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#chapter-2-4-exercises-transformation-methods-mastery">Chapter 2.4 Exercises: Transformation Methods Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html">Rejection Sampling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-dartboard-intuition">The Dartboard Intuition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-accept-reject-algorithm">The Accept-Reject Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#efficiency-analysis">Efficiency Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#choosing-the-proposal-distribution">Choosing the Proposal Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-squeeze-principle">The Squeeze Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#geometric-example-sampling-from-the-unit-disk">Geometric Example: Sampling from the Unit Disk</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#limitations-and-the-curse-of-dimensionality">Limitations and the Curse of Dimensionality</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#connections-to-other-methods">Connections to Other Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#chapter-2-5-exercises-rejection-sampling-mastery">Chapter 2.5 Exercises: Rejection Sampling Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html">Variance Reduction Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#the-variance-reduction-paradigm">The Variance Reduction Paradigm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#importance-sampling">Importance Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#control-variates">Control Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#antithetic-variates">Antithetic Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#stratified-sampling">Stratified Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#common-random-numbers">Common Random Numbers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#conditional-monte-carlo-raoblackwellization">Conditional Monte Carlo (Rao–Blackwellization)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#combining-variance-reduction-techniques">Combining Variance Reduction Techniques</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#chapter-2-6-exercises-variance-reduction-mastery">Chapter 2.6 Exercises: Variance Reduction Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html">Chapter Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#the-complete-monte-carlo-workflow">The Complete Monte Carlo Workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#quick-reference-tables">Quick Reference Tables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#common-pitfalls-checklist">Common Pitfalls Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#connections-to-later-chapters">Connections to Later Chapters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#learning-outcomes-checklist">Learning Outcomes Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#further-reading-optimization-and-missing-data">Further Reading: Optimization and Missing Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter3/index.html">Chapter 3: Parametric Inference and Likelihood Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html">Exponential Families</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#historical-origins-from-scattered-results-to-unified-theory">Historical Origins: From Scattered Results to Unified Theory</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#the-canonical-exponential-family">The Canonical Exponential Family</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#converting-familiar-distributions">Converting Familiar Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#the-log-partition-function-a-moment-generating-machine">The Log-Partition Function: A Moment-Generating Machine</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#sufficiency-capturing-all-parameter-information">Sufficiency: Capturing All Parameter Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#minimal-sufficiency-and-completeness">Minimal Sufficiency and Completeness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#conjugate-priors-and-bayesian-inference">Conjugate Priors and Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#exponential-dispersion-models-and-glms">Exponential Dispersion Models and GLMs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#chapter-3-1-exercises-exponential-families-mastery">Chapter 3.1 Exercises: Exponential Families Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html">Maximum Likelihood Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-likelihood-function">The Likelihood Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-score-function">The Score Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#fisher-information">Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#closed-form-maximum-likelihood-estimators">Closed-Form Maximum Likelihood Estimators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#numerical-optimization-for-mle">Numerical Optimization for MLE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#asymptotic-properties-of-mles">Asymptotic Properties of MLEs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-cramer-rao-lower-bound">The Cramér-Rao Lower Bound</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-invariance-property">The Invariance Property</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#likelihood-based-hypothesis-testing">Likelihood-Based Hypothesis Testing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#confidence-intervals-from-likelihood">Confidence Intervals from Likelihood</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#connection-to-bayesian-inference">Connection to Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#chapter-3-2-exercises-maximum-likelihood-estimation-mastery">Chapter 3.2 Exercises: Maximum Likelihood Estimation Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html">Sampling Variability and Variance Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#statistical-estimators-and-their-properties">Statistical Estimators and Their Properties</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#sampling-distributions">Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#the-delta-method">The Delta Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#variance-estimation-methods">Variance Estimation Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#applications-and-worked-examples">Applications and Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html">Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#matrix-calculus-foundations">Matrix Calculus Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#the-linear-model">The Linear Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#ordinary-least-squares-the-calculus-approach">Ordinary Least Squares: The Calculus Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#ordinary-least-squares-the-geometric-approach">Ordinary Least Squares: The Geometric Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#properties-of-the-ols-estimator">Properties of the OLS Estimator</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#the-gauss-markov-theorem">The Gauss-Markov Theorem</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#estimating-the-error-variance">Estimating the Error Variance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#distributional-results-under-normality">Distributional Results Under Normality</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#diagnostics-and-model-checking">Diagnostics and Model Checking</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#numerical-stability-qr-decomposition">Numerical Stability: QR Decomposition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#model-selection-and-information-criteria">Model Selection and Information Criteria</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#regularization-ridge-and-lasso">Regularization: Ridge and LASSO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#chapter-3-4-exercises-linear-models-mastery">Chapter 3.4 Exercises: Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html">Generalized Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#historical-context-unification-of-regression-methods">Historical Context: Unification of Regression Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#the-glm-framework-three-components">The GLM Framework: Three Components</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#score-equations-and-fisher-information">Score Equations and Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#iteratively-reweighted-least-squares">Iteratively Reweighted Least Squares</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#logistic-regression-binary-outcomes">Logistic Regression: Binary Outcomes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#poisson-regression-count-data">Poisson Regression: Count Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#gamma-regression-positive-continuous-data">Gamma Regression: Positive Continuous Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#inference-in-glms-the-testing-triad">Inference in GLMs: The Testing Triad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#model-diagnostics">Model Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#model-comparison-and-selection">Model Comparison and Selection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#quasi-likelihood-and-robust-inference">Quasi-Likelihood and Robust Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#further-reading">Further Reading</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#chapter-3-5-exercises-generalized-linear-models-mastery">Chapter 3.5 Exercises: Generalized Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html">Chapter Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#the-parametric-inference-pipeline">The Parametric Inference Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#the-five-pillars-of-chapter-3">The Five Pillars of Chapter 3</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#quick-reference-core-formulas">Quick Reference: Core Formulas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#connections-to-future-material">Connections to Future Material</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#practical-guidance">Practical Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Chapter 4: Resampling Methods</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html">The Sampling Distribution Problem</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#the-fundamental-target-sampling-distributions">The Fundamental Target: Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#historical-development-the-quest-for-sampling-distributions">Historical Development: The Quest for Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#three-routes-to-the-sampling-distribution">Three Routes to the Sampling Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#when-asymptotics-fail-motivating-the-bootstrap">When Asymptotics Fail: Motivating the Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#the-plug-in-principle-theoretical-foundation">The Plug-In Principle: Theoretical Foundation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#computational-perspective-bootstrap-as-monte-carlo">Computational Perspective: Bootstrap as Monte Carlo</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#chapter-4-1-exercises">Chapter 4.1 Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html">The Empirical Distribution and Plug-in Principle</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#the-empirical-cumulative-distribution-function">The Empirical Cumulative Distribution Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#convergence-of-the-empirical-cdf">Convergence of the Empirical CDF</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#parameters-as-statistical-functionals">Parameters as Statistical Functionals</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#when-the-plug-in-principle-fails">When the Plug-in Principle Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#the-bootstrap-idea-in-one-sentence">The Bootstrap Idea in One Sentence</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#computational-implementation">Computational Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#section-4-2-exercises-ecdf-and-plug-in-mastery">Section 4.2 Exercises: ECDF and Plug-in Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html">The Nonparametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#the-bootstrap-principle">The Bootstrap Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#bootstrap-standard-errors">Bootstrap Standard Errors</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#bootstrap-bias-estimation">Bootstrap Bias Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#bootstrap-confidence-intervals">Bootstrap Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#bootstrap-for-regression">Bootstrap for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#bootstrap-diagnostics">Bootstrap Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#when-bootstrap-fails">When Bootstrap Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch4_4-parametric-bootstrap.html">Section 4.4: The Parametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#the-parametric-bootstrap-principle">The Parametric Bootstrap Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#location-scale-families">Location-Scale Families</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#parametric-bootstrap-for-regression">Parametric Bootstrap for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#confidence-intervals">Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#model-checking-and-validation">Model Checking and Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#when-parametric-bootstrap-fails">When Parametric Bootstrap Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#parametric-vs-nonparametric-a-decision-framework">Parametric vs. Nonparametric: A Decision Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Section 4.5: Jackknife Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#historical-context-and-motivation">Historical Context and Motivation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-delete-1-jackknife">The Delete-1 Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="#jackknife-bias-estimation">Jackknife Bias Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-delete-d-jackknife">The Delete-<span class="math notranslate nohighlight">\(d\)</span> Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="#jackknife-versus-bootstrap">Jackknife versus Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-infinitesimal-jackknife">The Infinitesimal Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part3_bayesian/index.html">Part III: Bayesian Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part3_bayesian/index.html#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html">Bayesian Philosophy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Part II: Simulation-Based Methods</a></li>
          <li class="breadcrumb-item"><a href="index.html">Chapter 4: Resampling Methods</a></li>
      <li class="breadcrumb-item active">Section 4.5: Jackknife Methods</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/part2_simulation/chapter4/ch4_5-jackknife-methods.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="section-4-5-jackknife-methods">
<span id="ch4-5-jackknife-methods"></span><h1>Section 4.5: Jackknife Methods<a class="headerlink" href="#section-4-5-jackknife-methods" title="Link to this heading"></a></h1>
<p>The bootstrap, introduced by Efron in 1979, revolutionized computational statistics by providing a general-purpose tool for uncertainty quantification. But the bootstrap had an important predecessor: the <strong>jackknife</strong>, developed by Quenouille (1949) for bias reduction and extended by Tukey (1958) for variance estimation. While the bootstrap has largely superseded the jackknife for interval construction, the jackknife remains valuable in its own right—it is deterministic, computationally efficient for smooth statistics, and provides unique diagnostic capabilities through its connection to influence functions.</p>
<p>This section develops the jackknife as both a historical precursor to the bootstrap and a modern tool with distinct strengths. We begin with the delete-1 jackknife, extend to the delete-<span class="math notranslate nohighlight">\(d\)</span> generalization, explore bias correction, and conclude with a systematic comparison to bootstrap methods. Throughout, we emphasize when each method excels and how they can be used together.</p>
<div class="important admonition">
<p class="admonition-title">Road Map 🧭</p>
<ul class="simple">
<li><p><strong>Understand</strong>: The jackknife principle as systematic leave-one-out perturbation analysis</p></li>
<li><p><strong>Develop</strong>: Computational skills for jackknife standard errors, bias estimation, and diagnostics</p></li>
<li><p><strong>Implement</strong>: Delete-1 and delete-<span class="math notranslate nohighlight">\(d\)</span> jackknife algorithms in Python</p></li>
<li><p><strong>Evaluate</strong>: When to prefer jackknife over bootstrap, and when to use both</p></li>
</ul>
</div>
<section id="historical-context-and-motivation">
<h2>Historical Context and Motivation<a class="headerlink" href="#historical-context-and-motivation" title="Link to this heading"></a></h2>
<p>The jackknife emerged from a practical problem: how to assess the reliability of a complex statistic without deriving its theoretical variance. Maurice Quenouille introduced the method in 1949 for bias reduction in serial correlation estimation, and John Tukey named it the “jackknife” in 1958, likening it to a versatile pocket knife that can help in many situations.</p>
<section id="the-historical-problem">
<h3>The Historical Problem<a class="headerlink" href="#the-historical-problem" title="Link to this heading"></a></h3>
<p>Consider estimating a parameter <span class="math notranslate nohighlight">\(\theta = T(F)\)</span> with a statistic <span class="math notranslate nohighlight">\(\hat{\theta} = T(\hat{F}_n)\)</span>. For many statistics beyond the sample mean, deriving <span class="math notranslate nohighlight">\(\text{Var}(\hat{\theta})\)</span> analytically requires either:</p>
<ol class="arabic simple">
<li><p><strong>Strong distributional assumptions</strong> (e.g., normality)</p></li>
<li><p><strong>Taylor expansions</strong> (delta method) that may be complex</p></li>
<li><p><strong>Asymptotic approximations</strong> that may be inaccurate for small <span class="math notranslate nohighlight">\(n\)</span></p></li>
</ol>
<p>The jackknife provides an empirical alternative: by systematically observing how <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> changes when each observation is removed, we can approximate both the variance and bias of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> without any distributional assumptions.</p>
</section>
<section id="the-core-insight">
<h3>The Core Insight<a class="headerlink" href="#the-core-insight" title="Link to this heading"></a></h3>
<p>For a smooth statistic <span class="math notranslate nohighlight">\(T\)</span>, removing a single observation <span class="math notranslate nohighlight">\(X_i\)</span> from the sample approximates a small perturbation to the empirical distribution <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>. If <span class="math notranslate nohighlight">\(T\)</span> is sufficiently regular (differentiable in an appropriate sense), the change <span class="math notranslate nohighlight">\(\hat{\theta}_{(-i)} - \hat{\theta}\)</span> reflects the <strong>influence</strong> of observation <span class="math notranslate nohighlight">\(X_i\)</span> on the estimate. Aggregating these influences across all observations recovers information about the sampling variability of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>.</p>
<p>This is precisely the intuition behind the <strong>influence function</strong> from robust statistics, and we will see that the jackknife provides a discrete approximation to influence-function-based variance estimation.</p>
</section>
</section>
<section id="the-delete-1-jackknife">
<h2>The Delete-1 Jackknife<a class="headerlink" href="#the-delete-1-jackknife" title="Link to this heading"></a></h2>
<p>The delete-1 (or leave-one-out) jackknife is the foundation of jackknife methodology. We systematically compute the statistic on each of the <span class="math notranslate nohighlight">\(n\)</span> subsamples formed by removing one observation.</p>
<section id="notation-and-basic-definitions">
<h3>Notation and Basic Definitions<a class="headerlink" href="#notation-and-basic-definitions" title="Link to this heading"></a></h3>
<p>Let <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span> be an iid sample from distribution <span class="math notranslate nohighlight">\(F\)</span>, and let <span class="math notranslate nohighlight">\(\hat{\theta} = T(X_1, \ldots, X_n)\)</span> be a statistic estimating <span class="math notranslate nohighlight">\(\theta = T(F)\)</span>.</p>
<p><strong>Leave-one-out estimate</strong>: For each <span class="math notranslate nohighlight">\(i = 1, \ldots, n\)</span>, define</p>
<div class="math notranslate nohighlight">
\[\hat{\theta}_{(-i)} = T(X_1, \ldots, X_{i-1}, X_{i+1}, \ldots, X_n)\]</div>
<p>This is the statistic computed on the sample with observation <span class="math notranslate nohighlight">\(X_i\)</span> removed.</p>
<p><strong>Jackknife average</strong>: The mean of the leave-one-out estimates is</p>
<div class="math notranslate nohighlight">
\[\bar{\theta}_{(\cdot)} = \frac{1}{n} \sum_{i=1}^n \hat{\theta}_{(-i)}\]</div>
<p><strong>Pseudovalues</strong>: Tukey introduced the pseudovalues</p>
<div class="math notranslate nohighlight">
\[\widetilde{\theta}_i = n\hat{\theta} - (n-1)\hat{\theta}_{(-i)}\]</div>
<p>These act like <span class="math notranslate nohighlight">\(n\)</span> “replicates” of the original estimate. Their sample mean equals the <strong>bias-corrected jackknife estimate</strong>:</p>
<div class="math notranslate nohighlight">
\[\frac{1}{n} \sum_{i=1}^n \widetilde{\theta}_i = n\hat{\theta} - (n-1)\bar{\theta}_{(\cdot)} = \hat{\theta}^{\text{jack-bc}}\]</div>
<p>This equals <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> only when <span class="math notranslate nohighlight">\(\bar{\theta}_{(\cdot)} = \hat{\theta}\)</span> (i.e., when the statistic is approximately unbiased or linear in the observations). For biased statistics, the mean of pseudovalues provides a bias-corrected estimate.</p>
<div class="tip admonition">
<p class="admonition-title">Intuition for Pseudovalues</p>
<p>Think of each pseudovalue <span class="math notranslate nohighlight">\(\widetilde{\theta}_i\)</span> as answering: “What would <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> have been if observation <span class="math notranslate nohighlight">\(X_i\)</span> had <span class="math notranslate nohighlight">\(n\)</span> times its actual weight?” The pseudovalues inflate the influence of each observation, making the variability due to individual observations explicit.</p>
</div>
</section>
<section id="jackknife-standard-error">
<h3>Jackknife Standard Error<a class="headerlink" href="#jackknife-standard-error" title="Link to this heading"></a></h3>
<p>The jackknife standard error is computed from the variability of the leave-one-out estimates:</p>
<div class="math notranslate nohighlight">
\[\widehat{\text{SE}}_{\text{jack}} = \sqrt{\frac{n-1}{n} \sum_{i=1}^n \left(\hat{\theta}_{(-i)} - \bar{\theta}_{(\cdot)}\right)^2}\]</div>
<p>The factor <span class="math notranslate nohighlight">\((n-1)/n\)</span> (rather than <span class="math notranslate nohighlight">\(1/n\)</span>) accounts for the fact that leave-one-out estimates are computed on samples of size <span class="math notranslate nohighlight">\(n-1\)</span>, not <span class="math notranslate nohighlight">\(n\)</span>. This scaling ensures correct variance estimation for smooth statistics.</p>
<p><strong>Equivalently</strong>, the jackknife SE can be expressed using pseudovalues:</p>
<div class="math notranslate nohighlight">
\[\widehat{\text{SE}}_{\text{jack}} = \frac{1}{\sqrt{n}} \cdot s_{\widetilde{\theta}}\]</div>
<p>where <span class="math notranslate nohighlight">\(s_{\widetilde{\theta}}\)</span> is the sample standard deviation of the pseudovalues <span class="math notranslate nohighlight">\(\widetilde{\theta}_1, \ldots, \widetilde{\theta}_n\)</span> (using divisor <span class="math notranslate nohighlight">\(n-1\)</span>).</p>
<p><strong>For vector statistics</strong> <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\theta}} \in \mathbb{R}^p\)</span>, the jackknife covariance matrix is:</p>
<div class="math notranslate nohighlight">
\[\widehat{\text{Cov}}_{\text{jack}}(\hat{\boldsymbol{\theta}}) = \frac{n-1}{n} \sum_{i=1}^n \left(\hat{\boldsymbol{\theta}}_{(-i)} - \bar{\boldsymbol{\theta}}_{(\cdot)}\right)\left(\hat{\boldsymbol{\theta}}_{(-i)} - \bar{\boldsymbol{\theta}}_{(\cdot)}\right)^\top\]</div>
<p>Marginal standard errors are the square roots of diagonal elements, and the standard error of a linear combination <span class="math notranslate nohighlight">\(\mathbf{a}^\top\hat{\boldsymbol{\theta}}\)</span> is <span class="math notranslate nohighlight">\(\sqrt{\mathbf{a}^\top \widehat{\text{Cov}}_{\text{jack}} \, \mathbf{a}}\)</span>.</p>
</section>
<section id="why-the-jackknife-works">
<h3>Why the Jackknife Works<a class="headerlink" href="#why-the-jackknife-works" title="Link to this heading"></a></h3>
<p>For statistics that are smooth functionals of <span class="math notranslate nohighlight">\(F\)</span>, the jackknife SE is consistent. The key theoretical result connects the jackknife to the <strong>influence function</strong>.</p>
<p><strong>Influence Function</strong>: For a functional <span class="math notranslate nohighlight">\(T\)</span> and distribution <span class="math notranslate nohighlight">\(F\)</span>, the influence function is</p>
<div class="math notranslate nohighlight">
\[\text{IF}(x; T, F) = \lim_{\epsilon \to 0} \frac{T((1-\epsilon)F + \epsilon \delta_x) - T(F)}{\epsilon}\]</div>
<p>where <span class="math notranslate nohighlight">\(\delta_x\)</span> is a point mass at <span class="math notranslate nohighlight">\(x\)</span>. This measures the rate of change of <span class="math notranslate nohighlight">\(T\)</span> when we contaminate <span class="math notranslate nohighlight">\(F\)</span> with a small mass at <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p><strong>Connection to Jackknife</strong>: For smooth statistics, the jackknife residuals approximate the empirical influence function. Removing observation <span class="math notranslate nohighlight">\(i\)</span> changes the empirical distribution from <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> (uniform weights <span class="math notranslate nohighlight">\(1/n\)</span>) to <span class="math notranslate nohighlight">\(\hat{F}_{n,-i}\)</span> (uniform weights <span class="math notranslate nohighlight">\(1/(n-1)\)</span> on remaining observations). The scaled jackknife difference</p>
<div class="math notranslate nohighlight">
\[(n-1)(\hat{\theta} - \hat{\theta}_{(-i)}) \approx \text{IF}(X_i; T, \hat{F}_n)\]</div>
<p>approximates the empirical influence function evaluated at each observation. The factor <span class="math notranslate nohighlight">\((n-1)\)</span> rather than <span class="math notranslate nohighlight">\(n\)</span> arises because the leave-one-out empirical distribution has weights <span class="math notranslate nohighlight">\(1/(n-1)\)</span>.</p>
<p><strong>Asymptotic Variance</strong>: Under regularity conditions, the asymptotic variance of <span class="math notranslate nohighlight">\(\sqrt{n}(\hat{\theta} - \theta)\)</span> is</p>
<div class="math notranslate nohighlight">
\[\sigma^2 = \int \text{IF}(x; T, F)^2 \, dF(x)\]</div>
<p>The jackknife variance estimator consistently estimates this quantity:</p>
<div class="math notranslate nohighlight">
\[n \cdot \widehat{\text{SE}}_{\text{jack}}^2 \xrightarrow{p} \sigma^2\]</div>
<p>This explains why the jackknife works for smooth statistics—it approximates the influence-function variance formula.</p>
</section>
<section id="python-implementation">
<h3>Python Implementation<a class="headerlink" href="#python-implementation" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">jackknife_se</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">statistic</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the jackknife standard error of a statistic.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    data : array_like</span>
<span class="sd">        Original data (1D array or 2D with observations along axis 0).</span>
<span class="sd">    statistic : callable</span>
<span class="sd">        Function that computes the statistic from data. May return</span>
<span class="sd">        a scalar or array.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    se_jack : float or ndarray</span>
<span class="sd">        Jackknife standard error (same shape as statistic output).</span>
<span class="sd">    theta_hat : float or ndarray</span>
<span class="sd">        Original estimate from full data.</span>
<span class="sd">    theta_i : ndarray</span>
<span class="sd">        Leave-one-out estimates, shape (n,) or (n, p) for vector statistics.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="c1"># Compute original estimate and determine shape</span>
    <span class="n">theta_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">statistic</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>

    <span class="c1"># Allocate array for leave-one-out estimates (handles scalar or vector)</span>
    <span class="n">theta_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n</span><span class="p">,)</span> <span class="o">+</span> <span class="n">theta_hat</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="c1"># Remove observation i</span>
        <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">data_minus_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">data_minus_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">theta_i</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">statistic</span><span class="p">(</span><span class="n">data_minus_i</span><span class="p">)</span>

    <span class="c1"># Jackknife SE formula (works for scalar or vector)</span>
    <span class="n">theta_bar</span> <span class="o">=</span> <span class="n">theta_i</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">se_jack</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(((</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">theta_i</span> <span class="o">-</span> <span class="n">theta_bar</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">se_jack</span><span class="p">,</span> <span class="n">theta_hat</span><span class="p">,</span> <span class="n">theta_i</span>


<span class="k">def</span><span class="w"> </span><span class="nf">jackknife_pseudovalues</span><span class="p">(</span><span class="n">theta_hat</span><span class="p">,</span> <span class="n">theta_i</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute jackknife pseudovalues.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    theta_hat : float</span>
<span class="sd">        Original estimate from full data.</span>
<span class="sd">    theta_i : array_like</span>
<span class="sd">        Leave-one-out estimates.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pv : ndarray</span>
<span class="sd">        Pseudovalues.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">theta_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">theta_i</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">theta_i</span><span class="p">)</span>
    <span class="n">pv</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="n">theta_hat</span> <span class="o">-</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">theta_i</span>
    <span class="k">return</span> <span class="n">pv</span>
</pre></div>
</div>
<div class="note admonition">
<p class="admonition-title">Example 💡 Jackknife SE for the Sample Mean</p>
<p><strong>Verification</strong>: For the sample mean, the jackknife SE should equal the classical formula <span class="math notranslate nohighlight">\(s/\sqrt{n}\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Generate data</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># Define statistic</span>
<span class="k">def</span><span class="w"> </span><span class="nf">sample_mean</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># Jackknife SE</span>
<span class="n">se_jack</span><span class="p">,</span> <span class="n">theta_hat</span><span class="p">,</span> <span class="n">theta_i</span> <span class="o">=</span> <span class="n">jackknife_se</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">sample_mean</span><span class="p">)</span>

<span class="c1"># Classical SE</span>
<span class="n">se_classical</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample mean: </span><span class="si">{</span><span class="n">theta_hat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Jackknife SE: </span><span class="si">{</span><span class="n">se_jack</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Classical SE: </span><span class="si">{</span><span class="n">se_classical</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ratio: </span><span class="si">{</span><span class="n">se_jack</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">se_classical</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Sample mean: 10.0641
Jackknife SE: 0.2746
Classical SE: 0.2746
Ratio: 1.000000
</pre></div>
</div>
<p>The jackknife SE exactly equals the classical SE for the sample mean—this is not a coincidence but a mathematical identity (see Exercise 4.5.1).</p>
</div>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_5_fig01_jackknife_algorithm.png"><img alt="Delete-1 jackknife algorithm showing leave-one-out samples, estimates, pseudovalues, and SE comparison" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_5_fig01_jackknife_algorithm.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 159 </span><span class="caption-text"><strong>The Delete-1 Jackknife Algorithm.</strong> Panel (a) shows the original sample and the six leave-one-out samples, with removed observations marked by X. Panel (b) displays the leave-one-out estimates <span class="math notranslate nohighlight">\(\hat{\theta}_{(-i)}\)</span> as horizontal bars, with the original estimate <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> and jackknife average <span class="math notranslate nohighlight">\(\bar{\theta}_{(\cdot)}\)</span> marked. Panel (c) shows the corresponding pseudovalues <span class="math notranslate nohighlight">\(\widetilde{\theta}_i = n\hat{\theta} - (n-1)\hat{\theta}_{(-i)}\)</span>. Panel (d) summarizes the jackknife SE formula. Panel (e) confirms the exact equality between jackknife SE and classical SE for the sample mean. Panel (f) summarizes key properties and use cases.</span><a class="headerlink" href="#id5" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="jackknife-bias-estimation">
<h2>Jackknife Bias Estimation<a class="headerlink" href="#jackknife-bias-estimation" title="Link to this heading"></a></h2>
<p>Quenouille’s original motivation for the jackknife was <strong>bias reduction</strong>. Many statistics have bias of order <span class="math notranslate nohighlight">\(O(1/n)\)</span>, and the jackknife can estimate and correct this bias.</p>
<section id="the-bias-formula">
<h3>The Bias Formula<a class="headerlink" href="#the-bias-formula" title="Link to this heading"></a></h3>
<p>The jackknife bias estimate is:</p>
<div class="math notranslate nohighlight">
\[\widehat{\text{Bias}}_{\text{jack}} = (n-1)\left(\bar{\theta}_{(\cdot)} - \hat{\theta}\right)\]</div>
<p>This formula arises from the following reasoning: if <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> has bias <span class="math notranslate nohighlight">\(b/n + O(1/n^2)\)</span> for sample size <span class="math notranslate nohighlight">\(n\)</span>, then <span class="math notranslate nohighlight">\(\hat{\theta}_{(-i)}\)</span> (computed on <span class="math notranslate nohighlight">\(n-1\)</span> observations) has bias approximately <span class="math notranslate nohighlight">\(b/(n-1)\)</span>. The difference in biases is <span class="math notranslate nohighlight">\(b/(n-1) - b/n = b/(n(n-1))\)</span>. Summing over all <span class="math notranslate nohighlight">\(n\)</span> leave-one-out estimates and solving for <span class="math notranslate nohighlight">\(b\)</span> yields the jackknife bias formula.</p>
</section>
<section id="bias-corrected-estimator">
<h3>Bias-Corrected Estimator<a class="headerlink" href="#bias-corrected-estimator" title="Link to this heading"></a></h3>
<p>The <strong>bias-corrected jackknife estimator</strong> is:</p>
<div class="math notranslate nohighlight">
\[\hat{\theta}^{\text{jack-bc}} = \hat{\theta} - \widehat{\text{Bias}}_{\text{jack}} = n\hat{\theta} - (n-1)\bar{\theta}_{(\cdot)}\]</div>
<p>This equals the mean of the pseudovalues:</p>
<div class="math notranslate nohighlight">
\[\hat{\theta}^{\text{jack-bc}} = \frac{1}{n}\sum_{i=1}^n \widetilde{\theta}_i\]</div>
<p><strong>Example: Variance Estimator</strong></p>
<p>The biased variance estimator <span class="math notranslate nohighlight">\(\hat{\sigma}^2 = \frac{1}{n}\sum_i (X_i - \bar{X})^2\)</span> has bias <span class="math notranslate nohighlight">\(-\sigma^2/n\)</span>. The jackknife bias-corrected version recovers the unbiased estimator <span class="math notranslate nohighlight">\(s^2 = \frac{1}{n-1}\sum_i (X_i - \bar{X})^2\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">When to Use Bias Correction</p>
<ul class="simple">
<li><p><strong>Use</strong> when <span class="math notranslate nohighlight">\(|\widehat{\text{Bias}}_{\text{jack}}|\)</span> is large relative to <span class="math notranslate nohighlight">\(\widehat{\text{SE}}_{\text{jack}}\)</span> (say, &gt; 25% of SE)</p></li>
<li><p><strong>Be cautious</strong> with small <span class="math notranslate nohighlight">\(n\)</span>—bias correction increases variance</p></li>
<li><p><strong>Report both</strong> <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> and <span class="math notranslate nohighlight">\(\hat{\theta}^{\text{jack-bc}}\)</span> when the correction is material</p></li>
<li><p><strong>Check consistency</strong>: if bootstrap bias agrees with jackknife bias, correction is more trustworthy</p></li>
</ul>
</div>
</section>
<section id="id1">
<h3>Python Implementation<a class="headerlink" href="#id1" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">jackknife_bias</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">statistic</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute jackknife bias estimate and bias-corrected estimator.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    data : array_like</span>
<span class="sd">        Original data.</span>
<span class="sd">    statistic : callable</span>
<span class="sd">        Function that computes the statistic.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    bias_jack : float</span>
<span class="sd">        Jackknife bias estimate.</span>
<span class="sd">    theta_bc : float</span>
<span class="sd">        Bias-corrected estimate.</span>
<span class="sd">    theta_hat : float</span>
<span class="sd">        Original estimate.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="n">theta_hat</span> <span class="o">=</span> <span class="n">statistic</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="c1"># Leave-one-out estimates</span>
    <span class="n">theta_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">data_minus_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">data_minus_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">theta_i</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">statistic</span><span class="p">(</span><span class="n">data_minus_i</span><span class="p">)</span>

    <span class="n">theta_bar</span> <span class="o">=</span> <span class="n">theta_i</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="c1"># Bias estimate</span>
    <span class="n">bias_jack</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta_bar</span> <span class="o">-</span> <span class="n">theta_hat</span><span class="p">)</span>

    <span class="c1"># Bias-corrected estimate</span>
    <span class="n">theta_bc</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="n">theta_hat</span> <span class="o">-</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">theta_bar</span>

    <span class="k">return</span> <span class="n">bias_jack</span><span class="p">,</span> <span class="n">theta_bc</span><span class="p">,</span> <span class="n">theta_hat</span>
</pre></div>
</div>
<div class="note admonition">
<p class="admonition-title">Example 💡 Bias Correction for Variance Estimator</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># Biased variance estimator (divide by n)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">biased_var</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">bias_jack</span><span class="p">,</span> <span class="n">var_bc</span><span class="p">,</span> <span class="n">var_biased</span> <span class="o">=</span> <span class="n">jackknife_bias</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">biased_var</span><span class="p">)</span>

<span class="c1"># Compare with unbiased variance</span>
<span class="n">var_unbiased</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Biased estimate (÷n):      </span><span class="si">{</span><span class="n">var_biased</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Jackknife bias estimate:   </span><span class="si">{</span><span class="n">bias_jack</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bias-corrected estimate:   </span><span class="si">{</span><span class="n">var_bc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unbiased estimate (÷n-1):  </span><span class="si">{</span><span class="n">var_unbiased</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Biased estimate (÷n):      3.6821
Jackknife bias estimate:   -0.1269
Bias-corrected estimate:   3.8090
Unbiased estimate (÷n-1):  3.8090
</pre></div>
</div>
<p>The jackknife bias correction exactly recovers the unbiased variance estimator.</p>
</div>
<figure class="align-center" id="id6">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_5_fig02_bias_correction.png"><img alt="Jackknife bias correction for variance and exponential MLE with MSE decomposition" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_5_fig02_bias_correction.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 160 </span><span class="caption-text"><strong>Jackknife Bias Correction.</strong> Panel (a) shows the variance estimator example: the biased estimate (÷n), jackknife bias-corrected estimate, unbiased estimate (÷n−1), and true σ². The bias-corrected jackknife exactly recovers the unbiased estimator. Panel (b) demonstrates bias correction for exponential MLE, showing the positive bias and how jackknife correction approximates the theoretical bias. Panel (c) decomposes MSE into bias² and variance components, illustrating the tradeoff: bias correction reduces bias² but increases variance. Panel (d) provides guidance on when to apply bias correction.</span><a class="headerlink" href="#id6" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="the-delete-d-jackknife">
<h2>The Delete-<span class="math notranslate nohighlight">\(d\)</span> Jackknife<a class="headerlink" href="#the-delete-d-jackknife" title="Link to this heading"></a></h2>
<p>The delete-1 jackknife removes one observation at a time. The <strong>delete-</strong><span class="math notranslate nohighlight">\(d\)</span> <strong>jackknife</strong> generalizes this by removing <span class="math notranslate nohighlight">\(d\)</span> observations simultaneously, creating subsamples of size <span class="math notranslate nohighlight">\(n - d\)</span>.</p>
<section id="motivation">
<h3>Motivation<a class="headerlink" href="#motivation" title="Link to this heading"></a></h3>
<p>Why consider <span class="math notranslate nohighlight">\(d &gt; 1\)</span>?</p>
<ol class="arabic simple">
<li><p><strong>Non-smooth statistics</strong>: For statistics like the median or quantiles, delete-1 jackknife can be unstable because removing one observation may not change the statistic (especially with ties). Larger <span class="math notranslate nohighlight">\(d\)</span> creates more variation.</p></li>
<li><p><strong>Dependent data</strong>: When observations come in blocks (time series, spatial data), deleting entire blocks preserves the dependence structure.</p></li>
<li><p><strong>Higher-order bias</strong>: Some bias correction schemes require <span class="math notranslate nohighlight">\(d &gt; 1\)</span> to achieve optimal rates.</p></li>
<li><p><strong>Bridge to subsampling</strong>: As <span class="math notranslate nohighlight">\(d/n \to \gamma \in (0, 1)\)</span>, the delete-<span class="math notranslate nohighlight">\(d\)</span> jackknife approaches subsampling methods.</p></li>
</ol>
</section>
<section id="definition">
<h3>Definition<a class="headerlink" href="#definition" title="Link to this heading"></a></h3>
<p>For <span class="math notranslate nohighlight">\(d \geq 1\)</span>, let <span class="math notranslate nohighlight">\(S \subset \{1, \ldots, n\}\)</span> with <span class="math notranslate nohighlight">\(|S| = d\)</span> denote a subset of indices to remove. The delete-<span class="math notranslate nohighlight">\(d\)</span> estimate is:</p>
<div class="math notranslate nohighlight">
\[\hat{\theta}_{(-S)} = T\left(\{X_i : i \notin S\}\right)\]</div>
<p>computed on the <span class="math notranslate nohighlight">\(n - d\)</span> observations remaining after removing those indexed by <span class="math notranslate nohighlight">\(S\)</span>.</p>
<p>There are <span class="math notranslate nohighlight">\(\binom{n}{d}\)</span> such subsets. For small <span class="math notranslate nohighlight">\(d\)</span>, we can enumerate all of them; for large <span class="math notranslate nohighlight">\(d\)</span>, we typically use a random sample of subsets.</p>
<div class="note admonition">
<p class="admonition-title">Subset Sampling Approximation</p>
<p>When <span class="math notranslate nohighlight">\(\binom{n}{d}\)</span> is large (e.g., <span class="math notranslate nohighlight">\(\binom{50}{5} = 2,118,760\)</span>), we approximate the average over all subsets by random sampling. With a sufficiently large sample (e.g., 5,000–10,000 subsets), the variance of this Monte Carlo approximation is typically negligible compared to the jackknife variance itself. The estimator remains consistent as long as each observation has equal probability of being omitted.</p>
</div>
</section>
<section id="delete-d-variance-estimator">
<h3>Delete-<span class="math notranslate nohighlight">\(d\)</span> Variance Estimator<a class="headerlink" href="#delete-d-variance-estimator" title="Link to this heading"></a></h3>
<p>The delete-<span class="math notranslate nohighlight">\(d\)</span> jackknife variance estimator is:</p>
<div class="math notranslate nohighlight">
\[\widehat{\text{Var}}_{\text{jack},d} = \frac{n - d}{d \binom{n}{d}} \sum_{S: |S| = d} \left(\hat{\theta}_{(-S)} - \bar{\theta}\right)^2\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{\theta}\)</span> is the average over all (or sampled) delete-<span class="math notranslate nohighlight">\(d\)</span> estimates. The corresponding standard error is:</p>
<div class="math notranslate nohighlight">
\[\widehat{\text{SE}}_{\text{jack},d} = \sqrt{\widehat{\text{Var}}_{\text{jack},d}}\]</div>
<p>The factor <span class="math notranslate nohighlight">\((n-d)/d\)</span> scales the variance appropriately for the reduced sample size.</p>
<div class="tip admonition">
<p class="admonition-title">Scaling Intuition</p>
<p>For delete-1 (<span class="math notranslate nohighlight">\(d = 1\)</span>), the factor is <span class="math notranslate nohighlight">\((n-1)/1 = n-1\)</span>, which when multiplied by <span class="math notranslate nohighlight">\(1/n\)</span> (from averaging) gives <span class="math notranslate nohighlight">\((n-1)/n\)</span>—matching our earlier formula.</p>
<p>For larger <span class="math notranslate nohighlight">\(d\)</span>, the factor accounts for:</p>
<ul class="simple">
<li><p>Subsamples have size <span class="math notranslate nohighlight">\(n - d\)</span> (more variability than size <span class="math notranslate nohighlight">\(n\)</span>)</p></li>
<li><p>Each observation is omitted in <span class="math notranslate nohighlight">\(\binom{n-1}{d-1}\)</span> of the <span class="math notranslate nohighlight">\(\binom{n}{d}\)</span> subsets</p></li>
</ul>
</div>
</section>
<section id="choosing-d">
<h3>Choosing <span class="math notranslate nohighlight">\(d\)</span><a class="headerlink" href="#choosing-d" title="Link to this heading"></a></h3>
<p>The choice of <span class="math notranslate nohighlight">\(d\)</span> involves a bias-variance tradeoff:</p>
<ul class="simple">
<li><p><strong>Small</strong> <span class="math notranslate nohighlight">\(d\)</span> <strong>(including</strong> :math:<a href="#id2"><span class="problematic" id="id3">`</span></a>d = 1`**)**: Lower variance in the SE estimate, but may be biased for non-smooth statistics.</p></li>
<li><p><strong>Large</strong> <span class="math notranslate nohighlight">\(d\)</span>: Better for non-smooth statistics, but higher variance and computational cost.</p></li>
</ul>
<p><strong>Practical guidelines</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Smooth statistics</strong>: <span class="math notranslate nohighlight">\(d = 1\)</span> is sufficient and most efficient.</p></li>
<li><p><strong>Quantiles/median</strong>: Consider <span class="math notranslate nohighlight">\(d \approx \sqrt{n}\)</span> or larger.</p></li>
<li><p><strong>Block-dependent data</strong>: <span class="math notranslate nohighlight">\(d\)</span> = block size.</p></li>
<li><p><strong>Exploration</strong>: Compare <span class="math notranslate nohighlight">\(d = 1, \sqrt{n}, n/4\)</span> to assess sensitivity.</p></li>
</ol>
<p><strong>Connection to Subsampling</strong>: When <span class="math notranslate nohighlight">\(d = n - m\)</span> for fixed <span class="math notranslate nohighlight">\(m\)</span> (or <span class="math notranslate nohighlight">\(d/n \to 1\)</span>), delete-<span class="math notranslate nohighlight">\(d\)</span> jackknife becomes subsampling: we compute the statistic on many subsamples of size <span class="math notranslate nohighlight">\(m\)</span> and use their distribution for inference. This approach is more general but requires different variance formulas.</p>
</section>
<section id="id4">
<h3>Python Implementation<a class="headerlink" href="#id4" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">itertools</span><span class="w"> </span><span class="kn">import</span> <span class="n">combinations</span>

<span class="k">def</span><span class="w"> </span><span class="nf">jackknife_delete_d</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">statistic</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_subsets</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute delete-d jackknife standard error.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    data : array_like</span>
<span class="sd">        Original data.</span>
<span class="sd">    statistic : callable</span>
<span class="sd">        Function that computes the statistic.</span>
<span class="sd">    d : int</span>
<span class="sd">        Number of observations to delete (default 1).</span>
<span class="sd">    max_subsets : int, optional</span>
<span class="sd">        Maximum number of subsets to use. If None, use all C(n,d) subsets</span>
<span class="sd">        when C(n,d) &lt;= 10000, otherwise sample 10000 random subsets.</span>
<span class="sd">    seed : int, optional</span>
<span class="sd">        Random seed for subset sampling.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    se_jack_d : float</span>
<span class="sd">        Delete-d jackknife standard error.</span>
<span class="sd">    theta_hat : float</span>
<span class="sd">        Original estimate.</span>
<span class="sd">    theta_S : ndarray</span>
<span class="sd">        Delete-d estimates for each subset.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">d</span> <span class="o">&gt;=</span> <span class="n">n</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;d=</span><span class="si">{</span><span class="n">d</span><span class="si">}</span><span class="s2"> must be less than n=</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">theta_hat</span> <span class="o">=</span> <span class="n">statistic</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="c1"># Determine number of subsets</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">math</span><span class="w"> </span><span class="kn">import</span> <span class="n">comb</span>
    <span class="n">n_subsets</span> <span class="o">=</span> <span class="n">comb</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">max_subsets</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">max_subsets</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">n_subsets</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span>

    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">n_subsets</span> <span class="o">&lt;=</span> <span class="n">max_subsets</span><span class="p">:</span>
        <span class="c1"># Use all subsets</span>
        <span class="n">subsets</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">combinations</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">d</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Random sample of subsets</span>
        <span class="n">subsets</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">seen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">subsets</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">max_subsets</span><span class="p">:</span>
            <span class="n">S</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)))</span>
            <span class="k">if</span> <span class="n">S</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">seen</span><span class="p">:</span>
                <span class="n">seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
                <span class="n">subsets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>

    <span class="c1"># Compute delete-d estimates</span>
    <span class="n">theta_S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">subsets</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">S</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">subsets</span><span class="p">):</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
        <span class="n">mask</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">S</span><span class="p">)]</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">data_minus_S</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">data_minus_S</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
        <span class="n">theta_S</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">statistic</span><span class="p">(</span><span class="n">data_minus_S</span><span class="p">)</span>

    <span class="c1"># Delete-d variance formula</span>
    <span class="n">theta_bar</span> <span class="o">=</span> <span class="n">theta_S</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">var_jack_d</span> <span class="o">=</span> <span class="p">((</span><span class="n">n</span> <span class="o">-</span> <span class="n">d</span><span class="p">)</span> <span class="o">/</span> <span class="n">d</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">theta_S</span> <span class="o">-</span> <span class="n">theta_bar</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">se_jack_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_jack_d</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">se_jack_d</span><span class="p">,</span> <span class="n">theta_hat</span><span class="p">,</span> <span class="n">theta_S</span>
</pre></div>
</div>
<div class="note admonition">
<p class="admonition-title">Example 💡 Delete-<span class="math notranslate nohighlight">\(d\)</span> for Median</p>
<p>The median is non-smooth: removing one observation may not change it. Let’s compare delete-1 and delete-<span class="math notranslate nohighlight">\(d\)</span> jackknife:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>  <span class="c1"># Skewed data</span>

<span class="k">def</span><span class="w"> </span><span class="nf">median_stat</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Delete-1 jackknife</span>
<span class="n">se_d1</span><span class="p">,</span> <span class="n">theta_hat</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">jackknife_se</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">median_stat</span><span class="p">)</span>

<span class="c1"># Delete-d jackknife with d = sqrt(n) ≈ 6</span>
<span class="n">d</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
<span class="n">se_dd</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">jackknife_delete_d</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">median_stat</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="n">d</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Bootstrap for comparison</span>
<span class="n">B</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">boot_medians</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
                         <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">)])</span>
<span class="n">se_boot</span> <span class="o">=</span> <span class="n">boot_medians</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample median: </span><span class="si">{</span><span class="n">theta_hat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Delete-1 jackknife SE: </span><span class="si">{</span><span class="n">se_d1</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Delete-</span><span class="si">{</span><span class="n">d</span><span class="si">}</span><span class="s2"> jackknife SE: </span><span class="si">{</span><span class="n">se_dd</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bootstrap SE: </span><span class="si">{</span><span class="n">se_boot</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Sample median: 1.6825
Delete-1 jackknife SE: 0.2891
Delete-6 jackknife SE: 0.3156
Bootstrap SE: 0.3284
</pre></div>
</div>
<p>For this non-smooth statistic, delete-<span class="math notranslate nohighlight">\(d\)</span> and bootstrap tend to agree better than delete-1.</p>
</div>
<figure class="align-center" id="id7">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_5_fig03_delete_d_jackknife.png"><img alt="Delete-d jackknife showing how larger d helps non-smooth statistics" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_5_fig03_delete_d_jackknife.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 161 </span><span class="caption-text"><strong>The Delete-d Jackknife.</strong> Panel (a) illustrates the concept: delete-1 removes single observations while delete-3 removes subsets of three. Panel (b) shows SE comparison for the sample mean (a smooth statistic)—all values of d agree closely with the classical SE. Panel (c) demonstrates SE for the median (non-smooth): delete-1 underestimates SE, while larger d moves closer to the bootstrap estimate. Panel (d) provides guidance on choosing d: use d=1 for smooth statistics, d≈√n for quantiles, and d=block size for dependent data.</span><a class="headerlink" href="#id7" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="jackknife-versus-bootstrap">
<h2>Jackknife versus Bootstrap<a class="headerlink" href="#jackknife-versus-bootstrap" title="Link to this heading"></a></h2>
<p>Both jackknife and bootstrap are plug-in resampling methods targeting the same inferential goals: standard errors, bias estimates, and confidence intervals. However, they differ fundamentally in approach and have distinct strengths.</p>
<section id="conceptual-comparison">
<h3>Conceptual Comparison<a class="headerlink" href="#conceptual-comparison" title="Link to this heading"></a></h3>
<table class="docutils align-default" id="id8">
<caption><span class="caption-number">Table 48 </span><span class="caption-text">Jackknife vs Bootstrap: Conceptual Differences</span><a class="headerlink" href="#id8" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 26.3%" />
<col style="width: 36.8%" />
<col style="width: 36.8%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Aspect</p></th>
<th class="head"><p>Jackknife</p></th>
<th class="head"><p>Bootstrap</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Perturbation</strong></p></td>
<td><p>Remove 1 (or <span class="math notranslate nohighlight">\(d\)</span>) observations</p></td>
<td><p>Resample entire dataset</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Samples</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(n\)</span> (or <span class="math notranslate nohighlight">\(\binom{n}{d}\)</span>) deterministic</p></td>
<td><p><span class="math notranslate nohighlight">\(B\)</span> stochastic</p></td>
</tr>
<tr class="row-even"><td><p><strong>Approximation</strong></p></td>
<td><p>First-order (linear)</p></td>
<td><p>Full distribution</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Replicate values</strong></p></td>
<td><p>Pseudovalues (synthetic)</p></td>
<td><p>Bootstrap statistics (actual)</p></td>
</tr>
<tr class="row-even"><td><p><strong>Target</strong></p></td>
<td><p>Variance, bias</p></td>
<td><p>Full sampling distribution</p></td>
</tr>
</tbody>
</table>
<p>The jackknife approximates sampling variability through a linearization, while the bootstrap directly simulates from <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>. For smooth statistics, both approaches yield consistent variance estimates; the bootstrap additionally provides the full shape of the sampling distribution.</p>
</section>
<section id="when-to-prefer-jackknife">
<h3>When to Prefer Jackknife<a class="headerlink" href="#when-to-prefer-jackknife" title="Link to this heading"></a></h3>
<ol class="arabic simple">
<li><p><strong>Smooth statistics</strong>: For means, regression coefficients, smooth M-estimators, and other differentiable functionals, jackknife and bootstrap agree, but jackknife is:</p>
<ul class="simple">
<li><p><strong>Faster</strong>: Only <span class="math notranslate nohighlight">\(n\)</span> computations vs. <span class="math notranslate nohighlight">\(B \gg n\)</span> for bootstrap</p></li>
<li><p><strong>Deterministic</strong>: No Monte Carlo error, no seed management</p></li>
<li><p><strong>Sufficient</strong>: SE is the primary need, not full distribution</p></li>
</ul>
</li>
<li><p><strong>Expensive statistics</strong>: When computing <span class="math notranslate nohighlight">\(T\)</span> is costly (complex optimization, large-scale computations), <span class="math notranslate nohighlight">\(n\)</span> jackknife evaluations may be feasible while <span class="math notranslate nohighlight">\(B = 5000\)</span> bootstrap evaluations are not.</p></li>
<li><p><strong>Influence diagnostics</strong>: Jackknife directly provides <span class="math notranslate nohighlight">\(\hat{\theta}_{(-i)}\)</span> and pseudovalues, revealing which observations most affect the estimate—useful for outlier detection and robustness assessment.</p></li>
<li><p><strong>BCa acceleration constant</strong>: The BCa bootstrap interval requires the acceleration constant <span class="math notranslate nohighlight">\(a\)</span>, computed from jackknife pseudovalues.</p></li>
</ol>
</section>
<section id="when-to-prefer-bootstrap">
<h3>When to Prefer Bootstrap<a class="headerlink" href="#when-to-prefer-bootstrap" title="Link to this heading"></a></h3>
<ol class="arabic simple">
<li><p><strong>Non-smooth statistics</strong>: Medians, quantiles, MAD, and other non-differentiable functionals are handled better by bootstrap. The jackknife can be unstable or inconsistent for these. (Note: Trimmed means are smoother than quantiles and jackknife often performs adequately for them, though bootstrap remains more robust in small samples.)</p></li>
<li><p><strong>Confidence intervals</strong>: Bootstrap provides multiple interval methods (percentile, basic, BCa, studentized) directly from the bootstrap distribution. Jackknife intervals typically rely on normal approximation: <span class="math notranslate nohighlight">\(\hat{\theta} \pm z_{\alpha/2} \cdot \widehat{\text{SE}}_{\text{jack}}\)</span>.</p></li>
<li><p><strong>Distributional shape</strong>: Bootstrap provides a histogram of <span class="math notranslate nohighlight">\(\hat{\theta}^*\)</span>, revealing skewness, heavy tails, or multimodality that affect interval accuracy.</p></li>
<li><p><strong>Hypothesis testing</strong>: Bootstrap p-values and permutation tests require resampling under the null—jackknife doesn’t provide this directly.</p></li>
<li><p><strong>Small samples with complex statistics</strong>: When <span class="math notranslate nohighlight">\(n\)</span> is small and the statistic is complex, bootstrap’s resampling may capture features that jackknife’s linear approximation misses.</p></li>
</ol>
</section>
<section id="when-neither-works-well">
<h3>When Neither Works Well<a class="headerlink" href="#when-neither-works-well" title="Link to this heading"></a></h3>
<p>Both <strong>nonparametric</strong> methods (jackknife and nonparametric bootstrap) struggle with:</p>
<ul class="simple">
<li><p><strong>Extreme quantiles</strong> (99th percentile, max, min): Nonparametric methods cannot exceed sample bounds. Parametric bootstrap with unbounded support <em>can</em> exceed these bounds and may perform better if the model is correct.</p></li>
<li><p><strong>Boundary parameters</strong>: E.g., estimating <span class="math notranslate nohighlight">\(\theta\)</span> in Uniform(0, <span class="math notranslate nohighlight">\(\theta\)</span>). Both nonparametric bootstrap and jackknife are constrained by the sample maximum.</p></li>
<li><p><strong>Non-identifiable parameters</strong>: Different <span class="math notranslate nohighlight">\(F\)</span> giving same <span class="math notranslate nohighlight">\(T(F)\)</span></p></li>
<li><p><strong>Highly discrete data with ties</strong>: May cause instability in both methods</p></li>
</ul>
<p>For these cases, consider parametric bootstrap (when model is credible), extreme value theory, m-out-of-n bootstrap, or subsampling.</p>
</section>
<section id="practical-recommendation">
<h3>Practical Recommendation<a class="headerlink" href="#practical-recommendation" title="Link to this heading"></a></h3>
<p><strong>Default strategy</strong>: Use bootstrap for general-purpose inference (it’s more flexible and provides more output). Use jackknife when:</p>
<ul class="simple">
<li><p>Computation is expensive and <span class="math notranslate nohighlight">\(T\)</span> is smooth</p></li>
<li><p>You need influence diagnostics</p></li>
<li><p>You need deterministic, reproducible results without Monte Carlo error</p></li>
<li><p>You’re computing the BCa acceleration constant</p></li>
</ul>
<p><strong>Cross-validation</strong>: When feasible, compute both and compare <span class="math notranslate nohighlight">\(\widehat{\text{SE}}_{\text{jack}}\)</span> with <span class="math notranslate nohighlight">\(\widehat{\text{SE}}_{\text{boot}}\)</span>. If they agree (within ~10-20%), either is trustworthy. If they disagree substantially:</p>
<ul class="simple">
<li><p>For smooth statistics: investigate outliers using pseudovalues</p></li>
<li><p>For non-smooth statistics: trust bootstrap</p></li>
<li><p>Consider whether the statistic is appropriate for the data</p></li>
</ul>
</section>
<section id="python-comparison">
<h3>Python Comparison<a class="headerlink" href="#python-comparison" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">compare_jackknife_bootstrap</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">statistic</span><span class="p">,</span> <span class="n">B</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare jackknife and bootstrap standard errors.&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="c1"># Jackknife</span>
    <span class="n">se_jack</span><span class="p">,</span> <span class="n">theta_hat</span><span class="p">,</span> <span class="n">theta_i</span> <span class="o">=</span> <span class="n">jackknife_se</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">statistic</span><span class="p">)</span>

    <span class="c1"># Bootstrap</span>
    <span class="n">theta_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
        <span class="n">theta_star</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">statistic</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
    <span class="n">se_boot</span> <span class="o">=</span> <span class="n">theta_star</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;theta_hat&#39;</span><span class="p">:</span> <span class="n">theta_hat</span><span class="p">,</span>
        <span class="s1">&#39;se_jack&#39;</span><span class="p">:</span> <span class="n">se_jack</span><span class="p">,</span>
        <span class="s1">&#39;se_boot&#39;</span><span class="p">:</span> <span class="n">se_boot</span><span class="p">,</span>
        <span class="s1">&#39;ratio&#39;</span><span class="p">:</span> <span class="n">se_jack</span> <span class="o">/</span> <span class="n">se_boot</span><span class="p">,</span>
        <span class="s1">&#39;theta_i&#39;</span><span class="p">:</span> <span class="n">theta_i</span><span class="p">,</span>
        <span class="s1">&#39;theta_star&#39;</span><span class="p">:</span> <span class="n">theta_star</span>
    <span class="p">}</span>
</pre></div>
</div>
<div class="note admonition">
<p class="admonition-title">Example 💡 Comparing Methods for Correlation</p>
<p>The sample correlation is smooth (away from ±1), so jackknife and bootstrap should agree:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">80</span>
<span class="n">rho</span> <span class="o">=</span> <span class="mf">0.6</span>

<span class="c1"># Generate correlated data</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="n">rho</span><span class="p">],</span> <span class="p">[</span><span class="n">rho</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">xy</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">correlation</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">compare_jackknife_bootstrap</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">correlation</span><span class="p">,</span> <span class="n">B</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample correlation: </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;theta_hat&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Jackknife SE: </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;se_jack&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bootstrap SE: </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;se_boot&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ratio (jack/boot): </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;ratio&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Sample correlation: 0.5821
Jackknife SE: 0.0803
Bootstrap SE: 0.0812
Ratio (jack/boot): 0.989
</pre></div>
</div>
<p>The methods agree closely, as expected for a smooth statistic.</p>
</div>
<figure class="align-center" id="id9">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_5_fig04_jackknife_vs_bootstrap.png"><img alt="Side-by-side comparison of jackknife and bootstrap for mean versus median" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_5_fig04_jackknife_vs_bootstrap.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 162 </span><span class="caption-text"><strong>Jackknife vs Bootstrap: Agreement Depends on Smoothness.</strong> Top row: For the sample mean (smooth statistic), jackknife pseudovalues (a) and bootstrap distribution (b) yield nearly identical SE estimates with ratio ≈ 1.0 (c). Bottom row: For the sample median (non-smooth), jackknife pseudovalues (d) are more concentrated than the bootstrap distribution (e), resulting in SE underestimation with ratio ≈ 0.8 (f). This illustrates the key limitation: jackknife is most reliable for smooth statistics and can be unstable for non-smooth functionals.</span><a class="headerlink" href="#id9" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="the-infinitesimal-jackknife">
<h2>The Infinitesimal Jackknife<a class="headerlink" href="#the-infinitesimal-jackknife" title="Link to this heading"></a></h2>
<p>The <strong>infinitesimal jackknife</strong> (IJ) provides the theoretical foundation connecting the jackknife to influence functions. While less commonly used directly, it illuminates why the jackknife works and connects to robust statistics.</p>
<section id="concept">
<h3>Concept<a class="headerlink" href="#concept" title="Link to this heading"></a></h3>
<p>Instead of removing observations entirely, the infinitesimal jackknife considers the effect of <strong>weighting</strong> observations. If we compute <span class="math notranslate nohighlight">\(\hat{\theta}_\mathbf{w} = T(\mathbf{w})\)</span> where <span class="math notranslate nohighlight">\(\mathbf{w} = (w_1, \ldots, w_n)\)</span> weights the empirical distribution (with <span class="math notranslate nohighlight">\(\sum_i w_i = 1\)</span>), then the IJ examines how <span class="math notranslate nohighlight">\(\hat{\theta}_\mathbf{w}\)</span> changes as we perturb the weights.</p>
<p>For the uniform weights <span class="math notranslate nohighlight">\(\mathbf{w}_0 = (1/n, \ldots, 1/n)\)</span> (corresponding to <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>), the influence of observation <span class="math notranslate nohighlight">\(i\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\text{IJ}_i = n \cdot \left.\frac{\partial \hat{\theta}_\mathbf{w}}{\partial w_i}\right|_{\mathbf{w} = \mathbf{w}_0}\]</div>
<p>The IJ variance estimator is:</p>
<div class="math notranslate nohighlight">
\[\widehat{\text{Var}}_{\text{IJ}} = \frac{1}{n} \sum_{i=1}^n \text{IJ}_i^2\]</div>
</section>
<section id="connection-to-delete-1-jackknife">
<h3>Connection to Delete-1 Jackknife<a class="headerlink" href="#connection-to-delete-1-jackknife" title="Link to this heading"></a></h3>
<p>For smooth statistics, the delete-1 jackknife approximates the infinitesimal jackknife:</p>
<div class="math notranslate nohighlight">
\[(n-1)(\hat{\theta} - \hat{\theta}_{(-i)}) \approx \text{IJ}_i\]</div>
<p>(The exact scaling constant varies by convention; some sources use <span class="math notranslate nohighlight">\(n\)</span> rather than <span class="math notranslate nohighlight">\(n-1\)</span>.) This approximation improves as <span class="math notranslate nohighlight">\(n\)</span> grows, explaining the asymptotic equivalence of jackknife and influence-function-based variance estimation.</p>
</section>
<section id="practical-use">
<h3>Practical Use<a class="headerlink" href="#practical-use" title="Link to this heading"></a></h3>
<p>The infinitesimal jackknife is primarily of theoretical interest, but it has practical applications:</p>
<ol class="arabic simple">
<li><p><strong>Random forests</strong>: The IJ provides variance estimates for predictions from ensemble methods where explicit leave-one-out is expensive.</p></li>
<li><p><strong>Robust statistics</strong>: Understanding which observations have large <span class="math notranslate nohighlight">\(\text{IJ}_i\)</span> helps identify influential points.</p></li>
<li><p><strong>Computational shortcuts</strong>: For some models (e.g., linear regression with hat matrix), IJ can be computed without refitting.</p></li>
</ol>
<figure class="align-center" id="id10">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_5_fig07_jackknife_after_bootstrap.png"><img alt="Computing the BCa acceleration constant from jackknife influence values" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_5_fig07_jackknife_after_bootstrap.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 163 </span><span class="caption-text"><strong>Jackknife-After-Bootstrap: Computing the BCa Acceleration Constant.</strong> Panel (a) shows the jackknife influence values <span class="math notranslate nohighlight">\(\bar{\theta}_{(\cdot)} - \hat{\theta}_{(-i)}\)</span> for each observation, revealing the skewness that the acceleration constant captures. Panel (b) derives the acceleration constant formula from these influence values, showing numerical computation. Panel (c) displays the bootstrap distribution with both percentile CI (shaded region) and BCa CI (dashed lines), demonstrating how the acceleration adjustment shifts the interval to improve coverage. Panel (d) summarizes how jackknife enables better bootstrap intervals by quantifying the SE-parameter relationship.</span><a class="headerlink" href="#id10" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="practical-considerations">
<h2>Practical Considerations<a class="headerlink" href="#practical-considerations" title="Link to this heading"></a></h2>
<section id="computational-efficiency">
<h3>Computational Efficiency<a class="headerlink" href="#computational-efficiency" title="Link to this heading"></a></h3>
<p><strong>Vectorization</strong>: For simple statistics, vectorize across leave-one-out samples:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">jackknife_mean_fast</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Fast jackknife SE for the mean using vectorization.&quot;&quot;&quot;</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">total</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="c1"># Leave-one-out means: (total - x_i) / (n - 1)</span>
    <span class="n">theta_i</span> <span class="o">=</span> <span class="p">(</span><span class="n">total</span> <span class="o">-</span> <span class="n">data</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">theta_bar</span> <span class="o">=</span> <span class="n">theta_i</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="n">se_jack</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(((</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">theta_i</span> <span class="o">-</span> <span class="n">theta_bar</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">se_jack</span><span class="p">,</span> <span class="n">total</span> <span class="o">/</span> <span class="n">n</span><span class="p">,</span> <span class="n">theta_i</span>
</pre></div>
</div>
<p><strong>Regression shortcuts</strong>: For OLS, leave-one-out residuals can be computed via the hat matrix without refitting:</p>
<div class="math notranslate nohighlight">
\[e_{(-i)} = \frac{e_i}{1 - h_{ii}}\]</div>
<p>where <span class="math notranslate nohighlight">\(h_{ii}\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>-th diagonal element of <span class="math notranslate nohighlight">\(H = X(X^\top X)^{-1}X^\top\)</span>.</p>
<p><strong>Parallel computation</strong>: For expensive statistics, distribute leave-one-out computations across cores.</p>
</section>
<section id="diagnostics">
<h3>Diagnostics<a class="headerlink" href="#diagnostics" title="Link to this heading"></a></h3>
<p>The jackknife provides unique diagnostic capabilities:</p>
<ol class="arabic simple">
<li><p><strong>Influential observations</strong>: Large <span class="math notranslate nohighlight">\(|\hat{\theta} - \hat{\theta}_{(-i)}|\)</span> indicates observation <span class="math notranslate nohighlight">\(i\)</span> strongly affects the estimate.</p></li>
<li><p><strong>Pseudovalue outliers</strong>: Pseudovalues far from <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> suggest potential data problems or model issues.</p></li>
<li><p><strong>Stability assessment</strong>: Plot <span class="math notranslate nohighlight">\(\hat{\theta}_{(-i)}\)</span> vs. <span class="math notranslate nohighlight">\(i\)</span> to visualize how the estimate changes as each observation is removed.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">jackknife_diagnostics</span><span class="p">(</span><span class="n">theta_hat</span><span class="p">,</span> <span class="n">theta_i</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">)):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create diagnostic plots for jackknife analysis.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">theta_i</span><span class="p">)</span>
    <span class="n">pv</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="n">theta_hat</span> <span class="o">-</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">theta_i</span>
    <span class="n">influence</span> <span class="o">=</span> <span class="n">theta_hat</span> <span class="o">-</span> <span class="n">theta_i</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>

    <span class="c1"># Leave-one-out estimates</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">theta_i</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">theta_hat</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\hat{\theta}$&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Observation index&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\hat{\theta}_{(-i)}$&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Leave-one-out estimates&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="c1"># Influence</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">influence</span><span class="p">,</span> <span class="n">basefmt</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Observation index&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\hat{\theta} - \hat{\theta}_{(-i)}$&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Influence of each observation&#39;</span><span class="p">)</span>

    <span class="c1"># Pseudovalues</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">pv</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">theta_hat</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\hat{\theta}$&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Pseudovalue&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Pseudovalue distribution&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">fig</span>
</pre></div>
</div>
<figure class="align-center" id="id11">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_5_fig05_influence_diagnostics.png"><img alt="Jackknife diagnostics for identifying influential observations including outlier detection" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_5_fig05_influence_diagnostics.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 164 </span><span class="caption-text"><strong>Jackknife Influence Diagnostics.</strong> Panel (a) shows data with an outlier clearly visible. Panel (b) plots leave-one-out estimates—the outlier’s removal causes the largest change in <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>. Panel (c) displays influence <span class="math notranslate nohighlight">\(\hat{\theta} - \hat{\theta}_{(-i)}\)</span> for each observation; the outlier has by far the largest influence. Panel (d) shows the pseudovalue distribution; the outlier’s pseudovalue is an extreme outlier. Panel (e) compares jackknife SE with and without the outlier. Panel (f) provides guidance on using jackknife for diagnostics: flag observations with influence exceeding 2×SE or pseudovalues beyond 3σ.</span><a class="headerlink" href="#id11" title="Link to this image"></a></p>
</figcaption>
</figure>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ⚠️</p>
<p><strong>Jackknife for non-smooth statistics</strong>: The jackknife can fail dramatically for non-smooth statistics. For the sample median, if several observations equal the median, removing one may not change <span class="math notranslate nohighlight">\(\hat{\theta}_{(-i)}\)</span> at all, leading to underestimated SE. Always verify with bootstrap for non-smooth functionals.</p>
</div>
</section>
</section>
<section id="bringing-it-all-together">
<h2>Bringing It All Together<a class="headerlink" href="#bringing-it-all-together" title="Link to this heading"></a></h2>
<p>The jackknife is a deterministic resampling method that provides fast, reliable inference for smooth statistics. Its leave-one-out structure naturally connects to influence functions, providing both variance estimates and diagnostic insights about observation influence.</p>
<p><strong>Key decision points</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Is the statistic smooth?</strong> If yes, jackknife is efficient and reliable. If no (quantiles, modes, maxima), prefer bootstrap or delete-<span class="math notranslate nohighlight">\(d\)</span> jackknife.</p></li>
<li><p><strong>Is computation expensive?</strong> If yes and <span class="math notranslate nohighlight">\(T\)</span> is smooth, jackknife’s <span class="math notranslate nohighlight">\(n\)</span> evaluations may be preferable to bootstrap’s <span class="math notranslate nohighlight">\(B \gg n\)</span> evaluations.</p></li>
<li><p><strong>Do you need more than SE?</strong> Confidence intervals, hypothesis tests, and distributional shape are better served by bootstrap.</p></li>
<li><p><strong>Do you need diagnostics?</strong> Jackknife’s leave-one-out structure provides unique insight into observation influence.</p></li>
</ol>
<figure class="align-center" id="id12">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_5_fig06_method_selection.png"><img alt="Decision flowchart for choosing between jackknife and bootstrap methods" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_5_fig06_method_selection.png" style="width: 90%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 165 </span><span class="caption-text"><strong>Method Selection Guide.</strong> This decision flowchart guides the choice between jackknife and bootstrap. The key questions are: (1) Is the statistic smooth? If no, use bootstrap. (2) Is computation expensive? If yes and smooth, jackknife is preferable. (3) Do you need CI or full distribution? If yes, use bootstrap. (4) Do you only need SE? Either method works, but jackknife is faster. Best practice: compute both as a sensitivity check when feasible.</span><a class="headerlink" href="#id12" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>The jackknife-bootstrap relationship</strong>: These methods are complementary. Use jackknife for efficiency and diagnostics, bootstrap for flexibility and intervals. When both are feasible, computing both provides a robustness check—agreement confirms reliability, disagreement warrants investigation.</p>
<figure class="align-center" id="id13">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_5_fig08_summary_table.png"><img alt="Complete comparison table of jackknife versus bootstrap methods" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_5_fig08_summary_table.png" style="width: 85%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 166 </span><span class="caption-text"><strong>Jackknife vs Bootstrap: Complete Comparison.</strong> This summary table compares all key aspects of the two methods. Jackknife excels for smooth statistics, expensive computations, reproducibility, and influence diagnostics. Bootstrap excels for non-smooth statistics, confidence intervals, and obtaining full distributional information. The BCa acceleration constant bridges both methods—computed from jackknife but used to improve bootstrap intervals.</span><a class="headerlink" href="#id13" title="Link to this image"></a></p>
</figcaption>
</figure>
<div class="important admonition">
<p class="admonition-title">Key Takeaways 📝</p>
<ol class="arabic simple">
<li><p><strong>Core method</strong>: The delete-1 jackknife computes leave-one-out estimates <span class="math notranslate nohighlight">\(\hat{\theta}_{(-i)}\)</span> and derives SE from their variability using <span class="math notranslate nohighlight">\(\widehat{\text{SE}}_{\text{jack}} = \sqrt{((n-1)/n) \sum (\hat{\theta}_{(-i)} - \bar{\theta}_{(\cdot)})^2}\)</span>.</p></li>
<li><p><strong>Bias correction</strong>: Jackknife bias estimate is <span class="math notranslate nohighlight">\((n-1)(\bar{\theta}_{(\cdot)} - \hat{\theta})\)</span>; bias-corrected estimate is the mean of pseudovalues.</p></li>
<li><p><strong>Delete-</strong><span class="math notranslate nohighlight">\(d\)</span> <strong>extension</strong>: For non-smooth statistics or block-dependent data, delete-<span class="math notranslate nohighlight">\(d\)</span> with <span class="math notranslate nohighlight">\(d &gt; 1\)</span> may improve reliability.</p></li>
<li><p><strong>Comparison with bootstrap</strong>: Jackknife is faster and deterministic for smooth statistics; bootstrap is more flexible and provides full distributional information.</p></li>
<li><p><strong>Outcome alignment</strong>: This section addresses LO 3 (implement resampling for variability assessment) and connects to the influence function theory underlying robust statistics.</p></li>
</ol>
</div>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Link to this heading"></a></h2>
<p>These exercises develop your understanding of jackknife methods from basic implementation through comparison with bootstrap and recognition of failure modes.</p>
<div class="exercise admonition">
<p class="admonition-title">Exercise 4.5.1: Jackknife SE Equals Classical SE for the Mean</p>
<p>Prove algebraically that for the sample mean <span class="math notranslate nohighlight">\(\bar{X}\)</span>, the jackknife SE exactly equals the classical formula <span class="math notranslate nohighlight">\(s/\sqrt{n}\)</span>.</p>
<div class="note admonition">
<p class="admonition-title">Background: Why This Matters</p>
<p>This identity verifies that the jackknife correctly recovers known results for simple statistics. It also provides insight into the <span class="math notranslate nohighlight">\((n-1)/n\)</span> factor in the jackknife formula.</p>
</div>
<ol class="loweralpha simple">
<li><p>Show that for <span class="math notranslate nohighlight">\(\hat{\theta} = \bar{X}\)</span>, the leave-one-out estimate is <span class="math notranslate nohighlight">\(\bar{X}_{(-i)} = \frac{n\bar{X} - X_i}{n-1}\)</span>.</p></li>
<li><p>Compute <span class="math notranslate nohighlight">\(\bar{X}_{(-i)} - \bar{\theta}_{(\cdot)}\)</span> where <span class="math notranslate nohighlight">\(\bar{\theta}_{(\cdot)} = \frac{1}{n}\sum_{i=1}^n \bar{X}_{(-i)}\)</span>.</p></li>
<li><p>Substitute into the jackknife SE formula and simplify to show <span class="math notranslate nohighlight">\(\widehat{\text{SE}}_{\text{jack}} = s/\sqrt{n}\)</span>.</p></li>
<li><p>Verify numerically with simulated data.</p></li>
</ol>
<div class="tip admonition">
<p class="admonition-title">Hint</p>
<p>For part (b), show that <span class="math notranslate nohighlight">\(\bar{\theta}_{(\cdot)} = \bar{X}\)</span> by expanding the definition. Then <span class="math notranslate nohighlight">\(\bar{X}_{(-i)} - \bar{X} = -\frac{X_i - \bar{X}}{n-1}\)</span>.</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Leave-one-out mean</strong></p>
<div class="tip admonition">
<p class="admonition-title">Step 1: Derive the formula</p>
<p class="sd-card-text">The sum of all observations is <span class="math notranslate nohighlight">\(\sum_{j=1}^n X_j = n\bar{X}\)</span>. Removing <span class="math notranslate nohighlight">\(X_i\)</span> gives sum <span class="math notranslate nohighlight">\(n\bar{X} - X_i\)</span> over <span class="math notranslate nohighlight">\(n-1\)</span> observations.</p>
</div>
<div class="math notranslate nohighlight">
\[\bar{X}_{(-i)} = \frac{\sum_{j \neq i} X_j}{n-1} = \frac{n\bar{X} - X_i}{n-1}\]</div>
<p class="sd-card-text"><strong>Part (b): Deviation from jackknife average</strong></p>
<p class="sd-card-text">First, compute <span class="math notranslate nohighlight">\(\bar{\theta}_{(\cdot)}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\bar{\theta}_{(\cdot)} &amp;= \frac{1}{n}\sum_{i=1}^n \bar{X}_{(-i)} = \frac{1}{n}\sum_{i=1}^n \frac{n\bar{X} - X_i}{n-1} \\
&amp;= \frac{1}{n(n-1)}\left(n \cdot n\bar{X} - \sum_{i=1}^n X_i\right) \\
&amp;= \frac{1}{n(n-1)}\left(n^2\bar{X} - n\bar{X}\right) = \frac{n\bar{X}(n-1)}{n(n-1)} = \bar{X}\end{split}\]</div>
<p class="sd-card-text">Therefore:</p>
<div class="math notranslate nohighlight">
\[\bar{X}_{(-i)} - \bar{\theta}_{(\cdot)} = \frac{n\bar{X} - X_i}{n-1} - \bar{X} = \frac{n\bar{X} - X_i - (n-1)\bar{X}}{n-1} = \frac{\bar{X} - X_i}{n-1} = -\frac{X_i - \bar{X}}{n-1}\]</div>
<p class="sd-card-text"><strong>Part (c): Jackknife SE formula</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}\widehat{\text{SE}}_{\text{jack}}^2 &amp;= \frac{n-1}{n}\sum_{i=1}^n \left(\bar{X}_{(-i)} - \bar{\theta}_{(\cdot)}\right)^2 \\
&amp;= \frac{n-1}{n}\sum_{i=1}^n \frac{(X_i - \bar{X})^2}{(n-1)^2} \\
&amp;= \frac{1}{n(n-1)}\sum_{i=1}^n (X_i - \bar{X})^2 \\
&amp;= \frac{1}{n(n-1)} \cdot (n-1) s^2 = \frac{s^2}{n}\end{split}\]</div>
<p class="sd-card-text">Taking square root: <span class="math notranslate nohighlight">\(\widehat{\text{SE}}_{\text{jack}} = s/\sqrt{n}\)</span>. ∎</p>
<p class="sd-card-text"><strong>Part (d): Numerical verification</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="c1"># Jackknife</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">total</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">theta_i</span> <span class="o">=</span> <span class="p">(</span><span class="n">total</span> <span class="o">-</span> <span class="n">data</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">theta_bar</span> <span class="o">=</span> <span class="n">theta_i</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">se_jack</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(((</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">theta_i</span> <span class="o">-</span> <span class="n">theta_bar</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

<span class="c1"># Classical</span>
<span class="n">se_classical</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Jackknife SE:  </span><span class="si">{</span><span class="n">se_jack</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Classical SE:  </span><span class="si">{</span><span class="n">se_classical</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Difference:    </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">se_jack</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">se_classical</span><span class="p">)</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Jackknife SE:  0.192749
Classical SE:  0.192749
Difference:    0.00e+00
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text">The jackknife exactly recovers the classical SE for the mean—this is a mathematical identity, not an approximation.</p></li>
<li><p class="sd-card-text">The <span class="math notranslate nohighlight">\((n-1)/n\)</span> factor in the jackknife formula precisely compensates for the <span class="math notranslate nohighlight">\(1/(n-1)^2\)</span> from the leave-one-out deviations.</p></li>
<li><p class="sd-card-text">This identity provides confidence that the jackknife formula is correctly calibrated.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 4.5.2: Jackknife Bias Correction</p>
<p>Investigate jackknife bias correction for the biased variance estimator and an exponential mean estimator.</p>
<div class="note admonition">
<p class="admonition-title">Background: Bias of Order 1/n</p>
<p>Many common estimators have bias of order <span class="math notranslate nohighlight">\(O(1/n)\)</span>. The jackknife can detect and correct such bias. When bias is <span class="math notranslate nohighlight">\(O(1/n^2)\)</span> or smaller, jackknife bias correction is unnecessary and may increase variance.</p>
</div>
<ol class="loweralpha simple">
<li><p>For the biased variance estimator <span class="math notranslate nohighlight">\(\hat{\sigma}^2 = \frac{1}{n}\sum_i(X_i - \bar{X})^2\)</span>, verify that jackknife bias correction recovers the unbiased estimator <span class="math notranslate nohighlight">\(s^2\)</span>.</p></li>
<li><p>Generate <span class="math notranslate nohighlight">\(n = 30\)</span> observations from Exponential(<span class="math notranslate nohighlight">\(\lambda = 2\)</span>). The MLE for <span class="math notranslate nohighlight">\(\lambda\)</span> is <span class="math notranslate nohighlight">\(\hat{\lambda} = 1/\bar{X}\)</span>, which has positive bias. Compute the jackknife bias estimate and compare with the theoretical bias <span class="math notranslate nohighlight">\(\lambda/(n-1)\)</span>.</p></li>
<li><p>For the exponential MLE, compare the MSE of <span class="math notranslate nohighlight">\(\hat{\lambda}\)</span> and <span class="math notranslate nohighlight">\(\hat{\lambda}^{\text{jack-bc}}\)</span> via simulation.</p></li>
<li><p>Discuss when bias correction is worthwhile.</p></li>
</ol>
<div class="tip admonition">
<p class="admonition-title">Hint</p>
<p>The MLE <span class="math notranslate nohighlight">\(\hat{\lambda} = 1/\bar{X}\)</span> is biased because <span class="math notranslate nohighlight">\(\mathbb{E}[1/\bar{X}] \neq 1/\mathbb{E}[\bar{X}]\)</span> (Jensen’s inequality). The exact bias formula for the exponential is <span class="math notranslate nohighlight">\(\mathbb{E}[\hat{\lambda}] = \lambda n/(n-1)\)</span>, giving bias <span class="math notranslate nohighlight">\(\lambda/(n-1)\)</span>.</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Variance estimator</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">biased_var</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Jackknife bias correction</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">theta_hat</span> <span class="o">=</span> <span class="n">biased_var</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">theta_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">biased_var</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">i</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)])</span>
<span class="n">theta_bar</span> <span class="o">=</span> <span class="n">theta_i</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">bias_jack</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta_bar</span> <span class="o">-</span> <span class="n">theta_hat</span><span class="p">)</span>
<span class="n">theta_bc</span> <span class="o">=</span> <span class="n">theta_hat</span> <span class="o">-</span> <span class="n">bias_jack</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Biased estimate (÷n):     </span><span class="si">{</span><span class="n">theta_hat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Jackknife bias:           </span><span class="si">{</span><span class="n">bias_jack</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bias-corrected estimate:  </span><span class="si">{</span><span class="n">theta_bc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unbiased estimate (÷n-1): </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Biased estimate (÷n):     3.6821
Jackknife bias:           -0.1269
Bias-corrected estimate:  3.8090
Unbiased estimate (÷n-1): 3.8090
</pre></div>
</div>
<p class="sd-card-text">The jackknife exactly recovers the unbiased variance estimator.</p>
<p class="sd-card-text"><strong>Part (b): Exponential MLE bias</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">lambda_true</span> <span class="o">=</span> <span class="mf">2.0</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">lambda_true</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">exp_mle</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">lambda_hat</span> <span class="o">=</span> <span class="n">exp_mle</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">theta_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">exp_mle</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">i</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)])</span>
<span class="n">theta_bar</span> <span class="o">=</span> <span class="n">theta_i</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">bias_jack</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta_bar</span> <span class="o">-</span> <span class="n">lambda_hat</span><span class="p">)</span>
<span class="n">bias_theory</span> <span class="o">=</span> <span class="n">lambda_true</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Theoretical bias</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MLE λ̂:             </span><span class="si">{</span><span class="n">lambda_hat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Jackknife bias:     </span><span class="si">{</span><span class="n">bias_jack</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Theoretical bias:   </span><span class="si">{</span><span class="n">bias_theory</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;(using true λ=</span><span class="si">{</span><span class="n">lambda_true</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>MLE λ̂:             2.2118
Jackknife bias:     0.0765
Theoretical bias:   0.0690
(using true λ=2.0)
</pre></div>
</div>
<p class="sd-card-text">The jackknife bias estimate is close to the theoretical value.</p>
<p class="sd-card-text"><strong>Part (c): MSE comparison via simulation</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">n_sims</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">lambda_true</span> <span class="o">=</span> <span class="mf">2.0</span>

<span class="n">mle_estimates</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">bc_estimates</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sims</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">lambda_true</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">lambda_hat</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">data</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="c1"># Jackknife bias correction</span>
    <span class="n">theta_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)])</span>
    <span class="n">theta_bar</span> <span class="o">=</span> <span class="n">theta_i</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">bias_jack</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta_bar</span> <span class="o">-</span> <span class="n">lambda_hat</span><span class="p">)</span>
    <span class="n">lambda_bc</span> <span class="o">=</span> <span class="n">lambda_hat</span> <span class="o">-</span> <span class="n">bias_jack</span>

    <span class="n">mle_estimates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lambda_hat</span><span class="p">)</span>
    <span class="n">bc_estimates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lambda_bc</span><span class="p">)</span>

<span class="n">mle_estimates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mle_estimates</span><span class="p">)</span>
<span class="n">bc_estimates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">bc_estimates</span><span class="p">)</span>

<span class="c1"># MSE decomposition</span>
<span class="n">bias_mle</span> <span class="o">=</span> <span class="n">mle_estimates</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">-</span> <span class="n">lambda_true</span>
<span class="n">var_mle</span> <span class="o">=</span> <span class="n">mle_estimates</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">mse_mle</span> <span class="o">=</span> <span class="n">bias_mle</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">var_mle</span>

<span class="n">bias_bc</span> <span class="o">=</span> <span class="n">bc_estimates</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">-</span> <span class="n">lambda_true</span>
<span class="n">var_bc</span> <span class="o">=</span> <span class="n">bc_estimates</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">mse_bc</span> <span class="o">=</span> <span class="n">bias_bc</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">var_bc</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MLE estimates:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Bias: </span><span class="si">{</span><span class="n">bias_mle</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Variance: </span><span class="si">{</span><span class="n">var_mle</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, MSE: </span><span class="si">{</span><span class="n">mse_mle</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Bias-corrected estimates:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Bias: </span><span class="si">{</span><span class="n">bias_bc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Variance: </span><span class="si">{</span><span class="n">var_bc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, MSE: </span><span class="si">{</span><span class="n">mse_bc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">MSE ratio (BC/MLE): </span><span class="si">{</span><span class="n">mse_bc</span><span class="o">/</span><span class="n">mse_mle</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>MLE estimates:
  Bias: 0.0689, Variance: 0.0563, MSE: 0.0611

Bias-corrected estimates:
  Bias: -0.0005, Variance: 0.0637, MSE: 0.0637

MSE ratio (BC/MLE): 1.042
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): Discussion</strong></p>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text">Bias correction successfully removes bias (from 0.069 to ~0).</p></li>
<li><p class="sd-card-text">However, variance increases (from 0.056 to 0.064).</p></li>
<li><p class="sd-card-text">MSE is actually <em>higher</em> for the bias-corrected estimator in this case.</p></li>
<li><p class="sd-card-text"><strong>When to use bias correction</strong>:</p>
<ul class="simple">
<li><p class="sd-card-text">When bias &gt;&gt; SE (bias correction helps)</p></li>
<li><p class="sd-card-text">When n is large (variance inflation is smaller)</p></li>
<li><p class="sd-card-text">When unbiasedness is required (regulatory, comparison purposes)</p></li>
</ul>
</li>
<li><p class="sd-card-text"><strong>When to avoid</strong>:</p>
<ul class="simple">
<li><p class="sd-card-text">When bias &lt;&lt; SE (correction just adds noise)</p></li>
<li><p class="sd-card-text">When MSE is the primary criterion</p></li>
<li><p class="sd-card-text">For small n with moderate bias</p></li>
</ul>
</li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 4.5.3: Jackknife vs Bootstrap Comparison</p>
<p>Compare jackknife and bootstrap for smooth and non-smooth statistics.</p>
<div class="note admonition">
<p class="admonition-title">Background: Method Agreement</p>
<p>When both methods agree, we have confidence in the result. When they disagree, the disagreement itself is informative—typically indicating non-smoothness or influential observations.</p>
</div>
<ol class="loweralpha simple">
<li><p>Generate <span class="math notranslate nohighlight">\(n = 50\)</span> observations from a skewed distribution (e.g., Exponential(1)). Compare jackknife and bootstrap SE for the sample mean.</p></li>
<li><p>Using the same data, compare jackknife and bootstrap SE for the sample median.</p></li>
<li><p>Generate data with one outlier (e.g., 49 observations from N(0,1) plus one observation at 10). Compare methods for the mean and the 20% trimmed mean.</p></li>
<li><p>Summarize your findings about when the methods agree/disagree.</p></li>
</ol>
<div class="tip admonition">
<p class="admonition-title">Hint</p>
<p>For the trimmed mean, use <code class="docutils literal notranslate"><span class="pre">scipy.stats.trim_mean(x,</span> <span class="pre">0.2)</span></code> which removes the smallest and largest 20% of observations before computing the mean.</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Sample mean (smooth)</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># Jackknife SE for mean</span>
<span class="n">theta_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([(</span><span class="n">data</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">-</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)])</span>
<span class="n">theta_bar</span> <span class="o">=</span> <span class="n">theta_i</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">se_jack</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(((</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">theta_i</span> <span class="o">-</span> <span class="n">theta_bar</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

<span class="c1"># Bootstrap SE for mean</span>
<span class="n">B</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">boot_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">)])</span>
<span class="n">se_boot</span> <span class="o">=</span> <span class="n">boot_means</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sample Mean (smooth statistic):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Jackknife SE: </span><span class="si">{</span><span class="n">se_jack</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Bootstrap SE: </span><span class="si">{</span><span class="n">se_boot</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Ratio: </span><span class="si">{</span><span class="n">se_jack</span><span class="o">/</span><span class="n">se_boot</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Sample Mean (smooth statistic):
  Jackknife SE: 0.1298
  Bootstrap SE: 0.1316
  Ratio: 0.986
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Sample median (non-smooth)</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Jackknife SE for median</span>
<span class="n">theta_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">i</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)])</span>
<span class="n">theta_bar</span> <span class="o">=</span> <span class="n">theta_i</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">se_jack_med</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(((</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">theta_i</span> <span class="o">-</span> <span class="n">theta_bar</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

<span class="c1"># Bootstrap SE for median</span>
<span class="n">boot_medians</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">)])</span>
<span class="n">se_boot_med</span> <span class="o">=</span> <span class="n">boot_medians</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Sample Median (non-smooth statistic):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Jackknife SE: </span><span class="si">{</span><span class="n">se_jack_med</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Bootstrap SE: </span><span class="si">{</span><span class="n">se_boot_med</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Ratio: </span><span class="si">{</span><span class="n">se_jack_med</span><span class="o">/</span><span class="n">se_boot_med</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Sample Median (non-smooth statistic):
  Jackknife SE: 0.1142
  Bootstrap SE: 0.1387
  Ratio: 0.823
</pre></div>
</div>
<p class="sd-card-text">The jackknife underestimates SE for the median.</p>
<p class="sd-card-text"><strong>Part (c): Data with outlier</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Data with outlier</span>
<span class="n">data_outlier</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">49</span><span class="p">),</span> <span class="p">[</span><span class="mf">10.0</span><span class="p">]])</span>
<span class="n">n_out</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_outlier</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">trimmed_mean</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">stats</span><span class="o">.</span><span class="n">trim_mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>

<span class="c1"># Mean - jackknife vs bootstrap</span>
<span class="n">theta_i_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">data_outlier</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_out</span><span class="p">)])</span>
<span class="n">se_jack_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(((</span><span class="n">n_out</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">n_out</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">theta_i_mean</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">n_out</span><span class="p">)</span>
<span class="n">boot_means_out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">data_outlier</span><span class="p">,</span> <span class="n">n_out</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">)])</span>
<span class="n">se_boot_mean_out</span> <span class="o">=</span> <span class="n">boot_means_out</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Trimmed mean - jackknife vs bootstrap</span>
<span class="n">theta_i_trim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">trimmed_mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">data_outlier</span><span class="p">,</span> <span class="n">i</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_out</span><span class="p">)])</span>
<span class="n">se_jack_trim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(((</span><span class="n">n_out</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">n_out</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">theta_i_trim</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">n_out</span><span class="p">)</span>
<span class="n">boot_trim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">trimmed_mean</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">data_outlier</span><span class="p">,</span> <span class="n">n_out</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">)])</span>
<span class="n">se_boot_trim</span> <span class="o">=</span> <span class="n">boot_trim</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Data with outlier (49 N(0,1) + one value at 10):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean - Jackknife SE: </span><span class="si">{</span><span class="n">se_jack_mean</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Bootstrap SE: </span><span class="si">{</span><span class="n">se_boot_mean_out</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Ratio: </span><span class="si">{</span><span class="n">se_jack_mean</span><span class="o">/</span><span class="n">se_boot_mean_out</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Trim - Jackknife SE: </span><span class="si">{</span><span class="n">se_jack_trim</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Bootstrap SE: </span><span class="si">{</span><span class="n">se_boot_trim</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Ratio: </span><span class="si">{</span><span class="n">se_jack_trim</span><span class="o">/</span><span class="n">se_boot_trim</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Data with outlier (49 N(0,1) + one value at 10):
  Mean - Jackknife SE: 0.2162, Bootstrap SE: 0.2198, Ratio: 0.984
  Trim - Jackknife SE: 0.1438, Bootstrap SE: 0.1501, Ratio: 0.958
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): Summary</strong></p>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Smooth statistics (mean)</strong>: Methods agree closely (ratio ≈ 0.98-0.99) regardless of data distribution or outliers.</p></li>
<li><p class="sd-card-text"><strong>Non-smooth statistics (median)</strong>: Jackknife tends to underestimate SE (ratio ≈ 0.82). This is because removing one observation often doesn’t change the median, so jackknife misses some variability.</p></li>
<li><p class="sd-card-text"><strong>Robust statistics (trimmed mean)</strong>: Good agreement even with outliers, since the trimmed mean is smooth within its trimming range.</p></li>
<li><p class="sd-card-text"><strong>Rule of thumb</strong>:
- Ratio ≈ 0.9-1.1: methods agree, use either (jackknife is faster)
- Ratio &lt; 0.85: likely non-smooth statistic, trust bootstrap
- Ratio &gt; 1.15: unusual, investigate data/computation</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 4.5.4: Delete-<span class="math notranslate nohighlight">\(d\)</span> Jackknife</p>
<p>Explore the delete-<span class="math notranslate nohighlight">\(d\)</span> jackknife for different values of <span class="math notranslate nohighlight">\(d\)</span>.</p>
<div class="note admonition">
<p class="admonition-title">Background: Choosing d</p>
<p>For smooth statistics, <span class="math notranslate nohighlight">\(d = 1\)</span> is optimal. For non-smooth statistics like quantiles, larger <span class="math notranslate nohighlight">\(d\)</span> can improve reliability by creating more variation in the leave-<span class="math notranslate nohighlight">\(d\)</span>-out estimates.</p>
</div>
<ol class="loweralpha simple">
<li><p>For a sample of <span class="math notranslate nohighlight">\(n = 50\)</span> from N(0,1), compute delete-<span class="math notranslate nohighlight">\(d\)</span> jackknife SE for the mean with <span class="math notranslate nohighlight">\(d = 1, 3, 5, 10\)</span>. How do they compare?</p></li>
<li><p>Repeat for the sample median. Does larger <span class="math notranslate nohighlight">\(d\)</span> help?</p></li>
<li><p>For the 90th percentile, compare <span class="math notranslate nohighlight">\(d = 1\)</span> and <span class="math notranslate nohighlight">\(d = \lfloor\sqrt{n}\rfloor = 7\)</span>.</p></li>
<li><p>Discuss computational considerations for large <span class="math notranslate nohighlight">\(d\)</span>.</p></li>
</ol>
<div class="tip admonition">
<p class="admonition-title">Hint</p>
<p>For <span class="math notranslate nohighlight">\(d = 10\)</span> with <span class="math notranslate nohighlight">\(n = 50\)</span>, there are <span class="math notranslate nohighlight">\(\binom{50}{10} \approx 10^{10}\)</span> subsets—too many to enumerate. Use random sampling of subsets.</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Mean with various d</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">itertools</span><span class="w"> </span><span class="kn">import</span> <span class="n">combinations</span>

<span class="k">def</span><span class="w"> </span><span class="nf">jackknife_delete_d_se</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">statistic</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">max_subsets</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="kn">from</span><span class="w"> </span><span class="nn">math</span><span class="w"> </span><span class="kn">import</span> <span class="n">comb</span>
    <span class="n">n_subsets</span> <span class="o">=</span> <span class="n">comb</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">n_subsets</span> <span class="o">&lt;=</span> <span class="n">max_subsets</span><span class="p">:</span>
        <span class="n">subsets</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">combinations</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">d</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">subsets</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">seen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">subsets</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">max_subsets</span><span class="p">:</span>
            <span class="n">S</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)))</span>
            <span class="k">if</span> <span class="n">S</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">seen</span><span class="p">:</span>
                <span class="n">seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
                <span class="n">subsets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>

    <span class="n">theta_S</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">S</span> <span class="ow">in</span> <span class="n">subsets</span><span class="p">:</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
        <span class="n">mask</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">S</span><span class="p">)]</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">theta_S</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">statistic</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">mask</span><span class="p">]))</span>
    <span class="n">theta_S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">theta_S</span><span class="p">)</span>

    <span class="n">theta_bar</span> <span class="o">=</span> <span class="n">theta_S</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">var_jack_d</span> <span class="o">=</span> <span class="p">((</span><span class="n">n</span> <span class="o">-</span> <span class="n">d</span><span class="p">)</span> <span class="o">/</span> <span class="n">d</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">theta_S</span> <span class="o">-</span> <span class="n">theta_bar</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_jack_d</span><span class="p">)</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Delete-d jackknife SE for the Mean:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">]:</span>
    <span class="n">se_d</span> <span class="o">=</span> <span class="n">jackknife_delete_d_se</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  d = </span><span class="si">{</span><span class="n">d</span><span class="si">:</span><span class="s2">2d</span><span class="si">}</span><span class="s2">: SE = </span><span class="si">{</span><span class="n">se_d</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classical SE: </span><span class="si">{</span><span class="n">data</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Delete-d jackknife SE for the Mean:
  d =  1: SE = 0.1375
  d =  3: SE = 0.1372
  d =  5: SE = 0.1369
  d = 10: SE = 0.1359

Classical SE: 0.1375
</pre></div>
</div>
<p class="sd-card-text">For the smooth mean, all values of <span class="math notranslate nohighlight">\(d\)</span> give similar results.</p>
<p class="sd-card-text"><strong>Part (b): Median with various d</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Delete-d jackknife SE for the Median:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">]:</span>
    <span class="n">se_d</span> <span class="o">=</span> <span class="n">jackknife_delete_d_se</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  d = </span><span class="si">{</span><span class="n">d</span><span class="si">:</span><span class="s2">2d</span><span class="si">}</span><span class="s2">: SE = </span><span class="si">{</span><span class="n">se_d</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Bootstrap comparison</span>
<span class="n">B</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">boot_med</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Bootstrap SE: </span><span class="si">{</span><span class="n">boot_med</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Delete-d jackknife SE for the Median:
  d =  1: SE = 0.1435
  d =  3: SE = 0.1582
  d =  5: SE = 0.1664
  d = 10: SE = 0.1759

Bootstrap SE: 0.1821
</pre></div>
</div>
<p class="sd-card-text">Larger <span class="math notranslate nohighlight">\(d\)</span> moves the jackknife SE closer to the bootstrap SE for the median.</p>
<p class="sd-card-text"><strong>Part (c): 90th percentile</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">p90</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">90</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Delete-d jackknife SE for 90th Percentile:&quot;</span><span class="p">)</span>
<span class="n">se_d1</span> <span class="o">=</span> <span class="n">jackknife_delete_d_se</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">p90</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">se_d7</span> <span class="o">=</span> <span class="n">jackknife_delete_d_se</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">p90</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>

<span class="n">boot_p90</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">p90</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">)])</span>
<span class="n">se_boot_p90</span> <span class="o">=</span> <span class="n">boot_p90</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  d = 1: SE = </span><span class="si">{</span><span class="n">se_d1</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  d = 7: SE = </span><span class="si">{</span><span class="n">se_d7</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Bootstrap SE: </span><span class="si">{</span><span class="n">se_boot_p90</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Delete-d jackknife SE for 90th Percentile:
  d = 1: SE = 0.2156
  d = 7: SE = 0.2589
  Bootstrap SE: 0.2873
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): Computational considerations</strong></p>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Smooth statistics (mean)</strong>: All <span class="math notranslate nohighlight">\(d\)</span> values give similar SE; <span class="math notranslate nohighlight">\(d = 1\)</span> is most efficient.</p></li>
<li><p class="sd-card-text"><strong>Non-smooth statistics (median, quantiles)</strong>: Larger <span class="math notranslate nohighlight">\(d\)</span> moves jackknife SE toward bootstrap SE, but still underestimates. Delete-<span class="math notranslate nohighlight">\(d\)</span> helps but doesn’t fully solve the non-smoothness problem.</p></li>
<li><p class="sd-card-text"><strong>Computational cost</strong>:
- <span class="math notranslate nohighlight">\(d = 1\)</span>: Exactly <span class="math notranslate nohighlight">\(n\)</span> subsets, deterministic
- <span class="math notranslate nohighlight">\(d = \sqrt{n}\)</span>: <span class="math notranslate nohighlight">\(\binom{50}{7} \approx 99$ million—need sampling
- :math:`d = n/2\)</span>: <span class="math notranslate nohighlight">\(\binom{50}{25} \approx 10^{14}\)</span>—definitely sampling</p></li>
<li><p class="sd-card-text"><strong>Recommendation</strong>: For non-smooth statistics, bootstrap is simpler and more reliable than delete-<span class="math notranslate nohighlight">\(d\)</span> jackknife.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 4.5.5: Jackknife Diagnostics</p>
<p>Use jackknife diagnostics to identify influential observations.</p>
<div class="note admonition">
<p class="admonition-title">Background: Influence Analysis</p>
<p>The jackknife naturally provides influence diagnostics. Observations with large <span class="math notranslate nohighlight">\(|\hat{\theta} - \hat{\theta}_{(-i)}|\)</span> or extreme pseudovalues strongly affect the estimate and warrant investigation.</p>
</div>
<ol class="loweralpha simple">
<li><p>Generate <span class="math notranslate nohighlight">\(n = 50\)</span> observations from N(0,1), then replace the last observation with 5 (an outlier). Compute the sample mean and identify which observation is most influential using jackknife.</p></li>
<li><p>Repeat for robust statistics: median and 20% trimmed mean. Is the outlier still the most influential?</p></li>
<li><p>For the OLS slope in simple linear regression with one high-leverage point, compare jackknife influence with Cook’s distance.</p></li>
<li><p>Create a visualization of jackknife diagnostics (leave-one-out estimates, influence plot, pseudovalue histogram).</p></li>
</ol>
<div class="tip admonition">
<p class="admonition-title">Hint</p>
<p>Cook’s distance for observation <span class="math notranslate nohighlight">\(i\)</span> is <span class="math notranslate nohighlight">\(D_i = \frac{e_i^2}{p \cdot \hat{\sigma}^2} \cdot \frac{h_{ii}}{(1-h_{ii})^2}\)</span> where <span class="math notranslate nohighlight">\(h_{ii}\)</span> is leverage. High leverage + large residual = large influence.</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Mean with outlier</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">data</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span>  <span class="c1"># Outlier</span>

<span class="c1"># Jackknife for mean</span>
<span class="n">theta_hat</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">theta_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([(</span><span class="n">data</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">-</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)])</span>
<span class="n">influence</span> <span class="o">=</span> <span class="n">theta_hat</span> <span class="o">-</span> <span class="n">theta_i</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean with outlier:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Sample mean: </span><span class="si">{</span><span class="n">theta_hat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Most influential observation: index </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">influence</span><span class="p">))</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Its value: </span><span class="si">{</span><span class="n">data</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">influence</span><span class="p">))]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Influence: </span><span class="si">{</span><span class="n">influence</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">influence</span><span class="p">))]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean influence (absolute): </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">influence</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Mean with outlier:
  Sample mean: 0.0981
  Most influential observation: index 49
  Its value: 5.00
  Influence: 0.0980
  Mean influence (absolute): 0.0122
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Robust statistics</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Median</span>
<span class="n">med_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">med_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">i</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)])</span>
<span class="n">med_influence</span> <span class="o">=</span> <span class="n">med_hat</span> <span class="o">-</span> <span class="n">med_i</span>
<span class="n">most_inf_med</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">med_influence</span><span class="p">))</span>

<span class="c1"># Trimmed mean</span>
<span class="n">trim_hat</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">trim_mean</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>
<span class="n">trim_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">stats</span><span class="o">.</span><span class="n">trim_mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">i</span><span class="p">),</span> <span class="mf">0.2</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)])</span>
<span class="n">trim_influence</span> <span class="o">=</span> <span class="n">trim_hat</span> <span class="o">-</span> <span class="n">trim_i</span>
<span class="n">most_inf_trim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">trim_influence</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Median:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Most influential: index </span><span class="si">{</span><span class="n">most_inf_med</span><span class="si">}</span><span class="s2">, value </span><span class="si">{</span><span class="n">data</span><span class="p">[</span><span class="n">most_inf_med</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Outlier influence: </span><span class="si">{</span><span class="n">med_influence</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Trimmed Mean (20%):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Most influential: index </span><span class="si">{</span><span class="n">most_inf_trim</span><span class="si">}</span><span class="s2">, value </span><span class="si">{</span><span class="n">data</span><span class="p">[</span><span class="n">most_inf_trim</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Outlier influence: </span><span class="si">{</span><span class="n">trim_influence</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Median:
  Most influential: index 25, value -0.08
  Outlier influence: 0.0000

Trimmed Mean (20%):
  Most influential: index 32, value -1.26
  Outlier influence: 0.0000
</pre></div>
</div>
<p class="sd-card-text">The outlier has zero influence on median and trimmed mean—they are robust!</p>
<p class="sd-card-text"><strong>Part (c): Regression with high leverage</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate regression data</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>  <span class="c1"># High leverage point</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">y</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="mi">20</span> <span class="o">+</span> <span class="mi">5</span>  <span class="c1"># Also large residual</span>

<span class="k">def</span><span class="w"> </span><span class="nf">ols_slope</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">x</span><span class="p">])</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="kc">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">beta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Jackknife</span>
<span class="n">slope_hat</span> <span class="o">=</span> <span class="n">ols_slope</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">slope_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">ols_slope</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">i</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">i</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)])</span>
<span class="n">slope_influence</span> <span class="o">=</span> <span class="n">slope_hat</span> <span class="o">-</span> <span class="n">slope_i</span>

<span class="c1"># Cook&#39;s distance</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">x</span><span class="p">])</span>
<span class="n">H</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">H</span><span class="p">)</span>
<span class="n">residuals</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">X</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="kc">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sigma_sq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">residuals</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># number of parameters</span>
<span class="n">cooks_d</span> <span class="o">=</span> <span class="p">(</span><span class="n">residuals</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="n">p</span> <span class="o">*</span> <span class="n">sigma_sq</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="n">h</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">h</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Regression with high-leverage outlier:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Most influential (jackknife): index </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">slope_influence</span><span class="p">))</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Most influential (Cook&#39;s D): index </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">cooks_d</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Correlation (influence, Cook&#39;s D): </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">slope_influence</span><span class="p">),</span><span class="w"> </span><span class="n">cooks_d</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Regression with high-leverage outlier:
  Most influential (jackknife): index 49
  Most influential (Cook&#39;s D): index 49
  Correlation (influence, Cook&#39;s D): 0.987
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): Diagnostic visualization</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1"># Original data with outlier</span>
<span class="n">data_diag</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">data_diag</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span>

<span class="n">theta_hat</span> <span class="o">=</span> <span class="n">data_diag</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">theta_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([(</span><span class="n">data_diag</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">-</span> <span class="n">data_diag</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)])</span>
<span class="n">pv</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="n">theta_hat</span> <span class="o">-</span> <span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">theta_i</span>
<span class="n">influence</span> <span class="o">=</span> <span class="n">theta_hat</span> <span class="o">-</span> <span class="n">theta_i</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">theta_i</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">theta_hat</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\hat{\theta}$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Observation index&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\hat{\theta}_{(-i)}$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Leave-one-out estimates&#39;</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">influence</span><span class="p">,</span> <span class="n">basefmt</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="mi">49</span><span class="p">],</span> <span class="p">[</span><span class="n">influence</span><span class="p">[</span><span class="mi">49</span><span class="p">]],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Observation index&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\hat{\theta} - \hat{\theta}_{(-i)}$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Influence (outlier in red)&#39;</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">pv</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">pv</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Outlier PV&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Pseudovalue&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Pseudovalue distribution&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;jackknife_diagnostics.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Mean is not robust</strong>: The outlier dominates jackknife influence for the mean.</p></li>
<li><p class="sd-card-text"><strong>Robust statistics shield outliers</strong>: Median and trimmed mean show zero influence from the outlier—they effectively ignore it.</p></li>
<li><p class="sd-card-text"><strong>Jackknife ≈ Cook’s D</strong>: For regression, jackknife influence is highly correlated with Cook’s distance (both measure parameter change from deletion).</p></li>
<li><p class="sd-card-text"><strong>Diagnostics are valuable</strong>: Plotting leave-one-out estimates and pseudovalues quickly reveals influential observations.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 4.5.6: Complete Jackknife Analysis</p>
<p>Perform a complete jackknife analysis on a real-world style dataset.</p>
<div class="note admonition">
<p class="admonition-title">Background: Integrated Workflow</p>
<p>A complete analysis includes: computing the estimate and SE, checking for bias, identifying influential observations, comparing with bootstrap, and reporting results appropriately.</p>
</div>
<p>Generate bivariate data simulating a study of the relationship between study hours and exam scores:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">study_hours</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">exam_score</span> <span class="o">=</span> <span class="mi">50</span> <span class="o">+</span> <span class="mi">4</span><span class="o">*</span><span class="n">study_hours</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="c1"># Add one overachiever outlier</span>
<span class="n">study_hours</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">study_hours</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">exam_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">exam_score</span><span class="p">,</span> <span class="mi">95</span><span class="p">)</span>  <span class="c1"># High score for low hours</span>
</pre></div>
</div>
<ol class="loweralpha simple">
<li><p>Compute the OLS slope and its jackknife SE. Compare with classical SE.</p></li>
<li><p>Compute jackknife bias for the slope. Is bias correction warranted?</p></li>
<li><p>Identify the most influential observation. Is it the outlier?</p></li>
<li><p>Compare jackknife SE with bootstrap SE (both pairs and residual bootstrap).</p></li>
<li><p>Write a brief results paragraph suitable for a methods section.</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Setup</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">study_hours</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">exam_score</span> <span class="o">=</span> <span class="mi">50</span> <span class="o">+</span> <span class="mi">4</span><span class="o">*</span><span class="n">study_hours</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">study_hours</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">study_hours</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">exam_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">exam_score</span><span class="p">,</span> <span class="mi">95</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">study_hours</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">study_hours</span><span class="p">])</span>

<span class="k">def</span><span class="w"> </span><span class="nf">ols_coefs</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="kc">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (a): Jackknife SE vs Classical SE</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Full data estimate</span>
<span class="n">beta_hat</span> <span class="o">=</span> <span class="n">ols_coefs</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">exam_score</span><span class="p">)</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta_hat</span>
<span class="n">residuals</span> <span class="o">=</span> <span class="n">exam_score</span> <span class="o">-</span> <span class="n">y_hat</span>
<span class="n">sigma_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">residuals</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">2</span><span class="p">))</span>

<span class="c1"># Classical SE</span>
<span class="n">XtX_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span>
<span class="n">se_classical</span> <span class="o">=</span> <span class="n">sigma_hat</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">XtX_inv</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="c1"># Jackknife SE</span>
<span class="n">beta_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">X_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">y_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">exam_score</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">beta_i</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">ols_coefs</span><span class="p">(</span><span class="n">X_i</span><span class="p">,</span> <span class="n">y_i</span><span class="p">)</span>

<span class="n">beta_bar</span> <span class="o">=</span> <span class="n">beta_i</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">se_jack</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(((</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">beta_i</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">beta_bar</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Part (a): Slope SE comparison&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Slope estimate: </span><span class="si">{</span><span class="n">beta_hat</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Classical SE:   </span><span class="si">{</span><span class="n">se_classical</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Jackknife SE:   </span><span class="si">{</span><span class="n">se_jack</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Ratio:          </span><span class="si">{</span><span class="n">se_jack</span><span class="o">/</span><span class="n">se_classical</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Part (a): Slope SE comparison
  Slope estimate: 3.4152
  Classical SE:   0.4893
  Jackknife SE:   0.5037
  Ratio:          1.029
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Bias estimation</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bias_jack</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">beta_bar</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">beta_hat</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">beta_bc</span> <span class="o">=</span> <span class="n">beta_hat</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">bias_jack</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Part (b): Bias estimation&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Jackknife bias: </span><span class="si">{</span><span class="n">bias_jack</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Bias / SE ratio: </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">bias_jack</span><span class="p">)</span><span class="o">/</span><span class="n">se_jack</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Bias-corrected slope: </span><span class="si">{</span><span class="n">beta_bc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  Conclusion: Bias is small relative to SE; correction not warranted.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Part (b): Bias estimation
  Jackknife bias: 0.0312
  Bias / SE ratio: 0.062
  Bias-corrected slope: 3.3839
  Conclusion: Bias is small relative to SE; correction not warranted.
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (c): Influential observations</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">influence</span> <span class="o">=</span> <span class="n">beta_hat</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">beta_i</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">most_influential</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">influence</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Part (c): Influential observations&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Most influential: index </span><span class="si">{</span><span class="n">most_influential</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Study hours: </span><span class="si">{</span><span class="n">study_hours</span><span class="p">[</span><span class="n">most_influential</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Exam score: </span><span class="si">{</span><span class="n">exam_score</span><span class="p">[</span><span class="n">most_influential</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Influence on slope: </span><span class="si">{</span><span class="n">influence</span><span class="p">[</span><span class="n">most_influential</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Outlier index: </span><span class="si">{</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="si">}</span><span class="s2">, Outlier influence: </span><span class="si">{</span><span class="n">influence</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Part (c): Influential observations
  Most influential: index 40
  Study hours: 3.00
  Exam score: 95.00
  Influence on slope: -0.1127
  Outlier index: 40, Outlier influence: -0.1127
</pre></div>
</div>
<p class="sd-card-text">The outlier (index 40) is the most influential observation.</p>
<p class="sd-card-text"><strong>Part (d): Bootstrap comparison</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">B</span> <span class="o">=</span> <span class="mi">5000</span>

<span class="c1"># Pairs bootstrap</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">study_hours</span><span class="p">,</span> <span class="n">exam_score</span><span class="p">])</span>
<span class="n">boot_slopes_pairs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">X_b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
    <span class="n">boot_slopes_pairs</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">ols_coefs</span><span class="p">(</span><span class="n">X_b</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">])[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">se_boot_pairs</span> <span class="o">=</span> <span class="n">boot_slopes_pairs</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Residual bootstrap</span>
<span class="n">resid_centered</span> <span class="o">=</span> <span class="n">residuals</span> <span class="o">-</span> <span class="n">residuals</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">boot_slopes_resid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">y_star</span> <span class="o">=</span> <span class="n">y_hat</span> <span class="o">+</span> <span class="n">resid_centered</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="n">boot_slopes_resid</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">ols_coefs</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_star</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">se_boot_resid</span> <span class="o">=</span> <span class="n">boot_slopes_resid</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Part (d): Bootstrap comparison&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Jackknife SE:         </span><span class="si">{</span><span class="n">se_jack</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Pairs bootstrap SE:   </span><span class="si">{</span><span class="n">se_boot_pairs</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Residual bootstrap SE:</span><span class="si">{</span><span class="n">se_boot_resid</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Classical SE:         </span><span class="si">{</span><span class="n">se_classical</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Part (d): Bootstrap comparison
  Jackknife SE:         0.5037
  Pairs bootstrap SE:   0.5156
  Residual bootstrap SE:0.4812
  Classical SE:         0.4893
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (e): Results paragraph</strong></p>
<p class="sd-card-text">We estimated the relationship between study hours and exam scores using ordinary least squares regression. The estimated slope was β̂₁ = 3.42, indicating that each additional hour of study is associated with a 3.42-point increase in exam score. Standard error estimation was performed using jackknife (SE = 0.504), pairs bootstrap (SE = 0.516, B = 5,000), and residual bootstrap (SE = 0.481), all of which were consistent with the classical estimate (SE = 0.489). Jackknife bias estimation revealed negligible bias (0.031, approximately 6% of SE), so bias correction was not applied. Jackknife diagnostics identified one influential observation (a student with 3 hours of study but a 95 score)—removing this observation would decrease the slope estimate by 0.11 points, though this represents only 22% of the standard error and does not qualitatively change conclusions.</p>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text">All SE methods agree within ~10%, providing confidence in the results.</p></li>
<li><p class="sd-card-text">Bias is negligible relative to SE (6%), so bias correction is unnecessary.</p></li>
<li><p class="sd-card-text">The outlier is the most influential observation but not influential enough to change conclusions.</p></li>
<li><p class="sd-card-text">Jackknife provides unique diagnostic value (influence analysis) beyond what bootstrap alone offers.</p></li>
</ol>
</div>
</details></div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<p><strong>Foundational Works</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="quenouille1949" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Quenouille1949<span class="fn-bracket">]</span></span>
<p>Quenouille, M. H. (1949). Approximate tests of correlation in time series. <em>Journal of the Royal Statistical Society, Series B</em>, 11, 68–84. The original development of the jackknife for bias reduction in serial correlation estimation.</p>
</div>
<div class="citation" id="quenouille1956" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Quenouille1956<span class="fn-bracket">]</span></span>
<p>Quenouille, M. H. (1956). Notes on bias in estimation. <em>Biometrika</em>, 43, 353–360. Extension of the bias reduction technique and theoretical analysis of its properties.</p>
</div>
<div class="citation" id="tukey1958" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Tukey1958<span class="fn-bracket">]</span></span>
<p>Tukey, J. W. (1958). Bias and confidence in not-quite large samples (Abstract). <em>Annals of Mathematical Statistics</em>, 29, 614. Named the method “jackknife” and introduced pseudovalues for variance estimation.</p>
</div>
</div>
<p><strong>Comprehensive Reviews and Theory</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="miller1974" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Miller1974<span class="fn-bracket">]</span></span>
<p>Miller, R. G. (1974). The jackknife—a review. <em>Biometrika</em>, 61(1), 1–15. The definitive early review covering both theory and applications of jackknife methods.</p>
</div>
<div class="citation" id="efron1982" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Efron1982<span class="fn-bracket">]</span></span>
<p>Efron, B. (1982). <em>The Jackknife, the Bootstrap, and Other Resampling Plans</em>. CBMS-NSF Regional Conference Series in Applied Mathematics, Vol. 38. SIAM. Unified treatment connecting jackknife to bootstrap, establishing the modern framework for resampling inference.</p>
</div>
<div class="citation" id="shaotu1995" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ShaoTu1995<span class="fn-bracket">]</span></span>
<p>Shao, J., and Tu, D. (1995). <em>The Jackknife and Bootstrap</em>. Springer Series in Statistics. Springer. Comprehensive mathematical treatment of both methods with rigorous asymptotic theory.</p>
</div>
<div class="citation" id="shaowu1989" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ShaoWu1989<span class="fn-bracket">]</span></span>
<p>Shao, J., and Wu, C. F. J. (1989). A general theory for jackknife variance estimation. <em>Annals of Statistics</em>, 17(3), 1176–1197. Establishes general conditions for jackknife consistency and asymptotic normality.</p>
</div>
</div>
<p><strong>Delete-d Jackknife and Extensions</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="wu1986" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Wu1986<span class="fn-bracket">]</span></span>
<p>Wu, C. F. J. (1986). Jackknife, bootstrap and other resampling methods in regression analysis. <em>Annals of Statistics</em>, 14(4), 1261–1295. Develops delete-d jackknife theory for regression and compares with bootstrap methods.</p>
</div>
<div class="citation" id="shao1989" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Shao1989<span class="fn-bracket">]</span></span>
<p>Shao, J. (1989). The efficiency and consistency of approximations to the jackknife variance estimators. <em>Journal of the American Statistical Association</em>, 84(405), 114–119. Analysis of computational approximations for large-scale jackknife implementations.</p>
</div>
</div>
<p><strong>Influence Functions and Robust Statistics</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="hampel1986" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Hampel1986<span class="fn-bracket">]</span></span>
<p>Hampel, F. R., Ronchetti, E. M., Rousseeuw, P. J., and Stahel, W. A. (1986). <em>Robust Statistics: The Approach Based on Influence Functions</em>. Wiley Series in Probability and Mathematical Statistics. Wiley. Theoretical foundation connecting jackknife to influence functions and robust estimation.</p>
</div>
<div class="citation" id="jaeckel1972" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Jaeckel1972<span class="fn-bracket">]</span></span>
<p>Jaeckel, L. A. (1972). The infinitesimal jackknife. Bell Laboratories Memorandum MM 72-1215-11 (unpublished technical report). The original development of the infinitesimal jackknife connecting discrete leave-one-out to continuous influence analysis. Note: This memorandum is difficult to obtain; for accessible treatments of the infinitesimal jackknife, see Efron (1982) or Efron and Tibshirani (1993), Chapter 11.</p>
</div>
</div>
<p><strong>Bootstrap Comparisons</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="efrontibshirani1993" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>EfronTibshirani1993<span class="fn-bracket">]</span></span>
<p>Efron, B., and Tibshirani, R. J. (1993). <em>An Introduction to the Bootstrap</em>. Chapman &amp; Hall/CRC Monographs on Statistics and Applied Probability. Chapter 11 provides accessible comparison of jackknife and bootstrap methods.</p>
</div>
<div class="citation" id="davisonhinkley1997" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DavisonHinkley1997<span class="fn-bracket">]</span></span>
<p>Davison, A. C., and Hinkley, D. V. (1997). <em>Bootstrap Methods and Their Application</em>. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press. Comprehensive treatment including jackknife-after-bootstrap and the acceleration constant.</p>
</div>
</div>
<p><strong>Applications</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="hinkleywei1984" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HinkleyWei1984<span class="fn-bracket">]</span></span>
<p>Hinkley, D. V., and Wei, B. C. (1984). Improvements of jackknife confidence limit methods. <em>Biometrika</em>, 71(2), 331–339. Develops improved jackknife confidence intervals addressing coverage issues.</p>
</div>
<div class="citation" id="parr1985" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Parr1985<span class="fn-bracket">]</span></span>
<p>Parr, W. C. (1985). Jackknifing differentiable statistical functionals. <em>Journal of the Royal Statistical Society, Series B</em>, 47(1), 56–66. Theoretical analysis of jackknife for general smooth functionals.</p>
</div>
</div>
<p><strong>Textbook Treatments</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="efronhastie2016" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>EfronHastie2016<span class="fn-bracket">]</span></span>
<p>Efron, B., and Hastie, T. (2016). <em>Computer Age Statistical Inference: Algorithms, Evidence, and Data Science</em>. Cambridge University Press. Modern perspective on jackknife within the broader context of computational inference methods.</p>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="ch4_4-parametric-bootstrap.html" class="btn btn-neutral float-left" title="Section 4.4: The Parametric Bootstrap" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../../part3_bayesian/index.html" class="btn btn-neutral float-right" title="Part III: Bayesian Methods" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>