

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Section 4.4: The Parametric Bootstrap &mdash; STAT 418</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=3d0abd52" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT418/Website/part2_simulation/chapter4/ch4_4-parametric-bootstrap.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=30646c52"></script>
      <script>let toggleHintShow = 'Show solution';</script>
      <script>let toggleHintHide = 'Hide solution';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>window.MathJax = {"options": {"enableMenu": true, "menuOptions": {"settings": {"enrich": true, "speech": true, "braille": true, "collapsible": true, "assistiveMml": false}}}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
      <script src="../../_static/custom.js?v=8718e0ab"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Section 4.5: Jackknife Methods" href="ch4_5-jackknife-methods.html" />
    <link rel="prev" title="The Nonparametric Bootstrap" href="ch4_3-nonparametric-bootstrap.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Course Content</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../Homework/index.html">Homework Assignments</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Homework/index.html#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#assignment-policies">Assignment Policies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#recommended-workflow">Recommended Workflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#submission-requirements">Submission Requirements</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Homework/index.html#assignments-by-chapter">Assignments by Chapter</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#part-i-foundations">Part I: Foundations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Homework/hw1_distributional_relationships.html">Homework 1: Distributional Relationships and Computational Foundations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Homework/index.html#tips-for-success">Tips for Success</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#mathematical-derivations">Mathematical Derivations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#computational-verification">Computational Verification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#common-pitfalls">Common Pitfalls</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part1_foundations/index.html">Part I: Foundations of Probability and Computation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part1_foundations/chapter1/index.html">Chapter 1: Statistical Paradigms and Core Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html">Paradigms of Probability and Statistical Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#the-mathematical-foundation-kolmogorov-s-axioms">The Mathematical Foundation: Kolmogorov’s Axioms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#interpretations-of-probability">Interpretations of Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#statistical-inference-paradigms">Statistical Inference Paradigms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#historical-and-philosophical-debates">Historical and Philosophical Debates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#looking-ahead-our-course-focus">Looking Ahead: Our Course Focus</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html">Probability Distributions: Theory and Computation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#from-abstract-foundations-to-concrete-tools">From Abstract Foundations to Concrete Tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#the-python-ecosystem-for-probability">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#introduction-why-probability-distributions-matter">Introduction: Why Probability Distributions Matter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#id1">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#continuous-distributions">Continuous Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#additional-important-distributions">Additional Important Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#summary-and-practical-guidelines">Summary and Practical Guidelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#conclusion">Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html">Python Random Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#from-mathematical-distributions-to-computational-samples">From Mathematical Distributions to Computational Samples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-python-ecosystem-at-a-glance">The Python Ecosystem at a Glance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#understanding-pseudo-random-number-generation">Understanding Pseudo-Random Number Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-standard-library-random-module">The Standard Library: <code class="docutils literal notranslate"><span class="pre">random</span></code> Module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#numpy-fast-vectorized-random-sampling">NumPy: Fast Vectorized Random Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#scipy-stats-the-complete-statistical-toolkit">SciPy Stats: The Complete Statistical Toolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#bringing-it-all-together-library-selection-guide">Bringing It All Together: Library Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#looking-ahead-from-random-numbers-to-monte-carlo-methods">Looking Ahead: From Random Numbers to Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html">Chapter 1 Summary: Foundations in Place</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#the-three-pillars-of-chapter-1">The Three Pillars of Chapter 1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#what-lies-ahead-the-road-to-simulation">What Lies Ahead: The Road to Simulation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#chapter-1-exercises-synthesis-problems">Chapter 1 Exercises: Synthesis Problems</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Part II: Simulation-Based Methods</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../chapter2/index.html">Chapter 2: Monte Carlo Simulation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html">Monte Carlo Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#the-historical-development-of-monte-carlo-methods">The Historical Development of Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#the-core-principle-expectation-as-integration">The Core Principle: Expectation as Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#theoretical-foundations">Theoretical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#variance-estimation-and-confidence-intervals">Variance Estimation and Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#comparison-with-deterministic-methods">Comparison with Deterministic Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#sample-size-determination">Sample Size Determination</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#convergence-diagnostics-and-monitoring">Convergence Diagnostics and Monitoring</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#chapter-2-1-exercises-monte-carlo-fundamentals-mastery">Chapter 2.1 Exercises: Monte Carlo Fundamentals Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html">Uniform Random Variates</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#why-uniform-the-universal-currency-of-randomness">Why Uniform? The Universal Currency of Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#the-paradox-of-computational-randomness">The Paradox of Computational Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#chaotic-dynamical-systems-an-instructive-failure">Chaotic Dynamical Systems: An Instructive Failure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#linear-congruential-generators">Linear Congruential Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#shift-register-generators">Shift-Register Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#the-kiss-generator-combining-strategies">The KISS Generator: Combining Strategies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#modern-generators-mersenne-twister-and-pcg">Modern Generators: Mersenne Twister and PCG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#statistical-testing-of-random-number-generators">Statistical Testing of Random Number Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#chapter-2-2-exercises-uniform-random-variates-mastery">Chapter 2.2 Exercises: Uniform Random Variates Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html">Inverse CDF Method</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#mathematical-foundations">Mathematical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#continuous-distributions-with-closed-form-inverses">Continuous Distributions with Closed-Form Inverses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#numerical-inversion">Numerical Inversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#mixed-distributions">Mixed Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#chapter-2-3-exercises-inverse-cdf-method-mastery">Chapter 2.3 Exercises: Inverse CDF Method Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html">Transformation Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#why-transformation-methods">Why Transformation Methods?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-boxmuller-transform">The Box–Muller Transform</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-polar-marsaglia-method">The Polar (Marsaglia) Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#method-comparison-boxmuller-vs-polar-vs-ziggurat">Method Comparison: Box–Muller vs Polar vs Ziggurat</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-ziggurat-algorithm">The Ziggurat Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-clt-approximation-historical">The CLT Approximation (Historical)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#distributions-derived-from-the-normal">Distributions Derived from the Normal</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#multivariate-normal-generation">Multivariate Normal Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#implementation-guidance">Implementation Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#chapter-2-4-exercises-transformation-methods-mastery">Chapter 2.4 Exercises: Transformation Methods Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html">Rejection Sampling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-dartboard-intuition">The Dartboard Intuition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-accept-reject-algorithm">The Accept-Reject Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#efficiency-analysis">Efficiency Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#choosing-the-proposal-distribution">Choosing the Proposal Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-squeeze-principle">The Squeeze Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#geometric-example-sampling-from-the-unit-disk">Geometric Example: Sampling from the Unit Disk</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#limitations-and-the-curse-of-dimensionality">Limitations and the Curse of Dimensionality</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#connections-to-other-methods">Connections to Other Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#chapter-2-5-exercises-rejection-sampling-mastery">Chapter 2.5 Exercises: Rejection Sampling Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html">Variance Reduction Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#the-variance-reduction-paradigm">The Variance Reduction Paradigm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#importance-sampling">Importance Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#control-variates">Control Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#antithetic-variates">Antithetic Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#stratified-sampling">Stratified Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#common-random-numbers">Common Random Numbers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#conditional-monte-carlo-raoblackwellization">Conditional Monte Carlo (Rao–Blackwellization)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#combining-variance-reduction-techniques">Combining Variance Reduction Techniques</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#chapter-2-6-exercises-variance-reduction-mastery">Chapter 2.6 Exercises: Variance Reduction Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html">Chapter Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#the-complete-monte-carlo-workflow">The Complete Monte Carlo Workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#quick-reference-tables">Quick Reference Tables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#common-pitfalls-checklist">Common Pitfalls Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#connections-to-later-chapters">Connections to Later Chapters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#learning-outcomes-checklist">Learning Outcomes Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#further-reading-optimization-and-missing-data">Further Reading: Optimization and Missing Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter3/index.html">Chapter 3: Parametric Inference and Likelihood Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html">Exponential Families</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#historical-origins-from-scattered-results-to-unified-theory">Historical Origins: From Scattered Results to Unified Theory</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#the-canonical-exponential-family">The Canonical Exponential Family</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#converting-familiar-distributions">Converting Familiar Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#the-log-partition-function-a-moment-generating-machine">The Log-Partition Function: A Moment-Generating Machine</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#sufficiency-capturing-all-parameter-information">Sufficiency: Capturing All Parameter Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#minimal-sufficiency-and-completeness">Minimal Sufficiency and Completeness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#conjugate-priors-and-bayesian-inference">Conjugate Priors and Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#exponential-dispersion-models-and-glms">Exponential Dispersion Models and GLMs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#chapter-3-1-exercises-exponential-families-mastery">Chapter 3.1 Exercises: Exponential Families Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html">Maximum Likelihood Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-likelihood-function">The Likelihood Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-score-function">The Score Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#fisher-information">Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#closed-form-maximum-likelihood-estimators">Closed-Form Maximum Likelihood Estimators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#numerical-optimization-for-mle">Numerical Optimization for MLE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#asymptotic-properties-of-mles">Asymptotic Properties of MLEs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-cramer-rao-lower-bound">The Cramér-Rao Lower Bound</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-invariance-property">The Invariance Property</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#likelihood-based-hypothesis-testing">Likelihood-Based Hypothesis Testing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#confidence-intervals-from-likelihood">Confidence Intervals from Likelihood</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#connection-to-bayesian-inference">Connection to Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#chapter-3-2-exercises-maximum-likelihood-estimation-mastery">Chapter 3.2 Exercises: Maximum Likelihood Estimation Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html">Sampling Variability and Variance Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#statistical-estimators-and-their-properties">Statistical Estimators and Their Properties</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#sampling-distributions">Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#the-delta-method">The Delta Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#variance-estimation-methods">Variance Estimation Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#applications-and-worked-examples">Applications and Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html">Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#matrix-calculus-foundations">Matrix Calculus Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#the-linear-model">The Linear Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#ordinary-least-squares-the-calculus-approach">Ordinary Least Squares: The Calculus Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#ordinary-least-squares-the-geometric-approach">Ordinary Least Squares: The Geometric Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#properties-of-the-ols-estimator">Properties of the OLS Estimator</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#the-gauss-markov-theorem">The Gauss-Markov Theorem</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#estimating-the-error-variance">Estimating the Error Variance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#distributional-results-under-normality">Distributional Results Under Normality</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#diagnostics-and-model-checking">Diagnostics and Model Checking</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#numerical-stability-qr-decomposition">Numerical Stability: QR Decomposition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#model-selection-and-information-criteria">Model Selection and Information Criteria</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#regularization-ridge-and-lasso">Regularization: Ridge and LASSO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#chapter-3-4-exercises-linear-models-mastery">Chapter 3.4 Exercises: Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html">Generalized Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#historical-context-unification-of-regression-methods">Historical Context: Unification of Regression Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#the-glm-framework-three-components">The GLM Framework: Three Components</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#score-equations-and-fisher-information">Score Equations and Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#iteratively-reweighted-least-squares">Iteratively Reweighted Least Squares</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#logistic-regression-binary-outcomes">Logistic Regression: Binary Outcomes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#poisson-regression-count-data">Poisson Regression: Count Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#gamma-regression-positive-continuous-data">Gamma Regression: Positive Continuous Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#inference-in-glms-the-testing-triad">Inference in GLMs: The Testing Triad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#model-diagnostics">Model Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#model-comparison-and-selection">Model Comparison and Selection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#quasi-likelihood-and-robust-inference">Quasi-Likelihood and Robust Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#further-reading">Further Reading</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#chapter-3-5-exercises-generalized-linear-models-mastery">Chapter 3.5 Exercises: Generalized Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html">Chapter Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#the-parametric-inference-pipeline">The Parametric Inference Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#the-five-pillars-of-chapter-3">The Five Pillars of Chapter 3</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#quick-reference-core-formulas">Quick Reference: Core Formulas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#connections-to-future-material">Connections to Future Material</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#practical-guidance">Practical Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Chapter 4: Resampling Methods</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html">The Sampling Distribution Problem</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#the-fundamental-target-sampling-distributions">The Fundamental Target: Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#historical-development-the-quest-for-sampling-distributions">Historical Development: The Quest for Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#three-routes-to-the-sampling-distribution">Three Routes to the Sampling Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#when-asymptotics-fail-motivating-the-bootstrap">When Asymptotics Fail: Motivating the Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#the-plug-in-principle-theoretical-foundation">The Plug-In Principle: Theoretical Foundation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#computational-perspective-bootstrap-as-monte-carlo">Computational Perspective: Bootstrap as Monte Carlo</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#chapter-4-1-exercises">Chapter 4.1 Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html">The Empirical Distribution and Plug-in Principle</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#the-empirical-cumulative-distribution-function">The Empirical Cumulative Distribution Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#convergence-of-the-empirical-cdf">Convergence of the Empirical CDF</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#parameters-as-statistical-functionals">Parameters as Statistical Functionals</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#when-the-plug-in-principle-fails">When the Plug-in Principle Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#the-bootstrap-idea-in-one-sentence">The Bootstrap Idea in One Sentence</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#computational-implementation">Computational Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#section-4-2-exercises-ecdf-and-plug-in-mastery">Section 4.2 Exercises: ECDF and Plug-in Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html">The Nonparametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#the-bootstrap-principle">The Bootstrap Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#bootstrap-standard-errors">Bootstrap Standard Errors</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#bootstrap-bias-estimation">Bootstrap Bias Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#bootstrap-confidence-intervals">Bootstrap Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#bootstrap-for-regression">Bootstrap for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#bootstrap-diagnostics">Bootstrap Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#when-bootstrap-fails">When Bootstrap Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Section 4.4: The Parametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#the-parametric-bootstrap-principle">The Parametric Bootstrap Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="#location-scale-families">Location-Scale Families</a></li>
<li class="toctree-l4"><a class="reference internal" href="#parametric-bootstrap-for-regression">Parametric Bootstrap for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="#confidence-intervals">Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-checking-and-validation">Model Checking and Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#when-parametric-bootstrap-fails">When Parametric Bootstrap Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="#parametric-vs-nonparametric-a-decision-framework">Parametric vs. Nonparametric: A Decision Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch4_5-jackknife-methods.html">Section 4.5: Jackknife Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch4_5-jackknife-methods.html#historical-context-and-motivation">Historical Context and Motivation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_5-jackknife-methods.html#the-delete-1-jackknife">The Delete-1 Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_5-jackknife-methods.html#jackknife-bias-estimation">Jackknife Bias Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_5-jackknife-methods.html#the-delete-d-jackknife">The Delete-<span class="math notranslate nohighlight">\(d\)</span> Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_5-jackknife-methods.html#jackknife-versus-bootstrap">Jackknife versus Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_5-jackknife-methods.html#the-infinitesimal-jackknife">The Infinitesimal Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_5-jackknife-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_5-jackknife-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_5-jackknife-methods.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_5-jackknife-methods.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part3_bayesian/index.html">Part III: Bayesian Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part3_bayesian/index.html#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html">Bayesian Philosophy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Part II: Simulation-Based Methods</a></li>
          <li class="breadcrumb-item"><a href="index.html">Chapter 4: Resampling Methods</a></li>
      <li class="breadcrumb-item active">Section 4.4: The Parametric Bootstrap</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/part2_simulation/chapter4/ch4_4-parametric-bootstrap.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="section-4-4-the-parametric-bootstrap">
<span id="parametric-bootstrap"></span><h1>Section 4.4: The Parametric Bootstrap<a class="headerlink" href="#section-4-4-the-parametric-bootstrap" title="Link to this heading"></a></h1>
<p>The nonparametric bootstrap of <span class="xref std std-ref">nonparametric-bootstrap</span> treats the empirical distribution <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> as a proxy for the unknown population <span class="math notranslate nohighlight">\(F\)</span>. This approach requires minimal assumptions but ignores any structural knowledge about <span class="math notranslate nohighlight">\(F\)</span>. When a credible parametric model is available, we can do better: the <strong>parametric bootstrap</strong> simulates from a fitted parametric distribution <span class="math notranslate nohighlight">\(F_{\hat{\theta}}\)</span>, potentially achieving greater efficiency and better finite-sample performance.</p>
<p>This section develops the parametric bootstrap as a natural extension of the plug-in principle, explores its theoretical advantages and failure modes, and provides practical guidance for choosing between parametric and nonparametric approaches.</p>
<div class="important admonition">
<p class="admonition-title">Road Map 🧭</p>
<ul class="simple">
<li><p><strong>Understand</strong>: The parametric bootstrap principle and its relationship to the plug-in estimator</p></li>
<li><p><strong>Develop</strong>: Algorithms for location-scale families, regression, and GLMs</p></li>
<li><p><strong>Implement</strong>: Efficient Python code for parametric bootstrap inference</p></li>
<li><p><strong>Evaluate</strong>: Model diagnostics and the efficiency-robustness tradeoff</p></li>
</ul>
</div>
<section id="the-parametric-bootstrap-principle">
<h2>The Parametric Bootstrap Principle<a class="headerlink" href="#the-parametric-bootstrap-principle" title="Link to this heading"></a></h2>
<section id="conceptual-framework">
<h3>Conceptual Framework<a class="headerlink" href="#conceptual-framework" title="Link to this heading"></a></h3>
<p>Recall the bootstrap philosophy: we approximate the sampling distribution <span class="math notranslate nohighlight">\(G_F\)</span> of a statistic <span class="math notranslate nohighlight">\(\hat{\theta} = T(X_{1:n})\)</span> by studying its behavior under a proxy distribution. In the nonparametric bootstrap, this proxy is <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>, the empirical distribution placing mass <span class="math notranslate nohighlight">\(1/n\)</span> on each observation.</p>
<p>The <strong>parametric bootstrap</strong> takes a different approach. Given a parametric family <span class="math notranslate nohighlight">\(\{F_\theta : \theta \in \Theta\}\)</span>, we:</p>
<ol class="arabic simple">
<li><p><strong>Fit the model</strong>: Estimate <span class="math notranslate nohighlight">\(\hat{\theta}_n\)</span> from the data (typically via MLE).</p></li>
<li><p><strong>Simulate from the fitted model</strong>: Generate <span class="math notranslate nohighlight">\(X_1^*, \ldots, X_n^* \stackrel{iid}{\sim} F_{\hat{\theta}_n}\)</span>.</p></li>
<li><p><strong>Compute bootstrap statistics</strong>: Calculate <span class="math notranslate nohighlight">\(\hat{\theta}^*_b = T(X_1^*, \ldots, X_n^*)\)</span>.</p></li>
<li><p><strong>Approximate the sampling distribution</strong>: Use <span class="math notranslate nohighlight">\(\{\hat{\theta}^*_1, \ldots, \hat{\theta}^*_B\}\)</span> for inference.</p></li>
</ol>
<div class="note admonition">
<p class="admonition-title">Key Distinction</p>
<ul class="simple">
<li><p><strong>Nonparametric bootstrap</strong>: Resample from <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> (the data itself)</p></li>
<li><p><strong>Parametric bootstrap</strong>: Simulate fresh data from <span class="math notranslate nohighlight">\(F_{\hat{\theta}_n}\)</span> (the fitted model)</p></li>
</ul>
<p>The parametric bootstrap generates genuinely <em>new</em> observations—not rearrangements of the original data. Each bootstrap sample is a fresh draw from the estimated population.</p>
</div>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_4_fig01_parametric_vs_nonparametric.png"><img alt="Parametric vs nonparametric bootstrap comparison showing resampling vs simulation" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_4_fig01_parametric_vs_nonparametric.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 151 </span><span class="caption-text"><strong>Figure 4.4.1</strong>: The fundamental difference between parametric and nonparametric bootstrap. Panel (a) shows nonparametric bootstrap resampling the same data points with replacement. Panel (b) shows parametric bootstrap generating fresh observations from the fitted distribution. Panel (d) compares the resulting bootstrap distributions.</span><a class="headerlink" href="#id2" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="mathematical-framework">
<h3>Mathematical Framework<a class="headerlink" href="#mathematical-framework" title="Link to this heading"></a></h3>
<p>Let <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{iid}{\sim} F_{\theta_0}\)</span> where <span class="math notranslate nohighlight">\(\theta_0 \in \Theta\)</span> is the true parameter. The goal is to approximate the sampling distribution of <span class="math notranslate nohighlight">\(\hat{\theta}_n = T(X_{1:n})\)</span>.</p>
<p><strong>The parametric bootstrap distribution</strong> is the conditional distribution of <span class="math notranslate nohighlight">\(\hat{\theta}^*_n\)</span> given <span class="math notranslate nohighlight">\(\hat{\theta}_n\)</span>:</p>
<div class="math notranslate nohighlight">
\[G^*_{\hat{\theta}_n}(t) = P_{\hat{\theta}_n}\left(\hat{\theta}^*_n \leq t \mid \hat{\theta}_n\right)\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{\theta}^*_n = T(X_1^*, \ldots, X_n^*)\)</span> with <span class="math notranslate nohighlight">\(X_i^* \stackrel{iid}{\sim} F_{\hat{\theta}_n}\)</span>.</p>
<p><strong>Consistency</strong>: Under regularity conditions (continuous <span class="math notranslate nohighlight">\(F_\theta\)</span>, consistent <span class="math notranslate nohighlight">\(\hat{\theta}_n\)</span>, smooth <span class="math notranslate nohighlight">\(T\)</span>), the parametric bootstrap is consistent:</p>
<div class="math notranslate nohighlight">
\[\sup_t \left| G^*_{\hat{\theta}_n}(t) - G_{\theta_0}(t) \right| \xrightarrow{P} 0\]</div>
<p>where <span class="math notranslate nohighlight">\(G_{\theta_0}\)</span> is the true sampling distribution of <span class="math notranslate nohighlight">\(\hat{\theta}_n\)</span>.</p>
<p><strong>Higher-order accuracy</strong>: For studentized statistics <span class="math notranslate nohighlight">\(Z^* = \sqrt{n}(\hat{\theta}^* - \hat{\theta}_n)/\hat{\sigma}^*\)</span>, studentization combined with bootstrap calibration can achieve <strong>second-order accuracy</strong> under standard regularity conditions (smooth parametric models, asymptotically pivotal statistics):</p>
<div class="math notranslate nohighlight">
\[\sup_t \left| P^*(Z^* \leq t) - P(Z \leq t) \right| = O_P(n^{-1})\]</div>
<p>compared to <span class="math notranslate nohighlight">\(O_P(n^{-1/2})\)</span> for unstudentized percentile methods, which are typically first-order accurate. This improved convergence rate is a major advantage for confidence interval construction, though it requires proper studentization with per-replicate SE estimates.</p>
</section>
<section id="why-use-parametric-bootstrap">
<h3>Why Use Parametric Bootstrap?<a class="headerlink" href="#why-use-parametric-bootstrap" title="Link to this heading"></a></h3>
<p><strong>Efficiency when the model is correct</strong>: If <span class="math notranslate nohighlight">\(F_{\hat{\theta}_n}\)</span> is close to <span class="math notranslate nohighlight">\(F_{\theta_0}\)</span>, the parametric bootstrap produces tighter confidence intervals than the nonparametric bootstrap.</p>
<p><strong>Better finite-sample behavior</strong>: With small samples (<span class="math notranslate nohighlight">\(n &lt; 50\)</span>), the empirical distribution <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> is a crude approximation to <span class="math notranslate nohighlight">\(F\)</span>. A well-chosen parametric model can capture population features (tails, smoothness) that <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> misses.</p>
<p><strong>Natural for model-based inference</strong>: When the analysis already assumes a parametric model (regression, GLM, survival analysis), parametric bootstrap maintains consistency between model fitting and uncertainty quantification.</p>
<p><strong>Handles boundary constraints</strong>: Parametric models naturally respect parameter constraints (e.g., variances are positive, probabilities are in <span class="math notranslate nohighlight">\([0,1]\)</span>), while nonparametric bootstrap may produce invalid resamples.</p>
<div class="note admonition">
<p class="admonition-title">Parametric Bootstrap vs. Parametric Monte Carlo</p>
<p>Students sometimes ask: “Is parametric bootstrap just parametric Monte Carlo?” The distinction is subtle but important:</p>
<ul class="simple">
<li><p><strong>Parametric Monte Carlo</strong> simulates from a <em>known</em> parametric model with <span class="math notranslate nohighlight">\(\theta\)</span> treated as given or hypothesized (e.g., simulating under the null hypothesis).</p></li>
<li><p><strong>Parametric bootstrap</strong> simulates from an <em>estimated</em> model <span class="math notranslate nohighlight">\(F_{\hat{\theta}}\)</span>, approximating the sampling distribution under the true <span class="math notranslate nohighlight">\(F_{\theta_0}\)</span>.</p></li>
</ul>
<p>This distinction explains why model misspecification is fatal for parametric bootstrap: we’re using <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> as a proxy for <span class="math notranslate nohighlight">\(\theta_0\)</span>, and if the model family is wrong, this proxy breaks down.</p>
</div>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_4_fig03_efficiency_comparison.png"><img alt="Efficiency comparison between parametric and nonparametric bootstrap" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_4_fig03_efficiency_comparison.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 152 </span><span class="caption-text"><strong>Figure 4.4.3</strong>: Efficiency comparison when the parametric model is correct. (a) CI widths across sample sizes—parametric produces narrower intervals, especially for small n. (b) Coverage rates near nominal 95%. (c) Efficiency ratio shows parametric gains are largest for small samples.</span><a class="headerlink" href="#id3" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="the-algorithm-general-form">
<h3>The Algorithm: General Form<a class="headerlink" href="#the-algorithm-general-form" title="Link to this heading"></a></h3>
<p><strong>Algorithm: Parametric Bootstrap (General)</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input: Data X₁, ..., Xₙ; parametric family {F_θ}; estimator θ̂; statistic T; replicates B
Output: Bootstrap distribution {T*₁, ..., T*_B}

1. Fit model: Compute θ̂ₙ from data (e.g., MLE)
2. For b = 1, ..., B:
   a. Generate X*₁, ..., X*ₙ iid ~ F_{θ̂ₙ}
   b. Compute θ̂*_b from bootstrap sample
   c. Compute T*_b = T(θ̂*_b) or any functional of interest
3. Return {T*₁, ..., T*_B}
</pre></div>
</div>
<p><strong>Computational complexity</strong>: <span class="math notranslate nohighlight">\(O(B \cdot n \cdot c)\)</span> where <span class="math notranslate nohighlight">\(c\)</span> is the cost of computing <span class="math notranslate nohighlight">\(T\)</span>. For regression with <span class="math notranslate nohighlight">\(p\)</span> predictors, <span class="math notranslate nohighlight">\(c = O(np^2)\)</span> for OLS.</p>
</section>
<section id="python-implementation">
<h3>Python Implementation<a class="headerlink" href="#python-implementation" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">parametric_bootstrap</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">fit_model</span><span class="p">,</span> <span class="n">generate_data</span><span class="p">,</span> <span class="n">statistic</span><span class="p">,</span>
                         <span class="n">B</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    General parametric bootstrap.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    data : array_like</span>
<span class="sd">        Original data.</span>
<span class="sd">    fit_model : callable</span>
<span class="sd">        Function that takes data and returns fitted parameters.</span>
<span class="sd">    generate_data : callable</span>
<span class="sd">        Function that takes (params, n, rng) and returns simulated data.</span>
<span class="sd">    statistic : callable</span>
<span class="sd">        Function that computes the statistic from data.</span>
<span class="sd">    B : int</span>
<span class="sd">        Number of bootstrap replicates.</span>
<span class="sd">    seed : int, optional</span>
<span class="sd">        Random seed for reproducibility.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    theta_star : ndarray</span>
<span class="sd">        Bootstrap distribution of the statistic.</span>
<span class="sd">    theta_hat : float or ndarray</span>
<span class="sd">        Original estimate.</span>
<span class="sd">    params_hat : tuple</span>
<span class="sd">        Fitted model parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="c1"># Step 1: Fit model to original data</span>
    <span class="n">params_hat</span> <span class="o">=</span> <span class="n">fit_model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">theta_hat</span> <span class="o">=</span> <span class="n">statistic</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="c1"># Pre-allocate</span>
    <span class="n">theta_hat_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">theta_hat</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">theta_hat_arr</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">theta_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">theta_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">B</span><span class="p">,)</span> <span class="o">+</span> <span class="n">theta_hat_arr</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="c1"># Step 2: Bootstrap loop</span>
    <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
        <span class="c1"># Generate new data from fitted model</span>
        <span class="n">data_star</span> <span class="o">=</span> <span class="n">generate_data</span><span class="p">(</span><span class="n">params_hat</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
        <span class="n">theta_star</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">statistic</span><span class="p">(</span><span class="n">data_star</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">theta_star</span><span class="p">,</span> <span class="n">theta_hat</span><span class="p">,</span> <span class="n">params_hat</span>
</pre></div>
</div>
<div class="note admonition">
<p class="admonition-title">Example 💡 Parametric Bootstrap for Normal Mean</p>
<p><strong>Given:</strong> <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{iid}{\sim} N(\mu, \sigma^2)\)</span> with unknown <span class="math notranslate nohighlight">\(\mu, \sigma^2\)</span>.</p>
<p><strong>Find:</strong> Bootstrap SE and 95% CI for <span class="math notranslate nohighlight">\(\hat{\mu} = \bar{X}\)</span>.</p>
<p><strong>Implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Generate sample data</span>
<span class="n">n</span><span class="p">,</span> <span class="n">mu_true</span><span class="p">,</span> <span class="n">sigma_true</span> <span class="o">=</span> <span class="mi">30</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">,</span> <span class="mf">2.5</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu_true</span><span class="p">,</span> <span class="n">sigma_true</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># Fit normal model (plug-in estimates)</span>
<span class="n">mu_hat</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>                <span class="c1"># MLE for μ</span>
<span class="n">sigma_hat</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>        <span class="c1"># Sample SD (unbiased); MLE would use ddof=0</span>

<span class="c1"># Parametric bootstrap</span>
<span class="n">B</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">mu_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>

<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="c1"># Generate from fitted N(μ̂, σ̂²)</span>
    <span class="n">data_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu_hat</span><span class="p">,</span> <span class="n">sigma_hat</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">mu_star</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">data_star</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># Results</span>
<span class="n">se_boot</span> <span class="o">=</span> <span class="n">mu_star</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ci_percentile</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">mu_star</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.975</span><span class="p">])</span>

<span class="c1"># Compare to analytical</span>
<span class="n">se_analytical</span> <span class="o">=</span> <span class="n">sigma_hat</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">ci_t</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">interval</span><span class="p">(</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mu_hat</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">se_analytical</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample mean: </span><span class="si">{</span><span class="n">mu_hat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bootstrap SE: </span><span class="si">{</span><span class="n">se_boot</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Analytical SE: </span><span class="si">{</span><span class="n">se_analytical</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Percentile CI: [</span><span class="si">{</span><span class="n">ci_percentile</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_percentile</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t-interval CI: [</span><span class="si">{</span><span class="n">ci_t</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_t</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Result:</strong> For correctly specified normal models, the parametric bootstrap matches analytical results closely, validating the approach.</p>
</div>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_4_fig02_parametric_algorithm.png"><img alt="Parametric bootstrap algorithm walkthrough with exponential distribution example" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_4_fig02_parametric_algorithm.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 153 </span><span class="caption-text"><strong>Figure 4.4.2</strong>: The parametric bootstrap algorithm illustrated with exponential data. (a) Fit the model to data. (b) Simulate bootstrap samples from the fitted distribution. (c) Compute statistics on each sample. (d) Build the bootstrap distribution for inference.</span><a class="headerlink" href="#id4" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="location-scale-families">
<h2>Location-Scale Families<a class="headerlink" href="#location-scale-families" title="Link to this heading"></a></h2>
<p>Many common distributions belong to <strong>location-scale families</strong>, where <span class="math notranslate nohighlight">\(X = \mu + \sigma Z\)</span> for standardized <span class="math notranslate nohighlight">\(Z\)</span>. These include normal, Student-t, logistic, and Laplace distributions.</p>
<section id="general-algorithm">
<h3>General Algorithm<a class="headerlink" href="#general-algorithm" title="Link to this heading"></a></h3>
<p><strong>Algorithm: Parametric Bootstrap for Location-Scale Families</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input: Data X₁, ..., Xₙ; location-scale family F; replicates B
Output: Bootstrap distribution of (μ̂*, σ̂*)

1. Estimate location μ̂ and scale σ̂ (MLE or robust estimators)
2. [Optional] Fit shape parameter if needed (e.g., df for t-distribution)
3. For b = 1, ..., B:
   a. Generate Z*₁, ..., Z*ₙ iid from the standard form of F
   b. Transform: X*ᵢ = μ̂ + σ̂ · Z*ᵢ
   c. Re-estimate (μ̂*, σ̂*) from X*
4. Return bootstrap estimates
</pre></div>
</div>
<p>Note: This is a <em>pure parametric</em> bootstrap—we simulate fresh standardized values from the assumed distribution, not from the empirical residuals. For a semi-parametric approach that resamples standardized residuals <span class="math notranslate nohighlight">\(Z_i = (X_i - \hat{\mu})/\hat{\sigma}\)</span>, see the residual bootstrap in <span class="xref std std-ref">nonparametric-bootstrap</span>.</p>
</section>
<section id="estimator-choices">
<h3>Estimator Choices<a class="headerlink" href="#estimator-choices" title="Link to this heading"></a></h3>
<p>The parametric bootstrap requires consistent estimators. Common choices:</p>
<p><strong>Normal distribution</strong>:</p>
<ul class="simple">
<li><p>Location: <span class="math notranslate nohighlight">\(\hat{\mu} = \bar{X}\)</span> (MLE)</p></li>
<li><p>Scale: <span class="math notranslate nohighlight">\(\hat{\sigma} = s = \sqrt{\frac{1}{n-1}\sum(X_i - \bar{X})^2}\)</span> (unbiased) or <span class="math notranslate nohighlight">\(\hat{\sigma}_{MLE} = \sqrt{\frac{1}{n}\sum(X_i - \bar{X})^2}\)</span></p></li>
</ul>
<p><strong>Student-t distribution</strong>:</p>
<ul class="simple">
<li><p>Location: Sample mean or median</p></li>
<li><p>Scale: <span class="math notranslate nohighlight">\(\hat{\sigma} = s \cdot \sqrt{(\nu - 2)/\nu}\)</span> for known <span class="math notranslate nohighlight">\(\nu\)</span></p></li>
<li><p>Degrees of freedom: Estimate via MLE or fix based on prior knowledge</p></li>
</ul>
<p><strong>Exponential/Gamma</strong>:</p>
<ul class="simple">
<li><p>Rate: <span class="math notranslate nohighlight">\(\hat{\lambda} = 1/\bar{X}\)</span> (MLE for exponential)</p></li>
<li><p>Shape and rate: Method of moments or MLE for gamma</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.optimize</span><span class="w"> </span><span class="kn">import</span> <span class="n">minimize_scalar</span>

<span class="k">def</span><span class="w"> </span><span class="nf">bootstrap_location_scale</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">distribution</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span> <span class="n">B</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parametric bootstrap for location-scale families.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    data : array_like</span>
<span class="sd">        Sample data.</span>
<span class="sd">    distribution : str</span>
<span class="sd">        &#39;normal&#39;, &#39;t&#39;, &#39;laplace&#39;, or &#39;logistic&#39;.</span>
<span class="sd">    B : int</span>
<span class="sd">        Number of bootstrap replicates.</span>
<span class="sd">    seed : int, optional</span>
<span class="sd">        Random seed.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict with bootstrap results.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="c1"># Fit distribution</span>
    <span class="k">if</span> <span class="n">distribution</span> <span class="o">==</span> <span class="s1">&#39;normal&#39;</span><span class="p">:</span>
        <span class="n">mu_hat</span><span class="p">,</span> <span class="n">sigma_hat</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">data</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">rvs</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">size</span><span class="p">:</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu_hat</span><span class="p">,</span> <span class="n">sigma_hat</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">distribution</span> <span class="o">==</span> <span class="s1">&#39;t&#39;</span><span class="p">:</span>
        <span class="c1"># Fit t-distribution via MLE</span>
        <span class="n">df</span><span class="p">,</span> <span class="n">loc</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">mu_hat</span><span class="p">,</span> <span class="n">sigma_hat</span> <span class="o">=</span> <span class="n">loc</span><span class="p">,</span> <span class="n">scale</span>
        <span class="n">rvs</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">size</span><span class="p">:</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span>
                                       <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">distribution</span> <span class="o">==</span> <span class="s1">&#39;laplace&#39;</span><span class="p">:</span>
        <span class="n">mu_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>  <span class="c1"># MLE for location</span>
        <span class="n">sigma_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">data</span> <span class="o">-</span> <span class="n">mu_hat</span><span class="p">))</span>  <span class="c1"># MLE for scale</span>
        <span class="n">rvs</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">size</span><span class="p">:</span> <span class="n">rng</span><span class="o">.</span><span class="n">laplace</span><span class="p">(</span><span class="n">mu_hat</span><span class="p">,</span> <span class="n">sigma_hat</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">distribution</span> <span class="o">==</span> <span class="s1">&#39;logistic&#39;</span><span class="p">:</span>
        <span class="n">mu_hat</span><span class="p">,</span> <span class="n">sigma_hat</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">data</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span>
        <span class="n">rvs</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">size</span><span class="p">:</span> <span class="n">rng</span><span class="o">.</span><span class="n">logistic</span><span class="p">(</span><span class="n">mu_hat</span><span class="p">,</span> <span class="n">sigma_hat</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown distribution: </span><span class="si">{</span><span class="n">distribution</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Bootstrap</span>
    <span class="n">mu_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
    <span class="n">sigma_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
        <span class="n">data_star</span> <span class="o">=</span> <span class="n">rvs</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
        <span class="n">mu_star</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">data_star</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">sigma_star</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">data_star</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;mu_hat&#39;</span><span class="p">:</span> <span class="n">mu_hat</span><span class="p">,</span>
        <span class="s1">&#39;sigma_hat&#39;</span><span class="p">:</span> <span class="n">sigma_hat</span><span class="p">,</span>
        <span class="s1">&#39;mu_star&#39;</span><span class="p">:</span> <span class="n">mu_star</span><span class="p">,</span>
        <span class="s1">&#39;sigma_star&#39;</span><span class="p">:</span> <span class="n">sigma_star</span><span class="p">,</span>
        <span class="s1">&#39;se_mu&#39;</span><span class="p">:</span> <span class="n">mu_star</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="s1">&#39;se_sigma&#39;</span><span class="p">:</span> <span class="n">sigma_star</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="s1">&#39;ci_mu&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">mu_star</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.975</span><span class="p">]),</span>
        <span class="s1">&#39;ci_sigma&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">sigma_star</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.975</span><span class="p">])</span>
    <span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="parametric-bootstrap-for-regression">
<h2>Parametric Bootstrap for Regression<a class="headerlink" href="#parametric-bootstrap-for-regression" title="Link to this heading"></a></h2>
<p>The parametric bootstrap for regression extends the residual bootstrap by assuming a distributional form for the errors rather than resampling observed residuals.</p>
<section id="fixed-x-parametric-bootstrap">
<h3>Fixed-X Parametric Bootstrap<a class="headerlink" href="#fixed-x-parametric-bootstrap" title="Link to this heading"></a></h3>
<p>For the linear model <span class="math notranslate nohighlight">\(Y = X\beta + \varepsilon\)</span> with <span class="math notranslate nohighlight">\(\varepsilon \sim N(0, \sigma^2 I_n)\)</span>:</p>
<p><strong>Algorithm: Parametric Bootstrap for OLS (Normal Errors)</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input: Design X (n × p), response Y, replicates B
Output: Bootstrap distribution of β̂*

1. Fit OLS: β̂ = (X&#39;X)⁻¹X&#39;Y
2. Compute residuals: ê = Y - Xβ̂
3. Estimate error variance: σ̂² = ||ê||² / (n - p)
4. For b = 1, ..., B:
   a. Generate ε*₁, ..., ε*ₙ iid ~ N(0, σ̂²)
   b. Form Y* = Xβ̂ + ε*
   c. Refit: β̂*_b = (X&#39;X)⁻¹X&#39;Y*
5. Return {β̂*₁, ..., β̂*_B}
</pre></div>
</div>
<p><strong>Key insight</strong>: Unlike the residual bootstrap (which resamples <span class="math notranslate nohighlight">\(\hat{e}_i\)</span>), the parametric bootstrap draws fresh errors from <span class="math notranslate nohighlight">\(N(0, \hat{\sigma}^2)\)</span>. This:</p>
<ul class="simple">
<li><p>Produces smooth bootstrap distributions</p></li>
<li><p>Respects the assumed error structure</p></li>
<li><p>Can be more efficient when normality holds</p></li>
<li><p>Fails if errors are non-normal</p></li>
</ul>
<div class="note admonition">
<p class="admonition-title">Scope: Conditional Inference</p>
<p>This fixed-X parametric bootstrap targets <strong>conditional inference given X</strong>. If your predictors are random and you want unconditional inference on <span class="math notranslate nohighlight">\(\beta\)</span>, you need a different resampling scheme (pairs bootstrap, or a generative model for X).</p>
</div>
<p><strong>What is held fixed vs. resampled:</strong></p>
<table class="docutils align-default">
<colgroup>
<col style="width: 30.0%" />
<col style="width: 35.0%" />
<col style="width: 35.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Component</p></th>
<th class="head"><p>Fixed-X Parametric</p></th>
<th class="head"><p>Pairs Bootstrap</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Design matrix X</p></td>
<td><p>Fixed (same X in all replicates)</p></td>
<td><p>Resampled (rows of X change)</p></td>
</tr>
<tr class="row-odd"><td><p>Error distribution</p></td>
<td><p><span class="math notranslate nohighlight">\(N(0, \hat{\sigma}^2)\)</span> (assumed)</p></td>
<td><p>Empirical (from residuals)</p></td>
</tr>
<tr class="row-even"><td><p>Fitted means <span class="math notranslate nohighlight">\(X\hat{\beta}\)</span></p></td>
<td><p>Fixed</p></td>
<td><p>Recomputed each replicate</p></td>
</tr>
<tr class="row-odd"><td><p>Inference target</p></td>
<td><p>Conditional on X</p></td>
<td><p>Unconditional</p></td>
</tr>
</tbody>
</table>
</section>
<section id="id1">
<h3>Python Implementation<a class="headerlink" href="#id1" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">parametric_bootstrap_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">B</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parametric bootstrap for OLS regression with normal errors.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : ndarray, shape (n, p)</span>
<span class="sd">        Design matrix (should include intercept column if desired).</span>
<span class="sd">    y : ndarray, shape (n,)</span>
<span class="sd">        Response vector.</span>
<span class="sd">    B : int</span>
<span class="sd">        Number of bootstrap replicates.</span>
<span class="sd">    seed : int, optional</span>
<span class="sd">        Random seed.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    beta_star : ndarray, shape (B, p)</span>
<span class="sd">        Bootstrap distribution of coefficients.</span>
<span class="sd">    beta_hat : ndarray, shape (p,)</span>
<span class="sd">        OLS estimates.</span>
<span class="sd">    sigma_hat : float</span>
<span class="sd">        Estimated error standard deviation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Fit OLS</span>
    <span class="n">XtX_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span>
    <span class="n">beta_hat</span> <span class="o">=</span> <span class="n">XtX_inv</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta_hat</span>
    <span class="n">residuals</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span>

    <span class="c1"># Estimate error variance</span>
    <span class="n">sigma_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">residuals</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">p</span><span class="p">))</span>

    <span class="c1"># Parametric bootstrap</span>
    <span class="n">beta_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
        <span class="c1"># Generate errors from N(0, σ̂²)</span>
        <span class="n">eps_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma_hat</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
        <span class="n">y_star</span> <span class="o">=</span> <span class="n">y_hat</span> <span class="o">+</span> <span class="n">eps_star</span>
        <span class="n">beta_star</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">XtX_inv</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y_star</span>

    <span class="k">return</span> <span class="n">beta_star</span><span class="p">,</span> <span class="n">beta_hat</span><span class="p">,</span> <span class="n">sigma_hat</span>
</pre></div>
</div>
<div class="note admonition">
<p class="admonition-title">Example 💡 Comparing Parametric and Nonparametric Bootstrap for Regression</p>
<p><strong>Setup:</strong> Simulate data with normal errors and compare bootstrap approaches.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Generate data with normal errors</span>
<span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">)])</span>
<span class="n">beta_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>
<span class="n">sigma_true</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta_true</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma_true</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>

<span class="n">B</span> <span class="o">=</span> <span class="mi">3000</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>

<span class="c1"># Fit OLS</span>
<span class="n">XtX_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span>
<span class="n">beta_hat</span> <span class="o">=</span> <span class="n">XtX_inv</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta_hat</span>
<span class="n">residuals</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span>
<span class="n">sigma_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">residuals</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">p</span><span class="p">))</span>

<span class="c1"># Parametric bootstrap</span>
<span class="n">beta_param</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="n">eps_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma_hat</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">y_star</span> <span class="o">=</span> <span class="n">y_hat</span> <span class="o">+</span> <span class="n">eps_star</span>
    <span class="n">beta_param</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">XtX_inv</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y_star</span>

<span class="c1"># Nonparametric (residual) bootstrap</span>
<span class="n">resid_centered</span> <span class="o">=</span> <span class="n">residuals</span> <span class="o">-</span> <span class="n">residuals</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">beta_nonparam</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">e_star</span> <span class="o">=</span> <span class="n">resid_centered</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="n">y_star</span> <span class="o">=</span> <span class="n">y_hat</span> <span class="o">+</span> <span class="n">e_star</span>
    <span class="n">beta_nonparam</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">XtX_inv</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y_star</span>

<span class="c1"># Compare</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;SE for β₁ (slope):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Parametric:    </span><span class="si">{</span><span class="n">beta_param</span><span class="p">[:,</span><span class="w"> </span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Nonparametric: </span><span class="si">{</span><span class="n">beta_nonparam</span><span class="p">[:,</span><span class="w"> </span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Analytical:    </span><span class="si">{</span><span class="n">sigma_hat</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">XtX_inv</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Result:</strong> When errors are truly normal, all three approaches give similar SEs. The parametric bootstrap slightly outperforms when the model is correct.</p>
</div>
</section>
<section id="generalized-linear-models">
<h3>Generalized Linear Models<a class="headerlink" href="#generalized-linear-models" title="Link to this heading"></a></h3>
<p>For GLMs, the parametric bootstrap simulates from the fitted conditional distribution of <span class="math notranslate nohighlight">\(Y | X\)</span>.</p>
<p><strong>Algorithm: Parametric Bootstrap for GLM</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input: Design X, response Y, family (e.g., binomial, Poisson), replicates B
Output: Bootstrap distribution of β̂*

1. Fit GLM: β̂, compute μ̂ᵢ = g⁻¹(xᵢ&#39;β̂) where g is the link function
2. For b = 1, ..., B:
   a. For each i, generate Y*ᵢ ~ Family(μ̂ᵢ, φ̂)
      - Binomial: Y*ᵢ ~ Binomial(nᵢ, μ̂ᵢ)
      - Poisson: Y*ᵢ ~ Poisson(μ̂ᵢ)
      - Gamma: Y*ᵢ ~ Gamma(α̂, μ̂ᵢ/α̂)
   b. Refit GLM on (X, Y*) to get β̂*_b
3. Return {β̂*₁, ..., β̂*_B}
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>

<span class="k">def</span><span class="w"> </span><span class="nf">parametric_bootstrap_glm</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">family</span><span class="p">,</span> <span class="n">B</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parametric bootstrap for GLMs.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : ndarray</span>
<span class="sd">        Design matrix (include intercept).</span>
<span class="sd">    y : ndarray</span>
<span class="sd">        Response.</span>
<span class="sd">    family : statsmodels family</span>
<span class="sd">        GLM family (e.g., sm.families.Poisson()).</span>
<span class="sd">    B : int</span>
<span class="sd">        Number of replicates.</span>
<span class="sd">    seed : int</span>
<span class="sd">        Random seed.</span>
<span class="sd">    n_trials : ndarray, optional</span>
<span class="sd">        Number of trials for Binomial family. Required if y contains</span>
<span class="sd">        counts &gt; 1. If None and family is Binomial, assumes Bernoulli (n=1).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    beta_star : ndarray, shape (B, p)</span>
<span class="sd">        Bootstrap coefficients.</span>
<span class="sd">    model : fitted GLM</span>
<span class="sd">        Original fitted model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Fit original model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">family</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
    <span class="n">beta_hat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">params</span>
    <span class="n">mu_hat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">mu</span>  <span class="c1"># Fitted means</span>

    <span class="n">p</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">beta_hat</span><span class="p">)</span>
    <span class="n">beta_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>

    <span class="c1"># Check family type using class name (more robust than isinstance)</span>
    <span class="n">family_name</span> <span class="o">=</span> <span class="n">family</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>

    <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
        <span class="c1"># Generate from fitted distribution</span>
        <span class="k">if</span> <span class="n">family_name</span> <span class="o">==</span> <span class="s1">&#39;Poisson&#39;</span><span class="p">:</span>
            <span class="n">y_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="n">mu_hat</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">family_name</span> <span class="o">==</span> <span class="s1">&#39;Binomial&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">n_trials</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Check if data looks like Bernoulli</span>
                <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">((</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)):</span>
                    <span class="n">n_trials</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;n_trials required for Binomial with count data. &quot;</span>
                        <span class="s2">&quot;If y is binary (0/1), this is inferred automatically.&quot;</span>
                    <span class="p">)</span>
            <span class="n">y_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n_trials</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span> <span class="n">mu_hat</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">family_name</span> <span class="o">==</span> <span class="s1">&#39;Gamma&#39;</span><span class="p">:</span>
            <span class="c1"># Gamma with shape from dispersion</span>
            <span class="n">phi</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">scale</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">phi</span>
            <span class="n">y_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">mu_hat</span> <span class="o">/</span> <span class="n">shape</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Family </span><span class="si">{</span><span class="n">family_name</span><span class="si">}</span><span class="s2"> not implemented&quot;</span><span class="p">)</span>

        <span class="c1"># Refit</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">model_star</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">y_star</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">family</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">disp</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">beta_star</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_star</span><span class="o">.</span><span class="n">params</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
            <span class="n">beta_star</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>

    <span class="k">return</span> <span class="n">beta_star</span><span class="p">,</span> <span class="n">model</span>
</pre></div>
</div>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_4_fig05_regression_parametric.png"><img alt="Parametric bootstrap for regression with Q-Q diagnostics and SE comparison" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_4_fig05_regression_parametric.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 154 </span><span class="caption-text"><strong>Figure 4.4.5</strong>: Parametric bootstrap for regression. (a) Data and fitted model. (b) Q-Q plot to check normality assumption. (d-e) Bootstrap distributions for intercept and slope. (f) SE comparison shows agreement between analytical, parametric, and residual bootstrap when normality holds.</span><a class="headerlink" href="#id5" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="confidence-intervals">
<h2>Confidence Intervals<a class="headerlink" href="#confidence-intervals" title="Link to this heading"></a></h2>
<section id="percentile-intervals">
<h3>Percentile Intervals<a class="headerlink" href="#percentile-intervals" title="Link to this heading"></a></h3>
<p>The simplest parametric bootstrap CI uses quantiles of the bootstrap distribution:</p>
<div class="math notranslate nohighlight">
\[\text{CI}_{1-\alpha}^{\text{perc}} = \left[Q_{\alpha/2}(\hat{\theta}^*), Q_{1-\alpha/2}(\hat{\theta}^*)\right]\]</div>
<p>This is identical to nonparametric bootstrap percentile intervals in form, but the bootstrap distribution comes from parametric simulation.</p>
</section>
<section id="bootstrap-t-studentized-intervals">
<h3>Bootstrap-t (Studentized) Intervals<a class="headerlink" href="#bootstrap-t-studentized-intervals" title="Link to this heading"></a></h3>
<p>For second-order accuracy, use the <strong>studentized bootstrap</strong>:</p>
<ol class="arabic simple">
<li><p>For each bootstrap replicate, compute <span class="math notranslate nohighlight">\(Z^*_b = (\hat{\theta}^*_b - \hat{\theta})/\hat{\text{SE}}^*_b\)</span></p></li>
<li><p>Find quantiles <span class="math notranslate nohighlight">\(q_{\alpha/2}^*\)</span> and <span class="math notranslate nohighlight">\(q_{1-\alpha/2}^*\)</span> of <span class="math notranslate nohighlight">\(\{Z^*_b\}\)</span></p></li>
<li><p>CI: <span class="math notranslate nohighlight">\([\hat{\theta} - q_{1-\alpha/2}^* \cdot \widehat{\text{SE}}, \hat{\theta} - q_{\alpha/2}^* \cdot \widehat{\text{SE}}]\)</span></p></li>
</ol>
<div class="math notranslate nohighlight">
\[\text{CI}_{1-\alpha}^{\text{boot-t}} = \left[\hat{\theta} - q^*_{1-\alpha/2} \cdot \widehat{\text{SE}}, \hat{\theta} - q^*_{\alpha/2} \cdot \widehat{\text{SE}}\right]\]</div>
<p><strong>Advantage</strong>: Achieves <span class="math notranslate nohighlight">\(O(n^{-1})\)</span> coverage error versus <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> for percentile intervals.</p>
<p><strong>Disadvantage</strong>: Requires estimating SE within each bootstrap replicate (nested bootstrap or analytical formula).</p>
</section>
<section id="bca-intervals-with-parametric-bootstrap">
<h3>BCa Intervals with Parametric Bootstrap<a class="headerlink" href="#bca-intervals-with-parametric-bootstrap" title="Link to this heading"></a></h3>
<p>The BCa method (Bias-Corrected and accelerated) can be applied to parametric bootstrap as well:</p>
<div class="math notranslate nohighlight">
\[z_0 = \Phi^{-1}\left(\frac{\#\{\hat{\theta}^*_b &lt; \hat{\theta}\}}{B}\right), \quad
\hat{a} = \frac{\sum_{i=1}^n (\bar{\theta}_{(\cdot)} - \hat{\theta}_{(-i)})^3}{6\left[\sum_{i=1}^n (\bar{\theta}_{(\cdot)} - \hat{\theta}_{(-i)})^2\right]^{3/2}}\]</div>
<p>The adjusted quantiles are:</p>
<div class="math notranslate nohighlight">
\[\alpha_1 = \Phi\left(z_0 + \frac{z_0 + z_{\alpha/2}}{1 - \hat{a}(z_0 + z_{\alpha/2})}\right), \quad
\alpha_2 = \Phi\left(z_0 + \frac{z_0 + z_{1-\alpha/2}}{1 - \hat{a}(z_0 + z_{1-\alpha/2})}\right)\]</div>
<p>BCa intervals are transformation-invariant and range-preserving, making them excellent for parameters with natural bounds.</p>
</section>
<section id="python-implementation-of-ci-methods">
<h3>Python Implementation of CI Methods<a class="headerlink" href="#python-implementation-of-ci-methods" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">parametric_bootstrap_ci</span><span class="p">(</span><span class="n">theta_star</span><span class="p">,</span> <span class="n">theta_hat</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                             <span class="n">method</span><span class="o">=</span><span class="s1">&#39;percentile&#39;</span><span class="p">,</span> <span class="n">se_hat</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                             <span class="n">se_star</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">theta_jack</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Confidence intervals from parametric bootstrap.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    theta_star : ndarray</span>
<span class="sd">        Bootstrap replicates.</span>
<span class="sd">    theta_hat : float</span>
<span class="sd">        Original estimate.</span>
<span class="sd">    alpha : float</span>
<span class="sd">        Significance level.</span>
<span class="sd">    method : str</span>
<span class="sd">        &#39;percentile&#39;, &#39;basic&#39;, &#39;normal&#39;, &#39;studentized&#39;, or &#39;bca&#39;.</span>
<span class="sd">    se_hat : float, optional</span>
<span class="sd">        SE estimate for original data (needed for &#39;normal&#39;, &#39;studentized&#39;).</span>
<span class="sd">    se_star : ndarray, optional</span>
<span class="sd">        Per-replicate SE estimates (needed for true &#39;studentized&#39;).</span>
<span class="sd">        If None with &#39;studentized&#39;, falls back to pseudo-studentized.</span>
<span class="sd">    theta_jack : ndarray, optional</span>
<span class="sd">        Jackknife estimates (needed for &#39;bca&#39;).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ci : tuple</span>
<span class="sd">        (lower, upper) confidence limits.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">B</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">theta_star</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;percentile&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">theta_star</span><span class="p">,</span> <span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span>
                <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">theta_star</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">))</span>

    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;basic&#39;</span><span class="p">:</span>
        <span class="n">q_lo</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">theta_star</span><span class="p">,</span> <span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">q_hi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">theta_star</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">theta_hat</span> <span class="o">-</span> <span class="n">q_hi</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">theta_hat</span> <span class="o">-</span> <span class="n">q_lo</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;normal&#39;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">se_hat</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">se_hat</span> <span class="o">=</span> <span class="n">theta_star</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">theta_hat</span> <span class="o">-</span> <span class="n">z</span><span class="o">*</span><span class="n">se_hat</span><span class="p">,</span> <span class="n">theta_hat</span> <span class="o">+</span> <span class="n">z</span><span class="o">*</span><span class="n">se_hat</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;studentized&#39;</span><span class="p">:</span>
        <span class="c1"># True studentized bootstrap requires SE for each replicate</span>
        <span class="k">if</span> <span class="n">se_hat</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;se_hat required for studentized method&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">se_star</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Proper studentized bootstrap with per-replicate SEs</span>
            <span class="n">t_star</span> <span class="o">=</span> <span class="p">(</span><span class="n">theta_star</span> <span class="o">-</span> <span class="n">theta_hat</span><span class="p">)</span> <span class="o">/</span> <span class="n">se_star</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Pseudo-studentized: uses single SE (does NOT achieve</span>
            <span class="c1"># second-order accuracy; use only as approximation)</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;se_star not provided; using pseudo-studentized &quot;</span>
                        <span class="s2">&quot;method which does not achieve second-order accuracy&quot;</span><span class="p">)</span>
            <span class="n">t_star</span> <span class="o">=</span> <span class="p">(</span><span class="n">theta_star</span> <span class="o">-</span> <span class="n">theta_hat</span><span class="p">)</span> <span class="o">/</span> <span class="n">se_hat</span>

        <span class="n">q_lo</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">t_star</span><span class="p">,</span> <span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">q_hi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">t_star</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">theta_hat</span> <span class="o">-</span> <span class="n">q_hi</span><span class="o">*</span><span class="n">se_hat</span><span class="p">,</span> <span class="n">theta_hat</span> <span class="o">-</span> <span class="n">q_lo</span><span class="o">*</span><span class="n">se_hat</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;bca&#39;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">theta_jack</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Jackknife estimates required for BCa&quot;</span><span class="p">)</span>

        <span class="c1"># Bias correction</span>
        <span class="n">prop_below</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">theta_star</span> <span class="o">&lt;</span> <span class="n">theta_hat</span><span class="p">)</span>
        <span class="n">z0</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">prop_below</span><span class="p">)</span> <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">prop_below</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span>

        <span class="c1"># Acceleration</span>
        <span class="n">theta_bar</span> <span class="o">=</span> <span class="n">theta_jack</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">num</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">theta_bar</span> <span class="o">-</span> <span class="n">theta_jack</span><span class="p">)</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">denom</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">theta_bar</span> <span class="o">-</span> <span class="n">theta_jack</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mf">1.5</span>
        <span class="n">a_hat</span> <span class="o">=</span> <span class="n">num</span> <span class="o">/</span> <span class="n">denom</span> <span class="k">if</span> <span class="n">denom</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>

        <span class="c1"># Adjusted quantiles</span>
        <span class="n">z_alpha</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">z_1alpha</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">adj_quantile</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">z0</span> <span class="o">+</span> <span class="p">(</span><span class="n">z0</span> <span class="o">+</span> <span class="n">z</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">a_hat</span><span class="o">*</span><span class="p">(</span><span class="n">z0</span> <span class="o">+</span> <span class="n">z</span><span class="p">)))</span>

        <span class="n">alpha1</span> <span class="o">=</span> <span class="n">adj_quantile</span><span class="p">(</span><span class="n">z_alpha</span><span class="p">)</span>
        <span class="n">alpha2</span> <span class="o">=</span> <span class="n">adj_quantile</span><span class="p">(</span><span class="n">z_1alpha</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">theta_star</span><span class="p">,</span> <span class="n">alpha1</span><span class="p">),</span>
                <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">theta_star</span><span class="p">,</span> <span class="n">alpha2</span><span class="p">))</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown method: </span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="model-checking-and-validation">
<h2>Model Checking and Validation<a class="headerlink" href="#model-checking-and-validation" title="Link to this heading"></a></h2>
<p>The parametric bootstrap’s validity depends critically on model correctness. Before relying on parametric bootstrap, verify assumptions.</p>
<section id="diagnostic-checks">
<h3>Diagnostic Checks<a class="headerlink" href="#diagnostic-checks" title="Link to this heading"></a></h3>
<ol class="arabic simple">
<li><p><strong>Graphical diagnostics</strong>:</p>
<ul class="simple">
<li><p>Q-Q plot of residuals against assumed distribution</p></li>
<li><p>Residual plots for heteroscedasticity</p></li>
<li><p>Scale-location plot for variance stability</p></li>
</ul>
</li>
<li><p><strong>Formal tests</strong> (use with caution—low power for small <span class="math notranslate nohighlight">\(n\)</span>):</p>
<ul class="simple">
<li><p>Shapiro-Wilk for normality</p></li>
<li><p>Breusch-Pagan for heteroscedasticity</p></li>
<li><p>Anderson-Darling for distributional fit</p></li>
</ul>
</li>
<li><p><strong>Sensitivity analysis</strong>:</p>
<ul class="simple">
<li><p>Compare parametric and nonparametric bootstrap CIs</p></li>
<li><p>If they differ substantially, investigate model assumptions</p></li>
</ul>
</li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">check_parametric_assumptions</span><span class="p">(</span><span class="n">residuals</span><span class="p">,</span> <span class="n">distribution</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Diagnostic plots for parametric bootstrap assumptions.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    residuals : array_like</span>
<span class="sd">        Model residuals.</span>
<span class="sd">    distribution : str</span>
<span class="sd">        Assumed distribution (&#39;normal&#39;, &#39;t&#39;, etc.).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">residuals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">residuals</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">residuals</span><span class="p">)</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

    <span class="c1"># Q-Q plot</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">distribution</span> <span class="o">==</span> <span class="s1">&#39;normal&#39;</span><span class="p">:</span>
        <span class="n">stats</span><span class="o">.</span><span class="n">probplot</span><span class="p">(</span><span class="n">residuals</span><span class="p">,</span> <span class="n">dist</span><span class="o">=</span><span class="s2">&quot;norm&quot;</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">distribution</span> <span class="o">==</span> <span class="s1">&#39;t&#39;</span><span class="p">:</span>
        <span class="c1"># Fit df, then Q-Q</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">residuals</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">stats</span><span class="o">.</span><span class="n">probplot</span><span class="p">(</span><span class="n">residuals</span><span class="p">,</span> <span class="n">dist</span><span class="o">=</span><span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="p">(</span><span class="n">df</span><span class="p">),</span> <span class="n">plot</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Q-Q Plot&#39;</span><span class="p">)</span>

    <span class="c1"># Histogram with fitted distribution</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">residuals</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
            <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">residuals</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">residuals</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">100</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">distribution</span> <span class="o">==</span> <span class="s1">&#39;normal&#39;</span><span class="p">:</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">residuals</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">residuals</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">),</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Fitted Normal&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Histogram vs Fitted Distribution&#39;</span><span class="p">)</span>

    <span class="c1"># Residuals vs fitted (check for patterns)</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">residuals</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Index&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Residuals&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Residuals vs Index&#39;</span><span class="p">)</span>

    <span class="c1"># Scale-location plot</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">std_resid</span> <span class="o">=</span> <span class="n">residuals</span> <span class="o">/</span> <span class="n">residuals</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">std_resid</span><span class="p">)),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Index&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;√|Standardized Residuals|&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Scale-Location Plot&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

    <span class="c1"># Formal tests</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Diagnostic Tests:&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">distribution</span> <span class="o">==</span> <span class="s1">&#39;normal&#39;</span><span class="p">:</span>
        <span class="n">stat</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">shapiro</span><span class="p">(</span><span class="n">residuals</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Shapiro-Wilk: W=</span><span class="si">{</span><span class="n">stat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, p=</span><span class="si">{</span><span class="n">p</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Runs test for independence</span>
    <span class="n">median</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">residuals</span><span class="p">)</span>
    <span class="n">signs</span> <span class="o">=</span> <span class="n">residuals</span> <span class="o">&gt;</span> <span class="n">median</span>
    <span class="n">runs</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">signs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">!=</span> <span class="n">signs</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">n_pos</span><span class="p">,</span> <span class="n">n_neg</span> <span class="o">=</span> <span class="n">signs</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="p">(</span><span class="o">~</span><span class="n">signs</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">exp_runs</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">n_pos</span><span class="o">*</span><span class="n">n_neg</span> <span class="o">/</span> <span class="n">n</span>
    <span class="n">var_runs</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">n_pos</span><span class="o">*</span><span class="n">n_neg</span><span class="o">*</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">n_pos</span><span class="o">*</span><span class="n">n_neg</span> <span class="o">-</span> <span class="n">n</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">z_runs</span> <span class="o">=</span> <span class="p">(</span><span class="n">runs</span> <span class="o">-</span> <span class="n">exp_runs</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_runs</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Runs test: z=</span><span class="si">{</span><span class="n">z_runs</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, p=</span><span class="si">{</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">z_runs</span><span class="p">)))</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">fig</span>
</pre></div>
</div>
<figure class="align-center" id="id6">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_4_fig07_model_validation.png"><img alt="Model validation diagnostics including Q-Q plots and SE comparison" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_4_fig07_model_validation.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 155 </span><span class="caption-text"><strong>Figure 4.4.7</strong>: Model validation diagnostics. Left column shows a well-behaved case (normal data) where parametric assumptions hold. Right column shows a problematic case (bimodal data) where normality fails. When methods disagree (panel e), investigate model assumptions.</span><a class="headerlink" href="#id6" title="Link to this image"></a></p>
</figcaption>
</figure>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ⚠️</p>
<p><strong>Trusting the wrong model</strong>: If you use parametric bootstrap with a misspecified model (e.g., assuming normality when errors are heavy-tailed), confidence intervals may have severely incorrect coverage. Always validate assumptions before relying on parametric bootstrap.</p>
<p><strong>Course heuristic</strong>: If parametric and nonparametric bootstrap CIs differ by more than 20% in width, this is a flag to investigate model assumptions carefully. This threshold is not a formal statistical rule but a practical signal that something may warrant attention.</p>
</div>
</section>
</section>
<section id="when-parametric-bootstrap-fails">
<h2>When Parametric Bootstrap Fails<a class="headerlink" href="#when-parametric-bootstrap-fails" title="Link to this heading"></a></h2>
<section id="model-misspecification">
<h3>Model Misspecification<a class="headerlink" href="#model-misspecification" title="Link to this heading"></a></h3>
<p>The most serious parametric-bootstrap failure mode is <strong>model misspecification</strong>. If the true distribution
<span class="math notranslate nohighlight">\(F\)</span> is meaningfully outside the assumed family <span class="math notranslate nohighlight">\(\{F_\theta\}\)</span>, then simulating from
<span class="math notranslate nohighlight">\(F_{\hat\theta}\)</span> does not reproduce the sampling behavior of the statistic under the real data-generating process.
In that case, the parametric bootstrap can deliver <strong>biased standard errors</strong> and <strong>confidence intervals with severe undercoverage</strong>.</p>
<p>A particularly fragile setting is inference for a <strong>tail functional</strong>, such as
<span class="math notranslate nohighlight">\(p = \Pr(X &gt; c)\)</span>, because <span class="math notranslate nohighlight">\(p\)</span> depends on the distribution’s extreme-right behavior. In the example below,
the true distribution is a <strong>mixture</strong> (bimodal). Fitting a single Normal forces the model to “average” the two modes,
which misrepresents the right tail that determines <span class="math notranslate nohighlight">\(p\)</span>.</p>
<figure class="align-center" id="id7">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_4_fig04_model_misspecification.png"><img alt="Parametric bootstrap failure under model misspecification for a tail probability p = P(X &gt; c)" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_4_fig04_model_misspecification.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 156 </span><span class="caption-text"><strong>Figure 4.4.4</strong>: Model misspecification, parametric bootstrap can be confidently wrong.
(a) Data are generated from a mixture distribution, but a single Normal model is fitted, producing a tail mismatch beyond the threshold <span class="math notranslate nohighlight">\(c=6\)</span>.
(b) Bootstrap distributions for <span class="math notranslate nohighlight">\(\hat{p}^* = \Pr(X^* &gt; c)\)</span> differ sharply: the parametric bootstrap (wrong model) concentrates on smaller values,
while the nonparametric bootstrap remains anchored to the empirical tail behavior.
(c) In repeated-sampling experiments, the 95% parametric-bootstrap CI for <span class="math notranslate nohighlight">\(p\)</span> can have drastic undercoverage under misspecification,
whereas the nonparametric bootstrap is typically much closer to nominal.</span><a class="headerlink" href="#id7" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="boundary-parameters">
<h3>Boundary Parameters<a class="headerlink" href="#boundary-parameters" title="Link to this heading"></a></h3>
<p>Boundary and near boundary settings are <strong>nonregular</strong>: the estimator’s sampling distribution can be highly asymmetric, and the usual bootstrap logic can break. In these cases, the parametric bootstrap can fail, and the <strong>standard nonparametric bootstrap can fail as well</strong> when the statistic is constrained by the sample support.</p>
<ul class="simple">
<li><p><strong>Uniform[0, (theta)] (endpoint / maximum)</strong>:
If <span class="math notranslate nohighlight">\(\hat{\theta} = X_{(n)}\)</span> (the sample maximum), then a parametric bootstrap that simulates
<span class="math notranslate nohighlight">\(X^* \sim \mathrm{Unif}(0,\hat{\theta})\)</span> produces <span class="math notranslate nohighlight">\(\hat{\theta}^*=\max(X^*) \le \hat{\theta}\)</span> always.
This truncation makes it impossible to represent <strong>upward uncertainty</strong> in <span class="math notranslate nohighlight">\(\theta\)</span>.
The same structural problem occurs for the <strong>standard nonparametric bootstrap</strong>, since resampling from <span class="math notranslate nohighlight">\(\hat F_n\)</span>
cannot generate observations exceeding <span class="math notranslate nohighlight">\(X_{(n)}\)</span>.</p></li>
<li><p><strong>Variance near 0 (degenerate scale estimate)</strong>:
If the sample variance is extremely small, a parametric bootstrap from <span class="math notranslate nohighlight">\(N(\hat{\mu},\hat{\sigma}^2)\)</span> with tiny
<span class="math notranslate nohighlight">\(\hat{\sigma}\)</span> yields nearly degenerate bootstrap samples and misleadingly small standard errors.</p></li>
</ul>
<p><strong>Remedies</strong> (choose based on context):
- <strong>Exact finite-sample theory when available</strong>: for Uniform endpoints, order-statistic results yield exact tail probabilities and CIs.
- <strong>Bias-corrected or alternative estimators</strong>: for Uniform[0, (theta)],</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(\hat{\theta}_{bc}=\frac{n+1}{n}X_{(n)}\)</span> removes the downward bias in <span class="math notranslate nohighlight">\(X_{(n)}\)</span>.</p>
</div></blockquote>
<ul class="simple">
<li><p><strong>Subsampling / m-out-of-n bootstrap</strong>: resample <span class="math notranslate nohighlight">\(m&lt;n\)</span> observations (with or without replacement) to obtain a distributional
approximation that is often more stable in nonregular settings.</p></li>
<li><p><strong>Reparameterization</strong>: when possible, transform to an unconstrained scale to avoid boundary artifacts.</p></li>
</ul>
<figure class="align-center" id="id8">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_4_fig06_boundary_failure.png"><img alt="Boundary parameter failure showing bootstrap truncation at the sample maximum" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_4_fig06_boundary_failure.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 157 </span><span class="caption-text"><strong>Figure 4.4.6</strong>: Boundary parameter failure for Uniform[0, <span class="math notranslate nohighlight">\(\theta\)</span>]. (a) The MLE <span class="math notranslate nohighlight">\(\hat{\theta}=X_{(n)}\)</span> typically
underestimates the true endpoint. (b) Parametric bootstrap from <span class="math notranslate nohighlight">\(\mathrm{Unif}(0,\hat{\theta})\)</span> is truncated at
<span class="math notranslate nohighlight">\(\hat{\theta}\)</span> (and the standard nonparametric bootstrap is similarly truncated), so upward uncertainty cannot be represented.
(c) The resulting bootstrap standard error and percentile CI upper bound are biased downward relative to the true sampling behavior.</span><a class="headerlink" href="#id8" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="heavy-tailed-distributions">
<h3>Heavy-Tailed Distributions<a class="headerlink" href="#heavy-tailed-distributions" title="Link to this heading"></a></h3>
<p>Heavy tails can break bootstrap inference, but the issue depends on both the <strong>tail behavior</strong> of <span class="math notranslate nohighlight">\(F\)</span> and the <strong>statistic</strong> <span class="math notranslate nohighlight">\(T_n\)</span>.</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(F\)</span> has <strong>infinite variance</strong> (e.g., stable laws with <span class="math notranslate nohighlight">\(\alpha &lt; 2\)</span>), then the sample mean and other non-robust, variance-dependent functionals often have <strong>nonstandard</strong> asymptotics (no CLT, stable limits, or extremely slow convergence). In these regimes, the usual nonparametric bootstrap may give a poor or even inconsistent approximation to the sampling distribution of <span class="math notranslate nohighlight">\(T_n\)</span>.</p></li>
<li><p>In the extreme case of <strong>no finite mean</strong> (e.g., Cauchy), the sample mean does not stabilize in the usual sense; it does not concentrate around a constant. A bootstrap distribution built from <span class="math notranslate nohighlight">\(\hat F_n\)</span> cannot recover a “nice” sampling law for <span class="math notranslate nohighlight">\(\bar X\)</span> because the target itself is nonstandard.</p></li>
</ul>
<p>Robust statistics are typically safer under heavy tails. Medians, trimmed means, and many M-estimators can retain well-behaved limiting distributions (often asymptotically normal under mild conditions) and are therefore more compatible with bootstrap inference.</p>
<p><strong>Remedy</strong>: Prefer robust estimators (median, trimmed mean, appropriate M-estimators), and choose a bootstrap that matches the estimator and tail regime. When tails are very heavy, consider diagnostics (tail plots, extreme values) and sensitivity checks across multiple robust choices.</p>
</section>
</section>
<section id="parametric-vs-nonparametric-a-decision-framework">
<h2>Parametric vs. Nonparametric: A Decision Framework<a class="headerlink" href="#parametric-vs-nonparametric-a-decision-framework" title="Link to this heading"></a></h2>
<p>Choosing between parametric and nonparametric bootstrap involves balancing <strong>efficiency</strong> against <strong>robustness</strong>.</p>
<table class="docutils align-default" id="id9">
<caption><span class="caption-number">Table 49 </span><span class="caption-text">Parametric vs. Nonparametric Bootstrap</span><a class="headerlink" href="#id9" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 24.0%" />
<col style="width: 38.0%" />
<col style="width: 38.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Criterion</p></th>
<th class="head"><p>Parametric Bootstrap</p></th>
<th class="head"><p>Nonparametric Bootstrap</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Primary assumption</p></td>
<td><p>Data follow a specified family <span class="math notranslate nohighlight">\(\{F_\theta\}\)</span> (after fitting <span class="math notranslate nohighlight">\(\hat\theta\)</span>)</p></td>
<td><p>Uses <span class="math notranslate nohighlight">\(\hat F_n\)</span>; fewer distributional assumptions</p></td>
</tr>
<tr class="row-odd"><td><p>When it shines</p></td>
<td><p>Often more efficient when the model is credible, especially for small <span class="math notranslate nohighlight">\(n\)</span></p></td>
<td><p>Generally more robust to distributional misspecification</p></td>
</tr>
<tr class="row-even"><td><p>Small samples (<span class="math notranslate nohighlight">\(n \lesssim 30\)</span>)</p></td>
<td><p>Can give tighter, more stable inference if the model is correct</p></td>
<td><p>Can be noisier (discrete resampling, higher Monte Carlo variability), but often still reasonable</p></td>
</tr>
<tr class="row-odd"><td><p>Model misspecification</p></td>
<td><p>Can fail badly (bias, undercoverage, misleading SEs)</p></td>
<td><p>Typically more robust, but not universally “always valid”</p></td>
</tr>
<tr class="row-even"><td><p>Complex statistics / functionals</p></td>
<td><p>Often straightforward (simulate from fitted model, recompute statistic)</p></td>
<td><p>Often straightforward (resample data, recompute statistic)</p></td>
</tr>
<tr class="row-odd"><td><p>Support constraints / boundaries</p></td>
<td><p>Can fail for boundary MLEs (e.g., <span class="math notranslate nohighlight">\(\hat\theta = X_{(n)}\)</span>); may require special remedies</p></td>
<td><p>Can also fail or be conservative for extreme order statistics; may require m-out-of-n / subsampling</p></td>
</tr>
<tr class="row-even"><td><p>Computational cost</p></td>
<td><p>Usually <span class="math notranslate nohighlight">\(O(Bn)\)</span> plus model fitting if refit each replicate</p></td>
<td><p>Usually <span class="math notranslate nohighlight">\(O(Bn)\)</span> plus any refitting inside each replicate</p></td>
</tr>
</tbody>
</table>
<section id="decision-flowchart">
<h3>Decision Flowchart<a class="headerlink" href="#decision-flowchart" title="Link to this heading"></a></h3>
<ol class="arabic simple">
<li><p><strong>Do you have strong prior knowledge for a parametric family?</strong></p>
<ul class="simple">
<li><p>No → Start with <strong>nonparametric</strong>.</p></li>
<li><p>Yes → Continue.</p></li>
</ul>
</li>
<li><p><strong>Do basic diagnostics support the model (shape, tails, support)?</strong></p>
<ul class="simple">
<li><p>Clear departures (skewness, multimodality, heavy tails, truncation) → Prefer <strong>nonparametric</strong> or revise the model.</p></li>
<li><p>Diagnostics look reasonable → Continue.</p></li>
</ul>
</li>
<li><p><strong>Is the sample size small (:math:`n lesssim 30`)?</strong></p>
<ul class="simple">
<li><p>Yes and model credible → <strong>Parametric</strong> often improves efficiency and stability.</p></li>
<li><p>Yes and model uncertain → Prefer <strong>nonparametric</strong>, and treat parametric as a sensitivity check only.</p></li>
</ul>
</li>
<li><p><strong>Is the target sensitive to tails or support (extremes, thresholds, quantiles)?</strong></p>
<ul class="simple">
<li><p>Yes → Parametric assumptions matter more; validate aggressively and consider robust or tail-appropriate models.</p></li>
<li><p>No → Either method may work; compare as a robustness check.</p></li>
</ul>
</li>
<li><p><strong>When in doubt</strong>: Run both. If results differ materially, diagnose why (model fit, tails, boundaries, dependence).</p></li>
</ol>
<figure class="align-center" id="id10">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_4_fig08_decision_framework.png"><img alt="Decision framework flowchart for choosing between parametric and nonparametric bootstrap" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_4_fig08_decision_framework.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 158 </span><span class="caption-text"><strong>Figure 4.4.8</strong>: Decision framework for choosing between parametric and nonparametric bootstrap. Parametric bootstrap trades robustness for efficiency. A practical default is to start with nonparametric and use parametric only when the model is credible and diagnostics support it.</span><a class="headerlink" href="#id10" title="Link to this image"></a></p>
</figcaption>
</figure>
<div class="tip admonition">
<p class="admonition-title">Practical Recommendation</p>
<p><strong>Default</strong>: Start with <strong>nonparametric</strong> for robustness, then compare with a parametric bootstrap if a credible model exists.</p>
<p>Prefer <strong>parametric bootstrap</strong> when:</p>
<ol class="arabic simple">
<li><p>You have a well-justified parametric family for <span class="math notranslate nohighlight">\(F\)</span></p></li>
<li><p>Diagnostics do not show clear shape or tail conflicts</p></li>
<li><p><span class="math notranslate nohighlight">\(n\)</span> is small and efficiency matters</p></li>
</ol>
<p>Use <strong>special remedies</strong> near boundaries or extremes (for either method):</p>
<ul class="simple">
<li><p>Exact theory when available (order statistics, known pivots)</p></li>
<li><p>Bias-corrected estimators (e.g., <span class="math notranslate nohighlight">\(\hat\theta_{bc} = \frac{n+1}{n} X_{(n)}\)</span>)</p></li>
<li><p>m-out-of-n bootstrap or subsampling for boundary or extreme-value functionals</p></li>
</ul>
</div>
</section>
</section>
<section id="practical-considerations">
<h2>Practical Considerations<a class="headerlink" href="#practical-considerations" title="Link to this heading"></a></h2>
<section id="computational-efficiency">
<h3>Computational Efficiency<a class="headerlink" href="#computational-efficiency" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Vectorization</strong>: Pre-generate all <span class="math notranslate nohighlight">\(B \times n\)</span> random values at once, then reshape</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Instead of loop:</span>
<span class="c1"># for b in range(B):</span>
<span class="c1">#     eps = rng.normal(0, sigma, n)</span>

<span class="c1"># Vectorized:</span>
<span class="n">eps_all</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
<span class="c1"># Then process all B samples simultaneously</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>Parallel computation</strong>: Bootstrap replicates are independent; parallelize across cores</p></li>
<li><p><strong>Analytical shortcuts</strong>: For some parametric families, bootstrap distributions have closed forms (e.g., normal mean bootstrap is exactly <span class="math notranslate nohighlight">\(N(\hat{\mu}, \hat{\sigma}^2/n)\)</span>)</p></li>
</ul>
</section>
<section id="choosing-b">
<h3>Choosing B<a class="headerlink" href="#choosing-b" title="Link to this heading"></a></h3>
<p>Recommendations mirror nonparametric bootstrap:</p>
<ul class="simple">
<li><p><strong>SE estimation</strong>: <span class="math notranslate nohighlight">\(B = 500\)</span> to <span class="math notranslate nohighlight">\(2{,}000\)</span></p></li>
<li><p><strong>Confidence intervals</strong>: <span class="math notranslate nohighlight">\(B = 2{,}000\)</span> to <span class="math notranslate nohighlight">\(10{,}000\)</span></p></li>
<li><p><strong>P-values</strong>: <span class="math notranslate nohighlight">\(B \geq 10{,}000\)</span> for <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span></p></li>
</ul>
<p>Monte Carlo error decreases as <span class="math notranslate nohighlight">\(O(1/\sqrt{B})\)</span>, so doubling precision requires quadrupling <span class="math notranslate nohighlight">\(B\)</span>.</p>
</section>
<section id="reporting-standards">
<h3>Reporting Standards<a class="headerlink" href="#reporting-standards" title="Link to this heading"></a></h3>
<p>A parametric bootstrap analysis should report:</p>
<ol class="arabic simple">
<li><p><strong>The assumed model</strong>: Distribution family and any fixed parameters</p></li>
<li><p><strong>Estimation method</strong>: MLE, method of moments, etc.</p></li>
<li><p><strong>Model diagnostics</strong>: Evidence supporting distributional assumptions</p></li>
<li><p><strong>Bootstrap details</strong>: Number of replicates <span class="math notranslate nohighlight">\(B\)</span>, random seed</p></li>
<li><p><strong>CI method</strong>: Percentile, bootstrap-t, BCa</p></li>
<li><p><strong>Comparison</strong> (recommended): Results from nonparametric bootstrap as sensitivity check</p></li>
</ol>
</section>
</section>
<section id="bringing-it-all-together">
<h2>Bringing It All Together<a class="headerlink" href="#bringing-it-all-together" title="Link to this heading"></a></h2>
<p>The parametric bootstrap is a powerful tool when model assumptions are credible. It offers improved efficiency and finite-sample performance compared to nonparametric approaches, particularly for small samples and well-understood parametric families. The key tradeoff is robustness: parametric bootstrap can fail badly under model misspecification.</p>
<p>The decision between parametric and nonparametric bootstrap should be guided by:</p>
<ul class="simple">
<li><p>Prior knowledge about the data-generating process</p></li>
<li><p>Sample size and the stakes of the analysis</p></li>
<li><p>Diagnostic evidence for model fit</p></li>
<li><p>Sensitivity of conclusions to method choice</p></li>
</ul>
<div class="important admonition">
<p class="admonition-title">Key Takeaways 📝</p>
<ol class="arabic simple">
<li><p><strong>Core concept</strong>: Parametric bootstrap simulates from a fitted parametric model <span class="math notranslate nohighlight">\(F_{\hat{\theta}}\)</span> rather than resampling from the empirical distribution <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>.</p></li>
<li><p><strong>Efficiency-robustness tradeoff</strong>: Parametric bootstrap is more efficient when the model is correct, but fails under misspecification; nonparametric bootstrap is robust but less efficient.</p></li>
<li><p><strong>When to use parametric</strong>: Small samples with credible models, well-established parametric families, when efficiency matters, or when nonparametric methods encounter boundary issues.</p></li>
<li><p><strong>Critical requirement</strong>: Always validate model assumptions through diagnostic plots and, when possible, compare to nonparametric results.</p></li>
<li><p><strong>Outcome alignment</strong>: This section addresses LO 1 (simulation techniques) and LO 3 (resampling for CIs and variability assessment).</p></li>
</ol>
</div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<p><strong>Primary Sources</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="efron1979" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Efron1979<span class="fn-bracket">]</span></span>
<p>Efron, B. (1979). Bootstrap methods: Another look at the jackknife. <em>The Annals of Statistics</em>, 7(1), 1–26. The original bootstrap paper, introducing resampling from the ECDF as the foundation for all bootstrap methods.</p>
</div>
<div class="citation" id="efrontibshirani1993" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>EfronTibshirani1993<span class="fn-bracket">]</span></span>
<p>Efron, B., and Tibshirani, R. J. (1993). <em>An Introduction to the Bootstrap</em>. Chapman &amp; Hall/CRC. The definitive textbook on bootstrap methods, including comprehensive coverage of parametric approaches.</p>
</div>
<div class="citation" id="davisonhinkley1997" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DavisonHinkley1997<span class="fn-bracket">]</span></span>
<p>Davison, A. C., and Hinkley, D. V. (1997). <em>Bootstrap Methods and Their Application</em>. Cambridge University Press. Comprehensive treatment with extensive coverage of parametric bootstrap, regression methods, and practical applications.</p>
</div>
</div>
<p><strong>Theoretical Foundations</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="hall1992" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Hall1992<span class="fn-bracket">]</span></span>
<p>Hall, P. (1992). <em>The Bootstrap and Edgeworth Expansion</em>. Springer. Mathematical foundations of higher-order accuracy and studentization theory underlying bootstrap confidence intervals.</p>
</div>
<div class="citation" id="beran1997" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Beran1997<span class="fn-bracket">]</span></span>
<p>Beran, R. (1997). Diagnosing bootstrap success. <em>Annals of the Institute of Statistical Mathematics</em>, 49(1), 1–24. Conditions for bootstrap consistency and systematic treatment of failure modes.</p>
</div>
<div class="citation" id="shaotu1995" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ShaoTu1995<span class="fn-bracket">]</span></span>
<p>Shao, J., and Tu, D. (1995). <em>The Jackknife and Bootstrap</em>. Springer. Unified mathematical treatment of resampling methods with rigorous proofs of consistency.</p>
</div>
</div>
<p><strong>Regression Bootstrap</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="freedman1981" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Freedman1981<span class="fn-bracket">]</span></span>
<p>Freedman, D. A. (1981). Bootstrapping regression models. <em>The Annals of Statistics</em>, 9(6), 1218–1228. Foundational paper comparing pairs and residual bootstrap approaches for linear models.</p>
</div>
<div class="citation" id="wu1986" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Wu1986<span class="fn-bracket">]</span></span>
<p>Wu, C. F. J. (1986). Jackknife, bootstrap and other resampling methods in regression analysis. <em>The Annals of Statistics</em>, 14(4), 1261–1295. Introduction of the wild bootstrap for heteroscedasticity with fixed design matrices.</p>
</div>
</div>
<p><strong>Confidence Interval Methods</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="efron1987" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Efron1987<span class="fn-bracket">]</span></span>
<p>Efron, B. (1987). Better bootstrap confidence intervals. <em>Journal of the American Statistical Association</em>, 82(397), 171–185. Introduction of the bias-corrected and accelerated (BCa) interval method.</p>
</div>
<div class="citation" id="diciccioefron1996" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DiCiccioEfron1996<span class="fn-bracket">]</span></span>
<p>DiCiccio, T. J., and Efron, B. (1996). Bootstrap confidence intervals. <em>Statistical Science</em>, 11(3), 189–228. Comprehensive review of percentile, studentized, basic, and BCa confidence interval methods.</p>
</div>
</div>
<p><strong>Computational</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="cantyripley" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CantyRipley<span class="fn-bracket">]</span></span>
<p>Canty, A., and Ripley, B. D. boot: Bootstrap R (S-Plus) Functions. R package. <a class="reference external" href="https://CRAN.R-project.org/package=boot">https://CRAN.R-project.org/package=boot</a>. Standard R implementation of bootstrap methods; version numbers update regularly.</p>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="ch4_3-nonparametric-bootstrap.html" class="btn btn-neutral float-left" title="The Nonparametric Bootstrap" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ch4_5-jackknife-methods.html" class="btn btn-neutral float-right" title="Section 4.5: Jackknife Methods" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>