

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>The Empirical Distribution and Plug-in Principle &mdash; STAT 418</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=3d0abd52" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT418/Website/part2_simulation/chapter4/ch4_2-empirical-distribution-plugin.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=30646c52"></script>
      <script>let toggleHintShow = 'Show solution';</script>
      <script>let toggleHintHide = 'Hide solution';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>window.MathJax = {"options": {"enableMenu": true, "menuOptions": {"settings": {"enrich": true, "speech": true, "braille": true, "collapsible": true, "assistiveMml": false}}}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
      <script src="../../_static/custom.js?v=8718e0ab"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="The Nonparametric Bootstrap" href="ch4_3-nonparametric-bootstrap.html" />
    <link rel="prev" title="The Sampling Distribution Problem" href="ch4_1-sampling-distribution-problem.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Course Content</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../Homework/index.html">Homework Assignments</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Homework/index.html#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#assignment-policies">Assignment Policies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#recommended-workflow">Recommended Workflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#submission-requirements">Submission Requirements</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Homework/index.html#assignments-by-chapter">Assignments by Chapter</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#part-i-foundations">Part I: Foundations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Homework/hw1_distributional_relationships.html">Homework 1: Distributional Relationships and Computational Foundations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Homework/index.html#tips-for-success">Tips for Success</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#mathematical-derivations">Mathematical Derivations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#computational-verification">Computational Verification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#common-pitfalls">Common Pitfalls</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part1_foundations/index.html">Part I: Foundations of Probability and Computation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part1_foundations/chapter1/index.html">Chapter 1: Statistical Paradigms and Core Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html">Paradigms of Probability and Statistical Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#the-mathematical-foundation-kolmogorov-s-axioms">The Mathematical Foundation: Kolmogorov’s Axioms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#interpretations-of-probability">Interpretations of Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#statistical-inference-paradigms">Statistical Inference Paradigms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#historical-and-philosophical-debates">Historical and Philosophical Debates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#looking-ahead-our-course-focus">Looking Ahead: Our Course Focus</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html">Probability Distributions: Theory and Computation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#from-abstract-foundations-to-concrete-tools">From Abstract Foundations to Concrete Tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#the-python-ecosystem-for-probability">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#introduction-why-probability-distributions-matter">Introduction: Why Probability Distributions Matter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#id1">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#continuous-distributions">Continuous Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#additional-important-distributions">Additional Important Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#summary-and-practical-guidelines">Summary and Practical Guidelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#conclusion">Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html">Python Random Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#from-mathematical-distributions-to-computational-samples">From Mathematical Distributions to Computational Samples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-python-ecosystem-at-a-glance">The Python Ecosystem at a Glance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#understanding-pseudo-random-number-generation">Understanding Pseudo-Random Number Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-standard-library-random-module">The Standard Library: <code class="docutils literal notranslate"><span class="pre">random</span></code> Module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#numpy-fast-vectorized-random-sampling">NumPy: Fast Vectorized Random Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#scipy-stats-the-complete-statistical-toolkit">SciPy Stats: The Complete Statistical Toolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#bringing-it-all-together-library-selection-guide">Bringing It All Together: Library Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#looking-ahead-from-random-numbers-to-monte-carlo-methods">Looking Ahead: From Random Numbers to Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html">Chapter 1 Summary: Foundations in Place</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#the-three-pillars-of-chapter-1">The Three Pillars of Chapter 1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#what-lies-ahead-the-road-to-simulation">What Lies Ahead: The Road to Simulation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#chapter-1-exercises-synthesis-problems">Chapter 1 Exercises: Synthesis Problems</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Part II: Simulation-Based Methods</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../chapter2/index.html">Chapter 2: Monte Carlo Simulation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html">Monte Carlo Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#the-historical-development-of-monte-carlo-methods">The Historical Development of Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#the-core-principle-expectation-as-integration">The Core Principle: Expectation as Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#theoretical-foundations">Theoretical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#variance-estimation-and-confidence-intervals">Variance Estimation and Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#comparison-with-deterministic-methods">Comparison with Deterministic Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#sample-size-determination">Sample Size Determination</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#convergence-diagnostics-and-monitoring">Convergence Diagnostics and Monitoring</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#chapter-2-1-exercises-monte-carlo-fundamentals-mastery">Chapter 2.1 Exercises: Monte Carlo Fundamentals Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html">Uniform Random Variates</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#why-uniform-the-universal-currency-of-randomness">Why Uniform? The Universal Currency of Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#the-paradox-of-computational-randomness">The Paradox of Computational Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#chaotic-dynamical-systems-an-instructive-failure">Chaotic Dynamical Systems: An Instructive Failure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#linear-congruential-generators">Linear Congruential Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#shift-register-generators">Shift-Register Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#the-kiss-generator-combining-strategies">The KISS Generator: Combining Strategies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#modern-generators-mersenne-twister-and-pcg">Modern Generators: Mersenne Twister and PCG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#statistical-testing-of-random-number-generators">Statistical Testing of Random Number Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#chapter-2-2-exercises-uniform-random-variates-mastery">Chapter 2.2 Exercises: Uniform Random Variates Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html">Inverse CDF Method</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#mathematical-foundations">Mathematical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#continuous-distributions-with-closed-form-inverses">Continuous Distributions with Closed-Form Inverses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#numerical-inversion">Numerical Inversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#mixed-distributions">Mixed Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#chapter-2-3-exercises-inverse-cdf-method-mastery">Chapter 2.3 Exercises: Inverse CDF Method Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html">Transformation Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#why-transformation-methods">Why Transformation Methods?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-boxmuller-transform">The Box–Muller Transform</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-polar-marsaglia-method">The Polar (Marsaglia) Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#method-comparison-boxmuller-vs-polar-vs-ziggurat">Method Comparison: Box–Muller vs Polar vs Ziggurat</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-ziggurat-algorithm">The Ziggurat Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-clt-approximation-historical">The CLT Approximation (Historical)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#distributions-derived-from-the-normal">Distributions Derived from the Normal</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#multivariate-normal-generation">Multivariate Normal Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#implementation-guidance">Implementation Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#chapter-2-4-exercises-transformation-methods-mastery">Chapter 2.4 Exercises: Transformation Methods Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html">Rejection Sampling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-dartboard-intuition">The Dartboard Intuition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-accept-reject-algorithm">The Accept-Reject Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#efficiency-analysis">Efficiency Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#choosing-the-proposal-distribution">Choosing the Proposal Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-squeeze-principle">The Squeeze Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#geometric-example-sampling-from-the-unit-disk">Geometric Example: Sampling from the Unit Disk</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#limitations-and-the-curse-of-dimensionality">Limitations and the Curse of Dimensionality</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#connections-to-other-methods">Connections to Other Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#chapter-2-5-exercises-rejection-sampling-mastery">Chapter 2.5 Exercises: Rejection Sampling Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html">Variance Reduction Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#the-variance-reduction-paradigm">The Variance Reduction Paradigm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#importance-sampling">Importance Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#control-variates">Control Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#antithetic-variates">Antithetic Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#stratified-sampling">Stratified Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#common-random-numbers">Common Random Numbers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#conditional-monte-carlo-raoblackwellization">Conditional Monte Carlo (Rao–Blackwellization)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#combining-variance-reduction-techniques">Combining Variance Reduction Techniques</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#chapter-2-6-exercises-variance-reduction-mastery">Chapter 2.6 Exercises: Variance Reduction Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html">Chapter Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#the-complete-monte-carlo-workflow">The Complete Monte Carlo Workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#quick-reference-tables">Quick Reference Tables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#common-pitfalls-checklist">Common Pitfalls Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#connections-to-later-chapters">Connections to Later Chapters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#learning-outcomes-checklist">Learning Outcomes Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#further-reading-optimization-and-missing-data">Further Reading: Optimization and Missing Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter3/index.html">Chapter 3: Parametric Inference and Likelihood Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html">Exponential Families</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#historical-origins-from-scattered-results-to-unified-theory">Historical Origins: From Scattered Results to Unified Theory</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#the-canonical-exponential-family">The Canonical Exponential Family</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#converting-familiar-distributions">Converting Familiar Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#the-log-partition-function-a-moment-generating-machine">The Log-Partition Function: A Moment-Generating Machine</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#sufficiency-capturing-all-parameter-information">Sufficiency: Capturing All Parameter Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#minimal-sufficiency-and-completeness">Minimal Sufficiency and Completeness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#conjugate-priors-and-bayesian-inference">Conjugate Priors and Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#exponential-dispersion-models-and-glms">Exponential Dispersion Models and GLMs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#chapter-3-1-exercises-exponential-families-mastery">Chapter 3.1 Exercises: Exponential Families Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html">Maximum Likelihood Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-likelihood-function">The Likelihood Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-score-function">The Score Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#fisher-information">Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#closed-form-maximum-likelihood-estimators">Closed-Form Maximum Likelihood Estimators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#numerical-optimization-for-mle">Numerical Optimization for MLE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#asymptotic-properties-of-mles">Asymptotic Properties of MLEs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-cramer-rao-lower-bound">The Cramér-Rao Lower Bound</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-invariance-property">The Invariance Property</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#likelihood-based-hypothesis-testing">Likelihood-Based Hypothesis Testing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#confidence-intervals-from-likelihood">Confidence Intervals from Likelihood</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#connection-to-bayesian-inference">Connection to Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#chapter-3-2-exercises-maximum-likelihood-estimation-mastery">Chapter 3.2 Exercises: Maximum Likelihood Estimation Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html">Sampling Variability and Variance Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#statistical-estimators-and-their-properties">Statistical Estimators and Their Properties</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#sampling-distributions">Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#the-delta-method">The Delta Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#variance-estimation-methods">Variance Estimation Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#applications-and-worked-examples">Applications and Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html">Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#matrix-calculus-foundations">Matrix Calculus Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#the-linear-model">The Linear Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#ordinary-least-squares-the-calculus-approach">Ordinary Least Squares: The Calculus Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#ordinary-least-squares-the-geometric-approach">Ordinary Least Squares: The Geometric Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#properties-of-the-ols-estimator">Properties of the OLS Estimator</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#the-gauss-markov-theorem">The Gauss-Markov Theorem</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#estimating-the-error-variance">Estimating the Error Variance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#distributional-results-under-normality">Distributional Results Under Normality</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#diagnostics-and-model-checking">Diagnostics and Model Checking</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#numerical-stability-qr-decomposition">Numerical Stability: QR Decomposition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#model-selection-and-information-criteria">Model Selection and Information Criteria</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#regularization-ridge-and-lasso">Regularization: Ridge and LASSO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#chapter-3-4-exercises-linear-models-mastery">Chapter 3.4 Exercises: Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html">Generalized Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#historical-context-unification-of-regression-methods">Historical Context: Unification of Regression Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#the-glm-framework-three-components">The GLM Framework: Three Components</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#score-equations-and-fisher-information">Score Equations and Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#iteratively-reweighted-least-squares">Iteratively Reweighted Least Squares</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#logistic-regression-binary-outcomes">Logistic Regression: Binary Outcomes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#poisson-regression-count-data">Poisson Regression: Count Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#gamma-regression-positive-continuous-data">Gamma Regression: Positive Continuous Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#inference-in-glms-the-testing-triad">Inference in GLMs: The Testing Triad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#model-diagnostics">Model Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#model-comparison-and-selection">Model Comparison and Selection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#quasi-likelihood-and-robust-inference">Quasi-Likelihood and Robust Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#further-reading">Further Reading</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#chapter-3-5-exercises-generalized-linear-models-mastery">Chapter 3.5 Exercises: Generalized Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html">Chapter Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#the-parametric-inference-pipeline">The Parametric Inference Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#the-five-pillars-of-chapter-3">The Five Pillars of Chapter 3</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#quick-reference-core-formulas">Quick Reference: Core Formulas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#connections-to-future-material">Connections to Future Material</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#practical-guidance">Practical Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Chapter 4: Resampling Methods</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html">The Sampling Distribution Problem</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#the-fundamental-target-sampling-distributions">The Fundamental Target: Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#historical-development-the-quest-for-sampling-distributions">Historical Development: The Quest for Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#three-routes-to-the-sampling-distribution">Three Routes to the Sampling Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#when-asymptotics-fail-motivating-the-bootstrap">When Asymptotics Fail: Motivating the Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#the-plug-in-principle-theoretical-foundation">The Plug-In Principle: Theoretical Foundation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#computational-perspective-bootstrap-as-monte-carlo">Computational Perspective: Bootstrap as Monte Carlo</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#chapter-4-1-exercises">Chapter 4.1 Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">The Empirical Distribution and Plug-in Principle</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#the-empirical-cumulative-distribution-function">The Empirical Cumulative Distribution Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="#convergence-of-the-empirical-cdf">Convergence of the Empirical CDF</a></li>
<li class="toctree-l4"><a class="reference internal" href="#parameters-as-statistical-functionals">Parameters as Statistical Functionals</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="#when-the-plug-in-principle-fails">When the Plug-in Principle Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-bootstrap-idea-in-one-sentence">The Bootstrap Idea in One Sentence</a></li>
<li class="toctree-l4"><a class="reference internal" href="#computational-implementation">Computational Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="#section-4-2-exercises-ecdf-and-plug-in-mastery">Section 4.2 Exercises: ECDF and Plug-in Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html">The Nonparametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#the-bootstrap-principle">The Bootstrap Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#bootstrap-standard-errors">Bootstrap Standard Errors</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#bootstrap-bias-estimation">Bootstrap Bias Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#bootstrap-confidence-intervals">Bootstrap Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#bootstrap-for-regression">Bootstrap for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#bootstrap-diagnostics">Bootstrap Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#when-bootstrap-fails">When Bootstrap Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch4_4-parametric-bootstrap.html">Section 4.4: The Parametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#the-parametric-bootstrap-principle">The Parametric Bootstrap Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#location-scale-families">Location-Scale Families</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#parametric-bootstrap-for-regression">Parametric Bootstrap for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#confidence-intervals">Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#model-checking-and-validation">Model Checking and Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#when-parametric-bootstrap-fails">When Parametric Bootstrap Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#parametric-vs-nonparametric-a-decision-framework">Parametric vs. Nonparametric: A Decision Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch4_5-jackknife-methods.html">Section 4.5: Jackknife Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch4_5-jackknife-methods.html#historical-context-and-motivation">Historical Context and Motivation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_5-jackknife-methods.html#the-delete-1-jackknife">The Delete-1 Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_5-jackknife-methods.html#jackknife-bias-estimation">Jackknife Bias Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_5-jackknife-methods.html#the-delete-d-jackknife">The Delete-<span class="math notranslate nohighlight">\(d\)</span> Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_5-jackknife-methods.html#jackknife-versus-bootstrap">Jackknife versus Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_5-jackknife-methods.html#the-infinitesimal-jackknife">The Infinitesimal Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_5-jackknife-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_5-jackknife-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_5-jackknife-methods.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_5-jackknife-methods.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part3_bayesian/index.html">Part III: Bayesian Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part3_bayesian/index.html#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html">Bayesian Philosophy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Part II: Simulation-Based Methods</a></li>
          <li class="breadcrumb-item"><a href="index.html">Chapter 4: Resampling Methods</a></li>
      <li class="breadcrumb-item active">The Empirical Distribution and Plug-in Principle</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/part2_simulation/chapter4/ch4_2-empirical-distribution-plugin.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="the-empirical-distribution-and-plug-in-principle">
<span id="ch4-2-empirical-distribution-plugin"></span><h1>The Empirical Distribution and Plug-in Principle<a class="headerlink" href="#the-empirical-distribution-and-plug-in-principle" title="Link to this heading"></a></h1>
<p>In <a class="reference internal" href="ch4_1-sampling-distribution-problem.html#ch4-1-sampling-distribution-problem"><span class="std std-ref">Section 4.1</span></a>, we identified the sampling distribution <span class="math notranslate nohighlight">\(G\)</span> as the fundamental target of statistical inference—everything we want to know about uncertainty flows from <span class="math notranslate nohighlight">\(G\)</span>. We also introduced three routes to approximating <span class="math notranslate nohighlight">\(G\)</span>: analytic derivation, parametric Monte Carlo, and the bootstrap. The bootstrap route is based on a deceptively simple idea: replace the unknown population distribution <span class="math notranslate nohighlight">\(F\)</span> with the <strong>empirical distribution</strong> <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> computed from our observed sample.</p>
<p>But why should this work? What exactly is the empirical distribution, and in what sense does it approximate <span class="math notranslate nohighlight">\(F\)</span>? How do we formalize the transition from <span class="math notranslate nohighlight">\(\theta = T(F)\)</span> to <span class="math notranslate nohighlight">\(\hat{\theta} = T(\hat{F}_n)\)</span>? These questions lead us to the mathematical foundations that justify all bootstrap methods: the <strong>empirical cumulative distribution function</strong> and the <strong>plug-in principle</strong>.</p>
<p>This section develops these foundations rigorously. We begin with the precise definition of the empirical CDF <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> and establish its interpretation as a discrete probability distribution. We then prove two fundamental convergence theorems—pointwise convergence via the Strong Law of Large Numbers and uniform convergence via the Glivenko-Cantelli theorem—providing self-contained proofs at the level appropriate for this course. The Dvoretzky-Kiefer-Wolfowitz (DKW) inequality gives us finite-sample probability bounds that translate directly into confidence bands for <span class="math notranslate nohighlight">\(F\)</span>. With these tools in hand, we formalize the plug-in principle: viewing parameters as functionals <span class="math notranslate nohighlight">\(\theta = T(F)\)</span> and estimating them by <span class="math notranslate nohighlight">\(\hat{\theta} = T(\hat{F}_n)\)</span>. Finally, we examine cases where the plug-in approach fails, preparing for the more sophisticated methods in later sections.</p>
<div class="important admonition">
<p class="admonition-title">Road Map 🧭</p>
<ul class="simple">
<li><p><strong>Define</strong>: The empirical CDF <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> and its interpretation as a probability distribution</p></li>
<li><p><strong>Prove</strong>: Pointwise and uniform convergence (<span class="math notranslate nohighlight">\(\hat{F}_n \to F\)</span>) with complete, self-contained proofs</p></li>
<li><p><strong>Quantify</strong>: Finite-sample bounds via the DKW inequality; construct confidence bands</p></li>
<li><p><strong>Formalize</strong>: Parameters as functionals <span class="math notranslate nohighlight">\(T(F)\)</span> and the plug-in estimator <span class="math notranslate nohighlight">\(\hat{\theta} = T(\hat{F}_n)\)</span></p></li>
<li><p><strong>Diagnose</strong>: When plug-in fails and alternatives are needed</p></li>
</ul>
<p><strong>Learning Outcomes</strong>: LO 1 (simulation techniques—understanding random generation from <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>), LO 3 (resampling methods—theoretical foundation for bootstrap)</p>
</div>
<section id="the-empirical-cumulative-distribution-function">
<h2>The Empirical Cumulative Distribution Function<a class="headerlink" href="#the-empirical-cumulative-distribution-function" title="Link to this heading"></a></h2>
<p>The empirical CDF is perhaps the most natural nonparametric estimator of a distribution function. Given observed data, it simply counts the proportion of observations at or below each threshold.</p>
<section id="definition-and-basic-properties">
<h3>Definition and Basic Properties<a class="headerlink" href="#definition-and-basic-properties" title="Link to this heading"></a></h3>
<div class="note admonition">
<p class="admonition-title">Definition: Empirical Cumulative Distribution Function</p>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> be observations from a distribution <span class="math notranslate nohighlight">\(F\)</span>. The <strong>empirical cumulative distribution function</strong> (ECDF) is:</p>
<div class="math notranslate nohighlight" id="equation-ecdf-def">
<span class="eqno">(157)<a class="headerlink" href="#equation-ecdf-def" title="Link to this equation"></a></span>\[\hat{F}_n(x) = \frac{1}{n} \sum_{i=1}^{n} \mathbf{1}\{X_i \leq x\}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{1}\{X_i \leq x\}\)</span> is the indicator function that equals 1 if <span class="math notranslate nohighlight">\(X_i \leq x\)</span> and 0 otherwise.</p>
</div>
<p>Equivalently, <span class="math notranslate nohighlight">\(\hat{F}_n(x)\)</span> is the proportion of observations less than or equal to <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="math notranslate nohighlight">
\[\hat{F}_n(x) = \frac{\#\{i : X_i \leq x\}}{n}\]</div>
<p>The ECDF is a <strong>right-continuous step function</strong> that:</p>
<ul class="simple">
<li><p>Equals 0 for <span class="math notranslate nohighlight">\(x &lt; X_{(1)}\)</span> (below the minimum observation)</p></li>
<li><p>Jumps by <span class="math notranslate nohighlight">\(1/n\)</span> at each distinct observation (or by <span class="math notranslate nohighlight">\(k/n\)</span> if <span class="math notranslate nohighlight">\(k\)</span> observations share the same value)</p></li>
<li><p>Equals 1 for <span class="math notranslate nohighlight">\(x \geq X_{(n)}\)</span> (at or above the maximum observation)</p></li>
</ul>
<p>Here <span class="math notranslate nohighlight">\(X_{(1)} \leq X_{(2)} \leq \cdots \leq X_{(n)}\)</span> denote the <strong>order statistics</strong>—the observations sorted in increasing order.</p>
<div class="note admonition">
<p class="admonition-title">Example 💡 Computing the ECDF</p>
<p><strong>Given</strong>: Sample <span class="math notranslate nohighlight">\(X = (2.3, 5.1, 1.7, 3.8, 2.3)\)</span></p>
<p><strong>Order statistics</strong>: <span class="math notranslate nohighlight">\(X_{(1)} = 1.7, X_{(2)} = 2.3, X_{(3)} = 2.3, X_{(4)} = 3.8, X_{(5)} = 5.1\)</span></p>
<p><strong>ECDF values</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\hat{F}_5(x) = \begin{cases}
   0 &amp; x &lt; 1.7 \\
   1/5 = 0.2 &amp; 1.7 \leq x &lt; 2.3 \\
   3/5 = 0.6 &amp; 2.3 \leq x &lt; 3.8 \\
   4/5 = 0.8 &amp; 3.8 \leq x &lt; 5.1 \\
   1 &amp; x \geq 5.1
\end{cases}\end{split}\]</div>
<p>Note the jump of <span class="math notranslate nohighlight">\(2/5\)</span> at <span class="math notranslate nohighlight">\(x = 2.3\)</span> because two observations equal 2.3.</p>
</div>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_2_fig01_ecdf_definition.png"><img alt="Three-panel figure showing (a) mass 1/n at each observation, (b) ECDF as step function, and (c) formal definition" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_2_fig01_ecdf_definition.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 133 </span><span class="caption-text"><strong>Figure 4.2.1</strong>: The Empirical CDF. (a) The ECDF places probability mass <span class="math notranslate nohighlight">\(1/n\)</span> at each observed value. (b) As a CDF, <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> is a right-continuous step function that jumps at each observation. (c) The formal definition shows <span class="math notranslate nohighlight">\(\hat{F}_n(x)\)</span> counts the proportion of observations at or below <span class="math notranslate nohighlight">\(x\)</span>.</span><a class="headerlink" href="#id1" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="the-ecdf-as-a-probability-distribution">
<h3>The ECDF as a Probability Distribution<a class="headerlink" href="#the-ecdf-as-a-probability-distribution" title="Link to this heading"></a></h3>
<p>A crucial insight is that <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> is not just an estimator—it is itself a <strong>legitimate probability distribution</strong>. Specifically, <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> is the CDF of a discrete distribution that places probability mass <span class="math notranslate nohighlight">\(1/n\)</span> on each observed value.</p>
<div class="note admonition">
<p class="admonition-title">Proposition: ECDF as Discrete Distribution</p>
<p>The empirical CDF <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> corresponds to a discrete probability measure <span class="math notranslate nohighlight">\(\hat{P}_n\)</span> that assigns mass <span class="math notranslate nohighlight">\(1/n\)</span> to each observation:</p>
<div class="math notranslate nohighlight" id="equation-ecdf-measure">
<span class="eqno">(158)<a class="headerlink" href="#equation-ecdf-measure" title="Link to this equation"></a></span>\[\hat{P}_n(\{X_i\}) = \frac{1}{n} \quad \text{for } i = 1, \ldots, n\]</div>
<p>For any measurable set <span class="math notranslate nohighlight">\(A\)</span>:</p>
<div class="math notranslate nohighlight">
\[\hat{P}_n(A) = \frac{1}{n} \sum_{i=1}^{n} \mathbf{1}\{X_i \in A\} = \frac{\#\{i : X_i \in A\}}{n}\]</div>
</div>
<p>This interpretation is fundamental to the bootstrap. Sampling “from <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>” means selecting observations from <span class="math notranslate nohighlight">\(\{X_1, \ldots, X_n\}\)</span> with equal probability <span class="math notranslate nohighlight">\(1/n\)</span> for each. Since we sample with replacement, the same observation can appear multiple times in a bootstrap sample.</p>
<p><strong>Connection to Multinomial Distribution</strong></p>
<p>The bootstrap sampling process has a direct connection to the multinomial distribution. If we draw a bootstrap sample of size <span class="math notranslate nohighlight">\(n\)</span> from <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>, let <span class="math notranslate nohighlight">\(N_i\)</span> denote the number of times observation <span class="math notranslate nohighlight">\(X_i\)</span> appears in the bootstrap sample. Then:</p>
<div class="math notranslate nohighlight" id="equation-bootstrap-multinomial">
<span class="eqno">(159)<a class="headerlink" href="#equation-bootstrap-multinomial" title="Link to this equation"></a></span>\[(N_1, N_2, \ldots, N_n) \sim \text{Multinomial}\left(n; \frac{1}{n}, \frac{1}{n}, \ldots, \frac{1}{n}\right)\]</div>
<p>This means:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}[N_i] = 1\)</span> for each <span class="math notranslate nohighlight">\(i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\text{Var}(N_i) = (n-1)/n\)</span> for each <span class="math notranslate nohighlight">\(i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\text{Cov}(N_i, N_j) = -1/n\)</span> for <span class="math notranslate nohighlight">\(i \neq j\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_{i=1}^n N_i = n\)</span> (exactly <span class="math notranslate nohighlight">\(n\)</span> observations in bootstrap sample)</p></li>
</ul>
<p>The probability that a specific observation <span class="math notranslate nohighlight">\(X_i\)</span> is <strong>not</strong> included in a bootstrap sample is:</p>
<div class="math notranslate nohighlight" id="equation-not-included-prob">
<span class="eqno">(160)<a class="headerlink" href="#equation-not-included-prob" title="Link to this equation"></a></span>\[P(N_i = 0) = \left(1 - \frac{1}{n}\right)^n \to e^{-1} \approx 0.368 \quad \text{as } n \to \infty\]</div>
<p>Thus, on average, about 63.2% of the original observations appear in each bootstrap sample, with some appearing multiple times.</p>
</section>
<section id="the-ecdf-as-nonparametric-mle">
<h3>The ECDF as Nonparametric MLE<a class="headerlink" href="#the-ecdf-as-nonparametric-mle" title="Link to this heading"></a></h3>
<p>The empirical distribution <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> has a compelling optimality property: it is the <strong>nonparametric maximum likelihood estimator</strong> within the class of discrete distributions.</p>
<p>To formalize this, we consider the <strong>empirical likelihood</strong> framework. Among all discrete probability distributions supported on the observed sample points <span class="math notranslate nohighlight">\(\{X_1, \ldots, X_n\}\)</span>, we seek the one that maximizes the probability of the observed data. If we assign probability <span class="math notranslate nohighlight">\(p_i\)</span> to observation <span class="math notranslate nohighlight">\(X_i\)</span> (treating each observation as potentially distinct), the likelihood is:</p>
<div class="math notranslate nohighlight">
\[L(p_1, \ldots, p_n) = \prod_{i=1}^n p_i \quad \text{subject to} \quad \sum_{i=1}^n p_i = 1, \; p_i \geq 0\]</div>
<p>Maximizing <span class="math notranslate nohighlight">\(\prod p_i\)</span> subject to <span class="math notranslate nohighlight">\(\sum p_i = 1\)</span> yields <span class="math notranslate nohighlight">\(p_i = 1/n\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> by Lagrange multipliers (or by recognizing the geometric-arithmetic mean inequality). This is precisely the empirical distribution.</p>
<p>When there are ties (repeated values), grouping by distinct values <span class="math notranslate nohighlight">\(a_1, \ldots, a_k\)</span> with counts <span class="math notranslate nohighlight">\(n_1, \ldots, n_k\)</span>, the likelihood becomes <span class="math notranslate nohighlight">\(\prod_{j=1}^k p_j^{n_j}\)</span>, maximized at <span class="math notranslate nohighlight">\(p_j = n_j/n\)</span>.</p>
<div class="note admonition">
<p class="admonition-title">Theorem: ECDF is Nonparametric MLE</p>
<p>Among all discrete probability distributions supported on the observed sample values, the empirical distribution <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> maximizes the likelihood. The MLE places mass <span class="math notranslate nohighlight">\(1/n\)</span> at each observation (or <span class="math notranslate nohighlight">\(n_j/n\)</span> at each distinct value appearing <span class="math notranslate nohighlight">\(n_j\)</span> times).</p>
</div>
<p><strong>Remark</strong>: For continuous models, the likelihood is typically defined via densities and depends on the chosen dominating measure. The empirical likelihood approach avoids these complications by working directly with probabilities on the observed support points, yielding a clean optimization problem with a unique solution.</p>
</section>
<section id="python-implementation">
<h3>Python Implementation<a class="headerlink" href="#python-implementation" title="Link to this heading"></a></h3>
<p>Computing and visualizing the ECDF is straightforward:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">compute_ecdf</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the empirical CDF of a sample.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : array-like</span>
<span class="sd">        Sample observations.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    x_sorted : ndarray</span>
<span class="sd">        Sorted sample values (may include duplicates).</span>
<span class="sd">    ecdf_vals : ndarray</span>
<span class="sd">        ECDF values at each sorted value (i/n for i=1,...,n).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x_sorted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">ecdf_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span>
    <span class="k">return</span> <span class="n">x_sorted</span><span class="p">,</span> <span class="n">ecdf_vals</span>

<span class="k">def</span><span class="w"> </span><span class="nf">plot_ecdf_vs_true</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">true_cdf</span><span class="p">,</span> <span class="n">true_cdf_label</span><span class="o">=</span><span class="s1">&#39;True F&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plot ECDF overlaid on true CDF.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : array-like</span>
<span class="sd">        Sample observations.</span>
<span class="sd">    true_cdf : callable</span>
<span class="sd">        True CDF function.</span>
<span class="sd">    true_cdf_label : str</span>
<span class="sd">        Label for true CDF.</span>
<span class="sd">    ax : matplotlib axis, optional</span>
<span class="sd">        Axis to plot on.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ax : matplotlib axis</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

    <span class="c1"># Compute ECDF</span>
    <span class="n">x_sorted</span><span class="p">,</span> <span class="n">ecdf_vals</span> <span class="o">=</span> <span class="n">compute_ecdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Plot ECDF as step function</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x_sorted</span><span class="p">,</span> <span class="n">ecdf_vals</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;ECDF (n=</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#E74C3C&#39;</span><span class="p">)</span>

    <span class="c1"># Plot true CDF</span>
    <span class="n">x_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_sorted</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x_sorted</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_grid</span><span class="p">,</span> <span class="n">true_cdf</span><span class="p">(</span><span class="n">x_grid</span><span class="p">),</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="n">true_cdf_label</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#2E86AB&#39;</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;F(x)&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">ax</span>

<span class="c1"># Example: Compare ECDF to true Normal CDF</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plot_ecdf_vs_true</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">,</span> <span class="s1">&#39;True N(0,1)&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;ECDF vs True CDF (n=50 from Standard Normal)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_2_fig02_ecdf_vs_true.png"><img alt="Four-panel comparison of ECDF versus true CDF for different distributions and sample sizes" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_2_fig02_ecdf_vs_true.png" style="width: 90%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 134 </span><span class="caption-text"><strong>Figure 4.2.2</strong>: ECDF Approximates True CDF. Comparison across different distributions (Normal, Exponential, Beta) and sample sizes. Each panel shows the true CDF (blue) overlaid with the ECDF (red), with the supremum deviation <span class="math notranslate nohighlight">\(D_n\)</span> annotated. As sample size increases, <span class="math notranslate nohighlight">\(D_n\)</span> decreases.</span><a class="headerlink" href="#id2" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="convergence-of-the-empirical-cdf">
<h2>Convergence of the Empirical CDF<a class="headerlink" href="#convergence-of-the-empirical-cdf" title="Link to this heading"></a></h2>
<p>The utility of <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> as an estimator of <span class="math notranslate nohighlight">\(F\)</span> rests on convergence theorems that guarantee <span class="math notranslate nohighlight">\(\hat{F}_n \to F\)</span> as <span class="math notranslate nohighlight">\(n \to \infty\)</span>. We establish two levels of convergence: pointwise (for each fixed <span class="math notranslate nohighlight">\(x\)</span>) and uniform (simultaneously for all <span class="math notranslate nohighlight">\(x\)</span>). The proofs are self-contained.</p>
<section id="preliminary-tools">
<h3>Preliminary Tools<a class="headerlink" href="#preliminary-tools" title="Link to this heading"></a></h3>
<p>We first establish the basic probability inequalities needed for the proofs.</p>
<div class="note admonition">
<p class="admonition-title">Lemma: Markov’s Inequality</p>
<p>If <span class="math notranslate nohighlight">\(X \geq 0\)</span> is a non-negative random variable with <span class="math notranslate nohighlight">\(\mathbb{E}[X] &lt; \infty\)</span>, then for any <span class="math notranslate nohighlight">\(a &gt; 0\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-markov">
<span class="eqno">(161)<a class="headerlink" href="#equation-markov" title="Link to this equation"></a></span>\[P(X \geq a) \leq \frac{\mathbb{E}[X]}{a}\]</div>
</div>
<p><strong>Proof of Markov’s Inequality</strong>: Since <span class="math notranslate nohighlight">\(X \geq 0\)</span>, we have <span class="math notranslate nohighlight">\(X \geq a \cdot \mathbf{1}\{X \geq a\}\)</span> (the indicator is 1 when <span class="math notranslate nohighlight">\(X \geq a\)</span>, and <span class="math notranslate nohighlight">\(X \geq a\)</span> in that case). Taking expectations:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[X] \geq \mathbb{E}[a \cdot \mathbf{1}\{X \geq a\}] = a \cdot P(X \geq a)\]</div>
<p>Rearranging gives the result. ∎</p>
<p><strong>Intuition</strong>: If a non-negative random variable has small expectation, it cannot be large too often—otherwise the expectation would be large.</p>
<div class="note admonition">
<p class="admonition-title">Lemma: Chebyshev’s Inequality</p>
<p>If <span class="math notranslate nohighlight">\(Z\)</span> has mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2 &lt; \infty\)</span>, then for any <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-chebyshev">
<span class="eqno">(162)<a class="headerlink" href="#equation-chebyshev" title="Link to this equation"></a></span>\[P\bigl(|Z - \mu| \geq \varepsilon\bigr) \leq \frac{\sigma^2}{\varepsilon^2}\]</div>
</div>
<p><strong>Proof of Chebyshev’s Inequality</strong>: The key insight is to convert a question about <span class="math notranslate nohighlight">\(|Z - \mu|\)</span> into a question about the non-negative quantity <span class="math notranslate nohighlight">\((Z - \mu)^2\)</span>, then apply Markov.</p>
<p><em>Step 1</em>: Note that <span class="math notranslate nohighlight">\(|Z - \mu| \geq \varepsilon\)</span> if and only if <span class="math notranslate nohighlight">\((Z - \mu)^2 \geq \varepsilon^2\)</span>.</p>
<p><em>Step 2</em>: Apply Markov’s inequality to <span class="math notranslate nohighlight">\(X = (Z - \mu)^2\)</span> with <span class="math notranslate nohighlight">\(a = \varepsilon^2\)</span>:</p>
<div class="math notranslate nohighlight">
\[P\bigl(|Z - \mu| \geq \varepsilon\bigr) = P\bigl((Z - \mu)^2 \geq \varepsilon^2\bigr) \leq \frac{\mathbb{E}\bigl[(Z - \mu)^2\bigr]}{\varepsilon^2} = \frac{\sigma^2}{\varepsilon^2}\]</div>
<p>∎</p>
<p><strong>Intuition</strong>: Chebyshev says that a random variable is unlikely to be far from its mean if it has small variance. The probability of being more than <span class="math notranslate nohighlight">\(k\)</span> standard deviations from the mean is at most <span class="math notranslate nohighlight">\(1/k^2\)</span>.</p>
<div class="note admonition">
<p class="admonition-title">Lemma: First Borel-Cantelli</p>
<p>If <span class="math notranslate nohighlight">\(\{A_n\}\)</span> is a sequence of events with <span class="math notranslate nohighlight">\(\sum_{n=1}^{\infty} P(A_n) &lt; \infty\)</span>, then <span class="math notranslate nohighlight">\(P(A_n \text{ i.o.}) = 0\)</span>, where “i.o.” means “infinitely often.”</p>
</div>
<p><strong>What Does “Infinitely Often” Mean?</strong></p>
<p>The event “<span class="math notranslate nohighlight">\(A_n\)</span> infinitely often” (abbreviated <span class="math notranslate nohighlight">\(A_n\)</span> i.o.) means that <span class="math notranslate nohighlight">\(A_n\)</span> occurs for infinitely many values of <span class="math notranslate nohighlight">\(n\)</span>. In set notation:</p>
<div class="math notranslate nohighlight">
\[\{A_n \text{ i.o.}\} = \{\omega : \omega \in A_n \text{ for infinitely many } n\}\]</div>
<p>An outcome <span class="math notranslate nohighlight">\(\omega\)</span> belongs to this set if no matter how large an <span class="math notranslate nohighlight">\(N\)</span> we choose, there exists some <span class="math notranslate nohighlight">\(n \geq N\)</span> with <span class="math notranslate nohighlight">\(\omega \in A_n\)</span>.</p>
<p><strong>Equivalent Set-Theoretic Definition</strong></p>
<p>We can write this more precisely. First, define the event “at least one <span class="math notranslate nohighlight">\(A_n\)</span> occurs for <span class="math notranslate nohighlight">\(n \geq N\)</span>”:</p>
<div class="math notranslate nohighlight">
\[B_N = \bigcup_{n \geq N} A_n = A_N \cup A_{N+1} \cup A_{N+2} \cup \cdots\]</div>
<p>An outcome <span class="math notranslate nohighlight">\(\omega \in B_N\)</span> if <span class="math notranslate nohighlight">\(\omega\)</span> belongs to at least one of <span class="math notranslate nohighlight">\(A_N, A_{N+1}, A_{N+2}, \ldots\)</span></p>
<p>Now, “<span class="math notranslate nohighlight">\(A_n\)</span> infinitely often” means <span class="math notranslate nohighlight">\(\omega\)</span> is in <span class="math notranslate nohighlight">\(B_N\)</span> for <strong>every</strong> <span class="math notranslate nohighlight">\(N\)</span>:</p>
<div class="math notranslate nohighlight">
\[\{A_n \text{ i.o.}\} = \bigcap_{N=1}^{\infty} B_N = \bigcap_{N=1}^{\infty} \bigcup_{n \geq N} A_n\]</div>
<p><strong>Intuition</strong>: No matter how far out you start looking (at <span class="math notranslate nohighlight">\(N = 1, 2, 3, \ldots\)</span>), you always find at least one more <span class="math notranslate nohighlight">\(A_n\)</span> occurring.</p>
<p><strong>Proof of Borel-Cantelli</strong></p>
<p><em>Step 1: Bound the probability of</em> <span class="math notranslate nohighlight">\(B_N\)</span>.</p>
<p>By the union bound (subadditivity of probability):</p>
<div class="math notranslate nohighlight">
\[P(B_N) = P\left(\bigcup_{n \geq N} A_n\right) \leq \sum_{n=N}^{\infty} P(A_n)\]</div>
<p>This is the “tail” of the series starting at <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p><em>Step 2: Show the tail goes to zero.</em></p>
<p>Since the full series <span class="math notranslate nohighlight">\(\sum_{n=1}^{\infty} P(A_n) &lt; \infty\)</span> converges, the tail must vanish:</p>
<div class="math notranslate nohighlight">
\[\sum_{n=N}^{\infty} P(A_n) \to 0 \quad \text{as } N \to \infty\]</div>
<p>(This is a basic fact about convergent series: if <span class="math notranslate nohighlight">\(\sum a_n &lt; \infty\)</span>, then <span class="math notranslate nohighlight">\(\sum_{n \geq N} a_n \to 0\)</span>.)</p>
<p><em>Step 3: Use the nested structure.</em></p>
<p>Notice that <span class="math notranslate nohighlight">\(B_1 \supseteq B_2 \supseteq B_3 \supseteq \cdots\)</span> (each <span class="math notranslate nohighlight">\(B_N\)</span> is contained in the previous one—we’re looking at fewer and fewer events as <span class="math notranslate nohighlight">\(N\)</span> increases). Therefore:</p>
<div class="math notranslate nohighlight">
\[\{A_n \text{ i.o.}\} = \bigcap_{N=1}^{\infty} B_N\]</div>
<p><em>Step 4: Apply continuity of probability for nested sets.</em></p>
<p>For a decreasing sequence of sets <span class="math notranslate nohighlight">\(B_1 \supseteq B_2 \supseteq \cdots\)</span>, probability is continuous:</p>
<div class="math notranslate nohighlight">
\[P\left(\bigcap_{N=1}^{\infty} B_N\right) = \lim_{N \to \infty} P(B_N)\]</div>
<p>(This is analogous to how for nested intervals <span class="math notranslate nohighlight">\([a_1, b_1] \supseteq [a_2, b_2] \supseteq \cdots\)</span>, the intersection has length equal to the limit of the lengths.)</p>
<p><em>Step 5: Conclude.</em></p>
<p>Combining the pieces:</p>
<div class="math notranslate nohighlight">
\[P(A_n \text{ i.o.}) = \lim_{N \to \infty} P(B_N) \leq \lim_{N \to \infty} \sum_{n=N}^{\infty} P(A_n) = 0\]</div>
<p>∎</p>
<div class="tip admonition">
<p class="admonition-title">Why Borel-Cantelli Matters</p>
<p>The Borel-Cantelli Lemma converts <strong>summable probabilities</strong> into <strong>almost sure statements</strong>. If the probabilities of “bad events” <span class="math notranslate nohighlight">\(A_n\)</span> are summable, then with probability 1, only finitely many bad events occur—so eventually (for all large enough <span class="math notranslate nohighlight">\(n\)</span>) the bad events stop happening.</p>
<p>We use this to prove <strong>almost sure convergence</strong>: if <span class="math notranslate nohighlight">\(P\bigl(|X_n - X| &gt; \varepsilon\bigr)\)</span> is summable for each <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span>, then <span class="math notranslate nohighlight">\(X_n \to X\)</span> almost surely.</p>
</div>
</section>
<section id="pointwise-convergence">
<h3>Pointwise Convergence<a class="headerlink" href="#pointwise-convergence" title="Link to this heading"></a></h3>
<p>For any fixed <span class="math notranslate nohighlight">\(x\)</span>, the quantity <span class="math notranslate nohighlight">\(n \hat{F}_n(x) = \sum_{i=1}^n \mathbf{1}\{X_i \leq x\}\)</span> is a sum of iid Bernoulli random variables with success probability <span class="math notranslate nohighlight">\(p = F(x)\)</span>. The Strong Law of Large Numbers immediately yields pointwise almost sure convergence.</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Pointwise Convergence of ECDF</p>
<p>For each fixed <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-pointwise-prob">
<span class="eqno">(163)<a class="headerlink" href="#equation-pointwise-prob" title="Link to this equation"></a></span>\[\hat{F}_n(x) \xrightarrow{P} F(x) \quad \text{(convergence in probability)}\]</div>
<div class="math notranslate nohighlight" id="equation-pointwise-as">
<span class="eqno">(164)<a class="headerlink" href="#equation-pointwise-as" title="Link to this equation"></a></span>\[\hat{F}_n(x) \xrightarrow{a.s.} F(x) \quad \text{(almost sure convergence)}\]</div>
</div>
<p><strong>Proof of Convergence in Probability</strong>:</p>
<p>Define <span class="math notranslate nohighlight">\(Y_i(x) = \mathbf{1}\{X_i \leq x\}\)</span>. Then <span class="math notranslate nohighlight">\(Y_i(x) \sim \text{Bernoulli}(F(x))\)</span> and:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[Y_i(x)] = F(x), \quad \text{Var}(Y_i(x)) = F(x)(1 - F(x))\]</div>
<p>Since <span class="math notranslate nohighlight">\(\hat{F}_n(x) = \bar{Y}_n = \frac{1}{n}\sum_{i=1}^n Y_i(x)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\hat{F}_n(x)) = \frac{F(x)(1 - F(x))}{n} \leq \frac{1}{4n}\]</div>
<p>By Chebyshev’s inequality:</p>
<div class="math notranslate nohighlight">
\[P\bigl(|\hat{F}_n(x) - F(x)| &gt; \varepsilon\bigr) \leq \frac{F(x)(1 - F(x))}{n\varepsilon^2} \leq \frac{1}{4n\varepsilon^2} \to 0\]</div>
<p>∎</p>
<p><strong>Proof of Almost Sure Convergence</strong>:</p>
<p>We invoke the <strong>Strong Law of Large Numbers</strong> (SLLN) for i.i.d. sequences:</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Strong Law of Large Numbers</p>
<p>If <span class="math notranslate nohighlight">\(Y_1, Y_2, \ldots\)</span> are i.i.d. with <span class="math notranslate nohighlight">\(\mathbb{E}|Y_1| &lt; \infty\)</span>, then:</p>
<div class="math notranslate nohighlight">
\[\bar{Y}_n = \frac{1}{n}\sum_{i=1}^n Y_i \xrightarrow{a.s.} \mathbb{E}[Y_1]\]</div>
</div>
<p><strong>What does “almost surely” mean?</strong></p>
<p>The notation <span class="math notranslate nohighlight">\(\bar{Y}_n \xrightarrow{a.s.} \mu\)</span> (read “converges almost surely”) means:</p>
<div class="math notranslate nohighlight">
\[P\left(\lim_{n \to \infty} \bar{Y}_n = \mu\right) = 1\]</div>
<p>In words: the probability that <span class="math notranslate nohighlight">\(\bar{Y}_n\)</span> converges to <span class="math notranslate nohighlight">\(\mu\)</span> is 1. There might be some “bad” outcomes <span class="math notranslate nohighlight">\(\omega\)</span> where convergence fails, but the set of all such bad outcomes has probability zero.</p>
<p><strong>Contrast with convergence in probability</strong>: <span class="math notranslate nohighlight">\(\bar{Y}_n \xrightarrow{P} \mu\)</span> means for each <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span>, <span class="math notranslate nohighlight">\(P\bigl(|\bar{Y}_n - \mu| &gt; \varepsilon\bigr) \to 0\)</span>. This is weaker: it says each individual deviation becomes unlikely, but doesn’t rule out infinitely many small deviations.</p>
<p><strong>Almost sure convergence is stronger</strong>: <span class="math notranslate nohighlight">\(\bar{Y}_n \xrightarrow{a.s.} \mu\)</span> implies <span class="math notranslate nohighlight">\(\bar{Y}_n \xrightarrow{P} \mu\)</span>, but not conversely.</p>
<p><strong>Applying the SLLN to the ECDF</strong>:</p>
<p>Since <span class="math notranslate nohighlight">\(Y_i(x) = \mathbf{1}\{X_i \leq x\}\)</span> are i.i.d. Bernoulli with <span class="math notranslate nohighlight">\(\mathbb{E}[Y_i(x)] = F(x)\)</span>, and Bernoulli random variables trivially satisfy <span class="math notranslate nohighlight">\(\mathbb{E}|Y_i(x)| = F(x) \leq 1 &lt; \infty\)</span>, the SLLN immediately yields:</p>
<div class="math notranslate nohighlight">
\[\hat{F}_n(x) = \bar{Y}_n \xrightarrow{a.s.} \mathbb{E}[Y_1(x)] = F(x)\]</div>
<p>∎</p>
<p><em>Technical remark</em>: The proof of the SLLN itself is beyond our scope, but uses tools like Kolmogorov’s variance-summability criterion or truncation arguments. For Bernoulli random variables, the variance <span class="math notranslate nohighlight">\(\text{Var}(Y_i) \leq 1/4\)</span>, so <span class="math notranslate nohighlight">\(\sum_{i=1}^\infty \text{Var}(Y_i)/i^2 \leq \frac{1}{4}\sum_{i=1}^\infty 1/i^2 &lt; \infty\)</span>, satisfying Kolmogorov’s criterion.</p>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ⚠️ Pointwise vs Uniform Convergence</p>
<p>Pointwise convergence <span class="math notranslate nohighlight">\(\hat{F}_n(x) \to F(x)\)</span> for each fixed <span class="math notranslate nohighlight">\(x\)</span> does <strong>not</strong> immediately imply that <span class="math notranslate nohighlight">\(\sup_x |\hat{F}_n(x) - F(x)| \to 0\)</span>. The supremum is over uncountably many <span class="math notranslate nohighlight">\(x\)</span> values, and convergence at each point separately does not guarantee simultaneous convergence.</p>
<p><strong>Example of the gap</strong>: Consider <span class="math notranslate nohighlight">\(f_n(x) = x^n\)</span> on <span class="math notranslate nohighlight">\([0,1]\)</span>. For each <span class="math notranslate nohighlight">\(x \in [0,1)\)</span>, <span class="math notranslate nohighlight">\(f_n(x) \to 0\)</span>. But <span class="math notranslate nohighlight">\(\sup_{x \in [0,1]} |f_n(x) - 0| = 1\)</span> for all <span class="math notranslate nohighlight">\(n\)</span> (achieved at <span class="math notranslate nohighlight">\(x = 1\)</span>).</p>
<p>For the ECDF, proving uniform convergence requires additional work—this is the content of the Glivenko-Cantelli theorem.</p>
</div>
</section>
<section id="uniform-convergence-the-glivenko-cantelli-theorem">
<h3>Uniform Convergence: The Glivenko-Cantelli Theorem<a class="headerlink" href="#uniform-convergence-the-glivenko-cantelli-theorem" title="Link to this heading"></a></h3>
<p>The Glivenko-Cantelli theorem is one of the foundational results of nonparametric statistics. It states that the ECDF converges uniformly to the true CDF, not just pointwise.</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Glivenko-Cantelli</p>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots\)</span> be iid with distribution function <span class="math notranslate nohighlight">\(F\)</span>. Define <span class="math notranslate nohighlight">\(D_n = \sup_{x \in \mathbb{R}} |\hat{F}_n(x) - F(x)|\)</span>. Then:</p>
<div class="math notranslate nohighlight" id="equation-glivenko-cantelli">
<span class="eqno">(165)<a class="headerlink" href="#equation-glivenko-cantelli" title="Link to this equation"></a></span>\[D_n \xrightarrow{a.s.} 0 \quad \text{as } n \to \infty\]</div>
<p>That is, with probability 1, the empirical CDF converges uniformly to the true CDF.</p>
</div>
<p>Before proving the theorem, we establish an exponential probability bound on <span class="math notranslate nohighlight">\(D_n\)</span>.</p>
<p><strong>Reduction to Uniform Distribution</strong></p>
<p>When <span class="math notranslate nohighlight">\(F\)</span> is continuous, the <strong>probability integral transformation</strong> provides a useful proof device. Define <span class="math notranslate nohighlight">\(U_i = F(X_i)\)</span>. If <span class="math notranslate nohighlight">\(F\)</span> is continuous and strictly increasing, then <span class="math notranslate nohighlight">\(U_i \sim \text{Uniform}(0,1)\)</span>. Let <span class="math notranslate nohighlight">\(\hat{H}_n\)</span> be the ECDF of <span class="math notranslate nohighlight">\(U_1, \ldots, U_n\)</span>.</p>
<p>Then:</p>
<div class="math notranslate nohighlight">
\[\sup_x |\hat{F}_n(x) - F(x)| = \sup_{u \in [0,1]} |\hat{H}_n(u) - u|\]</div>
<p>This equality reduces the problem to the uniform case, simplifying derivations. For general <span class="math notranslate nohighlight">\(F\)</span> (possibly with atoms), the DKW inequality is <strong>distribution-free</strong> and yields the same type of uniform bound without requiring continuity—this is one of its key strengths.</p>
<p><strong>Hoeffding’s Inequality for Binomial Tails</strong></p>
<p>Before deriving the exponential bound, we need one more tool: Hoeffding’s inequality, which provides exponentially tight bounds on how far a sample average can deviate from its expectation.</p>
<div class="note admonition">
<p class="admonition-title">Lemma: Hoeffding’s Inequality (Binomial Form)</p>
<p>Let <span class="math notranslate nohighlight">\(B \sim \text{Binomial}(n, p)\)</span>, so <span class="math notranslate nohighlight">\(B/n\)</span> is the sample proportion from <span class="math notranslate nohighlight">\(n\)</span> Bernoulli(<span class="math notranslate nohighlight">\(p\)</span>) trials. Then for any <span class="math notranslate nohighlight">\(t &gt; 0\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-hoeffding">
<span class="eqno">(166)<a class="headerlink" href="#equation-hoeffding" title="Link to this equation"></a></span>\[P\left(\frac{B}{n} - p \geq t\right) \leq e^{-2nt^2}\]</div>
<p>Similarly, <span class="math notranslate nohighlight">\(P\left(p - \frac{B}{n} \geq t\right) \leq e^{-2nt^2}\)</span>.</p>
</div>
<p><strong>Why is this useful?</strong> Chebyshev’s inequality gives <span class="math notranslate nohighlight">\(P\bigl(|B/n - p| \geq t\bigr) \leq p(1-p)/(nt^2)\)</span>, which decreases as <span class="math notranslate nohighlight">\(1/n\)</span>. Hoeffding gives <span class="math notranslate nohighlight">\(e^{-2nt^2}\)</span>, which decreases <strong>exponentially</strong> in <span class="math notranslate nohighlight">\(n\)</span>. For large <span class="math notranslate nohighlight">\(n\)</span>, exponential decay is much faster.</p>
<p><strong>Intuition</strong>: Hoeffding says that for bounded random variables (like Bernoullis), the sample mean concentrates around its expectation exponentially fast. Deviations of size <span class="math notranslate nohighlight">\(t\)</span> become exponentially unlikely as <span class="math notranslate nohighlight">\(n\)</span> grows.</p>
<p>The proof of Hoeffding’s inequality uses the moment generating function and is beyond our scope, but you can verify its tightness numerically: for <span class="math notranslate nohighlight">\(n = 100\)</span>, <span class="math notranslate nohighlight">\(p = 0.5\)</span>, and <span class="math notranslate nohighlight">\(t = 0.1\)</span>, Chebyshev gives <span class="math notranslate nohighlight">\(P \leq 0.25\)</span>, while Hoeffding gives <span class="math notranslate nohighlight">\(P \leq e^{-2} \approx 0.135\)</span>.</p>
<p><strong>Exponential Tail Bound</strong></p>
<div class="note admonition">
<p class="admonition-title">Lemma: Exponential Bound on Supremum Deviation</p>
<p>For any <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-exponential-bound">
<span class="eqno">(167)<a class="headerlink" href="#equation-exponential-bound" title="Link to this equation"></a></span>\[P(D_n &gt; \varepsilon) \leq 2n \exp(-2n\varepsilon^2)\]</div>
</div>
<p><strong>Proof</strong>:</p>
<p>Let <span class="math notranslate nohighlight">\(U_{(1)} \leq U_{(2)} \leq \cdots \leq U_{(n)}\)</span> be the order statistics of <span class="math notranslate nohighlight">\(U_1, \ldots, U_n \sim \text{Uniform}(0,1)\)</span>.</p>
<p><strong>Key observation</strong>: The supremum of <span class="math notranslate nohighlight">\(|\hat{H}_n(u) - u|\)</span> is achieved at one of the order statistics (or just before/after a jump). We can express:</p>
<div class="math notranslate nohighlight">
\[\sup_u (\hat{H}_n(u) - u) &gt; \varepsilon \quad \Leftrightarrow \quad \exists k : \frac{k}{n} - U_{(k)} &gt; \varepsilon\]</div>
<div class="math notranslate nohighlight">
\[\sup_u (u - \hat{H}_n(u)) &gt; \varepsilon \quad \Leftrightarrow \quad \exists k : U_{(k)} - \frac{k-1}{n} &gt; \varepsilon\]</div>
<p><strong>The Binomial-Order Statistic Connection</strong></p>
<p>A crucial fact links order statistics of uniform samples to binomial counts:</p>
<div class="tip admonition">
<p class="admonition-title">Lemma: Distribution of Uniform Order Statistics</p>
<p>For <span class="math notranslate nohighlight">\(U_1, \ldots, U_n \stackrel{iid}{\sim} \text{Uniform}(0,1)\)</span> and any <span class="math notranslate nohighlight">\(a \in [0,1]\)</span>:</p>
<div class="math notranslate nohighlight">
\[P(U_{(k)} \leq a) = P(B \geq k) \quad \text{where } B \sim \text{Binomial}(n, a)\]</div>
</div>
<p><strong>Why is this true?</strong> The <span class="math notranslate nohighlight">\(k\)</span>-th smallest observation is <span class="math notranslate nohighlight">\(\leq a\)</span> if and only if at least <span class="math notranslate nohighlight">\(k\)</span> of the <span class="math notranslate nohighlight">\(n\)</span> observations fall in <span class="math notranslate nohighlight">\([0, a]\)</span>. Each observation falls in <span class="math notranslate nohighlight">\([0, a]\)</span> independently with probability <span class="math notranslate nohighlight">\(a\)</span> (since they’re uniform on <span class="math notranslate nohighlight">\([0,1]\)</span>), so the count of observations <span class="math notranslate nohighlight">\(\leq a\)</span> is <span class="math notranslate nohighlight">\(\text{Binomial}(n, a)\)</span>.</p>
<p><strong>Applying this to our bound</strong>: We want to bound <span class="math notranslate nohighlight">\(P(k/n - U_{(k)} &gt; \varepsilon)\)</span>, which equals <span class="math notranslate nohighlight">\(P(U_{(k)} &lt; k/n - \varepsilon)\)</span>:</p>
<div class="math notranslate nohighlight">
\[P\left(U_{(k)} &lt; \frac{k}{n} - \varepsilon\right) = P\left(B \geq k\right) \quad \text{where } B \sim \text{Binomial}\left(n, \frac{k}{n} - \varepsilon\right)\]</div>
<p>Now <span class="math notranslate nohighlight">\(B \sim \text{Binomial}(n, p)\)</span> with <span class="math notranslate nohighlight">\(p = k/n - \varepsilon\)</span>, so <span class="math notranslate nohighlight">\(\mathbb{E}[B] = np = k - n\varepsilon\)</span>. We need <span class="math notranslate nohighlight">\(P(B \geq k) = P(B/n \geq k/n)\)</span>.</p>
<p>Since <span class="math notranslate nohighlight">\(\mathbb{E}[B/n] = p = k/n - \varepsilon\)</span>, we’re asking for <span class="math notranslate nohighlight">\(P(B/n - p \geq \varepsilon)\)</span>. By Hoeffding’s inequality:</p>
<div class="math notranslate nohighlight">
\[P\left(\frac{B}{n} - p \geq \varepsilon\right) \leq e^{-2n\varepsilon^2}\]</div>
<p>Therefore:</p>
<div class="math notranslate nohighlight">
\[P\left(\frac{k}{n} - U_{(k)} &gt; \varepsilon\right) \leq \exp(-2n\varepsilon^2)\]</div>
<p>For indices where <span class="math notranslate nohighlight">\(k/n - \varepsilon \leq 0\)</span> or <span class="math notranslate nohighlight">\(k/n - \varepsilon \geq 1\)</span>, the event is either impossible or the bound is trivially satisfied, so we restrict to valid <span class="math notranslate nohighlight">\(k\)</span>. By union bound over the remaining <span class="math notranslate nohighlight">\(k = 1, \ldots, n\)</span>:</p>
<div class="math notranslate nohighlight">
\[P\left(\sup_u (\hat{H}_n(u) - u) &gt; \varepsilon\right) \leq n \exp(-2n\varepsilon^2)\]</div>
<p>Similarly for the other direction. Combining:</p>
<div class="math notranslate nohighlight">
\[P(D_n &gt; \varepsilon) \leq 2n \exp(-2n\varepsilon^2)\]</div>
<p>∎</p>
<p><strong>Proof of Glivenko-Cantelli Theorem</strong>:</p>
<p>The strategy is to use Borel-Cantelli: show that the probabilities <span class="math notranslate nohighlight">\(P(D_n &gt; \varepsilon)\)</span> are summable, conclude that <span class="math notranslate nohighlight">\(D_n &gt; \varepsilon\)</span> happens only finitely often (almost surely), and then let <span class="math notranslate nohighlight">\(\varepsilon \to 0\)</span>.</p>
<p><em>Step 1: Verify summability for a fixed</em> <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span>.</p>
<p>By the exponential bound above:</p>
<div class="math notranslate nohighlight">
\[\sum_{n=1}^{\infty} P(D_n &gt; \varepsilon) \leq \sum_{n=1}^{\infty} 2n \exp(-2n\varepsilon^2)\]</div>
<p><strong>Why does this series converge?</strong> For any <span class="math notranslate nohighlight">\(c &gt; 0\)</span>, the series <span class="math notranslate nohighlight">\(\sum_{n=1}^{\infty} n e^{-cn}\)</span> converges. To see this, note that <span class="math notranslate nohighlight">\(n e^{-cn}\)</span> eventually decays faster than any geometric series: for large <span class="math notranslate nohighlight">\(n\)</span>, <span class="math notranslate nohighlight">\(n e^{-cn} &lt; e^{-cn/2}\)</span>, and <span class="math notranslate nohighlight">\(\sum e^{-cn/2}\)</span> is a convergent geometric series. More directly, the ratio test gives:</p>
<div class="math notranslate nohighlight">
\[\frac{(n+1)e^{-c(n+1)}}{n e^{-cn}} = \frac{n+1}{n} \cdot e^{-c} \to e^{-c} &lt; 1\]</div>
<p>So the series converges for any <span class="math notranslate nohighlight">\(c &gt; 0\)</span>. Here <span class="math notranslate nohighlight">\(c = 2\varepsilon^2 &gt; 0\)</span>, so <span class="math notranslate nohighlight">\(\sum_{n=1}^{\infty} P(D_n &gt; \varepsilon) &lt; \infty\)</span>.</p>
<p><em>Step 2: Apply Borel-Cantelli.</em></p>
<p>By the First Borel-Cantelli Lemma (proved above), since <span class="math notranslate nohighlight">\(\sum P(D_n &gt; \varepsilon) &lt; \infty\)</span>:</p>
<div class="math notranslate nohighlight">
\[P(D_n &gt; \varepsilon \text{ infinitely often}) = 0\]</div>
<p><strong>In plain language</strong>: With probability 1, <span class="math notranslate nohighlight">\(D_n &gt; \varepsilon\)</span> happens for only finitely many <span class="math notranslate nohighlight">\(n\)</span>. Equivalently, there exists a (random) <span class="math notranslate nohighlight">\(N\)</span> such that <span class="math notranslate nohighlight">\(D_n \leq \varepsilon\)</span> for all <span class="math notranslate nohighlight">\(n \geq N\)</span>.</p>
<p><em>Step 3: Extend to</em> <span class="math notranslate nohighlight">\(D_n \to 0\)</span> <em>almost surely.</em></p>
<p>We’ve shown that for each fixed <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span>, eventually <span class="math notranslate nohighlight">\(D_n \leq \varepsilon\)</span> (almost surely). But <span class="math notranslate nohighlight">\(D_n \to 0\)</span> requires this for <em>all</em> <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span> simultaneously.</p>
<p><strong>The trick: use countably many</strong> <span class="math notranslate nohighlight">\(\varepsilon\)</span> <strong>values.</strong> Consider the rational numbers <span class="math notranslate nohighlight">\(\varepsilon_k = 1/k\)</span> for <span class="math notranslate nohighlight">\(k = 1, 2, 3, \ldots\)</span> Define:</p>
<div class="math notranslate nohighlight">
\[A_k = \{D_n \leq 1/k \text{ for all sufficiently large } n\}\]</div>
<p>We’ve shown <span class="math notranslate nohighlight">\(P(A_k) = 1\)</span> for each <span class="math notranslate nohighlight">\(k\)</span>. Now define:</p>
<div class="math notranslate nohighlight">
\[A = \bigcap_{k=1}^{\infty} A_k = \{D_n \to 0\}\]</div>
<p><strong>Why is</strong> <span class="math notranslate nohighlight">\(P(A) = 1\)</span>? By the “countable intersection of probability-1 events” principle:</p>
<div class="math notranslate nohighlight">
\[P(A^c) = P\left(\bigcup_{k=1}^{\infty} A_k^c\right) \leq \sum_{k=1}^{\infty} P(A_k^c) = \sum_{k=1}^{\infty} 0 = 0\]</div>
<p>Therefore <span class="math notranslate nohighlight">\(P(A) = 1\)</span>, meaning <span class="math notranslate nohighlight">\(D_n \to 0\)</span> almost surely.</p>
<div class="tip admonition">
<p class="admonition-title">The Countable Intersection Principle</p>
<p>If <span class="math notranslate nohighlight">\(P(A_k) = 1\)</span> for <span class="math notranslate nohighlight">\(k = 1, 2, 3, \ldots\)</span> (countably many events), then <span class="math notranslate nohighlight">\(P(\bigcap_k A_k) = 1\)</span>.</p>
<p><strong>Proof</strong>: <span class="math notranslate nohighlight">\(P(\bigcap_k A_k) = 1 - P(\bigcup_k A_k^c) \geq 1 - \sum_k P(A_k^c) = 1 - 0 = 1\)</span>.</p>
<p>This is why we need only consider <strong>countably many</strong> values of <span class="math notranslate nohighlight">\(\varepsilon\)</span> (like rationals), not all positive reals. The reals are uncountable, and the argument would fail.</p>
</div>
<p><em>Step 4: Conclude.</em></p>
<p>We have shown <span class="math notranslate nohighlight">\(P(D_n \to 0) = 1\)</span>, i.e., <span class="math notranslate nohighlight">\(\sup_x |\hat{F}_n(x) - F(x)| \to 0\)</span> almost surely. ∎</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_2_fig03_glivenko_cantelli.png"><img alt="Six-panel figure showing ECDF convergence to true CDF as sample size increases from n=10 to n=2000" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_2_fig03_glivenko_cantelli.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 135 </span><span class="caption-text"><strong>Figure 4.2.3</strong>: Glivenko-Cantelli Convergence. As <span class="math notranslate nohighlight">\(n\)</span> increases from 10 to 2000, the ECDF (red) converges uniformly to the true CDF (blue). The 95% DKW confidence band (shaded) shrinks at rate <span class="math notranslate nohighlight">\(O(1/\sqrt{n})\)</span>. The supremum deviation <span class="math notranslate nohighlight">\(D_n\)</span> decreases consistently.</span><a class="headerlink" href="#id3" title="Link to this image"></a></p>
</figcaption>
</figure>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_2_fig04_convergence_rate.png"><img alt="Three-panel figure showing (a) D_n vs n on log-log scale, (b) distribution of D_n, and (c) DKW bound vs empirical" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_2_fig04_convergence_rate.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 136 </span><span class="caption-text"><strong>Figure 4.2.4</strong>: Convergence Rate and DKW Bound. (a) The supremum deviation <span class="math notranslate nohighlight">\(D_n\)</span> decreases as <span class="math notranslate nohighlight">\(O(1/\sqrt{n})\)</span> (log-log scale). (b) Distribution of <span class="math notranslate nohighlight">\(D_n\)</span> for <span class="math notranslate nohighlight">\(n=100\)</span> with DKW bounds marked. (c) The DKW bound is conservative but provides valid coverage.</span><a class="headerlink" href="#id4" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="the-dkw-inequality-finite-sample-bounds">
<h3>The DKW Inequality: Finite-Sample Bounds<a class="headerlink" href="#the-dkw-inequality-finite-sample-bounds" title="Link to this heading"></a></h3>
<p>The exponential bound can be sharpened significantly. The <strong>Dvoretzky-Kiefer-Wolfowitz (DKW) inequality</strong> provides a tighter bound that does not depend on <span class="math notranslate nohighlight">\(n\)</span> in the leading constant.</p>
<div class="note admonition">
<p class="admonition-title">Theorem: DKW Inequality</p>
<p>For any <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-dkw">
<span class="eqno">(168)<a class="headerlink" href="#equation-dkw" title="Link to this equation"></a></span>\[P\left(\sup_x |\hat{F}_n(x) - F(x)| &gt; \varepsilon\right) \leq 2\exp(-2n\varepsilon^2)\]</div>
<p>This bound is distribution-free—it holds for any <span class="math notranslate nohighlight">\(F\)</span>.</p>
</div>
<p>The original DKW result (1956) had an unspecified constant. Massart (1990) proved the sharp constant is exactly 2.</p>
<p><strong>Comparison with Basic Exponential Bound</strong>: Our earlier union-bound argument gave <span class="math notranslate nohighlight">\(P(D_n &gt; \varepsilon) \leq 2n\exp(-2n\varepsilon^2)\)</span>, which is looser by a factor of <span class="math notranslate nohighlight">\(n\)</span>. The DKW improvement comes from a more refined analysis that avoids the crude union bound over all <span class="math notranslate nohighlight">\(n\)</span> jump points. The <span class="math notranslate nohighlight">\(\sqrt{n}D_n\)</span> statistic is the Kolmogorov-Smirnov statistic; its limiting distribution under continuity of <span class="math notranslate nohighlight">\(F\)</span> is the Kolmogorov distribution, which provides even tighter quantiles than DKW for asymptotic inference.</p>
<p><strong>Constructing Confidence Bands</strong></p>
<p>The DKW inequality immediately yields simultaneous confidence bands for <span class="math notranslate nohighlight">\(F\)</span>.</p>
<div class="note admonition">
<p class="admonition-title">Corollary: DKW Confidence Band</p>
<p>For confidence level <span class="math notranslate nohighlight">\(1 - \alpha\)</span>, define:</p>
<div class="math notranslate nohighlight" id="equation-dkw-band">
<span class="eqno">(169)<a class="headerlink" href="#equation-dkw-band" title="Link to this equation"></a></span>\[\varepsilon_n(\alpha) = \sqrt{\frac{\ln(2/\alpha)}{2n}}\]</div>
<p>Then:</p>
<div class="math notranslate nohighlight">
\[P\left(\hat{F}_n(x) - \varepsilon_n(\alpha) \leq F(x) \leq \hat{F}_n(x) + \varepsilon_n(\alpha) \text{ for all } x\right) \geq 1 - \alpha\]</div>
</div>
<p><strong>Proof</strong>: Setting <span class="math notranslate nohighlight">\(2e^{-2n\varepsilon^2} = \alpha\)</span> and solving gives <span class="math notranslate nohighlight">\(\varepsilon = \sqrt{\ln(2/\alpha)/(2n)}\)</span>. ∎</p>
<p>For a 95% confidence band (<span class="math notranslate nohighlight">\(\alpha = 0.05\)</span>):</p>
<div class="math notranslate nohighlight">
\[\varepsilon_n(0.05) = \sqrt{\frac{\ln(40)}{2n}} \approx \frac{1.36}{\sqrt{n}}\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">dkw_confidence_band</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute DKW confidence band for the true CDF.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : array-like</span>
<span class="sd">        Sample observations.</span>
<span class="sd">    alpha : float</span>
<span class="sd">        Significance level (default 0.05 for 95% confidence).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    x_sorted : ndarray</span>
<span class="sd">        Sorted observations.</span>
<span class="sd">    lower : ndarray</span>
<span class="sd">        Lower confidence band.</span>
<span class="sd">    upper : ndarray</span>
<span class="sd">        Upper confidence band.</span>
<span class="sd">    ecdf_vals : ndarray</span>
<span class="sd">        ECDF values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x_sorted</span><span class="p">,</span> <span class="n">ecdf_vals</span> <span class="o">=</span> <span class="n">compute_ecdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># DKW epsilon</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n</span><span class="p">))</span>

    <span class="n">lower</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">ecdf_vals</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">upper</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">ecdf_vals</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x_sorted</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">ecdf_vals</span>

<span class="c1"># Example: DKW band for Normal data</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="n">x_sorted</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">ecdf_vals</span> <span class="o">=</span> <span class="n">dkw_confidence_band</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># Plot confidence band</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_sorted</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#2E86AB&#39;</span><span class="p">,</span>
                <span class="n">step</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;95% DKW Band&#39;</span><span class="p">)</span>

<span class="c1"># Plot ECDF</span>
<span class="n">ax</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x_sorted</span><span class="p">,</span> <span class="n">ecdf_vals</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#E74C3C&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ECDF&#39;</span><span class="p">)</span>

<span class="c1"># Plot true CDF</span>
<span class="n">x_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_grid</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x_grid</span><span class="p">),</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#2E86AB&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True N(0,1)&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;F(x)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;DKW 95% Confidence Band (n=</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Verify: true CDF should be within band</span>
<span class="n">true_cdf_at_data</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x_sorted</span><span class="p">)</span>
<span class="n">coverage</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">((</span><span class="n">true_cdf_at_data</span> <span class="o">&gt;=</span> <span class="n">lower</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">true_cdf_at_data</span> <span class="o">&lt;=</span> <span class="n">upper</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True CDF within band: </span><span class="si">{</span><span class="n">coverage</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_2_fig05_dkw_confidence_band.png"><img alt="Three-panel figure showing 90%, 95%, and 99% DKW confidence bands around ECDF" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_2_fig05_dkw_confidence_band.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 137 </span><span class="caption-text"><strong>Figure 4.2.5</strong>: DKW Confidence Bands. For <span class="math notranslate nohighlight">\(n=150\)</span> observations from a mixture distribution, we show 90%, 95%, and 99% simultaneous confidence bands for the true CDF <span class="math notranslate nohighlight">\(F\)</span>. The true CDF (dashed blue) falls within all bands. The band width <span class="math notranslate nohighlight">\(\varepsilon_n(\alpha) = \sqrt{\ln(2/\alpha)/(2n)}\)</span> increases with confidence level.</span><a class="headerlink" href="#id5" title="Link to this image"></a></p>
</figcaption>
</figure>
<div class="tip admonition">
<p class="admonition-title">Try It Yourself 🖥️ Interactive ECDF Simulation</p>
<p>Explore ECDF convergence and confidence bands interactively:</p>
<p><strong>Distribution Sampler &amp; ECDF Visualizer</strong>: <a class="reference external" href="https://treese41528.github.io/ComputationalDataScience/Simulations/Bootstrap/distribution_sampler_ecdf.html">Interactive Simulation</a></p>
<ul class="simple">
<li><p>Sample from 14 different distributions (Normal, Exponential, Beta, Cauchy, Poisson, etc.)</p></li>
<li><p>Adjust sample size and see how <span class="math notranslate nohighlight">\(D_n\)</span> decreases</p></li>
<li><p>Toggle between <strong>pointwise</strong> and <strong>DKW uniform</strong> confidence bands</p></li>
<li><p>Observe how the KS statistic <span class="math notranslate nohighlight">\(D_n = \sup_x|\hat{F}_n(x) - F(x)|\)</span> changes with each sample</p></li>
</ul>
<p><strong>CLT for ECDF (3D Visualization)</strong>: <a class="reference external" href="https://treese41528.github.io/ComputationalDataScience/Simulations/Bootstrap/ecdf_convergence.html">3D Simulation</a></p>
<ul class="simple">
<li><p>Visualize CLT convergence at 10 different quantiles simultaneously</p></li>
<li><p>See how <span class="math notranslate nohighlight">\(\sqrt{n}(\hat{F}_n(x) - F(x))/\sqrt{F(x)(1-F(x))} \to N(0,1)\)</span></p></li>
<li><p>Compare convergence rates across different sample sizes</p></li>
<li><p>Observe how variance <span class="math notranslate nohighlight">\(F(x)(1-F(x))\)</span> is maximized at the median</p></li>
</ul>
</div>
</section>
<section id="visualizing-convergence">
<h3>Visualizing Convergence<a class="headerlink" href="#visualizing-convergence" title="Link to this heading"></a></h3>
<p>The following code demonstrates Glivenko-Cantelli convergence empirically:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">visualize_glivenko_cantelli</span><span class="p">(</span><span class="n">true_dist</span><span class="p">,</span> <span class="n">sample_sizes</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Visualize ECDF convergence to true CDF as n increases.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    true_dist : scipy.stats distribution</span>
<span class="sd">        True distribution to sample from.</span>
<span class="sd">    sample_sizes : list of int</span>
<span class="sd">        Sample sizes to demonstrate.</span>
<span class="sd">    seed : int</span>
<span class="sd">        Random seed for reproducibility.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

    <span class="n">x_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">true_dist</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.001</span><span class="p">),</span> <span class="n">true_dist</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.999</span><span class="p">),</span> <span class="mi">500</span><span class="p">)</span>
    <span class="n">true_cdf</span> <span class="o">=</span> <span class="n">true_dist</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x_grid</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">sample_sizes</span><span class="p">):</span>
        <span class="c1"># Generate sample</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">true_dist</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

        <span class="c1"># Compute ECDF</span>
        <span class="n">x_sorted</span><span class="p">,</span> <span class="n">ecdf_vals</span> <span class="o">=</span> <span class="n">compute_ecdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Compute supremum deviation (approximated on grid;</span>
        <span class="c1"># for exact computation, evaluate at jump points)</span>
        <span class="n">ecdf_interp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">interp</span><span class="p">(</span><span class="n">x_grid</span><span class="p">,</span> <span class="n">x_sorted</span><span class="p">,</span> <span class="n">ecdf_vals</span><span class="p">,</span>
                                <span class="n">left</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">D_n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">ecdf_interp</span> <span class="o">-</span> <span class="n">true_cdf</span><span class="p">))</span>

        <span class="c1"># Plot</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_grid</span><span class="p">,</span> <span class="n">true_cdf</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#2E86AB&#39;</span><span class="p">,</span>
                <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True F&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x_sorted</span><span class="p">,</span> <span class="n">ecdf_vals</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#E74C3C&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ECDF&#39;</span><span class="p">)</span>

        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s1">, $D_n$ = </span><span class="si">{</span><span class="n">D_n</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;F(x)&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Glivenko-Cantelli: ECDF Converges Uniformly to True CDF&#39;</span><span class="p">,</span>
                 <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Demonstrate convergence for Exponential distribution</span>
<span class="n">visualize_glivenko_cantelli</span><span class="p">(</span>
    <span class="n">stats</span><span class="o">.</span><span class="n">expon</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">sample_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="parameters-as-statistical-functionals">
<h2>Parameters as Statistical Functionals<a class="headerlink" href="#parameters-as-statistical-functionals" title="Link to this heading"></a></h2>
<p>With the ECDF established as a legitimate and consistent estimator of <span class="math notranslate nohighlight">\(F\)</span>, we now formalize how to use it for parameter estimation. The key insight is to view parameters as <strong>functionals</strong> of the distribution.</p>
<section id="definition-and-framework">
<h3>Definition and Framework<a class="headerlink" href="#definition-and-framework" title="Link to this heading"></a></h3>
<div class="note admonition">
<p class="admonition-title">Definition: Statistical Functional</p>
<p>A <strong>statistical functional</strong> is a mapping <span class="math notranslate nohighlight">\(T: \mathcal{F} \to \mathbb{R}\)</span> that assigns a real number to each distribution <span class="math notranslate nohighlight">\(F\)</span> in some class <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>.</p>
<p>The <strong>target parameter</strong> is <span class="math notranslate nohighlight">\(\theta = T(F)\)</span>, and the <strong>plug-in estimator</strong> is <span class="math notranslate nohighlight">\(\hat{\theta} = T(\hat{F}_n)\)</span>.</p>
</div>
<p>This framework unifies many estimators under a single conceptual umbrella. The parameter is defined as a functional of the unknown distribution, and we estimate it by applying the same functional to the empirical distribution.</p>
</section>
<section id="canonical-examples">
<h3>Canonical Examples<a class="headerlink" href="#canonical-examples" title="Link to this heading"></a></h3>
<p><strong>Mean Functional</strong></p>
<p>The population mean is the simplest functional:</p>
<div class="math notranslate nohighlight" id="equation-mean-functional">
<span class="eqno">(170)<a class="headerlink" href="#equation-mean-functional" title="Link to this equation"></a></span>\[T(F) = \int_{-\infty}^{\infty} x \, dF(x) = \mathbb{E}_F[X]\]</div>
<p>Applying this to <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>:</p>
<div class="math notranslate nohighlight">
\[T(\hat{F}_n) = \int x \, d\hat{F}_n(x) = \sum_{i=1}^n x_i \cdot \frac{1}{n} = \bar{X}\]</div>
<p>The plug-in estimator of the mean is the sample mean.</p>
<p><strong>Variance Functional</strong></p>
<p>The population variance:</p>
<div class="math notranslate nohighlight" id="equation-variance-functional">
<span class="eqno">(171)<a class="headerlink" href="#equation-variance-functional" title="Link to this equation"></a></span>\[T(F) = \int (x - \mu_F)^2 \, dF(x) = \mathbb{E}_F[(X - \mu_F)^2]\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu_F = \mathbb{E}_F[X]\)</span>. Applying to <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>:</p>
<div class="math notranslate nohighlight">
\[T(\hat{F}_n) = \frac{1}{n}\sum_{i=1}^n (X_i - \bar{X})^2 = \hat{\sigma}^2_{\text{plugin}}\]</div>
<p>Note this is the <strong>biased</strong> sample variance (dividing by <span class="math notranslate nohighlight">\(n\)</span>, not <span class="math notranslate nohighlight">\(n-1\)</span>). The plug-in principle yields the MLE, which has slight bias for variance estimation.</p>
<p><strong>Quantile Functional</strong></p>
<p>The <span class="math notranslate nohighlight">\(q\)</span>-th quantile (<span class="math notranslate nohighlight">\(q \in (0,1)\)</span>):</p>
<div class="math notranslate nohighlight" id="equation-quantile-functional">
<span class="eqno">(172)<a class="headerlink" href="#equation-quantile-functional" title="Link to this equation"></a></span>\[T(F) = F^{-1}(q) = \inf\{x : F(x) \geq q\}\]</div>
<p>Applying to <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>:</p>
<div class="math notranslate nohighlight">
\[T(\hat{F}_n) = \hat{F}_n^{-1}(q) = X_{(\lceil nq \rceil)}\]</div>
<p>The plug-in estimator is the sample quantile (order statistic). Different conventions exist for sample quantiles; the theoretical definition uses the generalized inverse <span class="math notranslate nohighlight">\(X_{(\lceil nq \rceil)}\)</span> without interpolation. Software implementations vary: R offers 9 different quantile types, and NumPy’s <code class="docutils literal notranslate"><span class="pre">np.percentile</span></code> uses linear interpolation by default.</p>
<div class="tip admonition">
<p class="admonition-title">Convention Note</p>
<p>Throughout this chapter, our <strong>theoretical derivations</strong> use the order-statistic definition <span class="math notranslate nohighlight">\(X_{(\lceil nq \rceil)}\)</span>, while <strong>code examples</strong> use <code class="docutils literal notranslate"><span class="pre">np.percentile</span></code> (which interpolates by default). For most practical purposes at moderate sample sizes, these differences are negligible. When implementing exact plug-in quantiles, use <code class="docutils literal notranslate"><span class="pre">np.percentile(x,</span> <span class="pre">100*q,</span> <span class="pre">method='lower')</span></code> or compute order statistics directly.</p>
</div>
<p><strong>Correlation Functional</strong></p>
<p>For bivariate data <span class="math notranslate nohighlight">\((X, Y)\)</span> with joint distribution <span class="math notranslate nohighlight">\(F\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-correlation-functional">
<span class="eqno">(173)<a class="headerlink" href="#equation-correlation-functional" title="Link to this equation"></a></span>\[T(F) = \frac{\mathbb{E}_F[(X - \mu_X)(Y - \mu_Y)]}{\sigma_X \sigma_Y}\]</div>
<p>The plug-in estimator is the sample correlation coefficient:</p>
<div class="math notranslate nohighlight">
\[T(\hat{F}_n) = \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\sqrt{\sum_{i=1}^n (X_i - \bar{X})^2 \sum_{i=1}^n (Y_i - \bar{Y})^2}}\]</div>
<p><strong>Regression Coefficients</strong></p>
<p>For the linear model <span class="math notranslate nohighlight">\(Y = \beta_0 + \beta_1 X + \varepsilon\)</span> with random <span class="math notranslate nohighlight">\((X, Y)\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-ols-functional">
<span class="eqno">(174)<a class="headerlink" href="#equation-ols-functional" title="Link to this equation"></a></span>\[\beta_1(F) = \frac{\text{Cov}_F(X, Y)}{\text{Var}_F(X)}\]</div>
<p>The plug-in estimator is the OLS coefficient computed from sample covariances.</p>
</section>
<section id="summary-table-of-functionals">
<h3>Summary Table of Functionals<a class="headerlink" href="#summary-table-of-functionals" title="Link to this heading"></a></h3>
<table class="docutils align-default" id="id6">
<caption><span class="caption-number">Table 44 </span><span class="caption-text">Common Statistical Functionals and Their Plug-in Estimators</span><a class="headerlink" href="#id6" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 25.0%" />
<col style="width: 35.0%" />
<col style="width: 40.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Functional <span class="math notranslate nohighlight">\(T(F)\)</span></p></th>
<th class="head"><p>Plug-in <span class="math notranslate nohighlight">\(T(\hat{F}_n)\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Mean</p></td>
<td><p><span class="math notranslate nohighlight">\(\int x \, dF(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\bar{X} = \frac{1}{n}\sum X_i\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Variance</p></td>
<td><p><span class="math notranslate nohighlight">\(\int (x - \mu)^2 \, dF(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{1}{n}\sum (X_i - \bar{X})^2\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(q\)</span>-Quantile</p></td>
<td><p><span class="math notranslate nohighlight">\(\inf\{x: F(x) \geq q\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(X_{(\lceil nq \rceil)}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Correlation</p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}\)</span></p></td>
<td><p>Sample correlation <span class="math notranslate nohighlight">\(r\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Interquartile Range</p></td>
<td><p><span class="math notranslate nohighlight">\(F^{-1}(0.75) - F^{-1}(0.25)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(X_{(\lceil 0.75n \rceil)} - X_{(\lceil 0.25n \rceil)}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Probability</p></td>
<td><p><span class="math notranslate nohighlight">\(P_F(X \in A) = \int_A dF\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{1}{n}\sum \mathbf{1}\{X_i \in A\}\)</span></p></td>
</tr>
</tbody>
</table>
<figure class="align-center" id="id7">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_2_fig06_parameters_as_functionals.png"><img alt="Four-panel figure showing plug-in estimation for mean, variance, median, and summary comparison" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_2_fig06_parameters_as_functionals.png" style="width: 90%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 138 </span><span class="caption-text"><strong>Figure 4.2.6</strong>: Parameters as Functionals. (a) The mean functional <span class="math notranslate nohighlight">\(T(F) = \int x\, dF(x)\)</span> becomes the sample mean when applied to <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>. (b) The variance functional and its plug-in estimator. (c) The median as the 0.5-quantile functional. (d) Comparison of true values and plug-in estimates for a Gamma(3,2) population with <span class="math notranslate nohighlight">\(n=100\)</span>.</span><a class="headerlink" href="#id7" title="Link to this image"></a></p>
</figcaption>
</figure>
<figure class="align-center" id="id8">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_2_fig07_bootstrap_multinomial.png"><img alt="Three-panel figure showing bootstrap sampling connection to multinomial distribution" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_2_fig07_bootstrap_multinomial.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 139 </span><span class="caption-text"><strong>Figure 4.2.7</strong>: Bootstrap Sampling and the Multinomial. (a) The ECDF places mass <span class="math notranslate nohighlight">\(1/n\)</span> on each observation. (b) One bootstrap sample: selection counts <span class="math notranslate nohighlight">\((N_1, \ldots, N_n)\)</span> follow a Multinomial:math:<cite>(n; 1/n, ldots, 1/n)</cite> distribution—some points selected multiple times, others not at all. (c) Each <span class="math notranslate nohighlight">\(N_i\)</span> is marginally Binomial:math:<cite>(n, 1/n)</cite>, with <span class="math notranslate nohighlight">\(P(N_i = 0) \approx e^{-1} \approx 0.368\)</span>.</span><a class="headerlink" href="#id8" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="the-plug-in-principle">
<h2>The Plug-in Principle<a class="headerlink" href="#the-plug-in-principle" title="Link to this heading"></a></h2>
<p>We now state the plug-in principle formally as the conceptual foundation for nonparametric estimation and, ultimately, for the bootstrap.</p>
<section id="formal-statement">
<h3>Formal Statement<a class="headerlink" href="#formal-statement" title="Link to this heading"></a></h3>
<div class="important admonition">
<p class="admonition-title">The Plug-in Principle</p>
<p><strong>Parameter Level</strong>: To estimate a parameter <span class="math notranslate nohighlight">\(\theta = T(F)\)</span>, substitute the empirical distribution <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> for the unknown <span class="math notranslate nohighlight">\(F\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-plugin-parameter">
<span class="eqno">(175)<a class="headerlink" href="#equation-plugin-parameter" title="Link to this equation"></a></span>\[\hat{\theta} = T(\hat{F}_n)\]</div>
<p><strong>Distribution Level</strong>: To approximate the sampling distribution <span class="math notranslate nohighlight">\(G_F\)</span> of a statistic under <span class="math notranslate nohighlight">\(F\)</span>, substitute <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> for <span class="math notranslate nohighlight">\(F\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-plugin-distribution">
<span class="eqno">(176)<a class="headerlink" href="#equation-plugin-distribution" title="Link to this equation"></a></span>\[\hat{G}(t) = G_{\hat{F}_n}(t) = P_{\hat{F}_n}\{T(X_1^*, \ldots, X_n^*) \leq t\}\]</div>
<p>where <span class="math notranslate nohighlight">\(X_1^*, \ldots, X_n^* \stackrel{\text{iid}}{\sim} \hat{F}_n\)</span>.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Notation Legend: Distributions and Sampling</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 15.0%" />
<col style="width: 85.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Symbol</p></th>
<th class="head"><p>Meaning</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(F\)</span></p></td>
<td><p><strong>Population distribution</strong>: The unknown true distribution generating the data</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\hat{F}_n\)</span></p></td>
<td><p><strong>Empirical distribution</strong>: The discrete distribution placing mass <span class="math notranslate nohighlight">\(1/n\)</span> at each observation</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(T\)</span></p></td>
<td><p><strong>Functional/statistic</strong>: A mapping from distributions to parameters, e.g., <span class="math notranslate nohighlight">\(T(F) = \int x\, dF\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(G_F\)</span></p></td>
<td><p><strong>Sampling distribution under</strong> <span class="math notranslate nohighlight">\(F\)</span>: The distribution of <span class="math notranslate nohighlight">\(T(X_{1:n})\)</span> when <span class="math notranslate nohighlight">\(X_i \stackrel{\text{iid}}{\sim} F\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(G_{\hat{F}_n}\)</span></p></td>
<td><p><strong>Bootstrap distribution</strong>: The sampling distribution of <span class="math notranslate nohighlight">\(T(X_{1:n}^*)\)</span> under resampling from <span class="math notranslate nohighlight">\(\hat{F}_n\)</span></p></td>
</tr>
</tbody>
</table>
<p>The bootstrap idea: since <span class="math notranslate nohighlight">\(\hat{F}_n \approx F\)</span> (Glivenko-Cantelli), we hope <span class="math notranslate nohighlight">\(G_{\hat{F}_n} \approx G_F\)</span>.</p>
</div>
<p>The parameter-level plug-in gives us point estimates. The distribution-level plug-in—computing <span class="math notranslate nohighlight">\(G_{\hat{F}_n}\)</span> by Monte Carlo—is the <strong>bootstrap</strong>.</p>
</section>
<section id="consistency-of-plug-in-estimators">
<h3>Consistency of Plug-in Estimators<a class="headerlink" href="#consistency-of-plug-in-estimators" title="Link to this heading"></a></h3>
<p>Under what conditions does <span class="math notranslate nohighlight">\(T(\hat{F}_n) \to T(F)\)</span> as <span class="math notranslate nohighlight">\(n \to \infty\)</span>? The key property is <strong>continuity</strong> of the functional <span class="math notranslate nohighlight">\(T\)</span>.</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Plug-in Consistency</p>
<p>If <span class="math notranslate nohighlight">\(T\)</span> is continuous with respect to a metric <span class="math notranslate nohighlight">\(d\)</span> on distributions, and <span class="math notranslate nohighlight">\(d(\hat{F}_n, F) \to 0\)</span> almost surely, then:</p>
<div class="math notranslate nohighlight" id="equation-plugin-consistency">
<span class="eqno">(177)<a class="headerlink" href="#equation-plugin-consistency" title="Link to this equation"></a></span>\[T(\hat{F}_n) \xrightarrow{a.s.} T(F)\]</div>
</div>
<p><strong>Two Convergence Concepts</strong>:</p>
<ul class="simple">
<li><p><strong>Glivenko-Cantelli</strong>: <span class="math notranslate nohighlight">\(\|\hat{F}_n - F\|_\infty \to 0\)</span> almost surely (uniform convergence of CDFs)</p></li>
<li><p><strong>Weak convergence</strong>: <span class="math notranslate nohighlight">\(\int \varphi \, d\hat{F}_n \to \int \varphi \, dF\)</span> for all bounded continuous <span class="math notranslate nohighlight">\(\varphi\)</span></p></li>
</ul>
<p>Uniform convergence implies weak convergence, but the relevant topology for functional continuity is weak convergence.</p>
<p><strong>Bounded Continuous Functionals</strong>: For functionals of the form <span class="math notranslate nohighlight">\(T(F) = \int \varphi \, dF\)</span> where <span class="math notranslate nohighlight">\(\varphi\)</span> is bounded and continuous, weak convergence directly implies <span class="math notranslate nohighlight">\(T(\hat{F}_n) \to T(F)\)</span>. The Glivenko-Cantelli theorem provides the underlying convergence that drives this.</p>
<p><strong>Moment Functionals</strong>: For unbounded integrands like <span class="math notranslate nohighlight">\(\varphi(x) = x\)</span> (mean) or <span class="math notranslate nohighlight">\(\varphi(x) = x^2\)</span> (variance), <strong>weak convergence alone is insufficient</strong>—these functionals are not continuous under weak convergence. Additional moment control is required: uniform integrability or explicit finite-moment assumptions bridge the gap. Consistency of <span class="math notranslate nohighlight">\(\bar{X}\)</span> for the mean follows from the Strong Law of Large Numbers (requiring <span class="math notranslate nohighlight">\(\mathbb{E}|X| &lt; \infty\)</span>), not from CDF uniform convergence. Variance consistency requires <span class="math notranslate nohighlight">\(\mathbb{E}[X^2] &lt; \infty\)</span>; finite fourth moments are needed for asymptotic normality of the variance estimator or for sharper MSE bounds.</p>
<p><strong>Examples of Continuous Functionals</strong> (under appropriate conditions):</p>
<ul class="simple">
<li><p>Quantiles at continuity points of <span class="math notranslate nohighlight">\(F\)</span></p></li>
<li><p>Trimmed means</p></li>
<li><p>Distribution functions of bounded continuous functions of <span class="math notranslate nohighlight">\(X\)</span></p></li>
</ul>
<p><strong>Examples Where Extra Care Is Needed</strong>:</p>
<ul class="simple">
<li><p>Mean and variance (require moment conditions; consistency via LLN)</p></li>
<li><p>Mode (can jump with small changes in <span class="math notranslate nohighlight">\(F\)</span>)</p></li>
<li><p>Number of modes (discrete-valued)</p></li>
<li><p>Extreme quantiles when <span class="math notranslate nohighlight">\(F\)</span> has atoms</p></li>
<li><p>Functionals involving the support of <span class="math notranslate nohighlight">\(F\)</span></p></li>
</ul>
</section>
<section id="worked-example-multiple-functionals">
<h3>Worked Example: Multiple Functionals<a class="headerlink" href="#worked-example-multiple-functionals" title="Link to this heading"></a></h3>
<div class="note admonition">
<p class="admonition-title">Example 💡 Plug-in Estimation for Multiple Parameters</p>
<p><strong>Setup</strong>: We observe <span class="math notranslate nohighlight">\(n = 100\)</span> observations from an unknown distribution. We want to estimate the mean, variance, median, and interquartile range.</p>
<p><strong>Python Implementation</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">plugin_estimates</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute plug-in estimates for common functionals.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : array-like</span>
<span class="sd">        Sample observations.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        Dictionary of plug-in estimates.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">estimates</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;mean&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>                           <span class="c1"># T(F) = ∫x dF</span>
        <span class="s1">&#39;variance_plugin&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>         <span class="c1"># T(F) = ∫(x-μ)² dF</span>
        <span class="s1">&#39;variance_unbiased&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>       <span class="c1"># Corrected estimator</span>
        <span class="s1">&#39;median&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>                       <span class="c1"># T(F) = F⁻¹(0.5)</span>
        <span class="s1">&#39;q25&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span>                  <span class="c1"># T(F) = F⁻¹(0.25)</span>
        <span class="s1">&#39;q75&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">75</span><span class="p">),</span>                  <span class="c1"># T(F) = F⁻¹(0.75)</span>
        <span class="s1">&#39;iqr&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">75</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span>
        <span class="s1">&#39;skewness&#39;</span><span class="p">:</span> <span class="n">stats</span><span class="o">.</span><span class="n">skew</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>         <span class="c1"># Plugin (biased)</span>
        <span class="s1">&#39;kurtosis&#39;</span><span class="p">:</span> <span class="n">stats</span><span class="o">.</span><span class="n">kurtosis</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>     <span class="c1"># Plugin (biased)</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">estimates</span>

<span class="c1"># Generate data from Gamma(3, 2) - right-skewed distribution</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">true_shape</span><span class="p">,</span> <span class="n">true_scale</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">true_shape</span><span class="p">,</span> <span class="n">true_scale</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="c1"># True values</span>
<span class="n">true_mean</span> <span class="o">=</span> <span class="n">true_shape</span> <span class="o">*</span> <span class="n">true_scale</span>  <span class="c1"># = 6</span>
<span class="n">true_var</span> <span class="o">=</span> <span class="n">true_shape</span> <span class="o">*</span> <span class="n">true_scale</span><span class="o">**</span><span class="mi">2</span>  <span class="c1"># = 12</span>
<span class="n">true_median</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">gamma</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">true_shape</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">true_scale</span><span class="p">)</span>
<span class="n">true_iqr</span> <span class="o">=</span> <span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">gamma</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">true_shape</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">true_scale</span><span class="p">)</span> <span class="o">-</span>
            <span class="n">stats</span><span class="o">.</span><span class="n">gamma</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">true_shape</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">true_scale</span><span class="p">))</span>

<span class="c1"># Compute plug-in estimates</span>
<span class="n">estimates</span> <span class="o">=</span> <span class="n">plugin_estimates</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Plug-in Estimates vs True Values&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean:     </span><span class="si">{</span><span class="n">estimates</span><span class="p">[</span><span class="s1">&#39;mean&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">  (true: </span><span class="si">{</span><span class="n">true_mean</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Variance: </span><span class="si">{</span><span class="n">estimates</span><span class="p">[</span><span class="s1">&#39;variance_plugin&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">  (true: </span><span class="si">{</span><span class="n">true_var</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Median:   </span><span class="si">{</span><span class="n">estimates</span><span class="p">[</span><span class="s1">&#39;median&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">  (true: </span><span class="si">{</span><span class="n">true_median</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;IQR:      </span><span class="si">{</span><span class="n">estimates</span><span class="p">[</span><span class="s1">&#39;iqr&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">  (true: </span><span class="si">{</span><span class="n">true_iqr</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Output</strong>:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Plug-in Estimates vs True Values
==================================================
Mean:     5.892  (true: 6.000)
Variance: 10.847  (true: 12.000)
Median:   5.203  (true: 5.348)
IQR:      4.644  (true: 4.419)
</pre></div>
</div>
</div>
<figure class="align-center" id="id9">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_2_fig08_plugin_consistency.png"><img alt="Four-panel figure showing plug-in estimator convergence for mean, variance, median, and MSE decay" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_2_fig08_plugin_consistency.png" style="width: 90%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 140 </span><span class="caption-text"><strong>Figure 4.2.8</strong>: Plug-in Consistency. For a Beta(2,5) population, plug-in estimators converge to true values as <span class="math notranslate nohighlight">\(n\)</span> increases. (a-c) Median estimates (with 80% bands) approach true values for mean, variance, and median. (d) MSE decreases as <span class="math notranslate nohighlight">\(O(1/n)\)</span> for all continuous functionals.</span><a class="headerlink" href="#id9" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="when-the-plug-in-principle-fails">
<h2>When the Plug-in Principle Fails<a class="headerlink" href="#when-the-plug-in-principle-fails" title="Link to this heading"></a></h2>
<p>While the plug-in principle is powerful and broadly applicable, it has important limitations. Understanding these failure modes is essential for choosing appropriate inference methods.</p>
<div class="important admonition">
<p class="admonition-title">Two-Axis Taxonomy of Failure Modes</p>
<p>Plug-in failures can be classified along two dimensions:</p>
<p><strong>Axis 1: Identification</strong></p>
<ul class="simple">
<li><p><strong>Identified</strong>: <span class="math notranslate nohighlight">\(\theta = T(F)\)</span> is a well-defined functional (mean, median, quantiles)</p></li>
<li><p><strong>Not identified from observed data</strong>: Target requires additional structure or assumptions (causal effects without ignorability, mixture component labels, censored survival beyond observation period)</p></li>
</ul>
<p><strong>Axis 2: Regularity/Continuity</strong></p>
<ul class="simple">
<li><p><strong>Smooth/continuous functional</strong>: Small changes in <span class="math notranslate nohighlight">\(F\)</span> yield small changes in <span class="math notranslate nohighlight">\(T(F)\)</span> (mean, variance, trimmed mean)</p></li>
<li><p><strong>Non-smooth/discontinuous functional</strong>: Small changes in <span class="math notranslate nohighlight">\(F\)</span> can cause jumps in <span class="math notranslate nohighlight">\(T(F)\)</span> (mode, extreme quantiles, boundary of support)</p></li>
</ul>
<p>Failures from <strong>non-identification</strong> cannot be fixed by more data or better algorithms—the target is fundamentally not available from the observed data distribution. Failures from <strong>non-smoothness</strong> make plug-in estimators highly variable but still consistent; bootstrap may require modifications.</p>
</div>
<section id="non-identification">
<h3>Non-identification<a class="headerlink" href="#non-identification" title="Link to this heading"></a></h3>
<p>The plug-in principle assumes the parameter <span class="math notranslate nohighlight">\(\theta = T(F)\)</span> is <strong>identified</strong>—that the functional <span class="math notranslate nohighlight">\(T\)</span> maps each distribution to a unique value. This fails in several important settings.</p>
<p><strong>Mixture Models</strong></p>
<p>In finite mixture models, the component labels are not identified. If <span class="math notranslate nohighlight">\(F = 0.3 \cdot N(\mu_1, 1) + 0.7 \cdot N(\mu_2, 1)\)</span>, swapping <span class="math notranslate nohighlight">\(\mu_1 \leftrightarrow \mu_2\)</span> and adjusting weights gives the same <span class="math notranslate nohighlight">\(F\)</span>. There is no unique <span class="math notranslate nohighlight">\((\mu_1, \mu_2)\)</span> functional.</p>
<p><strong>Fix</strong>: Impose identifiability constraints (e.g., order <span class="math notranslate nohighlight">\(\mu_1 &lt; \mu_2\)</span>) or target mixture-invariant functionals.</p>
<p><strong>Factor Models</strong></p>
<p>Factor loadings are identified only up to rotation. The factor covariance structure is a functional of <span class="math notranslate nohighlight">\(F\)</span>, but individual loadings are not.</p>
<p><strong>Fix</strong>: Impose rotation constraints or target rotation-invariant quantities.</p>
</section>
<section id="censoring-and-truncation">
<h3>Censoring and Truncation<a class="headerlink" href="#censoring-and-truncation" title="Link to this heading"></a></h3>
<p>When observations are censored or truncated, the empirical distribution <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> estimates the <strong>observed-data distribution</strong>, not the underlying distribution of interest.</p>
<p><strong>Right Censoring</strong> (common in survival analysis): We observe <span class="math notranslate nohighlight">\(\min(T_i, C_i)\)</span> where <span class="math notranslate nohighlight">\(T_i\)</span> is the survival time and <span class="math notranslate nohighlight">\(C_i\)</span> is the censoring time. The ECDF of observed values does not estimate <span class="math notranslate nohighlight">\(F_T\)</span>.</p>
<p><strong>Fix</strong>: Use the Kaplan-Meier estimator instead of the ECDF; bootstrap the Kaplan-Meier.</p>
<p><strong>Truncation</strong>: Only observations satisfying some condition are recorded. The ECDF estimates the conditional distribution, not the marginal.</p>
<p><strong>Fix</strong>: Model the selection mechanism; use inverse probability weighting.</p>
</section>
<section id="causal-parameters">
<h3>Causal Parameters<a class="headerlink" href="#causal-parameters" title="Link to this heading"></a></h3>
<p>Causal effects like the Average Treatment Effect (ATE) are defined in terms of <strong>potential outcomes</strong>, not the observational distribution alone.</p>
<div class="math notranslate nohighlight">
\[\text{ATE} = \mathbb{E}[Y^{(1)}] - \mathbb{E}[Y^{(0)}]\]</div>
<p>where <span class="math notranslate nohighlight">\(Y^{(a)}\)</span> is the potential outcome under treatment <span class="math notranslate nohighlight">\(a\)</span>. Without additional identifying assumptions, the ATE is not identified from the observed data distribution <span class="math notranslate nohighlight">\(P(Y, A, X)\)</span>—naively computing <span class="math notranslate nohighlight">\(\mathbb{E}[Y|A=1] - \mathbb{E}[Y|A=0]\)</span> conflates treatment effects with selection bias. However, under assumptions like <strong>ignorability</strong> (no unmeasured confounding), <strong>positivity</strong> (all covariate strata have both treated and untreated units), and <strong>consistency</strong> (observed outcome equals potential outcome for received treatment), the ATE becomes a functional of the observed data distribution: <span class="math notranslate nohighlight">\(\mathbb{E}_X[\mathbb{E}[Y|A=1,X] - \mathbb{E}[Y|A=0,X]]\)</span>.</p>
<p><strong>Fix</strong>: State identifying assumptions explicitly; use appropriate estimators (matching, IPW, doubly robust); bootstrap the design-respecting estimator, not the naive difference in means.</p>
</section>
<section id="design-based-inference">
<h3>Design-Based Inference<a class="headerlink" href="#design-based-inference" title="Link to this heading"></a></h3>
<p>In survey sampling, the parameter of interest is often a finite population quantity:</p>
<div class="math notranslate nohighlight">
\[\bar{Y}_U = \frac{1}{N}\sum_{i=1}^N Y_i\]</div>
<p>where the sum is over the entire finite population of size <span class="math notranslate nohighlight">\(N\)</span>. Observations come from a probability sample with unequal inclusion probabilities <span class="math notranslate nohighlight">\(\pi_i\)</span>.</p>
<p>The ECDF treats all observations equally, ignoring the sampling design.</p>
<p><strong>Fix</strong>: Use design-weighted estimators; bootstrap with survey weights (replicate weight methods).</p>
</section>
<section id="discontinuous-and-set-valued-functionals">
<h3>Discontinuous and Set-Valued Functionals<a class="headerlink" href="#discontinuous-and-set-valued-functionals" title="Link to this heading"></a></h3>
<p>The plug-in principle works well when <span class="math notranslate nohighlight">\(T\)</span> is continuous. It can fail for:</p>
<p><strong>Discontinuous Functionals</strong>:</p>
<ul class="simple">
<li><p>Mode: small changes in <span class="math notranslate nohighlight">\(F\)</span> can cause the mode to jump</p></li>
<li><p>Support boundaries: <span class="math notranslate nohighlight">\(\inf\{x: F(x) &gt; 0\}\)</span> depends on the far tail</p></li>
</ul>
<p><strong>Set-Valued Functionals</strong>:</p>
<ul class="simple">
<li><p>The set of minimizers of a non-convex criterion</p></li>
<li><p>Confidence sets for non-identified parameters</p></li>
</ul>
<p><strong>Fix</strong>: Add regularization; target a well-defined selection from the set (e.g., smallest minimizer); recognize that bootstrap may be inconsistent.</p>
</section>
<section id="summary-of-failure-modes">
<h3>Summary of Failure Modes<a class="headerlink" href="#summary-of-failure-modes" title="Link to this heading"></a></h3>
<table class="docutils align-default" id="id10">
<caption><span class="caption-number">Table 45 </span><span class="caption-text">When Plug-in Fails: Diagnostics and Remedies</span><a class="headerlink" href="#id10" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 25.0%" />
<col style="width: 35.0%" />
<col style="width: 40.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Failure Mode</p></th>
<th class="head"><p>Diagnostic</p></th>
<th class="head"><p>Remedy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Non-identification</p></td>
<td><p>Multiple <span class="math notranslate nohighlight">\(\theta\)</span> yield same <span class="math notranslate nohighlight">\(F\)</span></p></td>
<td><p>Impose constraints; target invariant functionals</p></td>
</tr>
<tr class="row-odd"><td><p>Censoring/Truncation</p></td>
<td><p>Incomplete observations</p></td>
<td><p>Kaplan-Meier; model selection mechanism</p></td>
</tr>
<tr class="row-even"><td><p>Causal targets</p></td>
<td><p>Need <span class="math notranslate nohighlight">\(P(Y^a)\)</span>, not <span class="math notranslate nohighlight">\(P(Y|A)\)</span></p></td>
<td><p>State assumptions; design-based estimators</p></td>
</tr>
<tr class="row-odd"><td><p>Survey sampling</p></td>
<td><p>Unequal inclusion probabilities</p></td>
<td><p>Weighted estimators; replicate weights</p></td>
</tr>
<tr class="row-even"><td><p>Discontinuous <span class="math notranslate nohighlight">\(T\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(T(\hat{F}_n)\)</span> unstable across samples</p></td>
<td><p>Regularize; smooth; use different functional</p></td>
</tr>
</tbody>
</table>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ⚠️ Bootstrap Cannot Fix Non-identification</p>
<p>If <span class="math notranslate nohighlight">\(\theta = T(F)\)</span> is not identified by the distribution <span class="math notranslate nohighlight">\(F\)</span>, then bootstrapping cannot help. The bootstrap approximates the sampling distribution of <span class="math notranslate nohighlight">\(T(\hat{F}_n)\)</span>, but if <span class="math notranslate nohighlight">\(T\)</span> is not well-defined, neither is its sampling distribution.</p>
<p><strong>Key principle</strong>: Bootstrap provides uncertainty quantification for identified quantities; it cannot identify parameters that the distribution does not identify.</p>
</div>
<figure class="align-center" id="id11">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_2_fig09_when_plugin_fails.png"><img alt="Four-panel figure showing plug-in failure modes: extreme quantiles, censoring, discontinuous functionals" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_2_fig09_when_plugin_fails.png" style="width: 90%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 141 </span><span class="caption-text"><strong>Figure 4.2.9</strong>: When Plug-in Fails. (a) Extreme quantiles: ECDF cannot extrapolate beyond observed data range. (b) Censoring: ECDF of observed times differs from true survival distribution. (c) Discontinuous functionals like the mode: small data changes cause large estimate jumps. (d) Summary of failure modes and remedies.</span><a class="headerlink" href="#id11" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="the-bootstrap-idea-in-one-sentence">
<h2>The Bootstrap Idea in One Sentence<a class="headerlink" href="#the-bootstrap-idea-in-one-sentence" title="Link to this heading"></a></h2>
<p>We have established that:</p>
<ol class="arabic simple">
<li><p>The ECDF <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> converges uniformly to <span class="math notranslate nohighlight">\(F\)</span> (Glivenko-Cantelli)</p></li>
<li><p>For continuous functionals, <span class="math notranslate nohighlight">\(T(\hat{F}_n) \to T(F)\)</span> (plug-in consistency)</p></li>
</ol>
<p>The bootstrap extends this logic from <strong>point estimation</strong> to <strong>distributional approximation</strong>:</p>
<div class="important admonition">
<p class="admonition-title">The Bootstrap Principle</p>
<p>To approximate the sampling distribution <span class="math notranslate nohighlight">\(G(t) = P_F\{T(X_{1:n}) \leq t\}\)</span>, use:</p>
<div class="math notranslate nohighlight" id="equation-bootstrap-principle">
<span class="eqno">(178)<a class="headerlink" href="#equation-bootstrap-principle" title="Link to this equation"></a></span>\[\hat{G}(t) = P_{\hat{F}_n}\{T(X^*_{1:n}) \leq t\}\]</div>
<p>where <span class="math notranslate nohighlight">\(X^*_1, \ldots, X^*_n \stackrel{\text{iid}}{\sim} \hat{F}_n\)</span>.</p>
</div>
<p>In words: <strong>compute the sampling distribution under</strong> <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> <strong>instead of</strong> <span class="math notranslate nohighlight">\(F\)</span>.</p>
<p>Since <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> is discrete, we compute <span class="math notranslate nohighlight">\(\hat{G}\)</span> by Monte Carlo: sample <span class="math notranslate nohighlight">\(X^*_{1:n}\)</span> from <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> (i.e., resample with replacement from the data), compute <span class="math notranslate nohighlight">\(T(X^*_{1:n})\)</span>, and repeat <span class="math notranslate nohighlight">\(B\)</span> times. The empirical distribution of the <span class="math notranslate nohighlight">\(B\)</span> values approximates <span class="math notranslate nohighlight">\(\hat{G}\)</span>, which in turn approximates <span class="math notranslate nohighlight">\(G\)</span>.</p>
<p>This is the subject of <a class="reference internal" href="ch4_3-nonparametric-bootstrap.html#ch4-3-nonparametric-bootstrap"><span class="std std-ref">Section 4.3</span></a>.</p>
</section>
<section id="computational-implementation">
<h2>Computational Implementation<a class="headerlink" href="#computational-implementation" title="Link to this heading"></a></h2>
<p>We conclude with a comprehensive example demonstrating the concepts of this section.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Section 4.2: Comprehensive Example</span>
<span class="sd">==================================</span>
<span class="sd">Demonstrates ECDF, convergence, DKW bands, and plug-in estimation.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Set up figure</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># True distribution: Mixture of two normals</span>
<span class="k">def</span><span class="w"> </span><span class="nf">true_cdf</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> \
           <span class="mf">0.7</span> <span class="o">*</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">sample_mixture</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">rng</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sample from mixture of two normals.&quot;&quot;&quot;</span>
    <span class="n">components</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">])</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
        <span class="n">components</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span>
        <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">samples</span>

<span class="c1"># Panel 1: ECDF vs True CDF</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">sample_mixture</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
<span class="n">x_sorted</span><span class="p">,</span> <span class="n">ecdf_vals</span> <span class="o">=</span> <span class="n">compute_ecdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_grid</span><span class="p">,</span> <span class="n">true_cdf</span><span class="p">(</span><span class="n">x_grid</span><span class="p">),</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True F&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x_sorted</span><span class="p">,</span> <span class="n">ecdf_vals</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;ECDF (n=</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;F(x)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;ECDF vs True CDF&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Panel 2: Glivenko-Cantelli convergence</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">sample_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">250</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]</span>
<span class="n">D_n_values</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">n_test</span> <span class="ow">in</span> <span class="n">sample_sizes</span><span class="p">:</span>
    <span class="n">x_test</span> <span class="o">=</span> <span class="n">sample_mixture</span><span class="p">(</span><span class="n">n_test</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
    <span class="n">x_s</span><span class="p">,</span> <span class="n">ecdf_v</span> <span class="o">=</span> <span class="n">compute_ecdf</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
    <span class="n">ecdf_interp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">interp</span><span class="p">(</span><span class="n">x_grid</span><span class="p">,</span> <span class="n">x_s</span><span class="p">,</span> <span class="n">ecdf_v</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">D_n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">ecdf_interp</span> <span class="o">-</span> <span class="n">true_cdf</span><span class="p">(</span><span class="n">x_grid</span><span class="p">)))</span>
    <span class="n">D_n_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">D_n</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">sample_sizes</span><span class="p">,</span> <span class="n">D_n_values</span><span class="p">,</span> <span class="s1">&#39;bo-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Observed $D_n$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">sample_sizes</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sample_sizes</span><span class="p">),</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$1/\sqrt</span><span class="si">{n}</span><span class="s1">$ reference&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Sample size n&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Supremum deviation $D_n$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Glivenko-Cantelli Convergence&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Panel 3: DKW Confidence Band</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">sample_mixture</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
<span class="n">x_sorted</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">ecdf_vals</span> <span class="o">=</span> <span class="n">dkw_confidence_band</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_sorted</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span>
                <span class="n">step</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;95% DKW Band&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x_sorted</span><span class="p">,</span> <span class="n">ecdf_vals</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ECDF&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_grid</span><span class="p">,</span> <span class="n">true_cdf</span><span class="p">(</span><span class="n">x_grid</span><span class="p">),</span> <span class="s1">&#39;b--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True F&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;F(x)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;DKW 95% Confidence Band (n=</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Panel 4: Plug-in for mean</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">n_sims</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">n_sample</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">plugin_means</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sims</span><span class="p">):</span>
    <span class="n">x_sim</span> <span class="o">=</span> <span class="n">sample_mixture</span><span class="p">(</span><span class="n">n_sample</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
    <span class="n">plugin_means</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x_sim</span><span class="p">))</span>

<span class="n">true_mean</span> <span class="o">=</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.7</span> <span class="o">*</span> <span class="mi">2</span>  <span class="c1"># = 0.8</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">plugin_means</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">,</span>
        <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Sampling distribution&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">true_mean</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;True μ = </span><span class="si">{</span><span class="n">true_mean</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">plugin_means</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Mean of estimates&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Sample mean&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Density&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Plug-in for Mean (n=50, 1000 simulations)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Panel 5: Plug-in for median</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">plugin_medians</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sims</span><span class="p">):</span>
    <span class="n">x_sim</span> <span class="o">=</span> <span class="n">sample_mixture</span><span class="p">(</span><span class="n">n_sample</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
    <span class="n">plugin_medians</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">x_sim</span><span class="p">))</span>

<span class="c1"># True median (numerical)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.optimize</span><span class="w"> </span><span class="kn">import</span> <span class="n">brentq</span>
<span class="n">true_median</span> <span class="o">=</span> <span class="n">brentq</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">true_cdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">plugin_medians</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">,</span>
        <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Sampling distribution&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">true_median</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span>
           <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;True median = </span><span class="si">{</span><span class="n">true_median</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">plugin_medians</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
           <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Mean of estimates = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">plugin_medians</span><span class="p">)</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Sample median&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Density&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Plug-in for Median (n=50, 1000 simulations)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Panel 6: Plug-in bias as function of n</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">sample_sizes_bias</span> <span class="o">=</span> <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">500</span><span class="p">]</span>
<span class="n">mean_bias</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">var_bias</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">true_var</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.3</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">4</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.7</span> <span class="o">*</span> <span class="p">(</span><span class="mf">2.25</span> <span class="o">+</span> <span class="mi">4</span><span class="p">))</span> <span class="o">-</span> <span class="n">true_mean</span><span class="o">**</span><span class="mi">2</span>

<span class="k">for</span> <span class="n">n_test</span> <span class="ow">in</span> <span class="n">sample_sizes_bias</span><span class="p">:</span>
    <span class="n">biases_mean</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">biases_var</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
        <span class="n">x_sim</span> <span class="o">=</span> <span class="n">sample_mixture</span><span class="p">(</span><span class="n">n_test</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
        <span class="n">biases_mean</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x_sim</span><span class="p">)</span> <span class="o">-</span> <span class="n">true_mean</span><span class="p">)</span>
        <span class="n">biases_var</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x_sim</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">true_var</span><span class="p">)</span>
    <span class="n">mean_bias</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">biases_mean</span><span class="p">))</span>
    <span class="n">var_bias</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">biases_var</span><span class="p">))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sample_sizes_bias</span><span class="p">,</span> <span class="n">mean_bias</span><span class="p">,</span> <span class="s1">&#39;bo-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Mean estimator bias&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sample_sizes_bias</span><span class="p">,</span> <span class="n">var_bias</span><span class="p">,</span> <span class="s1">&#39;rs-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Variance (plug-in) bias&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Sample size n&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Bias&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Plug-in Estimator Bias vs Sample Size&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Section 4.2: ECDF, Convergence, and Plug-in Principle&#39;</span><span class="p">,</span>
             <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="bringing-it-all-together">
<h2>Bringing It All Together<a class="headerlink" href="#bringing-it-all-together" title="Link to this heading"></a></h2>
<p>This section established the mathematical foundations for bootstrap inference. The empirical distribution <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> provides a nonparametric estimate of the unknown population distribution <span class="math notranslate nohighlight">\(F\)</span>, with strong convergence guarantees:</p>
<ul class="simple">
<li><p><strong>Pointwise</strong>: <span class="math notranslate nohighlight">\(\hat{F}_n(x) \to F(x)\)</span> almost surely for each <span class="math notranslate nohighlight">\(x\)</span> (SLLN)</p></li>
<li><p><strong>Uniform</strong>: <span class="math notranslate nohighlight">\(\sup_x |\hat{F}_n(x) - F(x)| \to 0\)</span> almost surely (Glivenko-Cantelli)</p></li>
<li><p><strong>Finite-sample</strong>: <span class="math notranslate nohighlight">\(P(\sup_x |\hat{F}_n(x) - F(x)| &gt; \varepsilon) \leq 2e^{-2n\varepsilon^2}\)</span> (DKW)</p></li>
</ul>
<p>The plug-in principle leverages these convergence results: viewing parameters as functionals <span class="math notranslate nohighlight">\(\theta = T(F)\)</span>, we estimate them by <span class="math notranslate nohighlight">\(\hat{\theta} = T(\hat{F}_n)\)</span>. For continuous functionals, this yields consistent estimators.</p>
<p>The bootstrap extends plug-in from parameters to distributions: to approximate the sampling distribution <span class="math notranslate nohighlight">\(G = G_F\)</span>, we compute <span class="math notranslate nohighlight">\(\hat{G} = G_{\hat{F}_n}\)</span> by Monte Carlo simulation. This is computationally feasible because <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> is discrete—sampling from it means resampling with replacement from the data.</p>
<p>The next section develops the nonparametric bootstrap algorithm in full detail, including standard error estimation, confidence interval construction, and diagnostics for assessing bootstrap performance.</p>
<figure class="align-center" id="id12">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_2_fig10_bootstrap_principle.png"><img alt="Summary diagram showing the bootstrap as distribution-level plug-in: F generates G_F, F_hat generates G_F_hat" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_2_fig10_bootstrap_principle.png" style="width: 90%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 142 </span><span class="caption-text"><strong>Figure 4.2.10</strong>: The Bootstrap Principle. The bootstrap applies plug-in at the distribution level: just as <span class="math notranslate nohighlight">\(\hat{F}_n \approx F\)</span> (Glivenko-Cantelli), we hope <span class="math notranslate nohighlight">\(G_{\hat{F}_n} \approx G_F\)</span> (bootstrap consistency). The key insight: compute the sampling distribution under <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> by resampling with replacement from the observed data.</span><a class="headerlink" href="#id12" title="Link to this image"></a></p>
</figcaption>
</figure>
<div class="important admonition">
<p class="admonition-title">Key Takeaways 📝</p>
<ol class="arabic simple">
<li><p><strong>ECDF Definition</strong>: <span class="math notranslate nohighlight">\(\hat{F}_n(x) = n^{-1}\sum_{i=1}^n \mathbf{1}\{X_i \leq x\}\)</span> is a step function placing mass <span class="math notranslate nohighlight">\(1/n\)</span> at each observation</p></li>
<li><p><strong>Convergence Guarantees</strong>: The Glivenko-Cantelli theorem ensures <span class="math notranslate nohighlight">\(\|\hat{F}_n - F\|_\infty \to 0\)</span> almost surely; the DKW inequality gives finite-sample probability bounds</p></li>
<li><p><strong>Plug-in Principle</strong>: Estimate <span class="math notranslate nohighlight">\(\theta = T(F)\)</span> by <span class="math notranslate nohighlight">\(\hat{\theta} = T(\hat{F}_n)\)</span>; this recovers familiar estimators (sample mean, sample quantiles, sample correlation) as special cases</p></li>
<li><p><strong>When It Fails</strong>: Plug-in requires identification and continuity; failures occur with censoring, causal targets, survey designs, and discontinuous functionals</p></li>
<li><p><strong>Bridge to Bootstrap</strong>: The bootstrap applies plug-in to the <em>sampling distribution</em> itself: approximate <span class="math notranslate nohighlight">\(G_F\)</span> by <span class="math notranslate nohighlight">\(G_{\hat{F}_n}\)</span> via Monte Carlo</p></li>
<li><p><strong>Learning Outcome Alignment</strong>: This section supports LO 1 (simulation techniques) and LO 3 (resampling methods) by establishing the theoretical foundation for bootstrap inference</p></li>
</ol>
</div>
</section>
<section id="section-4-2-exercises-ecdf-and-plug-in-mastery">
<h2>Section 4.2 Exercises: ECDF and Plug-in Mastery<a class="headerlink" href="#section-4-2-exercises-ecdf-and-plug-in-mastery" title="Link to this heading"></a></h2>
<p>These exercises progressively build your understanding of the empirical distribution function and plug-in estimation, from theoretical foundations through computational implementation to recognizing failure modes. Each exercise connects rigorous mathematics to practical data analysis skills.</p>
<div class="tip admonition">
<p class="admonition-title">A Note on These Exercises</p>
<p>These exercises are designed to deepen your understanding through hands-on exploration:</p>
<ul class="simple">
<li><p><strong>Exercise 4.2.1</strong> reinforces the ECDF as a probability measure through explicit computation</p></li>
<li><p><strong>Exercise 4.2.2</strong> explores Glivenko-Cantelli convergence empirically with visualization</p></li>
<li><p><strong>Exercise 4.2.3</strong> develops practical skills with DKW confidence bands</p></li>
<li><p><strong>Exercise 4.2.4</strong> connects plug-in estimation to familiar statistics via the functional perspective</p></li>
<li><p><strong>Exercise 4.2.5</strong> investigates when plug-in fails through simulation of failure modes</p></li>
<li><p><strong>Exercise 4.2.6</strong> synthesizes all concepts into a complete nonparametric analysis workflow</p></li>
</ul>
<p><strong>Interactive Companion</strong>: Before or after working these exercises, explore the <a class="reference external" href="https://treese41528.github.io/ComputationalDataScience/Simulations/Bootstrap/distribution_sampler_ecdf.html">Distribution Sampler &amp; ECDF Visualizer</a> to experiment with different distributions and sample sizes interactively.</p>
<p>Complete solutions with derivations, code, output, and interpretation are provided. Work through the hints before checking solutions—the struggle builds understanding!</p>
</div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 4.2.1: ECDF as a Probability Measure</p>
<p>This exercise reinforces the fundamental concept that the ECDF defines a legitimate probability distribution—a discrete measure placing mass <span class="math notranslate nohighlight">\(1/n\)</span> at each observation.</p>
<div class="note admonition">
<p class="admonition-title">Background: Why This Matters</p>
<p>Understanding the ECDF as a probability measure (not just a step function) is essential for the plug-in principle. When we write <span class="math notranslate nohighlight">\(T(\hat{F}_n)\)</span> for a functional like the mean, we are computing an expectation under a genuine probability distribution. This perspective connects abstract functional analysis to concrete computation.</p>
</div>
<ol class="loweralpha simple">
<li><p>For a sample <span class="math notranslate nohighlight">\(X_1 = 2, X_2 = 5, X_3 = 5, X_4 = 8, X_5 = 11\)</span>:</p>
<ul class="simple">
<li><p>Write out <span class="math notranslate nohighlight">\(\hat{F}_5(x)\)</span> as a piecewise function</p></li>
<li><p>Verify that <span class="math notranslate nohighlight">\(\hat{F}_5\)</span> satisfies the three defining properties of a CDF</p></li>
<li><p>Compute <span class="math notranslate nohighlight">\(P_{\hat{F}_5}(3 &lt; X \leq 8)\)</span> directly from the ECDF</p></li>
</ul>
</li>
<li><p>For the same sample, verify that <span class="math notranslate nohighlight">\(\hat{F}_5\)</span> places mass <span class="math notranslate nohighlight">\(1/5\)</span> at each unique observation by computing <span class="math notranslate nohighlight">\(\hat{F}_5(x) - \hat{F}_5(x^-)\)</span> at each jump point. Handle the tied observation at <span class="math notranslate nohighlight">\(x = 5\)</span> correctly.</p></li>
<li><p>Compute the mean under <span class="math notranslate nohighlight">\(\hat{F}_5\)</span> using the formula <span class="math notranslate nohighlight">\(\mathbb{E}_{\hat{F}_5}[X] = \int x \, d\hat{F}_5(x) = \sum_{i=1}^5 X_i \cdot (1/5)\)</span>. Verify this equals the sample mean <span class="math notranslate nohighlight">\(\bar{X}\)</span>.</p></li>
<li><p>Compute the variance under <span class="math notranslate nohighlight">\(\hat{F}_5\)</span> using <span class="math notranslate nohighlight">\(\text{Var}_{\hat{F}_5}(X) = \mathbb{E}_{\hat{F}_5}[X^2] - (\mathbb{E}_{\hat{F}_5}[X])^2\)</span>. Compare to the sample variance formula <span class="math notranslate nohighlight">\(s^2 = \frac{1}{n-1}\sum(X_i - \bar{X})^2\)</span>. Why do they differ?</p></li>
</ol>
<div class="tip admonition">
<p class="admonition-title">Hint</p>
<p>For part (b), remember that <span class="math notranslate nohighlight">\(\hat{F}_n(x^-)\)</span> means the left-hand limit. At <span class="math notranslate nohighlight">\(x = 5\)</span> with two tied observations, the jump is <span class="math notranslate nohighlight">\(2/5\)</span>, not <span class="math notranslate nohighlight">\(1/5\)</span>. For part (d), the ECDF variance divides by <span class="math notranslate nohighlight">\(n\)</span>, while the sample variance divides by <span class="math notranslate nohighlight">\(n-1\)</span>—the latter is an unbiased estimator of <span class="math notranslate nohighlight">\(\sigma^2\)</span>, but the plug-in estimator uses the population formula with <span class="math notranslate nohighlight">\(F\)</span> replaced by <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>.</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): ECDF as a piecewise function</strong></p>
<div class="tip admonition">
<p class="admonition-title">Step 1: Identify jump points</p>
<p class="sd-card-text">Sorted unique values: <span class="math notranslate nohighlight">\(2, 5, 8, 11\)</span>. Note that <span class="math notranslate nohighlight">\(5\)</span> appears twice.</p>
<p class="sd-card-text">The ECDF is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\hat{F}_5(x) = \begin{cases}
0 &amp; x &lt; 2 \\
1/5 &amp; 2 \leq x &lt; 5 \\
3/5 &amp; 5 \leq x &lt; 8 \\
4/5 &amp; 8 \leq x &lt; 11 \\
1 &amp; x \geq 11
\end{cases}\end{split}\]</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Step 2: Verify CDF properties</p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Right-continuity</strong>: <span class="math notranslate nohighlight">\(\hat{F}_5(x)\)</span> is right-continuous at each jump (includes the point, excludes left)</p></li>
<li><p class="sd-card-text"><strong>Monotonicity</strong>: <span class="math notranslate nohighlight">\(\hat{F}_5\)</span> is non-decreasing (jumps up, never down)</p></li>
<li><p class="sd-card-text"><strong>Limits</strong>: <span class="math notranslate nohighlight">\(\lim_{x \to -\infty} \hat{F}_5(x) = 0\)</span> and <span class="math notranslate nohighlight">\(\lim_{x \to \infty} \hat{F}_5(x) = 1\)</span></p></li>
</ol>
<p class="sd-card-text">All three CDF axioms are satisfied. ✓</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Step 3: Compute probability</p>
<div class="math notranslate nohighlight">
\[P_{\hat{F}_5}(3 &lt; X \leq 8) = \hat{F}_5(8) - \hat{F}_5(3) = 4/5 - 1/5 = 3/5\]</div>
<p class="sd-card-text">This corresponds to 3 of 5 observations (5, 5, 8) falling in the interval <span class="math notranslate nohighlight">\((3, 8]\)</span>.</p>
</div>
<p class="sd-card-text"><strong>Part (b): Jump sizes and mass placement</strong></p>
<p class="sd-card-text">At each jump point, compute <span class="math notranslate nohighlight">\(\hat{F}_5(x) - \hat{F}_5(x^-)\)</span>:</p>
<ul class="simple">
<li><p class="sd-card-text">At <span class="math notranslate nohighlight">\(x = 2\)</span>: <span class="math notranslate nohighlight">\(\hat{F}_5(2) - \hat{F}_5(2^-) = 1/5 - 0 = 1/5\)</span></p></li>
<li><p class="sd-card-text">At <span class="math notranslate nohighlight">\(x = 5\)</span>: <span class="math notranslate nohighlight">\(\hat{F}_5(5) - \hat{F}_5(5^-) = 3/5 - 1/5 = 2/5\)</span> (two tied values!)</p></li>
<li><p class="sd-card-text">At <span class="math notranslate nohighlight">\(x = 8\)</span>: <span class="math notranslate nohighlight">\(\hat{F}_5(8) - \hat{F}_5(8^-) = 4/5 - 3/5 = 1/5\)</span></p></li>
<li><p class="sd-card-text">At <span class="math notranslate nohighlight">\(x = 11\)</span>: <span class="math notranslate nohighlight">\(\hat{F}_5(11) - \hat{F}_5(11^-) = 1 - 4/5 = 1/5\)</span></p></li>
</ul>
<p class="sd-card-text">Total mass: <span class="math notranslate nohighlight">\(1/5 + 2/5 + 1/5 + 1/5 = 5/5 = 1\)</span> ✓</p>
<p class="sd-card-text"><strong>Key insight</strong>: The ECDF places mass <span class="math notranslate nohighlight">\(k/n\)</span> at any value that appears <span class="math notranslate nohighlight">\(k\)</span> times in the sample.</p>
<p class="sd-card-text"><strong>Part (c): Mean under the ECDF</strong></p>
<div class="math notranslate nohighlight">
\[\mathbb{E}_{\hat{F}_5}[X] = \sum_{i=1}^5 X_i \cdot \frac{1}{5} = \frac{2 + 5 + 5 + 8 + 11}{5} = \frac{31}{5} = 6.2\]</div>
<p class="sd-card-text">This equals the sample mean <span class="math notranslate nohighlight">\(\bar{X} = 6.2\)</span>. ✓</p>
<p class="sd-card-text"><strong>Part (d): Variance under the ECDF</strong></p>
<div class="tip admonition">
<p class="admonition-title">Step 1: Compute second moment</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}_{\hat{F}_5}[X^2] = \frac{2^2 + 5^2 + 5^2 + 8^2 + 11^2}{5} = \frac{4 + 25 + 25 + 64 + 121}{5} = \frac{239}{5} = 47.8\]</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Step 2: Compute plug-in variance</p>
<div class="math notranslate nohighlight">
\[\text{Var}_{\hat{F}_5}(X) = 47.8 - (6.2)^2 = 47.8 - 38.44 = 9.36\]</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Step 3: Compare to sample variance</p>
<p class="sd-card-text">Sample variance with <span class="math notranslate nohighlight">\(n-1\)</span> denominator:</p>
<div class="math notranslate nohighlight">
\[s^2 = \frac{1}{4}\sum_{i=1}^5 (X_i - 6.2)^2 = \frac{(2-6.2)^2 + 2(5-6.2)^2 + (8-6.2)^2 + (11-6.2)^2}{4}\]</div>
<div class="math notranslate nohighlight">
\[= \frac{17.64 + 2.88 + 3.24 + 23.04}{4} = \frac{46.8}{4} = 11.7\]</div>
</div>
<p class="sd-card-text"><strong>Why they differ</strong>: The plug-in variance <span class="math notranslate nohighlight">\(\text{Var}_{\hat{F}_5}(X) = 9.36\)</span> divides by <span class="math notranslate nohighlight">\(n = 5\)</span>. The sample variance <span class="math notranslate nohighlight">\(s^2 = 11.7\)</span> divides by <span class="math notranslate nohighlight">\(n-1 = 4\)</span> to achieve unbiasedness.</p>
<p class="sd-card-text">The relationship is:</p>
<div class="math notranslate nohighlight">
\[s^2 = \frac{n}{n-1} \cdot \text{Var}_{\hat{F}_n}(X)\]</div>
<p class="sd-card-text">Here: <span class="math notranslate nohighlight">\(11.7 = (5/4) \cdot 9.36 = 11.7\)</span> ✓</p>
<p class="sd-card-text"><strong>Key insight</strong>: The plug-in estimator uses the “population” formula applied to <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>. It is biased for <span class="math notranslate nohighlight">\(\sigma^2\)</span> but consistent. The factor <span class="math notranslate nohighlight">\(n/(n-1)\)</span> is Bessel’s correction for unbiasedness.</p>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 4.2.2: Glivenko-Cantelli Convergence in Action</p>
<p>This exercise builds intuition for uniform convergence through visualization and empirical investigation of convergence rates.</p>
<div class="note admonition">
<p class="admonition-title">Background: Seeing Convergence</p>
<p>The Glivenko-Cantelli theorem guarantees <span class="math notranslate nohighlight">\(\|\hat{F}_n - F\|_\infty \to 0\)</span> almost surely, but the theorem doesn’t tell us <em>how fast</em>. Through simulation, we can observe that <span class="math notranslate nohighlight">\(D_n = O(1/\sqrt{n})\)</span> and see the dramatic visual convergence as sample size increases. This empirical understanding complements the theoretical results.</p>
</div>
<ol class="loweralpha simple">
<li><p>Write a function that computes the exact supremum deviation <span class="math notranslate nohighlight">\(D_n = \sup_x |\hat{F}_n(x) - F(x)|\)</span> for a given sample and true CDF. Note: the supremum is achieved either just before or at a jump point of <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>.</p></li>
<li><p>Generate samples of sizes <span class="math notranslate nohighlight">\(n = 25, 100, 400, 1600\)</span> from <span class="math notranslate nohighlight">\(N(0, 1)\)</span>. For each, plot <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> versus the true CDF <span class="math notranslate nohighlight">\(\Phi\)</span>. Visually verify uniform convergence.</p></li>
<li><p>For <span class="math notranslate nohighlight">\(n = 50, 100, 200, 500, 1000, 2000\)</span>, simulate 1000 replications and compute <span class="math notranslate nohighlight">\(D_n\)</span> for each. Plot the mean and 95th percentile of <span class="math notranslate nohighlight">\(D_n\)</span> versus <span class="math notranslate nohighlight">\(n\)</span> on a log-log scale. Estimate the convergence rate by fitting a line.</p></li>
<li><p>Compare the empirical 95th percentile of <span class="math notranslate nohighlight">\(D_n\)</span> to the DKW bound <span class="math notranslate nohighlight">\(\varepsilon_n(0.05) = \sqrt{\ln(2/0.05)/(2n)}\)</span>. Is DKW conservative? By how much?</p></li>
</ol>
<div class="tip admonition">
<p class="admonition-title">Hint</p>
<p>For part (a), the supremum of <span class="math notranslate nohighlight">\(|\hat{F}_n(x) - F(x)|\)</span> can be computed exactly. At the sorted sample values <span class="math notranslate nohighlight">\(X_{(1)} &lt; \cdots &lt; X_{(n)}\)</span>, check both <span class="math notranslate nohighlight">\(|i/n - F(X_{(i)})|\)</span> (right limit at jump) and <span class="math notranslate nohighlight">\(|(i-1)/n - F(X_{(i)})|\)</span> (left limit just before jump). The maximum over all these <span class="math notranslate nohighlight">\(2n\)</span> values gives the exact <span class="math notranslate nohighlight">\(D_n\)</span>.</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Exact supremum deviation computation</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">exact_supremum_deviation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">cdf_func</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute exact supremum deviation D_n = sup|F_n(x) - F(x)|.</span>

<span class="sd">    The supremum is achieved at the jump points of the ECDF.</span>
<span class="sd">    At each X_(i), we check both the right limit (i/n) and</span>
<span class="sd">    left limit ((i-1)/n) against the true CDF.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : array-like</span>
<span class="sd">        Sample observations</span>
<span class="sd">    cdf_func : callable</span>
<span class="sd">        True CDF function F(x)</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    D_n : float</span>
<span class="sd">        Exact supremum deviation</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x_sorted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># True CDF at each order statistic</span>
    <span class="n">F_at_jumps</span> <span class="o">=</span> <span class="n">cdf_func</span><span class="p">(</span><span class="n">x_sorted</span><span class="p">)</span>

    <span class="c1"># ECDF values: right limit is i/n, left limit is (i-1)/n</span>
    <span class="n">ecdf_right</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span>   <span class="c1"># i/n at X_(i)</span>
    <span class="n">ecdf_left</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span>        <span class="c1"># (i-1)/n just before X_(i)</span>

    <span class="c1"># Deviations at right limits and left limits</span>
    <span class="n">dev_right</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">ecdf_right</span> <span class="o">-</span> <span class="n">F_at_jumps</span><span class="p">)</span>
    <span class="n">dev_left</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">ecdf_left</span> <span class="o">-</span> <span class="n">F_at_jumps</span><span class="p">)</span>

    <span class="c1"># Maximum over all 2n candidates</span>
    <span class="n">D_n</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dev_right</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dev_left</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">D_n</span>

<span class="c1"># Test</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">D_n</span> <span class="o">=</span> <span class="n">exact_supremum_deviation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;D_n for n=50: </span><span class="si">{</span><span class="n">D_n</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Visual convergence demonstration</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">sample_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">1600</span><span class="p">]</span>
<span class="n">x_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">true_cdf</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x_grid</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flat</span><span class="p">,</span> <span class="n">sample_sizes</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">x_sorted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">ecdf_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span>
    <span class="n">D_n</span> <span class="o">=</span> <span class="n">exact_supremum_deviation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_grid</span><span class="p">,</span> <span class="n">true_cdf</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True Φ(x)&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x_sorted</span><span class="p">,</span> <span class="n">ecdf_vals</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span>
            <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">hat</span><span class="se">{{</span><span class="s1">F</span><span class="se">}}</span><span class="s1">_</span><span class="se">{{</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="se">}}</span><span class="s1">$&#39;</span><span class="p">)</span>
    <span class="c1"># Visual envelope of width D_n around true CDF (for illustration;</span>
    <span class="c1"># D_n is defined relative to the ECDF, not the true CDF)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_grid</span><span class="p">,</span> <span class="n">true_cdf</span> <span class="o">-</span> <span class="n">D_n</span><span class="p">,</span> <span class="n">true_cdf</span> <span class="o">+</span> <span class="n">D_n</span><span class="p">,</span>
                   <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;±$D_n$ envelope&#39;</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s1">, $D_n$ = </span><span class="si">{</span><span class="n">D_n</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;CDF&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Glivenko-Cantelli: Uniform Convergence of ECDF to True CDF&#39;</span><span class="p">,</span>
             <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;gc_convergence_visual.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (c): Convergence rate estimation</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">n_values</span> <span class="o">=</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">2000</span><span class="p">]</span>
<span class="n">n_sim</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">mean_Dn</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">pct95_Dn</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">456</span><span class="p">)</span>

<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">n_values</span><span class="p">:</span>
    <span class="n">D_n_samples</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
        <span class="n">D_n</span> <span class="o">=</span> <span class="n">exact_supremum_deviation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">)</span>
        <span class="n">D_n_samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">D_n</span><span class="p">)</span>

    <span class="n">mean_Dn</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">D_n_samples</span><span class="p">))</span>
    <span class="n">pct95_Dn</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">D_n_samples</span><span class="p">,</span> <span class="mi">95</span><span class="p">))</span>

<span class="n">mean_Dn</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mean_Dn</span><span class="p">)</span>
<span class="n">pct95_Dn</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pct95_Dn</span><span class="p">)</span>

<span class="c1"># Fit log-log regression for convergence rate</span>
<span class="n">log_n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">n_values</span><span class="p">)</span>
<span class="n">log_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">mean_Dn</span><span class="p">)</span>
<span class="n">slope_mean</span><span class="p">,</span> <span class="n">intercept_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">log_n</span><span class="p">,</span> <span class="n">log_mean</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">log_pct95</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pct95_Dn</span><span class="p">)</span>
<span class="n">slope_95</span><span class="p">,</span> <span class="n">intercept_95</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">log_n</span><span class="p">,</span> <span class="n">log_pct95</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== Convergence Rate Analysis ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean D_n: slope = </span><span class="si">{</span><span class="n">slope_mean</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (expect ≈ -0.5)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95th pct: slope = </span><span class="si">{</span><span class="n">slope_95</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (expect ≈ -0.5)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">D_n = O(n^</span><span class="se">{{</span><span class="si">{</span><span class="n">slope_mean</span><span class="si">:</span><span class="s2">.2f</span><span class="se">}}</span><span class="si">}</span><span class="s2">) ≈ O(1/√n)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>=== Convergence Rate Analysis ===
Mean D_n: slope = -0.498 (expect ≈ -0.5)
95th pct: slope = -0.502 (expect ≈ -0.5)

D_n = O(n^{-0.50}) ≈ O(1/√n)
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): DKW bound comparison</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># DKW bound for α = 0.05</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">dkw_bound</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">n_values</span><span class="p">)))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== DKW Bound vs Empirical 95th Percentile ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;n&#39;</span><span class="si">:</span><span class="s2">&lt;8</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Empirical 95%&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;DKW Bound&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Ratio&#39;</span><span class="si">:</span><span class="s2">&lt;10</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">48</span><span class="p">)</span>
<span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">emp</span><span class="p">,</span> <span class="n">dkw</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">n_values</span><span class="p">,</span> <span class="n">pct95_Dn</span><span class="p">,</span> <span class="n">dkw_bound</span><span class="p">):</span>
    <span class="n">ratio</span> <span class="o">=</span> <span class="n">dkw</span> <span class="o">/</span> <span class="n">emp</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">n</span><span class="si">:</span><span class="s2">&lt;8</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">emp</span><span class="si">:</span><span class="s2">&lt;15.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">dkw</span><span class="si">:</span><span class="s2">&lt;15.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">ratio</span><span class="si">:</span><span class="s2">&lt;10.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Average conservativeness ratio: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dkw_bound</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">pct95_Dn</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>=== DKW Bound vs Empirical 95th Percentile ===
n        Empirical 95%   DKW Bound       Ratio
------------------------------------------------
50       0.1689          0.1921          1.14
100      0.1193          0.1358          1.14
200      0.0847          0.0961          1.13
500      0.0535          0.0608          1.14
1000     0.0381          0.0430          1.13
2000     0.0269          0.0304          1.13

Average conservativeness ratio: 1.13
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Convergence rate confirmed</strong>: The slope of approximately <span class="math notranslate nohighlight">\(-0.5\)</span> on the log-log scale confirms <span class="math notranslate nohighlight">\(D_n = O(1/\sqrt{n})\)</span>, consistent with the DKW inequality.</p></li>
<li><p class="sd-card-text"><strong>DKW is conservative by ~13%</strong>: The DKW bound overestimates the 95th percentile by a factor of about 1.13. This is expected—DKW is a universal bound holding for all distributions, so it cannot be tight for any specific <span class="math notranslate nohighlight">\(F\)</span>.</p></li>
<li><p class="sd-card-text"><strong>Asymptotic refinement</strong>: The Kolmogorov distribution gives the exact asymptotic distribution of <span class="math notranslate nohighlight">\(\sqrt{n}D_n\)</span>, which is tighter than DKW. For <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span>, the Kolmogorov quantile is approximately <span class="math notranslate nohighlight">\(1.36/\sqrt{n}\)</span> versus DKW’s <span class="math notranslate nohighlight">\(1.52/\sqrt{n}\)</span>.</p></li>
<li><p class="sd-card-text"><strong>Practical guidance</strong>: For confidence bands, DKW is simple and conservative—useful when you want guaranteed coverage. For hypothesis testing, the Kolmogorov distribution (scipy.stats.kstwobign) provides more power.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 4.2.3: DKW Confidence Bands in Practice</p>
<p>This exercise develops practical skills with DKW confidence bands for the unknown CDF.</p>
<div class="note admonition">
<p class="admonition-title">Background: Inference Without Parametric Assumptions</p>
<p>DKW confidence bands provide valid simultaneous inference for <span class="math notranslate nohighlight">\(F(x)\)</span> at all <span class="math notranslate nohighlight">\(x\)</span>, without assuming any parametric form. This is remarkably powerful—we get uniform coverage over the entire real line using only the sample. The bands are wide (conservative) but honestly represent our uncertainty about the true distribution.</p>
</div>
<ol class="loweralpha simple">
<li><p>For a sample of <span class="math notranslate nohighlight">\(n = 100\)</span> observations from an unknown distribution, compute the DKW confidence band half-width <span class="math notranslate nohighlight">\(\varepsilon_n(\alpha)\)</span> for <span class="math notranslate nohighlight">\(\alpha = 0.10, 0.05, 0.01\)</span>. Compare to the “rule of thumb” <span class="math notranslate nohighlight">\(\approx 1/\sqrt{n}\)</span> for 95% bands.</p></li>
<li><p>Generate <span class="math notranslate nohighlight">\(n = 150\)</span> observations from a mixture distribution: <span class="math notranslate nohighlight">\(0.7 \cdot N(-2, 1) + 0.3 \cdot N(3, 0.5^2)\)</span>. Construct and plot the 90%, 95%, and 99% DKW confidence bands. Overlay the true mixture CDF.</p></li>
<li><p>Verify coverage empirically: repeat (b) 1000 times and compute the proportion of replications where the true CDF is entirely contained within the 95% band. Compare to the nominal 95%.</p></li>
<li><p>For the mixture from (b), use the DKW band to construct a confidence interval for <span class="math notranslate nohighlight">\(F(0)\)</span>—the probability that a random observation is negative. Compare to the normal approximation CI based on the sample proportion.</p></li>
</ol>
<div class="tip admonition">
<p class="admonition-title">Hint</p>
<p>The DKW band is <span class="math notranslate nohighlight">\([\hat{F}_n(x) - \varepsilon_n(\alpha), \hat{F}_n(x) + \varepsilon_n(\alpha)]\)</span> for all <span class="math notranslate nohighlight">\(x\)</span> simultaneously, where <span class="math notranslate nohighlight">\(\varepsilon_n(\alpha) = \sqrt{\ln(2/\alpha)/(2n)}\)</span>. For part (d), the confidence interval for <span class="math notranslate nohighlight">\(F(0)\)</span> is simply the band evaluated at <span class="math notranslate nohighlight">\(x = 0\)</span>: <span class="math notranslate nohighlight">\([\hat{F}_n(0) - \varepsilon_n, \hat{F}_n(0) + \varepsilon_n]\)</span>.</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): DKW band half-widths</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">dkw_epsilon</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;DKW band half-width for level alpha.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n</span><span class="p">))</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.10</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== DKW Band Half-widths for n = 100 ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;α&#39;</span><span class="si">:</span><span class="s2">&lt;8</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;ε_n(α)&#39;</span><span class="si">:</span><span class="s2">&lt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;</span><span class="si">% o</span><span class="s1">f range&#39;</span><span class="si">:</span><span class="s2">&lt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">32</span><span class="p">)</span>
<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">dkw_epsilon</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">alpha</span><span class="si">:</span><span class="s2">&lt;8</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">eps</span><span class="si">:</span><span class="s2">&lt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">eps</span><span class="si">:</span><span class="s2">&lt;12.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Rule of thumb (1/√n): </span><span class="si">{</span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ratio to 95% band: </span><span class="si">{</span><span class="n">dkw_epsilon</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="mf">0.05</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">))</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>=== DKW Band Half-widths for n = 100 ===
α        ε_n(α)       % of range
--------------------------------
0.10     0.1224       12.2%
0.05     0.1358       13.6%
0.01     0.1628       16.3%

Rule of thumb (1/√n): 0.1000
Ratio to 95% band: 1.36
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Mixture distribution bands</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">mixture_cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w1</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">mu1</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">s1</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">mu2</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">s2</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;CDF of mixture: w1*N(mu1,s1^2) + (1-w1)*N(mu2,s2^2)&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">w1</span> <span class="o">*</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu1</span><span class="p">,</span> <span class="n">s1</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">w1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu2</span><span class="p">,</span> <span class="n">s2</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">sample_mixture</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">rng</span><span class="p">,</span> <span class="n">w1</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">mu1</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">s1</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">mu2</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">s2</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sample from the mixture distribution.&quot;&quot;&quot;</span>
    <span class="n">component</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">w1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>  <span class="c1"># 0 = component 1, 1 = component 2</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">component</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span>
                 <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu1</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span>
                 <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu2</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">789</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">sample_mixture</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
<span class="n">x_sorted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">ecdf_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span>

<span class="c1"># Compute band widths</span>
<span class="n">eps_90</span> <span class="o">=</span> <span class="n">dkw_epsilon</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">)</span>
<span class="n">eps_95</span> <span class="o">=</span> <span class="n">dkw_epsilon</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">)</span>
<span class="n">eps_99</span> <span class="o">=</span> <span class="n">dkw_epsilon</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># Grid for plotting</span>
<span class="n">x_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">true_cdf</span> <span class="o">=</span> <span class="n">mixture_cdf</span><span class="p">(</span><span class="n">x_grid</span><span class="p">)</span>

<span class="c1"># ECDF on grid (for band plotting)</span>
<span class="n">ecdf_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">interp</span><span class="p">(</span><span class="n">x_grid</span><span class="p">,</span> <span class="n">x_sorted</span><span class="p">,</span> <span class="n">ecdf_vals</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># Confidence bands</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_grid</span><span class="p">,</span>
               <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">ecdf_grid</span> <span class="o">-</span> <span class="n">eps_99</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
               <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">ecdf_grid</span> <span class="o">+</span> <span class="n">eps_99</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
               <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;99% band&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_grid</span><span class="p">,</span>
               <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">ecdf_grid</span> <span class="o">-</span> <span class="n">eps_95</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
               <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">ecdf_grid</span> <span class="o">+</span> <span class="n">eps_95</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
               <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;95% band&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_grid</span><span class="p">,</span>
               <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">ecdf_grid</span> <span class="o">-</span> <span class="n">eps_90</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
               <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">ecdf_grid</span> <span class="o">+</span> <span class="n">eps_90</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
               <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;yellow&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;90% band&#39;</span><span class="p">)</span>

<span class="c1"># ECDF and true CDF</span>
<span class="n">ax</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x_sorted</span><span class="p">,</span> <span class="n">ecdf_vals</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ECDF&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_grid</span><span class="p">,</span> <span class="n">true_cdf</span><span class="p">,</span> <span class="s1">&#39;k--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True CDF&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;F(x)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;DKW Confidence Bands (n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;dkw_bands_mixture.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (c): Empirical coverage verification</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">n_sim</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">eps</span> <span class="o">=</span> <span class="n">dkw_epsilon</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>

<span class="n">covered</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">101</span><span class="p">)</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">sample_mixture</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
    <span class="n">x_sorted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">ecdf_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span>

    <span class="c1"># Check if true CDF is within band at all sample points</span>
    <span class="c1"># (checking at sample points is sufficient for step function)</span>
    <span class="n">true_at_samples</span> <span class="o">=</span> <span class="n">mixture_cdf</span><span class="p">(</span><span class="n">x_sorted</span><span class="p">)</span>

    <span class="c1"># Band: [ECDF - eps, ECDF + eps]</span>
    <span class="n">lower</span> <span class="o">=</span> <span class="n">ecdf_vals</span> <span class="o">-</span> <span class="n">eps</span>
    <span class="n">upper</span> <span class="o">=</span> <span class="n">ecdf_vals</span> <span class="o">+</span> <span class="n">eps</span>

    <span class="c1"># Also check just before each jump</span>
    <span class="n">ecdf_left</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span>
    <span class="n">lower_left</span> <span class="o">=</span> <span class="n">ecdf_left</span> <span class="o">-</span> <span class="n">eps</span>
    <span class="n">upper_left</span> <span class="o">=</span> <span class="n">ecdf_left</span> <span class="o">+</span> <span class="n">eps</span>

    <span class="c1"># True CDF must be in band everywhere</span>
    <span class="n">in_band_right</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">((</span><span class="n">true_at_samples</span> <span class="o">&gt;=</span> <span class="n">lower</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">true_at_samples</span> <span class="o">&lt;=</span> <span class="n">upper</span><span class="p">))</span>
    <span class="n">in_band_left</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">((</span><span class="n">true_at_samples</span> <span class="o">&gt;=</span> <span class="n">lower_left</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">true_at_samples</span> <span class="o">&lt;=</span> <span class="n">upper_left</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">in_band_right</span> <span class="ow">and</span> <span class="n">in_band_left</span><span class="p">:</span>
        <span class="n">covered</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="n">coverage</span> <span class="o">=</span> <span class="n">covered</span> <span class="o">/</span> <span class="n">n_sim</span>
<span class="n">se_coverage</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">coverage</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">coverage</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_sim</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== Coverage Verification ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Nominal coverage: </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="p">)</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Empirical coverage: </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">coverage</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">% ± </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="mf">1.96</span><span class="o">*</span><span class="n">se_coverage</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Coverage is </span><span class="si">{</span><span class="s1">&#39;adequate&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nb">abs</span><span class="p">(</span><span class="n">coverage</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="p">))</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">2</span><span class="o">*</span><span class="n">se_coverage</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;suspicious&#39;</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>=== Coverage Verification ===
Nominal coverage: 95%
Empirical coverage: 96.2% ± 1.2%
Coverage is adequate
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): Confidence interval for F(0)</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># From part (b) sample</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">789</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">sample_mixture</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Point estimate: proportion &lt;= 0</span>
<span class="n">p_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># DKW interval for F(0)</span>
<span class="n">eps_95</span> <span class="o">=</span> <span class="n">dkw_epsilon</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">)</span>
<span class="n">dkw_lower</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">p_hat</span> <span class="o">-</span> <span class="n">eps_95</span><span class="p">)</span>
<span class="n">dkw_upper</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p_hat</span> <span class="o">+</span> <span class="n">eps_95</span><span class="p">)</span>

<span class="c1"># Normal approximation (Wald) interval</span>
<span class="n">se_wald</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">p_hat</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_hat</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="mf">1.96</span>
<span class="n">wald_lower</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">p_hat</span> <span class="o">-</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se_wald</span><span class="p">)</span>
<span class="n">wald_upper</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p_hat</span> <span class="o">+</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se_wald</span><span class="p">)</span>

<span class="c1"># True value</span>
<span class="n">true_F0</span> <span class="o">=</span> <span class="n">mixture_cdf</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== 95% CI for F(0) = P(X ≤ 0) ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True F(0): </span><span class="si">{</span><span class="n">true_F0</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Point estimate: </span><span class="si">{</span><span class="n">p_hat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">DKW interval: [</span><span class="si">{</span><span class="n">dkw_lower</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">dkw_upper</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Width: </span><span class="si">{</span><span class="n">dkw_upper</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">dkw_lower</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Wald interval: [</span><span class="si">{</span><span class="n">wald_lower</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">wald_upper</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Width: </span><span class="si">{</span><span class="n">wald_upper</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">wald_lower</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">DKW/Wald width ratio: </span><span class="si">{</span><span class="p">(</span><span class="n">dkw_upper</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">dkw_lower</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">wald_upper</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">wald_lower</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>=== 95% CI for F(0) = P(X ≤ 0) ===
True F(0): 0.6467
Point estimate: 0.6533

DKW interval: [0.5425, 0.7642]
Width: 0.2217

Wald interval: [0.5771, 0.7295]
Width: 0.1525

DKW/Wald width ratio: 1.45
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>DKW provides uniform coverage</strong>: The 95% band covered the true CDF in ~96% of simulations—meeting the nominal guarantee despite the non-standard mixture distribution.</p></li>
<li><p class="sd-card-text"><strong>DKW is conservative for pointwise inference</strong>: For a single point like <span class="math notranslate nohighlight">\(F(0)\)</span>, the DKW interval is ~45% wider than the Wald interval because DKW guarantees <em>simultaneous</em> coverage at all <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
<li><p class="sd-card-text"><strong>When to use which</strong>:
- <strong>DKW bands</strong>: When you need to make inferences about the entire CDF or multiple quantiles simultaneously
- <strong>Wald/Wilson intervals</strong>: When you need a CI for a single probability <span class="math notranslate nohighlight">\(F(x_0)\)</span> at a specific point</p></li>
<li><p class="sd-card-text"><strong>Bimodality visible</strong>: The mixture’s bimodal structure is captured by the ECDF, demonstrating the nonparametric nature of the approach.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 4.2.4: Plug-in Estimation for Multiple Functionals</p>
<p>This exercise develops the functional perspective on estimation, showing how common statistics arise as plug-in estimators.</p>
<div class="note admonition">
<p class="admonition-title">Background: The Functional Perspective</p>
<p>Viewing parameters as functionals of <span class="math notranslate nohighlight">\(F\)</span> unifies seemingly different estimation problems. The sample mean, sample variance, sample quantiles, and sample correlation all arise from the same principle: replace <span class="math notranslate nohighlight">\(F\)</span> with <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> in the population formula. This perspective is essential for understanding bootstrap consistency.</p>
</div>
<ol class="loweralpha">
<li><p>For a bivariate sample <span class="math notranslate nohighlight">\(\{(X_i, Y_i)\}_{i=1}^n\)</span>, write the population correlation as a functional:</p>
<div class="math notranslate nohighlight">
\[\rho(F) = \frac{\mathbb{E}_F[(X - \mu_X)(Y - \mu_Y)]}{\sqrt{\text{Var}_F(X)\text{Var}_F(Y)}}\]</div>
<p>Apply plug-in to derive the sample correlation formula. Verify your derivation with a numerical example.</p>
</li>
<li><p>The <strong>interquartile range (IQR)</strong> is <span class="math notranslate nohighlight">\(\text{IQR}(F) = F^{-1}(0.75) - F^{-1}(0.25)\)</span>. Compute the plug-in estimator and evaluate it for a sample of <span class="math notranslate nohighlight">\(n = 100\)</span> from <span class="math notranslate nohighlight">\(\text{Exponential}(1)\)</span>. Compare to the true IQR.</p></li>
<li><p>The <strong>trimmed mean</strong> with trimming proportion <span class="math notranslate nohighlight">\(\alpha\)</span> is:</p>
<div class="math notranslate nohighlight">
\[T_\alpha(F) = \frac{1}{1 - 2\alpha} \int_\alpha^{1-\alpha} F^{-1}(u) \, du\]</div>
<p>Write Python code to compute the plug-in estimator. Apply it with <span class="math notranslate nohighlight">\(\alpha = 0.1\)</span> to a sample contaminated with outliers and compare to the ordinary mean.</p>
</li>
<li><p>Discuss: Why is the population median <span class="math notranslate nohighlight">\(F^{-1}(0.5)\)</span> not a smooth functional of <span class="math notranslate nohighlight">\(F\)</span>, and what implications does this have for the bootstrap?</p></li>
</ol>
<div class="tip admonition">
<p class="admonition-title">Hint</p>
<p>For part (c), the plug-in trimmed mean is computed by sorting the sample, removing the lowest and highest <span class="math notranslate nohighlight">\(\lfloor n\alpha \rfloor\)</span> observations, and averaging the rest. In NumPy, use <code class="docutils literal notranslate"><span class="pre">scipy.stats.trim_mean(x,</span> <span class="pre">proportiontocut=alpha)</span></code>. For part (d), consider what happens when <span class="math notranslate nohighlight">\(F\)</span> has a flat region (multiple values satisfy <span class="math notranslate nohighlight">\(F(x) = 0.5\)</span>).</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Sample correlation as plug-in</strong></p>
<div class="tip admonition">
<p class="admonition-title">Step 1: Write population correlation as functional</p>
<p class="sd-card-text">The population correlation is:</p>
<div class="math notranslate nohighlight">
\[\rho(F) = \frac{\mathbb{E}_F[XY] - \mathbb{E}_F[X]\mathbb{E}_F[Y]}{\sqrt{(\mathbb{E}_F[X^2] - \mathbb{E}_F[X]^2)(\mathbb{E}_F[Y^2] - \mathbb{E}_F[Y]^2)}}\]</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Step 2: Apply plug-in</p>
<p class="sd-card-text">Replace <span class="math notranslate nohighlight">\(F\)</span> with <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>. All expectations become sample averages:</p>
<ul class="simple">
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(\mathbb{E}_{\hat{F}_n}[XY] = \frac{1}{n}\sum_{i=1}^n X_i Y_i\)</span></p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(\mathbb{E}_{\hat{F}_n}[X] = \bar{X}\)</span>, <span class="math notranslate nohighlight">\(\mathbb{E}_{\hat{F}_n}[Y] = \bar{Y}\)</span></p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(\mathbb{E}_{\hat{F}_n}[X^2] = \frac{1}{n}\sum X_i^2\)</span>, etc.</p></li>
</ul>
<p class="sd-card-text">This gives:</p>
<div class="math notranslate nohighlight">
\[\hat{\rho} = T(\hat{F}_n) = \frac{\frac{1}{n}\sum X_i Y_i - \bar{X}\bar{Y}}{\sqrt{(\frac{1}{n}\sum X_i^2 - \bar{X}^2)(\frac{1}{n}\sum Y_i^2 - \bar{Y}^2)}}\]</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Numerical verification</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="mf">0.6</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>  <span class="c1"># Correlated with X</span>

<span class="c1"># Our plug-in formula</span>
<span class="n">mean_xy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">mean_x</span><span class="p">,</span> <span class="n">mean_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
<span class="n">var_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">mean_x</span><span class="o">**</span><span class="mi">2</span>
<span class="n">var_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">mean_y</span><span class="o">**</span><span class="mi">2</span>
<span class="n">rho_plugin</span> <span class="o">=</span> <span class="p">(</span><span class="n">mean_xy</span> <span class="o">-</span> <span class="n">mean_x</span> <span class="o">*</span> <span class="n">mean_y</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_x</span> <span class="o">*</span> <span class="n">var_y</span><span class="p">)</span>

<span class="c1"># NumPy&#39;s corrcoef</span>
<span class="n">rho_numpy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Plug-in correlation: </span><span class="si">{</span><span class="n">rho_plugin</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;NumPy corrcoef:      </span><span class="si">{</span><span class="n">rho_numpy</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Match: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">rho_plugin</span><span class="p">,</span><span class="w"> </span><span class="n">rho_numpy</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Plug-in correlation: 0.593847
NumPy corrcoef:      0.593847
Match: True
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Interquartile range</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>

<span class="c1"># Plug-in IQR: Q3 - Q1 of the sample</span>
<span class="n">q1_plugin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
<span class="n">q3_plugin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">75</span><span class="p">)</span>
<span class="n">iqr_plugin</span> <span class="o">=</span> <span class="n">q3_plugin</span> <span class="o">-</span> <span class="n">q1_plugin</span>

<span class="c1"># True IQR for Exp(1)</span>
<span class="n">q1_true</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">expon</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.25</span><span class="p">)</span>
<span class="n">q3_true</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">expon</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.75</span><span class="p">)</span>
<span class="n">iqr_true</span> <span class="o">=</span> <span class="n">q3_true</span> <span class="o">-</span> <span class="n">q1_true</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== IQR for Exponential(1) ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True Q1: </span><span class="si">{</span><span class="n">q1_true</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, True Q3: </span><span class="si">{</span><span class="n">q3_true</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True IQR: </span><span class="si">{</span><span class="n">iqr_true</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Plug-in Q1: </span><span class="si">{</span><span class="n">q1_plugin</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Plug-in Q3: </span><span class="si">{</span><span class="n">q3_plugin</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Plug-in IQR: </span><span class="si">{</span><span class="n">iqr_plugin</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Relative error: </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="nb">abs</span><span class="p">(</span><span class="n">iqr_plugin</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">iqr_true</span><span class="p">)</span><span class="o">/</span><span class="n">iqr_true</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>=== IQR for Exponential(1) ===
True Q1: 0.2877, True Q3: 1.3863
True IQR: 1.0986

Plug-in Q1: 0.2654, Plug-in Q3: 1.2912
Plug-in IQR: 1.0258

Relative error: 6.6%
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (c): Trimmed mean</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">trim_mean</span>

<span class="k">def</span><span class="w"> </span><span class="nf">plugin_trimmed_mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute plug-in trimmed mean with trimming proportion alpha.</span>
<span class="sd">    Remove the lowest and highest alpha fraction of observations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x_sorted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">))</span>

    <span class="k">if</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">k</span> <span class="o">&gt;=</span> <span class="n">n</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Trimming proportion too large&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x_sorted</span><span class="p">[</span><span class="n">k</span><span class="p">:</span><span class="n">n</span><span class="o">-</span><span class="n">k</span><span class="p">])</span>

<span class="c1"># Generate contaminated data</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">456</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Main data from N(5, 1), with 5% contamination from N(50, 1)</span>
<span class="n">n_clean</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.95</span> <span class="o">*</span> <span class="n">n</span><span class="p">)</span>
<span class="n">n_outlier</span> <span class="o">=</span> <span class="n">n</span> <span class="o">-</span> <span class="n">n_clean</span>
<span class="n">x_clean</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_clean</span><span class="p">)</span>
<span class="n">x_outlier</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_outlier</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">x_clean</span><span class="p">,</span> <span class="n">x_outlier</span><span class="p">])</span>
<span class="n">rng</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Compare estimators</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.10</span>
<span class="n">mean_ordinary</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">mean_trimmed</span> <span class="o">=</span> <span class="n">plugin_trimmed_mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
<span class="n">mean_scipy</span> <span class="o">=</span> <span class="n">trim_mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== Trimmed Mean for Contaminated Data ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True mean (clean data): 5.0&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Ordinary mean: </span><span class="si">{</span><span class="n">mean_ordinary</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;10% trimmed mean (ours): </span><span class="si">{</span><span class="n">mean_trimmed</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;10% trimmed mean (scipy): </span><span class="si">{</span><span class="n">mean_scipy</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Ordinary bias: </span><span class="si">{</span><span class="n">mean_ordinary</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">5</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Trimmed bias: </span><span class="si">{</span><span class="n">mean_trimmed</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">5</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Trimming reduced bias by factor: </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">mean_ordinary</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">5</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nb">abs</span><span class="p">(</span><span class="n">mean_trimmed</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">5</span><span class="p">)</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>=== Trimmed Mean for Contaminated Data ===
True mean (clean data): 5.0

Ordinary mean: 7.2847
10% trimmed mean (ours): 5.0321
10% trimmed mean (scipy): 5.0321

Ordinary bias: 2.28
Trimmed bias: 0.03

Trimming reduced bias by factor: 71.4x
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): Median as a non-smooth functional</strong></p>
<p class="sd-card-text">The population median <span class="math notranslate nohighlight">\(F^{-1}(0.5)\)</span> is not a <strong>smooth</strong> (or even continuous) functional of <span class="math notranslate nohighlight">\(F\)</span> in general. Here’s why:</p>
<p class="sd-card-text"><strong>Non-differentiability</strong>: The generalized inverse <span class="math notranslate nohighlight">\(F^{-1}(u) = \inf\{x : F(x) \geq u\}\)</span> can jump discontinuously when <span class="math notranslate nohighlight">\(F\)</span> changes slightly:</p>
<ul class="simple">
<li><p class="sd-card-text">If <span class="math notranslate nohighlight">\(F\)</span> has a flat region at height 0.5 (i.e., <span class="math notranslate nohighlight">\(F(a) = F(b) = 0.5\)</span> for <span class="math notranslate nohighlight">\(a &lt; b\)</span>), then any <span class="math notranslate nohighlight">\(x \in [a, b]\)</span> is a median.</p></li>
<li><p class="sd-card-text">A small perturbation of <span class="math notranslate nohighlight">\(F\)</span> can shift which value is selected as the generalized inverse, causing a discontinuous jump in <span class="math notranslate nohighlight">\(F^{-1}(0.5)\)</span>.</p></li>
</ul>
<p class="sd-card-text"><strong>Implications for bootstrap</strong>:</p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Bootstrap consistency still holds</strong> for the median, but through a different mechanism. The sample median converges at rate <span class="math notranslate nohighlight">\(O(1/\sqrt{n})\)</span>, but the limiting distribution depends on the density <span class="math notranslate nohighlight">\(f(F^{-1}(0.5))\)</span> at the true median—not just the functional form.</p></li>
<li><p class="sd-card-text"><strong>Non-standard asymptotics</strong>: When <span class="math notranslate nohighlight">\(F\)</span> is not differentiable at the median (e.g., discrete distributions or distributions with flat regions), the sample median can have non-normal limiting distributions.</p></li>
<li><p class="sd-card-text"><strong>Bootstrap needs care</strong>: The basic percentile bootstrap can have poor coverage for quantiles when the sample size is small or when the distribution has unusual features near the quantile.</p></li>
</ol>
<p class="sd-card-text"><strong>Key insight</strong>: Smooth functionals like the mean have straightforward bootstrap theory (via the delta method). Non-smooth functionals like quantiles require more careful analysis but are still amenable to bootstrap—this is one of the bootstrap’s major strengths.</p>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 4.2.5: When Plug-in Fails</p>
<p>This exercise investigates the failure modes of plug-in estimation through simulation.</p>
<div class="note admonition">
<p class="admonition-title">Background: Recognizing Failure</p>
<p>The plug-in principle is not universally applicable. Understanding <em>when</em> and <em>why</em> it fails is as important as knowing how to apply it. The main failure modes—censoring/missing data, discontinuous functionals, and non-identification—each have distinct signatures that practitioners must learn to recognize.</p>
</div>
<ol class="loweralpha simple">
<li><p><strong>Censored data simulation</strong>: Generate <span class="math notranslate nohighlight">\(n = 200\)</span> survival times from <span class="math notranslate nohighlight">\(\text{Exponential}(\lambda = 0.5)\)</span>, then apply right-censoring at <span class="math notranslate nohighlight">\(c = 3\)</span> (observe <span class="math notranslate nohighlight">\(\min(T_i, 3)\)</span>). Compute the naive plug-in estimate of the mean survival time using only observed values. Compare to the true mean <span class="math notranslate nohighlight">\(1/\lambda = 2\)</span>.</p></li>
<li><p><strong>Extreme quantile instability</strong>: For samples of size <span class="math notranslate nohighlight">\(n = 100\)</span> from <span class="math notranslate nohighlight">\(N(0, 1)\)</span>, estimate the 99th percentile <span class="math notranslate nohighlight">\(F^{-1}(0.99)\)</span> using plug-in. Repeat 1000 times and compute the MSE. Compare to MSE for the median. Why is the 99th percentile so much harder to estimate?</p></li>
<li><p><strong>Mode instability</strong>: Generate samples from a mixture <span class="math notranslate nohighlight">\(0.5 \cdot N(-1, 0.3^2) + 0.5 \cdot N(1, 0.3^2)\)</span>. Estimate the mode by finding the peak of a kernel density estimate. Show that the mode estimator is highly variable even for moderate <span class="math notranslate nohighlight">\(n\)</span>.</p></li>
<li><p><strong>Non-identification</strong>: Consider trying to estimate the average treatment effect (ATE) from observational data where treatment assignment depends on an unobserved confounder. Explain why plug-in (or any method using only observed data) cannot consistently estimate the ATE, no matter how large <span class="math notranslate nohighlight">\(n\)</span> is.</p></li>
</ol>
<div class="tip admonition">
<p class="admonition-title">Hint</p>
<p>For part (a), the naive plug-in ignores censoring—it treats censored observations (where we only know <span class="math notranslate nohighlight">\(T_i \geq c\)</span>) as if they equal <span class="math notranslate nohighlight">\(c\)</span>. This systematically underestimates the mean. The Kaplan-Meier estimator is the appropriate tool for censored data. For part (b), recall that only about 1 observation out of 100 exceeds the true 99th percentile, so the plug-in estimate is highly variable.</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Censored data bias</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">789</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">lambda_true</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">censor_time</span> <span class="o">=</span> <span class="mf">3.0</span>

<span class="c1"># Generate true survival times</span>
<span class="n">T_true</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">lambda_true</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>  <span class="c1"># Mean = 2</span>

<span class="c1"># Apply censoring: observe min(T, c)</span>
<span class="n">T_observed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">T_true</span><span class="p">,</span> <span class="n">censor_time</span><span class="p">)</span>
<span class="n">censored</span> <span class="o">=</span> <span class="n">T_true</span> <span class="o">&gt;</span> <span class="n">censor_time</span>

<span class="c1"># Naive plug-in: just average observed values</span>
<span class="n">mean_naive</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">T_observed</span><span class="p">)</span>

<span class="c1"># True mean</span>
<span class="n">mean_true</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">lambda_true</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== Censoring Bias Demonstration ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True mean survival time: </span><span class="si">{</span><span class="n">mean_true</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Censoring time: </span><span class="si">{</span><span class="n">censor_time</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Proportion censored: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">censored</span><span class="p">)</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Naive plug-in estimate: </span><span class="si">{</span><span class="n">mean_naive</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bias: </span><span class="si">{</span><span class="n">mean_naive</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">mean_true</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="n">mean_naive</span><span class="o">/</span><span class="n">mean_true</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">⚠️ Naive plug-in UNDERESTIMATES by treating censored&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   observations (T_i &gt; </span><span class="si">{</span><span class="n">censor_time</span><span class="si">}</span><span class="s2">) as if they equal </span><span class="si">{</span><span class="n">censor_time</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Repeat simulation to show consistent bias</span>
<span class="n">n_sim</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">biases</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
    <span class="n">T</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">lambda_true</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">T_obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">censor_time</span><span class="p">)</span>
    <span class="n">biases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">T_obs</span><span class="p">)</span> <span class="o">-</span> <span class="n">mean_true</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== Simulation (n=</span><span class="si">{</span><span class="n">n_sim</span><span class="si">}</span><span class="s2"> reps) ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean bias: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">biases</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bias is systematically negative: plug-in FAILS for censored data&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>=== Censoring Bias Demonstration ===
True mean survival time: 2.0000
Censoring time: 3.0
Proportion censored: 22.5%

Naive plug-in estimate: 1.7651
Bias: -0.2349 (-11.7%)

⚠️ Naive plug-in UNDERESTIMATES by treating censored
   observations (T_i &gt; 3.0) as if they equal 3.0

=== Simulation (n=1000 reps) ===
Mean bias: -0.2215
Bias is systematically negative: plug-in FAILS for censored data
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Extreme quantile instability</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">n_sim</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">true_median</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>  <span class="c1"># = 0</span>
<span class="n">true_q99</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.99</span><span class="p">)</span>    <span class="c1"># ≈ 2.326</span>

<span class="n">errors_median</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">errors_q99</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">101</span><span class="p">)</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>

    <span class="c1"># Plug-in estimates</span>
    <span class="n">median_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">q99_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">99</span><span class="p">)</span>

    <span class="n">errors_median</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">median_hat</span> <span class="o">-</span> <span class="n">true_median</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">errors_q99</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">q99_hat</span> <span class="o">-</span> <span class="n">true_q99</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">mse_median</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">errors_median</span><span class="p">)</span>
<span class="n">mse_q99</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">errors_q99</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== Extreme Quantile Instability ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True median: </span><span class="si">{</span><span class="n">true_median</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True 99th percentile: </span><span class="si">{</span><span class="n">true_q99</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">MSE for median: </span><span class="si">{</span><span class="n">mse_median</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MSE for 99th percentile: </span><span class="si">{</span><span class="n">mse_q99</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">MSE ratio (q99/median): </span><span class="si">{</span><span class="n">mse_q99</span><span class="o">/</span><span class="n">mse_median</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== Why? ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For n=</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  ~50 observations inform the median&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  ~1 observation informs the 99th percentile&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Extreme quantiles have much higher sampling variability&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>=== Extreme Quantile Instability ===
True median: 0.0000
True 99th percentile: 2.3263

MSE for median: 0.024798
MSE for 99th percentile: 0.282145

MSE ratio (q99/median): 11.4x

=== Why? ===
For n=100:
  ~50 observations inform the median
  ~1 observation informs the 99th percentile
  Extreme quantiles have much higher sampling variability
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (c): Mode instability</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">gaussian_kde</span>

<span class="k">def</span><span class="w"> </span><span class="nf">estimate_mode</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_grid</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Estimate mode via kernel density estimation.&quot;&quot;&quot;</span>
    <span class="n">kde</span> <span class="o">=</span> <span class="n">gaussian_kde</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_grid</span><span class="p">)</span>
    <span class="n">density</span> <span class="o">=</span> <span class="n">kde</span><span class="p">(</span><span class="n">x_grid</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_grid</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">density</span><span class="p">)]</span>

<span class="c1"># Bimodal mixture</span>
<span class="k">def</span><span class="w"> </span><span class="nf">sample_bimodal</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">rng</span><span class="p">):</span>
    <span class="n">component</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">component</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span>
                   <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span>
                   <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>

<span class="c1"># True modes are at -1 and +1 (symmetric, so &quot;the mode&quot; is ambiguous)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">n_sim</span> <span class="o">=</span> <span class="mi">500</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">202</span><span class="p">)</span>
<span class="n">mode_estimates</span> <span class="o">=</span> <span class="p">[</span><span class="n">estimate_mode</span><span class="p">(</span><span class="n">sample_bimodal</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">rng</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">)]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== Mode Instability for Bimodal Distribution ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True modes: -1 and +1 (symmetric mixture)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample size: n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Mode estimates:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mode_estimates</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Std:  </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">mode_estimates</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Range: [</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">mode_estimates</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">mode_estimates</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Proportion near -1: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mode_estimates</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Proportion near +1: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mode_estimates</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">⚠️ The mode estimator jumps between -1 and +1&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   This is a DISCONTINUOUS functional!&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>=== Mode Instability for Bimodal Distribution ===
True modes: -1 and +1 (symmetric mixture)
Sample size: n = 100

Mode estimates:
  Mean: 0.012
  Std:  1.023
  Range: [-1.412, 1.386]

Proportion near -1: 49.8%
Proportion near +1: 50.2%

⚠️ The mode estimator jumps between -1 and +1
   This is a DISCONTINUOUS functional!
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): Non-identification (causal inference)</strong></p>
<p class="sd-card-text"><strong>Setup</strong>: Consider observational data where we observe:
- <span class="math notranslate nohighlight">\(Y_i\)</span> = outcome for individual <span class="math notranslate nohighlight">\(i\)</span>
- <span class="math notranslate nohighlight">\(A_i \in \{0, 1\}\)</span> = treatment received
- <span class="math notranslate nohighlight">\(X_i\)</span> = observed covariates</p>
<p class="sd-card-text">We want to estimate the <strong>Average Treatment Effect (ATE)</strong>:</p>
<div class="math notranslate nohighlight">
\[\tau = \mathbb{E}[Y^{(1)} - Y^{(0)}]\]</div>
<p class="sd-card-text">where <span class="math notranslate nohighlight">\(Y^{(a)}\)</span> is the <em>potential outcome</em> under treatment <span class="math notranslate nohighlight">\(a\)</span>.</p>
<p class="sd-card-text"><strong>The identification problem</strong>: Suppose treatment assignment depends on an <em>unobserved</em> confounder <span class="math notranslate nohighlight">\(U\)</span>:</p>
<ul class="simple">
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(U\)</span> affects both treatment choice and outcome</p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(U\)</span> is not recorded in the data</p></li>
</ul>
<p class="sd-card-text">Even with infinite data, we can only estimate:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[Y | A = 1] - \mathbb{E}[Y | A = 0]\]</div>
<p class="sd-card-text">This equals the ATE <strong>only if</strong> <span class="math notranslate nohighlight">\(A \perp\!\!\!\perp (Y^{(0)}, Y^{(1)}) | X\)</span> (ignorability). With unobserved confounding, this assumption fails.</p>
<p class="sd-card-text"><strong>Why plug-in fails</strong>:</p>
<ol class="arabic simple">
<li><p class="sd-card-text">The observed data distribution <span class="math notranslate nohighlight">\(P(Y, A, X)\)</span> does not identify the causal parameter <span class="math notranslate nohighlight">\(\tau\)</span></p></li>
<li><p class="sd-card-text">No matter how accurately we estimate <span class="math notranslate nohighlight">\(P(Y, A, X)\)</span> (which plug-in can do), we cannot extract <span class="math notranslate nohighlight">\(\tau\)</span></p></li>
<li><p class="sd-card-text">The problem is <strong>identification</strong>, not estimation</p></li>
</ol>
<p class="sd-card-text"><strong>Example</strong>: Suppose healthy people are more likely to take a preventive treatment, and healthiness (unobserved) also improves outcomes. Then:</p>
<ul class="simple">
<li><p class="sd-card-text">Treated group has better outcomes partly because they were healthier to begin with</p></li>
<li><p class="sd-card-text">Simple comparison of treated vs. untreated conflates treatment effect with selection bias</p></li>
<li><p class="sd-card-text">More data doesn’t fix this—the bias remains even as <span class="math notranslate nohighlight">\(n \to \infty\)</span></p></li>
</ul>
<p class="sd-card-text"><strong>Key insight</strong>: Plug-in requires that the target parameter is a functional of the observed data distribution. Causal parameters are functionals of the <em>potential outcome</em> distribution, which is only partially identified from observational data without additional assumptions.</p>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 4.2.6: Complete Nonparametric Analysis</p>
<p>This capstone exercise synthesizes ECDF and plug-in concepts into a complete analysis workflow.</p>
<div class="note admonition">
<p class="admonition-title">Background: Putting It All Together</p>
<p>Real data analysis rarely involves just computing a single statistic. A complete nonparametric analysis uses the ECDF and plug-in principle to characterize the entire distribution, construct confidence bands, estimate multiple parameters, and assess where the methods perform well or poorly. This exercise walks through such an analysis.</p>
</div>
<p><strong>Scenario</strong>: You have <span class="math notranslate nohighlight">\(n = 250\)</span> observations of wait times (in minutes) at a service center. Your goals are: characterize the wait time distribution, estimate key parameters (mean, median, 90th percentile), construct confidence intervals, and assess whether a proposed benchmark (90% of customers served within 15 minutes) is met.</p>
<ol class="loweralpha simple">
<li><p>Conduct exploratory analysis: plot the ECDF, overlay DKW confidence bands, and assess the shape of the distribution (symmetric? heavy-tailed? bimodal?).</p></li>
<li><p>Compute plug-in estimates for: mean, standard deviation, median, IQR, 90th percentile, and <span class="math notranslate nohighlight">\(P(X &gt; 15)\)</span>. For each, compute a 95% confidence interval using the appropriate method (normal approximation for mean, DKW for CDF-based quantities).</p></li>
<li><p>Test whether the benchmark “<span class="math notranslate nohighlight">\(P(X \leq 15) \geq 0.90\)</span>” is met. Use the DKW band to construct a one-sided confidence bound for <span class="math notranslate nohighlight">\(F(15)\)</span>. State your conclusion at the 5% significance level.</p></li>
<li><p>Compare your nonparametric analysis to a parametric approach: fit an exponential distribution by MLE and repeat parts (b)-(c). When might the parametric approach be preferred despite being more restrictive?</p></li>
<li><p>Summarize your findings in 3-4 sentences suitable for a non-technical manager.</p></li>
</ol>
<div class="tip admonition">
<p class="admonition-title">Hint</p>
<p>For the wait time data, use a realistic simulation such as a mixture of service types (quick transactions vs. complex issues). The exponential is often used for wait times but may not capture the full picture. For part (c), the one-sided DKW bound gives <span class="math notranslate nohighlight">\(F(15) \geq \hat{F}_n(15) - \sqrt{\ln(1/\alpha)/(2n)}\)</span> with coverage <span class="math notranslate nohighlight">\(1-\alpha\)</span>.</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Simulate realistic wait time data</span>
<span class="c1"># Mixture: 70% quick service (Gamma(2, 2)), 30% complex (Gamma(4, 4))</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">2024</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">250</span>

<span class="n">component</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">wait_times</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
    <span class="n">component</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">rng</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span>      <span class="c1"># Quick: mean=4, sd=2.8</span>
    <span class="n">rng</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>       <span class="c1"># Complex: mean=16, sd=8</span>
<span class="p">)</span>

<span class="c1"># ========== Part (a): Exploratory Analysis ==========</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;PART (a): EXPLORATORY ANALYSIS&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

<span class="n">x_sorted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">wait_times</span><span class="p">)</span>
<span class="n">ecdf_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span>

<span class="c1"># DKW bands</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">eps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n</span><span class="p">))</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># ECDF with bands</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x_sorted</span><span class="p">,</span> <span class="n">ecdf_vals</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ECDF&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_sorted</span><span class="p">,</span>
               <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">ecdf_vals</span> <span class="o">-</span> <span class="n">eps</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
               <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">ecdf_vals</span> <span class="o">+</span> <span class="n">eps</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
               <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;95% DKW band&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Benchmark (15 min)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Wait Time (minutes)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Cumulative Probability&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;ECDF of Wait Times with 95% DKW Band&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Histogram</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">wait_times</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">wait_times</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Mean = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">wait_times</span><span class="p">)</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">wait_times</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Median = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">wait_times</span><span class="p">)</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Wait Time (minutes)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Density&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Distribution of Wait Times&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;wait_time_analysis.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample size: n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Range: [</span><span class="si">{</span><span class="n">wait_times</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">wait_times</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">] minutes&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shape assessment: Right-skewed (mean &gt; median), possibly bimodal&quot;</span><span class="p">)</span>

<span class="c1"># ========== Part (b): Plug-in Estimates with CIs ==========</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;PART (b): PLUG-IN ESTIMATES WITH CONFIDENCE INTERVALS&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

<span class="c1"># Mean (normal approx CI)</span>
<span class="n">mean_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">wait_times</span><span class="p">)</span>
<span class="n">se_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">wait_times</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">ci_mean</span> <span class="o">=</span> <span class="p">(</span><span class="n">mean_hat</span> <span class="o">-</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">se_mean</span><span class="p">,</span> <span class="n">mean_hat</span> <span class="o">+</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">se_mean</span><span class="p">)</span>

<span class="c1"># Standard deviation (approximate CI via sqrt(n))</span>
<span class="n">sd_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">wait_times</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Median (DKW-based CI)</span>
<span class="n">median_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">wait_times</span><span class="p">)</span>
<span class="c1"># For median CI, invert the DKW band at F(x) = 0.5</span>
<span class="n">idx_lower</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">searchsorted</span><span class="p">(</span><span class="n">ecdf_vals</span> <span class="o">+</span> <span class="n">eps</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
<span class="n">idx_upper</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">searchsorted</span><span class="p">(</span><span class="n">ecdf_vals</span> <span class="o">-</span> <span class="n">eps</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">ci_median</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_sorted</span><span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx_lower</span><span class="p">)],</span> <span class="n">x_sorted</span><span class="p">[</span><span class="nb">min</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">idx_upper</span><span class="p">)])</span>

<span class="c1"># IQR</span>
<span class="n">q25</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">wait_times</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
<span class="n">q75</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">wait_times</span><span class="p">,</span> <span class="mi">75</span><span class="p">)</span>
<span class="n">iqr_hat</span> <span class="o">=</span> <span class="n">q75</span> <span class="o">-</span> <span class="n">q25</span>

<span class="c1"># 90th percentile</span>
<span class="n">q90_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">wait_times</span><span class="p">,</span> <span class="mi">90</span><span class="p">)</span>

<span class="c1"># P(X &gt; 15)</span>
<span class="n">p_exceed_15</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">wait_times</span> <span class="o">&gt;</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">se_p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">p_exceed_15</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_exceed_15</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span>
<span class="n">ci_p_exceed</span> <span class="o">=</span> <span class="p">(</span><span class="n">p_exceed_15</span> <span class="o">-</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">se_p</span><span class="p">,</span> <span class="n">p_exceed_15</span> <span class="o">+</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">se_p</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Statistic&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Estimate&#39;</span><span class="si">:</span><span class="s2">&lt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;95% CI&#39;</span><span class="si">:</span><span class="s2">&lt;25</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">57</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Mean&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">mean_hat</span><span class="si">:</span><span class="s2">&lt;12.2f</span><span class="si">}</span><span class="s2"> [</span><span class="si">{</span><span class="n">ci_mean</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_mean</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Std Dev&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">sd_hat</span><span class="si">:</span><span class="s2">&lt;12.2f</span><span class="si">}</span><span class="s2"> --&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Median&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">median_hat</span><span class="si">:</span><span class="s2">&lt;12.2f</span><span class="si">}</span><span class="s2"> [</span><span class="si">{</span><span class="n">ci_median</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_median</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;IQR&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">iqr_hat</span><span class="si">:</span><span class="s2">&lt;12.2f</span><span class="si">}</span><span class="s2"> --&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;90th Percentile&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">q90_hat</span><span class="si">:</span><span class="s2">&lt;12.2f</span><span class="si">}</span><span class="s2"> --&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;P(X &gt; 15 min)&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">p_exceed_15</span><span class="si">:</span><span class="s2">&lt;12.2%</span><span class="si">}</span><span class="s2"> [</span><span class="si">{</span><span class="n">ci_p_exceed</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_p_exceed</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>

<span class="c1"># ========== Part (c): Benchmark Test ==========</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;PART (c): BENCHMARK TEST&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

<span class="c1"># One-sided DKW: P(F(15) &lt; F_hat(15) - eps_one) &lt;= alpha</span>
<span class="n">alpha_one</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">eps_one</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">alpha_one</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n</span><span class="p">))</span>

<span class="n">F_hat_15</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">wait_times</span> <span class="o">&lt;=</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">lower_bound_F15</span> <span class="o">=</span> <span class="n">F_hat_15</span> <span class="o">-</span> <span class="n">eps_one</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Benchmark: P(Wait ≤ 15 min) ≥ 90%&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Point estimate: P(X ≤ 15) = </span><span class="si">{</span><span class="n">F_hat_15</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% one-sided lower bound: </span><span class="si">{</span><span class="n">lower_bound_F15</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Conclusion at α = 0.05:&quot;</span><span class="p">)</span>
<span class="k">if</span> <span class="n">lower_bound_F15</span> <span class="o">&gt;=</span> <span class="mf">0.90</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  ✓ Benchmark MET: Lower bound </span><span class="si">{</span><span class="n">lower_bound_F15</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2"> ≥ 90%&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  ✗ Benchmark NOT MET: Lower bound </span><span class="si">{</span><span class="n">lower_bound_F15</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2"> &lt; 90%&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;    Cannot conclude P(X ≤ 15) ≥ 90% at 95% confidence&quot;</span><span class="p">)</span>

<span class="c1"># ========== Part (d): Parametric Comparison ==========</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;PART (d): PARAMETRIC (EXPONENTIAL) COMPARISON&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

<span class="c1"># Fit exponential: MLE is lambda_hat = 1/mean</span>
<span class="n">lambda_hat</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">mean_hat</span>

<span class="c1"># Exponential predictions</span>
<span class="n">mean_exp</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">lambda_hat</span>
<span class="n">median_exp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">lambda_hat</span>
<span class="n">q90_exp</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">expon</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.90</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">lambda_hat</span><span class="p">)</span>
<span class="n">p_exceed_15_exp</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">expon</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">lambda_hat</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitted exponential rate: λ = </span><span class="si">{</span><span class="n">lambda_hat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;Statistic&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Nonparametric&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Exponential&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Mean&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">mean_hat</span><span class="si">:</span><span class="s2">&lt;15.2f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">mean_exp</span><span class="si">:</span><span class="s2">&lt;15.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Median&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">median_hat</span><span class="si">:</span><span class="s2">&lt;15.2f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">median_exp</span><span class="si">:</span><span class="s2">&lt;15.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;90th Percentile&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">q90_hat</span><span class="si">:</span><span class="s2">&lt;15.2f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">q90_exp</span><span class="si">:</span><span class="s2">&lt;15.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;P(X &gt; 15)&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">p_exceed_15</span><span class="si">:</span><span class="s2">&lt;15.1%</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">p_exceed_15_exp</span><span class="si">:</span><span class="s2">&lt;15.1%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Benchmark under exponential</span>
<span class="n">F_15_exp</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">expon</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">lambda_hat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Exponential P(X ≤ 15) = </span><span class="si">{</span><span class="n">F_15_exp</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- When to prefer parametric? ---&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;• When the model is correct: tighter CIs (uses all data efficiently)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;• Small samples: more stable estimates&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;• Extrapolation: can estimate extreme quantiles beyond data range&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;• BUT: exponential assumes memoryless property &amp; constant hazard&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;       Real wait times often have different shapes (bimodal here)&quot;</span><span class="p">)</span>

<span class="c1"># ========== Part (e): Executive Summary ==========</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;PART (e): EXECUTIVE SUMMARY&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

<span class="n">summary</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">WAIT TIME ANALYSIS SUMMARY (n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2"> customers)</span>

<span class="s2">The average wait time is </span><span class="si">{</span><span class="n">mean_hat</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> minutes, but the distribution is</span>
<span class="s2">right-skewed: half of customers are served within </span><span class="si">{</span><span class="n">median_hat</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> minutes</span>
<span class="s2">(the median), while </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">p_exceed_15</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">% wait longer than 15 minutes.</span>

<span class="s2">The benchmark that 90% of customers should be served within 15 minutes</span>
<span class="si">{</span><span class="s2">&quot;IS&quot;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">lower_bound_F15</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="mf">0.90</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s2">&quot;is NOT&quot;</span><span class="si">}</span><span class="s2"> currently being met.</span>
<span class="s2">Our best estimate is that </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">F_hat_15</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">% are served within 15 minutes,</span>
<span class="s2">with a 95% confidence lower bound of </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">lower_bound_F15</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">%.</span>

<span class="s2">Recommendation: </span><span class="si">{</span><span class="s2">&quot;Current service levels meet the benchmark. Continue monitoring.&quot;</span>
<span class="k">if</span><span class="w"> </span><span class="n">lower_bound_F15</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="mf">0.90</span><span class="w"> </span><span class="k">else</span>
<span class="s2">&quot;Service improvements are needed to meet the 90% benchmark. Focus on reducing long waits for complex service requests.&quot;</span><span class="si">}</span>
<span class="s2">&quot;&quot;&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">summary</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Nonparametric methods reveal true distribution shape</strong>: The ECDF shows clear evidence of right-skewness and possible bimodality (mixture of service types), which the exponential model cannot capture.</p></li>
<li><p class="sd-card-text"><strong>Parametric vs. nonparametric tradeoff</strong>: The exponential model underestimates the 90th percentile (14.0 vs. 18.5 minutes) because it cannot capture the heavy right tail from complex service requests.</p></li>
<li><p class="sd-card-text"><strong>Confidence intervals vary by method</strong>: Mean uses normal approximation (efficient), CDF-based quantities use DKW (conservative but distribution-free), and the choice matters in practice.</p></li>
<li><p class="sd-card-text"><strong>Benchmark testing uses one-sided bounds</strong>: For showing compliance with a threshold, we need the <em>lower</em> bound on the good outcome probability—DKW provides this rigorously.</p></li>
<li><p class="sd-card-text"><strong>Communication matters</strong>: The executive summary translates statistical findings into actionable business language without sacrificing accuracy.</p></li>
</ol>
</div>
</details></div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<p><strong>Foundational Works on Empirical Distributions</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="glivenko1933" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Glivenko1933<span class="fn-bracket">]</span></span>
<p>Glivenko, V. (1933). Sulla determinazione empirica delle leggi di probabilità. <em>Giornale dell’Istituto Italiano degli Attuari</em>, 4, 92–99. First proof of the uniform convergence of the ECDF to the true CDF, establishing the theoretical foundation for nonparametric inference.</p>
</div>
<div class="citation" id="cantelli1933" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Cantelli1933<span class="fn-bracket">]</span></span>
<p>Cantelli, F. P. (1933). Sulla determinazione empirica delle leggi di probabilità. <em>Giornale dell’Istituto Italiano degli Attuari</em>, 4, 421–424. Independent proof of uniform ECDF convergence, leading to the combined Glivenko-Cantelli theorem.</p>
</div>
<div class="citation" id="kolmogorov1933" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Kolmogorov1933<span class="fn-bracket">]</span></span>
<p>Kolmogorov, A. N. (1933). Sulla determinazione empirica di una legge di distribuzione. <em>Giornale dell’Istituto Italiano degli Attuari</em>, 4, 83–91. Established the exact limiting distribution of <span class="math notranslate nohighlight">\(\sqrt{n}D_n\)</span>, providing the foundation for Kolmogorov-Smirnov tests.</p>
</div>
<div class="citation" id="dvoretzkykieferwolfowitz1956" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DvoretzkyKieferWolfowitz1956<span class="fn-bracket">]</span></span>
<p>Dvoretzky, A., Kiefer, J., and Wolfowitz, J. (1956). Asymptotic minimax character of the sample distribution function and of the classical multinomial estimator. <em>Annals of Mathematical Statistics</em>, 27(3), 642–669. Original proof of the DKW inequality establishing finite-sample probability bounds for the supremum deviation.</p>
</div>
<div class="citation" id="massart1990" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Massart1990<span class="fn-bracket">]</span></span>
<p>Massart, P. (1990). The tight constant in the Dvoretzky-Kiefer-Wolfowitz inequality. <em>Annals of Probability</em>, 18(3), 1269–1283. Proved that the constant 2 in the DKW inequality is tight, resolving a longstanding conjecture.</p>
</div>
</div>
<p><strong>The Plug-in Principle and Functional Estimation</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="vonmises1947" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>vonMises1947<span class="fn-bracket">]</span></span>
<p>von Mises, R. (1947). On the asymptotic distribution of differentiable statistical functions. <em>Annals of Mathematical Statistics</em>, 18(3), 309–348. Introduced the functional approach to statistical estimation, viewing parameters as functionals of the distribution function.</p>
</div>
<div class="citation" id="filippova1962" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Filippova1962<span class="fn-bracket">]</span></span>
<p>Filippova, A. A. (1962). Mises’ theorem on the asymptotic behavior of functionals of empirical distribution functions and its statistical applications. <em>Theory of Probability and Its Applications</em>, 7(1), 24–57. Extended von Mises’ theory with applications to plug-in estimation.</p>
</div>
<div class="citation" id="fernholz1983" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Fernholz1983<span class="fn-bracket">]</span></span>
<p>Fernholz, L. T. (1983). <em>Von Mises Calculus for Statistical Functionals</em>. Springer. Comprehensive development of the influence function and functional delta method, essential for understanding plug-in estimator properties.</p>
</div>
<div class="citation" id="serf2009" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Serf2009<span class="fn-bracket">]</span></span>
<p>Serfling, R. J. (2009). <em>Approximation Theorems of Mathematical Statistics</em>. Wiley. Graduate-level treatment of asymptotic theory including extensive coverage of empirical processes and functional estimation.</p>
</div>
</div>
<p><strong>Bootstrap Theory and ECDF</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="efron1979" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Efron1979<span class="fn-bracket">]</span></span>
<p>Efron, B. (1979). Bootstrap methods: Another look at the jackknife. <em>Annals of Statistics</em>, 7(1), 1–26. Landmark paper introducing the bootstrap, fundamentally based on resampling from the ECDF.</p>
</div>
<div class="citation" id="efrontibshirani1993" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>EfronTibshirani1993<span class="fn-bracket">]</span></span>
<p>Efron, B., and Tibshirani, R. J. (1993). <em>An Introduction to the Bootstrap</em>. Chapman and Hall/CRC. The definitive introduction to bootstrap methods, explaining the connection between the ECDF and bootstrap resampling.</p>
</div>
<div class="citation" id="davisonhinkley1997" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DavisonHinkley1997<span class="fn-bracket">]</span></span>
<p>Davison, A. C., and Hinkley, D. V. (1997). <em>Bootstrap Methods and Their Application</em>. Cambridge University Press. Comprehensive treatment of bootstrap theory and practice with extensive discussion of the plug-in principle.</p>
</div>
<div class="citation" id="vandervaartwellner1996" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>vanderVaartWellner1996<span class="fn-bracket">]</span></span>
<p>van der Vaart, A. W., and Wellner, J. A. (1996). <em>Weak Convergence and Empirical Processes</em>. Springer. Rigorous mathematical treatment of empirical process theory underlying bootstrap consistency.</p>
</div>
</div>
<p><strong>Empirical Process Theory</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="shorack1986" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Shorack1986<span class="fn-bracket">]</span></span>
<p>Shorack, G. R., and Wellner, J. A. (1986). <em>Empirical Processes with Applications to Statistics</em>. Wiley. Authoritative reference on empirical process theory with detailed proofs of Glivenko-Cantelli and DKW results.</p>
</div>
<div class="citation" id="pollard1984" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Pollard1984<span class="fn-bracket">]</span></span>
<p>Pollard, D. (1984). <em>Convergence of Stochastic Processes</em>. Springer. Accessible introduction to empirical process theory with applications to statistics.</p>
</div>
<div class="citation" id="dudley2014" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Dudley2014<span class="fn-bracket">]</span></span>
<p>Dudley, R. M. (2014). <em>Uniform Central Limit Theorems</em> (2nd ed.). Cambridge University Press. Advanced treatment of uniform convergence results for empirical processes.</p>
</div>
</div>
<p><strong>Confidence Bands and Distribution-Free Inference</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="feller1948" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Feller1948<span class="fn-bracket">]</span></span>
<p>Feller, W. (1948). On the Kolmogorov-Smirnov limit theorems for empirical distributions. <em>Annals of Mathematical Statistics</em>, 19(2), 177–189. Detailed analysis of the Kolmogorov distribution and its application to confidence bands.</p>
</div>
<div class="citation" id="birnbaumtingey1951" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>BirnbaumTingey1951<span class="fn-bracket">]</span></span>
<p>Birnbaum, Z. W., and Tingey, F. H. (1951). One-sided confidence contours for probability distribution functions. <em>Annals of Mathematical Statistics</em>, 22(4), 592–596. Development of one-sided confidence bands for CDFs.</p>
</div>
<div class="citation" id="owen1995" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Owen1995<span class="fn-bracket">]</span></span>
<p>Owen, A. B. (1995). Nonparametric likelihood confidence bands for a distribution function. <em>Journal of the American Statistical Association</em>, 90(430), 516–521. Empirical likelihood approach to confidence bands, often tighter than DKW.</p>
</div>
</div>
<p><strong>Quantile Estimation</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="mosteller1946" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Mosteller1946<span class="fn-bracket">]</span></span>
<p>Mosteller, F. (1946). On some useful “inefficient” statistics. <em>Annals of Mathematical Statistics</em>, 17(4), 377–408. Early systematic treatment of order statistics and quantile estimation.</p>
</div>
<div class="citation" id="hyndmanfan1996" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HyndmanFan1996<span class="fn-bracket">]</span></span>
<p>Hyndman, R. J., and Fan, Y. (1996). Sample quantiles in statistical packages. <em>American Statistician</em>, 50(4), 361–365. Comprehensive comparison of different sample quantile definitions used in statistical software.</p>
</div>
<div class="citation" id="dasgupta2008" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DasGupta2008<span class="fn-bracket">]</span></span>
<p>DasGupta, A. (2008). <em>Asymptotic Theory of Statistics and Probability</em>. Springer. Modern treatment including detailed coverage of quantile estimation and its asymptotics.</p>
</div>
</div>
<p><strong>Survival Analysis and Censoring</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="kaplanmeier1958" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>KaplanMeier1958<span class="fn-bracket">]</span></span>
<p>Kaplan, E. L., and Meier, P. (1958). Nonparametric estimation from incomplete observations. <em>Journal of the American Statistical Association</em>, 53(282), 457–481. Introduced the product-limit estimator for survival functions with censored data, the appropriate analog of the ECDF when plug-in fails due to censoring.</p>
</div>
<div class="citation" id="kleinmoeschberger2003" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>KleinMoeschberger2003<span class="fn-bracket">]</span></span>
<p>Klein, J. P., and Moeschberger, M. L. (2003). <em>Survival Analysis: Techniques for Censored and Truncated Data</em> (2nd ed.). Springer. Comprehensive treatment of survival analysis methods when naive plug-in fails.</p>
</div>
</div>
<p><strong>Modern Computational References</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="wasserman2006" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Wasserman2006<span class="fn-bracket">]</span></span>
<p>Wasserman, L. (2006). <em>All of Nonparametric Statistics</em>. Springer. Modern graduate-level treatment of nonparametric statistics with computational perspective, including detailed coverage of ECDFs and bootstrap.</p>
</div>
<div class="citation" id="efronhastie2016" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>EfronHastie2016<span class="fn-bracket">]</span></span>
<p>Efron, B., and Hastie, T. (2016). <em>Computer Age Statistical Inference: Algorithms, Evidence, and Data Science</em>. Cambridge University Press. Contemporary perspective on statistical inference including the role of ECDFs in modern data science.</p>
</div>
<div class="citation" id="ricebook2007" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>RiceBook2007<span class="fn-bracket">]</span></span>
<p>Rice, J. A. (2007). <em>Mathematical Statistics and Data Analysis</em> (3rd ed.). Duxbury Press. Accessible introduction to mathematical statistics with clear treatment of order statistics and empirical distributions.</p>
</div>
</div>
<p><strong>Historical and Pedagogical</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="stigler1986" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Stigler1986<span class="fn-bracket">]</span></span>
<p>Stigler, S. M. (1986). <em>The History of Statistics: The Measurement of Uncertainty before 1900</em>. Harvard University Press. Historical context for the development of empirical distribution methods.</p>
</div>
<div class="citation" id="lecam1986" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LeCam1986<span class="fn-bracket">]</span></span>
<p>Le Cam, L. (1986). The central limit theorem around 1935. <em>Statistical Science</em>, 1(1), 78–91. Historical perspective on the development of asymptotic theory underlying ECDF results.</p>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="ch4_1-sampling-distribution-problem.html" class="btn btn-neutral float-left" title="The Sampling Distribution Problem" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ch4_3-nonparametric-bootstrap.html" class="btn btn-neutral float-right" title="The Nonparametric Bootstrap" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>