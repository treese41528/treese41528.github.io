

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>The Nonparametric Bootstrap &mdash; STAT 418</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=b32ea9d2" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT418/Website/part2_simulation/chapter4/ch4_3-nonparametric-bootstrap.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=30646c52"></script>
      <script>let toggleHintShow = 'Show solution';</script>
      <script>let toggleHintHide = 'Hide solution';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>window.MathJax = {"options": {"enableMenu": true, "menuOptions": {"settings": {"enrich": true, "speech": true, "braille": true, "collapsible": true, "assistiveMml": false}}}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
      <script src="../../_static/custom.js?v=8718e0ab"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Section 4.4: The Parametric Bootstrap" href="ch4_4-parametric-bootstrap.html" />
    <link rel="prev" title="The Empirical Distribution and Plug-in Principle" href="ch4_2-empirical-distribution-plugin.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Course Content</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../part1_foundations/index.html">Part I: Foundations of Probability and Computation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part1_foundations/chapter1/index.html">Chapter 1: Statistical Paradigms and Core Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html">Paradigms of Probability and Statistical Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#the-mathematical-foundation-kolmogorov-s-axioms">The Mathematical Foundation: Kolmogorov’s Axioms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#interpretations-of-probability">Interpretations of Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#statistical-inference-paradigms">Statistical Inference Paradigms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#historical-and-philosophical-debates">Historical and Philosophical Debates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#looking-ahead-our-course-focus">Looking Ahead: Our Course Focus</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html">Probability Distributions: Theory and Computation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#from-abstract-foundations-to-concrete-tools">From Abstract Foundations to Concrete Tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#the-python-ecosystem-for-probability">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#introduction-why-probability-distributions-matter">Introduction: Why Probability Distributions Matter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#id1">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#continuous-distributions">Continuous Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#additional-important-distributions">Additional Important Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#summary-and-practical-guidelines">Summary and Practical Guidelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#conclusion">Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html">Python Random Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#from-mathematical-distributions-to-computational-samples">From Mathematical Distributions to Computational Samples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-python-ecosystem-at-a-glance">The Python Ecosystem at a Glance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#understanding-pseudo-random-number-generation">Understanding Pseudo-Random Number Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-standard-library-random-module">The Standard Library: <code class="docutils literal notranslate"><span class="pre">random</span></code> Module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#numpy-fast-vectorized-random-sampling">NumPy: Fast Vectorized Random Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#scipy-stats-the-complete-statistical-toolkit">SciPy Stats: The Complete Statistical Toolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#bringing-it-all-together-library-selection-guide">Bringing It All Together: Library Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#looking-ahead-from-random-numbers-to-monte-carlo-methods">Looking Ahead: From Random Numbers to Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html">Chapter 1 Summary: Foundations in Place</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#the-three-pillars-of-chapter-1">The Three Pillars of Chapter 1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#what-lies-ahead-the-road-to-simulation">What Lies Ahead: The Road to Simulation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#chapter-1-exercises-synthesis-problems">Chapter 1 Exercises: Synthesis Problems</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Part II: Simulation-Based Methods</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../chapter2/index.html">Chapter 2: Monte Carlo Simulation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html">Monte Carlo Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#the-historical-development-of-monte-carlo-methods">The Historical Development of Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#the-core-principle-expectation-as-integration">The Core Principle: Expectation as Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#theoretical-foundations">Theoretical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#variance-estimation-and-confidence-intervals">Variance Estimation and Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#comparison-with-deterministic-methods">Comparison with Deterministic Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#sample-size-determination">Sample Size Determination</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#convergence-diagnostics-and-monitoring">Convergence Diagnostics and Monitoring</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#chapter-2-1-exercises-monte-carlo-fundamentals-mastery">Chapter 2.1 Exercises: Monte Carlo Fundamentals Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html">Uniform Random Variates</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#why-uniform-the-universal-currency-of-randomness">Why Uniform? The Universal Currency of Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#the-paradox-of-computational-randomness">The Paradox of Computational Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#chaotic-dynamical-systems-an-instructive-failure">Chaotic Dynamical Systems: An Instructive Failure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#linear-congruential-generators">Linear Congruential Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#shift-register-generators">Shift-Register Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#the-kiss-generator-combining-strategies">The KISS Generator: Combining Strategies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#modern-generators-mersenne-twister-and-pcg">Modern Generators: Mersenne Twister and PCG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#statistical-testing-of-random-number-generators">Statistical Testing of Random Number Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#chapter-2-2-exercises-uniform-random-variates-mastery">Chapter 2.2 Exercises: Uniform Random Variates Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html">Inverse CDF Method</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#mathematical-foundations">Mathematical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#continuous-distributions-with-closed-form-inverses">Continuous Distributions with Closed-Form Inverses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#numerical-inversion">Numerical Inversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#mixed-distributions">Mixed Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#chapter-2-3-exercises-inverse-cdf-method-mastery">Chapter 2.3 Exercises: Inverse CDF Method Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html">Transformation Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#why-transformation-methods">Why Transformation Methods?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-boxmuller-transform">The Box–Muller Transform</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-polar-marsaglia-method">The Polar (Marsaglia) Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#method-comparison-boxmuller-vs-polar-vs-ziggurat">Method Comparison: Box–Muller vs Polar vs Ziggurat</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-ziggurat-algorithm">The Ziggurat Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-clt-approximation-historical">The CLT Approximation (Historical)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#distributions-derived-from-the-normal">Distributions Derived from the Normal</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#multivariate-normal-generation">Multivariate Normal Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#implementation-guidance">Implementation Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#chapter-2-4-exercises-transformation-methods-mastery">Chapter 2.4 Exercises: Transformation Methods Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html">Rejection Sampling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-dartboard-intuition">The Dartboard Intuition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-accept-reject-algorithm">The Accept-Reject Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#efficiency-analysis">Efficiency Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#choosing-the-proposal-distribution">Choosing the Proposal Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-squeeze-principle">The Squeeze Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#geometric-example-sampling-from-the-unit-disk">Geometric Example: Sampling from the Unit Disk</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#limitations-and-the-curse-of-dimensionality">Limitations and the Curse of Dimensionality</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#connections-to-other-methods">Connections to Other Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#chapter-2-5-exercises-rejection-sampling-mastery">Chapter 2.5 Exercises: Rejection Sampling Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html">Variance Reduction Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#the-variance-reduction-paradigm">The Variance Reduction Paradigm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#importance-sampling">Importance Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#control-variates">Control Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#antithetic-variates">Antithetic Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#stratified-sampling">Stratified Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#common-random-numbers">Common Random Numbers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#conditional-monte-carlo-raoblackwellization">Conditional Monte Carlo (Rao–Blackwellization)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#combining-variance-reduction-techniques">Combining Variance Reduction Techniques</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#chapter-2-6-exercises-variance-reduction-mastery">Chapter 2.6 Exercises: Variance Reduction Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html">Chapter Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#the-complete-monte-carlo-workflow">The Complete Monte Carlo Workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#quick-reference-tables">Quick Reference Tables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#common-pitfalls-checklist">Common Pitfalls Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#connections-to-later-chapters">Connections to Later Chapters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#learning-outcomes-checklist">Learning Outcomes Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#further-reading-optimization-and-missing-data">Further Reading: Optimization and Missing Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter3/index.html">Chapter 3: Parametric Inference and Likelihood Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html">Exponential Families</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#historical-origins-from-scattered-results-to-unified-theory">Historical Origins: From Scattered Results to Unified Theory</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#the-canonical-exponential-family">The Canonical Exponential Family</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#converting-familiar-distributions">Converting Familiar Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#the-log-partition-function-a-moment-generating-machine">The Log-Partition Function: A Moment-Generating Machine</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#sufficiency-capturing-all-parameter-information">Sufficiency: Capturing All Parameter Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#minimal-sufficiency-and-completeness">Minimal Sufficiency and Completeness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#conjugate-priors-and-bayesian-inference">Conjugate Priors and Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#exponential-dispersion-models-and-glms">Exponential Dispersion Models and GLMs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#chapter-3-1-exercises-exponential-families-mastery">Chapter 3.1 Exercises: Exponential Families Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html">Maximum Likelihood Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-likelihood-function">The Likelihood Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-score-function">The Score Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#fisher-information">Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#closed-form-maximum-likelihood-estimators">Closed-Form Maximum Likelihood Estimators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#numerical-optimization-for-mle">Numerical Optimization for MLE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#asymptotic-properties-of-mles">Asymptotic Properties of MLEs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-cramer-rao-lower-bound">The Cramér-Rao Lower Bound</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-invariance-property">The Invariance Property</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#likelihood-based-hypothesis-testing">Likelihood-Based Hypothesis Testing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#confidence-intervals-from-likelihood">Confidence Intervals from Likelihood</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#connection-to-bayesian-inference">Connection to Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#chapter-3-2-exercises-maximum-likelihood-estimation-mastery">Chapter 3.2 Exercises: Maximum Likelihood Estimation Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html">Sampling Variability and Variance Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#statistical-estimators-and-their-properties">Statistical Estimators and Their Properties</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#sampling-distributions">Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#the-delta-method">The Delta Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#variance-estimation-methods">Variance Estimation Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#applications-and-worked-examples">Applications and Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html">Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#matrix-calculus-foundations">Matrix Calculus Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#the-linear-model">The Linear Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#ordinary-least-squares-the-calculus-approach">Ordinary Least Squares: The Calculus Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#ordinary-least-squares-the-geometric-approach">Ordinary Least Squares: The Geometric Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#properties-of-the-ols-estimator">Properties of the OLS Estimator</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#the-gauss-markov-theorem">The Gauss-Markov Theorem</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#estimating-the-error-variance">Estimating the Error Variance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#distributional-results-under-normality">Distributional Results Under Normality</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#diagnostics-and-model-checking">Diagnostics and Model Checking</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#numerical-stability-qr-decomposition">Numerical Stability: QR Decomposition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#model-selection-and-information-criteria">Model Selection and Information Criteria</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#regularization-ridge-and-lasso">Regularization: Ridge and LASSO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#chapter-3-4-exercises-linear-models-mastery">Chapter 3.4 Exercises: Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html">Generalized Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#historical-context-unification-of-regression-methods">Historical Context: Unification of Regression Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#the-glm-framework-three-components">The GLM Framework: Three Components</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#score-equations-and-fisher-information">Score Equations and Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#iteratively-reweighted-least-squares">Iteratively Reweighted Least Squares</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#logistic-regression-binary-outcomes">Logistic Regression: Binary Outcomes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#poisson-regression-count-data">Poisson Regression: Count Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#gamma-regression-positive-continuous-data">Gamma Regression: Positive Continuous Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#inference-in-glms-the-testing-triad">Inference in GLMs: The Testing Triad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#model-diagnostics">Model Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#model-comparison-and-selection">Model Comparison and Selection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#quasi-likelihood-and-robust-inference">Quasi-Likelihood and Robust Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#further-reading">Further Reading</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#chapter-3-5-exercises-generalized-linear-models-mastery">Chapter 3.5 Exercises: Generalized Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html">Chapter Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#the-parametric-inference-pipeline">The Parametric Inference Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#the-five-pillars-of-chapter-3">The Five Pillars of Chapter 3</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#quick-reference-core-formulas">Quick Reference: Core Formulas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#connections-to-future-material">Connections to Future Material</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#practical-guidance">Practical Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Chapter 4: Resampling Methods</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html">The Sampling Distribution Problem</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#the-fundamental-target-sampling-distributions">The Fundamental Target: Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#historical-development-the-quest-for-sampling-distributions">Historical Development: The Quest for Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#three-routes-to-the-sampling-distribution">Three Routes to the Sampling Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#when-asymptotics-fail-motivating-the-bootstrap">When Asymptotics Fail: Motivating the Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#the-plug-in-principle-theoretical-foundation">The Plug-In Principle: Theoretical Foundation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#computational-perspective-bootstrap-as-monte-carlo">Computational Perspective: Bootstrap as Monte Carlo</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#chapter-4-1-exercises">Chapter 4.1 Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_1-sampling-distribution-problem.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html">The Empirical Distribution and Plug-in Principle</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#the-empirical-cumulative-distribution-function">The Empirical Cumulative Distribution Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#convergence-of-the-empirical-cdf">Convergence of the Empirical CDF</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#parameters-as-statistical-functionals">Parameters as Statistical Functionals</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#when-the-plug-in-principle-fails">When the Plug-in Principle Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#the-bootstrap-idea-in-one-sentence">The Bootstrap Idea in One Sentence</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#computational-implementation">Computational Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#section-4-2-exercises-ecdf-and-plug-in-mastery">Section 4.2 Exercises: ECDF and Plug-in Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">The Nonparametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#the-bootstrap-principle">The Bootstrap Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bootstrap-standard-errors">Bootstrap Standard Errors</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bootstrap-bias-estimation">Bootstrap Bias Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bootstrap-confidence-intervals">Bootstrap Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bootstrap-for-regression">Bootstrap for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bootstrap-diagnostics">Bootstrap Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="#when-bootstrap-fails">When Bootstrap Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch4_4-parametric-bootstrap.html">Section 4.4: The Parametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#the-parametric-bootstrap-principle">The Parametric Bootstrap Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#location-scale-families">Location-Scale Families</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#parametric-bootstrap-for-regression">Parametric Bootstrap for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#confidence-intervals">Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#model-checking-and-validation">Model Checking and Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#when-parametric-bootstrap-fails">When Parametric Bootstrap Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#parametric-vs-nonparametric-a-decision-framework">Parametric vs. Nonparametric: A Decision Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch4_4-parametric-bootstrap.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part3_bayesian/index.html">Part III: Bayesian Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part3_bayesian/index.html#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html">Bayesian Philosophy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Part II: Simulation-Based Methods</a></li>
          <li class="breadcrumb-item"><a href="index.html">Chapter 4: Resampling Methods</a></li>
      <li class="breadcrumb-item active">The Nonparametric Bootstrap</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/part2_simulation/chapter4/ch4_3-nonparametric-bootstrap.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="the-nonparametric-bootstrap">
<span id="ch4-3-nonparametric-bootstrap"></span><h1>The Nonparametric Bootstrap<a class="headerlink" href="#the-nonparametric-bootstrap" title="Link to this heading"></a></h1>
<p>In <a class="reference internal" href="ch4_2-empirical-distribution-plugin.html#ch4-2-empirical-distribution-plugin"><span class="std std-ref">Section 4.2</span></a>, we established that the empirical distribution <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> converges uniformly to the true distribution <span class="math notranslate nohighlight">\(F\)</span> (Glivenko-Cantelli), and we introduced the plug-in principle: approximate parameters <span class="math notranslate nohighlight">\(\theta = T(F)\)</span> with <span class="math notranslate nohighlight">\(\hat{\theta} = T(\hat{F}_n)\)</span>. We also previewed the distribution-level plug-in: approximate the sampling distribution <span class="math notranslate nohighlight">\(G_F\)</span> with <span class="math notranslate nohighlight">\(G_{\hat{F}_n}\)</span>. This section develops that idea into a complete methodology—the <strong>nonparametric bootstrap</strong>—one of the most versatile tools in modern statistical practice.</p>
<p>The bootstrap, introduced by Bradley Efron in 1979, transforms a conceptually simple insight into a computationally powerful framework. If we knew <span class="math notranslate nohighlight">\(F\)</span>, we could simulate many datasets and study how our statistic varies. Since we don’t know <span class="math notranslate nohighlight">\(F\)</span>, we substitute <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> and simulate from it instead. Remarkably, this works: under mild conditions, the bootstrap distribution approximates the true sampling distribution, giving us standard errors, bias estimates, and confidence intervals without requiring closed-form formulas or distributional assumptions.</p>
<div class="important admonition">
<p class="admonition-title">Road Map 🧭</p>
<ul class="simple">
<li><p><strong>Understand</strong>: The bootstrap principle as Monte Carlo approximation of plug-in at the distribution level</p></li>
<li><p><strong>Develop</strong>: Computational skills for bootstrap standard errors, bias estimation, and confidence intervals</p></li>
<li><p><strong>Implement</strong>: Bootstrap algorithms for iid data and three regression resampling schemes</p></li>
<li><p><strong>Evaluate</strong>: Diagnostics for bootstrap distributions and recognition of failure modes</p></li>
</ul>
</div>
<div class="tip admonition">
<p class="admonition-title">Interactive Resource 🖥️</p>
<p>Explore the bootstrap algorithm interactively with our <a class="reference external" href="https://treese41528.github.io/ComputationalDataScience/Simulations/Bootstrap/bootstrap_sampling_simulator.html">Bootstrap Sampling Simulator</a>. This tool lets you:</p>
<ul class="simple">
<li><p>Visualize resampling with replacement from your data</p></li>
<li><p>Watch the bootstrap distribution build in real-time</p></li>
<li><p>Experiment with different sample sizes and statistics</p></li>
<li><p>See how confidence intervals are constructed</p></li>
</ul>
</div>
<section id="the-bootstrap-principle">
<h2>The Bootstrap Principle<a class="headerlink" href="#the-bootstrap-principle" title="Link to this heading"></a></h2>
<p>The fundamental insight of the bootstrap is that sampling from <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> mimics sampling from <span class="math notranslate nohighlight">\(F\)</span>. Since <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> places mass <span class="math notranslate nohighlight">\(1/n\)</span> on each observed value <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span>, sampling from <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> is equivalent to <strong>sampling with replacement</strong> from the original data.</p>
<section id="conceptual-framework">
<h3>Conceptual Framework<a class="headerlink" href="#conceptual-framework" title="Link to this heading"></a></h3>
<p>Let <span class="math notranslate nohighlight">\(\hat{\theta} = T(X_1, \ldots, X_n)\)</span> be a statistic computed from iid data <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \sim F\)</span>. We seek to understand the sampling distribution <span class="math notranslate nohighlight">\(G_F\)</span> of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>—in particular, its standard deviation (the standard error), its bias, and its quantiles (for confidence intervals).</p>
<p>The bootstrap replaces the unknown <span class="math notranslate nohighlight">\(F\)</span> with the empirical distribution <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>:</p>
<ol class="arabic simple">
<li><p><strong>Generate bootstrap samples</strong>: Draw <span class="math notranslate nohighlight">\(X_1^*, \ldots, X_n^*\)</span> independently from <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> (equivalently, sample with replacement from <span class="math notranslate nohighlight">\(\{X_1, \ldots, X_n\}\)</span>).</p></li>
<li><p><strong>Compute bootstrap statistics</strong>: Calculate <span class="math notranslate nohighlight">\(\hat{\theta}^* = T(X_1^*, \ldots, X_n^*)\)</span> on each bootstrap sample.</p></li>
<li><p><strong>Approximate the sampling distribution</strong>: The empirical distribution of <span class="math notranslate nohighlight">\(\{\hat{\theta}^*_1, \ldots, \hat{\theta}^*_B\}\)</span> over <span class="math notranslate nohighlight">\(B\)</span> bootstrap replicates approximates <span class="math notranslate nohighlight">\(G_{\hat{F}_n}\)</span>, which in turn approximates <span class="math notranslate nohighlight">\(G_F\)</span>.</p></li>
</ol>
<div class="tip admonition">
<p class="admonition-title">Notation Convention</p>
<p>We use <span class="math notranslate nohighlight">\(\hat{\theta}^*_b\)</span> to denote the <span class="math notranslate nohighlight">\(b\)</span>-th bootstrap replicate (in order of computation). When sorted values are needed (e.g., for quantiles), we write <span class="math notranslate nohighlight">\(\hat{\theta}^*_{(k)}\)</span> for the <span class="math notranslate nohighlight">\(k\)</span>-th order statistic of the bootstrap distribution.</p>
</div>
<div class="note admonition">
<p class="admonition-title">Why Does This Work?</p>
<p>The bootstrap succeeds because of two key facts:</p>
<ol class="arabic simple">
<li><p><strong>Glivenko-Cantelli</strong>: <span class="math notranslate nohighlight">\(\hat{F}_n \xrightarrow{a.s.} F\)</span> uniformly, so for smooth functionals, <span class="math notranslate nohighlight">\(T(\hat{F}_n) \xrightarrow{a.s.} T(F)\)</span>.</p></li>
<li><p><strong>Functional continuity</strong>: Under functional delta method conditions (specifically, Hadamard differentiability of <span class="math notranslate nohighlight">\(T\)</span> at <span class="math notranslate nohighlight">\(F\)</span>), the conditional law of <span class="math notranslate nohighlight">\(\sqrt{n}(T(\hat{F}_n^*) - T(\hat{F}_n))\)</span> given the data consistently estimates the law of <span class="math notranslate nohighlight">\(\sqrt{n}(T(\hat{F}_n) - T(F))\)</span>.</p></li>
</ol>
<p>The bootstrap distribution <span class="math notranslate nohighlight">\(G_{\hat{F}_n}\)</span> is itself random (it depends on the observed sample), but consistency means <span class="math notranslate nohighlight">\(G_{\hat{F}_n} \xrightarrow{d} G_F\)</span> in an appropriate sense as <span class="math notranslate nohighlight">\(n \to \infty\)</span>.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Bootstrap Targets: What Are We Estimating?</p>
<p>A common source of confusion: the <strong>bootstrap distribution</strong> is not the same as the <strong>sampling distribution</strong>, though it estimates it.</p>
<ul class="simple">
<li><p>The <strong>sampling distribution</strong> <span class="math notranslate nohighlight">\(G_F\)</span> is a fixed (but unknown) distribution determined by the true <span class="math notranslate nohighlight">\(F\)</span>.</p></li>
<li><p>The <strong>bootstrap distribution</strong> <span class="math notranslate nohighlight">\(G_{\hat{F}_n}\)</span> is <em>random</em>—it depends on the observed sample through <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>.</p></li>
<li><p><strong>Bootstrap consistency</strong> means that <span class="math notranslate nohighlight">\(G_{\hat{F}_n}\)</span> converges to <span class="math notranslate nohighlight">\(G_F\)</span> as <span class="math notranslate nohighlight">\(n \to \infty\)</span>, so for large <span class="math notranslate nohighlight">\(n\)</span>, the bootstrap distribution is a good proxy for the sampling distribution.</p></li>
</ul>
<p>In finite samples, the bootstrap distribution reflects both statistical uncertainty (what we want) and sampling variability in <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> (unavoidable but diminishing with <span class="math notranslate nohighlight">\(n\)</span>).</p>
</div>
</section>
<section id="the-bootstrap-algorithm">
<h3>The Bootstrap Algorithm<a class="headerlink" href="#the-bootstrap-algorithm" title="Link to this heading"></a></h3>
<p>We now formalize the core bootstrap algorithm for iid data.</p>
<p><strong>Algorithm: Nonparametric Bootstrap (iid)</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input: Data X₁, ..., Xₙ; statistic T; number of replicates B
Output: Bootstrap distribution {θ̂*₁, ..., θ̂*_B}

1. Compute original estimate: θ̂ = T(X₁, ..., Xₙ)
2. For b = 1, ..., B:
   a. Sample indices I₁, ..., Iₙ uniformly from {1, ..., n} with replacement
   b. Form bootstrap sample: Xⱼ* = X_{Iⱼ} for j = 1, ..., n
   c. Compute: θ̂*_b = T(X₁*, ..., Xₙ*)
3. Return {θ̂*₁, ..., θ̂*_B}
</pre></div>
</div>
<p><strong>Implementation Notes:</strong></p>
<ul class="simple">
<li><p><strong>Index resampling</strong>: Sample integer indices, not data copies—this is faster and uses less memory.</p></li>
<li><p><strong>Vectorization</strong>: Pre-generate an <span class="math notranslate nohighlight">\(n \times B\)</span> index matrix and compute <span class="math notranslate nohighlight">\(T\)</span> column-wise when possible.</p></li>
<li><p><strong>Parallelization</strong>: Bootstrap replicates are embarrassingly parallel; assign independent RNG streams per worker.</p></li>
<li><p><strong>Store only what you need</strong>: Often just <span class="math notranslate nohighlight">\(\hat{\theta}^*\)</span>; avoid saving full resampled datasets.</p></li>
</ul>
<p><strong>Computational Complexity</strong>: Each bootstrap replicate requires <span class="math notranslate nohighlight">\(O(n)\)</span> for resampling plus <span class="math notranslate nohighlight">\(O(C(T))\)</span> for computing the statistic, where <span class="math notranslate nohighlight">\(C(T)\)</span> is the cost of evaluating <span class="math notranslate nohighlight">\(T\)</span>. Total cost is <span class="math notranslate nohighlight">\(O(B \cdot (n + C(T)))\)</span>.</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_3_fig01_bootstrap_algorithm.png"><img alt="Bootstrap algorithm visualization showing original sample, resamples, and bootstrap distribution" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_3_fig01_bootstrap_algorithm.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 143 </span><span class="caption-text"><strong>Figure 4.3.1</strong>: The bootstrap algorithm in action. (a) Original sample with <span class="math notranslate nohighlight">\(n = 7\)</span> observations. (b) Four bootstrap resamples showing how the same index can appear multiple times. (c) The bootstrap distribution of <span class="math notranslate nohighlight">\(\bar{X}^*\)</span> from 5,000 replicates, with SE and CI summaries.</span><a class="headerlink" href="#id3" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="properties-of-bootstrap-samples">
<h3>Properties of Bootstrap Samples<a class="headerlink" href="#properties-of-bootstrap-samples" title="Link to this heading"></a></h3>
<p>Understanding the structure of bootstrap samples provides insight into the method’s behavior.</p>
<p><strong>Multinomial Structure</strong>: The resample counts <span class="math notranslate nohighlight">\((N_1, \ldots, N_n)\)</span>, where <span class="math notranslate nohighlight">\(N_i\)</span> is the number of times observation <span class="math notranslate nohighlight">\(i\)</span> appears in a bootstrap sample, follow</p>
<div class="math notranslate nohighlight">
\[(N_1, \ldots, N_n) \sim \text{Multinomial}\left(n; \frac{1}{n}, \ldots, \frac{1}{n}\right)\]</div>
<p>Each <span class="math notranslate nohighlight">\(N_i\)</span> has <span class="math notranslate nohighlight">\(\mathbb{E}[N_i] = 1\)</span> and <span class="math notranslate nohighlight">\(\text{Var}(N_i) = (n-1)/n\)</span>. For large <span class="math notranslate nohighlight">\(n\)</span>, <span class="math notranslate nohighlight">\(N_i \approx \text{Poisson}(1)\)</span>.</p>
<p><strong>Expected Unique Observations</strong>: The expected number of unique observations in a bootstrap sample is</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[U] = n \cdot \left(1 - \left(1 - \frac{1}{n}\right)^n\right) \to n(1 - e^{-1}) \approx 0.632n\]</div>
<p>Thus, about 63.2% of observations appear at least once, while 36.8% are omitted (and some appear multiple times). This “leaving out” property is exploited later for the .632 estimator in prediction error estimation.</p>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_3_fig02_multinomial_structure.png"><img alt="Multinomial structure of bootstrap sampling showing inclusion counts and unique observations" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_3_fig02_multinomial_structure.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 144 </span><span class="caption-text"><strong>Figure 4.3.2</strong>: Multinomial structure of bootstrap sampling. (a) Each observation appears on average once per resample. (b) Individual inclusion counts follow approximately Poisson(1). (c) About 63.2% of observations are unique in each bootstrap sample.</span><a class="headerlink" href="#id4" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="python-implementation">
<h3>Python Implementation<a class="headerlink" href="#python-implementation" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">bootstrap_distribution</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">statistic</span><span class="p">,</span> <span class="n">B</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate the bootstrap distribution of a statistic.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    data : array_like</span>
<span class="sd">        Original data (1D array for univariate, 2D with observations</span>
<span class="sd">        along first axis for multivariate).</span>
<span class="sd">    statistic : callable</span>
<span class="sd">        Function that computes the statistic from data.</span>
<span class="sd">    B : int</span>
<span class="sd">        Number of bootstrap replicates.</span>
<span class="sd">    seed : int, optional</span>
<span class="sd">        Random seed for reproducibility.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    theta_star : ndarray</span>
<span class="sd">        Bootstrap distribution of the statistic.</span>
<span class="sd">    theta_hat : float or ndarray</span>
<span class="sd">        Original estimate from the data.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>  <span class="c1"># observations along first axis</span>

    <span class="c1"># Compute original estimate (once!)</span>
    <span class="n">theta_hat</span> <span class="o">=</span> <span class="n">statistic</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="c1"># Pre-allocate based on statistic output shape</span>
    <span class="n">theta_hat_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">theta_hat</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">theta_hat_arr</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">theta_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">theta_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">B</span><span class="p">,)</span> <span class="o">+</span> <span class="n">theta_hat_arr</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>  <span class="c1"># sample with replacement</span>
        <span class="n">theta_star</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">statistic</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">theta_star</span><span class="p">,</span> <span class="n">theta_hat</span>
</pre></div>
</div>
</section>
</section>
<section id="bootstrap-standard-errors">
<h2>Bootstrap Standard Errors<a class="headerlink" href="#bootstrap-standard-errors" title="Link to this heading"></a></h2>
<p>The most fundamental bootstrap quantity is the <strong>standard error</strong>—the standard deviation of the sampling distribution. The bootstrap SE is simply the sample standard deviation of the bootstrap replicates.</p>
<section id="definition-and-computation">
<h3>Definition and Computation<a class="headerlink" href="#definition-and-computation" title="Link to this heading"></a></h3>
<p>For a statistic <span class="math notranslate nohighlight">\(\hat{\theta} = T(X_{1:n})\)</span> targeting <span class="math notranslate nohighlight">\(\theta = T(F)\)</span>, the true standard error is <span class="math notranslate nohighlight">\(\text{SE}(\hat{\theta}) = \sqrt{\text{Var}_F(\hat{\theta})}\)</span>. This measures the spread of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> over hypothetical repeated samples from <span class="math notranslate nohighlight">\(F\)</span>.</p>
<p>The <strong>bootstrap standard error</strong> estimates this by:</p>
<div class="math notranslate nohighlight" id="equation-bootstrap-se">
<span class="eqno">(179)<a class="headerlink" href="#equation-bootstrap-se" title="Link to this equation"></a></span>\[\widehat{\text{SE}}_{\text{boot}} = \sqrt{\frac{1}{B-1} \sum_{b=1}^{B} \left(\hat{\theta}^*_b - \bar{\theta}^*\right)^2}\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{\theta}^* = \frac{1}{B}\sum_{b=1}^{B} \hat{\theta}^*_b\)</span> is the mean of bootstrap replicates.</p>
<p>For vector statistics <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\theta}} \in \mathbb{R}^p\)</span>, the bootstrap covariance matrix is:</p>
<div class="math notranslate nohighlight">
\[\widehat{\text{Cov}}_{\text{boot}}(\hat{\boldsymbol{\theta}}) = \frac{1}{B-1} \sum_{b=1}^{B} (\hat{\boldsymbol{\theta}}^*_b - \bar{\boldsymbol{\theta}}^*)(\hat{\boldsymbol{\theta}}^*_b - \bar{\boldsymbol{\theta}}^*)^\top\]</div>
<p>Marginal standard errors are the square roots of diagonal entries.</p>
</section>
<section id="choosing-b-the-number-of-bootstrap-replicates">
<h3>Choosing B: The Number of Bootstrap Replicates<a class="headerlink" href="#choosing-b-the-number-of-bootstrap-replicates" title="Link to this heading"></a></h3>
<p>The choice of <span class="math notranslate nohighlight">\(B\)</span> involves a tradeoff between computational cost and Monte Carlo accuracy. Two sources of uncertainty coexist:</p>
<ol class="arabic simple">
<li><p><strong>Statistical uncertainty</strong>: The inherent variability of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> across samples from <span class="math notranslate nohighlight">\(F\)</span>.</p></li>
<li><p><strong>Monte Carlo error</strong>: Variability from using finite <span class="math notranslate nohighlight">\(B\)</span> instead of the full bootstrap distribution.</p></li>
</ol>
<p>We want Monte Carlo error to be negligible relative to statistical uncertainty.</p>
<p><strong>Monte Carlo error of the bootstrap SE</strong>: When <span class="math notranslate nohighlight">\(\hat{\theta}^*\)</span> is approximately normal with standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>, the Monte Carlo standard deviation of <span class="math notranslate nohighlight">\(\widehat{\text{SE}}_{\text{boot}}\)</span> is approximately:</p>
<div class="math notranslate nohighlight">
\[\text{sd}_{\text{MC}}\left(\widehat{\text{SE}}_{\text{boot}}\right) \approx \frac{\widehat{\text{SE}}_{\text{boot}}}{\sqrt{2(B-1)}}\]</div>
<p><strong>Practical guidance</strong>:</p>
<ul class="simple">
<li><p><strong>SE estimation only</strong>: <span class="math notranslate nohighlight">\(B \approx 1{,}000\)</span> to <span class="math notranslate nohighlight">\(2{,}000\)</span> usually suffices.</p></li>
<li><p><strong>Confidence intervals</strong> (which use tail quantiles): <span class="math notranslate nohighlight">\(B \geq 5{,}000\)</span>, often <span class="math notranslate nohighlight">\(10{,}000\)</span>.</p></li>
<li><p><strong>Stability check</strong>: Increase <span class="math notranslate nohighlight">\(B\)</span> in steps (e.g., <span class="math notranslate nohighlight">\(1{,}000 \to 2{,}000 \to 5{,}000\)</span>) and stop when <span class="math notranslate nohighlight">\(\widehat{\text{SE}}_{\text{boot}}\)</span> changes by less than your tolerance (e.g., <span class="math notranslate nohighlight">\(&lt; 1\%\)</span>).</p></li>
</ul>
<p><strong>Monte Carlo error for CI endpoints</strong>: Tail quantiles are inherently noisier than central summaries like SE. The Monte Carlo error of the <span class="math notranslate nohighlight">\(\alpha/2\)</span> quantile scales as <span class="math notranslate nohighlight">\(O(1/\sqrt{B})\)</span> but also depends on the bootstrap density at that quantile. Endpoints stabilize more slowly than SE—rerunning with different seeds or using repeated bootstraps can assess this variability.</p>
</section>
<section id="comparison-with-analytical-standard-errors">
<h3>Comparison with Analytical Standard Errors<a class="headerlink" href="#comparison-with-analytical-standard-errors" title="Link to this heading"></a></h3>
<p>When analytical formulas exist, bootstrap SE should agree closely—this serves as a sanity check.</p>
<p><strong>Mean</strong>: For <span class="math notranslate nohighlight">\(\hat{\theta} = \bar{X}\)</span>, the analytical SE is <span class="math notranslate nohighlight">\(s/\sqrt{n}\)</span> where <span class="math notranslate nohighlight">\(s\)</span> is the sample standard deviation. Bootstrap SE should match this closely.</p>
<p><strong>Median</strong>: The analytical SE involves the unknown density at the median: <span class="math notranslate nohighlight">\(\text{SE}(\text{med}) \approx 1/(2f(m)\sqrt{n})\)</span>. Bootstrap SE sidesteps this estimation problem and adapts to skewness.</p>
<p><strong>Correlation</strong>: Analytical SE for Pearson correlation <span class="math notranslate nohighlight">\(r\)</span> involves complex expressions. Bootstrap SE adapts to boundary effects; as <span class="math notranslate nohighlight">\(|r| \to 1\)</span>, variability on the <span class="math notranslate nohighlight">\(r\)</span>-scale often <em>shrinks</em>, but the distribution becomes strongly skewed and non-normal, motivating transformations (Fisher <span class="math notranslate nohighlight">\(z\)</span>) or percentile-style intervals.</p>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_3_fig03_bootstrap_vs_analytical_se.png"><img alt="Comparison of bootstrap SE to analytical SE for different statistics" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_3_fig03_bootstrap_vs_analytical_se.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 145 </span><span class="caption-text"><strong>Figure 4.3.3</strong>: Bootstrap SE compared to analytical formulas. (a) For the mean, bootstrap matches <span class="math notranslate nohighlight">\(s/\sqrt{n}\)</span> closely. (b) For the median, bootstrap avoids difficult density estimation. (c) For correlation, bootstrap captures the complex sampling behavior. (d) Summary showing when bootstrap is necessary.</span><a class="headerlink" href="#id5" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="id1">
<h3>Python Implementation<a class="headerlink" href="#id1" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">bootstrap_se</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">statistic</span><span class="p">,</span> <span class="n">B</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute bootstrap standard error for a statistic.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    data : array_like</span>
<span class="sd">        Original data.</span>
<span class="sd">    statistic : callable</span>
<span class="sd">        Function that computes the statistic.</span>
<span class="sd">    B : int</span>
<span class="sd">        Number of bootstrap replicates.</span>
<span class="sd">    seed : int, optional</span>
<span class="sd">        Random seed.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    se_boot : float</span>
<span class="sd">        Bootstrap standard error.</span>
<span class="sd">    theta_hat : float</span>
<span class="sd">        Original estimate.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">theta_star</span><span class="p">,</span> <span class="n">theta_hat</span> <span class="o">=</span> <span class="n">bootstrap_distribution</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">statistic</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>
    <span class="n">se_boot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">theta_star</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">se_boot</span><span class="p">,</span> <span class="n">theta_hat</span>
</pre></div>
</div>
<div class="note admonition">
<p class="admonition-title">Example 💡 Bootstrap SE for the Sample Median</p>
<p><strong>Given</strong>: <span class="math notranslate nohighlight">\(n = 60\)</span> observations from a log-normal distribution (skewed data).</p>
<p><strong>Find</strong>: Bootstrap SE for the sample median.</p>
<p><strong>Python implementation</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Generate skewed data</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">60</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">))</span>  <span class="c1"># log-normal</span>

<span class="c1"># Original median</span>
<span class="n">median_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Bootstrap</span>
<span class="n">B</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">median_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">median_star</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>

<span class="n">se_boot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">median_star</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample median: </span><span class="si">{</span><span class="n">median_hat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bootstrap SE:  </span><span class="si">{</span><span class="n">se_boot</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Result</strong>: The bootstrap provides an SE estimate without requiring density estimation at the median. The histogram of <code class="docutils literal notranslate"><span class="pre">median_star</span></code> typically shows some skewness, reflecting the asymmetry of the underlying distribution.</p>
</div>
</section>
</section>
<section id="bootstrap-bias-estimation">
<h2>Bootstrap Bias Estimation<a class="headerlink" href="#bootstrap-bias-estimation" title="Link to this heading"></a></h2>
<p>Beyond standard errors, the bootstrap can estimate <strong>bias</strong>—the systematic difference between the expected value of an estimator and the true parameter.</p>
<section id="definition-and-estimation">
<h3>Definition and Estimation<a class="headerlink" href="#definition-and-estimation" title="Link to this heading"></a></h3>
<p>For a statistic <span class="math notranslate nohighlight">\(\hat{\theta} = T(X_{1:n})\)</span> targeting <span class="math notranslate nohighlight">\(\theta = T(F)\)</span>, the finite-sample bias under <span class="math notranslate nohighlight">\(F\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\text{Bias}_F(\hat{\theta}) = \mathbb{E}_F[\hat{\theta}] - \theta\]</div>
<p>The bootstrap estimates this by replacing <span class="math notranslate nohighlight">\(F\)</span> with <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-bootstrap-bias">
<span class="eqno">(180)<a class="headerlink" href="#equation-bootstrap-bias" title="Link to this equation"></a></span>\[\widehat{\text{Bias}}_{\text{boot}} = \bar{\theta}^* - \hat{\theta}\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{\theta}^* = \frac{1}{B}\sum_{b=1}^B \hat{\theta}^*_b\)</span> approximates <span class="math notranslate nohighlight">\(\mathbb{E}_{\hat{F}_n}[\hat{\theta}]\)</span>.</p>
</section>
<section id="bias-corrected-estimator">
<h3>Bias-Corrected Estimator<a class="headerlink" href="#bias-corrected-estimator" title="Link to this heading"></a></h3>
<p>If the bootstrap detects substantial bias, we can construct a bias-corrected estimator:</p>
<div class="math notranslate nohighlight" id="equation-bias-corrected">
<span class="eqno">(181)<a class="headerlink" href="#equation-bias-corrected" title="Link to this equation"></a></span>\[\hat{\theta}^{bc} = \hat{\theta} - \widehat{\text{Bias}}_{\text{boot}} = 2\hat{\theta} - \bar{\theta}^*\]</div>
<p>If the bootstrap average <span class="math notranslate nohighlight">\(\bar{\theta}^*\)</span> overshoots <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>, we pull back; if it undershoots, we push up.</p>
</section>
<section id="when-to-apply-bias-correction">
<h3>When to Apply Bias Correction<a class="headerlink" href="#when-to-apply-bias-correction" title="Link to this heading"></a></h3>
<p>Bias correction is not always beneficial—it increases variance. The goal is to reduce <strong>mean squared error</strong> <span class="math notranslate nohighlight">\(\text{MSE} = \text{Bias}^2 + \text{Var}\)</span>.</p>
<p><strong>Rule of thumb</strong>: Compare <span class="math notranslate nohighlight">\(|\widehat{\text{Bias}}_{\text{boot}}|\)</span> to <span class="math notranslate nohighlight">\(\widehat{\text{SE}}_{\text{boot}}\)</span>:</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(|\widehat{\text{Bias}}_{\text{boot}}| &lt; 0.25 \cdot \widehat{\text{SE}}_{\text{boot}}\)</span>: Bias is negligible; correction rarely helps.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(|\widehat{\text{Bias}}_{\text{boot}}| \approx 0.25\text{--}0.5 \cdot \widehat{\text{SE}}_{\text{boot}}\)</span>: Consider reporting both <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> and <span class="math notranslate nohighlight">\(\hat{\theta}^{bc}\)</span>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(|\widehat{\text{Bias}}_{\text{boot}}| &gt; 0.5 \cdot \widehat{\text{SE}}_{\text{boot}}\)</span>: Bias correction may help, but check stability with larger <span class="math notranslate nohighlight">\(B\)</span>.</p></li>
</ul>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ⚠️</p>
<p><strong>Variance inflation from bias correction</strong>: The corrected estimator <span class="math notranslate nohighlight">\(\hat{\theta}^{bc}\)</span> uses <span class="math notranslate nohighlight">\(\bar{\theta}^*\)</span>, which has both Monte Carlo and statistical variability. This can make <span class="math notranslate nohighlight">\(\hat{\theta}^{bc}\)</span> noisier than <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>. Always report <span class="math notranslate nohighlight">\(\widehat{\text{SE}}_{\text{boot}}\)</span> alongside bias estimates, and increase <span class="math notranslate nohighlight">\(B\)</span> if conclusions depend on the correction.</p>
</div>
</section>
<section id="canonical-examples">
<h3>Canonical Examples<a class="headerlink" href="#canonical-examples" title="Link to this heading"></a></h3>
<p><strong>Sample standard deviation</strong>: The sample SD with <span class="math notranslate nohighlight">\(1/(n-1)\)</span> divisor has small negative bias for estimating <span class="math notranslate nohighlight">\(\sigma\)</span>. Bootstrap typically detects this, but correction is rarely needed.</p>
<p><strong>Ratio estimators</strong> <span class="math notranslate nohighlight">\(\hat{\theta} = \bar{X}/\bar{Y}\)</span>: Nonlinearity induces bias, especially when <span class="math notranslate nohighlight">\(\bar{Y}\)</span> is variable. Bootstrap bias can be non-negligible for small <span class="math notranslate nohighlight">\(n\)</span>.</p>
<p><strong>Log-scale back-transforms</strong>: If you estimate a mean on the log scale and exponentiate, Jensen’s inequality creates bias. Bootstrap can quantify (and sometimes correct) this.</p>
</section>
</section>
<section id="bootstrap-confidence-intervals">
<h2>Bootstrap Confidence Intervals<a class="headerlink" href="#bootstrap-confidence-intervals" title="Link to this heading"></a></h2>
<p>While standard errors summarize spread, confidence intervals provide ranges that capture the parameter with specified probability. The bootstrap offers several interval construction methods with different properties.</p>
<section id="percentile-interval">
<h3>Percentile Interval<a class="headerlink" href="#percentile-interval" title="Link to this heading"></a></h3>
<p>The simplest bootstrap interval uses the quantiles of the bootstrap distribution directly.</p>
<p><strong>Definition</strong>: The <span class="math notranslate nohighlight">\((1-\alpha)\)</span> <strong>percentile interval</strong> is:</p>
<div class="math notranslate nohighlight" id="equation-percentile-ci">
<span class="eqno">(182)<a class="headerlink" href="#equation-percentile-ci" title="Link to this equation"></a></span>\[\left[Q_{\alpha/2}(\hat{\theta}^*), Q_{1-\alpha/2}(\hat{\theta}^*)\right]\]</div>
<p>where <span class="math notranslate nohighlight">\(Q_p(\hat{\theta}^*)\)</span> denotes the <span class="math notranslate nohighlight">\(p\)</span>-th quantile of the bootstrap distribution.</p>
<p><strong>Properties</strong>:</p>
<ul class="simple">
<li><p><strong>Simple and automatic</strong>: No variance formula or studentization needed.</p></li>
<li><p><strong>Transformation invariant</strong>: If <span class="math notranslate nohighlight">\(\phi(\cdot)\)</span> is strictly monotone, the <span class="math notranslate nohighlight">\(\phi\)</span>-scale interval maps back exactly to the original scale.</p></li>
<li><p><strong>Respects skewness</strong>: Uses the shape of the bootstrap distribution directly.</p></li>
</ul>
<p><strong>Limitations</strong>:</p>
<ul class="simple">
<li><p>Coverage error can be non-negligible in small samples.</p></li>
<li><p>Not centered at <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>; if the statistic is biased, the interval drifts.</p></li>
<li><p>Can include impossible values near boundaries without truncation.</p></li>
</ul>
</section>
<section id="basic-pivotal-interval">
<h3>Basic (Pivotal) Interval<a class="headerlink" href="#basic-pivotal-interval" title="Link to this heading"></a></h3>
<p>The <strong>basic interval</strong> (also called pivotal) reflects the percentile interval about <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>, partially correcting for bias drift.</p>
<p><strong>Definition</strong>: The <span class="math notranslate nohighlight">\((1-\alpha)\)</span> <strong>basic interval</strong> is:</p>
<div class="math notranslate nohighlight" id="equation-basic-ci">
<span class="eqno">(183)<a class="headerlink" href="#equation-basic-ci" title="Link to this equation"></a></span>\[\left[2\hat{\theta} - Q_{1-\alpha/2}(\hat{\theta}^*), 2\hat{\theta} - Q_{\alpha/2}(\hat{\theta}^*)\right]\]</div>
<p><strong>Derivation</strong>: The bootstrap distribution of <span class="math notranslate nohighlight">\(\hat{\theta}^* - \hat{\theta}\)</span> approximates the distribution of <span class="math notranslate nohighlight">\(\hat{\theta} - \theta\)</span>. Inverting this relationship gives the basic interval.</p>
<p><strong>Properties</strong>:</p>
<ul class="simple">
<li><p>Centered at <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>, which partially corrects bias drift seen in percentile intervals.</p></li>
<li><p>Not transformation invariant.</p></li>
<li><p>Still sensitive to skewness.</p></li>
</ul>
</section>
<section id="normal-approximation-interval">
<h3>Normal Approximation Interval<a class="headerlink" href="#normal-approximation-interval" title="Link to this heading"></a></h3>
<p>When the bootstrap distribution is approximately normal, we can use:</p>
<div class="math notranslate nohighlight">
\[\hat{\theta} \pm z_{1-\alpha/2} \cdot \widehat{\text{SE}}_{\text{boot}}\]</div>
<p>where <span class="math notranslate nohighlight">\(z_{1-\alpha/2}\)</span> is the standard normal quantile.</p>
<p>This is appropriate when the bootstrap histogram looks symmetric and bell-shaped, but ignores any asymmetry in the true sampling distribution.</p>
</section>
<section id="comparison-of-interval-methods">
<h3>Comparison of Interval Methods<a class="headerlink" href="#comparison-of-interval-methods" title="Link to this heading"></a></h3>
<table class="docutils align-default" id="id6">
<caption><span class="caption-number">Table 44 </span><span class="caption-text">Bootstrap Confidence Interval Methods</span><a class="headerlink" href="#id6" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 25.0%" />
<col style="width: 25.0%" />
<col style="width: 30.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Formula</p></th>
<th class="head"><p>Strengths</p></th>
<th class="head"><p>Limitations</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Percentile</p></td>
<td><p><span class="math notranslate nohighlight">\([Q_{\alpha/2}, Q_{1-\alpha/2}]\)</span></p></td>
<td><p>Simple; transformation invariant; captures skewness</p></td>
<td><p>Can drift with bias; boundary issues</p></td>
</tr>
<tr class="row-odd"><td><p>Basic</p></td>
<td><p><span class="math notranslate nohighlight">\([2\hat{\theta} - Q_{1-\alpha/2}, 2\hat{\theta} - Q_{\alpha/2}]\)</span></p></td>
<td><p>Corrects bias drift; simple</p></td>
<td><p>Not transformation invariant; skewness sensitive</p></td>
</tr>
<tr class="row-even"><td><p>Normal</p></td>
<td><p><span class="math notranslate nohighlight">\(\hat{\theta} \pm z \cdot \widehat{\text{SE}}_{\text{boot}}\)</span></p></td>
<td><p>Fast; symmetric</p></td>
<td><p>Ignores skewness; poor for non-normal cases</p></td>
</tr>
</tbody>
</table>
<figure class="align-center" id="id7">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_3_fig04_ci_methods.png"><img alt="Comparison of bootstrap confidence interval methods for symmetric and skewed data" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_3_fig04_ci_methods.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 146 </span><span class="caption-text"><strong>Figure 4.3.4</strong>: Bootstrap CI methods compared. (a) For symmetric data, all three methods (percentile, basic, normal) give similar intervals. (b) For skewed data, the methods diverge—percentile captures asymmetry while normal assumes symmetry. (c-d) Practical recommendations for choosing among methods.</span><a class="headerlink" href="#id7" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="id2">
<h3>Python Implementation<a class="headerlink" href="#id2" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">bootstrap_ci</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">statistic</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">B</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
                 <span class="n">method</span><span class="o">=</span><span class="s1">&#39;percentile&#39;</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute bootstrap confidence interval.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    data : array_like</span>
<span class="sd">        Original data.</span>
<span class="sd">    statistic : callable</span>
<span class="sd">        Function that computes the statistic.</span>
<span class="sd">    alpha : float</span>
<span class="sd">        Significance level (1-alpha confidence).</span>
<span class="sd">    B : int</span>
<span class="sd">        Number of bootstrap replicates.</span>
<span class="sd">    method : str</span>
<span class="sd">        &#39;percentile&#39;, &#39;basic&#39;, or &#39;normal&#39;.</span>
<span class="sd">    seed : int, optional</span>
<span class="sd">        Random seed.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ci : tuple</span>
<span class="sd">        (lower, upper) confidence bounds.</span>
<span class="sd">    theta_hat : float</span>
<span class="sd">        Original estimate.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">theta_star</span><span class="p">,</span> <span class="n">theta_hat</span> <span class="o">=</span> <span class="n">bootstrap_distribution</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">statistic</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;percentile&#39;</span><span class="p">:</span>
        <span class="n">lower</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">theta_star</span><span class="p">,</span> <span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">upper</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">theta_star</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;basic&#39;</span><span class="p">:</span>
        <span class="n">q_lower</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">theta_star</span><span class="p">,</span> <span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">q_upper</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">theta_star</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">lower</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">theta_hat</span> <span class="o">-</span> <span class="n">q_upper</span>
        <span class="n">upper</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">theta_hat</span> <span class="o">-</span> <span class="n">q_lower</span>

    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;normal&#39;</span><span class="p">:</span>
        <span class="n">se_boot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">theta_star</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">lower</span> <span class="o">=</span> <span class="n">theta_hat</span> <span class="o">-</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se_boot</span>
        <span class="n">upper</span> <span class="o">=</span> <span class="n">theta_hat</span> <span class="o">+</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se_boot</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown method: </span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">),</span> <span class="n">theta_hat</span>
</pre></div>
</div>
<div class="note admonition">
<p class="admonition-title">Example 💡 Comparing Interval Methods for the Correlation Coefficient</p>
<p><strong>Given</strong>: <span class="math notranslate nohighlight">\(n = 100\)</span> bivariate observations with true correlation <span class="math notranslate nohighlight">\(\rho = 0.6\)</span>.</p>
<p><strong>Find</strong>: 95% bootstrap CIs using percentile, basic, and normal methods.</p>
<p><strong>Python implementation</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Generate correlated data</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">rho</span> <span class="o">=</span> <span class="mf">0.6</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="n">rho</span><span class="p">],</span> <span class="p">[</span><span class="n">rho</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">xy</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">correlation</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">r_hat</span> <span class="o">=</span> <span class="n">correlation</span><span class="p">(</span><span class="n">xy</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample correlation: </span><span class="si">{</span><span class="n">r_hat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Bootstrap</span>
<span class="n">B</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">r_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">r_star</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">correlation</span><span class="p">(</span><span class="n">xy</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>

<span class="c1"># Percentile CI</span>
<span class="n">ci_perc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">r_star</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.975</span><span class="p">])</span>

<span class="c1"># Basic CI</span>
<span class="n">q_lo</span><span class="p">,</span> <span class="n">q_hi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">r_star</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.975</span><span class="p">])</span>
<span class="n">ci_basic</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">r_hat</span> <span class="o">-</span> <span class="n">q_hi</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">r_hat</span> <span class="o">-</span> <span class="n">q_lo</span><span class="p">)</span>

<span class="c1"># Normal CI</span>
<span class="n">se_boot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">r_star</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.975</span><span class="p">)</span>
<span class="n">ci_normal</span> <span class="o">=</span> <span class="p">(</span><span class="n">r_hat</span> <span class="o">-</span> <span class="n">z</span><span class="o">*</span><span class="n">se_boot</span><span class="p">,</span> <span class="n">r_hat</span> <span class="o">+</span> <span class="n">z</span><span class="o">*</span><span class="n">se_boot</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Percentile CI: [</span><span class="si">{</span><span class="n">ci_perc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_perc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Basic CI:      [</span><span class="si">{</span><span class="n">ci_basic</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_basic</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Normal CI:     [</span><span class="si">{</span><span class="n">ci_normal</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_normal</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Result</strong>: For correlation away from boundaries (<span class="math notranslate nohighlight">\(\pm 1\)</span>), the three methods typically agree closely. Near boundaries, the bootstrap distribution becomes skewed, and percentile/basic intervals may differ from normal intervals.</p>
</div>
</section>
</section>
<section id="bootstrap-for-regression">
<h2>Bootstrap for Regression<a class="headerlink" href="#bootstrap-for-regression" title="Link to this heading"></a></h2>
<p>Regression models require careful attention to <strong>what gets resampled</strong>. The appropriate scheme depends on whether the regressors are treated as fixed (controlled by design) or random (observed from a population), and whether errors are homoscedastic or heteroscedastic.</p>
<p>This section develops three resampling schemes: <strong>residual bootstrap</strong> (fixed-X, homoscedastic), <strong>pairs bootstrap</strong> (random-X or heteroscedastic), and <strong>wild bootstrap</strong> (fixed-X, heteroscedastic). Understanding when to use each connects bootstrap methodology to experimental design principles.</p>
<section id="the-regression-setup">
<h3>The Regression Setup<a class="headerlink" href="#the-regression-setup" title="Link to this heading"></a></h3>
<p>Consider the linear model:</p>
<div class="math notranslate nohighlight">
\[Y_i = \mathbf{x}_i^\top \boldsymbol{\beta} + \varepsilon_i, \quad i = 1, \ldots, n\]</div>
<p>or in matrix form:</p>
<div class="math notranslate nohighlight">
\[\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{n \times p}\)</span> is the design matrix (including intercept), <span class="math notranslate nohighlight">\(\boldsymbol{\beta} \in \mathbb{R}^p\)</span> are coefficients, and <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon}\)</span> are errors.</p>
<p>The OLS estimator is <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{Y}\)</span>.</p>
<p>Under the classical model with <span class="math notranslate nohighlight">\(\mathbb{E}[\boldsymbol{\varepsilon}|\mathbf{X}] = \mathbf{0}\)</span> and <span class="math notranslate nohighlight">\(\text{Var}(\boldsymbol{\varepsilon}|\mathbf{X}) = \sigma^2\mathbf{I}_n\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\hat{\boldsymbol{\beta}}|\mathbf{X}) = \sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}\]</div>
<p>But what if assumptions fail? What if <span class="math notranslate nohighlight">\(n\)</span> is modest? What if the statistic of interest is a complex function of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>? Bootstrap provides a flexible alternative.</p>
</section>
<section id="residual-bootstrap-fixed-x-homoscedastic">
<h3>Residual Bootstrap (Fixed-X, Homoscedastic)<a class="headerlink" href="#residual-bootstrap-fixed-x-homoscedastic" title="Link to this heading"></a></h3>
<p>When the design is <strong>fixed</strong> (as in designed experiments) and errors are <strong>exchangeable</strong> (homoscedastic), we resample residuals while keeping <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> fixed.</p>
<p><strong>Algorithm: Residual Bootstrap</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input: Design matrix X, response Y, number of replicates B
Output: Bootstrap distribution of β̂*

1. Fit OLS: β̂ = (X&#39;X)⁻¹X&#39;Y
2. Compute fitted values: Ŷ = Xβ̂
3. Compute residuals: ê = Y - Ŷ
4. Center residuals: ẽᵢ = êᵢ - mean(ê)  [ensures Σẽᵢ = 0]
   (With an intercept in the model, Σêᵢ = 0 up to numerical error;
    centering is still good practice and matters in models without intercept)
5. For b = 1, ..., B:
   a. Sample ẽ*₁, ..., ẽ*ₙ with replacement from {ẽ₁, ..., ẽₙ}
   b. Create: Y*ᵢ = Ŷᵢ + ẽ*ᵢ for all i
   c. Refit: β̂*_b = (X&#39;X)⁻¹X&#39;Y*
6. Return {β̂*₁, ..., β̂*_B}
</pre></div>
</div>
<p><strong>When to use</strong>: Designed experiments (DOE) where regressors are set by the experimenter. The fixed-X perspective is appropriate when <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is under experimental control, not sampled from a population.</p>
<p><strong>Assumptions</strong>: Errors are iid (or at least exchangeable) and homoscedastic. The residual bootstrap <strong>fails</strong> under heteroscedasticity because resampling mixes residuals from low-variance and high-variance regions.</p>
</section>
<section id="leverage-adjusted-residual-bootstrap">
<h3>Leverage-Adjusted Residual Bootstrap<a class="headerlink" href="#leverage-adjusted-residual-bootstrap" title="Link to this heading"></a></h3>
<p>High-leverage observations have residuals that are systematically smaller in magnitude (they “pull” the fit toward themselves). For better finite-sample behavior, we can adjust for leverage.</p>
<p>The <strong>leverage</strong> of observation <span class="math notranslate nohighlight">\(i\)</span> is <span class="math notranslate nohighlight">\(h_{ii}\)</span>, the <span class="math notranslate nohighlight">\(i\)</span>-th diagonal of the hat matrix <span class="math notranslate nohighlight">\(\mathbf{H} = \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\)</span>.</p>
<p><strong>Algorithm modification</strong>:</p>
<ol class="arabic simple">
<li><p>Compute studentized residuals: <span class="math notranslate nohighlight">\(r_i = \hat{e}_i / \sqrt{1 - h_{ii}}\)</span></p></li>
<li><p>Center: <span class="math notranslate nohighlight">\(\tilde{r}_i = r_i - \bar{r}\)</span></p></li>
<li><p>Resample <span class="math notranslate nohighlight">\(\tilde{r}^*\)</span> and set <span class="math notranslate nohighlight">\(\tilde{e}^*_i = \tilde{r}^*_i \sqrt{1 - h_{ii}}\)</span></p></li>
<li><p>Continue as before with <span class="math notranslate nohighlight">\(Y^*_i = \hat{Y}_i + \tilde{e}^*_i\)</span></p></li>
</ol>
<p>This improves calibration when leverage varies substantially across observations.</p>
</section>
<section id="pairs-bootstrap-random-x">
<h3>Pairs Bootstrap (Random-X)<a class="headerlink" href="#pairs-bootstrap-random-x" title="Link to this heading"></a></h3>
<p>When observations <span class="math notranslate nohighlight">\((\mathbf{x}_i, Y_i)\)</span> are iid draws from a joint distribution—as in observational studies—we resample <strong>entire rows</strong> (cases).</p>
<p><strong>Algorithm: Pairs (Case) Bootstrap</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input: Data matrix (X, Y) with n rows, number of replicates B
Output: Bootstrap distribution of β̂*

1. Fit OLS on original data: β̂
2. For b = 1, ..., B:
   a. Sample indices I₁, ..., Iₙ with replacement from {1, ..., n}
   b. Form bootstrap dataset: (X*, Y*) = rows {I₁, ..., Iₙ}
   c. Refit: β̂*_b = (X*&#39;X*)⁻¹X*&#39;Y*
3. Return {β̂*₁, ..., β̂*_B}
</pre></div>
</div>
<p><strong>When to use</strong>: Observational studies where both <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are measured (not controlled). Also use when heteroscedasticity is present and you want robustness without modeling the variance structure.</p>
<p><strong>Properties</strong>:</p>
<ul class="simple">
<li><p>Automatically respects heteroscedasticity tied to <span class="math notranslate nohighlight">\(X\)</span></p></li>
<li><p>Preserves the joint distribution of <span class="math notranslate nohighlight">\((X, Y)\)</span></p></li>
<li><p>More variable than residual bootstrap when leverage points are rare (some resamples may omit them)</p></li>
<li><p>Robust to model misspecification</p></li>
</ul>
<p><strong>Connection to design</strong>: Pairs resampling treats the entire observation <span class="math notranslate nohighlight">\((\mathbf{x}_i, Y_i)\)</span> as the sampling unit, appropriate when data come from random sampling rather than controlled experimentation.</p>
</section>
<section id="wild-bootstrap-fixed-x-heteroscedastic">
<h3>Wild Bootstrap (Fixed-X, Heteroscedastic)<a class="headerlink" href="#wild-bootstrap-fixed-x-heteroscedastic" title="Link to this heading"></a></h3>
<p>When the design is <strong>fixed</strong> but errors are <strong>heteroscedastic</strong>, the wild bootstrap preserves the <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> structure while respecting location-specific variance.</p>
<p><strong>Algorithm: Wild Bootstrap (Rademacher)</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input: Design matrix X, response Y, number of replicates B
Output: Bootstrap distribution of β̂*

1. Fit OLS: β̂, compute Ŷ and ê
2. For b = 1, ..., B:
   a. Generate w₁, ..., wₙ iid with P(wᵢ = ±1) = 1/2
   b. Create: Y*ᵢ = Ŷᵢ + wᵢ · êᵢ
   c. Refit: β̂*_b = (X&#39;X)⁻¹X&#39;Y*
3. Return {β̂*₁, ..., β̂*_B}
</pre></div>
</div>
<p><strong>Weight choices</strong>: The multipliers <span class="math notranslate nohighlight">\(w_i\)</span> should satisfy <span class="math notranslate nohighlight">\(\mathbb{E}[w_i] = 0\)</span> and <span class="math notranslate nohighlight">\(\mathbb{E}[w_i^2] = 1\)</span>.</p>
<ul class="simple">
<li><p><strong>Rademacher</strong>: <span class="math notranslate nohighlight">\(P(w_i = \pm 1) = 1/2\)</span>. Simple; preserves residual magnitude.</p></li>
<li><p><strong>Mammen</strong>: <span class="math notranslate nohighlight">\(w_i \in \{-(√5-1)/2, (√5+1)/2\}\)</span> with appropriate probabilities. Matches third moment of residuals.</p></li>
</ul>
<p><strong>Why it works</strong>: Multiplying residuals by <span class="math notranslate nohighlight">\(\pm 1\)</span> preserves mean zero and variance <span class="math notranslate nohighlight">\(\hat{e}_i^2\)</span>, allowing heteroscedastic variance patterns to be respected. The fixed <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> structure is maintained exactly.</p>
<div class="tip admonition">
<p class="admonition-title">Leverage Adjustment for Wild Bootstrap</p>
<p>For designs with high leverage points, finite-sample calibration improves when using HC2/HC3-scaled residuals before applying multipliers:</p>
<div class="math notranslate nohighlight">
\[\tilde{e}_i = \frac{\hat{e}_i}{\sqrt{1 - h_{ii}}} \quad \text{(HC2-style)}\]</div>
<p>Then create <span class="math notranslate nohighlight">\(Y^*_i = \hat{Y}_i + w_i \cdot \tilde{e}_i \cdot \sqrt{1 - h_{ii}}\)</span> to map back to the original scale. This adjustment is especially important when leverage values <span class="math notranslate nohighlight">\(h_{ii}\)</span> vary substantially across observations.</p>
</div>
</section>
<section id="comparison-of-regression-bootstrap-schemes">
<h3>Comparison of Regression Bootstrap Schemes<a class="headerlink" href="#comparison-of-regression-bootstrap-schemes" title="Link to this heading"></a></h3>
<table class="docutils align-default" id="id8">
<caption><span class="caption-number">Table 45 </span><span class="caption-text">Regression Bootstrap Scheme Selection</span><a class="headerlink" href="#id8" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 18.0%" />
<col style="width: 18.0%" />
<col style="width: 16.0%" />
<col style="width: 16.0%" />
<col style="width: 16.0%" />
<col style="width: 16.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Aspect</p></th>
<th class="head"><p>Residual</p></th>
<th class="head"><p>Residual + Leverage</p></th>
<th class="head"><p>Pairs</p></th>
<th class="head"><p>Wild</p></th>
<th class="head"><p>Parametric</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>X treatment</p></td>
<td><p>Fixed</p></td>
<td><p>Fixed</p></td>
<td><p>Random</p></td>
<td><p>Fixed</p></td>
<td><p>Fixed</p></td>
</tr>
<tr class="row-odd"><td><p>Homoscedasticity</p></td>
<td><p>Required</p></td>
<td><p>Required</p></td>
<td><p>Not required</p></td>
<td><p>Not required</p></td>
<td><p>Model-dependent</p></td>
</tr>
<tr class="row-even"><td><p>Heteroscedasticity</p></td>
<td><p>✗ Fails</p></td>
<td><p>✗ Fails</p></td>
<td><p>✓ Robust</p></td>
<td><p>✓ Handles</p></td>
<td><p>Model-dependent</p></td>
</tr>
<tr class="row-odd"><td><p>Model misspecification</p></td>
<td><p>✗ Sensitive</p></td>
<td><p>✗ Sensitive</p></td>
<td><p>✓ Robust</p></td>
<td><p>✗ Sensitive</p></td>
<td><p>✗ Sensitive</p></td>
</tr>
<tr class="row-even"><td><p>DOE appropriate</p></td>
<td><p>✓ Yes</p></td>
<td><p>✓ Yes</p></td>
<td><p>✗ No</p></td>
<td><p>✓ Yes</p></td>
<td><p>✓ Yes</p></td>
</tr>
<tr class="row-odd"><td><p>Observational appropriate</p></td>
<td><p>✗ No</p></td>
<td><p>✗ No</p></td>
<td><p>✓ Yes</p></td>
<td><p>Depends</p></td>
<td><p>Depends</p></td>
</tr>
<tr class="row-even"><td><p>Efficiency</p></td>
<td><p>High</p></td>
<td><p>High</p></td>
<td><p>Lower</p></td>
<td><p>Medium</p></td>
<td><p>Highest if correct</p></td>
</tr>
</tbody>
</table>
<figure class="align-center" id="id9">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_3_fig05_regression_bootstrap_schemes.png"><img alt="Comparison of residual, pairs, and wild bootstrap for regression" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_3_fig05_regression_bootstrap_schemes.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 147 </span><span class="caption-text"><strong>Figure 4.3.5</strong>: Bootstrap schemes for regression. (a) Homoscedastic errors: variance is constant. (b) Heteroscedastic errors: variance depends on <span class="math notranslate nohighlight">\(x\)</span>. (c) Scheme selection guide. (d) Under homoscedasticity, residual and pairs give similar results. (e) Under heteroscedasticity, residual bootstrap <strong>fails</strong>—pairs and wild are correct. (f) SE comparison across schemes.</span><a class="headerlink" href="#id9" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="connection-to-experimental-design">
<h3>Connection to Experimental Design<a class="headerlink" href="#connection-to-experimental-design" title="Link to this heading"></a></h3>
<p>The choice between residual and pairs bootstrap connects deeply to the philosophy of experimental design:</p>
<p><strong>Fixed-X (DOE perspective)</strong>: In a designed experiment, the experimenter controls <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. The randomness comes only from <span class="math notranslate nohighlight">\(\varepsilon\)</span>. Residual bootstrap (or wild, for heteroscedasticity) respects this: it fixes <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and resamples only the error distribution.</p>
<p><strong>Random-X (observational study perspective)</strong>: In an observational study, both <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are sampled. The joint distribution <span class="math notranslate nohighlight">\(F_{X,Y}\)</span> is the relevant population. Pairs bootstrap respects this: it resamples complete observations.</p>
<p><strong>Practical guidance</strong>: Match your resampling scheme to the data-generating mechanism:</p>
<ul class="simple">
<li><p>Controlled experiment with treatment levels <span class="math notranslate nohighlight">\(\to\)</span> Residual (or Wild if heteroscedastic)</p></li>
<li><p>Survey or observational sample <span class="math notranslate nohighlight">\(\to\)</span> Pairs</p></li>
<li><p>Unknown or mixed <span class="math notranslate nohighlight">\(\to\)</span> Pairs (more robust, at some efficiency cost)</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Scheme Selection Vignettes</p>
<p><strong>Vignette 1</strong>: A chemist runs a DOE with 5 fixed temperature levels (20°, 40°, 60°, 80°, 100°C), measuring reaction yield at each. Residual plots show variance increasing with temperature.</p>
<p><em>Decision</em>: Fixed X (DOE) + heteroscedastic errors → <strong>Wild bootstrap</strong>.</p>
<p><strong>Vignette 2</strong>: An economist analyzes survey data on income vs. education, where both variables were recorded (not controlled) for a random sample of households.</p>
<p><em>Decision</em>: Random X (observational) → <strong>Pairs bootstrap</strong>.</p>
<p><strong>Vignette 3</strong>: A biologist studies plant growth under controlled light levels (fixed X), and residuals appear homoscedastic with no pattern in variance.</p>
<p><em>Decision</em>: Fixed X (DOE) + homoscedastic errors → <strong>Residual bootstrap</strong>.</p>
<p>The key question is always: “What is the source of randomness in my data-generating process?”</p>
</div>
</section>
<section id="python-implementation-for-regression-bootstrap">
<h3>Python Implementation for Regression Bootstrap<a class="headerlink" href="#python-implementation-for-regression-bootstrap" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">numpy.linalg</span><span class="w"> </span><span class="kn">import</span> <span class="n">lstsq</span>

<span class="k">def</span><span class="w"> </span><span class="nf">ols_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Fit OLS, return coefficients.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">lstsq</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="kc">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">def</span><span class="w"> </span><span class="nf">bootstrap_regression_residual</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">B</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Residual bootstrap for fixed-X homoscedastic regression.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : ndarray, shape (n, p)</span>
<span class="sd">        Design matrix.</span>
<span class="sd">    y : ndarray, shape (n,)</span>
<span class="sd">        Response vector.</span>
<span class="sd">    B : int</span>
<span class="sd">        Number of bootstrap replicates.</span>
<span class="sd">    seed : int, optional</span>
<span class="sd">        Random seed.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    beta_star : ndarray, shape (B, p)</span>
<span class="sd">        Bootstrap distribution of coefficient estimates.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Original fit</span>
    <span class="n">beta_hat</span> <span class="o">=</span> <span class="n">ols_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta_hat</span>
    <span class="n">residuals</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span>

    <span class="c1"># Center residuals</span>
    <span class="n">residuals_centered</span> <span class="o">=</span> <span class="n">residuals</span> <span class="o">-</span> <span class="n">residuals</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="n">beta_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
        <span class="c1"># Resample residuals</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
        <span class="n">e_star</span> <span class="o">=</span> <span class="n">residuals_centered</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">y_star</span> <span class="o">=</span> <span class="n">y_hat</span> <span class="o">+</span> <span class="n">e_star</span>
        <span class="n">beta_star</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">ols_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_star</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">beta_star</span>

<span class="k">def</span><span class="w"> </span><span class="nf">bootstrap_regression_pairs</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">B</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pairs bootstrap for random-X or heteroscedastic regression.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : ndarray, shape (n, p)</span>
<span class="sd">        Design matrix.</span>
<span class="sd">    y : ndarray, shape (n,)</span>
<span class="sd">        Response vector.</span>
<span class="sd">    B : int</span>
<span class="sd">        Number of bootstrap replicates.</span>
<span class="sd">    seed : int, optional</span>
<span class="sd">        Random seed.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    beta_star : ndarray, shape (B, p)</span>
<span class="sd">        Bootstrap distribution of coefficient estimates.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="n">beta_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
        <span class="c1"># Resample rows (cases)</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
        <span class="n">X_star</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">y_star</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">beta_star</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">ols_fit</span><span class="p">(</span><span class="n">X_star</span><span class="p">,</span> <span class="n">y_star</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">beta_star</span>

<span class="k">def</span><span class="w"> </span><span class="nf">bootstrap_regression_wild</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">B</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Wild bootstrap (Rademacher) for fixed-X heteroscedastic regression.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : ndarray, shape (n, p)</span>
<span class="sd">        Design matrix.</span>
<span class="sd">    y : ndarray, shape (n,)</span>
<span class="sd">        Response vector.</span>
<span class="sd">    B : int</span>
<span class="sd">        Number of bootstrap replicates.</span>
<span class="sd">    seed : int, optional</span>
<span class="sd">        Random seed.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    beta_star : ndarray, shape (B, p)</span>
<span class="sd">        Bootstrap distribution of coefficient estimates.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Original fit</span>
    <span class="n">beta_hat</span> <span class="o">=</span> <span class="n">ols_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta_hat</span>
    <span class="n">residuals</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span>

    <span class="n">beta_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
        <span class="c1"># Rademacher weights</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
        <span class="n">e_star</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">residuals</span>
        <span class="n">y_star</span> <span class="o">=</span> <span class="n">y_hat</span> <span class="o">+</span> <span class="n">e_star</span>
        <span class="n">beta_star</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">ols_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_star</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">beta_star</span>
</pre></div>
</div>
<div class="note admonition">
<p class="admonition-title">Example 💡 Comparing Bootstrap Schemes Under Heteroscedasticity</p>
<p><strong>Given</strong>: Regression data with heteroscedastic errors (variance increases with <span class="math notranslate nohighlight">\(|x|\)</span>).</p>
<p><strong>Find</strong>: Compare SE estimates from residual, pairs, and wild bootstrap.</p>
<p><strong>Python implementation</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Generate heteroscedastic data</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">sigma_i</span> <span class="o">=</span> <span class="mf">0.3</span> <span class="o">+</span> <span class="mf">0.7</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Heteroscedastic: variance depends on |x|</span>
<span class="n">eps</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma_i</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">eps</span>

<span class="c1"># Design matrix</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">x</span><span class="p">])</span>

<span class="c1"># Classical OLS SE (assumes homoscedasticity)</span>
<span class="n">beta_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="kc">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta_hat</span>
<span class="n">residuals</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span>
<span class="n">sigma2_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">residuals</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">XtX_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span>
<span class="n">se_classical</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma2_hat</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">XtX_inv</span><span class="p">))</span>

<span class="c1"># Bootstrap SEs</span>
<span class="n">B</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">beta_residual</span> <span class="o">=</span> <span class="n">bootstrap_regression_residual</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">beta_pairs</span> <span class="o">=</span> <span class="n">bootstrap_regression_pairs</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">beta_wild</span> <span class="o">=</span> <span class="n">bootstrap_regression_wild</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;SE estimates for slope coefficient:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Classical (wrong):     </span><span class="si">{</span><span class="n">se_classical</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Residual bootstrap:    </span><span class="si">{</span><span class="n">beta_residual</span><span class="p">[:,</span><span class="w"> </span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Pairs bootstrap:       </span><span class="si">{</span><span class="n">beta_pairs</span><span class="p">[:,</span><span class="w"> </span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Wild bootstrap:        </span><span class="si">{</span><span class="n">beta_wild</span><span class="p">[:,</span><span class="w"> </span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Result</strong>: Under heteroscedasticity, classical SE and residual bootstrap underestimate uncertainty because they assume constant variance. Pairs and wild bootstrap give more accurate (larger) SE estimates that properly reflect the heterogeneous variability.</p>
</div>
</section>
</section>
<section id="bootstrap-diagnostics">
<h2>Bootstrap Diagnostics<a class="headerlink" href="#bootstrap-diagnostics" title="Link to this heading"></a></h2>
<p>The bootstrap produces more than a number—the collection <span class="math notranslate nohighlight">\(\{\hat{\theta}^*_b\}_{b=1}^B\)</span> contains shape, skewness, and stability information that should guide interval choice and identify potential problems.</p>
<section id="core-diagnostic-displays">
<h3>Core Diagnostic Displays<a class="headerlink" href="#core-diagnostic-displays" title="Link to this heading"></a></h3>
<p>Always produce these visualizations when using bootstrap methods:</p>
<ol class="arabic simple">
<li><p><strong>Histogram/density of</strong> <span class="math notranslate nohighlight">\(\hat{\theta}^*\)</span>: Look for symmetry, skewness, heavy tails, multimodality.</p></li>
<li><p><strong>Overlay of</strong> <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> <strong>and CI endpoints</strong>: Visual check that the interval makes sense.</p></li>
<li><p><strong>Normal Q-Q plot of</strong> <span class="math notranslate nohighlight">\(\hat{\theta}^*\)</span>: Rough normality assessment; departures suggest percentile or BCa intervals may be preferable to normal approximation.</p></li>
<li><p><strong>Endpoint stability plot</strong>: Track CI endpoints as <span class="math notranslate nohighlight">\(B\)</span> increases to verify Monte Carlo convergence.</p></li>
</ol>
</section>
<section id="quantitative-shape-summaries">
<h3>Quantitative Shape Summaries<a class="headerlink" href="#quantitative-shape-summaries" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Skewness</strong>: <span class="math notranslate nohighlight">\(\gamma_1 = \mathbb{E}[(\hat{\theta}^* - \bar{\theta}^*)^3]/\text{SD}(\hat{\theta}^*)^3\)</span>. Large <span class="math notranslate nohighlight">\(|\gamma_1|\)</span> suggests percentile intervals may under-cover on the short tail.</p></li>
<li><p><strong>Kurtosis</strong>: Heavy tails inflate this; consider robust statistics or increased <span class="math notranslate nohighlight">\(B\)</span>.</p></li>
<li><p><strong>Bias ratio</strong>: <span class="math notranslate nohighlight">\(|\widehat{\text{Bias}}_{\text{boot}}| / \widehat{\text{SE}}_{\text{boot}}\)</span>. Values above 0.25 warrant attention; above 0.5 suggest BCa or reporting both corrected and uncorrected estimates.</p></li>
</ul>
</section>
<section id="red-flags-and-remedies">
<h3>Red Flags and Remedies<a class="headerlink" href="#red-flags-and-remedies" title="Link to this heading"></a></h3>
<table class="docutils align-default" id="id10">
<caption><span class="caption-number">Table 46 </span><span class="caption-text">Bootstrap Diagnostic Red Flags</span><a class="headerlink" href="#id10" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 35.0%" />
<col style="width: 65.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Diagnostic finding</p></th>
<th class="head"><p>Suggested remedy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Strong skewness in <span class="math notranslate nohighlight">\(\hat{\theta}^*\)</span></p></td>
<td><p>Use BCa or studentized bootstrap; avoid symmetric normal approximation</p></td>
</tr>
<tr class="row-odd"><td><p>Heavy tails or multimodality</p></td>
<td><p>Consider robust statistics; increase <span class="math notranslate nohighlight">\(B\)</span>; report multiple intervals</p></td>
</tr>
<tr class="row-even"><td><p>Many <span class="math notranslate nohighlight">\(\hat{\theta}^*\)</span> equal to <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> (discreteness)</p></td>
<td><p>Use smoothed bootstrap for quantiles; parametric bootstrap if model is credible</p></td>
</tr>
<tr class="row-odd"><td><p>Endpoints pile at bounds (e.g., CI crosses 0 for variances)</p></td>
<td><p>Use transformations (log-scale); parametric tails; truncate with caution</p></td>
</tr>
<tr class="row-even"><td><p>Large <span class="math notranslate nohighlight">\(|\widehat{\text{Bias}}|\)</span> relative to <span class="math notranslate nohighlight">\(\widehat{\text{SE}}\)</span></p></td>
<td><p>Use BCa; report both <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> and <span class="math notranslate nohighlight">\(\hat{\theta}^{bc}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>CI endpoints unstable as <span class="math notranslate nohighlight">\(B\)</span> increases</p></td>
<td><p>Increase <span class="math notranslate nohighlight">\(B\)</span>; if persistent, statistic may be near-boundary or ill-posed</p></td>
</tr>
</tbody>
</table>
</section>
<section id="interval-choice-decision-aid">
<h3>Interval Choice Decision Aid<a class="headerlink" href="#interval-choice-decision-aid" title="Link to this heading"></a></h3>
<p>Based on diagnostics, choose among interval methods:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\hat{\theta}^*\)</span> roughly normal with small bias <span class="math notranslate nohighlight">\(\to\)</span> Normal approximation or basic interval; percentile also fine.</p></li>
<li><p>Skewed <span class="math notranslate nohighlight">\(\hat{\theta}^*\)</span> with modest bias <span class="math notranslate nohighlight">\(\to\)</span> Percentile or BCa (covered in Section 4.7).</p></li>
<li><p>Skewed with noticeable curvature <span class="math notranslate nohighlight">\(\to\)</span> BCa (requires acceleration constant from jackknife).</p></li>
<li><p>Extreme or boundary-driven target (max, outer quantiles) <span class="math notranslate nohighlight">\(\to\)</span> Parametric bootstrap, m-out-of-n, or EVT-based methods.</p></li>
</ol>
<figure class="align-center" id="id11">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_3_fig06_bootstrap_diagnostics.png"><img alt="Bootstrap diagnostic plots including histogram, Q-Q plot, and convergence" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_3_fig06_bootstrap_diagnostics.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 148 </span><span class="caption-text"><strong>Figure 4.3.6</strong>: Essential bootstrap diagnostics. (a) Histogram with CI endpoints marked. (b) Normal Q-Q plot revealing departures from normality. (c) SE convergence as <span class="math notranslate nohighlight">\(B\)</span> increases—use this to verify stability. (d) Bias assessment relative to SE. (e) Comparison across statistics. (f) Summary of red flags to watch for.</span><a class="headerlink" href="#id11" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="when-bootstrap-fails">
<h2>When Bootstrap Fails<a class="headerlink" href="#when-bootstrap-fails" title="Link to this heading"></a></h2>
<p>The nonparametric bootstrap, while remarkably general, has known failure modes. Recognizing these prevents overconfidence in bootstrap results.</p>
<section id="extreme-value-statistics">
<h3>Extreme Value Statistics<a class="headerlink" href="#extreme-value-statistics" title="Link to this heading"></a></h3>
<p>Statistics like the sample maximum, minimum, range, and outer quantiles (1st, 99th percentile) suffer from <strong>support truncation</strong>: the bootstrap cannot generate values beyond the observed sample range.</p>
<p><strong>Why it fails</strong>: The maximum of <span class="math notranslate nohighlight">\(n\)</span> bootstrap observations cannot exceed <span class="math notranslate nohighlight">\(\max(X_1, \ldots, X_n)\)</span>. But the true sampling distribution of the maximum extends beyond observed data.</p>
<p><strong>Remedies</strong>:
- Parametric bootstrap with a fitted tail model (GPD, GEV from extreme value theory)
- m-out-of-n bootstrap (subsample with <span class="math notranslate nohighlight">\(m &lt; n\)</span>)
- Bias-corrected endpoint estimators</p>
</section>
<section id="non-smooth-and-non-regular-statistics">
<h3>Non-smooth and Non-regular Statistics<a class="headerlink" href="#non-smooth-and-non-regular-statistics" title="Link to this heading"></a></h3>
<p>The bootstrap’s behavior depends critically on the regularity of the target functional.</p>
<p><strong>Regular quantiles</strong> (e.g., median, quartiles): When the population density <span class="math notranslate nohighlight">\(f(\xi_p) &gt; 0\)</span> at the <span class="math notranslate nohighlight">\(p\)</span>-th quantile, the bootstrap typically performs well. The variance is <span class="math notranslate nohighlight">\(O(1/n)\)</span> and standard theory applies.</p>
<p><strong>Extreme quantiles and boundary cases</strong>: Outer quantiles (e.g., 1st or 99th percentile) suffer because the bootstrap cannot produce values beyond the observed sample range. As <span class="math notranslate nohighlight">\(p \to 0\)</span> or <span class="math notranslate nohighlight">\(p \to 1\)</span>, bootstrap approximations degrade.</p>
<p><strong>Genuinely non-smooth functionals</strong> (e.g., mode): The sample mode depends discontinuously on the data—small perturbations can cause large jumps. This violates the continuity conditions required for bootstrap consistency.</p>
<p><strong>Why it fails</strong>: Bootstrap consistency typically requires Hadamard differentiability of the functional <span class="math notranslate nohighlight">\(T\)</span> at <span class="math notranslate nohighlight">\(F\)</span>. The mode is the canonical example of a functional that fails this requirement. Quantiles with <span class="math notranslate nohighlight">\(f(\xi_p) = 0\)</span> or near zero also cause problems.</p>
<p><strong>Remedies</strong>:
- For extreme quantiles: parametric bootstrap or EVT-based methods
- For mode: regularize (e.g., use highest-density region) or change the target
- BCa intervals (partially correct for non-smoothness)
- Smoothed bootstrap (add small noise before resampling)
- Increase <span class="math notranslate nohighlight">\(B\)</span> substantially</p>
</section>
<section id="small-sample-sizes">
<h3>Small Sample Sizes<a class="headerlink" href="#small-sample-sizes" title="Link to this heading"></a></h3>
<p>With very small <span class="math notranslate nohighlight">\(n\)</span>, bootstrap resamples don’t contain much new information—the bootstrap distribution is volatile and may poorly approximate the true sampling distribution.</p>
<p><strong>Remedies</strong>:
- Studentized (bootstrap-t) intervals
- Parametric bootstrap if a credible model exists
- Exact methods when available
- Report wide intervals with appropriate caveats</p>
</section>
<section id="dependent-data">
<h3>Dependent Data<a class="headerlink" href="#dependent-data" title="Link to this heading"></a></h3>
<p>The iid bootstrap destroys dependence structure, leading to incorrect standard errors for time series, clustered, or spatial data.</p>
<p><strong>Remedies</strong> (previewed for Section 4.9):
- Block bootstrap (moving or circular blocks)
- Stationary bootstrap (random block lengths)
- Cluster bootstrap (resample clusters, not individuals)
- Wild bootstrap with HAC-appropriate weights</p>
<figure class="align-center" id="id12">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_3_fig07_failure_modes.png"><img alt="Bootstrap failure modes including extreme statistics and small samples" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_3_fig07_failure_modes.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 149 </span><span class="caption-text"><strong>Figure 4.3.7</strong>: When bootstrap fails. (a) For the sample maximum, bootstrap cannot exceed the observed range—it systematically underestimates tail behavior. (b) Comparison of true vs. bootstrap SE for extremes shows significant underestimation. (c) Small sample instability: SE estimates become highly variable as <span class="math notranslate nohighlight">\(n\)</span> decreases. (d) Summary of failure modes and remedies.</span><a class="headerlink" href="#id12" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="practical-considerations">
<h2>Practical Considerations<a class="headerlink" href="#practical-considerations" title="Link to this heading"></a></h2>
<section id="numerical-stability-and-precision">
<h3>Numerical Stability and Precision<a class="headerlink" href="#numerical-stability-and-precision" title="Link to this heading"></a></h3>
<ul>
<li><p><strong>Degenerate resamples</strong>: Some resamples may have identical observations (e.g., all bootstrap observations are the same point), making statistics like correlation or variance undefined.</p>
<p><strong>Explicit policy</strong> (choose one and be consistent):</p>
<ol class="arabic simple">
<li><p><strong>Skip and resample</strong>: If a replicate returns NaN/undefined, discard and draw again until <span class="math notranslate nohighlight">\(B\)</span> valid replicates are obtained.</p></li>
<li><p><strong>Record NaN and drop</strong>: Allow NaN values, then compute summaries (SE, quantiles) excluding them. Report the count of degenerate cases.</p></li>
<li><p><strong>Smoothed bootstrap</strong>: Add tiny jitter before resampling to prevent exact ties (see Section 4.7).</p></li>
</ol>
<p>For most applications, option 1 or 2 is appropriate. Always report if degeneracy occurred and how it was handled.</p>
</li>
<li><p><strong>Floating-point quantiles</strong>: Use consistent quantile interpolation methods (e.g., specify <code class="docutils literal notranslate"><span class="pre">method</span></code> in NumPy, <code class="docutils literal notranslate"><span class="pre">type</span></code> in R).</p></li>
<li><p><strong>Seed management</strong>: Always record seeds for reproducibility. For parallel execution, use independent RNG streams.</p></li>
</ul>
</section>
<section id="computational-efficiency">
<h3>Computational Efficiency<a class="headerlink" href="#computational-efficiency" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Vectorized index generation</strong>: Pre-generate an <span class="math notranslate nohighlight">\(n \times B\)</span> matrix of indices.</p></li>
<li><p><strong>Avoid redundant fitting</strong>: For regression, precompute <span class="math notranslate nohighlight">\((\mathbf{X}^\top\mathbf{X})^{-1}\)</span> when <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is fixed.</p></li>
<li><p><strong>Parallelization</strong>: Bootstrap replicates are independent—distribute across cores with proper seed management.</p></li>
<li><p><strong>Memory</strong>: Store only <span class="math notranslate nohighlight">\(\hat{\theta}^*\)</span>, not full resampled datasets.</p></li>
</ul>
</section>
<section id="reporting-standards">
<h3>Reporting Standards<a class="headerlink" href="#reporting-standards" title="Link to this heading"></a></h3>
<p>A bootstrap analysis should report:</p>
<ol class="arabic simple">
<li><p><strong>The statistic</strong> <span class="math notranslate nohighlight">\(T\)</span> and its interpretation</p></li>
<li><p><strong>The resampling scheme</strong>: iid observations, pairs, residual, wild</p></li>
<li><p><strong>Number of replicates</strong> <span class="math notranslate nohighlight">\(B\)</span> and evidence of stability</p></li>
<li><p><strong>RNG seed</strong> for reproducibility</p></li>
<li><p><strong>Point estimate</strong> <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> with <span class="math notranslate nohighlight">\(\widehat{\text{SE}}_{\text{boot}}\)</span></p></li>
<li><p><strong>Confidence interval</strong> with method (percentile, basic, BCa) and rationale</p></li>
<li><p><strong>Diagnostics</strong>: histogram shape, bias assessment, any red flags</p></li>
</ol>
</section>
</section>
<section id="bringing-it-all-together">
<h2>Bringing It All Together<a class="headerlink" href="#bringing-it-all-together" title="Link to this heading"></a></h2>
<p>The nonparametric bootstrap transforms the plug-in principle into a practical tool for inference. By replacing the unknown <span class="math notranslate nohighlight">\(F\)</span> with <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> and simulating, we obtain standard errors, bias estimates, and confidence intervals for virtually any computable statistic.</p>
<figure class="align-center" id="id13">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_3_fig08_bootstrap_workflow.png"><img alt="Complete bootstrap workflow from data to inference" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter4/ch4_3_fig08_bootstrap_workflow.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 150 </span><span class="caption-text"><strong>Figure 4.3.8</strong>: The complete bootstrap workflow. Starting from data, we compute the statistic, choose an appropriate resampling scheme, run the bootstrap loop, build the bootstrap distribution, check diagnostics, extract summaries (SE, bias, CI), and report results with method details.</span><a class="headerlink" href="#id13" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Summary of methods introduced</strong>:</p>
<ul class="simple">
<li><p><strong>Bootstrap SE</strong>: Sample standard deviation of <span class="math notranslate nohighlight">\(\{\hat{\theta}^*_b\}\)</span></p></li>
<li><p><strong>Bootstrap bias</strong>: <span class="math notranslate nohighlight">\(\bar{\theta}^* - \hat{\theta}\)</span></p></li>
<li><p><strong>Percentile CI</strong>: Quantiles of bootstrap distribution</p></li>
<li><p><strong>Basic CI</strong>: Reflection of percentile about <span class="math notranslate nohighlight">\(\hat{\theta}\)</span></p></li>
<li><p><strong>Residual bootstrap</strong>: Fixed-X, homoscedastic regression</p></li>
<li><p><strong>Pairs bootstrap</strong>: Random-X or robust to heteroscedasticity</p></li>
<li><p><strong>Wild bootstrap</strong>: Fixed-X with heteroscedasticity</p></li>
</ul>
<p><strong>Looking ahead</strong>: Section 4.4 develops the <strong>parametric bootstrap</strong>, which replaces <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> with a fitted parametric model <span class="math notranslate nohighlight">\(\hat{F}_{\hat{\theta}}\)</span>—more efficient when the model is correct, but risky under misspecification. Section 4.5 introduces the <strong>jackknife</strong>, providing a different perspective on resampling that is particularly useful for bias correction and computing the acceleration constant in BCa intervals. Section 4.6 covers <strong>bootstrap hypothesis testing</strong>, including permutation tests. Section 4.7 develops advanced interval methods (BCa, studentized) with improved coverage properties.</p>
<div class="important admonition">
<p class="admonition-title">Key Takeaways 📝</p>
<ol class="arabic simple">
<li><p><strong>Core concept</strong>: The bootstrap approximates the sampling distribution by resampling from <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>—computationally intensive but assumption-light</p></li>
<li><p><strong>Computational insight</strong>: Bootstrap SE is simply <span class="math notranslate nohighlight">\(\text{SD}(\hat{\theta}^*)\)</span> over <span class="math notranslate nohighlight">\(B\)</span> replicates; choose <span class="math notranslate nohighlight">\(B\)</span> by stability (1,000–2,000 for SE, 5,000–10,000 for CIs)</p></li>
<li><p><strong>Practical application</strong>: Match resampling scheme to data structure—residual for designed experiments with homoscedastic errors, pairs for observational studies, wild for heteroscedasticity with fixed design</p></li>
<li><p><strong>Diagnostics are essential</strong>: Always examine the bootstrap distribution; shape guides interval choice and reveals failure modes</p></li>
<li><p><strong>Outcome alignment</strong>: Develops simulation techniques for inference (LO 1), implements resampling for variability assessment and CIs (LO 3)</p></li>
</ol>
</div>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Link to this heading"></a></h2>
<div class="tip admonition">
<p class="admonition-title">Exercise 4.3.1: Bootstrap SE for the Mean</p>
<p><strong>Background</strong>: The sample mean <span class="math notranslate nohighlight">\(\bar{X}\)</span> has analytical SE <span class="math notranslate nohighlight">\(s/\sqrt{n}\)</span>, making it an ideal test case for bootstrap implementation.</p>
<p><strong>Tasks</strong>:</p>
<ol class="loweralpha simple">
<li><p>Generate <span class="math notranslate nohighlight">\(n = 50\)</span> observations from <span class="math notranslate nohighlight">\(N(0, 1)\)</span>. Compute the bootstrap SE for the mean using <span class="math notranslate nohighlight">\(B = 2{,}000\)</span> replicates. Compare to the analytical formula <span class="math notranslate nohighlight">\(s/\sqrt{n}\)</span>.</p></li>
<li><p>Repeat with <span class="math notranslate nohighlight">\(n = 50\)</span> observations from <span class="math notranslate nohighlight">\(\text{Exp}(1)\)</span>. How does the bootstrap SE compare to <span class="math notranslate nohighlight">\(s/\sqrt{n}\)</span>? Plot the histogram of <span class="math notranslate nohighlight">\(\bar{X}^*\)</span> and comment on its shape.</p></li>
<li><p>For the exponential case, how does the shape of the bootstrap distribution change as <span class="math notranslate nohighlight">\(n\)</span> increases to 200? Relate this to the Central Limit Theorem.</p></li>
</ol>
<p><strong>Hint</strong>: Use <code class="docutils literal notranslate"><span class="pre">np.random.default_rng(seed)</span></code> for reproducibility.</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Part (a): Normal data</span>
<span class="n">n</span><span class="p">,</span> <span class="n">B</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">2000</span>
<span class="n">x_norm</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

<span class="n">mean_hat</span> <span class="o">=</span> <span class="n">x_norm</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">se_analytical</span> <span class="o">=</span> <span class="n">x_norm</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="n">mean_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">mean_star</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_norm</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">se_boot_norm</span> <span class="o">=</span> <span class="n">mean_star</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Normal data: Analytical SE = </span><span class="si">{</span><span class="n">se_analytical</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Bootstrap SE = </span><span class="si">{</span><span class="n">se_boot_norm</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># Typical output: very close agreement</span>

<span class="c1"># Part (b): Exponential data</span>
<span class="n">x_exp</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">se_analytical_exp</span> <span class="o">=</span> <span class="n">x_exp</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="n">mean_star_exp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">mean_star_exp</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_exp</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">se_boot_exp</span> <span class="o">=</span> <span class="n">mean_star_exp</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Exponential: Analytical SE = </span><span class="si">{</span><span class="n">se_analytical_exp</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Bootstrap SE = </span><span class="si">{</span><span class="n">se_boot_exp</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Histogram shows right skew for n=50</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">mean_star_exp</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x_exp</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Original mean&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\bar</span><span class="si">{X}</span><span class="s1">^*$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Bootstrap distribution of mean (Exponential, n=50)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Part (c): CLT effect</span>
<span class="n">n_large</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">x_exp_large</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_large</span><span class="p">)</span>
<span class="n">mean_star_large</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x_exp_large</span><span class="p">[</span><span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_large</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_large</span><span class="p">)]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
                             <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">)])</span>
<span class="c1"># Histogram is more symmetric with larger n</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Key insights</strong>:</p>
<ul class="simple">
<li><p class="sd-card-text">For both distributions, bootstrap SE agrees closely with <span class="math notranslate nohighlight">\(s/\sqrt{n}\)</span> because the mean is a smooth, linear functional.</p></li>
<li><p class="sd-card-text">The bootstrap histogram for exponential data shows right skew at <span class="math notranslate nohighlight">\(n=50\)</span>, but becomes more symmetric as <span class="math notranslate nohighlight">\(n\)</span> increases per CLT.</p></li>
<li><p class="sd-card-text">SE measures spread only; skewness affects interval choice but not SE validity.</p></li>
</ul>
</div>
</details><div class="tip admonition">
<p class="admonition-title">Exercise 4.3.2: Bootstrap SE for the Median</p>
<p><strong>Background</strong>: The median’s analytical SE involves the unknown density <span class="math notranslate nohighlight">\(f\)</span> at the median: <span class="math notranslate nohighlight">\(\text{SE}(\text{med}) \approx 1/(2f(m)\sqrt{n})\)</span>. Bootstrap avoids this problem.</p>
<p><strong>Tasks</strong>:</p>
<ol class="loweralpha simple">
<li><p>Generate <span class="math notranslate nohighlight">\(n = 80\)</span> observations from a log-normal distribution with <span class="math notranslate nohighlight">\(\mu = 0, \sigma = 1\)</span>. Compute the bootstrap SE for the median using <span class="math notranslate nohighlight">\(B = 5{,}000\)</span>.</p></li>
<li><p>Compare the bootstrap SEs for the mean, median, and 20% trimmed mean on the same data. Which has the smallest SE? Largest?</p></li>
<li><p>Add two outliers at <span class="math notranslate nohighlight">\(+15\)</span> and <span class="math notranslate nohighlight">\(+20\)</span> to your data. Recompute the three SEs. How do the relative SEs change?</p></li>
<li><p>What does this tell you about choosing between mean, median, and trimmed mean for skewed data with potential outliers?</p></li>
</ol>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">n</span><span class="p">,</span> <span class="n">B</span> <span class="o">=</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">5000</span>

<span class="c1"># Part (a): Log-normal data</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">bootstrap_se</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">statistic</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">rng</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">theta_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
        <span class="n">theta_star</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">statistic</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">theta_star</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">se_median</span> <span class="o">=</span> <span class="n">bootstrap_se</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Median SE: </span><span class="si">{</span><span class="n">se_median</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Part (b): Compare three estimators</span>
<span class="k">def</span><span class="w"> </span><span class="nf">trimmed_mean</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">prop</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">stats</span><span class="o">.</span><span class="n">trim_mean</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">prop</span><span class="p">)</span>

<span class="n">se_mean</span> <span class="o">=</span> <span class="n">bootstrap_se</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
<span class="n">se_trim</span> <span class="o">=</span> <span class="n">bootstrap_se</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">z</span><span class="p">:</span> <span class="n">trimmed_mean</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">),</span> <span class="n">B</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean SE: </span><span class="si">{</span><span class="n">se_mean</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Trimmed mean SE: </span><span class="si">{</span><span class="n">se_trim</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Median SE: </span><span class="si">{</span><span class="n">se_median</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># Typical: Mean &gt; Trimmed &gt; Median for right-skewed data</span>

<span class="c1"># Part (c): Add outliers</span>
<span class="n">x_outliers</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="mf">15.0</span><span class="p">,</span> <span class="mf">20.0</span><span class="p">]])</span>

<span class="n">se_mean_out</span> <span class="o">=</span> <span class="n">bootstrap_se</span><span class="p">(</span><span class="n">x_outliers</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
<span class="n">se_trim_out</span> <span class="o">=</span> <span class="n">bootstrap_se</span><span class="p">(</span><span class="n">x_outliers</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">z</span><span class="p">:</span> <span class="n">trimmed_mean</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">),</span> <span class="n">B</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
<span class="n">se_median_out</span> <span class="o">=</span> <span class="n">bootstrap_se</span><span class="p">(</span><span class="n">x_outliers</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">With outliers:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean SE: </span><span class="si">{</span><span class="n">se_mean_out</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (increased significantly)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Trimmed mean SE: </span><span class="si">{</span><span class="n">se_trim_out</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (moderate increase)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Median SE: </span><span class="si">{</span><span class="n">se_median_out</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (minimal change)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Key insights</strong>:</p>
<ul class="simple">
<li><p class="sd-card-text">For right-skewed data, the mean typically has the largest SE because it’s pulled by tail observations.</p></li>
<li><p class="sd-card-text">Outliers dramatically increase mean SE but barely affect median SE.</p></li>
<li><p class="sd-card-text">Trimmed mean provides a middle ground: robust to outliers while using more data than the median.</p></li>
<li><p class="sd-card-text">Choice of estimator can matter more than choice of inference method.</p></li>
</ul>
</div>
</details><div class="tip admonition">
<p class="admonition-title">Exercise 4.3.3: Bootstrap Bias Estimation</p>
<p><strong>Background</strong>: The sample variance <span class="math notranslate nohighlight">\(s^2 = \frac{1}{n-1}\sum(X_i - \bar{X})^2\)</span> is unbiased for <span class="math notranslate nohighlight">\(\sigma^2\)</span>, but the sample standard deviation <span class="math notranslate nohighlight">\(s\)</span> has a small negative bias for <span class="math notranslate nohighlight">\(\sigma\)</span>.</p>
<p><strong>Tasks</strong>:</p>
<ol class="loweralpha simple">
<li><p>Generate <span class="math notranslate nohighlight">\(n = 30\)</span> observations from <span class="math notranslate nohighlight">\(N(0, 4)\)</span> (so <span class="math notranslate nohighlight">\(\sigma = 2\)</span>). Compute the bootstrap estimate of bias for <span class="math notranslate nohighlight">\(s\)</span> using <span class="math notranslate nohighlight">\(B = 5{,}000\)</span>.</p></li>
<li><p>Compute the bias-corrected estimator <span class="math notranslate nohighlight">\(s^{bc} = 2s - \bar{s}^*\)</span>. Is it closer to the true <span class="math notranslate nohighlight">\(\sigma = 2\)</span>?</p></li>
<li><p>Compare <span class="math notranslate nohighlight">\(|\widehat{\text{Bias}}|\)</span> to <span class="math notranslate nohighlight">\(\widehat{\text{SE}}_{\text{boot}}\)</span>. Based on the rule of thumb, should you apply bias correction?</p></li>
<li><p>Repeat parts (a)-(c) with <span class="math notranslate nohighlight">\(n = 100\)</span>. How does the bias-to-SE ratio change?</p></li>
</ol>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">456</span><span class="p">)</span>
<span class="n">sigma_true</span> <span class="o">=</span> <span class="mf">2.0</span>
<span class="n">B</span> <span class="o">=</span> <span class="mi">5000</span>

<span class="c1"># Part (a) and (b): n = 30</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">30</span>
<span class="c1"># Note: scale parameter is σ (std), not σ² (variance)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma_true</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

<span class="n">s_hat</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">s_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">s_star</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">s_bar_star</span> <span class="o">=</span> <span class="n">s_star</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">se_boot</span> <span class="o">=</span> <span class="n">s_star</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">bias_hat</span> <span class="o">=</span> <span class="n">s_bar_star</span> <span class="o">-</span> <span class="n">s_hat</span>
<span class="n">s_bc</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">s_hat</span> <span class="o">-</span> <span class="n">s_bar_star</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  True σ = </span><span class="si">{</span><span class="n">sigma_true</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Sample s = </span><span class="si">{</span><span class="n">s_hat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Bootstrap mean s̄* = </span><span class="si">{</span><span class="n">s_bar_star</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Estimated bias = </span><span class="si">{</span><span class="n">bias_hat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Bias-corrected s_bc = </span><span class="si">{</span><span class="n">s_bc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Bootstrap SE = </span><span class="si">{</span><span class="n">se_boot</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  |Bias|/SE ratio = </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">bias_hat</span><span class="p">)</span><span class="o">/</span><span class="n">se_boot</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Part (d): n = 100</span>
<span class="n">n_large</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">x_large</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma_true</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_large</span><span class="p">)</span>
<span class="n">s_hat_large</span> <span class="o">=</span> <span class="n">x_large</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">s_star_large</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x_large</span><span class="p">[</span><span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_large</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_large</span><span class="p">)]</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                         <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">)])</span>

<span class="n">bias_hat_large</span> <span class="o">=</span> <span class="n">s_star_large</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">-</span> <span class="n">s_hat_large</span>
<span class="n">se_boot_large</span> <span class="o">=</span> <span class="n">s_star_large</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">n = </span><span class="si">{</span><span class="n">n_large</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  |Bias|/SE ratio = </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">bias_hat_large</span><span class="p">)</span><span class="o">/</span><span class="n">se_boot_large</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Key insights</strong>:</p>
<ul class="simple">
<li><p class="sd-card-text">Sample SD has small negative bias (it tends to underestimate <span class="math notranslate nohighlight">\(\sigma\)</span>).</p></li>
<li><p class="sd-card-text">For <span class="math notranslate nohighlight">\(n = 30\)</span>, bias is typically 5-15% of SE, so correction is borderline useful.</p></li>
<li><p class="sd-card-text">For <span class="math notranslate nohighlight">\(n = 100\)</span>, bias becomes a smaller fraction of SE; correction rarely helps.</p></li>
<li><p class="sd-card-text">Rule of thumb: correct only when <span class="math notranslate nohighlight">\(|\text{Bias}|/\text{SE} &gt; 0.25\)</span>.</p></li>
</ul>
</div>
</details><div class="tip admonition">
<p class="admonition-title">Exercise 4.3.4: Comparing Confidence Interval Methods</p>
<p><strong>Background</strong>: Different bootstrap CI methods have different properties. This exercise explores their behavior through simulation.</p>
<p><strong>Tasks</strong>:</p>
<ol class="loweralpha simple">
<li><p>Generate <span class="math notranslate nohighlight">\(n = 40\)</span> observations from a chi-squared distribution with <span class="math notranslate nohighlight">\(df = 4\)</span> (positively skewed). Compute 95% CIs for the population mean using: (i) percentile, (ii) basic, and (iii) normal approximation methods.</p></li>
<li><p>The true mean is <span class="math notranslate nohighlight">\(\mu = df = 4\)</span>. Which CI contains the true mean? Which is widest? Which is most symmetric around <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>?</p></li>
<li><p>Run a simulation study: repeat the above 500 times and compute the <strong>coverage probability</strong> (fraction of times the CI contains <span class="math notranslate nohighlight">\(\mu = 4\)</span>) for each method. Which performs best?</p></li>
<li><p>Plot the bootstrap distribution from one sample. Based on its shape, which CI method would you recommend?</p></li>
</ol>
<p><strong>Hint</strong>: For coverage studies, generate data with a known true parameter.</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">789</span><span class="p">)</span>
<span class="n">n</span><span class="p">,</span> <span class="n">B</span> <span class="o">=</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">10000</span>
<span class="n">df</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">mu_true</span> <span class="o">=</span> <span class="n">df</span>  <span class="c1"># True mean of chi-squared(df)</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span>

<span class="c1"># Part (a): Single sample CIs</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">chisquare</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">mean_hat</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">mean_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">)])</span>
<span class="n">se_boot</span> <span class="o">=</span> <span class="n">mean_star</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Percentile CI</span>
<span class="n">ci_perc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">mean_star</span><span class="p">,</span> <span class="p">[</span><span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">])</span>

<span class="c1"># Basic CI</span>
<span class="n">q_lo</span><span class="p">,</span> <span class="n">q_hi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">mean_star</span><span class="p">,</span> <span class="p">[</span><span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">])</span>
<span class="n">ci_basic</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">mean_hat</span> <span class="o">-</span> <span class="n">q_hi</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">mean_hat</span> <span class="o">-</span> <span class="n">q_lo</span><span class="p">)</span>

<span class="c1"># Normal CI</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ci_normal</span> <span class="o">=</span> <span class="p">(</span><span class="n">mean_hat</span> <span class="o">-</span> <span class="n">z</span><span class="o">*</span><span class="n">se_boot</span><span class="p">,</span> <span class="n">mean_hat</span> <span class="o">+</span> <span class="n">z</span><span class="o">*</span><span class="n">se_boot</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample mean: </span><span class="si">{</span><span class="n">mean_hat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, True mean: </span><span class="si">{</span><span class="n">mu_true</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Percentile CI: [</span><span class="si">{</span><span class="n">ci_perc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_perc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Basic CI:      [</span><span class="si">{</span><span class="n">ci_basic</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_basic</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Normal CI:     [</span><span class="si">{</span><span class="n">ci_normal</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_normal</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>

<span class="c1"># Part (c): Coverage simulation</span>
<span class="n">n_sim</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">B_sim</span> <span class="o">=</span> <span class="mi">2000</span>  <span class="c1"># Fewer replicates for speed</span>

<span class="n">coverage</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;percentile&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;basic&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;normal&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>

<span class="k">for</span> <span class="n">sim</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
    <span class="n">x_sim</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">chisquare</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">mean_hat_sim</span> <span class="o">=</span> <span class="n">x_sim</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="n">mean_star_sim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x_sim</span><span class="p">[</span><span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
                              <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B_sim</span><span class="p">)])</span>
    <span class="n">se_sim</span> <span class="o">=</span> <span class="n">mean_star_sim</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Percentile</span>
    <span class="n">ci_p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">mean_star_sim</span><span class="p">,</span> <span class="p">[</span><span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">ci_p</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">mu_true</span> <span class="o">&lt;=</span> <span class="n">ci_p</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">coverage</span><span class="p">[</span><span class="s1">&#39;percentile&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># Basic</span>
    <span class="n">q_l</span><span class="p">,</span> <span class="n">q_u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">mean_star_sim</span><span class="p">,</span> <span class="p">[</span><span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">ci_b</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">mean_hat_sim</span> <span class="o">-</span> <span class="n">q_u</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">mean_hat_sim</span> <span class="o">-</span> <span class="n">q_l</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ci_b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">mu_true</span> <span class="o">&lt;=</span> <span class="n">ci_b</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">coverage</span><span class="p">[</span><span class="s1">&#39;basic&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># Normal</span>
    <span class="n">ci_n</span> <span class="o">=</span> <span class="p">(</span><span class="n">mean_hat_sim</span> <span class="o">-</span> <span class="n">z</span><span class="o">*</span><span class="n">se_sim</span><span class="p">,</span> <span class="n">mean_hat_sim</span> <span class="o">+</span> <span class="n">z</span><span class="o">*</span><span class="n">se_sim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ci_n</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">mu_true</span> <span class="o">&lt;=</span> <span class="n">ci_n</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">coverage</span><span class="p">[</span><span class="s1">&#39;normal&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Coverage probabilities (target: 95%):&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">method</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">coverage</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">count</span><span class="o">/</span><span class="n">n_sim</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Key insights</strong>:</p>
<ul class="simple">
<li><p class="sd-card-text">For right-skewed data, the bootstrap distribution is also right-skewed.</p></li>
<li><p class="sd-card-text">Normal CI is symmetric and may under-cover on the right tail.</p></li>
<li><p class="sd-card-text">Percentile CI respects skewness but can have coverage issues.</p></li>
<li><p class="sd-card-text">Basic CI recenters at <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> but doesn’t fix skewness.</p></li>
<li><p class="sd-card-text">With <span class="math notranslate nohighlight">\(n = 40\)</span> and chi-squared(4), all methods typically achieve 92-96% coverage.</p></li>
</ul>
</div>
</details><div class="tip admonition">
<p class="admonition-title">Exercise 4.3.5: Residual vs. Pairs Bootstrap for Regression</p>
<p><strong>Background</strong>: The choice between residual and pairs bootstrap depends on whether X is fixed or random and whether errors are homoscedastic.</p>
<p><strong>Tasks</strong>:</p>
<ol class="loweralpha">
<li><p>Generate regression data with <strong>homoscedastic</strong> errors:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n = 100\)</span>, <span class="math notranslate nohighlight">\(x_i \sim U(-2, 2)\)</span>, <span class="math notranslate nohighlight">\(\varepsilon_i \sim N(0, 0.5^2)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(y_i = 1 + 2x_i + \varepsilon_i\)</span></p></li>
</ul>
<p>Compute 95% CIs for <span class="math notranslate nohighlight">\(\beta_1\)</span> using both residual and pairs bootstrap. How do they compare?</p>
</li>
<li><p>Generate regression data with <strong>heteroscedastic</strong> errors:</p>
<ul class="simple">
<li><p>Same setup, but <span class="math notranslate nohighlight">\(\varepsilon_i \sim N(0, (0.3 + 0.5|x_i|)^2)\)</span></p></li>
</ul>
<p>Compare residual and pairs bootstrap SEs for <span class="math notranslate nohighlight">\(\beta_1\)</span>. Which is larger? Why?</p>
</li>
<li><p>Compare both bootstrap SEs to the classical OLS SE (which assumes homoscedasticity). What happens under heteroscedasticity?</p></li>
<li><p>Based on your results, when should you use residual bootstrap vs. pairs bootstrap?</p></li>
</ol>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">2024</span><span class="p">)</span>
<span class="n">n</span><span class="p">,</span> <span class="n">B</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">5000</span>
<span class="n">beta_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>

<span class="k">def</span><span class="w"> </span><span class="nf">ols_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="kc">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Part (a): Homoscedastic case</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">eps_homo</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">y_homo</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">eps_homo</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">x</span><span class="p">])</span>

<span class="n">beta_hat</span> <span class="o">=</span> <span class="n">ols_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_homo</span><span class="p">)</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta_hat</span>
<span class="n">resid</span> <span class="o">=</span> <span class="n">y_homo</span> <span class="o">-</span> <span class="n">y_hat</span>
<span class="n">resid_centered</span> <span class="o">=</span> <span class="n">resid</span> <span class="o">-</span> <span class="n">resid</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># Residual bootstrap</span>
<span class="n">beta_resid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="n">e_star</span> <span class="o">=</span> <span class="n">resid_centered</span><span class="p">[</span><span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)]</span>
    <span class="n">y_star</span> <span class="o">=</span> <span class="n">y_hat</span> <span class="o">+</span> <span class="n">e_star</span>
    <span class="n">beta_resid</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">ols_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_star</span><span class="p">)</span>

<span class="c1"># Pairs bootstrap</span>
<span class="n">beta_pairs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">y_homo</span><span class="p">])</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Number of predictors (generalizes beyond p=2)</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">X_star</span><span class="p">,</span> <span class="n">y_star</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:</span><span class="n">p</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">p</span><span class="p">]</span>
    <span class="n">beta_pairs</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">ols_fit</span><span class="p">(</span><span class="n">X_star</span><span class="p">,</span> <span class="n">y_star</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;HOMOSCEDASTIC CASE:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Residual SE(β₁): </span><span class="si">{</span><span class="n">beta_resid</span><span class="p">[:,</span><span class="w"> </span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Pairs SE(β₁):    </span><span class="si">{</span><span class="n">beta_pairs</span><span class="p">[:,</span><span class="w"> </span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Part (b): Heteroscedastic case</span>
<span class="n">sigma_i</span> <span class="o">=</span> <span class="mf">0.3</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">eps_hetero</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma_i</span><span class="p">)</span>
<span class="n">y_hetero</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">eps_hetero</span>

<span class="n">beta_hat_h</span> <span class="o">=</span> <span class="n">ols_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_hetero</span><span class="p">)</span>
<span class="n">y_hat_h</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta_hat_h</span>
<span class="n">resid_h</span> <span class="o">=</span> <span class="n">y_hetero</span> <span class="o">-</span> <span class="n">y_hat_h</span>
<span class="n">resid_h_centered</span> <span class="o">=</span> <span class="n">resid_h</span> <span class="o">-</span> <span class="n">resid_h</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># Residual bootstrap (inappropriate here)</span>
<span class="n">beta_resid_h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="n">e_star</span> <span class="o">=</span> <span class="n">resid_h_centered</span><span class="p">[</span><span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)]</span>
    <span class="n">y_star</span> <span class="o">=</span> <span class="n">y_hat_h</span> <span class="o">+</span> <span class="n">e_star</span>
    <span class="n">beta_resid_h</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">ols_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_star</span><span class="p">)</span>

<span class="c1"># Pairs bootstrap (appropriate here)</span>
<span class="n">data_h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">y_hetero</span><span class="p">])</span>
<span class="n">beta_pairs_h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">X_star</span><span class="p">,</span> <span class="n">y_star</span> <span class="o">=</span> <span class="n">data_h</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:</span><span class="n">p</span><span class="p">],</span> <span class="n">data_h</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">p</span><span class="p">]</span>
    <span class="n">beta_pairs_h</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">ols_fit</span><span class="p">(</span><span class="n">X_star</span><span class="p">,</span> <span class="n">y_star</span><span class="p">)</span>

<span class="c1"># Classical OLS SE (wrong under heteroscedasticity)</span>
<span class="n">sigma2_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">resid_h</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">XtX_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span>
<span class="n">se_classical</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma2_hat</span> <span class="o">*</span> <span class="n">XtX_inv</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">HETEROSCEDASTIC CASE:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Residual SE(β₁): </span><span class="si">{</span><span class="n">beta_resid_h</span><span class="p">[:,</span><span class="w"> </span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (WRONG)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Pairs SE(β₁):    </span><span class="si">{</span><span class="n">beta_pairs_h</span><span class="p">[:,</span><span class="w"> </span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (CORRECT)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Classical SE:    </span><span class="si">{</span><span class="n">se_classical</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (WRONG)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Key insights</strong>:</p>
<ul class="simple">
<li><p class="sd-card-text">Under homoscedasticity, both methods give similar SEs; residual is slightly more efficient.</p></li>
<li><p class="sd-card-text">Under heteroscedasticity, residual bootstrap <strong>underestimates</strong> SE because it mixes residuals from different variance regions.</p></li>
<li><p class="sd-card-text">Pairs bootstrap correctly captures the heteroscedastic structure.</p></li>
<li><p class="sd-card-text">Classical OLS SE also underestimates uncertainty under heteroscedasticity.</p></li>
<li><p class="sd-card-text"><strong>Recommendation</strong>: Use pairs when heteroscedasticity is suspected or X is truly random.</p></li>
</ul>
</div>
</details><div class="tip admonition">
<p class="admonition-title">Exercise 4.3.6: Wild Bootstrap for Heteroscedasticity</p>
<p><strong>Background</strong>: The wild bootstrap handles heteroscedasticity while keeping X fixed, making it appropriate for designed experiments with non-constant variance.</p>
<p><strong>Tasks</strong>:</p>
<ol class="loweralpha">
<li><p>Using the heteroscedastic data from Exercise 4.3.5b, implement the wild bootstrap with Rademacher weights (<span class="math notranslate nohighlight">\(P(w = \pm 1) = 0.5\)</span>).</p></li>
<li><p>Compare the wild bootstrap SE for <span class="math notranslate nohighlight">\(\beta_1\)</span> to the residual and pairs bootstrap SEs. Which is closest to the pairs bootstrap?</p></li>
<li><p>Implement the wild bootstrap with Mammen weights:</p>
<div class="math notranslate nohighlight">
\[\begin{split}w = \begin{cases}
-(\sqrt{5} - 1)/2 &amp; \text{with prob } (\sqrt{5} + 1)/(2\sqrt{5}) \\
(\sqrt{5} + 1)/2 &amp; \text{with prob } (\sqrt{5} - 1)/(2\sqrt{5})
\end{cases}\end{split}\]</div>
<p>How does this compare to Rademacher?</p>
</li>
<li><p>In what situations would you prefer wild bootstrap over pairs bootstrap?</p></li>
</ol>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">2025</span><span class="p">)</span>
<span class="n">n</span><span class="p">,</span> <span class="n">B</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">5000</span>

<span class="c1"># Generate heteroscedastic data</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">sigma_i</span> <span class="o">=</span> <span class="mf">0.3</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">eps</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma_i</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">eps</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">x</span><span class="p">])</span>

<span class="k">def</span><span class="w"> </span><span class="nf">ols_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="kc">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">beta_hat</span> <span class="o">=</span> <span class="n">ols_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta_hat</span>
<span class="n">resid</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span>

<span class="c1"># Part (a): Wild bootstrap with Rademacher</span>
<span class="n">beta_wild_rad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">e_star</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">resid</span>
    <span class="n">y_star</span> <span class="o">=</span> <span class="n">y_hat</span> <span class="o">+</span> <span class="n">e_star</span>
    <span class="n">beta_wild_rad</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">ols_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_star</span><span class="p">)</span>

<span class="c1"># Part (c): Wild bootstrap with Mammen</span>
<span class="c1"># Mammen weights: E[w]=0, E[w²]=1, E[w³]=1</span>
<span class="n">p1</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">w1</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
<span class="n">w2</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>

<span class="n">beta_wild_mammen</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">p1</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">)</span>
    <span class="n">e_star</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">resid</span>
    <span class="n">y_star</span> <span class="o">=</span> <span class="n">y_hat</span> <span class="o">+</span> <span class="n">e_star</span>
    <span class="n">beta_wild_mammen</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">ols_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_star</span><span class="p">)</span>

<span class="c1"># Pairs bootstrap for comparison</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">beta_pairs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">X_star</span><span class="p">,</span> <span class="n">y_star</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:</span><span class="n">p</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">p</span><span class="p">]</span>
    <span class="n">beta_pairs</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">ols_fit</span><span class="p">(</span><span class="n">X_star</span><span class="p">,</span> <span class="n">y_star</span><span class="p">)</span>

<span class="c1"># Residual bootstrap (wrong)</span>
<span class="n">resid_centered</span> <span class="o">=</span> <span class="n">resid</span> <span class="o">-</span> <span class="n">resid</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">beta_resid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="n">e_star</span> <span class="o">=</span> <span class="n">resid_centered</span><span class="p">[</span><span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)]</span>
    <span class="n">y_star</span> <span class="o">=</span> <span class="n">y_hat</span> <span class="o">+</span> <span class="n">e_star</span>
    <span class="n">beta_resid</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">ols_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_star</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;SE(β₁) by method:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Residual (wrong):  </span><span class="si">{</span><span class="n">beta_resid</span><span class="p">[:,</span><span class="w"> </span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Pairs:             </span><span class="si">{</span><span class="n">beta_pairs</span><span class="p">[:,</span><span class="w"> </span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Wild (Rademacher): </span><span class="si">{</span><span class="n">beta_wild_rad</span><span class="p">[:,</span><span class="w"> </span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Wild (Mammen):     </span><span class="si">{</span><span class="n">beta_wild_mammen</span><span class="p">[:,</span><span class="w"> </span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Key insights</strong>:</p>
<ul class="simple">
<li><p class="sd-card-text">Wild bootstrap SEs are close to pairs bootstrap, both correctly handling heteroscedasticity.</p></li>
<li><p class="sd-card-text">Mammen weights can better preserve skewness in the error distribution (matches third moment).</p></li>
<li><p class="sd-card-text">Wild bootstrap keeps X fixed (like residual) but respects variance structure (like pairs).</p></li>
<li><p class="sd-card-text"><strong>Use wild when</strong>: X is fixed by design (DOE) but errors are heteroscedastic.</p></li>
<li><p class="sd-card-text"><strong>Use pairs when</strong>: X is random (observational study) or model may be misspecified.</p></li>
</ul>
</div>
</details><div class="tip admonition">
<p class="admonition-title">Exercise 4.3.7: Bootstrap Diagnostics</p>
<p><strong>Background</strong>: Examining the bootstrap distribution guides interval choice and reveals potential problems.</p>
<p><strong>Tasks</strong>:</p>
<ol class="loweralpha simple">
<li><p>Generate <span class="math notranslate nohighlight">\(n = 50\)</span> observations from a mixture: 90% from <span class="math notranslate nohighlight">\(N(0, 1)\)</span> and 10% from <span class="math notranslate nohighlight">\(N(5, 0.5^2)\)</span>. Compute the bootstrap distribution of the mean.</p></li>
<li><p>Create diagnostic plots: (i) histogram of <span class="math notranslate nohighlight">\(\bar{X}^*\)</span>, (ii) normal Q-Q plot, (iii) convergence plot of SE vs. B.</p></li>
<li><p>Compute the skewness and kurtosis of <span class="math notranslate nohighlight">\(\bar{X}^*\)</span>. How do they compare to normal values (skew = 0, kurtosis = 3)?</p></li>
<li><p>Based on your diagnostics, which CI method would you recommend: normal, percentile, or basic?</p></li>
<li><p>Now compute the same diagnostics for the median. How do they differ?</p></li>
</ol>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">999</span><span class="p">)</span>
<span class="n">n</span><span class="p">,</span> <span class="n">B</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">10000</span>

<span class="c1"># Part (a): Generate mixture data</span>
<span class="n">n_main</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.9</span> <span class="o">*</span> <span class="n">n</span><span class="p">)</span>
<span class="n">n_outlier</span> <span class="o">=</span> <span class="n">n</span> <span class="o">-</span> <span class="n">n_main</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span>
    <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_main</span><span class="p">),</span>
    <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_outlier</span><span class="p">)</span>
<span class="p">])</span>
<span class="n">rng</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">mean_hat</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">median_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Bootstrap distributions</span>
<span class="n">mean_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="n">median_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">mean_star</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">median_star</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>

<span class="c1"># Part (b): Diagnostic plots for mean</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1"># Histogram</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">mean_star</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">mean_hat</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Original&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\bar</span><span class="si">{X}</span><span class="s1">^*$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Bootstrap Distribution of Mean&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Q-Q plot</span>
<span class="n">stats</span><span class="o">.</span><span class="n">probplot</span><span class="p">(</span><span class="n">mean_star</span><span class="p">,</span> <span class="n">dist</span><span class="o">=</span><span class="s2">&quot;norm&quot;</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Normal Q-Q Plot&#39;</span><span class="p">)</span>

<span class="c1"># Convergence plot</span>
<span class="n">B_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">B</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">se_by_B</span> <span class="o">=</span> <span class="p">[</span><span class="n">mean_star</span><span class="p">[:</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">B_values</span><span class="p">]</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">B_values</span><span class="p">,</span> <span class="n">se_by_B</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">mean_star</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;B&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Bootstrap SE&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;SE Convergence&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Part (c): Shape statistics</span>
<span class="n">skew_mean</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">skew</span><span class="p">(</span><span class="n">mean_star</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">kurt_mean</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">kurtosis</span><span class="p">(</span><span class="n">mean_star</span><span class="p">,</span> <span class="n">fisher</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">skew_median</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">skew</span><span class="p">(</span><span class="n">median_star</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">kurt_median</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">kurtosis</span><span class="p">(</span><span class="n">median_star</span><span class="p">,</span> <span class="n">fisher</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean bootstrap distribution:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Skewness: </span><span class="si">{</span><span class="n">skew_mean</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (normal = 0)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Kurtosis: </span><span class="si">{</span><span class="n">kurt_mean</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (normal = 3)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Bias/SE:  </span><span class="si">{</span><span class="p">(</span><span class="n">mean_star</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">mean_hat</span><span class="p">)</span><span class="o">/</span><span class="n">mean_star</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Median bootstrap distribution:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Skewness: </span><span class="si">{</span><span class="n">skew_median</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Kurtosis: </span><span class="si">{</span><span class="n">kurt_median</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Bias/SE:  </span><span class="si">{</span><span class="p">(</span><span class="n">median_star</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">median_hat</span><span class="p">)</span><span class="o">/</span><span class="n">median_star</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Key insights</strong>:</p>
<ul class="simple">
<li><p class="sd-card-text">Mixture data creates a right-skewed bootstrap distribution for the mean.</p></li>
<li><p class="sd-card-text">Q-Q plot shows departure from normality in the upper tail.</p></li>
<li><p class="sd-card-text">With positive skewness, percentile CI may under-cover on the right; basic CI may help.</p></li>
<li><p class="sd-card-text">Median bootstrap distribution may show different characteristics (possibly multimodal near the 10% outlier threshold).</p></li>
<li><p class="sd-card-text">SE convergence plot helps choose <span class="math notranslate nohighlight">\(B\)</span>; should stabilize by <span class="math notranslate nohighlight">\(B = 5{,}000\)</span>.</p></li>
</ul>
</div>
</details><div class="tip admonition">
<p class="admonition-title">Exercise 4.3.8: Bootstrap for the Correlation Coefficient</p>
<p><strong>Background</strong>: The correlation coefficient <span class="math notranslate nohighlight">\(r\)</span> has a complex sampling distribution, especially near the boundaries <span class="math notranslate nohighlight">\(\pm 1\)</span>.</p>
<p><strong>Tasks</strong>:</p>
<ol class="loweralpha simple">
<li><p>Generate <span class="math notranslate nohighlight">\(n = 80\)</span> bivariate observations with true correlation <span class="math notranslate nohighlight">\(\rho = 0.3\)</span>. Use pairs bootstrap to compute SE and 95% percentile CI for <span class="math notranslate nohighlight">\(r\)</span>.</p></li>
<li><p>Repeat with <span class="math notranslate nohighlight">\(\rho = 0.9\)</span>. How does the bootstrap SE change? How does the shape of the bootstrap distribution change?</p></li>
<li><p>For <span class="math notranslate nohighlight">\(\rho = 0.9\)</span>, apply Fisher’s z-transformation: <span class="math notranslate nohighlight">\(z = \tanh^{-1}(r)\)</span>. Compute the bootstrap CI on the z-scale and transform back. Compare to the direct percentile CI on the r-scale.</p></li>
<li><p>Why is the z-transformation helpful for correlations near <span class="math notranslate nohighlight">\(\pm 1\)</span>?</p></li>
</ol>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">n</span><span class="p">,</span> <span class="n">B</span> <span class="o">=</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">10000</span>

<span class="k">def</span><span class="w"> </span><span class="nf">generate_corr_data</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">rho</span><span class="p">,</span> <span class="n">rng</span><span class="p">):</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="n">rho</span><span class="p">],</span> <span class="p">[</span><span class="n">rho</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
    <span class="k">return</span> <span class="n">rng</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">corr</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1"># Part (a): rho = 0.3</span>
<span class="n">rho_low</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">xy_low</span> <span class="o">=</span> <span class="n">generate_corr_data</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">rho_low</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
<span class="n">r_hat_low</span> <span class="o">=</span> <span class="n">corr</span><span class="p">(</span><span class="n">xy_low</span><span class="p">)</span>

<span class="n">r_star_low</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">r_star_low</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">corr</span><span class="p">(</span><span class="n">xy_low</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>

<span class="n">se_low</span> <span class="o">=</span> <span class="n">r_star_low</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ci_low</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">r_star_low</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.975</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;rho = </span><span class="si">{</span><span class="n">rho_low</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  r_hat = </span><span class="si">{</span><span class="n">r_hat_low</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  SE = </span><span class="si">{</span><span class="n">se_low</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  95% CI: [</span><span class="si">{</span><span class="n">ci_low</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_low</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Skewness: </span><span class="si">{</span><span class="n">stats</span><span class="o">.</span><span class="n">skew</span><span class="p">(</span><span class="n">r_star_low</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Part (b): rho = 0.9</span>
<span class="n">rho_high</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">xy_high</span> <span class="o">=</span> <span class="n">generate_corr_data</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">rho_high</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
<span class="n">r_hat_high</span> <span class="o">=</span> <span class="n">corr</span><span class="p">(</span><span class="n">xy_high</span><span class="p">)</span>

<span class="n">r_star_high</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">r_star_high</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">corr</span><span class="p">(</span><span class="n">xy_high</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>

<span class="n">se_high</span> <span class="o">=</span> <span class="n">r_star_high</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ci_high</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">r_star_high</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.975</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">rho = </span><span class="si">{</span><span class="n">rho_high</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  r_hat = </span><span class="si">{</span><span class="n">r_hat_high</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  SE = </span><span class="si">{</span><span class="n">se_high</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  95% CI: [</span><span class="si">{</span><span class="n">ci_high</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_high</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Skewness: </span><span class="si">{</span><span class="n">stats</span><span class="o">.</span><span class="n">skew</span><span class="p">(</span><span class="n">r_star_high</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Part (c): Fisher z-transformation for high correlation</span>
<span class="n">z_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arctanh</span><span class="p">(</span><span class="n">r_hat_high</span><span class="p">)</span>
<span class="n">z_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arctanh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">r_star_high</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9999</span><span class="p">,</span> <span class="mf">0.9999</span><span class="p">))</span>  <span class="c1"># Avoid arctanh(±1)</span>

<span class="n">ci_z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">z_star</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.975</span><span class="p">])</span>
<span class="n">ci_r_from_z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">ci_z</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Fisher z-transformation (rho = </span><span class="si">{</span><span class="n">rho_high</span><span class="si">}</span><span class="s2">):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  z_hat = </span><span class="si">{</span><span class="n">z_hat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  z-scale CI: [</span><span class="si">{</span><span class="n">ci_z</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_z</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Back-transformed CI: [</span><span class="si">{</span><span class="n">ci_r_from_z</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_r_from_z</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Direct percentile CI: [</span><span class="si">{</span><span class="n">ci_high</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_high</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Key insights</strong>:</p>
<ul class="simple">
<li><p class="sd-card-text">As <span class="math notranslate nohighlight">\(|\rho| \to 1\)</span>, the distribution of <span class="math notranslate nohighlight">\(r\)</span> becomes increasingly skewed and bounded; variance on the <span class="math notranslate nohighlight">\(r\)</span>-scale typically <em>shrinks</em>, but normal approximations degrade due to boundary effects.</p></li>
<li><p class="sd-card-text">Fisher <span class="math notranslate nohighlight">\(z = \text{arctanh}(r)\)</span> stabilizes variance and symmetrizes the distribution, making it preferred for inference near boundaries.</p></li>
<li><p class="sd-card-text">Bootstrap distribution becomes left-skewed as <span class="math notranslate nohighlight">\(r \to 1\)</span> (bounded above by 1).</p></li>
<li><p class="sd-card-text">Fisher z-transformation <span class="math notranslate nohighlight">\(z = \tanh^{-1}(r)\)</span> stabilizes variance and normalizes the distribution.</p></li>
<li><p class="sd-card-text">Z-transformed CIs respect the boundary constraint more naturally.</p></li>
<li><p class="sd-card-text">For <span class="math notranslate nohighlight">\(|r| &gt; 0.7\)</span>, z-transformation is generally recommended.</p></li>
</ul>
</div>
</details><div class="tip admonition">
<p class="admonition-title">Exercise 4.3.9: Bootstrap Failure Mode—Extreme Statistics</p>
<p><strong>Background</strong>: The bootstrap fails for extreme value statistics because it cannot generate values beyond the sample range.</p>
<p><strong>Tasks</strong>:</p>
<ol class="loweralpha simple">
<li><p>Generate <span class="math notranslate nohighlight">\(n = 100\)</span> observations from <span class="math notranslate nohighlight">\(U(0, 1)\)</span>. The true maximum is <span class="math notranslate nohighlight">\(\max_{x \in [0,1]} = 1\)</span>. Compute the bootstrap distribution of the sample maximum.</p></li>
<li><p>What is the largest possible value in the bootstrap distribution? Compute the bootstrap SE and bias for the maximum.</p></li>
<li><p>The theoretical SE of the sample maximum from <span class="math notranslate nohighlight">\(U(0,1)\)</span> is approximately <span class="math notranslate nohighlight">\(1/(n+1) \approx 0.01\)</span>. How does the bootstrap SE compare?</p></li>
<li><p>Suggest a remedy for this failure mode. (Hint: Consider parametric bootstrap or extreme value theory.)</p></li>
</ol>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">222</span><span class="p">)</span>
<span class="n">n</span><span class="p">,</span> <span class="n">B</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10000</span>

<span class="c1"># Part (a): Generate uniform data</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">max_hat</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

<span class="c1"># Bootstrap</span>
<span class="n">max_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">max_star</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

<span class="c1"># Part (b): Analyze bootstrap distribution</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample maximum: </span><span class="si">{</span><span class="n">max_hat</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bootstrap max possible: </span><span class="si">{</span><span class="n">max_hat</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (same as sample max!)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bootstrap SE: </span><span class="si">{</span><span class="n">max_star</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bootstrap bias: </span><span class="si">{</span><span class="n">max_star</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">max_hat</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Part (c): Theoretical SE</span>
<span class="c1"># For U(0,1), max has mean n/(n+1) and variance n/((n+1)²(n+2))</span>
<span class="n">theoretical_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span> <span class="o">/</span> <span class="p">((</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="mi">2</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Theoretical SE: </span><span class="si">{</span><span class="n">theoretical_se</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bootstrap SE:   </span><span class="si">{</span><span class="n">max_star</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ratio (bootstrap/theoretical): </span><span class="si">{</span><span class="n">max_star</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">theoretical_se</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># The bootstrap severely underestimates the true SE!</span>

<span class="c1"># Part (d): Parametric bootstrap remedy</span>
<span class="c1"># Fit U(0, max_hat) and simulate</span>
<span class="n">max_star_param</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="n">x_star</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_hat</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">max_star_param</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_star</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Parametric bootstrap SE: </span><span class="si">{</span><span class="n">max_star_param</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># Still biased but less severely</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Key insights</strong>:</p>
<ul class="simple">
<li><p class="sd-card-text">Bootstrap cannot exceed <span class="math notranslate nohighlight">\(\max(X_1, \ldots, X_n)\)</span>, so <span class="math notranslate nohighlight">\(\max^* \leq \max\)</span>.</p></li>
<li><p class="sd-card-text">Bootstrap SE is typically 30-50% of the true SE for maxima.</p></li>
<li><p class="sd-card-text">This is a <strong>fundamental limitation</strong> of nonparametric bootstrap for extreme statistics.</p></li>
<li><p class="sd-card-text"><strong>Remedies</strong>:
- Parametric bootstrap with fitted distribution
- Extreme value theory (GPD/GEV for block maxima)
- m-out-of-n bootstrap (subsample with <span class="math notranslate nohighlight">\(m &lt; n\)</span>)</p></li>
<li><p class="sd-card-text">Similar problems occur for minimum, range, and outer quantiles (1st, 99th percentiles).</p></li>
</ul>
</div>
</details><div class="tip admonition">
<p class="admonition-title">Exercise 4.3.10: Complete Bootstrap Analysis Workflow</p>
<p><strong>Background</strong>: This exercise synthesizes all concepts from the section into a complete analysis.</p>
<p><strong>Scenario</strong>: You are analyzing the relationship between advertising spending (X, in thousands of dollars) and sales (Y, in units). You have data from 60 markets.</p>
<p><strong>Data generation</strong> (use this code):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">60</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>  <span class="c1"># Right-skewed spending</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">+</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="n">x</span>  <span class="c1"># Heteroscedastic errors</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">50</span> <span class="o">+</span> <span class="mf">0.8</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Tasks</strong>:</p>
<ol class="loweralpha simple">
<li><p>Fit OLS and compute classical SE for <span class="math notranslate nohighlight">\(\beta_1\)</span> (the slope).</p></li>
<li><p>Recognizing that X is observational (random) and errors may be heteroscedastic, choose an appropriate bootstrap scheme. Justify your choice.</p></li>
<li><p>Compute bootstrap SE and compare to the classical SE. Which is larger? Why?</p></li>
<li><p>Construct a 95% CI for <span class="math notranslate nohighlight">\(\beta_1\)</span> using the percentile method.</p></li>
<li><p>Create diagnostic plots (histogram and Q-Q plot of <span class="math notranslate nohighlight">\(\hat{\beta}_1^*\)</span>). Based on these, would you change your CI method?</p></li>
<li><p>Write a brief (2-3 sentence) summary of your findings, including the point estimate, CI, and any caveats.</p></li>
</ol>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Data generation</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">60</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">+</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="n">x</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">50</span> <span class="o">+</span> <span class="mf">0.8</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">x</span><span class="p">])</span>

<span class="c1"># Part (a): Classical OLS</span>
<span class="n">beta_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="kc">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta_hat</span>
<span class="n">resid</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span>
<span class="n">sigma2_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">resid</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">XtX_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span>
<span class="n">se_classical</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma2_hat</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">XtX_inv</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Part (a): Classical OLS&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  β̂₀ = </span><span class="si">{</span><span class="n">beta_hat</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, SE = </span><span class="si">{</span><span class="n">se_classical</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  β̂₁ = </span><span class="si">{</span><span class="n">beta_hat</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, SE = </span><span class="si">{</span><span class="n">se_classical</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Part (b): Choose bootstrap scheme</span>
<span class="c1"># X is observational (random) and errors are heteroscedastic</span>
<span class="c1"># → Use PAIRS bootstrap</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Part (b): Using PAIRS bootstrap&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  Justification: X is observational (not controlled) and &quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  errors appear heteroscedastic (variance increases with X)&quot;</span><span class="p">)</span>

<span class="c1"># Part (c): Bootstrap SE</span>
<span class="n">B</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">beta_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">X_star</span><span class="p">,</span> <span class="n">y_star</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:</span><span class="n">p</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">p</span><span class="p">]</span>
    <span class="n">beta_star</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X_star</span><span class="p">,</span> <span class="n">y_star</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="kc">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">se_boot</span> <span class="o">=</span> <span class="n">beta_star</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Part (c): Bootstrap vs Classical SE&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  β̂₁ classical SE: </span><span class="si">{</span><span class="n">se_classical</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  β̂₁ bootstrap SE: </span><span class="si">{</span><span class="n">se_boot</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Ratio: </span><span class="si">{</span><span class="n">se_boot</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">se_classical</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  Bootstrap SE is larger because it accounts for heteroscedasticity&quot;</span><span class="p">)</span>

<span class="c1"># Part (d): 95% CI</span>
<span class="n">ci_perc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">beta_star</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.975</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Part (d): 95% percentile CI for β₁: [</span><span class="si">{</span><span class="n">ci_perc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_perc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>

<span class="c1"># Part (e): Diagnostics</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">beta_star</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">beta_hat</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;β̂₁&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">ci_perc</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;CI&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">ci_perc</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\hat{\beta}_1^*$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Bootstrap Distribution of Slope&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">stats</span><span class="o">.</span><span class="n">probplot</span><span class="p">(</span><span class="n">beta_star</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dist</span><span class="o">=</span><span class="s2">&quot;norm&quot;</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Normal Q-Q Plot&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;bootstrap_diagnostics.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">skewness</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">skew</span><span class="p">(</span><span class="n">beta_star</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Part (e): Diagnostics&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Skewness of β̂₁*: </span><span class="si">{</span><span class="n">skewness</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">skewness</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  Distribution is approximately symmetric; percentile CI is appropriate&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  Distribution shows skewness; consider BCa interval&quot;</span><span class="p">)</span>

<span class="c1"># Part (f): Summary</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Part (f): SUMMARY&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The estimated effect of advertising spending on sales is&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;β̂₁ = </span><span class="si">{</span><span class="n">beta_hat</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> units per $1,000 spent (95% CI: [</span><span class="si">{</span><span class="n">ci_perc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_perc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">]).&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The pairs bootstrap was used due to heteroscedastic errors&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;(variance increases with spending level).&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Key insights</strong>:</p>
<ul class="simple">
<li><p class="sd-card-text">Heteroscedasticity inflates classical SE relative to truth; pairs bootstrap captures this.</p></li>
<li><p class="sd-card-text">Diagnostic plots confirm the choice of interval method.</p></li>
<li><p class="sd-card-text">A complete analysis includes: point estimate, SE, CI, method justification, and diagnostics.</p></li>
<li><p class="sd-card-text">Clear communication of results with appropriate caveats is essential.</p></li>
</ul>
</div>
</details></section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<p><strong>Primary Sources</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="efron1979" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Efron1979<span class="fn-bracket">]</span></span>
<p>Efron, B. (1979). Bootstrap methods: Another look at the jackknife. <em>The Annals of Statistics</em>, 7(1), 1–26. The original bootstrap paper, establishing resampling from the ECDF as the foundation for nonparametric inference.</p>
</div>
<div class="citation" id="efrontibshirani1993" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>EfronTibshirani1993<span class="fn-bracket">]</span></span>
<p>Efron, B., and Tibshirani, R. J. (1993). <em>An Introduction to the Bootstrap</em>. Chapman &amp; Hall/CRC. The definitive textbook on bootstrap methods with extensive practical guidance.</p>
</div>
<div class="citation" id="davisonhinkley1997" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DavisonHinkley1997<span class="fn-bracket">]</span></span>
<p>Davison, A. C., and Hinkley, D. V. (1997). <em>Bootstrap Methods and Their Application</em>. Cambridge University Press. Comprehensive treatment with extensive examples and code for both theoretical and applied audiences.</p>
</div>
</div>
<p><strong>Regression Bootstrap</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="freedman1981" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Freedman1981<span class="fn-bracket">]</span></span>
<p>Freedman, D. A. (1981). Bootstrapping regression models. <em>The Annals of Statistics</em>, 9(6), 1218–1228. Foundational paper on residual bootstrap and comparison with pairs bootstrap for linear models.</p>
</div>
<div class="citation" id="wu1986" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Wu1986<span class="fn-bracket">]</span></span>
<p>Wu, C. F. J. (1986). Jackknife, bootstrap and other resampling methods in regression analysis. <em>The Annals of Statistics</em>, 14(4), 1261–1295. Introduction of the wild bootstrap for heteroscedasticity-robust inference.</p>
</div>
<div class="citation" id="mammen1993" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Mammen1993<span class="fn-bracket">]</span></span>
<p>Mammen, E. (1993). Bootstrap and wild bootstrap for high dimensional linear models. <em>The Annals of Statistics</em>, 21(1), 255–285. Theoretical foundations of wild bootstrap with optimal weight distributions.</p>
</div>
</div>
<p><strong>Theoretical Foundations</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="hall1992" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Hall1992<span class="fn-bracket">]</span></span>
<p>Hall, P. (1992). <em>The Bootstrap and Edgeworth Expansion</em>. Springer. Mathematical foundations of higher-order accuracy and the theory of bootstrap confidence intervals.</p>
</div>
<div class="citation" id="shaotu1995" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ShaoTu1995<span class="fn-bracket">]</span></span>
<p>Shao, J., and Tu, D. (1995). <em>The Jackknife and Bootstrap</em>. Springer. Unified treatment of resampling methods with rigorous proofs and mathematical detail.</p>
</div>
<div class="citation" id="vandervaart1998" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>vanderVaart1998<span class="fn-bracket">]</span></span>
<p>van der Vaart, A. W. (1998). <em>Asymptotic Statistics</em>, Chapters 23–25. Cambridge University Press. Rigorous asymptotic theory for bootstrap consistency and weak convergence.</p>
</div>
</div>
<p><strong>Confidence Interval Methods</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="diciccioefron1996" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DiCiccioEfron1996<span class="fn-bracket">]</span></span>
<p>DiCiccio, T. J., and Efron, B. (1996). Bootstrap confidence intervals. <em>Statistical Science</em>, 11(3), 189–228. Comprehensive review of percentile, basic, studentized, and BCa interval methods.</p>
</div>
</div>
<p><strong>Computational</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="cantyripley" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CantyRipley<span class="fn-bracket">]</span></span>
<p>Canty, A., and Ripley, B. D. boot: Bootstrap R (S-Plus) Functions. R package. <a class="reference external" href="https://cran.r-project.org/package=boot">https://cran.r-project.org/package=boot</a>. Standard R implementation of bootstrap methods; version numbers update regularly.</p>
</div>
</div>
<p><strong>Failure Modes and Extensions</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="bickelfreedman1981" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>BickelFreedman1981<span class="fn-bracket">]</span></span>
<p>Bickel, P. J., and Freedman, D. A. (1981). Some asymptotic theory for the bootstrap. <em>The Annals of Statistics</em>, 9(6), 1196–1217. Conditions for bootstrap consistency and characterization of failure cases.</p>
</div>
<div class="citation" id="politisromanowolf1999" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>PolitisRomanoWolf1999<span class="fn-bracket">]</span></span>
<p>Politis, D. N., Romano, J. P., and Wolf, M. (1999). <em>Subsampling</em>. Springer. The m-out-of-n bootstrap and subsampling alternatives for non-standard problems.</p>
</div>
<div class="citation" id="lahiri2003" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Lahiri2003<span class="fn-bracket">]</span></span>
<p>Lahiri, S. N. (2003). <em>Resampling Methods for Dependent Data</em>. Springer. Block bootstrap and related methods for time series and spatial data.</p>
</div>
</div>
<p><strong>Historical Context</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="quenouille1949" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Quenouille1949<span class="fn-bracket">]</span></span>
<p>Quenouille, M. H. (1949). Approximate tests of correlation in time-series. <em>Journal of the Royal Statistical Society B</em>, 11(1), 68–84. Precursor to resampling: bias reduction via leaving-one-out, inspiring the jackknife.</p>
</div>
<div class="citation" id="tukey1958" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Tukey1958<span class="fn-bracket">]</span></span>
<p>Tukey, J. W. (1958). Bias and confidence in not quite large samples (abstract). <em>The Annals of Mathematical Statistics</em>, 29(2), 614. Introduction of the jackknife for variance estimation; full treatment in unpublished Princeton notes.</p>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="ch4_2-empirical-distribution-plugin.html" class="btn btn-neutral float-left" title="The Empirical Distribution and Plug-in Principle" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ch4_4-parametric-bootstrap.html" class="btn btn-neutral float-right" title="Section 4.4: The Parametric Bootstrap" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>