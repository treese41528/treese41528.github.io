

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Generalized Linear Models &mdash; STAT 418</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=3d0abd52" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT418/Website/part2_simulation/chapter3/ch3_5-generalized-linear-models.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=30646c52"></script>
      <script>let toggleHintShow = 'Show solution';</script>
      <script>let toggleHintHide = 'Hide solution';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>window.MathJax = {"options": {"enableMenu": true, "menuOptions": {"settings": {"enrich": true, "speech": true, "braille": true, "collapsible": true, "assistiveMml": false}}}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
      <script src="../../_static/custom.js?v=8718e0ab"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Chapter Summary" href="ch3_6-chapter-summary.html" />
    <link rel="prev" title="Linear Models" href="ch3_4-linear-models.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Course Content</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../Homework/index.html">Homework Assignments</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Homework/index.html#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#assignment-policies">Assignment Policies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#recommended-workflow">Recommended Workflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#submission-requirements">Submission Requirements</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Homework/index.html#assignments-by-chapter">Assignments by Chapter</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#part-i-foundations">Part I: Foundations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Homework/hw1_distributional_relationships.html">Homework 1: Distributional Relationships and Computational Foundations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Homework/index.html#tips-for-success">Tips for Success</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#mathematical-derivations">Mathematical Derivations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#computational-verification">Computational Verification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#common-pitfalls">Common Pitfalls</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part1_foundations/index.html">Part I: Foundations of Probability and Computation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part1_foundations/chapter1/index.html">Chapter 1: Statistical Paradigms and Core Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html">Paradigms of Probability and Statistical Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#the-mathematical-foundation-kolmogorov-s-axioms">The Mathematical Foundation: Kolmogorov’s Axioms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#interpretations-of-probability">Interpretations of Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#statistical-inference-paradigms">Statistical Inference Paradigms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#historical-and-philosophical-debates">Historical and Philosophical Debates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#looking-ahead-our-course-focus">Looking Ahead: Our Course Focus</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html">Probability Distributions: Theory and Computation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#from-abstract-foundations-to-concrete-tools">From Abstract Foundations to Concrete Tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#the-python-ecosystem-for-probability">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#introduction-why-probability-distributions-matter">Introduction: Why Probability Distributions Matter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#id1">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#continuous-distributions">Continuous Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#additional-important-distributions">Additional Important Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#summary-and-practical-guidelines">Summary and Practical Guidelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#conclusion">Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html">Python Random Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#from-mathematical-distributions-to-computational-samples">From Mathematical Distributions to Computational Samples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-python-ecosystem-at-a-glance">The Python Ecosystem at a Glance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#understanding-pseudo-random-number-generation">Understanding Pseudo-Random Number Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-standard-library-random-module">The Standard Library: <code class="docutils literal notranslate"><span class="pre">random</span></code> Module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#numpy-fast-vectorized-random-sampling">NumPy: Fast Vectorized Random Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#scipy-stats-the-complete-statistical-toolkit">SciPy Stats: The Complete Statistical Toolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#bringing-it-all-together-library-selection-guide">Bringing It All Together: Library Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#looking-ahead-from-random-numbers-to-monte-carlo-methods">Looking Ahead: From Random Numbers to Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html">Chapter 1 Summary: Foundations in Place</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#the-three-pillars-of-chapter-1">The Three Pillars of Chapter 1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#what-lies-ahead-the-road-to-simulation">What Lies Ahead: The Road to Simulation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#chapter-1-exercises-synthesis-problems">Chapter 1 Exercises: Synthesis Problems</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Part II: Simulation-Based Methods</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../chapter2/index.html">Chapter 2: Monte Carlo Simulation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html">Monte Carlo Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#the-historical-development-of-monte-carlo-methods">The Historical Development of Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#the-core-principle-expectation-as-integration">The Core Principle: Expectation as Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#theoretical-foundations">Theoretical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#variance-estimation-and-confidence-intervals">Variance Estimation and Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#comparison-with-deterministic-methods">Comparison with Deterministic Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#sample-size-determination">Sample Size Determination</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#convergence-diagnostics-and-monitoring">Convergence Diagnostics and Monitoring</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#chapter-2-1-exercises-monte-carlo-fundamentals-mastery">Chapter 2.1 Exercises: Monte Carlo Fundamentals Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html">Uniform Random Variates</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#why-uniform-the-universal-currency-of-randomness">Why Uniform? The Universal Currency of Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#the-paradox-of-computational-randomness">The Paradox of Computational Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#chaotic-dynamical-systems-an-instructive-failure">Chaotic Dynamical Systems: An Instructive Failure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#linear-congruential-generators">Linear Congruential Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#shift-register-generators">Shift-Register Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#the-kiss-generator-combining-strategies">The KISS Generator: Combining Strategies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#modern-generators-mersenne-twister-and-pcg">Modern Generators: Mersenne Twister and PCG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#statistical-testing-of-random-number-generators">Statistical Testing of Random Number Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#chapter-2-2-exercises-uniform-random-variates-mastery">Chapter 2.2 Exercises: Uniform Random Variates Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html">Inverse CDF Method</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#mathematical-foundations">Mathematical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#continuous-distributions-with-closed-form-inverses">Continuous Distributions with Closed-Form Inverses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#numerical-inversion">Numerical Inversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#mixed-distributions">Mixed Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#chapter-2-3-exercises-inverse-cdf-method-mastery">Chapter 2.3 Exercises: Inverse CDF Method Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html">Transformation Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#why-transformation-methods">Why Transformation Methods?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-boxmuller-transform">The Box–Muller Transform</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-polar-marsaglia-method">The Polar (Marsaglia) Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#method-comparison-boxmuller-vs-polar-vs-ziggurat">Method Comparison: Box–Muller vs Polar vs Ziggurat</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-ziggurat-algorithm">The Ziggurat Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-clt-approximation-historical">The CLT Approximation (Historical)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#distributions-derived-from-the-normal">Distributions Derived from the Normal</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#multivariate-normal-generation">Multivariate Normal Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#implementation-guidance">Implementation Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#chapter-2-4-exercises-transformation-methods-mastery">Chapter 2.4 Exercises: Transformation Methods Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html">Rejection Sampling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-dartboard-intuition">The Dartboard Intuition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-accept-reject-algorithm">The Accept-Reject Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#efficiency-analysis">Efficiency Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#choosing-the-proposal-distribution">Choosing the Proposal Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-squeeze-principle">The Squeeze Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#geometric-example-sampling-from-the-unit-disk">Geometric Example: Sampling from the Unit Disk</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#limitations-and-the-curse-of-dimensionality">Limitations and the Curse of Dimensionality</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#connections-to-other-methods">Connections to Other Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#chapter-2-5-exercises-rejection-sampling-mastery">Chapter 2.5 Exercises: Rejection Sampling Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html">Variance Reduction Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#the-variance-reduction-paradigm">The Variance Reduction Paradigm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#importance-sampling">Importance Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#control-variates">Control Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#antithetic-variates">Antithetic Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#stratified-sampling">Stratified Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#common-random-numbers">Common Random Numbers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#conditional-monte-carlo-raoblackwellization">Conditional Monte Carlo (Rao–Blackwellization)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#combining-variance-reduction-techniques">Combining Variance Reduction Techniques</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#chapter-2-6-exercises-variance-reduction-mastery">Chapter 2.6 Exercises: Variance Reduction Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html">Chapter Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#the-complete-monte-carlo-workflow">The Complete Monte Carlo Workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#quick-reference-tables">Quick Reference Tables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#common-pitfalls-checklist">Common Pitfalls Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#connections-to-later-chapters">Connections to Later Chapters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#learning-outcomes-checklist">Learning Outcomes Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#further-reading-optimization-and-missing-data">Further Reading: Optimization and Missing Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Chapter 3: Parametric Inference and Likelihood Methods</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="ch3_1-exponential-families.html">Exponential Families</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#historical-origins-from-scattered-results-to-unified-theory">Historical Origins: From Scattered Results to Unified Theory</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#the-canonical-exponential-family">The Canonical Exponential Family</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#converting-familiar-distributions">Converting Familiar Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#the-log-partition-function-a-moment-generating-machine">The Log-Partition Function: A Moment-Generating Machine</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#sufficiency-capturing-all-parameter-information">Sufficiency: Capturing All Parameter Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#minimal-sufficiency-and-completeness">Minimal Sufficiency and Completeness</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#conjugate-priors-and-bayesian-inference">Conjugate Priors and Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#exponential-dispersion-models-and-glms">Exponential Dispersion Models and GLMs</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#chapter-3-1-exercises-exponential-families-mastery">Chapter 3.1 Exercises: Exponential Families Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html">Maximum Likelihood Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#the-likelihood-function">The Likelihood Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#the-score-function">The Score Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#fisher-information">Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#closed-form-maximum-likelihood-estimators">Closed-Form Maximum Likelihood Estimators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#numerical-optimization-for-mle">Numerical Optimization for MLE</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#asymptotic-properties-of-mles">Asymptotic Properties of MLEs</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#the-cramer-rao-lower-bound">The Cramér-Rao Lower Bound</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#the-invariance-property">The Invariance Property</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#likelihood-based-hypothesis-testing">Likelihood-Based Hypothesis Testing</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#confidence-intervals-from-likelihood">Confidence Intervals from Likelihood</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#connection-to-bayesian-inference">Connection to Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#chapter-3-2-exercises-maximum-likelihood-estimation-mastery">Chapter 3.2 Exercises: Maximum Likelihood Estimation Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch3_3-sampling-variability.html">Sampling Variability and Variance Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#statistical-estimators-and-their-properties">Statistical Estimators and Their Properties</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#sampling-distributions">Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#the-delta-method">The Delta Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#variance-estimation-methods">Variance Estimation Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#applications-and-worked-examples">Applications and Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch3_4-linear-models.html">Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#matrix-calculus-foundations">Matrix Calculus Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#the-linear-model">The Linear Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#ordinary-least-squares-the-calculus-approach">Ordinary Least Squares: The Calculus Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#ordinary-least-squares-the-geometric-approach">Ordinary Least Squares: The Geometric Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#properties-of-the-ols-estimator">Properties of the OLS Estimator</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#the-gauss-markov-theorem">The Gauss-Markov Theorem</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#estimating-the-error-variance">Estimating the Error Variance</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#distributional-results-under-normality">Distributional Results Under Normality</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#diagnostics-and-model-checking">Diagnostics and Model Checking</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#numerical-stability-qr-decomposition">Numerical Stability: QR Decomposition</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#model-selection-and-information-criteria">Model Selection and Information Criteria</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#regularization-ridge-and-lasso">Regularization: Ridge and LASSO</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#chapter-3-4-exercises-linear-models-mastery">Chapter 3.4 Exercises: Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Generalized Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#historical-context-unification-of-regression-methods">Historical Context: Unification of Regression Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-glm-framework-three-components">The GLM Framework: Three Components</a></li>
<li class="toctree-l4"><a class="reference internal" href="#score-equations-and-fisher-information">Score Equations and Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="#iteratively-reweighted-least-squares">Iteratively Reweighted Least Squares</a></li>
<li class="toctree-l4"><a class="reference internal" href="#logistic-regression-binary-outcomes">Logistic Regression: Binary Outcomes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#poisson-regression-count-data">Poisson Regression: Count Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="#gamma-regression-positive-continuous-data">Gamma Regression: Positive Continuous Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="#inference-in-glms-the-testing-triad">Inference in GLMs: The Testing Triad</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-diagnostics">Model Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-comparison-and-selection">Model Comparison and Selection</a></li>
<li class="toctree-l4"><a class="reference internal" href="#quasi-likelihood-and-robust-inference">Quasi-Likelihood and Robust Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="#further-reading">Further Reading</a></li>
<li class="toctree-l4"><a class="reference internal" href="#chapter-3-5-exercises-generalized-linear-models-mastery">Chapter 3.5 Exercises: Generalized Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch3_6-chapter-summary.html">Chapter Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#the-parametric-inference-pipeline">The Parametric Inference Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#the-five-pillars-of-chapter-3">The Five Pillars of Chapter 3</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#quick-reference-core-formulas">Quick Reference: Core Formulas</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#connections-to-future-material">Connections to Future Material</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#practical-guidance">Practical Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter4/index.html">Chapter 4: Resampling Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html">The Sampling Distribution Problem</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#the-fundamental-target-sampling-distributions">The Fundamental Target: Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#historical-development-the-quest-for-sampling-distributions">Historical Development: The Quest for Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#three-routes-to-the-sampling-distribution">Three Routes to the Sampling Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#when-asymptotics-fail-motivating-the-bootstrap">When Asymptotics Fail: Motivating the Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#the-plug-in-principle-theoretical-foundation">The Plug-In Principle: Theoretical Foundation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#computational-perspective-bootstrap-as-monte-carlo">Computational Perspective: Bootstrap as Monte Carlo</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#chapter-4-1-exercises">Chapter 4.1 Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html">The Empirical Distribution and Plug-in Principle</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#the-empirical-cumulative-distribution-function">The Empirical Cumulative Distribution Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#convergence-of-the-empirical-cdf">Convergence of the Empirical CDF</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#parameters-as-statistical-functionals">Parameters as Statistical Functionals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#when-the-plug-in-principle-fails">When the Plug-in Principle Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#the-bootstrap-idea-in-one-sentence">The Bootstrap Idea in One Sentence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#computational-implementation">Computational Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#section-4-2-exercises-ecdf-and-plug-in-mastery">Section 4.2 Exercises: ECDF and Plug-in Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html">The Nonparametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#the-bootstrap-principle">The Bootstrap Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-standard-errors">Bootstrap Standard Errors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-bias-estimation">Bootstrap Bias Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-confidence-intervals">Bootstrap Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-for-regression">Bootstrap for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-diagnostics">Bootstrap Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#when-bootstrap-fails">When Bootstrap Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html">Section 4.4: The Parametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#the-parametric-bootstrap-principle">The Parametric Bootstrap Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#location-scale-families">Location-Scale Families</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#parametric-bootstrap-for-regression">Parametric Bootstrap for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#confidence-intervals">Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#model-checking-and-validation">Model Checking and Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#when-parametric-bootstrap-fails">When Parametric Bootstrap Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#parametric-vs-nonparametric-a-decision-framework">Parametric vs. Nonparametric: A Decision Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html">Section 4.5: Jackknife Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#historical-context-and-motivation">Historical Context and Motivation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#the-delete-1-jackknife">The Delete-1 Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#jackknife-bias-estimation">Jackknife Bias Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#the-delete-d-jackknife">The Delete-<span class="math notranslate nohighlight">\(d\)</span> Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#jackknife-versus-bootstrap">Jackknife versus Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#the-infinitesimal-jackknife">The Infinitesimal Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part3_bayesian/index.html">Part III: Bayesian Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part3_bayesian/index.html#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html">Bayesian Philosophy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Part II: Simulation-Based Methods</a></li>
          <li class="breadcrumb-item"><a href="index.html">Chapter 3: Parametric Inference and Likelihood Methods</a></li>
      <li class="breadcrumb-item active">Generalized Linear Models</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/part2_simulation/chapter3/ch3_5-generalized-linear-models.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="generalized-linear-models">
<span id="ch3-5-generalized-linear-models"></span><h1>Generalized Linear Models<a class="headerlink" href="#generalized-linear-models" title="Link to this heading"></a></h1>
<p>In Section 3.4, we developed the classical linear model—a powerful framework that has dominated statistical practice for over a century. The ordinary least squares estimator is elegant, computationally simple, and possesses optimal properties under the Gauss-Markov assumptions. Yet this elegance comes at a price: the linear model assumes that responses are continuous, unbounded, and follow a normal distribution with constant variance. Nature, unfortunately, is rarely so accommodating.</p>
<p>Consider the challenges a data scientist faces daily. <strong>Binary outcomes</strong> abound: Does a customer churn? Is an email spam? Will a patient respond to treatment? These yes/no responses cannot possibly be normally distributed—they take only two values. <strong>Count data</strong> appears everywhere: How many insurance claims will a policyholder file? How many website visits occur per hour? How many defects appear on a manufacturing line? Counts are non-negative integers, yet the normal distribution places positive probability on negative values and non-integers. <strong>Positive continuous data</strong> requires special handling: How much does a medical procedure cost? How long until a machine fails? These responses are strictly positive and often highly skewed, violating the symmetry and constant variance of normal errors.</p>
<p>For decades, statisticians addressed these challenges through ad hoc transformations—taking logarithms of positive data, using arcsine transformations for proportions, applying square roots to counts. Such transformations often improved normality but created new problems: interpretations became awkward, predictions could fall outside valid ranges, and the theoretical foundation was shaky. The statistical world needed a unified framework that could handle diverse response types while preserving the interpretability and computational tractability of linear models.</p>
<p>That framework arrived in 1972 when <strong>John Nelder and Robert Wedderburn</strong> published their landmark paper “Generalized Linear Models” in the <em>Journal of the Royal Statistical Society</em>. Their insight was profound yet simple: rather than transforming the data to fit the model, transform the model to fit the data. By recognizing that the normal distribution is just one member of the exponential family we studied in Section 3.1, they showed that logistic regression, Poisson regression, and many other specialized techniques are all special cases of a single unified framework. The exponential family provides the random component, a linear predictor provides the systematic component, and a link function connects the two.</p>
<p>This chapter builds directly on the foundations we have laid. Section 3.1 introduced exponential dispersion models with their canonical parameters, cumulant functions, and variance functions. Section 3.2 developed maximum likelihood estimation, including the Fisher scoring algorithm. Section 3.4 established the linear model with its normal equations and Gauss-Markov optimality. Generalized linear models synthesize all three: they use exponential family likelihoods, employ Fisher scoring for estimation (which reduces to the elegant IRLS algorithm), and generalize the normal equations to accommodate non-normal responses.</p>
<div class="important admonition">
<p class="admonition-title">Road Map 🧭</p>
<ul class="simple">
<li><p><strong>Understand</strong>: The three-component GLM framework (random, systematic, link) and why canonical links simplify computation</p></li>
<li><p><strong>Develop</strong>: The IRLS algorithm from Fisher scoring principles; master the score equations and Fisher information for GLMs</p></li>
<li><p><strong>Implement</strong>: Fit logistic, Poisson, and Gamma regressions using both custom code and statsmodels/sklearn</p></li>
<li><p><strong>Evaluate</strong>: Diagnose model fit using deviance, Pearson statistics, and residual analysis; compare nested models via likelihood ratio tests</p></li>
</ul>
</div>
<section id="historical-context-unification-of-regression-methods">
<h2>Historical Context: Unification of Regression Methods<a class="headerlink" href="#historical-context-unification-of-regression-methods" title="Link to this heading"></a></h2>
<p>Before Nelder and Wedderburn’s synthesis, each regression problem with non-normal responses required its own specialized methodology. Logistic regression had been developed independently for bioassay problems in the 1940s. Poisson regression emerged from actuarial science and epidemiology. Probit analysis, log-linear models for contingency tables, and gamma regression for survival times each had separate theoretical developments, separate estimation procedures, and separate software implementations.</p>
<p>The 1972 paper changed everything. Nelder and Wedderburn showed that all these methods share a common mathematical structure rooted in the exponential family. They introduced the concept of the <strong>link function</strong>—a monotonic transformation connecting the mean response to the linear predictor—and demonstrated that maximum likelihood estimation for all GLMs reduces to a single algorithm: <strong>Iteratively Reweighted Least Squares (IRLS)</strong>. Write the algorithm once, and it applies to logistic regression, Poisson regression, gamma regression, and any other exponential family member.</p>
<p>The impact was immediate and lasting. The GLIM (Generalized Linear Interactive Modeling) software, released in 1974, implemented the unified framework. Today, GLM functionality appears in every major statistical package: R’s <code class="docutils literal notranslate"><span class="pre">glm()</span></code> function, Python’s <code class="docutils literal notranslate"><span class="pre">statsmodels.GLM</span></code>, SAS PROC GENMOD, and many others. The unification also catalyzed theoretical advances, leading to quasi-likelihood methods for overdispersion, generalized estimating equations for correlated data, and generalized additive models for nonlinear effects.</p>
<p>For computational data scientists, GLMs represent the ideal balance of flexibility and interpretability. Unlike black-box machine learning methods, GLM coefficients have clear interpretations as log-odds ratios (logistic), log-rate ratios (Poisson), or log-fold changes (gamma with log link). Unlike classical linear regression, GLMs accommodate the discrete, bounded, or skewed responses that pervade real applications. This combination—principled statistical inference with practical flexibility—makes GLMs indispensable.</p>
</section>
<section id="the-glm-framework-three-components">
<h2>The GLM Framework: Three Components<a class="headerlink" href="#the-glm-framework-three-components" title="Link to this heading"></a></h2>
<p>A generalized linear model consists of three interlinked components that together specify how responses relate to predictors. Understanding each component—and how they connect—is essential for both fitting GLMs and interpreting their results.</p>
<section id="the-random-component">
<h3>The Random Component<a class="headerlink" href="#the-random-component" title="Link to this heading"></a></h3>
<p>The <strong>random component</strong> specifies the probability distribution of the response variable <span class="math notranslate nohighlight">\(Y_i\)</span> given the predictors. In GLMs, this distribution must belong to the <strong>exponential dispersion family</strong> introduced in Section 3.1:</p>
<div class="math notranslate nohighlight" id="equation-glm-density">
<span class="eqno">(104)<a class="headerlink" href="#equation-glm-density" title="Link to this equation"></a></span>\[f(y_i | \theta_i, \phi) = \exp\left\{ \frac{y_i \theta_i - b(\theta_i)}{\phi} + c(y_i, \phi) \right\}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\theta_i\)</span> is the <strong>canonical parameter</strong> (natural parameter) for observation <span class="math notranslate nohighlight">\(i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\phi &gt; 0\)</span> is the <strong>dispersion parameter</strong>, shared across all observations</p></li>
<li><p><span class="math notranslate nohighlight">\(b(\theta)\)</span> is the <strong>cumulant function</strong>, determining the distribution’s shape</p></li>
<li><p><span class="math notranslate nohighlight">\(c(y, \phi)\)</span> is a normalizing term depending on <span class="math notranslate nohighlight">\(y\)</span> and <span class="math notranslate nohighlight">\(\phi\)</span> but not <span class="math notranslate nohighlight">\(\theta\)</span></p></li>
</ul>
<p>From Section 3.1, recall that the cumulant function generates the moments:</p>
<div class="math notranslate nohighlight">
\[\mu_i = \mathbb{E}[Y_i] = b'(\theta_i), \qquad \text{Var}(Y_i) = \phi \cdot b''(\theta_i) = \phi \cdot V(\mu_i)\]</div>
<p>The function <span class="math notranslate nohighlight">\(V(\mu) = b''(\theta)\)</span> expressed in terms of <span class="math notranslate nohighlight">\(\mu\)</span> is the <strong>variance function</strong>—it specifies how variance depends on the mean. Different choices of <span class="math notranslate nohighlight">\(b(\theta)\)</span> yield different distributions:</p>
<table class="docutils align-default" id="id3">
<caption><span class="caption-number">Table 35 </span><span class="caption-text">Common GLM Families</span><a class="headerlink" href="#id3" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 18.0%" />
<col style="width: 22.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Distribution</p></th>
<th class="head"><p>Canonical Parameter <span class="math notranslate nohighlight">\(\theta\)</span></p></th>
<th class="head"><p>Cumulant <span class="math notranslate nohighlight">\(b(\theta)\)</span></p></th>
<th class="head"><p>Variance Function <span class="math notranslate nohighlight">\(V(\mu)\)</span></p></th>
<th class="head"><p>Dispersion <span class="math notranslate nohighlight">\(\phi\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Normal</p></td>
<td><p><span class="math notranslate nohighlight">\(\mu\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\theta^2/2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\sigma^2\)</span> (estimated)</p></td>
</tr>
<tr class="row-odd"><td><p>Bernoulli</p></td>
<td><p><span class="math notranslate nohighlight">\(\log\frac{\mu}{1-\mu}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\log(1 + e^\theta)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu(1-\mu)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span> (fixed)</p></td>
</tr>
<tr class="row-even"><td><p>Binomial(<span class="math notranslate nohighlight">\(n\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\log\frac{\mu/n}{1-\mu/n}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n\log(1 + e^\theta)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu(1-\mu/n)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span> (fixed)</p></td>
</tr>
<tr class="row-odd"><td><p>Poisson</p></td>
<td><p><span class="math notranslate nohighlight">\(\log \mu\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(e^\theta\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span> (fixed)</p></td>
</tr>
<tr class="row-even"><td><p>Gamma</p></td>
<td><p><span class="math notranslate nohighlight">\(-1/\mu\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\log(-\theta)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu^2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1/\alpha\)</span> (estimated)</p></td>
</tr>
<tr class="row-odd"><td><p>Inverse Gaussian</p></td>
<td><p><span class="math notranslate nohighlight">\(-1/(2\mu^2)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\sqrt{-2\theta}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu^3\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\lambda^{-1}\)</span> (estimated)</p></td>
</tr>
</tbody>
</table>
<p>The choice of distribution should match the data’s characteristics: Bernoulli/Binomial for binary/proportion data, Poisson for counts, Gamma for positive continuous with constant coefficient of variation, and so forth.</p>
</section>
<section id="the-systematic-component">
<h3>The Systematic Component<a class="headerlink" href="#the-systematic-component" title="Link to this heading"></a></h3>
<p>The <strong>systematic component</strong> specifies how predictors combine to influence the response. In GLMs, predictors enter through a <strong>linear predictor</strong>:</p>
<div class="math notranslate nohighlight" id="equation-linear-predictor">
<span class="eqno">(105)<a class="headerlink" href="#equation-linear-predictor" title="Link to this equation"></a></span>\[\eta_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_{p-1} x_{i,p-1} = \mathbf{x}_i^\top \boldsymbol{\beta}\]</div>
<p>In matrix form for all <span class="math notranslate nohighlight">\(n\)</span> observations:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\eta} = \mathbf{X}\boldsymbol{\beta}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is the <span class="math notranslate nohighlight">\(n \times p\)</span> design matrix (including a column of ones for the intercept) and <span class="math notranslate nohighlight">\(\boldsymbol{\beta} \in \mathbb{R}^p\)</span> is the parameter vector.</p>
<p>The systematic component is identical to the linear model from Section 3.4—the predictors combine linearly. What differs is how this linear combination relates to the mean response.</p>
</section>
<section id="the-link-function">
<h3>The Link Function<a class="headerlink" href="#the-link-function" title="Link to this heading"></a></h3>
<p>The <strong>link function</strong> <span class="math notranslate nohighlight">\(g(\cdot)\)</span> connects the random and systematic components by relating the mean <span class="math notranslate nohighlight">\(\mu_i\)</span> to the linear predictor <span class="math notranslate nohighlight">\(\eta_i\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-link-function">
<span class="eqno">(106)<a class="headerlink" href="#equation-link-function" title="Link to this equation"></a></span>\[g(\mu_i) = \eta_i = \mathbf{x}_i^\top \boldsymbol{\beta}\]</div>
<p>The link function must be monotonic and differentiable. Its inverse <span class="math notranslate nohighlight">\(g^{-1}\)</span> gives the <strong>mean function</strong>:</p>
<div class="math notranslate nohighlight">
\[\mu_i = g^{-1}(\eta_i) = g^{-1}(\mathbf{x}_i^\top \boldsymbol{\beta})\]</div>
<div class="note admonition">
<p class="admonition-title">Definition: Canonical Link</p>
<p>The <strong>canonical link</strong> is the link function that equates the linear predictor to the canonical parameter:</p>
<div class="math notranslate nohighlight">
\[g(\mu) = \theta\]</div>
<p>Since <span class="math notranslate nohighlight">\(\mu = b'(\theta)\)</span>, the canonical link is <span class="math notranslate nohighlight">\(g = (b')^{-1}\)</span>. Equivalently, <span class="math notranslate nohighlight">\(g'(\mu) = 1/b''(\theta) = 1/V(\mu)\)</span>.</p>
</div>
<p>For each exponential family distribution, there is exactly one canonical link:</p>
<table class="docutils align-default" id="id4">
<caption><span class="caption-number">Table 36 </span><span class="caption-text">Canonical and Common Alternative Links</span><a class="headerlink" href="#id4" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 30.0%" />
<col style="width: 50.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Distribution</p></th>
<th class="head"><p>Canonical Link</p></th>
<th class="head"><p>Common Alternatives</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Normal</p></td>
<td><p>Identity: <span class="math notranslate nohighlight">\(g(\mu) = \mu\)</span></p></td>
<td><p>(Rarely changed)</p></td>
</tr>
<tr class="row-odd"><td><p>Bernoulli/Binomial</p></td>
<td><p>Logit: <span class="math notranslate nohighlight">\(g(\mu) = \log\frac{\mu}{1-\mu}\)</span></p></td>
<td><p>Probit: <span class="math notranslate nohighlight">\(\Phi^{-1}(\mu)\)</span>, Complementary log-log</p></td>
</tr>
<tr class="row-even"><td><p>Poisson</p></td>
<td><p>Log: <span class="math notranslate nohighlight">\(g(\mu) = \log\mu\)</span></p></td>
<td><p>Identity (for additive models), Square root</p></td>
</tr>
<tr class="row-odd"><td><p>Gamma</p></td>
<td><p>Inverse: <span class="math notranslate nohighlight">\(g(\mu) = -1/\mu\)</span></p></td>
<td><p>Log (most common in practice), Identity</p></td>
</tr>
<tr class="row-even"><td><p>Inverse Gaussian</p></td>
<td><p><span class="math notranslate nohighlight">\(g(\mu) = -1/(2\mu^2)\)</span></p></td>
<td><p>Log, Inverse: <span class="math notranslate nohighlight">\(1/\mu^2\)</span> (absorbs constant)</p></td>
</tr>
</tbody>
</table>
</section>
<section id="canonical-links-computational-advantages">
<h3>Canonical Links: Computational Advantages<a class="headerlink" href="#canonical-links-computational-advantages" title="Link to this heading"></a></h3>
<p>Using the canonical link provides substantial computational benefits. When <span class="math notranslate nohighlight">\(\eta_i = \theta_i\)</span>:</p>
<ol class="arabic">
<li><p><strong>Sufficient statistic simplification</strong>: The sufficient statistic for <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> is <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{y}\)</span>, independent of <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>.</p></li>
<li><p><strong>Concave log-likelihood</strong>: The log-likelihood is globally concave in <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>, guaranteeing a unique maximum (when it exists).</p></li>
<li><p><strong>Information matrix equality</strong>: The observed Fisher information equals the expected Fisher information:</p>
<div class="math notranslate nohighlight">
\[-\frac{\partial^2 \ell}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^\top} = \mathbb{E}\left[-\frac{\partial^2 \ell}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^\top}\right] = \mathbf{I}(\boldsymbol{\beta})\]</div>
<p>This means Newton-Raphson and Fisher scoring produce identical iterations.</p>
</li>
<li><p><strong>Simplified score equations</strong>: The score function takes the elegant form <span class="math notranslate nohighlight">\(\mathbf{U}(\boldsymbol{\beta}) = \mathbf{X}^\top(\mathbf{y} - \boldsymbol{\mu})/\phi\)</span>.</p></li>
</ol>
<p>Despite these advantages, non-canonical links are sometimes preferred for interpretability or to ensure predictions stay within valid ranges. The log link for Gamma regression, for instance, guarantees positive fitted means and provides multiplicative interpretations of coefficients—advantages that often outweigh the computational simplicity of the canonical inverse link.</p>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_5_fig01_glm_components_diagram.png"><img alt="GLM three-component framework showing random component, systematic component, and link function" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_5_fig01_glm_components_diagram.png" style="width: 85%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 116 </span><span class="caption-text"><strong>Figure 3.5.1</strong>: The three-component GLM framework. The random component specifies the distribution of <span class="math notranslate nohighlight">\(Y\)</span> (exponential family). The systematic component combines predictors linearly into <span class="math notranslate nohighlight">\(\eta = \mathbf{X}\boldsymbol{\beta}\)</span>. The link function <span class="math notranslate nohighlight">\(g\)</span> connects them: <span class="math notranslate nohighlight">\(g(\mu) = \eta\)</span>. The canonical link sets <span class="math notranslate nohighlight">\(\eta = \theta\)</span>, yielding computational simplifications.</span><a class="headerlink" href="#id5" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="score-equations-and-fisher-information">
<h2>Score Equations and Fisher Information<a class="headerlink" href="#score-equations-and-fisher-information" title="Link to this heading"></a></h2>
<p>Maximum likelihood estimation for GLMs requires solving a system of nonlinear equations. In this section, we derive the score function and Fisher information matrix, which form the foundation for the IRLS algorithm.</p>
<section id="the-log-likelihood">
<h3>The Log-Likelihood<a class="headerlink" href="#the-log-likelihood" title="Link to this heading"></a></h3>
<p>For <span class="math notranslate nohighlight">\(n\)</span> independent observations <span class="math notranslate nohighlight">\((y_i, \mathbf{x}_i)\)</span>, the log-likelihood is:</p>
<div class="math notranslate nohighlight" id="equation-glm-loglik">
<span class="eqno">(107)<a class="headerlink" href="#equation-glm-loglik" title="Link to this equation"></a></span>\[\ell(\boldsymbol{\beta}) = \sum_{i=1}^n \ell_i(\boldsymbol{\beta}) = \sum_{i=1}^n \left[ \frac{y_i \theta_i - b(\theta_i)}{\phi} + c(y_i, \phi) \right]\]</div>
<p>where <span class="math notranslate nohighlight">\(\theta_i = \theta(\mu_i)\)</span> and <span class="math notranslate nohighlight">\(\mu_i = g^{-1}(\mathbf{x}_i^\top\boldsymbol{\beta})\)</span>. The parameter <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> enters through the chain:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\beta} \xrightarrow{\text{linear predictor}} \eta_i = \mathbf{x}_i^\top\boldsymbol{\beta} \xrightarrow{\text{inverse link}} \mu_i = g^{-1}(\eta_i) \xrightarrow{\text{canonical param}} \theta_i = \theta(\mu_i)\]</div>
</section>
<section id="deriving-the-score-function">
<h3>Deriving the Score Function<a class="headerlink" href="#deriving-the-score-function" title="Link to this heading"></a></h3>
<p>The <strong>score function</strong> is the gradient of the log-likelihood with respect to <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>. Applying the chain rule:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial \ell}{\partial \beta_j} = \sum_{i=1}^n \frac{\partial \ell_i}{\partial \theta_i} \cdot \frac{\partial \theta_i}{\partial \mu_i} \cdot \frac{\partial \mu_i}{\partial \eta_i} \cdot \frac{\partial \eta_i}{\partial \beta_j}\]</div>
<p>Let us compute each factor:</p>
<ol class="arabic">
<li><p><strong>Derivative with respect to</strong> <span class="math notranslate nohighlight">\(\theta_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial \ell_i}{\partial \theta_i} = \frac{y_i - b'(\theta_i)}{\phi} = \frac{y_i - \mu_i}{\phi}\]</div>
<p>since <span class="math notranslate nohighlight">\(\mu_i = b'(\theta_i)\)</span>.</p>
</li>
<li><p><strong>Derivative of</strong> <span class="math notranslate nohighlight">\(\theta_i\)</span> <strong>with respect to</strong> <span class="math notranslate nohighlight">\(\mu_i\)</span>:</p>
<p>Since <span class="math notranslate nohighlight">\(\mu = b'(\theta)\)</span>, by inverse function differentiation:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial \theta_i}{\partial \mu_i} = \frac{1}{b''(\theta_i)} = \frac{1}{V(\mu_i)}\]</div>
</li>
<li><p><strong>Derivative of</strong> <span class="math notranslate nohighlight">\(\mu_i\)</span> <strong>with respect to</strong> <span class="math notranslate nohighlight">\(\eta_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial \mu_i}{\partial \eta_i} = \frac{1}{g'(\mu_i)}\]</div>
<p>where <span class="math notranslate nohighlight">\(g'(\mu)\)</span> is the derivative of the link function.</p>
</li>
<li><p><strong>Derivative of</strong> <span class="math notranslate nohighlight">\(\eta_i\)</span> <strong>with respect to</strong> <span class="math notranslate nohighlight">\(\beta_j\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial \eta_i}{\partial \beta_j} = x_{ij}\]</div>
</li>
</ol>
<p>Combining these:</p>
<div class="math notranslate nohighlight" id="equation-score-component">
<span class="eqno">(108)<a class="headerlink" href="#equation-score-component" title="Link to this equation"></a></span>\[\frac{\partial \ell}{\partial \beta_j} = \sum_{i=1}^n \frac{y_i - \mu_i}{\phi \cdot V(\mu_i) \cdot g'(\mu_i)} \cdot x_{ij}\]</div>
<p>Define the <strong>working weight</strong> for observation <span class="math notranslate nohighlight">\(i\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-working-weight">
<span class="eqno">(109)<a class="headerlink" href="#equation-working-weight" title="Link to this equation"></a></span>\[w_i = \frac{1}{\phi \cdot V(\mu_i) \cdot [g'(\mu_i)]^2}\]</div>
<p>Then the score equations <span class="math notranslate nohighlight">\(\partial \ell / \partial \beta_j = 0\)</span> become:</p>
<div class="math notranslate nohighlight" id="equation-score-equations">
<span class="eqno">(110)<a class="headerlink" href="#equation-score-equations" title="Link to this equation"></a></span>\[\boxed{\sum_{i=1}^n w_i (y_i - \mu_i) \cdot g'(\mu_i) \cdot x_{ij} = 0, \quad j = 0, 1, \ldots, p-1}\]</div>
<div class="note admonition">
<p class="admonition-title">Note: Different Conventions in the Literature</p>
<p>Different authors package the weight factor differently. Some texts write <span class="math notranslate nohighlight">\(\sum_i (y_i - \mu_i) (\partial\mu_i/\partial\eta_i) x_{ij} / [V(\mu_i)\phi]\)</span>, others use <span class="math notranslate nohighlight">\(\mathbf{X}^\top \mathbf{W} (\mathbf{y} - \boldsymbol{\mu})\)</span> where <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> absorbs all variance and derivative terms. Our formulation <a class="reference internal" href="#equation-score-equations">(110)</a> explicitly separates the weight <span class="math notranslate nohighlight">\(w_i\)</span> from the link derivative <span class="math notranslate nohighlight">\(g'(\mu_i)\)</span>, making the connection to weighted least squares transparent. When translating between sources, verify how each defines its weight matrix—the coefficient estimates will be identical, but intermediate quantities may differ.</p>
</div>
</section>
<section id="matrix-form-of-score-equations">
<h3>Matrix Form of Score Equations<a class="headerlink" href="#matrix-form-of-score-equations" title="Link to this heading"></a></h3>
<p>Define diagonal weight matrix <span class="math notranslate nohighlight">\(\mathbf{W} = \text{diag}(w_1, \ldots, w_n)\)</span> and the diagonal matrix of link derivatives <span class="math notranslate nohighlight">\(\mathbf{G} = \text{diag}(g'(\mu_1), \ldots, g'(\mu_n))\)</span>. The score vector is:</p>
<div class="math notranslate nohighlight">
\[\mathbf{U}(\boldsymbol{\beta}) = \nabla_{\boldsymbol{\beta}} \ell = \mathbf{X}^\top \mathbf{W} \mathbf{G} (\mathbf{y} - \boldsymbol{\mu})\]</div>
<p>The score equations <span class="math notranslate nohighlight">\(\mathbf{U}(\boldsymbol{\beta}) = \mathbf{0}\)</span> generalize the normal equations from linear regression. When the link is canonical, <span class="math notranslate nohighlight">\(g'(\mu_i) = 1/V(\mu_i)\)</span>, so <span class="math notranslate nohighlight">\(w_i \cdot g'(\mu_i) = 1/(\phi \cdot V(\mu_i) \cdot g'(\mu_i)) = 1/\phi\)</span>, and the score simplifies to:</p>
<div class="math notranslate nohighlight">
\[\mathbf{U}(\boldsymbol{\beta}) = \frac{1}{\phi} \mathbf{X}^\top (\mathbf{y} - \boldsymbol{\mu}) \quad \text{(canonical link)}\]</div>
<p>This elegant form—residuals projected onto the column space of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>—directly generalizes the OLS normal equations.</p>
</section>
<section id="fisher-information-matrix">
<h3>Fisher Information Matrix<a class="headerlink" href="#fisher-information-matrix" title="Link to this heading"></a></h3>
<p>The <strong>Fisher information matrix</strong> is the expected negative Hessian of the log-likelihood:</p>
<div class="math notranslate nohighlight" id="equation-fisher-info">
<span class="eqno">(111)<a class="headerlink" href="#equation-fisher-info" title="Link to this equation"></a></span>\[\mathbf{I}(\boldsymbol{\beta}) = \mathbb{E}\left[ -\frac{\partial^2 \ell}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^\top} \right]\]</div>
<p>For GLMs, this takes the form:</p>
<div class="math notranslate nohighlight" id="equation-fisher-info-glm">
<span class="eqno">(112)<a class="headerlink" href="#equation-fisher-info-glm" title="Link to this equation"></a></span>\[\boxed{\mathbf{I}(\boldsymbol{\beta}) = \mathbf{X}^\top \mathbf{W} \mathbf{X}}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> is the diagonal weight matrix with entries <span class="math notranslate nohighlight">\(w_i\)</span> from <a class="reference internal" href="#equation-working-weight">(109)</a>.</p>
<p><strong>Proof sketch</strong>: The second derivative of <span class="math notranslate nohighlight">\(\ell_i\)</span> with respect to <span class="math notranslate nohighlight">\(\beta_j\)</span> and <span class="math notranslate nohighlight">\(\beta_k\)</span> involves terms from differentiating <span class="math notranslate nohighlight">\(\mu_i\)</span> and <span class="math notranslate nohighlight">\(w_i\)</span>. Taking expectations, terms involving <span class="math notranslate nohighlight">\((y_i - \mu_i)\)</span> vanish since <span class="math notranslate nohighlight">\(\mathbb{E}[Y_i] = \mu_i\)</span>. The remaining terms yield <span class="math notranslate nohighlight">\(-w_i x_{ij} x_{ik}\)</span>, and summing gives <span class="math notranslate nohighlight">\(-[\mathbf{X}^\top\mathbf{W}\mathbf{X}]_{jk}\)</span>.</p>
<p>For <strong>canonical links</strong>, the observed and expected information coincide:</p>
<div class="math notranslate nohighlight">
\[-\frac{\partial^2 \ell}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^\top} = \mathbf{X}^\top \mathbf{W} \mathbf{X} = \mathbf{I}(\boldsymbol{\beta})\]</div>
<p>This equality is a consequence of the sufficient statistic being linear in <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> and is one of the key computational advantages of canonical links.</p>
</section>
<section id="asymptotic-distribution-of-mles">
<h3>Asymptotic Distribution of MLEs<a class="headerlink" href="#asymptotic-distribution-of-mles" title="Link to this heading"></a></h3>
<p>Under standard regularity conditions, the MLE <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> is:</p>
<ol class="arabic">
<li><p><strong>Consistent</strong>: <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}} \xrightarrow{p} \boldsymbol{\beta}_0\)</span> as <span class="math notranslate nohighlight">\(n \to \infty\)</span></p></li>
<li><p><strong>Asymptotically normal</strong>:</p>
<div class="math notranslate nohighlight">
\[\sqrt{n}(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}_0) \xrightarrow{d} N(\mathbf{0}, \mathbf{I}(\boldsymbol{\beta}_0)^{-1})\]</div>
</li>
<li><p><strong>Asymptotically efficient</strong>: Achieves the Cramér-Rao lower bound.</p></li>
</ol>
<p>The estimated covariance matrix is:</p>
<div class="math notranslate nohighlight">
\[\widehat{\text{Var}}(\hat{\boldsymbol{\beta}}) = (\mathbf{X}^\top \hat{\mathbf{W}} \mathbf{X})^{-1}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{\mathbf{W}}\)</span> is the weight matrix evaluated at <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>. Standard errors are the square roots of the diagonal elements.</p>
</section>
</section>
<section id="iteratively-reweighted-least-squares">
<h2>Iteratively Reweighted Least Squares<a class="headerlink" href="#iteratively-reweighted-least-squares" title="Link to this heading"></a></h2>
<p>The score equations <a class="reference internal" href="#equation-score-equations">(110)</a> are nonlinear in <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> because the weights <span class="math notranslate nohighlight">\(w_i\)</span> and means <span class="math notranslate nohighlight">\(\mu_i\)</span> depend on <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>. Closed-form solutions generally do not exist, so we must resort to iterative numerical methods. The standard approach is <strong>Iteratively Reweighted Least Squares (IRLS)</strong>, which emerges naturally from Fisher scoring.</p>
<section id="from-fisher-scoring-to-irls">
<h3>From Fisher Scoring to IRLS<a class="headerlink" href="#from-fisher-scoring-to-irls" title="Link to this heading"></a></h3>
<p>Recall from Section 3.2 that <strong>Fisher scoring</strong> updates the parameter estimate using:</p>
<div class="math notranslate nohighlight" id="equation-fisher-scoring">
<span class="eqno">(113)<a class="headerlink" href="#equation-fisher-scoring" title="Link to this equation"></a></span>\[\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} + \mathbf{I}(\boldsymbol{\beta}^{(t)})^{-1} \mathbf{U}(\boldsymbol{\beta}^{(t)})\]</div>
<p>Substituting the GLM expressions for <span class="math notranslate nohighlight">\(\mathbf{I}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{U}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} + (\mathbf{X}^\top \mathbf{W}^{(t)} \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{W}^{(t)} \mathbf{G}^{(t)} (\mathbf{y} - \boldsymbol{\mu}^{(t)})\]</div>
<p>where superscript <span class="math notranslate nohighlight">\((t)\)</span> indicates evaluation at <span class="math notranslate nohighlight">\(\boldsymbol{\beta}^{(t)}\)</span>.</p>
<p>Now multiply through by <span class="math notranslate nohighlight">\(\mathbf{X}^\top \mathbf{W}^{(t)} \mathbf{X}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbf{X}^\top \mathbf{W}^{(t)} \mathbf{X} \boldsymbol{\beta}^{(t+1)} = \mathbf{X}^\top \mathbf{W}^{(t)} \mathbf{X} \boldsymbol{\beta}^{(t)} + \mathbf{X}^\top \mathbf{W}^{(t)} \mathbf{G}^{(t)} (\mathbf{y} - \boldsymbol{\mu}^{(t)})\]</div>
<p>Define the <strong>working response</strong> (adjusted dependent variable):</p>
<div class="math notranslate nohighlight" id="equation-working-response">
<span class="eqno">(114)<a class="headerlink" href="#equation-working-response" title="Link to this equation"></a></span>\[\boxed{z_i^{(t)} = \eta_i^{(t)} + (y_i - \mu_i^{(t)}) \cdot g'(\mu_i^{(t)})}\]</div>
<p>In vector form: <span class="math notranslate nohighlight">\(\mathbf{z}^{(t)} = \boldsymbol{\eta}^{(t)} + \mathbf{G}^{(t)}(\mathbf{y} - \boldsymbol{\mu}^{(t)})\)</span>.</p>
<p>Since <span class="math notranslate nohighlight">\(\boldsymbol{\eta}^{(t)} = \mathbf{X}\boldsymbol{\beta}^{(t)}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbf{X}^\top \mathbf{W}^{(t)} \mathbf{X} \boldsymbol{\beta}^{(t+1)} = \mathbf{X}^\top \mathbf{W}^{(t)} \mathbf{z}^{(t)}\]</div>
<p>This is exactly the <strong>normal equation for weighted least squares</strong> of <span class="math notranslate nohighlight">\(\mathbf{z}^{(t)}\)</span> on <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> with weights <span class="math notranslate nohighlight">\(\mathbf{W}^{(t)}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-irls-update">
<span class="eqno">(115)<a class="headerlink" href="#equation-irls-update" title="Link to this equation"></a></span>\[\boxed{\boldsymbol{\beta}^{(t+1)} = (\mathbf{X}^\top \mathbf{W}^{(t)} \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{W}^{(t)} \mathbf{z}^{(t)}}\]</div>
</section>
<section id="the-irls-algorithm">
<h3>The IRLS Algorithm<a class="headerlink" href="#the-irls-algorithm" title="Link to this heading"></a></h3>
<p>The derivation above reveals the IRLS algorithm: at each iteration, construct a working response and weights from the current fit, then solve a weighted least squares problem.</p>
<p><strong>Algorithm: Iteratively Reweighted Least Squares (IRLS)</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input: Design matrix X (n × p), response y, link function g, variance function V
       Dispersion φ, tolerance ε, maximum iterations max_iter
Output: MLE β̂, fitted means μ̂, convergence flag

1. Initialize: β⁽⁰⁾ ← starting values (e.g., from OLS on transformed y)

2. For t = 0, 1, 2, ... until convergence:

   a. Compute linear predictor: η⁽ᵗ⁾ = X β⁽ᵗ⁾

   b. Compute fitted means: μ⁽ᵗ⁾ = g⁻¹(η⁽ᵗ⁾)

   c. Compute weights: w_i⁽ᵗ⁾ = 1 / [φ · V(μ_i⁽ᵗ⁾) · (g&#39;(μ_i⁽ᵗ⁾))²]
      W⁽ᵗ⁾ = diag(w₁⁽ᵗ⁾, ..., wₙ⁽ᵗ⁾)

   d. Compute working response: z_i⁽ᵗ⁾ = η_i⁽ᵗ⁾ + (y_i - μ_i⁽ᵗ⁾) · g&#39;(μ_i⁽ᵗ⁾)

   e. Solve weighted least squares:
      β⁽ᵗ⁺¹⁾ = (X&#39;W⁽ᵗ⁾X)⁻¹ X&#39;W⁽ᵗ⁾ z⁽ᵗ⁾

   f. Check convergence: if ||β⁽ᵗ⁺¹⁾ - β⁽ᵗ⁾|| / ||β⁽ᵗ⁾|| &lt; ε, stop

   g. Check iteration limit: if t ≥ max_iter, warn and stop

3. Return β̂ = β⁽ᵗ⁺¹⁾, μ̂ = g⁻¹(X β̂), converged = (t &lt; max_iter)
</pre></div>
</div>
<p><strong>Computational Complexity</strong>: Each iteration requires <span class="math notranslate nohighlight">\(O(np^2)\)</span> operations for forming <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{W}\mathbf{X}\)</span> and <span class="math notranslate nohighlight">\(O(p^3)\)</span> for solving the linear system. Total complexity is <span class="math notranslate nohighlight">\(O(T(np^2 + p^3))\)</span> where <span class="math notranslate nohighlight">\(T\)</span> is the number of iterations. For well-behaved problems, <span class="math notranslate nohighlight">\(T\)</span> is typically 3-8.</p>
</section>
<section id="convergence-properties">
<h3>Convergence Properties<a class="headerlink" href="#convergence-properties" title="Link to this heading"></a></h3>
<p>IRLS inherits the convergence properties of Fisher scoring:</p>
<ol class="arabic simple">
<li><p><strong>Quadratic convergence</strong> near the solution for canonical links: the number of correct digits roughly doubles per iteration.</p></li>
<li><p><strong>Global convergence</strong> for canonical links when the MLE exists: the log-likelihood is concave, so any starting point eventually reaches the global maximum.</p></li>
<li><p><strong>Local convergence</strong> for non-canonical links: convergence is guaranteed only in a neighborhood of the MLE. Poor starting values may cause divergence.</p></li>
</ol>
<p>Typical stopping criteria include:</p>
<ul class="simple">
<li><p><strong>Relative parameter change</strong>: <span class="math notranslate nohighlight">\(\|\boldsymbol{\beta}^{(t+1)} - \boldsymbol{\beta}^{(t)}\| / \|\boldsymbol{\beta}^{(t)}\| &lt; 10^{-8}\)</span></p></li>
<li><p><strong>Relative deviance change</strong>: <span class="math notranslate nohighlight">\(|D^{(t+1)} - D^{(t)}| / |D^{(t)}| &lt; 10^{-8}\)</span></p></li>
<li><p><strong>Score norm</strong>: <span class="math notranslate nohighlight">\(\|\mathbf{U}(\boldsymbol{\beta}^{(t)})\| &lt; 10^{-6}\)</span></p></li>
</ul>
</section>
<section id="numerical-stability-considerations">
<h3>Numerical Stability Considerations<a class="headerlink" href="#numerical-stability-considerations" title="Link to this heading"></a></h3>
<p>Several issues can cause IRLS to fail or produce unreliable results:</p>
<p><strong>Extreme weights</strong>: When fitted probabilities approach 0 or 1 in logistic regression, the variance <span class="math notranslate nohighlight">\(V(\mu) = \mu(1-\mu)\)</span> approaches 0, causing weights <span class="math notranslate nohighlight">\(w_i \to \infty\)</span>. Simultaneously, the working response <span class="math notranslate nohighlight">\(z_i\)</span> can become extremely large. This typically signals <strong>separation</strong> (discussed below).</p>
<p><strong>Ill-conditioned</strong> <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{W}\mathbf{X}\)</span>: Multicollinearity or extreme weight variations can make the weighted normal equations numerically unstable. QR decomposition is more stable than forming and inverting <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{W}\mathbf{X}\)</span> directly.</p>
<p><strong>Boundary parameters</strong>: When the MLE lies at or near the boundary of the parameter space (e.g., a coefficient approaching <span class="math notranslate nohighlight">\(\pm\infty\)</span>), standard algorithms struggle.</p>
<p>Common remedies include:</p>
<ul class="simple">
<li><p><strong>Step-halving</strong>: If the deviance increases, reduce the step size by half until improvement occurs</p></li>
<li><p><strong>Ridge stabilization</strong>: Add <span class="math notranslate nohighlight">\(\lambda\mathbf{I}\)</span> to <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{W}\mathbf{X}\)</span> for small <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span></p></li>
<li><p><strong>Bound predictions</strong>: Clip fitted probabilities to <span class="math notranslate nohighlight">\([\epsilon, 1-\epsilon]\)</span> for small <span class="math notranslate nohighlight">\(\epsilon\)</span></p></li>
<li><p><strong>QR decomposition</strong>: Solve the weighted least squares problem via QR rather than normal equations</p></li>
</ul>
</section>
<section id="python-implementation">
<h3>Python Implementation<a class="headerlink" href="#python-implementation" title="Link to this heading"></a></h3>
<p>The following implementation demonstrates IRLS for a generic GLM:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">linalg</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">NamedTuple</span>

<span class="k">class</span><span class="w"> </span><span class="nc">GLMFamily</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Specification of a GLM family.&quot;&quot;&quot;</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">variance</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span>  <span class="c1"># V(μ)</span>
    <span class="n">link</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span>       <span class="c1"># g(μ)</span>
    <span class="n">link_deriv</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="c1"># g&#39;(μ)</span>
    <span class="n">link_inv</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span>   <span class="c1"># g⁻¹(η)</span>

<span class="c1"># Define common families</span>
<span class="n">BINOMIAL</span> <span class="o">=</span> <span class="n">GLMFamily</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;binomial&#39;</span><span class="p">,</span>
    <span class="n">variance</span><span class="o">=</span><span class="k">lambda</span> <span class="n">mu</span><span class="p">:</span> <span class="n">mu</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">mu</span><span class="p">),</span>
    <span class="n">link</span><span class="o">=</span><span class="k">lambda</span> <span class="n">mu</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">mu</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)),</span>          <span class="c1"># logit</span>
    <span class="n">link_deriv</span><span class="o">=</span><span class="k">lambda</span> <span class="n">mu</span><span class="p">:</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">mu</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)),</span>
    <span class="n">link_inv</span><span class="o">=</span><span class="k">lambda</span> <span class="n">eta</span><span class="p">:</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">eta</span><span class="p">))</span>     <span class="c1"># sigmoid</span>
<span class="p">)</span>

<span class="n">POISSON</span> <span class="o">=</span> <span class="n">GLMFamily</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;poisson&#39;</span><span class="p">,</span>
    <span class="n">variance</span><span class="o">=</span><span class="k">lambda</span> <span class="n">mu</span><span class="p">:</span> <span class="n">mu</span><span class="p">,</span>
    <span class="n">link</span><span class="o">=</span><span class="k">lambda</span> <span class="n">mu</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">mu</span><span class="p">),</span>                      <span class="c1"># log</span>
    <span class="n">link_deriv</span><span class="o">=</span><span class="k">lambda</span> <span class="n">mu</span><span class="p">:</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">mu</span><span class="p">,</span>
    <span class="n">link_inv</span><span class="o">=</span><span class="k">lambda</span> <span class="n">eta</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">eta</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">GAMMA_LOG</span> <span class="o">=</span> <span class="n">GLMFamily</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;gamma&#39;</span><span class="p">,</span>
    <span class="n">variance</span><span class="o">=</span><span class="k">lambda</span> <span class="n">mu</span><span class="p">:</span> <span class="n">mu</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">link</span><span class="o">=</span><span class="k">lambda</span> <span class="n">mu</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">mu</span><span class="p">),</span>                      <span class="c1"># log (not canonical)</span>
    <span class="n">link_deriv</span><span class="o">=</span><span class="k">lambda</span> <span class="n">mu</span><span class="p">:</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">mu</span><span class="p">,</span>
    <span class="n">link_inv</span><span class="o">=</span><span class="k">lambda</span> <span class="n">eta</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">eta</span><span class="p">)</span>
<span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">irls_fit</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">family</span><span class="p">:</span> <span class="n">GLMFamily</span><span class="p">,</span>
             <span class="n">phi</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">tol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">25</span><span class="p">,</span>
             <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fit a GLM using Iteratively Reweighted Least Squares.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : ndarray of shape (n, p)</span>
<span class="sd">        Design matrix (should include intercept column if desired)</span>
<span class="sd">    y : ndarray of shape (n,)</span>
<span class="sd">        Response vector</span>
<span class="sd">    family : GLMFamily</span>
<span class="sd">        GLM family specification</span>
<span class="sd">    phi : float</span>
<span class="sd">        Dispersion parameter (1 for binomial/Poisson)</span>
<span class="sd">    tol : float</span>
<span class="sd">        Convergence tolerance for relative parameter change</span>
<span class="sd">    max_iter : int</span>
<span class="sd">        Maximum number of iterations</span>
<span class="sd">    verbose : bool</span>
<span class="sd">        Print iteration information</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    beta : ndarray of shape (p,)</span>
<span class="sd">        Estimated coefficients</span>
<span class="sd">    mu : ndarray of shape (n,)</span>
<span class="sd">        Fitted means</span>
<span class="sd">    n_iter : int</span>
<span class="sd">        Number of iterations performed</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Initialize: use simple starting values</span>
    <span class="c1"># For binomial: start with empirical proportions</span>
    <span class="c1"># For Poisson/Gamma: start with log of y (adjusted for zeros)</span>
    <span class="k">if</span> <span class="n">family</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;binomial&#39;</span><span class="p">:</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>

    <span class="n">eta</span> <span class="o">=</span> <span class="n">family</span><span class="o">.</span><span class="n">link</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="kc">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
        <span class="c1"># Current linear predictor and mean</span>
        <span class="n">eta</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">family</span><span class="o">.</span><span class="n">link_inv</span><span class="p">(</span><span class="n">eta</span><span class="p">)</span>

        <span class="c1"># Numerical safeguards</span>
        <span class="k">if</span> <span class="n">family</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;binomial&#39;</span><span class="p">:</span>
            <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="mf">1e-10</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">)</span>

        <span class="c1"># Compute weights: w = 1 / [φ * V(μ) * (g&#39;(μ))²]</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">family</span><span class="o">.</span><span class="n">variance</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>
        <span class="n">g_prime</span> <span class="o">=</span> <span class="n">family</span><span class="o">.</span><span class="n">link_deriv</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>
        <span class="n">w</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">phi</span> <span class="o">*</span> <span class="n">V</span> <span class="o">*</span> <span class="n">g_prime</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Compute working response: z = η + (y - μ) * g&#39;(μ)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">eta</span> <span class="o">+</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">*</span> <span class="n">g_prime</span>

        <span class="c1"># Weighted least squares update</span>
        <span class="n">W_sqrt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="n">X_tilde</span> <span class="o">=</span> <span class="n">X</span> <span class="o">*</span> <span class="n">W_sqrt</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>  <span class="c1"># √W X</span>
        <span class="n">z_tilde</span> <span class="o">=</span> <span class="n">z</span> <span class="o">*</span> <span class="n">W_sqrt</span>                  <span class="c1"># √W z</span>

        <span class="c1"># Solve via QR for numerical stability</span>
        <span class="n">beta_new</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X_tilde</span><span class="p">,</span> <span class="n">z_tilde</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

        <span class="c1"># Check convergence</span>
        <span class="n">relative_change</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">beta_new</span> <span class="o">-</span> <span class="n">beta</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="n">deviance</span> <span class="o">=</span> <span class="n">compute_deviance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">family</span><span class="p">,</span> <span class="n">phi</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iteration </span><span class="si">{</span><span class="n">iteration</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: deviance = </span><span class="si">{</span><span class="n">deviance</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">, &quot;</span>
                  <span class="sa">f</span><span class="s2">&quot;relative change = </span><span class="si">{</span><span class="n">relative_change</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">beta</span> <span class="o">=</span> <span class="n">beta_new</span>

        <span class="k">if</span> <span class="n">relative_change</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Converged after </span><span class="si">{</span><span class="n">iteration</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2"> iterations&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">beta</span><span class="p">,</span> <span class="n">family</span><span class="o">.</span><span class="n">link_inv</span><span class="p">(</span><span class="n">X</span> <span class="o">@</span> <span class="n">beta</span><span class="p">),</span> <span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: IRLS did not converge after </span><span class="si">{</span><span class="n">max_iter</span><span class="si">}</span><span class="s2"> iterations&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">beta</span><span class="p">,</span> <span class="n">family</span><span class="o">.</span><span class="n">link_inv</span><span class="p">(</span><span class="n">X</span> <span class="o">@</span> <span class="n">beta</span><span class="p">),</span> <span class="n">max_iter</span>

<span class="k">def</span><span class="w"> </span><span class="nf">compute_deviance</span><span class="p">(</span><span class="n">y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">mu</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                     <span class="n">family</span><span class="p">:</span> <span class="n">GLMFamily</span><span class="p">,</span> <span class="n">phi</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the deviance for a fitted GLM.&quot;&quot;&quot;</span>
    <span class="c1"># Avoid log(0) issues</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-10</span>

    <span class="k">if</span> <span class="n">family</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;binomial&#39;</span><span class="p">:</span>
        <span class="c1"># D = 2 Σ [y log(y/μ) + (1-y) log((1-y)/(1-μ))]</span>
        <span class="n">y_safe</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">eps</span><span class="p">)</span>
        <span class="n">mu_safe</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">eps</span><span class="p">)</span>
        <span class="n">d</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
            <span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_safe</span> <span class="o">/</span> <span class="n">mu_safe</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">+</span>
            <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_safe</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">mu_safe</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">family</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;poisson&#39;</span><span class="p">:</span>
        <span class="c1"># D = 2 Σ [y log(y/μ) - (y - μ)]</span>
        <span class="n">y_safe</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
        <span class="n">d</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_safe</span> <span class="o">/</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">mu</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">family</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;gamma&#39;</span><span class="p">:</span>
        <span class="c1"># D = 2 Σ [-log(y/μ) + (y - μ)/μ]</span>
        <span class="n">d</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y</span> <span class="o">/</span> <span class="n">mu</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">/</span> <span class="n">mu</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Normal: D = Σ (y - μ)²</span>
        <span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">phi</span>

    <span class="k">return</span> <span class="n">d</span>
</pre></div>
</div>
<div class="note admonition">
<p class="admonition-title">Example 💡 IRLS Convergence for Logistic Regression</p>
<p><strong>Given</strong>: Simulated binary data with known coefficients <span class="math notranslate nohighlight">\(\beta_0 = -2, \beta_1 = 0.5\)</span>.</p>
<p><strong>Find</strong>: MLE via IRLS and observe convergence behavior.</p>
<p><strong>Python implementation</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Generate data</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">eta_true</span> <span class="o">=</span> <span class="o">-</span><span class="mf">2.0</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x</span>
<span class="n">p_true</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">eta_true</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p_true</span><span class="p">)</span>

<span class="c1"># Design matrix with intercept</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">x</span><span class="p">])</span>

<span class="c1"># Fit via IRLS</span>
<span class="n">beta_hat</span><span class="p">,</span> <span class="n">mu_hat</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">=</span> <span class="n">irls_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">BINOMIAL</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">True coefficients: β₀ = -2.0, β₁ = 0.5&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MLE estimates:     β₀ = </span><span class="si">{</span><span class="n">beta_hat</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, β₁ = </span><span class="si">{</span><span class="n">beta_hat</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Converged in </span><span class="si">{</span><span class="n">n_iter</span><span class="si">}</span><span class="s2"> iterations&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Result</strong>:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Iteration 1: deviance = 177.4231, relative change = 1.12e+00
Iteration 2: deviance = 169.8845, relative change = 1.67e-01
Iteration 3: deviance = 169.6012, relative change = 1.52e-02
Iteration 4: deviance = 169.5987, relative change = 1.41e-04
Iteration 5: deviance = 169.5987, relative change = 1.24e-08
Converged after 5 iterations

True coefficients: β₀ = -2.0, β₁ = 0.5
MLE estimates:     β₀ = -2.1893, β₁ = 0.5207
Converged in 5 iterations
</pre></div>
</div>
<p>The rapid convergence—relative change dropping from <span class="math notranslate nohighlight">\(10^0\)</span> to <span class="math notranslate nohighlight">\(10^{-8}\)</span> in just 5 iterations—demonstrates the quadratic convergence of IRLS for the canonical logit link.</p>
</div>
<figure class="align-center" id="id6">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_5_fig02_irls_convergence.png"><img alt="IRLS convergence showing deviance decrease and quadratic convergence rate" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_5_fig02_irls_convergence.png" style="width: 90%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 117 </span><span class="caption-text"><strong>Figure 3.5.2</strong>: IRLS convergence for logistic regression. (a) Deviance decreases monotonically, approaching the minimum in few iterations. (b) Log-scale error <span class="math notranslate nohighlight">\(\log_{10}|\boldsymbol{\beta}^{(t)} - \hat{\boldsymbol{\beta}}|\)</span> shows quadratic convergence—the slope doubles each iteration, meaning the number of correct digits roughly doubles.</span><a class="headerlink" href="#id6" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="logistic-regression-binary-outcomes">
<h2>Logistic Regression: Binary Outcomes<a class="headerlink" href="#logistic-regression-binary-outcomes" title="Link to this heading"></a></h2>
<p>Logistic regression is the most widely used GLM, modeling binary outcomes (yes/no, success/failure, 0/1) as functions of predictors. Its applications span medicine (disease diagnosis), marketing (customer churn), finance (credit default), and countless other domains.</p>
<section id="model-specification">
<h3>Model Specification<a class="headerlink" href="#model-specification" title="Link to this heading"></a></h3>
<p>For binary responses <span class="math notranslate nohighlight">\(Y_i \in \{0, 1\}\)</span>, we model <span class="math notranslate nohighlight">\(Y_i \sim \text{Bernoulli}(\pi_i)\)</span> where <span class="math notranslate nohighlight">\(\pi_i = P(Y_i = 1 | \mathbf{x}_i)\)</span>. The <strong>logit link</strong> (canonical for Bernoulli) connects <span class="math notranslate nohighlight">\(\pi_i\)</span> to the linear predictor:</p>
<div class="math notranslate nohighlight" id="equation-logit-link">
<span class="eqno">(116)<a class="headerlink" href="#equation-logit-link" title="Link to this equation"></a></span>\[\text{logit}(\pi_i) = \log\left(\frac{\pi_i}{1 - \pi_i}\right) = \mathbf{x}_i^\top \boldsymbol{\beta}\]</div>
<p>The quantity <span class="math notranslate nohighlight">\(\pi_i / (1 - \pi_i)\)</span> is the <strong>odds</strong> of success—the ratio of the probability of success to the probability of failure. The logit is the <strong>log-odds</strong>.</p>
<p>Inverting the link gives the <strong>logistic function</strong> (sigmoid):</p>
<div class="math notranslate nohighlight" id="equation-logistic-function">
<span class="eqno">(117)<a class="headerlink" href="#equation-logistic-function" title="Link to this equation"></a></span>\[\pi_i = \frac{1}{1 + e^{-\mathbf{x}_i^\top\boldsymbol{\beta}}} = \frac{e^{\mathbf{x}_i^\top\boldsymbol{\beta}}}{1 + e^{\mathbf{x}_i^\top\boldsymbol{\beta}}}\]</div>
<p>The S-shaped logistic curve maps the unbounded linear predictor <span class="math notranslate nohighlight">\(\eta \in (-\infty, \infty)\)</span> to the probability interval <span class="math notranslate nohighlight">\(\pi \in (0, 1)\)</span>.</p>
<p>For the Bernoulli distribution:</p>
<ul class="simple">
<li><p>Variance function: <span class="math notranslate nohighlight">\(V(\pi) = \pi(1 - \pi)\)</span></p></li>
<li><p>Dispersion: <span class="math notranslate nohighlight">\(\phi = 1\)</span> (fixed, not estimated)</p></li>
<li><p>Weight: <span class="math notranslate nohighlight">\(w_i = \pi_i(1 - \pi_i)\)</span> (for canonical logit link)</p></li>
</ul>
</section>
<section id="interpretation-odds-ratios">
<h3>Interpretation: Odds Ratios<a class="headerlink" href="#interpretation-odds-ratios" title="Link to this heading"></a></h3>
<p>Logistic regression coefficients have a natural interpretation in terms of <strong>odds ratios</strong>. Consider a single predictor:</p>
<div class="math notranslate nohighlight">
\[\log\left(\frac{\pi}{1-\pi}\right) = \beta_0 + \beta_1 x\]</div>
<p>For two values <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(x + 1\)</span>:</p>
<div class="math notranslate nohighlight">
\[\log\left(\frac{\pi(x+1)}{1-\pi(x+1)}\right) - \log\left(\frac{\pi(x)}{1-\pi(x)}\right) = \beta_1\]</div>
<p>Exponentiating:</p>
<div class="math notranslate nohighlight" id="equation-odds-ratio">
<span class="eqno">(118)<a class="headerlink" href="#equation-odds-ratio" title="Link to this equation"></a></span>\[\text{Odds Ratio} = \frac{\text{odds at } x+1}{\text{odds at } x} = e^{\beta_1}\]</div>
<p>Thus <span class="math notranslate nohighlight">\(e^{\beta_1}\)</span> is the <strong>multiplicative change in odds</strong> for a one-unit increase in <span class="math notranslate nohighlight">\(x\)</span>. If <span class="math notranslate nohighlight">\(\beta_1 = 0.5\)</span>, then <span class="math notranslate nohighlight">\(e^{0.5} \approx 1.65\)</span>: each unit increase in <span class="math notranslate nohighlight">\(x\)</span> multiplies the odds by 1.65 (a 65% increase).</p>
<p>For <strong>categorical predictors</strong> with dummy coding, <span class="math notranslate nohighlight">\(e^{\beta_j}\)</span> is the odds ratio comparing category <span class="math notranslate nohighlight">\(j\)</span> to the reference category.</p>
<p>In <strong>multiple logistic regression</strong>, each <span class="math notranslate nohighlight">\(e^{\beta_j}\)</span> is an <strong>adjusted odds ratio</strong>—the multiplicative change in odds per unit increase in <span class="math notranslate nohighlight">\(x_j\)</span>, <em>holding all other predictors constant</em>.</p>
<figure class="align-center" id="id7">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_5_fig03_logistic_curve.png"><img alt="Logistic regression S-curve with simulated binary data" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_5_fig03_logistic_curve.png" style="width: 85%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 118 </span><span class="caption-text"><strong>Figure 3.5.3</strong>: Logistic regression with simulated data. Points show binary outcomes <span class="math notranslate nohighlight">\(y \in \{0, 1\}\)</span> (jittered vertically for visibility). The S-shaped curve is the fitted probability <span class="math notranslate nohighlight">\(\hat{\pi}(x) = 1/(1 + e^{-\hat{\eta}})\)</span>. As <span class="math notranslate nohighlight">\(x\)</span> increases, the probability transitions smoothly from near 0 to near 1. The inflection point occurs where <span class="math notranslate nohighlight">\(\hat{\eta} = 0\)</span>, i.e., <span class="math notranslate nohighlight">\(x = -\hat{\beta}_0/\hat{\beta}_1\)</span>.</span><a class="headerlink" href="#id7" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="the-separation-problem">
<h3>The Separation Problem<a class="headerlink" href="#the-separation-problem" title="Link to this heading"></a></h3>
<p>A unique challenge in logistic regression is <strong>separation</strong>—when a linear combination of predictors perfectly predicts the binary outcome. When this occurs, the MLE does not exist in the usual sense: coefficients diverge to <span class="math notranslate nohighlight">\(\pm\infty\)</span> while the likelihood continues increasing.</p>
<p><strong>Complete separation</strong> occurs when the classes can be perfectly separated by a hyperplane:</p>
<div class="math notranslate nohighlight">
\[y_i = 1 \implies \mathbf{x}_i^\top\boldsymbol{\beta} &gt; 0, \quad y_i = 0 \implies \mathbf{x}_i^\top\boldsymbol{\beta} &lt; 0\]</div>
<p>for some <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>. In this case, scaling <span class="math notranslate nohighlight">\(\boldsymbol{\beta} \to c\boldsymbol{\beta}\)</span> with <span class="math notranslate nohighlight">\(c \to \infty\)</span> drives all predicted probabilities toward their observed values (0 or 1), and the log-likelihood approaches 0 from below.</p>
<p><strong>Quasi-complete separation</strong> occurs when separation exists only at boundary points—some observations lie exactly on the separating hyperplane.</p>
<p><strong>Symptoms of separation</strong>:</p>
<ul class="simple">
<li><p>Very large coefficient estimates (<span class="math notranslate nohighlight">\(|\hat{\beta}| &gt; 10\)</span>)</p></li>
<li><p>Extremely large standard errors (sometimes reported as infinity)</p></li>
<li><p>Convergence warnings or failure to converge</p></li>
<li><p>Fitted probabilities exactly 0 or 1</p></li>
</ul>
<p><strong>Firth’s Penalized Likelihood</strong></p>
<p>David Firth (1993) proposed a penalized likelihood that removes first-order bias and produces finite estimates even under separation:</p>
<div class="math notranslate nohighlight" id="equation-firth-penalty">
<span class="eqno">(119)<a class="headerlink" href="#equation-firth-penalty" title="Link to this equation"></a></span>\[\ell^*(\boldsymbol{\beta}) = \ell(\boldsymbol{\beta}) + \frac{1}{2}\log|\mathbf{I}(\boldsymbol{\beta})|\]</div>
<p>The penalty <span class="math notranslate nohighlight">\(\frac{1}{2}\log|\mathbf{I}(\boldsymbol{\beta})|\)</span> is the Jeffreys prior in Bayesian terms. The modified score equations are:</p>
<div class="math notranslate nohighlight">
\[U_j^*(\boldsymbol{\beta}) = U_j(\boldsymbol{\beta}) + \frac{1}{2}\text{tr}\left(\mathbf{I}^{-1}\frac{\partial \mathbf{I}}{\partial \beta_j}\right)\]</div>
<p>For logistic regression, this simplifies to adding a term involving the hat matrix diagonal <span class="math notranslate nohighlight">\(h_{ii}\)</span>:</p>
<div class="math notranslate nohighlight">
\[U_j^*(\boldsymbol{\beta}) = \sum_{i=1}^n \left(y_i - \pi_i + h_{ii}\left(\frac{1}{2} - \pi_i\right)\right) x_{ij}\]</div>
<p>Firth’s method is not available as a built-in option in standard Python libraries like statsmodels or sklearn. For production use, implement the modified score equations directly as shown in Exercise 3.5.3, use L2 regularization (<code class="docutils literal notranslate"><span class="pre">LogisticRegression(penalty='l2',</span> <span class="pre">C=10)</span></code>) as a practical approximation, or install the <code class="docutils literal notranslate"><span class="pre">firthlogist</span></code> package from PyPI which provides a pure Python implementation.</p>
<figure class="align-center" id="id8">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_5_fig04_separation_illustration.png"><img alt="Complete separation in logistic regression showing diverging coefficients" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_5_fig04_separation_illustration.png" style="width: 90%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 119 </span><span class="caption-text"><strong>Figure 3.5.4</strong>: Separation in logistic regression. (a) Separable data: all <span class="math notranslate nohighlight">\(y=1\)</span> cases have <span class="math notranslate nohighlight">\(x &gt; 5\)</span>, all <span class="math notranslate nohighlight">\(y=0\)</span> cases have <span class="math notranslate nohighlight">\(x &lt; 5\)</span>. No finite MLE exists. (b) Coefficient estimates vs. iteration: <span class="math notranslate nohighlight">\(|\hat{\beta}_1|\)</span> grows without bound as IRLS attempts to achieve perfect separation. (c) Standard MLE produces infinite coefficients; Firth’s method produces finite, interpretable estimates.</span><a class="headerlink" href="#id8" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="worked-example-logistic-regression-in-python">
<h3>Worked Example: Logistic Regression in Python<a class="headerlink" href="#worked-example-logistic-regression-in-python" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Generate data</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">eta_true</span> <span class="o">=</span> <span class="o">-</span><span class="mf">3.0</span> <span class="o">+</span> <span class="mf">0.6</span> <span class="o">*</span> <span class="n">x</span>
<span class="n">p_true</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">eta_true</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p_true</span><span class="p">)</span>

<span class="c1"># Fit with statsmodels</span>
<span class="n">X_sm</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">logit_model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X_sm</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Binomial</span><span class="p">())</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">logit_model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== Statsmodels GLM Results ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>

<span class="c1"># Extract key results</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Coefficients: β₀ = </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, β₁ = </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Standard errors: SE(β₀) = </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">bse</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, SE(β₁) = </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">bse</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Odds ratio for x: exp(β₁) = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Deviance: </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">deviance</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;AIC: </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">aic</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Fit with scikit-learn (for comparison)</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== Scikit-learn Results ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Coefficients: β₀ = </span><span class="si">{</span><span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, β₁ = </span><span class="si">{</span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Predictions</span>
<span class="n">x_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">X_pred</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">x_pred</span><span class="p">)</span>
<span class="n">p_pred</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_pred</span><span class="p">)</span>

<span class="c1"># Plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Data&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">p_pred</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Fitted probability&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="o">-</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;P(Y = 1)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Logistic Regression Fit&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;logistic_fit.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="poisson-regression-count-data">
<h2>Poisson Regression: Count Data<a class="headerlink" href="#poisson-regression-count-data" title="Link to this heading"></a></h2>
<p>Poisson regression models count outcomes—non-negative integers representing the number of events occurring in a fixed period or region. Applications include modeling the number of insurance claims, website visits, disease cases, or manufacturing defects.</p>
<section id="id1">
<h3>Model Specification<a class="headerlink" href="#id1" title="Link to this heading"></a></h3>
<p>For count responses <span class="math notranslate nohighlight">\(Y_i \in \{0, 1, 2, \ldots\}\)</span>, we model <span class="math notranslate nohighlight">\(Y_i \sim \text{Poisson}(\mu_i)\)</span>. The <strong>log link</strong> (canonical for Poisson) connects <span class="math notranslate nohighlight">\(\mu_i\)</span> to the linear predictor:</p>
<div class="math notranslate nohighlight" id="equation-log-link">
<span class="eqno">(120)<a class="headerlink" href="#equation-log-link" title="Link to this equation"></a></span>\[\log(\mu_i) = \mathbf{x}_i^\top \boldsymbol{\beta}\]</div>
<p>Equivalently:</p>
<div class="math notranslate nohighlight">
\[\mu_i = e^{\mathbf{x}_i^\top\boldsymbol{\beta}}\]</div>
<p>This ensures <span class="math notranslate nohighlight">\(\mu_i &gt; 0\)</span> as required for a Poisson mean.</p>
<p>For the Poisson distribution:</p>
<ul class="simple">
<li><p>Variance function: <span class="math notranslate nohighlight">\(V(\mu) = \mu\)</span></p></li>
<li><p>Dispersion: <span class="math notranslate nohighlight">\(\phi = 1\)</span> (fixed)</p></li>
<li><p>Weight: <span class="math notranslate nohighlight">\(w_i = \mu_i\)</span> (for canonical log link)</p></li>
</ul>
<p>The Poisson assumption implies <span class="math notranslate nohighlight">\(\text{Var}(Y_i) = \mathbb{E}[Y_i] = \mu_i\)</span>—the variance equals the mean. This <strong>equidispersion</strong> property is often violated in practice.</p>
</section>
<section id="interpretation-rate-ratios">
<h3>Interpretation: Rate Ratios<a class="headerlink" href="#interpretation-rate-ratios" title="Link to this heading"></a></h3>
<p>Poisson regression coefficients have multiplicative interpretations. For a single predictor:</p>
<div class="math notranslate nohighlight">
\[\log\mu(x+1) - \log\mu(x) = \beta_1\]</div>
<p>Exponentiating:</p>
<div class="math notranslate nohighlight" id="equation-rate-ratio">
<span class="eqno">(121)<a class="headerlink" href="#equation-rate-ratio" title="Link to this equation"></a></span>\[\text{Rate Ratio} = \frac{\mu(x+1)}{\mu(x)} = e^{\beta_1}\]</div>
<p>Thus <span class="math notranslate nohighlight">\(e^{\beta_1}\)</span> is the <strong>multiplicative change in expected count</strong> for a one-unit increase in <span class="math notranslate nohighlight">\(x\)</span>. If <span class="math notranslate nohighlight">\(\beta_1 = 0.3\)</span>, then <span class="math notranslate nohighlight">\(e^{0.3} \approx 1.35\)</span>: each unit increase in <span class="math notranslate nohighlight">\(x\)</span> multiplies the expected count by 1.35 (a 35% increase).</p>
</section>
<section id="offsets-for-rate-modeling">
<h3>Offsets for Rate Modeling<a class="headerlink" href="#offsets-for-rate-modeling" title="Link to this heading"></a></h3>
<p>Often we model <strong>rates</strong> rather than raw counts. If <span class="math notranslate nohighlight">\(Y_i\)</span> is the count and <span class="math notranslate nohighlight">\(t_i\)</span> is the exposure (time period, population size, area), then:</p>
<div class="math notranslate nohighlight">
\[Y_i \sim \text{Poisson}(t_i \lambda_i)\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda_i\)</span> is the rate. Taking logarithms:</p>
<div class="math notranslate nohighlight">
\[\log(\mu_i) = \log(t_i \lambda_i) = \log(t_i) + \log(\lambda_i) = \log(t_i) + \mathbf{x}_i^\top\boldsymbol{\beta}\]</div>
<p>The term <span class="math notranslate nohighlight">\(\log(t_i)\)</span> is called an <strong>offset</strong>—it enters the linear predictor with coefficient fixed at 1. In software:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Statsmodels</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Poisson</span><span class="p">(),</span>
               <span class="n">offset</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">exposure</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="the-overdispersion-problem">
<h3>The Overdispersion Problem<a class="headerlink" href="#the-overdispersion-problem" title="Link to this heading"></a></h3>
<p>Real count data often exhibits <strong>overdispersion</strong>: <span class="math notranslate nohighlight">\(\text{Var}(Y_i) &gt; \mathbb{E}[Y_i]\)</span>. Causes include:</p>
<ul class="simple">
<li><p><strong>Unobserved heterogeneity</strong>: Unmeasured factors cause variation in rates across observations</p></li>
<li><p><strong>Clustering</strong>: Observations within groups are correlated</p></li>
<li><p><strong>Zero-inflation</strong>: Excess zeros beyond what Poisson predicts</p></li>
</ul>
<p><strong>Detecting overdispersion</strong>: Compare the Pearson chi-square statistic to its degrees of freedom:</p>
<div class="math notranslate nohighlight">
\[\hat{\phi} = \frac{X^2}{n - p} = \frac{\sum_{i=1}^n (y_i - \hat{\mu}_i)^2 / \hat{\mu}_i}{n - p}\]</div>
<p>Under correct specification, <span class="math notranslate nohighlight">\(\hat{\phi} \approx 1\)</span>. Values substantially greater than 1 indicate overdispersion.</p>
<p><strong>Consequences of ignoring overdispersion</strong>:</p>
<ul class="simple">
<li><p>Standard errors are too small</p></li>
<li><p>Confidence intervals are too narrow</p></li>
<li><p>p-values are too small</p></li>
<li><p>Type I error rate inflated</p></li>
</ul>
<p><strong>Remedies</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Quasi-Poisson</strong>: Estimate <span class="math notranslate nohighlight">\(\phi\)</span> from data and inflate standard errors by <span class="math notranslate nohighlight">\(\sqrt{\hat{\phi}}\)</span></p></li>
<li><p><strong>Negative Binomial regression</strong>: Model <span class="math notranslate nohighlight">\(Y_i \sim \text{NegBin}(\mu_i, \alpha)\)</span> with <span class="math notranslate nohighlight">\(\text{Var}(Y_i) = \mu_i + \alpha\mu_i^2\)</span></p></li>
<li><p><strong>Robust standard errors</strong>: Use sandwich estimator (see Section 3.5.10)</p></li>
</ol>
<figure class="align-center" id="id9">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_5_fig05_poisson_overdispersion.png"><img alt="Comparison of Poisson-distributed data vs overdispersed data" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_5_fig05_poisson_overdispersion.png" style="width: 90%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 120 </span><span class="caption-text"><strong>Figure 3.5.5</strong>: Poisson regression and overdispersion. (a) Well-behaved Poisson data: variance approximately equals mean, residuals show no pattern. (b) Overdispersed data: variance exceeds mean, Pearson <span class="math notranslate nohighlight">\(\chi^2/\text{df} = 3.2\)</span>. Standard Poisson SEs are too small; quasi-Poisson or negative binomial provides correct inference.</span><a class="headerlink" href="#id9" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="worked-example-poisson-regression">
<h3>Worked Example: Poisson Regression<a class="headerlink" href="#worked-example-poisson-regression" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>

<span class="c1"># Generate Poisson data</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">101</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">mu_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">+</span> <span class="mf">0.8</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="n">mu_true</span><span class="p">)</span>

<span class="c1"># Fit Poisson GLM</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">poisson_model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Poisson</span><span class="p">())</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">poisson_model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== Poisson Regression Results ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Coefficients: β₀ = </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, β₁ = </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True values:  β₀ = 0.5000, β₁ = 0.8000&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Rate ratio for x: exp(β₁) = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;(Each unit increase in x multiplies expected count by </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># Check for overdispersion</span>
<span class="n">pearson_chi2</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">pearson_chi2</span>
<span class="n">df_resid</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">df_resid</span>
<span class="n">phi_hat</span> <span class="o">=</span> <span class="n">pearson_chi2</span> <span class="o">/</span> <span class="n">df_resid</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== Overdispersion Check ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Pearson χ²: </span><span class="si">{</span><span class="n">pearson_chi2</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Degrees of freedom: </span><span class="si">{</span><span class="n">df_resid</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimated φ = χ²/df: </span><span class="si">{</span><span class="n">phi_hat</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Conclusion: </span><span class="si">{</span><span class="s1">&#39;Overdispersion detected&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">phi_hat</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mf">1.5</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;No strong overdispersion&#39;</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="gamma-regression-positive-continuous-data">
<h2>Gamma Regression: Positive Continuous Data<a class="headerlink" href="#gamma-regression-positive-continuous-data" title="Link to this heading"></a></h2>
<p>Gamma regression models positive continuous responses, particularly when variance increases with the mean. It is widely used in insurance (claim amounts), reliability engineering (failure times), and economics (income, costs).</p>
<section id="id2">
<h3>Model Specification<a class="headerlink" href="#id2" title="Link to this heading"></a></h3>
<p>For positive continuous <span class="math notranslate nohighlight">\(Y_i &gt; 0\)</span>, we model <span class="math notranslate nohighlight">\(Y_i \sim \text{Gamma}(\alpha, \mu_i)\)</span> with shape <span class="math notranslate nohighlight">\(\alpha\)</span> and mean <span class="math notranslate nohighlight">\(\mu_i\)</span>. The variance is:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(Y_i) = \frac{\mu_i^2}{\alpha} = \phi \mu_i^2\]</div>
<p>where <span class="math notranslate nohighlight">\(\phi = 1/\alpha\)</span> is the dispersion parameter. This implies a <strong>constant coefficient of variation</strong>:</p>
<div class="math notranslate nohighlight">
\[\text{CV}(Y_i) = \frac{\sqrt{\text{Var}(Y_i)}}{\mathbb{E}[Y_i]} = \frac{1}{\sqrt{\alpha}}\]</div>
<p>The <strong>canonical link</strong> for Gamma is the inverse <span class="math notranslate nohighlight">\(g(\mu) = -1/\mu\)</span>, but the <strong>log link</strong> is almost always preferred:</p>
<div class="math notranslate nohighlight" id="equation-gamma-log-link">
<span class="eqno">(122)<a class="headerlink" href="#equation-gamma-log-link" title="Link to this equation"></a></span>\[\log(\mu_i) = \mathbf{x}_i^\top\boldsymbol{\beta}\]</div>
<p>The log link ensures <span class="math notranslate nohighlight">\(\mu_i &gt; 0\)</span> and provides multiplicative coefficient interpretations, just as in Poisson regression.</p>
<p>For the Gamma distribution with log link:</p>
<ul class="simple">
<li><p>Variance function: <span class="math notranslate nohighlight">\(V(\mu) = \mu^2\)</span></p></li>
<li><p>Dispersion: <span class="math notranslate nohighlight">\(\phi = 1/\alpha\)</span> (estimated from data)</p></li>
<li><p>Weight factor: <span class="math notranslate nohighlight">\(V(\mu) \cdot (g'(\mu))^2 = \mu^2 \cdot (1/\mu)^2 = 1\)</span>, so <span class="math notranslate nohighlight">\(w_i = 1/\phi\)</span> for all <span class="math notranslate nohighlight">\(i\)</span></p></li>
</ul>
<p>The key insight is that weights do not vary across observations—a constant multiplier on all weights does not change the WLS coefficient solution, so IRLS behaves like unweighted least squares on the working response (though standard errors still depend on <span class="math notranslate nohighlight">\(\phi\)</span>).</p>
</section>
<section id="interpretation">
<h3>Interpretation<a class="headerlink" href="#interpretation" title="Link to this heading"></a></h3>
<p>With a log link, <span class="math notranslate nohighlight">\(e^{\beta_j}\)</span> is the multiplicative change in expected response for a one-unit increase in <span class="math notranslate nohighlight">\(x_j\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac{\mu(x_j + 1)}{\mu(x_j)} = e^{\beta_j}\]</div>
<p>If <span class="math notranslate nohighlight">\(\beta_1 = 0.4\)</span>, then <span class="math notranslate nohighlight">\(e^{0.4} \approx 1.49\)</span>: each unit increase multiplies the expected response by 1.49 (a 49% increase).</p>
<figure class="align-center" id="id10">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_5_fig06_gamma_regression.png"><img alt="Gamma regression showing increasing variance with mean" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_5_fig06_gamma_regression.png" style="width: 85%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 121 </span><span class="caption-text"><strong>Figure 3.5.6</strong>: Gamma regression for positive continuous data. The exponential mean function <span class="math notranslate nohighlight">\(\hat{\mu}(x) = \exp(\hat{\beta}_0 + \hat{\beta}_1 x)\)</span> fits the data well. Note how the spread of observations increases with <span class="math notranslate nohighlight">\(x\)</span>—this heteroskedasticity (variance proportional to <span class="math notranslate nohighlight">\(\mu^2\)</span>) is exactly what the Gamma model accommodates.</span><a class="headerlink" href="#id10" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="worked-example-gamma-regression">
<h3>Worked Example: Gamma Regression<a class="headerlink" href="#worked-example-gamma-regression" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>

<span class="c1"># Generate Gamma data</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">202</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">mu_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">+</span> <span class="mf">0.4</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">5.0</span>  <span class="c1"># shape parameter</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">mu_true</span> <span class="o">/</span> <span class="n">alpha</span><span class="p">)</span>

<span class="c1"># Fit Gamma GLM with log link</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">gamma_model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span>
                     <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Gamma</span><span class="p">(</span><span class="n">link</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">links</span><span class="o">.</span><span class="n">Log</span><span class="p">()))</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">gamma_model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== Gamma Regression Results ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Coefficients: β₀ = </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, β₁ = </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True values:  β₀ = 2.0000, β₁ = 0.4000&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Multiplicative effect: exp(β₁) = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimated dispersion φ: </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">scale</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True dispersion 1/α:    </span><span class="si">{</span><span class="mi">1</span><span class="o">/</span><span class="n">alpha</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="inference-in-glms-the-testing-triad">
<h2>Inference in GLMs: The Testing Triad<a class="headerlink" href="#inference-in-glms-the-testing-triad" title="Link to this heading"></a></h2>
<p>Three classical approaches to hypothesis testing are available for GLMs: Wald tests, likelihood ratio tests, and score tests. All three are asymptotically equivalent under the null hypothesis but can differ substantially in finite samples.</p>
<section id="wald-tests">
<h3>Wald Tests<a class="headerlink" href="#wald-tests" title="Link to this heading"></a></h3>
<p>The <strong>Wald test</strong> exploits the asymptotic normality of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>. For testing <span class="math notranslate nohighlight">\(H_0: \beta_j = 0\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-wald-z">
<span class="eqno">(123)<a class="headerlink" href="#equation-wald-z" title="Link to this equation"></a></span>\[z = \frac{\hat{\beta}_j}{\text{SE}(\hat{\beta}_j)} \sim N(0, 1) \quad \text{asymptotically under } H_0\]</div>
<p>For testing multiple parameters <span class="math notranslate nohighlight">\(H_0: \mathbf{C}\boldsymbol{\beta} = \mathbf{d}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-wald-chi2">
<span class="eqno">(124)<a class="headerlink" href="#equation-wald-chi2" title="Link to this equation"></a></span>\[W = (\mathbf{C}\hat{\boldsymbol{\beta}} - \mathbf{d})^\top [\mathbf{C}\widehat{\text{Var}}(\hat{\boldsymbol{\beta}})\mathbf{C}^\top]^{-1} (\mathbf{C}\hat{\boldsymbol{\beta}} - \mathbf{d}) \sim \chi^2_q\]</div>
<p>where <span class="math notranslate nohighlight">\(q = \text{rank}(\mathbf{C})\)</span>.</p>
<p><strong>Advantages</strong>: Requires only the fitted model (no need to fit reduced models).</p>
<p><strong>Disadvantages</strong>:</p>
<ul class="simple">
<li><p>Not invariant to reparameterization</p></li>
<li><p>Can behave poorly near parameter boundaries (Hauck-Donner effect)</p></li>
<li><p>Less accurate than LRT in small samples</p></li>
</ul>
</section>
<section id="likelihood-ratio-tests">
<h3>Likelihood Ratio Tests<a class="headerlink" href="#likelihood-ratio-tests" title="Link to this heading"></a></h3>
<p>The <strong>likelihood ratio test (LRT)</strong> compares nested models directly:</p>
<div class="math notranslate nohighlight" id="equation-lrt">
<span class="eqno">(125)<a class="headerlink" href="#equation-lrt" title="Link to this equation"></a></span>\[\Lambda = 2[\ell(\hat{\boldsymbol{\beta}}_{\text{full}}) - \ell(\hat{\boldsymbol{\beta}}_{\text{reduced}})] = D_{\text{reduced}} - D_{\text{full}} \sim \chi^2_q\]</div>
<p>where <span class="math notranslate nohighlight">\(q\)</span> is the difference in number of parameters.</p>
<p>For GLMs, this equals the <strong>difference in deviances</strong> between models.</p>
<p><strong>Advantages</strong>:</p>
<ul class="simple">
<li><p>Invariant to reparameterization</p></li>
<li><p>Generally more accurate than Wald in finite samples</p></li>
<li><p>Natural interpretation as deviance reduction</p></li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul class="simple">
<li><p>Requires fitting both full and reduced models</p></li>
</ul>
</section>
<section id="score-tests">
<h3>Score Tests<a class="headerlink" href="#score-tests" title="Link to this heading"></a></h3>
<p>The <strong>score test</strong> (Rao test) evaluates whether the score function is significantly different from zero when evaluated at the null hypothesis:</p>
<div class="math notranslate nohighlight" id="equation-score-test">
<span class="eqno">(126)<a class="headerlink" href="#equation-score-test" title="Link to this equation"></a></span>\[S = \mathbf{U}(\boldsymbol{\beta}_0)^\top \mathbf{I}(\boldsymbol{\beta}_0)^{-1} \mathbf{U}(\boldsymbol{\beta}_0) \sim \chi^2_q\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\beta}_0\)</span> is the parameter value under <span class="math notranslate nohighlight">\(H_0\)</span>.</p>
<p><strong>Advantages</strong>:</p>
<ul class="simple">
<li><p>Requires only the null model</p></li>
<li><p>Efficient for screening many potential predictors</p></li>
<li><p>Useful when full model is difficult to fit</p></li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul class="simple">
<li><p>Requires computing score and information under null</p></li>
</ul>
</section>
<section id="asymptotic-equivalence">
<h3>Asymptotic Equivalence<a class="headerlink" href="#asymptotic-equivalence" title="Link to this heading"></a></h3>
<p>Under standard regularity conditions and the null hypothesis:</p>
<div class="math notranslate nohighlight">
\[W_n = \Lambda_n + o_p(1) = S_n + o_p(1)\]</div>
<p>All three statistics converge to the same <span class="math notranslate nohighlight">\(\chi^2_q\)</span> distribution. In linear models with known variance, there is an ordering: <span class="math notranslate nohighlight">\(W \geq \Lambda \geq S\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Practical Guidance</p>
<ul class="simple">
<li><p><strong>Default choice</strong>: Use the <strong>likelihood ratio test</strong> for its accuracy and invariance</p></li>
<li><p><strong>Quick screening</strong>: Use <strong>Wald tests</strong> from model summary output</p></li>
<li><p><strong>Difficult full models</strong>: Use <strong>score test</strong> when the alternative model is hard to fit</p></li>
<li><p><strong>Near boundaries/separation</strong>: Prefer <strong>LRT or score</strong> over Wald</p></li>
</ul>
</div>
</section>
<section id="worked-example-comparing-tests">
<h3>Worked Example: Comparing Tests<a class="headerlink" href="#worked-example-comparing-tests" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Generate data</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">eta</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">+</span> <span class="mf">0.8</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">+</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">x2</span>
<span class="n">p</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">eta</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>

<span class="c1"># Fit full and reduced models</span>
<span class="n">X_full</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">]))</span>
<span class="n">X_reduced</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>

<span class="n">model_full</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X_full</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Binomial</span><span class="p">())</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">model_reduced</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X_reduced</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Binomial</span><span class="p">())</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Test H0: β₂ = 0</span>

<span class="c1"># 1. Wald test (from full model)</span>
<span class="n">z_wald</span> <span class="o">=</span> <span class="n">model_full</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">/</span> <span class="n">model_full</span><span class="o">.</span><span class="n">bse</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">p_wald</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">z_wald</span><span class="p">)))</span>

<span class="c1"># 2. Likelihood ratio test</span>
<span class="n">lr_stat</span> <span class="o">=</span> <span class="n">model_reduced</span><span class="o">.</span><span class="n">deviance</span> <span class="o">-</span> <span class="n">model_full</span><span class="o">.</span><span class="n">deviance</span>
<span class="n">p_lrt</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">chi2</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">lr_stat</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== Testing H₀: β₂ = 0 ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Wald test:     z = </span><span class="si">{</span><span class="n">z_wald</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, p = </span><span class="si">{</span><span class="n">p_wald</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;LR test:       χ² = </span><span class="si">{</span><span class="n">lr_stat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, p = </span><span class="si">{</span><span class="n">p_lrt</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">True β₂ = 0.3 (not zero), so we expect to reject H₀&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="model-diagnostics">
<h2>Model Diagnostics<a class="headerlink" href="#model-diagnostics" title="Link to this heading"></a></h2>
<p>Assessing model fit is crucial for GLMs. Unlike linear regression where residuals are straightforward to interpret, GLM diagnostics require careful attention to the choice of residual type and the non-constant variance structure.</p>
<section id="deviance-and-model-fit">
<h3>Deviance and Model Fit<a class="headerlink" href="#deviance-and-model-fit" title="Link to this heading"></a></h3>
<p>The <strong>deviance</strong> measures how far the fitted model is from a saturated model (one parameter per observation):</p>
<div class="math notranslate nohighlight" id="equation-deviance-def">
<span class="eqno">(127)<a class="headerlink" href="#equation-deviance-def" title="Link to this equation"></a></span>\[D = 2[\ell(\text{saturated}) - \ell(\text{fitted})] = 2\sum_{i=1}^n [\ell_i(y_i; y_i) - \ell_i(y_i; \hat{\mu}_i)]\]</div>
<p>Each observation contributes a <strong>unit deviance</strong> <span class="math notranslate nohighlight">\(d_i\)</span>:</p>
<table class="docutils align-default" id="id11">
<caption><span class="caption-number">Table 37 </span><span class="caption-text">Unit Deviance Formulas</span><a class="headerlink" href="#id11" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 25.0%" />
<col style="width: 75.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Distribution</p></th>
<th class="head"><p>Unit Deviance <span class="math notranslate nohighlight">\(d_i\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Normal</p></td>
<td><p><span class="math notranslate nohighlight">\((y_i - \hat{\mu}_i)^2\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Binomial</p></td>
<td><p><span class="math notranslate nohighlight">\(2[y_i \log(y_i/\hat{\pi}_i) + (1-y_i)\log((1-y_i)/(1-\hat{\pi}_i))]\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Poisson</p></td>
<td><p><span class="math notranslate nohighlight">\(2[y_i \log(y_i/\hat{\mu}_i) - (y_i - \hat{\mu}_i)]\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Gamma</p></td>
<td><p><span class="math notranslate nohighlight">\(2[-\log(y_i/\hat{\mu}_i) + (y_i - \hat{\mu}_i)/\hat{\mu}_i]\)</span></p></td>
</tr>
</tbody>
</table>
<p>The total deviance is <span class="math notranslate nohighlight">\(D = \sum_i d_i\)</span>.</p>
<p><strong>Deviance as goodness-of-fit</strong>: Under correct specification with grouped data or large counts, <span class="math notranslate nohighlight">\(D \sim \chi^2_{n-p}\)</span> approximately. Thus <span class="math notranslate nohighlight">\(D/(n-p) \approx 1\)</span> indicates adequate fit. However, this approximation fails for ungrouped binary data—use the Hosmer-Lemeshow test instead.</p>
<p><strong>Comparing nested models</strong>: For models <span class="math notranslate nohighlight">\(M_0 \subset M_1\)</span>:</p>
<div class="math notranslate nohighlight">
\[\Delta D = D_0 - D_1 \sim \chi^2_{p_1 - p_0}\]</div>
<p>under <span class="math notranslate nohighlight">\(H_0\)</span> that the reduced model is adequate.</p>
</section>
<section id="residual-types">
<h3>Residual Types<a class="headerlink" href="#residual-types" title="Link to this heading"></a></h3>
<p>Several residual types are available for GLMs:</p>
<p><strong>Pearson residuals</strong>:</p>
<div class="math notranslate nohighlight" id="equation-pearson-resid">
<span class="eqno">(128)<a class="headerlink" href="#equation-pearson-resid" title="Link to this equation"></a></span>\[r_i^P = \frac{y_i - \hat{\mu}_i}{\sqrt{V(\hat{\mu}_i)}}\]</div>
<p>These have approximately unit variance under correct specification. Their sum of squares gives the Pearson <span class="math notranslate nohighlight">\(\chi^2\)</span> statistic: <span class="math notranslate nohighlight">\(X^2 = \sum_i (r_i^P)^2\)</span>.</p>
<p><strong>Deviance residuals</strong>:</p>
<div class="math notranslate nohighlight" id="equation-deviance-resid">
<span class="eqno">(129)<a class="headerlink" href="#equation-deviance-resid" title="Link to this equation"></a></span>\[r_i^D = \text{sign}(y_i - \hat{\mu}_i) \sqrt{d_i}\]</div>
<p>These are defined so <span class="math notranslate nohighlight">\(\sum_i (r_i^D)^2 = D\)</span>. Deviance residuals are often more symmetric than Pearson residuals, especially for skewed response distributions.</p>
<p><strong>Standardized residuals</strong> adjust for leverage:</p>
<div class="math notranslate nohighlight">
\[r_i^{\text{std}} = \frac{r_i}{\sqrt{\hat{\phi}(1 - h_{ii})}}\]</div>
<p>where <span class="math notranslate nohighlight">\(h_{ii}\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>-th diagonal of the hat matrix <span class="math notranslate nohighlight">\(\mathbf{H} = \mathbf{W}^{1/2}\mathbf{X}(\mathbf{X}^\top\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{W}^{1/2}\)</span>.</p>
</section>
<section id="diagnostic-plots">
<h3>Diagnostic Plots<a class="headerlink" href="#diagnostic-plots" title="Link to this heading"></a></h3>
<p>Standard diagnostic plots for GLMs include:</p>
<ol class="arabic simple">
<li><p><strong>Residuals vs. fitted values</strong>: Look for patterns indicating model misspecification (nonlinearity, omitted variables)</p></li>
<li><p><strong>Q-Q plot of deviance residuals</strong>: Check approximate normality of residuals</p></li>
<li><p><strong>Scale-location plot</strong>: <span class="math notranslate nohighlight">\(\sqrt{|r_i^D|}\)</span> vs. fitted, checking for heteroskedasticity beyond what the model accommodates</p></li>
<li><p><strong>Residuals vs. each predictor</strong>: Detect nonlinear relationships</p></li>
</ol>
<p><strong>Special case—binary outcomes</strong>: For ungrouped binary data, individual residual plots are uninformative (residuals take only two distinct values for each fitted probability). Use <strong>binned residual plots</strong>: group observations by fitted probability, compute average residual in each bin, and check if averages are approximately zero.</p>
<figure class="align-center" id="id12">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_5_fig07_glm_diagnostics.png"><img alt="GLM diagnostic plots showing residuals vs fitted and Q-Q plot" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_5_fig07_glm_diagnostics.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 122 </span><span class="caption-text"><strong>Figure 3.5.7</strong>: GLM diagnostic plots for Poisson regression. (a) Deviance residuals vs. fitted values—no obvious pattern indicates adequate fit. (b) Q-Q plot of deviance residuals against standard normal quantiles—approximate linearity suggests the Poisson model is reasonable. (c) Binned residual plot for logistic regression showing average residuals within probability bins.</span><a class="headerlink" href="#id12" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="model-comparison-and-selection">
<h2>Model Comparison and Selection<a class="headerlink" href="#model-comparison-and-selection" title="Link to this heading"></a></h2>
<p>When multiple candidate models exist, we need criteria for choosing among them.</p>
<section id="information-criteria">
<h3>Information Criteria<a class="headerlink" href="#information-criteria" title="Link to this heading"></a></h3>
<p><strong>Akaike Information Criterion (AIC)</strong>:</p>
<div class="math notranslate nohighlight" id="equation-aic">
<span class="eqno">(130)<a class="headerlink" href="#equation-aic" title="Link to this equation"></a></span>\[\text{AIC} = -2\ell(\hat{\boldsymbol{\beta}}) + 2p\]</div>
<p>where <span class="math notranslate nohighlight">\(p\)</span> is the number of parameters. AIC estimates the expected Kullback-Leibler divergence and balances fit (deviance) against complexity (number of parameters). <strong>Lower AIC is better</strong>.</p>
<p><strong>Bayesian Information Criterion (BIC)</strong>:</p>
<div class="math notranslate nohighlight" id="equation-bic">
<span class="eqno">(131)<a class="headerlink" href="#equation-bic" title="Link to this equation"></a></span>\[\text{BIC} = -2\ell(\hat{\boldsymbol{\beta}}) + p \log n\]</div>
<p>BIC penalizes complexity more heavily than AIC for <span class="math notranslate nohighlight">\(n &gt; 7\)</span>. It is consistent: as <span class="math notranslate nohighlight">\(n \to \infty\)</span>, BIC selects the true model with probability approaching 1 (if the true model is among candidates).</p>
<p><strong>Interpretation</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\Delta\text{AIC} &gt; 2\)</span>: Meaningful evidence favoring lower-AIC model</p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta\text{AIC} &gt; 10\)</span>: Strong evidence</p></li>
<li><p>Similar thresholds for BIC</p></li>
</ul>
</section>
<section id="analysis-of-deviance">
<h3>Analysis of Deviance<a class="headerlink" href="#analysis-of-deviance" title="Link to this heading"></a></h3>
<p>For nested models, <strong>analysis of deviance</strong> tables present sequential model comparisons:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>

<span class="c1"># Compare nested models</span>
<span class="n">model_null</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Poisson</span><span class="p">())</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">model_x1</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Poisson</span><span class="p">())</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">model_full</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">])),</span>
                    <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Poisson</span><span class="p">())</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Analysis of Deviance&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Null model:        Deviance = </span><span class="si">{</span><span class="n">model_null</span><span class="o">.</span><span class="n">deviance</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;+ x1:              Deviance = </span><span class="si">{</span><span class="n">model_x1</span><span class="o">.</span><span class="n">deviance</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, &quot;</span>
      <span class="sa">f</span><span class="s2">&quot;Δ = </span><span class="si">{</span><span class="n">model_null</span><span class="o">.</span><span class="n">deviance</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">model_x1</span><span class="o">.</span><span class="n">deviance</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;+ x2:              Deviance = </span><span class="si">{</span><span class="n">model_full</span><span class="o">.</span><span class="n">deviance</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, &quot;</span>
      <span class="sa">f</span><span class="s2">&quot;Δ = </span><span class="si">{</span><span class="n">model_x1</span><span class="o">.</span><span class="n">deviance</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">model_full</span><span class="o">.</span><span class="n">deviance</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="quasi-likelihood-and-robust-inference">
<h2>Quasi-Likelihood and Robust Inference<a class="headerlink" href="#quasi-likelihood-and-robust-inference" title="Link to this heading"></a></h2>
<p>When the variance structure is uncertain or the distributional assumption may be wrong, quasi-likelihood methods provide valid inference without full distributional specification.</p>
<section id="quasi-likelihood-framework">
<h3>Quasi-Likelihood Framework<a class="headerlink" href="#quasi-likelihood-framework" title="Link to this heading"></a></h3>
<p>The <strong>quasi-likelihood</strong> approach specifies only:</p>
<ol class="arabic simple">
<li><p>The mean model: <span class="math notranslate nohighlight">\(\mu_i = g^{-1}(\mathbf{x}_i^\top\boldsymbol{\beta})\)</span></p></li>
<li><p>A variance function: <span class="math notranslate nohighlight">\(\text{Var}(Y_i) = \phi V(\mu_i)\)</span></p></li>
</ol>
<p>No full distributional form is assumed. The quasi-score equations are identical to GLM score equations, yielding consistent <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> even if the distribution is misspecified.</p>
<p><strong>Quasi-Poisson</strong> uses <span class="math notranslate nohighlight">\(V(\mu) = \mu\)</span> with <span class="math notranslate nohighlight">\(\phi\)</span> estimated from data:</p>
<div class="math notranslate nohighlight">
\[\hat{\phi} = \frac{1}{n-p} \sum_{i=1}^n \frac{(y_i - \hat{\mu}_i)^2}{\hat{\mu}_i}\]</div>
<p>Standard errors are inflated by <span class="math notranslate nohighlight">\(\sqrt{\hat{\phi}}\)</span> compared to standard Poisson.</p>
<p><strong>Quasi-Binomial</strong> similarly accommodates overdispersion in binary/proportion data.</p>
</section>
<section id="sandwich-robust-standard-errors">
<h3>Sandwich (Robust) Standard Errors<a class="headerlink" href="#sandwich-robust-standard-errors" title="Link to this heading"></a></h3>
<p>The <strong>sandwich estimator</strong> provides valid standard errors even when the variance function is misspecified:</p>
<div class="math notranslate nohighlight" id="equation-sandwich">
<span class="eqno">(132)<a class="headerlink" href="#equation-sandwich" title="Link to this equation"></a></span>\[\widehat{\text{Var}}_{\text{robust}}(\hat{\boldsymbol{\beta}}) = (\mathbf{X}^\top\mathbf{W}\mathbf{X})^{-1} \mathbf{X}^\top \hat{\boldsymbol{\Omega}} \mathbf{X} (\mathbf{X}^\top\mathbf{W}\mathbf{X})^{-1}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Omega}} = \text{diag}((y_i - \hat{\mu}_i)^2)\)</span> uses squared residuals rather than model-based variances.</p>
<p>This is the GLM analog of the HC (heteroskedasticity-consistent) estimators from Section 3.4.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Robust standard errors in statsmodels</span>
<span class="n">result_robust</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">cov_type</span><span class="o">=</span><span class="s1">&#39;HC1&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result_robust</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</section>
</section>
<section id="practical-considerations">
<h2>Practical Considerations<a class="headerlink" href="#practical-considerations" title="Link to this heading"></a></h2>
<section id="link-function-selection">
<h3>Link Function Selection<a class="headerlink" href="#link-function-selection" title="Link to this heading"></a></h3>
<p>Choose the link function based on:</p>
<ol class="arabic simple">
<li><p><strong>Canonical link</strong>: Simplest computationally; use unless there’s reason not to</p></li>
<li><p><strong>Range constraints</strong>: Log link ensures positivity; logit ensures <span class="math notranslate nohighlight">\([0,1]\)</span></p></li>
<li><p><strong>Interpretation</strong>: Log link gives multiplicative effects; identity gives additive</p></li>
<li><p><strong>Model fit</strong>: If canonical link fits poorly, try alternatives</p></li>
</ol>
</section>
<section id="software-implementation">
<h3>Software Implementation<a class="headerlink" href="#software-implementation" title="Link to this heading"></a></h3>
<p><strong>Statsmodels</strong> (recommended for inference):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>

<span class="c1"># Logistic regression</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Binomial</span><span class="p">())</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>

<span class="c1"># Poisson with offset</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Poisson</span><span class="p">(),</span> <span class="n">offset</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">exposure</span><span class="p">))</span>

<span class="c1"># Gamma with log link</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Gamma</span><span class="p">(</span><span class="n">link</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">links</span><span class="o">.</span><span class="n">Log</span><span class="p">()))</span>

<span class="c1"># Negative binomial for overdispersed counts</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">NegativeBinomial</span><span class="p">())</span>
</pre></div>
</div>
<p><strong>Scikit-learn</strong> (recommended for prediction pipelines):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span><span class="p">,</span> <span class="n">PoissonRegressor</span><span class="p">,</span> <span class="n">GammaRegressor</span>

<span class="c1"># Logistic regression (disable regularization for MLE)</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Poisson regression</span>
<span class="n">pr</span> <span class="o">=</span> <span class="n">PoissonRegressor</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># alpha=0 for unpenalized</span>
<span class="n">pr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Gamma regression</span>
<span class="n">gr</span> <span class="o">=</span> <span class="n">GammaRegressor</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">gr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="bringing-it-all-together">
<h2>Bringing It All Together<a class="headerlink" href="#bringing-it-all-together" title="Link to this heading"></a></h2>
<p>Generalized linear models provide a unified framework for regression with non-normal responses. The three components—exponential family distribution, linear predictor, and link function—together specify how predictors relate to the mean response. Maximum likelihood estimation via IRLS leverages the exponential family structure for efficient, reliable computation.</p>
<p>The framework encompasses:</p>
<ul class="simple">
<li><p><strong>Logistic regression</strong> for binary outcomes (Bernoulli/Binomial + logit link)</p></li>
<li><p><strong>Poisson regression</strong> for counts (Poisson + log link)</p></li>
<li><p><strong>Gamma regression</strong> for positive continuous data (Gamma + log link)</p></li>
<li><p>Classical linear regression as a special case (Normal + identity link)</p></li>
</ul>
<p>Inference proceeds through Wald, likelihood ratio, or score tests, with model comparison via deviance, AIC, and BIC. Diagnostics require attention to residual type and the mean-variance relationship inherent in each family.</p>
<p>Looking ahead, Chapter 4 develops <strong>bootstrap methods</strong> for GLMs—resampling approaches that provide robust inference without relying on asymptotic approximations. Chapter 5 presents <strong>Bayesian GLMs</strong>, placing priors on <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> and obtaining posterior distributions via MCMC.</p>
<div class="important admonition">
<p class="admonition-title">Key Takeaways 📝</p>
<ol class="arabic simple">
<li><p><strong>Core concept</strong>: GLMs unify regression for diverse response types by combining exponential family distributions with linear predictors via link functions. The canonical link provides computational advantages; alternative links may improve interpretability.</p></li>
<li><p><strong>Computational insight</strong>: IRLS reduces Fisher scoring to iterated weighted least squares. Convergence is fast (quadratic for canonical links) and reliable when the MLE exists. Watch for separation (logistic) and overdispersion (Poisson).</p></li>
<li><p><strong>Practical application</strong>: Choose the family by response type (binary → Binomial, counts → Poisson, positive continuous → Gamma). Coefficients have interpretable meanings: log-odds ratios (logistic), rate ratios (Poisson), multiplicative effects (Gamma with log link).</p></li>
<li><p><strong>Outcome alignment</strong>: This section addresses Learning Outcomes 2 and 3—comparing inference frameworks (Frequentist MLE for GLMs) and implementing computational methods (IRLS, model diagnostics). The exponential family foundation from Section 3.1 directly enables the unified GLM framework. [LO 2, 3]</p></li>
</ol>
</div>
</section>
<section id="further-reading">
<h2>Further Reading<a class="headerlink" href="#further-reading" title="Link to this heading"></a></h2>
<p>For deeper exploration of generalized linear models:</p>
<ul class="simple">
<li><p><strong>McCullagh &amp; Nelder</strong> (1989), <em>Generalized Linear Models</em> (2nd ed.): The definitive reference, comprehensive and mathematically rigorous</p></li>
<li><p><strong>Agresti</strong> (2013), <em>Categorical Data Analysis</em> (3rd ed.): Excellent treatment of logistic regression and models for categorical outcomes</p></li>
<li><p><strong>Dobson &amp; Barnett</strong> (2018), <em>An Introduction to Generalized Linear Models</em> (4th ed.): Accessible introduction with many examples</p></li>
<li><p><strong>Hardin &amp; Hilbe</strong> (2012), <em>Generalized Linear Models and Extensions</em> (3rd ed.): Covers quasi-likelihood, robust methods, and extensions</p></li>
<li><p><strong>Efron &amp; Hastie</strong> (2016), <em>Computer Age Statistical Inference</em>, Chapter 4: Modern perspective on GLMs in the context of statistical learning</p></li>
</ul>
</section>
<section id="chapter-3-5-exercises-generalized-linear-models-mastery">
<h2>Chapter 3.5 Exercises: Generalized Linear Models Mastery<a class="headerlink" href="#chapter-3-5-exercises-generalized-linear-models-mastery" title="Link to this heading"></a></h2>
<p>These exercises progressively build your understanding of GLMs from mathematical foundations through implementation to handling real-world complications. Each exercise connects theoretical derivations to computational practice and statistical interpretation, following the pattern established in earlier sections.</p>
<div class="tip admonition">
<p class="admonition-title">A Note on These Exercises</p>
<p>These exercises are designed to deepen your understanding of GLMs through hands-on exploration:</p>
<ul class="simple">
<li><p><strong>Exercise 3.5.1</strong> reinforces the connection between exponential families (Section 3.1) and GLMs through explicit derivation</p></li>
<li><p><strong>Exercise 3.5.2</strong> builds computational understanding by implementing IRLS from scratch</p></li>
<li><p><strong>Exercise 3.5.3</strong> explores the separation problem in logistic regression and its remedies</p></li>
<li><p><strong>Exercise 3.5.4</strong> addresses overdispersion detection and remedies in Poisson regression</p></li>
<li><p><strong>Exercise 3.5.5</strong> compares the Wald, LR, and Score tests empirically via simulation</p></li>
<li><p><strong>Exercise 3.5.6</strong> synthesizes the material into a complete GLM analysis workflow</p></li>
</ul>
<p>Complete solutions with derivations, code, output, and interpretation are provided. Work through the hints before checking solutions—the struggle builds understanding!</p>
</div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 3.5.1: Exponential Family to GLM</p>
<p>This exercise reinforces the connection between exponential family theory (Section 3.1) and the GLM framework. Understanding how specific distributions fit into the general framework is essential for applying GLMs correctly.</p>
<div class="note admonition">
<p class="admonition-title">Background: Why This Matters</p>
<p>Every GLM is built on an exponential family distribution. When you fit a logistic regression or Poisson regression, you’re implicitly using the exponential family structure we developed in Section 3.1. Making this connection explicit helps you understand why certain link functions are “canonical,” why the IRLS algorithm works, and how to extend GLMs to new situations.</p>
</div>
<ol class="loweralpha simple">
<li><p>Starting from the Poisson PMF <span class="math notranslate nohighlight">\(P(Y = y) = e^{-\mu}\mu^y / y!\)</span>, derive the exponential dispersion form <a class="reference internal" href="#equation-glm-density">(104)</a>. Identify <span class="math notranslate nohighlight">\(\theta\)</span>, <span class="math notranslate nohighlight">\(b(\theta)\)</span>, <span class="math notranslate nohighlight">\(\phi\)</span>, and <span class="math notranslate nohighlight">\(c(y, \phi)\)</span>.</p></li>
<li><p>Verify that <span class="math notranslate nohighlight">\(\mu = b'(\theta)\)</span> and <span class="math notranslate nohighlight">\(V(\mu) = b''(\theta)\)</span> yield the correct mean and variance function for Poisson.</p></li>
<li><p>Derive the canonical link <span class="math notranslate nohighlight">\(g(\mu) = \theta\)</span> and verify it equals <span class="math notranslate nohighlight">\(\log\mu\)</span>.</p></li>
<li><p>For a Poisson GLM with log link and design matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, write out the score equations <span class="math notranslate nohighlight">\(\partial \ell / \partial \beta_j = 0\)</span> explicitly. Verify they match the general form <a class="reference internal" href="#equation-score-equations">(110)</a>.</p></li>
</ol>
<div class="tip admonition">
<p class="admonition-title">Hint</p>
<p>For part (a), rewrite the Poisson PMF as <span class="math notranslate nohighlight">\(\exp\{\text{something}\}\)</span> by taking logs and rearranging. Group terms to match the exponential family form <span class="math notranslate nohighlight">\(\exp\{(y\theta - b(\theta))/\phi + c(y,\phi)\}\)</span>. Remember that <span class="math notranslate nohighlight">\(\log(\mu^y) = y\log\mu\)</span>.</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Exponential dispersion form</strong></p>
<div class="tip admonition">
<p class="admonition-title">Step 1: Rewrite the Poisson PMF</p>
<p class="sd-card-text">Starting from <span class="math notranslate nohighlight">\(P(Y = y) = e^{-\mu}\mu^y / y!\)</span>:</p>
<div class="math notranslate nohighlight">
\[P(Y = y) = \frac{e^{-\mu}\mu^y}{y!} = \exp\left\{-\mu + y\log\mu - \log(y!)\right\}\]</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Step 2: Match to exponential family form</p>
<p class="sd-card-text">Rearranging to match <span class="math notranslate nohighlight">\(\exp\{(y\theta - b(\theta))/\phi + c(y,\phi)\}\)</span>:</p>
<div class="math notranslate nohighlight">
\[P(Y = y) = \exp\left\{\frac{y \cdot \log\mu - \mu}{1} + (-\log(y!))\right\}\]</div>
</div>
<p class="sd-card-text"><strong>Identification:</strong></p>
<ul class="simple">
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(\theta = \log\mu\)</span> (canonical/natural parameter)</p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(b(\theta) = e^\theta = \mu\)</span> (cumulant function)</p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(\phi = 1\)</span> (dispersion parameter, fixed for Poisson)</p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(c(y, \phi) = -\log(y!)\)</span> (normalizing term)</p></li>
</ul>
<p class="sd-card-text"><strong>Part (b): Mean and variance from</strong> <span class="math notranslate nohighlight">\(b(\theta)\)</span></p>
<div class="tip admonition">
<p class="admonition-title">Step 1: Compute the mean</p>
<div class="math notranslate nohighlight">
\[\mu = b'(\theta) = \frac{d}{d\theta}e^\theta = e^\theta\]</div>
<p class="sd-card-text">Since <span class="math notranslate nohighlight">\(\theta = \log\mu\)</span>, we have <span class="math notranslate nohighlight">\(b'(\theta) = e^{\log\mu} = \mu\)</span>. ✓</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Step 2: Compute the variance function</p>
<div class="math notranslate nohighlight">
\[V(\mu) = b''(\theta) = \frac{d^2}{d\theta^2}e^\theta = e^\theta = \mu\]</div>
</div>
<p class="sd-card-text">This confirms the fundamental Poisson property: <span class="math notranslate nohighlight">\(\text{Var}(Y) = \phi \cdot V(\mu) = 1 \cdot \mu = \mu\)</span>.</p>
<p class="sd-card-text"><strong>Part (c): Canonical link</strong></p>
<p class="sd-card-text">The canonical link satisfies <span class="math notranslate nohighlight">\(g(\mu) = \theta\)</span>. Since we identified <span class="math notranslate nohighlight">\(\theta = \log\mu\)</span>:</p>
<div class="math notranslate nohighlight">
\[g(\mu) = \log\mu \quad \text{(the log link)}\]</div>
<p class="sd-card-text">This confirms that <strong>log is the canonical link for Poisson</strong>.</p>
<p class="sd-card-text"><strong>Part (d): Score equations</strong></p>
<div class="tip admonition">
<p class="admonition-title">Step 1: Write the log-likelihood</p>
<div class="math notranslate nohighlight">
\[\ell(\boldsymbol{\beta}) = \sum_{i=1}^n \left[y_i\log\mu_i - \mu_i - \log(y_i!)\right]\]</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Step 2: Compute the derivative</p>
<p class="sd-card-text">With <span class="math notranslate nohighlight">\(\mu_i = e^{\mathbf{x}_i^\top\boldsymbol{\beta}}\)</span> (log link), we have <span class="math notranslate nohighlight">\(\partial\mu_i/\partial\beta_j = \mu_i x_{ij}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\frac{\partial\ell}{\partial\beta_j} = \sum_{i=1}^n \left[\frac{y_i}{\mu_i} - 1\right] \cdot \mu_i x_{ij} = \sum_{i=1}^n (y_i - \mu_i) x_{ij} = 0\]</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Step 3: Express in matrix form</p>
<div class="math notranslate nohighlight">
\[\mathbf{X}^\top(\mathbf{y} - \boldsymbol{\mu}) = \mathbf{0}\]</div>
</div>
<p class="sd-card-text"><strong>Verification against general form</strong>: For canonical link with <span class="math notranslate nohighlight">\(\phi = 1\)</span>, the working weights are <span class="math notranslate nohighlight">\(w_i = 1/[V(\mu_i)(g'(\mu_i))^2] = 1/[\mu_i \cdot (1/\mu_i)^2] = \mu_i\)</span>. The general score form is <span class="math notranslate nohighlight">\(\sum_i w_i(y_i - \mu_i)g'(\mu_i)x_{ij} = 0\)</span>. Substituting:</p>
<div class="math notranslate nohighlight">
\[\sum_i \mu_i(y_i - \mu_i)\frac{1}{\mu_i}x_{ij} = \sum_i (y_i - \mu_i)x_{ij} = 0 \quad \checkmark\]</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>The canonical link simplifies everything</strong>: With log link for Poisson, the score equations reduce to the elegant form <span class="math notranslate nohighlight">\(\mathbf{X}^\top(\mathbf{y} - \boldsymbol{\mu}) = \mathbf{0}\)</span>—residuals orthogonal to the column space of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p></li>
<li><p class="sd-card-text"><strong>Direct generalization of OLS</strong>: This is exactly the normal equations from linear regression (Section 3.4), but with <span class="math notranslate nohighlight">\(\boldsymbol{\mu} = \exp(\mathbf{X}\boldsymbol{\beta})\)</span> instead of <span class="math notranslate nohighlight">\(\boldsymbol{\mu} = \mathbf{X}\boldsymbol{\beta}\)</span>.</p></li>
<li><p class="sd-card-text"><strong>Variance function determines weights</strong>: The fact that <span class="math notranslate nohighlight">\(V(\mu) = \mu\)</span> for Poisson means observations with larger means get less weight in the estimating equations—higher variance implies less information.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 3.5.2: Implementing IRLS from Scratch</p>
<p>This exercise builds deep understanding of the IRLS algorithm through implementation. Writing the algorithm yourself reveals how Fisher scoring, weighted least squares, and the GLM components combine.</p>
<div class="note admonition">
<p class="admonition-title">Background: Why Implement IRLS?</p>
<p>Software packages hide the algorithmic details. By implementing IRLS yourself, you understand: (1) why weights change at each iteration, (2) how the working response linearizes the link function, (3) what convergence behavior to expect, and (4) what can go wrong (separation, non-convergence). This knowledge is essential for diagnosing fitting problems in practice.</p>
</div>
<ol class="loweralpha simple">
<li><p>Implement IRLS for <strong>logistic regression</strong> without using the generic code provided in the text. Your function should:</p>
<ul class="simple">
<li><p>Accept design matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, binary response <span class="math notranslate nohighlight">\(\mathbf{y}\)</span></p></li>
<li><p>Initialize with <span class="math notranslate nohighlight">\(\boldsymbol{\beta}^{(0)} = \mathbf{0}\)</span></p></li>
<li><p>Iterate until convergence (relative change <span class="math notranslate nohighlight">\(&lt; 10^{-8}\)</span>)</p></li>
<li><p>Return <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>, number of iterations, and final deviance</p></li>
</ul>
</li>
<li><p>Test your implementation on simulated data with <span class="math notranslate nohighlight">\(n = 500\)</span>, <span class="math notranslate nohighlight">\(\beta_0 = -1, \beta_1 = 2\)</span>. Compare your estimates to <code class="docutils literal notranslate"><span class="pre">statsmodels.GLM</span></code>.</p></li>
<li><p>Track and plot the deviance at each iteration. Verify quadratic convergence by plotting <span class="math notranslate nohighlight">\(\log|\boldsymbol{\beta}^{(t)} - \hat{\boldsymbol{\beta}}|\)</span> vs. iteration.</p></li>
<li><p>Modify your code to use <strong>step-halving</strong>: if the deviance increases, halve the step until improvement occurs. Test on challenging data (add outliers or near-separation).</p></li>
</ol>
<div class="tip admonition">
<p class="admonition-title">Hint</p>
<p>For logistic regression with canonical logit link: (1) weights are <span class="math notranslate nohighlight">\(w_i = \mu_i(1-\mu_i)\)</span>, (2) working response is <span class="math notranslate nohighlight">\(z_i = \eta_i + (y_i - \mu_i)/w_i\)</span>, (3) update via weighted least squares: <span class="math notranslate nohighlight">\(\boldsymbol{\beta}^{(t+1)} = (\mathbf{X}^\top\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{W}\mathbf{z}\)</span>. Use np.clip on probabilities to prevent numerical issues near 0 and 1.</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): IRLS implementation for logistic regression</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">logistic_irls</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">25</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fit logistic regression via IRLS.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : ndarray (n, p)</span>
<span class="sd">        Design matrix (include intercept column)</span>
<span class="sd">    y : ndarray (n,)</span>
<span class="sd">        Binary response (0 or 1)</span>
<span class="sd">    tol : float</span>
<span class="sd">        Convergence tolerance</span>
<span class="sd">    max_iter : int</span>
<span class="sd">        Maximum iterations</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    beta : ndarray (p,)</span>
<span class="sd">        Coefficient estimates</span>
<span class="sd">    n_iter : int</span>
<span class="sd">        Number of iterations</span>
<span class="sd">    deviance : float</span>
<span class="sd">        Final deviance</span>
<span class="sd">    history : dict</span>
<span class="sd">        Iteration history (deviances, betas)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>  <span class="c1"># Initialize at zero</span>

    <span class="n">history</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;deviance&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;beta&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">beta</span><span class="o">.</span><span class="n">copy</span><span class="p">()]}</span>

    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
        <span class="c1"># Linear predictor</span>
        <span class="n">eta</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta</span>

        <span class="c1"># Fitted probabilities (with numerical safeguards)</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">eta</span><span class="p">))</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="mf">1e-10</span><span class="p">)</span>

        <span class="c1"># Weights: w = mu(1-mu) for logistic</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span>

        <span class="c1"># Working response: z = eta + (y - mu) / w</span>
        <span class="c1"># Since g&#39;(mu) = 1/(mu(1-mu)), we have (y-mu)*g&#39;(mu) = (y-mu)/(mu(1-mu))</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">eta</span> <span class="o">+</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">/</span> <span class="n">w</span>

        <span class="c1"># Weighted least squares</span>
        <span class="n">W_sqrt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="n">X_tilde</span> <span class="o">=</span> <span class="n">X</span> <span class="o">*</span> <span class="n">W_sqrt</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
        <span class="n">z_tilde</span> <span class="o">=</span> <span class="n">z</span> <span class="o">*</span> <span class="n">W_sqrt</span>

        <span class="n">beta_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X_tilde</span><span class="p">,</span> <span class="n">z_tilde</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="kc">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Compute deviance</span>
        <span class="n">deviance</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
            <span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">mu</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">mu</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">history</span><span class="p">[</span><span class="s1">&#39;deviance&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">deviance</span><span class="p">)</span>
        <span class="n">history</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">beta_new</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>

        <span class="c1"># Check convergence</span>
        <span class="n">rel_change</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">beta_new</span> <span class="o">-</span> <span class="n">beta</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">)</span>

        <span class="n">beta</span> <span class="o">=</span> <span class="n">beta_new</span>

        <span class="k">if</span> <span class="n">rel_change</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">beta</span><span class="p">,</span> <span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">deviance</span><span class="p">,</span> <span class="n">history</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: Did not converge in </span><span class="si">{</span><span class="n">max_iter</span><span class="si">}</span><span class="s2"> iterations&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">beta</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">,</span> <span class="n">deviance</span><span class="p">,</span> <span class="n">history</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Testing on simulated data</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>

<span class="c1"># Generate data</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">eta_true</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span>
<span class="n">p_true</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">eta_true</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p_true</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">x</span><span class="p">])</span>

<span class="c1"># Our implementation</span>
<span class="n">beta_ours</span><span class="p">,</span> <span class="n">n_iter</span><span class="p">,</span> <span class="n">dev_ours</span><span class="p">,</span> <span class="n">history</span> <span class="o">=</span> <span class="n">logistic_irls</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Statsmodels</span>
<span class="n">sm_model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Binomial</span><span class="p">())</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== Comparison ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True β:        [</span><span class="si">{</span><span class="o">-</span><span class="mi">1</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="mi">2</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Our IRLS:      [</span><span class="si">{</span><span class="n">beta_ours</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">beta_ours</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Statsmodels:   [</span><span class="si">{</span><span class="n">sm_model</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">sm_model</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Iterations: </span><span class="si">{</span><span class="n">n_iter</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Deviance difference: </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">dev_ours</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">sm_model</span><span class="o">.</span><span class="n">deviance</span><span class="p">)</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>=== Comparison ===
True β:        [-1.0000, 2.0000]
Our IRLS:      [-1.0847, 2.1203]
Statsmodels:   [-1.0847, 2.1203]

Iterations: 5
Deviance difference: 2.84e-14
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (c): Convergence visualization</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Extract history</span>
<span class="n">deviances</span> <span class="o">=</span> <span class="n">history</span><span class="p">[</span><span class="s1">&#39;deviance&#39;</span><span class="p">]</span>
<span class="n">betas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">])</span>
<span class="n">beta_final</span> <span class="o">=</span> <span class="n">betas</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Compute errors</span>
<span class="n">errors</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">beta_final</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">betas</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1"># Deviance trajectory</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">deviances</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">deviances</span><span class="p">,</span> <span class="s1">&#39;bo-&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Deviance&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Deviance Convergence&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Log error (quadratic convergence)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">errors</span><span class="p">)),</span> <span class="n">errors</span><span class="p">,</span> <span class="s1">&#39;ro-&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">|</span><span class="se">\\</span><span class="s1">beta^{(t)} - </span><span class="se">\\</span><span class="s1">hat{</span><span class="se">\\</span><span class="s1">beta}</span><span class="se">\\</span><span class="s1">|$ (log scale)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Quadratic Convergence&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;irls_convergence_exercise.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text">The log-error plot shows approximately linear decrease with steepening slope—characteristic of quadratic convergence where the number of correct digits doubles per iteration.</p>
<p class="sd-card-text"><strong>Part (d): Step-halving modification</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">logistic_irls_robust</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">max_halving</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;IRLS with step-halving for robustness.&quot;&quot;&quot;</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

    <span class="c1"># Initial deviance</span>
    <span class="n">eta</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">eta</span><span class="p">))</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="mf">1e-10</span><span class="p">)</span>
    <span class="n">deviance</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">mu</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
        <span class="n">eta</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">eta</span><span class="p">))</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="mf">1e-10</span><span class="p">)</span>

        <span class="n">w</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">eta</span> <span class="o">+</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">/</span> <span class="n">w</span>

        <span class="n">W_sqrt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="n">X_tilde</span> <span class="o">=</span> <span class="n">X</span> <span class="o">*</span> <span class="n">W_sqrt</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
        <span class="n">z_tilde</span> <span class="o">=</span> <span class="n">z</span> <span class="o">*</span> <span class="n">W_sqrt</span>

        <span class="n">beta_full</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X_tilde</span><span class="p">,</span> <span class="n">z_tilde</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="kc">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">direction</span> <span class="o">=</span> <span class="n">beta_full</span> <span class="o">-</span> <span class="n">beta</span>

        <span class="c1"># Step halving</span>
        <span class="n">step</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_halving</span><span class="p">):</span>
            <span class="n">beta_try</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">+</span> <span class="n">step</span> <span class="o">*</span> <span class="n">direction</span>
            <span class="n">eta_try</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta_try</span>
            <span class="n">mu_try</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">eta_try</span><span class="p">))</span>
            <span class="n">mu_try</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">mu_try</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="mf">1e-10</span><span class="p">)</span>
            <span class="n">dev_try</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">mu_try</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">mu_try</span><span class="p">))</span>

            <span class="k">if</span> <span class="n">dev_try</span> <span class="o">&lt;</span> <span class="n">deviance</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="n">step</span> <span class="o">*=</span> <span class="mf">0.5</span>

        <span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">+</span> <span class="n">step</span> <span class="o">*</span> <span class="n">direction</span>
        <span class="n">deviance</span> <span class="o">=</span> <span class="n">dev_try</span>

        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">step</span> <span class="o">*</span> <span class="n">direction</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">beta</span><span class="p">,</span> <span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">deviance</span>

    <span class="k">return</span> <span class="n">beta</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">,</span> <span class="n">deviance</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Exact match to statsmodels</strong>: Our implementation matches statsmodels to machine precision (deviance difference ~10⁻¹⁴), confirming the IRLS algorithm is implemented correctly.</p></li>
<li><p class="sd-card-text"><strong>Rapid convergence</strong>: Only 5 iterations needed for well-conditioned logistic regression—typical when data has reasonable separation between classes.</p></li>
<li><p class="sd-card-text"><strong>Quadratic convergence</strong>: The log-error plot shows the characteristic pattern of Newton’s method—the slope steepens each iteration, meaning the number of correct digits roughly doubles per iteration.</p></li>
<li><p class="sd-card-text"><strong>Step-halving for robustness</strong>: Near separation, the full Newton step can overshoot (increasing deviance). Step-halving ensures monotonic deviance decrease, making the algorithm more robust for challenging data.</p></li>
<li><p class="sd-card-text"><strong>Connection to theory</strong>: Each IRLS iteration solves a weighted least squares problem—the weights <span class="math notranslate nohighlight">\(w_i = \mu_i(1-\mu_i)\)</span> are exactly the Fisher information weights from Section 3.2, making IRLS equivalent to Fisher scoring.</p></li>
</ol>
</div>
</details><dl class="field-list simple">
<dt class="field-odd">class<span class="colon">:</span></dt>
<dd class="field-odd"><p>exercise</p>
</dd>
</dl>
<p>This exercise explores the separation problem—one of the most important practical issues in logistic regression—and its remedies.</p>
<div class="note admonition">
<p class="admonition-title">Background: Why Separation Matters</p>
<p>In clinical trials, financial modeling, and many other applications, perfect or near-perfect prediction can occur. When one predictor (or combination) perfectly separates the classes, standard MLE fails—coefficients diverge to infinity while standard errors explode. Understanding this phenomenon and knowing remedies (Firth’s method, exact logistic regression, regularization) is essential for applied work.</p>
</div>
<ol class="loweralpha simple">
<li><p>Generate <strong>completely separable</strong> data: <span class="math notranslate nohighlight">\(n = 50\)</span> observations where <span class="math notranslate nohighlight">\(y_i = 1\)</span> if <span class="math notranslate nohighlight">\(x_i &gt; 0\)</span> and <span class="math notranslate nohighlight">\(y_i = 0\)</span> if <span class="math notranslate nohighlight">\(x_i &lt; 0\)</span>, with <span class="math notranslate nohighlight">\(x_i \sim N(0, 1)\)</span>.</p></li>
<li><p>Fit logistic regression using <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>. What happens to the coefficient estimates? What warnings appear? How many iterations does the algorithm use?</p></li>
<li><p>Implement a simple check for separation: after fitting, test whether <span class="math notranslate nohighlight">\(\min_{y_i=1} \hat{\eta}_i &gt; \max_{y_i=0} \hat{\eta}_i\)</span>.</p></li>
<li><p>Apply <strong>Firth’s penalized likelihood</strong> (or use L2 regularization as an approximation). Compare the resulting coefficients to the standard MLE attempt.</p></li>
</ol>
<div class="tip admonition">
<p class="admonition-title">Hint</p>
<p>For part (d), Firth’s method adds a penalty <span class="math notranslate nohighlight">\(\frac{1}{2}\log|\mathbf{I}(\boldsymbol{\beta})|\)</span> to the log-likelihood. In Python, you can approximate this with <code class="docutils literal notranslate"><span class="pre">LogisticRegression(penalty='l2',</span> <span class="pre">C=large_value)</span></code> from sklearn, or implement the modified score equations. The key insight is that the penalty “pulls” coefficients away from infinity.</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Generate separable data</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>  <span class="c1"># Perfect separation at x = 0</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Data summary:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  y=1 range: x ∈ [</span><span class="si">{</span><span class="n">x</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">x</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  y=0 range: x ∈ [</span><span class="si">{</span><span class="n">x</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">x</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Standard logistic regression</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">(</span><span class="n">record</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">w</span><span class="p">:</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s2">&quot;always&quot;</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Binomial</span><span class="p">())</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== Standard Logistic Regression ===&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Coefficients: β₀ = </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, β₁ = </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Standard errors: </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">bse</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">bse</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iterations: </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">fit_history</span><span class="p">[</span><span class="s1">&#39;iteration&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">w</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Warnings captured:&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">warning</span> <span class="ow">in</span> <span class="n">w</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  - </span><span class="si">{</span><span class="n">warning</span><span class="o">.</span><span class="n">message</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>=== Standard Logistic Regression ===
Coefficients: β₀ = 0.40, β₁ = 23.87
Standard errors: 18.29, 193.47
Iterations: 25

Warnings captured:
  - Maximum number of iterations has been exceeded.
</pre></div>
</div>
<p class="sd-card-text">Note the enormous coefficient (<span class="math notranslate nohighlight">\(\beta_1 \approx 24\)</span>) and standard error (<span class="math notranslate nohighlight">\(\text{SE} \approx 193\)</span>). The algorithm hit the iteration limit without converging.</p>
<p class="sd-card-text"><strong>Part (c): Separation check</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">check_separation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check for complete separation.&quot;&quot;&quot;</span>
    <span class="n">eta</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta</span>
    <span class="n">eta_y1</span> <span class="o">=</span> <span class="n">eta</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">eta_y0</span> <span class="o">=</span> <span class="n">eta</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>

    <span class="n">separated</span> <span class="o">=</span> <span class="n">eta_y1</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">eta_y0</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="n">margin</span> <span class="o">=</span> <span class="n">eta_y1</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="n">eta_y0</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Separation check:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  min(η | y=1) = </span><span class="si">{</span><span class="n">eta_y1</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  max(η | y=0) = </span><span class="si">{</span><span class="n">eta_y0</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Margin = </span><span class="si">{</span><span class="n">margin</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Complete separation: </span><span class="si">{</span><span class="n">separated</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">separated</span>

<span class="n">check_separation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Separation check:
  min(η | y=1) = 1.569
  max(η | y=0) = -0.919
  Margin = 2.488
  Complete separation: True
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): Firth’s penalized likelihood</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="c1"># Method 1: Simple implementation of Firth-corrected IRLS</span>
<span class="k">def</span><span class="w"> </span><span class="nf">firth_logistic</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Firth&#39;s penalized logistic regression.&quot;&quot;&quot;</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
        <span class="n">eta</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">eta</span><span class="p">))</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="mf">1e-10</span><span class="p">)</span>

        <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">mu</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">mu</span><span class="p">))</span>

        <span class="c1"># Hat matrix diagonal</span>
        <span class="n">W_sqrt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">W</span><span class="p">))</span>
        <span class="n">XW</span> <span class="o">=</span> <span class="n">X</span> <span class="o">*</span> <span class="n">W_sqrt</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">XtWX_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">W</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span>
        <span class="k">except</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">LinAlgError</span><span class="p">:</span>
            <span class="n">XtWX_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">W</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span>

        <span class="n">H</span> <span class="o">=</span> <span class="n">XW</span> <span class="o">@</span> <span class="n">XtWX_inv</span> <span class="o">@</span> <span class="n">XW</span><span class="o">.</span><span class="n">T</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">H</span><span class="p">)</span>

        <span class="c1"># Firth-corrected working response</span>
        <span class="c1"># Modified residual: (y - mu) + h*(0.5 - mu)</span>
        <span class="n">resid_firth</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">+</span> <span class="n">h</span> <span class="o">*</span> <span class="p">(</span><span class="mf">0.5</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span>

        <span class="c1"># Update (simplified version)</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">resid_firth</span>
        <span class="n">beta_new</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">+</span> <span class="n">XtWX_inv</span> <span class="o">@</span> <span class="n">score</span>

        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">beta_new</span> <span class="o">-</span> <span class="n">beta</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">beta_new</span><span class="p">,</span> <span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="n">beta</span> <span class="o">=</span> <span class="n">beta_new</span>

    <span class="k">return</span> <span class="n">beta</span><span class="p">,</span> <span class="n">max_iter</span>

<span class="n">beta_firth</span><span class="p">,</span> <span class="n">n_iter_firth</span> <span class="o">=</span> <span class="n">firth_logistic</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Method 2: L2 regularization approximation</span>
<span class="n">clf_reg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">)</span>
<span class="n">clf_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;COMPARISON: STANDARD MLE vs PENALIZED METHODS&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;Method&#39;</span><span class="si">:</span><span class="s2">&lt;30</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;β₀&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;β₁&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Standard MLE (diverging)&#39;</span><span class="si">:</span><span class="s2">&lt;30</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;10.2f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;10.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Firth penalized&#39;</span><span class="si">:</span><span class="s2">&lt;30</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">beta_firth</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;10.2f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">beta_firth</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;10.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;L2 regularized (C=10)&#39;</span><span class="si">:</span><span class="s2">&lt;30</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">clf_reg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;10.2f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">clf_reg</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;10.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>============================================================
COMPARISON: STANDARD MLE vs PENALIZED METHODS
============================================================

Method                               β₀         β₁
--------------------------------------------------
Standard MLE (diverging)           0.40      23.87
Firth penalized                    0.12       3.45
L2 regularized (C=10)              0.17       4.28
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Standard MLE fails catastrophically</strong>: Coefficients grow without bound (would → ∞ with more iterations); standard errors are enormous and meaningless; inference is invalid.</p></li>
<li><p class="sd-card-text"><strong>Symptoms to recognize</strong>: (a) Huge coefficients (<a href="#id13"><span class="problematic" id="id14">|β|</span></a> &gt; 10 is suspicious), (b) Enormous standard errors, (c) Algorithm hits iteration limit, (d) Predicted probabilities all near 0 or 1.</p></li>
<li><p class="sd-card-text"><strong>Firth’s method works</strong>: By adding the Jeffreys prior penalty <span class="math notranslate nohighlight">\(\frac{1}{2}\log|\mathbf{I}(\boldsymbol{\beta})|\)</span>, coefficients remain finite and have meaningful interpretations. The penalty “pulls” estimates away from the boundary.</p></li>
<li><p class="sd-card-text"><strong>L2 regularization as practical alternative</strong>: When Firth is unavailable, simple L2 regularization achieves similar stabilization. Stronger penalty (smaller C) → more shrinkage.</p></li>
<li><p class="sd-card-text"><strong>Practical recommendation</strong>: Always check for separation when you see huge coefficients or SEs. Use Firth’s method (theoretically motivated) or regularization (practical). Report that separation occurred.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 3.5.4: Overdispersion in Poisson Regression</p>
<p>This exercise explores overdispersion—when the variance exceeds what the Poisson model predicts—and its consequences for inference.</p>
<div class="note admonition">
<p class="admonition-title">Background: The Overdispersion Problem</p>
<p>Poisson regression assumes <span class="math notranslate nohighlight">\(\text{Var}(Y) = \mu\)</span>. Real count data often violates this: unobserved heterogeneity, clustering, or zero-inflation can cause <span class="math notranslate nohighlight">\(\text{Var}(Y) &gt; \mu\)</span>. Ignoring overdispersion leads to standard errors that are too small and p-values that are too small—we think effects are more significant than they really are. This is one of the most common mistakes in applied count regression.</p>
</div>
<ol class="loweralpha simple">
<li><p>Generate <strong>overdispersed count data</strong> using a negative binomial distribution: <span class="math notranslate nohighlight">\(Y_i \sim \text{NegBin}(\mu_i, \alpha)\)</span> with <span class="math notranslate nohighlight">\(\mu_i = \exp(1 + 0.5x_i)\)</span> and <span class="math notranslate nohighlight">\(\alpha = 2\)</span> (giving <span class="math notranslate nohighlight">\(\text{Var}(Y) = \mu + \mu^2/2\)</span>).</p></li>
<li><p>Fit a standard Poisson GLM. Compute the Pearson <span class="math notranslate nohighlight">\(\chi^2\)</span> statistic and estimate <span class="math notranslate nohighlight">\(\hat{\phi} = \chi^2/(n-p)\)</span>. Is there evidence of overdispersion?</p></li>
<li><p>Compare standard errors from: (i) standard Poisson, (ii) quasi-Poisson (scaled by <span class="math notranslate nohighlight">\(\sqrt{\hat{\phi}}\)</span>), and (iii) robust sandwich estimator. What is the practical impact on inference?</p></li>
<li><p>Fit a negative binomial model using <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>. Compare coefficient estimates, standard errors, and AIC to the Poisson models.</p></li>
</ol>
<div class="tip admonition">
<p class="admonition-title">Hint</p>
<p>In scipy, use <code class="docutils literal notranslate"><span class="pre">stats.nbinom.rvs(n,</span> <span class="pre">p)</span></code> where the mean is <span class="math notranslate nohighlight">\(\mu = n(1-p)/p\)</span>. To get a negative binomial with mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\mu + \mu^2/r\)</span>, set <span class="math notranslate nohighlight">\(n = r\)</span> and <span class="math notranslate nohighlight">\(p = r/(r + \mu)\)</span>. The parameter <span class="math notranslate nohighlight">\(\alpha = 1/r\)</span> is the overdispersion parameter. For detecting overdispersion, <span class="math notranslate nohighlight">\(\hat{\phi} &gt; 1.5\)</span> is a common rule of thumb.</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Generate overdispersed data</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">456</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">mu_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># Negative binomial with size parameter (r) related to alpha</span>
<span class="c1"># scipy&#39;s nbinom: Var = μ + μ²/r, so r = 1/alpha for our parameterization</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">2.0</span>
<span class="n">r</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">alpha</span>  <span class="c1"># r = 0.5</span>
<span class="n">p_nb</span> <span class="o">=</span> <span class="n">r</span> <span class="o">/</span> <span class="p">(</span><span class="n">r</span> <span class="o">+</span> <span class="n">mu_true</span><span class="p">)</span>  <span class="c1"># success probability for scipy</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">nbinom</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">p_nb</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== Data Summary ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Var/Mean ratio: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;(Should be &gt; 1 for overdispersed data)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Standard Poisson and overdispersion detection</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Standard Poisson</span>
<span class="n">poisson_model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Poisson</span><span class="p">())</span>
<span class="n">poisson_result</span> <span class="o">=</span> <span class="n">poisson_model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Overdispersion statistics</span>
<span class="n">pearson_chi2</span> <span class="o">=</span> <span class="n">poisson_result</span><span class="o">.</span><span class="n">pearson_chi2</span>
<span class="n">df_resid</span> <span class="o">=</span> <span class="n">poisson_result</span><span class="o">.</span><span class="n">df_resid</span>
<span class="n">phi_hat</span> <span class="o">=</span> <span class="n">pearson_chi2</span> <span class="o">/</span> <span class="n">df_resid</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== Overdispersion Detection ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Pearson χ²: </span><span class="si">{</span><span class="n">pearson_chi2</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Degrees of freedom: </span><span class="si">{</span><span class="n">df_resid</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;φ̂ = χ²/df: </span><span class="si">{</span><span class="n">phi_hat</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Conclusion: </span><span class="si">{</span><span class="s1">&#39;OVERDISPERSION (φ̂ &gt;&gt; 1)&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">phi_hat</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mf">1.5</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;No strong overdispersion&#39;</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>=== Overdispersion Detection ===
Pearson χ²: 612.45
Degrees of freedom: 198
φ̂ = χ²/df: 3.093

Conclusion: OVERDISPERSION (φ̂ &gt;&gt; 1)
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (c): Comparing standard errors</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Standard Poisson SE</span>
<span class="n">se_poisson</span> <span class="o">=</span> <span class="n">poisson_result</span><span class="o">.</span><span class="n">bse</span>

<span class="c1"># Quasi-Poisson SE (inflate by sqrt(phi_hat))</span>
<span class="n">se_quasi</span> <span class="o">=</span> <span class="n">se_poisson</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">phi_hat</span><span class="p">)</span>

<span class="c1"># Robust (sandwich) SE</span>
<span class="n">poisson_robust</span> <span class="o">=</span> <span class="n">poisson_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">cov_type</span><span class="o">=</span><span class="s1">&#39;HC1&#39;</span><span class="p">)</span>
<span class="n">se_robust</span> <span class="o">=</span> <span class="n">poisson_robust</span><span class="o">.</span><span class="n">bse</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== Standard Error Comparison ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Method&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;SE(β₀)&#39;</span><span class="si">:</span><span class="s2">&lt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;SE(β₁)&#39;</span><span class="si">:</span><span class="s2">&lt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">44</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Standard Poisson&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">se_poisson</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">se_poisson</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;12.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Quasi-Poisson&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">se_quasi</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">se_quasi</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;12.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Robust (HC1)&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">se_robust</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">se_robust</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;12.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Impact on z-statistics</span>
<span class="n">z_poisson</span> <span class="o">=</span> <span class="n">poisson_result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">se_poisson</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">z_quasi</span> <span class="o">=</span> <span class="n">poisson_result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">se_quasi</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">z_robust</span> <span class="o">=</span> <span class="n">poisson_result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">se_robust</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Impact on z-statistic for β₁ ---&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Standard Poisson: z = </span><span class="si">{</span><span class="n">z_poisson</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> (p &lt; 0.001)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quasi-Poisson:    z = </span><span class="si">{</span><span class="n">z_quasi</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Robust (HC1):     z = </span><span class="si">{</span><span class="n">z_robust</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>=== Standard Error Comparison ===
Method               SE(β₀)       SE(β₁)
--------------------------------------------
Standard Poisson     0.0745       0.0372
Quasi-Poisson        0.1310       0.0654
Robust (HC1)         0.1289       0.0628

--- Impact on z-statistic for β₁ ---
Standard Poisson: z = 13.44 (p &lt; 0.001)
Quasi-Poisson:    z = 7.65
Robust (HC1):     z = 7.96
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): Negative binomial model</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Negative binomial GLM</span>
<span class="c1"># statsmodels NegativeBinomial uses log link by default</span>
<span class="n">nb_model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">NegativeBinomial</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">))</span>
<span class="n">nb_result</span> <span class="o">=</span> <span class="n">nb_model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== Model Comparison ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Model&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;β₀&#39;</span><span class="si">:</span><span class="s2">&lt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;β₁&#39;</span><span class="si">:</span><span class="s2">&lt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;SE(β₁)&#39;</span><span class="si">:</span><span class="s2">&lt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;AIC&#39;</span><span class="si">:</span><span class="s2">&lt;10</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Poisson&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">poisson_result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;10.4f</span><span class="si">}</span><span class="s2"> &quot;</span>
      <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">poisson_result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;10.4f</span><span class="si">}</span><span class="s2"> &quot;</span>
      <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">poisson_result</span><span class="o">.</span><span class="n">bse</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;10.4f</span><span class="si">}</span><span class="s2"> &quot;</span>
      <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">poisson_result</span><span class="o">.</span><span class="n">aic</span><span class="si">:</span><span class="s2">&lt;10.1f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Quasi-Poisson&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">poisson_result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;10.4f</span><span class="si">}</span><span class="s2"> &quot;</span>
      <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">poisson_result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;10.4f</span><span class="si">}</span><span class="s2"> &quot;</span>
      <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">se_quasi</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;10.4f</span><span class="si">}</span><span class="s2"> &quot;</span>
      <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;N/A&#39;</span><span class="si">:</span><span class="s2">&lt;10</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Negative Binomial&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">nb_result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;10.4f</span><span class="si">}</span><span class="s2"> &quot;</span>
      <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">nb_result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;10.4f</span><span class="si">}</span><span class="s2"> &quot;</span>
      <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">nb_result</span><span class="o">.</span><span class="n">bse</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;10.4f</span><span class="si">}</span><span class="s2"> &quot;</span>
      <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">nb_result</span><span class="o">.</span><span class="n">aic</span><span class="si">:</span><span class="s2">&lt;10.1f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">True values: β₀ = 1.0, β₁ = 0.5&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>=== Model Comparison ===
Model                β₀         β₁         SE(β₁)     AIC
------------------------------------------------------------
Poisson              0.9547     0.4998     0.0372     1289.3
Quasi-Poisson        0.9547     0.4998     0.0654     N/A
Negative Binomial    0.9612     0.4921     0.0687     1042.8

True values: β₀ = 1.0, β₁ = 0.5
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Point estimates are similar</strong>: All methods give nearly identical <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> values—overdispersion affects inference (SEs, p-values), not point estimation.</p></li>
<li><p class="sd-card-text"><strong>Standard Poisson SEs are too small by factor ~1.8</strong>: The z-statistic drops from 13.4 to 7.6 when using corrected SEs—still significant, but much less extreme.</p></li>
<li><p class="sd-card-text"><strong>Quasi-Poisson and robust SEs agree</strong>: Both give approximately correct SEs without modeling the full distribution—quasi multiplies by <span class="math notranslate nohighlight">\(\sqrt{\hat{\phi}}\)</span>, robust uses sandwich formula.</p></li>
<li><p class="sd-card-text"><strong>Negative binomial has much better AIC</strong>: AIC drops from 1289 to 1043—the negative binomial properly models the overdispersion, giving both valid inference and better fit.</p></li>
<li><p class="sd-card-text"><strong>Practical decision tree</strong>:</p>
<ul class="simple">
<li><p class="sd-card-text">Always compute <span class="math notranslate nohighlight">\(\hat{\phi} = \chi^2/(n-p)\)</span> after fitting Poisson</p></li>
<li><p class="sd-card-text">If <span class="math notranslate nohighlight">\(\hat{\phi} &gt; 1.5\)</span>: use quasi-Poisson, robust SEs, or negative binomial</p></li>
<li><p class="sd-card-text">If <span class="math notranslate nohighlight">\(\hat{\phi} &gt; 2\)</span>: strongly prefer negative binomial for proper modeling</p></li>
<li><p class="sd-card-text">Report the overdispersion in your analysis</p></li>
</ul>
</li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 3.5.5: Comparing Wald, LR, and Score Tests</p>
<p>This exercise empirically compares the three classical tests for GLM inference under various conditions.</p>
<div class="note admonition">
<p class="admonition-title">Background: The Testing Triad</p>
<p>Wald, likelihood ratio (LR), and score tests are asymptotically equivalent but can differ substantially in finite samples. Understanding their relative performance helps you choose the right test for your application. The LR test is generally most reliable but requires fitting two models; the Wald test is convenient (available in model summaries) but can be anti-conservative in small samples; the score test only requires fitting the null model.</p>
</div>
<ol class="loweralpha simple">
<li><p>Generate logistic regression data with <span class="math notranslate nohighlight">\(n = 100\)</span>, true <span class="math notranslate nohighlight">\(\beta_0 = 0, \beta_1 = 0.5, \beta_2 = 0\)</span> (so <span class="math notranslate nohighlight">\(\beta_2\)</span> is truly zero).</p></li>
<li><p>For testing <span class="math notranslate nohighlight">\(H_0: \beta_2 = 0\)</span>, compute all three test statistics:</p>
<ul class="simple">
<li><p>Wald: <span class="math notranslate nohighlight">\(z^2 = (\hat{\beta}_2/\text{SE})^2\)</span></p></li>
<li><p>LR: <span class="math notranslate nohighlight">\(D_{\text{reduced}} - D_{\text{full}}\)</span></p></li>
<li><p>Score: evaluate score at null parameters</p></li>
</ul>
</li>
<li><p>Run a simulation with 10,000 replications. Under <span class="math notranslate nohighlight">\(H_0\)</span>, p-values should be uniformly distributed. Create Q-Q plots against <span class="math notranslate nohighlight">\(\text{Uniform}(0,1)\)</span> and compute actual Type I error rates at <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span>.</p></li>
<li><p>Repeat with <span class="math notranslate nohighlight">\(n = 30\)</span> (small sample). Which test maintains the best calibration?</p></li>
</ol>
<div class="tip admonition">
<p class="admonition-title">Hint</p>
<p>The score test evaluates the gradient of the log-likelihood at the null hypothesis parameters. For logistic regression testing <span class="math notranslate nohighlight">\(\beta_2 = 0\)</span>, fit the reduced model (without <span class="math notranslate nohighlight">\(x_2\)</span>), get predicted probabilities <span class="math notranslate nohighlight">\(\hat{\mu}_i\)</span>, then compute <span class="math notranslate nohighlight">\(\text{Score} = \sum_i (y_i - \hat{\mu}_i)x_{i2}\)</span> and <span class="math notranslate nohighlight">\(\text{Info} = \sum_i \hat{\mu}_i(1-\hat{\mu}_i)x_{i2}^2\)</span>. The score statistic is <span class="math notranslate nohighlight">\(\text{Score}^2/\text{Info} \sim \chi^2_1\)</span> under <span class="math notranslate nohighlight">\(H_0\)</span>.</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Parts (a)-(c): Test comparison simulation</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">compare_tests_simulation</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">n_sim</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare Wald, LR, and Score tests via simulation.&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="n">p_wald</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">p_lr</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">p_score</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
        <span class="c1"># Generate data under H0: β₂ = 0</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>  <span class="c1"># This has no effect</span>
        <span class="n">eta</span> <span class="o">=</span> <span class="mi">0</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">+</span> <span class="mi">0</span> <span class="o">*</span> <span class="n">x2</span>  <span class="c1"># β₂ = 0</span>
        <span class="n">p</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">eta</span><span class="p">))</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>

        <span class="n">X_full</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">])</span>
        <span class="n">X_reduced</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">x1</span><span class="p">])</span>

        <span class="c1"># Fit models</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">full</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X_full</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Binomial</span><span class="p">())</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
            <span class="n">reduced</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X_reduced</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Binomial</span><span class="p">())</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

            <span class="c1"># Wald test for β₂</span>
            <span class="n">z_wald</span> <span class="o">=</span> <span class="n">full</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">/</span> <span class="n">full</span><span class="o">.</span><span class="n">bse</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
            <span class="n">p_wald</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">z_wald</span><span class="p">))))</span>

            <span class="c1"># LR test</span>
            <span class="n">lr_stat</span> <span class="o">=</span> <span class="n">reduced</span><span class="o">.</span><span class="n">deviance</span> <span class="o">-</span> <span class="n">full</span><span class="o">.</span><span class="n">deviance</span>
            <span class="n">p_lr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">chi2</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">lr_stat</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

            <span class="c1"># Score test (approximation using fitted reduced model)</span>
            <span class="c1"># Score at reduced: evaluate gradient of full likelihood at reduced params</span>
            <span class="n">mu_reduced</span> <span class="o">=</span> <span class="n">reduced</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_full</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">])</span>
            <span class="n">residuals</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">mu_reduced</span>
            <span class="n">score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">residuals</span> <span class="o">*</span> <span class="n">x2</span><span class="p">)</span>
            <span class="n">info_approx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">mu_reduced</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">mu_reduced</span><span class="p">)</span> <span class="o">*</span> <span class="n">x2</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">score_stat</span> <span class="o">=</span> <span class="n">score</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">info_approx</span>
            <span class="n">p_score</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">chi2</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">score_stat</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
            <span class="k">continue</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">p_wald</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">p_lr</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">p_score</span><span class="p">)</span>

<span class="c1"># Run simulation</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Running simulation with n=100...&quot;</span><span class="p">)</span>
<span class="n">p_wald_100</span><span class="p">,</span> <span class="n">p_lr_100</span><span class="p">,</span> <span class="n">p_score_100</span> <span class="o">=</span> <span class="n">compare_tests_simulation</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Running simulation with n=30...&quot;</span><span class="p">)</span>
<span class="n">p_wald_30</span><span class="p">,</span> <span class="n">p_lr_30</span><span class="p">,</span> <span class="n">p_score_30</span> <span class="o">=</span> <span class="n">compare_tests_simulation</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Visualization and comparison:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">p_vals</span><span class="p">,</span> <span class="n">title</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span>
    <span class="p">(</span><span class="n">p_wald_100</span><span class="p">,</span> <span class="s1">&#39;Wald (n=100)&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="n">p_lr_100</span><span class="p">,</span> <span class="s1">&#39;LR (n=100)&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="n">p_score_100</span><span class="p">,</span> <span class="s1">&#39;Score (n=100)&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="n">p_wald_30</span><span class="p">,</span> <span class="s1">&#39;Wald (n=30)&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="n">p_lr_30</span><span class="p">,</span> <span class="s1">&#39;LR (n=30)&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="n">p_score_30</span><span class="p">,</span> <span class="s1">&#39;Score (n=30)&#39;</span><span class="p">)</span>
<span class="p">]):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">3</span><span class="p">]</span>

    <span class="c1"># Q-Q plot against Uniform(0,1)</span>
    <span class="n">sorted_p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">p_vals</span><span class="p">)</span>
    <span class="n">theoretical</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sorted_p</span><span class="p">))</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theoretical</span><span class="p">,</span> <span class="n">sorted_p</span><span class="p">,</span> <span class="s1">&#39;b.&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Theoretical Uniform&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Observed p-value&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Type I error at α = 0.05</span>
    <span class="n">type1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p_vals</span> <span class="o">&lt;</span> <span class="mf">0.05</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;Type I @ 0.05: </span><span class="si">{</span><span class="n">type1</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;test_comparison.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== Type I Error Rates at α = 0.05 ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Test&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;n=100&#39;</span><span class="si">:</span><span class="s2">&lt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;n=30&#39;</span><span class="si">:</span><span class="s2">&lt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Target&#39;</span><span class="si">:</span><span class="s2">&lt;10</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">45</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Wald&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p_wald_100</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mf">0.05</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">     </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p_wald_30</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mf">0.05</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">     0.050&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;LR&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p_lr_100</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mf">0.05</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">     </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p_lr_30</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mf">0.05</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">     0.050&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Score&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p_score_100</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mf">0.05</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">     </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p_score_30</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mf">0.05</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">     0.050&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>=== Type I Error Rates at α = 0.05 ===
Test            n=100      n=30       Target
---------------------------------------------
Wald            0.051      0.062      0.050
LR              0.049      0.052      0.050
Score           0.048      0.047      0.050
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>All tests well-calibrated at n=100</strong>: Type I error rates are within simulation error of the nominal 0.05—all three tests work well with moderate sample sizes.</p></li>
<li><p class="sd-card-text"><strong>Wald is anti-conservative at n=30</strong>: Type I error = 0.062 represents 24% inflation above nominal. This is a known issue—Wald relies heavily on asymptotics and the <span class="math notranslate nohighlight">\(\chi^2\)</span> approximation breaks down in small samples.</p></li>
<li><p class="sd-card-text"><strong>LR and Score maintain calibration</strong>: Both tests stay near 0.05 even with n=30. The LR test uses the actual likelihood, while the score test only requires the null model fit.</p></li>
<li><p class="sd-card-text"><strong>Why Wald fails in small samples</strong>: The Wald test uses the asymptotic variance formula <span class="math notranslate nohighlight">\(\text{Var}(\hat{\beta}) \approx [I(\hat{\beta})]^{-1}\)</span>, which can be inaccurate when the sample is small or the information matrix estimate is unstable.</p></li>
<li><p class="sd-card-text"><strong>Practical recommendations</strong>:</p>
<ul class="simple">
<li><p class="sd-card-text"><strong>Default choice</strong>: Use LR test (most reliable, invariant to parameterization)</p></li>
<li><p class="sd-card-text"><strong>Quick screening</strong>: Wald from model summary is convenient, but verify important conclusions with LR</p></li>
<li><p class="sd-card-text"><strong>When you can’t fit the full model</strong>: Score test only requires the null model</p></li>
<li><p class="sd-card-text"><strong>Small samples (n &lt; 50)</strong>: Avoid Wald; prefer LR or exact methods</p></li>
</ul>
</li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 3.5.6: Complete GLM Analysis Workflow</p>
<p>This capstone exercise synthesizes all GLM concepts into a complete analysis workflow, mimicking real-world data analysis.</p>
<div class="note admonition">
<p class="admonition-title">Background: The Complete Analysis</p>
<p>Real GLM analysis involves many steps: choosing the appropriate family based on response type, fitting the model, checking diagnostics, handling violations, comparing alternative models, and interpreting results for stakeholders. This exercise walks through a complete analysis from start to finish, integrating all the concepts from this section.</p>
</div>
<p><strong>Scenario</strong>: You have data on hospital length of stay (positive continuous, in days) with predictors: age (continuous), admission type (emergency=1 vs. elective=0), and number of diagnoses (count).</p>
<ol class="loweralpha simple">
<li><p>Choose an appropriate GLM family and link function. Justify your choice based on the response type and expected variance structure.</p></li>
<li><p>Fit the model and interpret all coefficients on the original scale (not link scale). What is the estimated multiplicative effect of emergency admission?</p></li>
<li><p>Perform model diagnostics:</p>
<ul class="simple">
<li><p>Check deviance and Pearson residuals</p></li>
<li><p>Assess influential observations using Cook’s distance</p></li>
<li><p>Verify the variance function assumption via residual plots</p></li>
</ul>
</li>
<li><p>Compare to alternative models using AIC/BIC. Consider different links (identity vs. log) or different families (Normal vs. Gamma vs. Inverse Gaussian).</p></li>
<li><p>Compute and interpret 95% confidence intervals for expected length of stay for two patient profiles: (i) 40-year-old elective patient with 2 diagnoses, (ii) 70-year-old emergency patient with 6 diagnoses.</p></li>
</ol>
<div class="tip admonition">
<p class="admonition-title">Hint</p>
<p>For positive continuous data with variance increasing with the mean, consider the Gamma family. The log link is often preferred over the canonical inverse link because it: (1) ensures positive predictions, (2) gives multiplicative coefficient interpretations (exp(<span class="math notranslate nohighlight">\(\beta\)</span>) is a multiplier), and (3) is more numerically stable. For confidence intervals on the mean response, work on the link scale first, then back-transform.</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Complete analysis workflow:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Simulate realistic hospital length-of-stay data</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">789</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">300</span>

<span class="n">age</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">emergency</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>  <span class="c1"># 40% emergency admissions</span>
<span class="n">n_diagnoses</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># At least 1 diagnosis</span>

<span class="c1"># True model: Gamma with log link</span>
<span class="c1"># log(μ) = 0.5 + 0.02*age + 0.5*emergency + 0.15*n_diagnoses</span>
<span class="n">log_mu</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">+</span> <span class="mf">0.02</span> <span class="o">*</span> <span class="n">age</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">emergency</span> <span class="o">+</span> <span class="mf">0.15</span> <span class="o">*</span> <span class="n">n_diagnoses</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_mu</span><span class="p">)</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># Shape parameter (CV ≈ 0.58)</span>

<span class="n">los</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">mu</span><span class="o">/</span><span class="n">alpha</span><span class="p">)</span>  <span class="c1"># Length of stay</span>

<span class="c1"># Part (a): Model selection justification</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;PART (a): MODEL SELECTION&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Response: Length of stay (LOS)</span>
<span class="s2">- Strictly positive continuous → rules out Normal (allows negative)</span>
<span class="s2">- Right-skewed → Gamma or Inverse Gaussian</span>
<span class="s2">- Variance likely increases with mean (longer stays more variable)</span>

<span class="s2">Exploratory analysis:</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean LOS: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">los</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> days&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;SD LOS: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">los</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> days&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;CV: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">los</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">los</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Skewness: </span><span class="si">{</span><span class="n">stats</span><span class="o">.</span><span class="n">skew</span><span class="p">(</span><span class="n">los</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Choice: Gamma GLM with log link</span>
<span class="s2">- Gamma: V(μ) = μ² → constant CV, appropriate for positive skewed data</span>
<span class="s2">- Log link: ensures positive predictions, multiplicative interpretation</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="c1"># Part (b): Model fitting and interpretation</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;PART (b): MODEL FITTING AND INTERPRETATION&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span>
    <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span>
    <span class="n">age</span><span class="p">,</span>
    <span class="n">emergency</span><span class="p">,</span>
    <span class="n">n_diagnoses</span>
<span class="p">])</span>

<span class="n">gamma_model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">los</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span>
                     <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Gamma</span><span class="p">(</span><span class="n">link</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">links</span><span class="o">.</span><span class="n">Log</span><span class="p">()))</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">gamma_model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Coefficient Interpretation ---&quot;</span><span class="p">)</span>
<span class="n">coef_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Intercept&#39;</span><span class="p">,</span> <span class="s1">&#39;Age&#39;</span><span class="p">,</span> <span class="s1">&#39;Emergency&#39;</span><span class="p">,</span> <span class="s1">&#39;N_Diagnoses&#39;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">coef_names</span><span class="p">):</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">se</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">bse</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">: β = </span><span class="si">{</span><span class="n">beta</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  → Baseline LOS = exp(</span><span class="si">{</span><span class="n">beta</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">) = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> days&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">: β = </span><span class="si">{</span><span class="n">beta</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, exp(β) = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;Age&#39;</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  → Each year of age multiplies LOS by </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  → 10-year increase: ×</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">10</span><span class="o">*</span><span class="n">beta</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;Emergency&#39;</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  → Emergency admission increases LOS by </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;N_Diagnoses&#39;</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  → Each additional diagnosis increases LOS by </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>

<span class="c1"># Part (c): Diagnostics</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;PART (c): MODEL DIAGNOSTICS&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

<span class="c1"># Residuals</span>
<span class="n">mu_hat</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>
<span class="n">pearson_resid</span> <span class="o">=</span> <span class="p">(</span><span class="n">los</span> <span class="o">-</span> <span class="n">mu_hat</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mu_hat</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">alpha</span><span class="p">)</span>
<span class="n">deviance_resid</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">resid_deviance</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Pearson residuals: mean = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">pearson_resid</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, &quot;</span>
      <span class="sa">f</span><span class="s2">&quot;std = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">pearson_resid</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Deviance residuals: mean = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">deviance_resid</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, &quot;</span>
      <span class="sa">f</span><span class="s2">&quot;std = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">deviance_resid</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Dispersion estimate</span>
<span class="n">phi_hat</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">pearson_chi2</span> <span class="o">/</span> <span class="n">result</span><span class="o">.</span><span class="n">df_resid</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Dispersion estimate: φ̂ = </span><span class="si">{</span><span class="n">phi_hat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True 1/α = </span><span class="si">{</span><span class="mi">1</span><span class="o">/</span><span class="n">alpha</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Part (d): Model comparison</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;PART (d): MODEL COMPARISON&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

<span class="c1"># Alternative 1: Gamma with inverse (canonical) link</span>
<span class="n">gamma_inv</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">los</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Gamma</span><span class="p">())</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Alternative 2: Inverse Gaussian with log link</span>
<span class="n">ig_model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">los</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span>
                 <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">InverseGaussian</span><span class="p">(</span><span class="n">link</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">links</span><span class="o">.</span><span class="n">Log</span><span class="p">()))</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Alternative 3: Log-normal (fit linear model to log(y))</span>
<span class="n">lm_log</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">los</span><span class="p">),</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Model&#39;</span><span class="si">:</span><span class="s2">&lt;30</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;AIC&#39;</span><span class="si">:</span><span class="s2">&lt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Deviance&#39;</span><span class="si">:</span><span class="s2">&lt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">54</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Gamma + log link&#39;</span><span class="si">:</span><span class="s2">&lt;30</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">aic</span><span class="si">:</span><span class="s2">&lt;12.2f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">deviance</span><span class="si">:</span><span class="s2">&lt;12.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Gamma + inverse link&#39;</span><span class="si">:</span><span class="s2">&lt;30</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">gamma_inv</span><span class="o">.</span><span class="n">aic</span><span class="si">:</span><span class="s2">&lt;12.2f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">gamma_inv</span><span class="o">.</span><span class="n">deviance</span><span class="si">:</span><span class="s2">&lt;12.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Inverse Gaussian + log&#39;</span><span class="si">:</span><span class="s2">&lt;30</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">ig_model</span><span class="o">.</span><span class="n">aic</span><span class="si">:</span><span class="s2">&lt;12.2f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">ig_model</span><span class="o">.</span><span class="n">deviance</span><span class="si">:</span><span class="s2">&lt;12.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Log-normal (OLS on log y)&#39;</span><span class="si">:</span><span class="s2">&lt;30</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;N/A&#39;</span><span class="si">:</span><span class="s2">&lt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;N/A&#39;</span><span class="si">:</span><span class="s2">&lt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Part (e): Confidence intervals for predictions</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;PART (e): PREDICTION WITH CONFIDENCE INTERVALS&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

<span class="c1"># Predictions for specific patients</span>
<span class="n">patients</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s1">&#39;desc&#39;</span><span class="p">:</span> <span class="s1">&#39;40yo elective, 2 diagnoses&#39;</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]},</span>
    <span class="p">{</span><span class="s1">&#39;desc&#39;</span><span class="p">:</span> <span class="s1">&#39;70yo emergency, 5 diagnoses&#39;</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">]},</span>
    <span class="p">{</span><span class="s1">&#39;desc&#39;</span><span class="p">:</span> <span class="s1">&#39;55yo elective, 3 diagnoses&#39;</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">55</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">]}</span>
<span class="p">]</span>

<span class="k">for</span> <span class="n">patient</span> <span class="ow">in</span> <span class="n">patients</span><span class="p">:</span>
    <span class="n">x_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">patient</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>

    <span class="c1"># Point prediction on link scale</span>
    <span class="n">eta_hat</span> <span class="o">=</span> <span class="n">x_new</span> <span class="o">@</span> <span class="n">result</span><span class="o">.</span><span class="n">params</span>
    <span class="n">mu_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">eta_hat</span><span class="p">)</span>

    <span class="c1"># SE on link scale via delta method</span>
    <span class="c1"># Var(η̂) = x&#39; Var(β̂) x</span>
    <span class="n">var_eta</span> <span class="o">=</span> <span class="n">x_new</span> <span class="o">@</span> <span class="n">result</span><span class="o">.</span><span class="n">cov_params</span><span class="p">()</span> <span class="o">@</span> <span class="n">x_new</span>
    <span class="n">se_eta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_eta</span><span class="p">)</span>

    <span class="c1"># CI on link scale, then transform</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.975</span><span class="p">)</span>
    <span class="n">eta_lower</span> <span class="o">=</span> <span class="n">eta_hat</span> <span class="o">-</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se_eta</span>
    <span class="n">eta_upper</span> <span class="o">=</span> <span class="n">eta_hat</span> <span class="o">+</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se_eta</span>
    <span class="n">mu_lower</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">eta_lower</span><span class="p">)</span>
    <span class="n">mu_upper</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">eta_upper</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="n">patient</span><span class="p">[</span><span class="s1">&#39;desc&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Expected LOS: </span><span class="si">{</span><span class="n">mu_hat</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> days&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  95% CI: [</span><span class="si">{</span><span class="n">mu_lower</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">mu_upper</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">] days&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>Key insights from the analysis:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Model selection</strong>: Gamma with log link is appropriate for positive, right-skewed response with variance increasing proportionally to <span class="math notranslate nohighlight">\(\mu^2\)</span>.</p></li>
<li><p class="sd-card-text"><strong>Interpretation</strong>: All effects are multiplicative due to the log link. Emergency admission increases expected LOS by approximately 65%, each additional diagnosis by about 16%.</p></li>
<li><p class="sd-card-text"><strong>Diagnostics</strong>: Deviance residuals should be approximately standard normal; check for patterns against fitted values.</p></li>
<li><p class="sd-card-text"><strong>Model comparison</strong>: AIC allows comparison of non-nested models; Gamma + log typically outperforms alternatives for length-of-stay data.</p></li>
<li><p class="sd-card-text"><strong>Prediction intervals</strong>: Compute on link scale for correct coverage, then back-transform. The asymmetric intervals reflect the skewed nature of the response.</p></li>
</ol>
</div>
</details></div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<p><strong>Foundational Works</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="nelderwedderburn1972" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>NelderWedderburn1972<span class="fn-bracket">]</span></span>
<p>Nelder, J. A., and Wedderburn, R. W. M. (1972). Generalized linear models. <em>Journal of the Royal Statistical Society, Series A</em>, 135(3), 370–384. The foundational paper introducing the GLM framework, demonstrating that logistic regression, Poisson regression, and other techniques share a common mathematical structure based on exponential families.</p>
</div>
<div class="citation" id="wedderburn1974" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Wedderburn1974<span class="fn-bracket">]</span></span>
<p>Wedderburn, R. W. M. (1974). Quasi-likelihood functions, generalized linear models, and the Gauss-Newton method. <em>Biometrika</em>, 61(3), 439–447. Extends GLM theory to quasi-likelihood allowing inference when only mean-variance relationships are specified rather than full distributions.</p>
</div>
<div class="citation" id="mccullaghnelder1989" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>McCullaghNelder1989<span class="fn-bracket">]</span></span>
<p>McCullagh, P., and Nelder, J. A. (1989). <em>Generalized Linear Models</em> (2nd ed.). Chapman and Hall. The definitive reference on GLM theory and applications, covering exponential dispersion models, deviance, diagnostics, and extended quasi-likelihood.</p>
</div>
</div>
<p><strong>Early Developments in Component Models</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="berkson1944" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Berkson1944<span class="fn-bracket">]</span></span>
<p>Berkson, J. (1944). Application of the logistic function to bio-assay. <em>Journal of the American Statistical Association</em>, 39(227), 357–365. Early systematic treatment of logistic regression for dose-response analysis.</p>
</div>
<div class="citation" id="cox1958" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Cox1958<span class="fn-bracket">]</span></span>
<p>Cox, D. R. (1958). The regression analysis of binary sequences. <em>Journal of the Royal Statistical Society, Series B</em>, 20(2), 215–242. Foundational paper on binary regression with discussion of logit, probit, and complementary log-log links.</p>
</div>
<div class="citation" id="finney1952" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Finney1952<span class="fn-bracket">]</span></span>
<p>Finney, D. J. (1952). <em>Probit Analysis</em>. Cambridge University Press. Classic treatment of probit analysis for bioassay predating the unified GLM framework.</p>
</div>
</div>
<p><strong>Exponential Dispersion Models</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="jorgensen1987" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Jorgensen1987<span class="fn-bracket">]</span></span>
<p>Jørgensen, B. (1987). Exponential dispersion models. <em>Journal of the Royal Statistical Society, Series B</em>, 49(2), 127–162. Develops the theory of exponential dispersion models underlying GLMs, introducing the dispersion parameter and variance function.</p>
</div>
<div class="citation" id="jorgensen1997" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Jorgensen1997<span class="fn-bracket">]</span></span>
<p>Jørgensen, B. (1997). <em>The Theory of Dispersion Models</em>. Chapman and Hall. Comprehensive monograph on dispersion models extending GLM theory.</p>
</div>
</div>
<p><strong>The IRLS Algorithm</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="green1984" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Green1984<span class="fn-bracket">]</span></span>
<p>Green, P. J. (1984). Iteratively reweighted least squares for maximum likelihood estimation, and some robust and resistant alternatives. <em>Journal of the Royal Statistical Society, Series B</em>, 46(2), 149–192. Detailed analysis of IRLS including convergence properties and extensions to robust estimation.</p>
</div>
<div class="citation" id="pregibon1981" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Pregibon1981<span class="fn-bracket">]</span></span>
<p>Pregibon, D. (1981). Logistic regression diagnostics. <em>Annals of Statistics</em>, 9(4), 705–724. Develops diagnostic methods for logistic regression including hat values and influence measures.</p>
</div>
</div>
<p><strong>Logistic Regression</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="hosmer2013" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Hosmer2013<span class="fn-bracket">]</span></span>
<p>Hosmer, D. W., Lemeshow, S., and Sturdivant, R. X. (2013). <em>Applied Logistic Regression</em> (3rd ed.). Wiley. Comprehensive applied treatment of logistic regression with emphasis on model building and interpretation.</p>
</div>
<div class="citation" id="agresti2002" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Agresti2002<span class="fn-bracket">]</span></span>
<p>Agresti, A. (2002). <em>Categorical Data Analysis</em> (2nd ed.). Wiley. Graduate-level treatment of categorical data analysis including logistic regression, log-linear models, and extensions.</p>
</div>
<div class="citation" id="firth1993" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Firth1993<span class="fn-bracket">]</span></span>
<p>Firth, D. (1993). Bias reduction of maximum likelihood estimates. <em>Biometrika</em>, 80(1), 27–38. Introduces penalized likelihood to address separation problems in logistic regression where MLE does not exist.</p>
</div>
</div>
<p><strong>Poisson and Count Regression</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="cameron1998" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Cameron1998<span class="fn-bracket">]</span></span>
<p>Cameron, A. C., and Trivedi, P. K. (1998). <em>Regression Analysis of Count Data</em>. Cambridge University Press. Comprehensive treatment of count data regression including Poisson, negative binomial, and zero-inflated models.</p>
</div>
<div class="citation" id="mccullagh1984" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>McCullagh1984<span class="fn-bracket">]</span></span>
<p>McCullagh, P. (1984). Generalized linear models. <em>European Journal of Operational Research</em>, 16(3), 285–292. Accessible overview of GLM theory with applications.</p>
</div>
</div>
<p><strong>Overdispersion</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="breslow1984" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Breslow1984<span class="fn-bracket">]</span></span>
<p>Breslow, N. E. (1984). Extra-Poisson variation in log-linear models. <em>Journal of the Royal Statistical Society, Series C</em>, 33(1), 38–44. Addresses overdispersion in Poisson regression through random effects and quasi-likelihood.</p>
</div>
<div class="citation" id="dean1989" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Dean1989<span class="fn-bracket">]</span></span>
<p>Dean, C., and Lawless, J. F. (1989). Tests for detecting overdispersion in Poisson regression models. <em>Journal of the American Statistical Association</em>, 84(406), 467–472. Methods for detecting and testing overdispersion in count models.</p>
</div>
</div>
<p><strong>Model Diagnostics</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="mccullagh1983" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>McCullagh1983<span class="fn-bracket">]</span></span>
<p>McCullagh, P. (1983). Quasi-likelihood functions. <em>Annals of Statistics</em>, 11(1), 59–67. Theoretical development of quasi-likelihood extending GLM inference to non-exponential family models.</p>
</div>
<div class="citation" id="pierce1986" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Pierce1986<span class="fn-bracket">]</span></span>
<p>Pierce, D. A., and Schafer, D. W. (1986). Residuals in generalized linear models. <em>Journal of the American Statistical Association</em>, 81(396), 977–986. Develops residual analysis methods for GLMs including Pearson and deviance residuals.</p>
</div>
<div class="citation" id="collett2003" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Collett2003<span class="fn-bracket">]</span></span>
<p>Collett, D. (2003). <em>Modelling Binary Data</em> (2nd ed.). Chapman and Hall/CRC. Practical guide to binary regression with extensive coverage of diagnostics.</p>
</div>
</div>
<p><strong>Quasi-Likelihood and GEE</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="liang1986" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Liang1986<span class="fn-bracket">]</span></span>
<p>Liang, K.-Y., and Zeger, S. L. (1986). Longitudinal data analysis using generalized linear models. <em>Biometrika</em>, 73(1), 13–22. Introduces generalized estimating equations (GEE) extending GLMs to correlated data.</p>
</div>
<div class="citation" id="zeger1988" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Zeger1988<span class="fn-bracket">]</span></span>
<p>Zeger, S. L., and Liang, K.-Y. (1988). An overview of methods for the analysis of longitudinal data. <em>Statistics in Medicine</em>, 7(1–2), 13–28. Survey of methods for correlated data including GEE and random effects approaches.</p>
</div>
</div>
<p><strong>Modern Textbooks</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="dobson2018" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Dobson2018<span class="fn-bracket">]</span></span>
<p>Dobson, A. J., and Barnett, A. G. (2018). <em>An Introduction to Generalized Linear Models</em> (4th ed.). CRC Press. Accessible introduction to GLMs suitable for advanced undergraduates with worked examples and R code.</p>
</div>
<div class="citation" id="faraway2006" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Faraway2006<span class="fn-bracket">]</span></span>
<p>Faraway, J. J. (2006). <em>Extending the Linear Model with R: Generalized Linear, Mixed Effects and Nonparametric Regression Models</em>. Chapman and Hall/CRC. Practical R-based treatment of GLMs and extensions.</p>
</div>
<div class="citation" id="dunnsmyth2018" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DunnSmyth2018<span class="fn-bracket">]</span></span>
<p>Dunn, P. K., and Smyth, G. K. (2018). <em>Generalized Linear Models with Examples in R</em>. Springer. Modern treatment emphasizing the R implementation of GLMs with extensive examples.</p>
</div>
</div>
<p><strong>Historical Perspective</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="mccullagh1992" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>McCullagh1992<span class="fn-bracket">]</span></span>
<p>McCullagh, P. (1992). Introduction to Nelder and Wedderburn (1972) Generalized Linear Models. In S. Kotz and N. L. Johnson (Eds.), <em>Breakthroughs in Statistics</em>, Vol. II, 547–550. Springer. Reflection on the impact and significance of the original GLM paper.</p>
</div>
</div>
<p><strong>Software and Computation</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="baker1978" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Baker1978<span class="fn-bracket">]</span></span>
<p>Baker, R. J., and Nelder, J. A. (1978). <em>The GLIM System, Release 3: Generalized Linear Interactive Modelling</em>. Numerical Algorithms Group, Oxford. Documentation for the first comprehensive software implementation of GLMs.</p>
</div>
<div class="citation" id="hardin2007" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Hardin2007<span class="fn-bracket">]</span></span>
<p>Hardin, J. W., and Hilbe, J. M. (2007). <em>Generalized Linear Models and Extensions</em> (2nd ed.). Stata Press. Comprehensive guide to GLMs with emphasis on Stata implementation and practical applications.</p>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="ch3_4-linear-models.html" class="btn btn-neutral float-left" title="Linear Models" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ch3_6-chapter-summary.html" class="btn btn-neutral float-right" title="Chapter Summary" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>