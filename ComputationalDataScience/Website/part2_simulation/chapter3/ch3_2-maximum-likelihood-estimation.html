

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Maximum Likelihood Estimation &mdash; STAT 418</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=3d0abd52" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT418/Website/part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=30646c52"></script>
      <script>let toggleHintShow = 'Show solution';</script>
      <script>let toggleHintHide = 'Hide solution';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>window.MathJax = {"options": {"enableMenu": true, "menuOptions": {"settings": {"enrich": true, "speech": true, "braille": true, "collapsible": true, "assistiveMml": false}}}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
      <script src="../../_static/custom.js?v=8718e0ab"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Sampling Variability and Variance Estimation" href="ch3_3-sampling-variability.html" />
    <link rel="prev" title="Exponential Families" href="ch3_1-exponential-families.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Course Content</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../Homework/index.html">Homework Assignments</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Homework/index.html#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#assignment-policies">Assignment Policies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#recommended-workflow">Recommended Workflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#submission-requirements">Submission Requirements</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Homework/index.html#assignments-by-chapter">Assignments by Chapter</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#part-i-foundations">Part I: Foundations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Homework/hw1_distributional_relationships.html">Homework 1: Distributional Relationships and Computational Foundations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Homework/index.html#tips-for-success">Tips for Success</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#mathematical-derivations">Mathematical Derivations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#computational-verification">Computational Verification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#common-pitfalls">Common Pitfalls</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part1_foundations/index.html">Part I: Foundations of Probability and Computation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part1_foundations/chapter1/index.html">Chapter 1: Statistical Paradigms and Core Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html">Paradigms of Probability and Statistical Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#the-mathematical-foundation-kolmogorov-s-axioms">The Mathematical Foundation: Kolmogorov’s Axioms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#interpretations-of-probability">Interpretations of Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#statistical-inference-paradigms">Statistical Inference Paradigms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#historical-and-philosophical-debates">Historical and Philosophical Debates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#looking-ahead-our-course-focus">Looking Ahead: Our Course Focus</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html">Probability Distributions: Theory and Computation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#from-abstract-foundations-to-concrete-tools">From Abstract Foundations to Concrete Tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#the-python-ecosystem-for-probability">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#introduction-why-probability-distributions-matter">Introduction: Why Probability Distributions Matter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#id1">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#continuous-distributions">Continuous Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#additional-important-distributions">Additional Important Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#summary-and-practical-guidelines">Summary and Practical Guidelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#conclusion">Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html">Python Random Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#from-mathematical-distributions-to-computational-samples">From Mathematical Distributions to Computational Samples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-python-ecosystem-at-a-glance">The Python Ecosystem at a Glance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#understanding-pseudo-random-number-generation">Understanding Pseudo-Random Number Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-standard-library-random-module">The Standard Library: <code class="docutils literal notranslate"><span class="pre">random</span></code> Module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#numpy-fast-vectorized-random-sampling">NumPy: Fast Vectorized Random Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#scipy-stats-the-complete-statistical-toolkit">SciPy Stats: The Complete Statistical Toolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#bringing-it-all-together-library-selection-guide">Bringing It All Together: Library Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#looking-ahead-from-random-numbers-to-monte-carlo-methods">Looking Ahead: From Random Numbers to Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html">Chapter 1 Summary: Foundations in Place</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#the-three-pillars-of-chapter-1">The Three Pillars of Chapter 1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#what-lies-ahead-the-road-to-simulation">What Lies Ahead: The Road to Simulation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#chapter-1-exercises-synthesis-problems">Chapter 1 Exercises: Synthesis Problems</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Part II: Simulation-Based Methods</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../chapter2/index.html">Chapter 2: Monte Carlo Simulation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html">Monte Carlo Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#the-historical-development-of-monte-carlo-methods">The Historical Development of Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#the-core-principle-expectation-as-integration">The Core Principle: Expectation as Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#theoretical-foundations">Theoretical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#variance-estimation-and-confidence-intervals">Variance Estimation and Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#comparison-with-deterministic-methods">Comparison with Deterministic Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#sample-size-determination">Sample Size Determination</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#convergence-diagnostics-and-monitoring">Convergence Diagnostics and Monitoring</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#chapter-2-1-exercises-monte-carlo-fundamentals-mastery">Chapter 2.1 Exercises: Monte Carlo Fundamentals Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html">Uniform Random Variates</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#why-uniform-the-universal-currency-of-randomness">Why Uniform? The Universal Currency of Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#the-paradox-of-computational-randomness">The Paradox of Computational Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#chaotic-dynamical-systems-an-instructive-failure">Chaotic Dynamical Systems: An Instructive Failure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#linear-congruential-generators">Linear Congruential Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#shift-register-generators">Shift-Register Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#the-kiss-generator-combining-strategies">The KISS Generator: Combining Strategies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#modern-generators-mersenne-twister-and-pcg">Modern Generators: Mersenne Twister and PCG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#statistical-testing-of-random-number-generators">Statistical Testing of Random Number Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#chapter-2-2-exercises-uniform-random-variates-mastery">Chapter 2.2 Exercises: Uniform Random Variates Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html">Inverse CDF Method</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#mathematical-foundations">Mathematical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#continuous-distributions-with-closed-form-inverses">Continuous Distributions with Closed-Form Inverses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#numerical-inversion">Numerical Inversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#mixed-distributions">Mixed Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#chapter-2-3-exercises-inverse-cdf-method-mastery">Chapter 2.3 Exercises: Inverse CDF Method Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html">Transformation Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#why-transformation-methods">Why Transformation Methods?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-boxmuller-transform">The Box–Muller Transform</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-polar-marsaglia-method">The Polar (Marsaglia) Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#method-comparison-boxmuller-vs-polar-vs-ziggurat">Method Comparison: Box–Muller vs Polar vs Ziggurat</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-ziggurat-algorithm">The Ziggurat Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-clt-approximation-historical">The CLT Approximation (Historical)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#distributions-derived-from-the-normal">Distributions Derived from the Normal</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#multivariate-normal-generation">Multivariate Normal Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#implementation-guidance">Implementation Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#chapter-2-4-exercises-transformation-methods-mastery">Chapter 2.4 Exercises: Transformation Methods Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html">Rejection Sampling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-dartboard-intuition">The Dartboard Intuition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-accept-reject-algorithm">The Accept-Reject Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#efficiency-analysis">Efficiency Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#choosing-the-proposal-distribution">Choosing the Proposal Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-squeeze-principle">The Squeeze Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#geometric-example-sampling-from-the-unit-disk">Geometric Example: Sampling from the Unit Disk</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#limitations-and-the-curse-of-dimensionality">Limitations and the Curse of Dimensionality</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#connections-to-other-methods">Connections to Other Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#chapter-2-5-exercises-rejection-sampling-mastery">Chapter 2.5 Exercises: Rejection Sampling Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html">Variance Reduction Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#the-variance-reduction-paradigm">The Variance Reduction Paradigm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#importance-sampling">Importance Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#control-variates">Control Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#antithetic-variates">Antithetic Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#stratified-sampling">Stratified Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#common-random-numbers">Common Random Numbers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#conditional-monte-carlo-raoblackwellization">Conditional Monte Carlo (Rao–Blackwellization)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#combining-variance-reduction-techniques">Combining Variance Reduction Techniques</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#chapter-2-6-exercises-variance-reduction-mastery">Chapter 2.6 Exercises: Variance Reduction Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html">Chapter Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#the-complete-monte-carlo-workflow">The Complete Monte Carlo Workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#quick-reference-tables">Quick Reference Tables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#common-pitfalls-checklist">Common Pitfalls Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#connections-to-later-chapters">Connections to Later Chapters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#learning-outcomes-checklist">Learning Outcomes Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#further-reading-optimization-and-missing-data">Further Reading: Optimization and Missing Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Chapter 3: Parametric Inference and Likelihood Methods</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="ch3_1-exponential-families.html">Exponential Families</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#historical-origins-from-scattered-results-to-unified-theory">Historical Origins: From Scattered Results to Unified Theory</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#the-canonical-exponential-family">The Canonical Exponential Family</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#converting-familiar-distributions">Converting Familiar Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#the-log-partition-function-a-moment-generating-machine">The Log-Partition Function: A Moment-Generating Machine</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#sufficiency-capturing-all-parameter-information">Sufficiency: Capturing All Parameter Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#minimal-sufficiency-and-completeness">Minimal Sufficiency and Completeness</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#conjugate-priors-and-bayesian-inference">Conjugate Priors and Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#exponential-dispersion-models-and-glms">Exponential Dispersion Models and GLMs</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#chapter-3-1-exercises-exponential-families-mastery">Chapter 3.1 Exercises: Exponential Families Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Maximum Likelihood Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#the-likelihood-function">The Likelihood Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-score-function">The Score Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="#fisher-information">Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="#closed-form-maximum-likelihood-estimators">Closed-Form Maximum Likelihood Estimators</a></li>
<li class="toctree-l4"><a class="reference internal" href="#numerical-optimization-for-mle">Numerical Optimization for MLE</a></li>
<li class="toctree-l4"><a class="reference internal" href="#asymptotic-properties-of-mles">Asymptotic Properties of MLEs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-cramer-rao-lower-bound">The Cramér-Rao Lower Bound</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-invariance-property">The Invariance Property</a></li>
<li class="toctree-l4"><a class="reference internal" href="#likelihood-based-hypothesis-testing">Likelihood-Based Hypothesis Testing</a></li>
<li class="toctree-l4"><a class="reference internal" href="#confidence-intervals-from-likelihood">Confidence Intervals from Likelihood</a></li>
<li class="toctree-l4"><a class="reference internal" href="#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#connection-to-bayesian-inference">Connection to Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="#chapter-3-2-exercises-maximum-likelihood-estimation-mastery">Chapter 3.2 Exercises: Maximum Likelihood Estimation Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch3_3-sampling-variability.html">Sampling Variability and Variance Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#statistical-estimators-and-their-properties">Statistical Estimators and Their Properties</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#sampling-distributions">Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#the-delta-method">The Delta Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#variance-estimation-methods">Variance Estimation Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#applications-and-worked-examples">Applications and Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch3_4-linear-models.html">Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#matrix-calculus-foundations">Matrix Calculus Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#the-linear-model">The Linear Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#ordinary-least-squares-the-calculus-approach">Ordinary Least Squares: The Calculus Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#ordinary-least-squares-the-geometric-approach">Ordinary Least Squares: The Geometric Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#properties-of-the-ols-estimator">Properties of the OLS Estimator</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#the-gauss-markov-theorem">The Gauss-Markov Theorem</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#estimating-the-error-variance">Estimating the Error Variance</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#distributional-results-under-normality">Distributional Results Under Normality</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#diagnostics-and-model-checking">Diagnostics and Model Checking</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#numerical-stability-qr-decomposition">Numerical Stability: QR Decomposition</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#model-selection-and-information-criteria">Model Selection and Information Criteria</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#regularization-ridge-and-lasso">Regularization: Ridge and LASSO</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#chapter-3-4-exercises-linear-models-mastery">Chapter 3.4 Exercises: Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch3_5-generalized-linear-models.html">Generalized Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#historical-context-unification-of-regression-methods">Historical Context: Unification of Regression Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#the-glm-framework-three-components">The GLM Framework: Three Components</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#score-equations-and-fisher-information">Score Equations and Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#iteratively-reweighted-least-squares">Iteratively Reweighted Least Squares</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#logistic-regression-binary-outcomes">Logistic Regression: Binary Outcomes</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#poisson-regression-count-data">Poisson Regression: Count Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#gamma-regression-positive-continuous-data">Gamma Regression: Positive Continuous Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#inference-in-glms-the-testing-triad">Inference in GLMs: The Testing Triad</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#model-diagnostics">Model Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#model-comparison-and-selection">Model Comparison and Selection</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#quasi-likelihood-and-robust-inference">Quasi-Likelihood and Robust Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#further-reading">Further Reading</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#chapter-3-5-exercises-generalized-linear-models-mastery">Chapter 3.5 Exercises: Generalized Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch3_6-chapter-summary.html">Chapter Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#the-parametric-inference-pipeline">The Parametric Inference Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#the-five-pillars-of-chapter-3">The Five Pillars of Chapter 3</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#quick-reference-core-formulas">Quick Reference: Core Formulas</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#connections-to-future-material">Connections to Future Material</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#practical-guidance">Practical Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter4/index.html">Chapter 4: Resampling Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html">The Sampling Distribution Problem</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#the-fundamental-target-sampling-distributions">The Fundamental Target: Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#historical-development-the-quest-for-sampling-distributions">Historical Development: The Quest for Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#three-routes-to-the-sampling-distribution">Three Routes to the Sampling Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#when-asymptotics-fail-motivating-the-bootstrap">When Asymptotics Fail: Motivating the Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#the-plug-in-principle-theoretical-foundation">The Plug-In Principle: Theoretical Foundation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#computational-perspective-bootstrap-as-monte-carlo">Computational Perspective: Bootstrap as Monte Carlo</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#chapter-4-1-exercises">Chapter 4.1 Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html">The Empirical Distribution and Plug-in Principle</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#the-empirical-cumulative-distribution-function">The Empirical Cumulative Distribution Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#convergence-of-the-empirical-cdf">Convergence of the Empirical CDF</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#parameters-as-statistical-functionals">Parameters as Statistical Functionals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#when-the-plug-in-principle-fails">When the Plug-in Principle Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#the-bootstrap-idea-in-one-sentence">The Bootstrap Idea in One Sentence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#computational-implementation">Computational Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#section-4-2-exercises-ecdf-and-plug-in-mastery">Section 4.2 Exercises: ECDF and Plug-in Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html">The Nonparametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#the-bootstrap-principle">The Bootstrap Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-standard-errors">Bootstrap Standard Errors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-bias-estimation">Bootstrap Bias Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-confidence-intervals">Bootstrap Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-for-regression">Bootstrap for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-diagnostics">Bootstrap Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#when-bootstrap-fails">When Bootstrap Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html">Section 4.4: The Parametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#the-parametric-bootstrap-principle">The Parametric Bootstrap Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#location-scale-families">Location-Scale Families</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#parametric-bootstrap-for-regression">Parametric Bootstrap for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#confidence-intervals">Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#model-checking-and-validation">Model Checking and Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#when-parametric-bootstrap-fails">When Parametric Bootstrap Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#parametric-vs-nonparametric-a-decision-framework">Parametric vs. Nonparametric: A Decision Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html">Section 4.5: Jackknife Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#historical-context-and-motivation">Historical Context and Motivation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#the-delete-1-jackknife">The Delete-1 Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#jackknife-bias-estimation">Jackknife Bias Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#the-delete-d-jackknife">The Delete-<span class="math notranslate nohighlight">\(d\)</span> Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#jackknife-versus-bootstrap">Jackknife versus Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#the-infinitesimal-jackknife">The Infinitesimal Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part3_bayesian/index.html">Part III: Bayesian Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part3_bayesian/index.html#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html">Bayesian Philosophy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Part II: Simulation-Based Methods</a></li>
          <li class="breadcrumb-item"><a href="index.html">Chapter 3: Parametric Inference and Likelihood Methods</a></li>
      <li class="breadcrumb-item active">Maximum Likelihood Estimation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/part2_simulation/chapter3/ch3_2-maximum-likelihood-estimation.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="maximum-likelihood-estimation">
<span id="ch3-2-maximum-likelihood-estimation"></span><h1>Maximum Likelihood Estimation<a class="headerlink" href="#maximum-likelihood-estimation" title="Link to this heading"></a></h1>
<p>In the summer of 1912, a young statistician named Ronald Aylmer Fisher was grappling with a fundamental question: given a sample of data, how should we estimate the parameters of a probability distribution? Fisher was not satisfied with the existing answers—Karl Pearson’s method of moments, while simple, seemed to throw away information. Surely there was a principled way to extract <em>all</em> the information the data contained about the unknown parameters.</p>
<p>Fisher’s answer, published in a series of papers between 1912 and 1925, was maximum likelihood estimation: choose the parameter values that make the observed data most probable. This deceptively simple idea revolutionized statistical inference. Fisher showed that maximum likelihood estimators (MLEs) have remarkable properties—they are consistent, asymptotically normal, and asymptotically efficient, achieving the theoretical lower bound on estimator variance. These properties made MLE the workhorse of parametric inference for a century.</p>
<p>This section develops the theory and practice of maximum likelihood estimation. We begin with the likelihood function itself—the mathematical object that quantifies how well different parameter values explain the observed data. We derive the score function and Fisher information, which together characterize the geometry of the likelihood surface. For simple models, we obtain closed-form MLEs; for complex models, we develop numerical optimization algorithms including Newton-Raphson and Fisher scoring. We establish the asymptotic theory that justifies using MLEs for inference, including a complete proof of the Cramér-Rao lower bound. Finally, we connect MLE to hypothesis testing through likelihood ratio, Wald, and score tests.</p>
<p>The exponential family framework from <span class="xref std std-ref">Section 3.1</span> will prove essential here: for exponential families, the score equation takes a particularly elegant form, and the MLE has explicit connections to sufficient statistics. But MLE extends far beyond exponential families—it applies to any parametric model, making it the universal tool for parametric inference.</p>
<div class="important admonition">
<p class="admonition-title">Road Map 🧭</p>
<ul class="simple">
<li><p><strong>Understand</strong>: The likelihood function as a measure of parameter support, the score function as its gradient, and Fisher information as its curvature</p></li>
<li><p><strong>Derive</strong>: Closed-form MLEs for normal, exponential, Poisson, and Bernoulli distributions; understand why Gamma and Beta require numerical methods</p></li>
<li><p><strong>Implement</strong>: Newton-Raphson and Fisher scoring algorithms; leverage <code class="docutils literal notranslate"><span class="pre">scipy.optimize</span></code> for production code</p></li>
<li><p><strong>Prove</strong>: Asymptotic consistency, normality, and efficiency; the Cramér-Rao lower bound with full regularity conditions</p></li>
<li><p><strong>Apply</strong>: Likelihood ratio, Wald, and score tests for hypothesis testing; construct confidence intervals via multiple methods</p></li>
</ul>
</div>
<section id="the-likelihood-function">
<h2>The Likelihood Function<a class="headerlink" href="#the-likelihood-function" title="Link to this heading"></a></h2>
<p>The likelihood function is the foundation of maximum likelihood estimation. It answers a simple question: for fixed data, how probable would that data be under different parameter values?</p>
<section id="definition-and-interpretation">
<h3>Definition and Interpretation<a class="headerlink" href="#definition-and-interpretation" title="Link to this heading"></a></h3>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> be independent random variables, each with probability density (or mass) function <span class="math notranslate nohighlight">\(f(x|\theta)\)</span> depending on an unknown parameter <span class="math notranslate nohighlight">\(\theta \in \Theta\)</span>. After observing data <span class="math notranslate nohighlight">\(x_1, x_2, \ldots, x_n\)</span>, we define the <strong>likelihood function</strong>:</p>
<div class="math notranslate nohighlight" id="equation-likelihood-def">
<span class="eqno">(38)<a class="headerlink" href="#equation-likelihood-def" title="Link to this equation"></a></span>\[L(\theta) = L(\theta; x_1, \ldots, x_n) = \prod_{i=1}^{n} f(x_i | \theta)\]</div>
<p>The crucial conceptual shift is this: we view the likelihood as a function of <span class="math notranslate nohighlight">\(\theta\)</span> for <em>fixed</em> data, not as a probability of data for fixed <span class="math notranslate nohighlight">\(\theta\)</span>. The data are observed and therefore fixed; the parameter is unknown and therefore variable.</p>
<div class="warning admonition">
<p class="admonition-title">The Likelihood is Not a Probability Density</p>
<p>While <span class="math notranslate nohighlight">\(L(\theta)\)</span> is constructed from probability densities, it is <em>not</em> a probability density over <span class="math notranslate nohighlight">\(\theta\)</span>. There is no requirement that <span class="math notranslate nohighlight">\(\int L(\theta) d\theta = 1\)</span>, and indeed this integral often diverges or depends on the data in complex ways. The likelihood measures <em>relative support</em>—how much more or less the data support one parameter value versus another—not absolute probability.</p>
</div>
<p>The <strong>maximum likelihood estimator</strong> (MLE) is the parameter value that maximizes the likelihood:</p>
<div class="math notranslate nohighlight" id="equation-mle-def">
<span class="eqno">(39)<a class="headerlink" href="#equation-mle-def" title="Link to this equation"></a></span>\[\hat{\theta}_{\text{MLE}} = \arg\max_{\theta \in \Theta} L(\theta)\]</div>
<p>Intuitively, the MLE is the parameter value that makes the observed data most probable.</p>
</section>
<section id="the-log-likelihood">
<h3>The Log-Likelihood<a class="headerlink" href="#the-log-likelihood" title="Link to this heading"></a></h3>
<p>In practice, we almost always work with the <strong>log-likelihood</strong>:</p>
<div class="math notranslate nohighlight" id="equation-log-likelihood">
<span class="eqno">(40)<a class="headerlink" href="#equation-log-likelihood" title="Link to this equation"></a></span>\[\ell(\theta) = \log L(\theta) = \sum_{i=1}^{n} \log f(x_i | \theta)\]</div>
<p>This transformation offers multiple advantages:</p>
<ol class="arabic simple">
<li><p><strong>Numerical stability</strong>: Products of many small probabilities underflow floating-point arithmetic; sums of log-probabilities do not.</p></li>
<li><p><strong>Computational convenience</strong>: Sums are easier to differentiate and optimize than products.</p></li>
<li><p><strong>Theoretical elegance</strong>: Asymptotic theory for the log-likelihood has cleaner formulations.</p></li>
</ol>
<p>Since <span class="math notranslate nohighlight">\(\log\)</span> is monotonically increasing, maximizing <span class="math notranslate nohighlight">\(\ell(\theta)\)</span> is equivalent to maximizing <span class="math notranslate nohighlight">\(L(\theta)\)</span>. The MLE is unchanged.</p>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_2_fig01_likelihood_concept.png"><img alt="Likelihood function concept showing data, likelihood, and log-likelihood" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_2_fig01_likelihood_concept.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 83 </span><span class="caption-text"><strong>Figure 3.2.1</strong>: The likelihood function for Poisson data. (a) Histogram of observed counts with sample mean <span class="math notranslate nohighlight">\(\bar{x} = 3.10\)</span>. (b) Normalized likelihood function <span class="math notranslate nohighlight">\(L(\lambda)/L(\hat{\lambda})\)</span> showing the MLE at the peak. (c) Log-likelihood function <span class="math notranslate nohighlight">\(\ell(\lambda)\)</span>, whose curvature at the maximum relates to Fisher information. The true parameter <span class="math notranslate nohighlight">\(\lambda = 3.5\)</span> is marked for comparison.</span><a class="headerlink" href="#id1" title="Link to this image"></a></p>
</figcaption>
</figure>
<div class="note admonition">
<p class="admonition-title">Example 💡 Normal Likelihood</p>
<p><strong>Setup</strong>: Let <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} \mathcal{N}(\mu, \sigma^2)\)</span> with both parameters unknown. The density is:</p>
<div class="math notranslate nohighlight">
\[f(x|\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\]</div>
<p><strong>Log-likelihood derivation</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\ell(\mu, \sigma^2) &amp;= \sum_{i=1}^n \log f(x_i|\mu, \sigma^2) \\
&amp;= \sum_{i=1}^n \left[ -\frac{1}{2}\log(2\pi) - \frac{1}{2}\log(\sigma^2) - \frac{(x_i - \mu)^2}{2\sigma^2} \right] \\
&amp;= -\frac{n}{2}\log(2\pi) - \frac{n}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (x_i - \mu)^2\end{split}\]</div>
<p>This expression will be maximized shortly to derive the normal MLEs.</p>
</div>
</section>
</section>
<section id="the-score-function">
<h2>The Score Function<a class="headerlink" href="#the-score-function" title="Link to this heading"></a></h2>
<p>The <strong>score function</strong> is the gradient of the log-likelihood with respect to the parameters. It plays a central role in both the theory and computation of MLE.</p>
<section id="definition-and-properties">
<h3>Definition and Properties<a class="headerlink" href="#definition-and-properties" title="Link to this heading"></a></h3>
<p>For a scalar parameter <span class="math notranslate nohighlight">\(\theta\)</span>, the score function is:</p>
<div class="math notranslate nohighlight" id="equation-score-def">
<span class="eqno">(41)<a class="headerlink" href="#equation-score-def" title="Link to this equation"></a></span>\[U(\theta) = \frac{\partial \ell(\theta)}{\partial \theta} = \sum_{i=1}^{n} \frac{\partial}{\partial \theta} \log f(X_i | \theta)\]</div>
<p>For a parameter vector <span class="math notranslate nohighlight">\(\boldsymbol{\theta} = (\theta_1, \ldots, \theta_p)^\top\)</span>, the score is the gradient vector:</p>
<div class="math notranslate nohighlight">
\[\mathbf{U}(\boldsymbol{\theta}) = \nabla \ell(\boldsymbol{\theta}) = \left( \frac{\partial \ell}{\partial \theta_1}, \ldots, \frac{\partial \ell}{\partial \theta_p} \right)^\top\]</div>
<p>The MLE is typically found by solving the <strong>score equation</strong> <span class="math notranslate nohighlight">\(U(\hat{\theta}) = 0\)</span>. At the maximum, the gradient of the log-likelihood vanishes.</p>
<p>The score function has a fundamental property that underlies much of likelihood theory:</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Score Has Mean Zero</p>
<p>Under regularity conditions (see below), the expected value of the score function at the true parameter value is zero:</p>
<div class="math notranslate nohighlight" id="equation-score-mean-zero">
<span class="eqno">(42)<a class="headerlink" href="#equation-score-mean-zero" title="Link to this equation"></a></span>\[\mathbb{E}_{\theta_0}\left[ U(\theta_0) \right] = 0\]</div>
</div>
<p><strong>Proof</strong>: For a single observation, the score contribution is <span class="math notranslate nohighlight">\(u(X; \theta) = \frac{\partial}{\partial \theta} \log f(X|\theta)\)</span>. We compute:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbb{E}_{\theta}\left[ \frac{\partial}{\partial \theta} \log f(X|\theta) \right] &amp;= \int \frac{\partial}{\partial \theta} \log f(x|\theta) \cdot f(x|\theta) \, dx \\
&amp;= \int \frac{1}{f(x|\theta)} \cdot \frac{\partial f(x|\theta)}{\partial \theta} \cdot f(x|\theta) \, dx \\
&amp;= \int \frac{\partial f(x|\theta)}{\partial \theta} \, dx\end{split}\]</div>
<p>Now, assuming we can interchange differentiation and integration (this is one of the regularity conditions):</p>
<div class="math notranslate nohighlight">
\[\int \frac{\partial f(x|\theta)}{\partial \theta} \, dx = \frac{\partial}{\partial \theta} \int f(x|\theta) \, dx = \frac{\partial}{\partial \theta} (1) = 0\]</div>
<p>Since the score for <span class="math notranslate nohighlight">\(n\)</span> iid observations is the sum of <span class="math notranslate nohighlight">\(n\)</span> individual scores, each with mean zero, we have <span class="math notranslate nohighlight">\(\mathbb{E}[U(\theta_0)] = 0\)</span>. ∎</p>
<p>This result has an intuitive interpretation: at the true parameter value, the log-likelihood is “locally flat” <em>on average</em>—positive and negative slopes cancel out when we average over all possible datasets.</p>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_2_fig02_score_function.png"><img alt="Score function geometry showing zero-crossing at MLE and score distribution" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_2_fig02_score_function.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 84 </span><span class="caption-text"><strong>Figure 3.2.2</strong>: The score function and its properties. (a) For Poisson data, the score <span class="math notranslate nohighlight">\(U(\lambda) = n(\bar{x}/\lambda - 1)\)</span> crosses zero at the MLE <span class="math notranslate nohighlight">\(\hat{\lambda} = \bar{x}\)</span>. When <span class="math notranslate nohighlight">\(U &gt; 0\)</span> (green region), the likelihood increases with <span class="math notranslate nohighlight">\(\lambda\)</span>; when <span class="math notranslate nohighlight">\(U &lt; 0\)</span> (red region), it decreases. (b) At the true parameter <span class="math notranslate nohighlight">\(\lambda_0\)</span>, the score has mean zero and variance equal to the Fisher information: <span class="math notranslate nohighlight">\(\text{Var}[U(\lambda_0)] = nI_1(\lambda_0)\)</span>.</span><a class="headerlink" href="#id2" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="fisher-information">
<h2>Fisher Information<a class="headerlink" href="#fisher-information" title="Link to this heading"></a></h2>
<p>While the score tells us the direction of steepest ascent on the likelihood surface, the <strong>Fisher information</strong> tells us about the surface’s curvature—how sharply peaked the likelihood is around its maximum.</p>
<section id="definition">
<h3>Definition<a class="headerlink" href="#definition" title="Link to this heading"></a></h3>
<p>The <strong>Fisher information</strong> for a scalar parameter <span class="math notranslate nohighlight">\(\theta\)</span> is defined as the variance of the score:</p>
<div class="math notranslate nohighlight" id="equation-fisher-info-def1">
<span class="eqno">(43)<a class="headerlink" href="#equation-fisher-info-def1" title="Link to this equation"></a></span>\[I(\theta) = \text{Var}_\theta\left[ U(\theta) \right] = \mathbb{E}_\theta\left[ U(\theta)^2 \right]\]</div>
<p>where the second equality follows from <span class="math notranslate nohighlight">\(\mathbb{E}[U(\theta)] = 0\)</span>.</p>
<div class="important admonition">
<p class="admonition-title">Notation Convention: Expected vs. Observed Information</p>
<p>We adopt the following notation throughout this chapter:</p>
<p><strong>Per-observation vs. total information:</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(I_1(\theta)\)</span> = Fisher information from a <strong>single observation</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(I_n(\theta) = n \cdot I_1(\theta)\)</span> = total Fisher information from <span class="math notranslate nohighlight">\(n\)</span> iid observations</p></li>
</ul>
<p><strong>Expected vs. observed information:</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(I(\theta) = -\mathbb{E}\left[\frac{\partial^2 \ell}{\partial \theta^2}\right]\)</span> = <strong>expected</strong> (Fisher) information</p></li>
<li><p><span class="math notranslate nohighlight">\(J(\theta) = -\frac{\partial^2 \ell}{\partial \theta^2}\bigg|_{\text{observed}}\)</span> = <strong>observed</strong> information (data-dependent)</p></li>
</ul>
<p>Under correct model specification, <span class="math notranslate nohighlight">\(\mathbb{E}[J(\theta_0)] = I(\theta_0)\)</span>. Under misspecification, these may differ, leading to the sandwich variance estimator (see <span class="xref std std-ref">below</span>).</p>
<p><strong>Convention in formulas:</strong> Unless subscripted, <span class="math notranslate nohighlight">\(I(\theta)\)</span> denotes <strong>per-observation expected information</strong> <span class="math notranslate nohighlight">\(I_1(\theta)\)</span>. The Wald statistic <span class="math notranslate nohighlight">\(W = n I_1(\hat{\theta})(\hat{\theta} - \theta_0)^2\)</span> uses per-observation information.</p>
</div>
<p>Under the same regularity conditions that gave us <span class="math notranslate nohighlight">\(\mathbb{E}[U] = 0\)</span>, we have an equivalent expression involving the second derivative:</p>
<div class="math notranslate nohighlight" id="equation-fisher-info-def2">
<span class="eqno">(44)<a class="headerlink" href="#equation-fisher-info-def2" title="Link to this equation"></a></span>\[I(\theta) = -\mathbb{E}_\theta\left[ \frac{\partial^2 \ell}{\partial \theta^2} \right]\]</div>
<p>This is the <strong>information equality</strong>: the variance of the score equals the negative expected Hessian.</p>
<p><strong>Proof of information equality</strong>: We differentiate the identity <span class="math notranslate nohighlight">\(\mathbb{E}_\theta[U(\theta)] = 0\)</span> with respect to <span class="math notranslate nohighlight">\(\theta\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \theta} \int \frac{\partial \log f(x|\theta)}{\partial \theta} f(x|\theta) \, dx = 0\]</div>
<p>Applying the product rule inside the integral:</p>
<div class="math notranslate nohighlight">
\[\int \frac{\partial^2 \log f}{\partial \theta^2} f \, dx + \int \frac{\partial \log f}{\partial \theta} \cdot \frac{\partial f}{\partial \theta} \, dx = 0\]</div>
<p>The first integral is <span class="math notranslate nohighlight">\(\mathbb{E}[\partial^2 \ell / \partial \theta^2]\)</span>. For the second integral, note that <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial \theta} = f \cdot \frac{\partial \log f}{\partial \theta}\)</span>, so:</p>
<div class="math notranslate nohighlight">
\[\int \frac{\partial \log f}{\partial \theta} \cdot f \cdot \frac{\partial \log f}{\partial \theta} \, dx = \mathbb{E}\left[\left(\frac{\partial \log f}{\partial \theta}\right)^2\right] = \mathbb{E}[U^2]\]</div>
<p>Therefore: <span class="math notranslate nohighlight">\(\mathbb{E}[\partial^2 \ell / \partial \theta^2] + \mathbb{E}[U^2] = 0\)</span>, giving <span class="math notranslate nohighlight">\(I(\theta) = \mathbb{E}[U^2] = -\mathbb{E}[\partial^2 \ell / \partial \theta^2]\)</span>. ∎</p>
</section>
<section id="additivity-for-iid-samples">
<h3>Additivity for IID Samples<a class="headerlink" href="#additivity-for-iid-samples" title="Link to this heading"></a></h3>
<p>For <span class="math notranslate nohighlight">\(n\)</span> iid observations, the Fisher information is additive:</p>
<div class="math notranslate nohighlight" id="equation-fisher-additive">
<span class="eqno">(45)<a class="headerlink" href="#equation-fisher-additive" title="Link to this equation"></a></span>\[I_n(\theta) = n \cdot I_1(\theta)\]</div>
<p>where <span class="math notranslate nohighlight">\(I_1(\theta)\)</span> is the information from a single observation. This follows because the score is a sum of independent terms, and variance is additive for independent random variables.</p>
<p>This additivity has profound implications: <em>more data means more information</em>. Specifically, information grows linearly with sample size, which (as we will see) implies that standard errors shrink as <span class="math notranslate nohighlight">\(1/\sqrt{n}\)</span>.</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_2_fig03_fisher_information.png"><img alt="Fisher information as curvature of log-likelihood" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_2_fig03_fisher_information.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 85 </span><span class="caption-text"><strong>Figure 3.2.3</strong>: Fisher information as log-likelihood curvature. (a) High information (<span class="math notranslate nohighlight">\(I = 4.0\)</span>): a sharply peaked log-likelihood means small changes in <span class="math notranslate nohighlight">\(\theta\)</span> produce large changes in <span class="math notranslate nohighlight">\(\ell\)</span>—the data strongly constrain the parameter. (b) Low information (<span class="math notranslate nohighlight">\(I = 0.44\)</span>): a broad peak indicates the data are consistent with many parameter values. (c) For the Bernoulli distribution, <span class="math notranslate nohighlight">\(I(p) = 1/[p(1-p)]\)</span> is minimized at <span class="math notranslate nohighlight">\(p = 0.5\)</span>, where the outcome is most uncertain and each observation is most informative about <span class="math notranslate nohighlight">\(p\)</span>.</span><a class="headerlink" href="#id3" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="multivariate-fisher-information">
<h3>Multivariate Fisher Information<a class="headerlink" href="#multivariate-fisher-information" title="Link to this heading"></a></h3>
<p>For a parameter vector <span class="math notranslate nohighlight">\(\boldsymbol{\theta} \in \mathbb{R}^p\)</span>, the Fisher information becomes a <span class="math notranslate nohighlight">\(p \times p\)</span> matrix:</p>
<div class="math notranslate nohighlight" id="equation-fisher-matrix">
<span class="eqno">(46)<a class="headerlink" href="#equation-fisher-matrix" title="Link to this equation"></a></span>\[\mathbf{I}(\boldsymbol{\theta})_{jk} = \mathbb{E}\left[ \frac{\partial \ell}{\partial \theta_j} \cdot \frac{\partial \ell}{\partial \theta_k} \right] = -\mathbb{E}\left[ \frac{\partial^2 \ell}{\partial \theta_j \partial \theta_k} \right]\]</div>
<p>The Fisher information matrix is positive semi-definite (as a covariance matrix must be), and under regularity conditions, positive definite.</p>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_2_fig12_information_matrix.png"><img alt="Fisher information matrix and parameter orthogonality" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_2_fig12_information_matrix.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 86 </span><span class="caption-text"><strong>Figure 3.2.12</strong>: Multivariate Fisher information for the Normal distribution. (a) Joint sampling distribution of <span class="math notranslate nohighlight">\((\hat{\mu}, \hat{\sigma}^2)\)</span> from 1000 simulations (<span class="math notranslate nohighlight">\(n=50\)</span>). The elliptical 95% contour reflects the diagonal Fisher information matrix—the near-zero correlation (−0.04) confirms that <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> and <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> are asymptotically independent. (b) Per-observation information components: <span class="math notranslate nohighlight">\(I_{\mu\mu} = 1/\sigma^2\)</span> for the mean and <span class="math notranslate nohighlight">\(I_{\sigma^2\sigma^2} = 1/(2\sigma^4)\)</span> for the variance. The off-diagonal <span class="math notranslate nohighlight">\(I_{\mu,\sigma^2} = 0\)</span> demonstrates <strong>parameter orthogonality</strong>—inference about one parameter does not depend on the other.</span><a class="headerlink" href="#id4" title="Link to this image"></a></p>
</figcaption>
</figure>
<div class="note admonition">
<p class="admonition-title">Example 💡 Fisher Information for Exponential Distribution</p>
<p><strong>Setup</strong>: Let <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} \text{Exponential}(\lambda)\)</span> where <span class="math notranslate nohighlight">\(\lambda\)</span> is the rate parameter. The density is <span class="math notranslate nohighlight">\(f(x|\lambda) = \lambda e^{-\lambda x}\)</span> for <span class="math notranslate nohighlight">\(x &gt; 0\)</span>.</p>
<p><strong>Log-likelihood</strong>: <span class="math notranslate nohighlight">\(\ell(\lambda) = n \log \lambda - \lambda \sum_{i=1}^n x_i\)</span></p>
<p><strong>Score</strong>: <span class="math notranslate nohighlight">\(U(\lambda) = \frac{n}{\lambda} - \sum_{i=1}^n x_i = \frac{n}{\lambda} - n\bar{x}\)</span></p>
<p><strong>Second derivative</strong>: <span class="math notranslate nohighlight">\(\frac{\partial^2 \ell}{\partial \lambda^2} = -\frac{n}{\lambda^2}\)</span></p>
<p><strong>Fisher information</strong>: <span class="math notranslate nohighlight">\(I_n(\lambda) = -\mathbb{E}\left[-\frac{n}{\lambda^2}\right] = \frac{n}{\lambda^2}\)</span></p>
<p>The per-observation information is <span class="math notranslate nohighlight">\(I_1(\lambda) = 1/\lambda^2\)</span>. <strong>Lower rates (longer mean lifetimes) provide more information per observation.</strong> This may seem counterintuitive: shouldn’t more events (higher rate) tell us more? The resolution lies in understanding what “information about <span class="math notranslate nohighlight">\(\lambda\)</span>” means.</p>
<p>When <span class="math notranslate nohighlight">\(\lambda\)</span> is small, lifetimes are long and vary considerably—the data spread out, allowing us to distinguish between nearby <span class="math notranslate nohighlight">\(\lambda\)</span> values. When <span class="math notranslate nohighlight">\(\lambda\)</span> is large, lifetimes are short and tightly concentrated near zero—the data provide less resolution for distinguishing <span class="math notranslate nohighlight">\(\lambda\)</span> from <span class="math notranslate nohighlight">\(\lambda + \epsilon\)</span>.</p>
<p><strong>Reparameterization perspective</strong>: In terms of the mean lifetime <span class="math notranslate nohighlight">\(\theta = 1/\lambda\)</span>, the information is <span class="math notranslate nohighlight">\(I_1(\theta) = 1/\theta^2\)</span>. The <em>coefficient of variation</em> of the MLE is <span class="math notranslate nohighlight">\(\text{CV}(\hat{\theta}) = \text{SE}(\hat{\theta})/\theta = 1/\sqrt{n}\)</span>, which is constant regardless of <span class="math notranslate nohighlight">\(\theta\)</span>. The absolute precision scales with the parameter, but relative precision is parameter-independent.</p>
</div>
</section>
</section>
<section id="closed-form-maximum-likelihood-estimators">
<h2>Closed-Form Maximum Likelihood Estimators<a class="headerlink" href="#closed-form-maximum-likelihood-estimators" title="Link to this heading"></a></h2>
<p>For many common distributions, setting the score equal to zero yields explicit formulas for the MLE.</p>
<section id="normal-distribution">
<h3>Normal Distribution<a class="headerlink" href="#normal-distribution" title="Link to this heading"></a></h3>
<p>For <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} \mathcal{N}(\mu, \sigma^2)\)</span>:</p>
<p><strong>Score equations</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial \ell}{\partial \mu} &amp;= \frac{1}{\sigma^2} \sum_{i=1}^n (x_i - \mu) = 0 \\
\frac{\partial \ell}{\partial \sigma^2} &amp;= -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i=1}^n (x_i - \mu)^2 = 0\end{split}\]</div>
<p><strong>Solutions</strong>:</p>
<div class="math notranslate nohighlight" id="equation-normal-mle">
<span class="eqno">(47)<a class="headerlink" href="#equation-normal-mle" title="Link to this equation"></a></span>\[\hat{\mu} = \bar{x} = \frac{1}{n}\sum_{i=1}^n x_i, \qquad \hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2\]</div>
<p>The MLE for <span class="math notranslate nohighlight">\(\mu\)</span> is the sample mean—unbiased and efficient. The MLE for <span class="math notranslate nohighlight">\(\sigma^2\)</span> is the <em>biased</em> sample variance (dividing by <span class="math notranslate nohighlight">\(n\)</span> rather than <span class="math notranslate nohighlight">\(n-1\)</span>). This illustrates an important point: MLEs are not always unbiased, though their bias typically vanishes as <span class="math notranslate nohighlight">\(n \to \infty\)</span>.</p>
</section>
<section id="exponential-distribution">
<h3>Exponential Distribution<a class="headerlink" href="#exponential-distribution" title="Link to this heading"></a></h3>
<p>For <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} \text{Exponential}(\lambda)\)</span> (rate parameterization):</p>
<p>Setting <span class="math notranslate nohighlight">\(U(\lambda) = n/\lambda - n\bar{x} = 0\)</span> gives:</p>
<div class="math notranslate nohighlight" id="equation-exp-mle">
<span class="eqno">(48)<a class="headerlink" href="#equation-exp-mle" title="Link to this equation"></a></span>\[\hat{\lambda} = \frac{1}{\bar{x}}\]</div>
<p>The MLE is the reciprocal of the sample mean.</p>
</section>
<section id="poisson-distribution">
<h3>Poisson Distribution<a class="headerlink" href="#poisson-distribution" title="Link to this heading"></a></h3>
<p>For <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} \text{Poisson}(\lambda)\)</span>:</p>
<p><strong>Log-likelihood</strong>: <span class="math notranslate nohighlight">\(\ell(\lambda) = \sum_{i=1}^n (x_i \log \lambda - \lambda - \log(x_i!))\)</span></p>
<p><strong>Score</strong>: <span class="math notranslate nohighlight">\(U(\lambda) = \frac{\sum_{i=1}^n x_i}{\lambda} - n = \frac{n\bar{x}}{\lambda} - n\)</span></p>
<p>Setting <span class="math notranslate nohighlight">\(U(\lambda) = 0\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-poisson-mle">
<span class="eqno">(49)<a class="headerlink" href="#equation-poisson-mle" title="Link to this equation"></a></span>\[\hat{\lambda} = \bar{x}\]</div>
<p>The MLE equals the sample mean—exactly what we would expect given that <span class="math notranslate nohighlight">\(\mathbb{E}[X] = \lambda\)</span> for the Poisson.</p>
</section>
<section id="bernoulli-and-binomial">
<h3>Bernoulli and Binomial<a class="headerlink" href="#bernoulli-and-binomial" title="Link to this heading"></a></h3>
<p>For <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} \text{Bernoulli}(p)\)</span>:</p>
<p><strong>Log-likelihood</strong>: <span class="math notranslate nohighlight">\(\ell(p) = \sum_{i=1}^n [x_i \log p + (1-x_i) \log(1-p)]\)</span></p>
<p><strong>Score</strong>: <span class="math notranslate nohighlight">\(U(p) = \frac{\sum x_i}{p} - \frac{n - \sum x_i}{1-p}\)</span></p>
<p>Setting <span class="math notranslate nohighlight">\(U(p) = 0\)</span> and solving:</p>
<div class="math notranslate nohighlight" id="equation-bernoulli-mle">
<span class="eqno">(50)<a class="headerlink" href="#equation-bernoulli-mle" title="Link to this equation"></a></span>\[\hat{p} = \bar{x} = \frac{\sum_{i=1}^n x_i}{n}\]</div>
<p>The MLE is the sample proportion.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">mle_normal</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute MLEs for Normal(μ, σ²) distribution.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : array-like</span>
<span class="sd">        Observed data.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        MLEs and standard errors.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Point estimates</span>
    <span class="n">mu_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">sigma2_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># MLE uses divisor n, not n-1</span>

    <span class="c1"># Fisher information (evaluated at MLE)</span>
    <span class="c1"># I(μ) = n/σ², I(σ²) = n/(2σ⁴)</span>
    <span class="n">se_mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma2_hat</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">se_sigma2</span> <span class="o">=</span> <span class="n">sigma2_hat</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;mu_hat&#39;</span><span class="p">:</span> <span class="n">mu_hat</span><span class="p">,</span>
        <span class="s1">&#39;sigma2_hat&#39;</span><span class="p">:</span> <span class="n">sigma2_hat</span><span class="p">,</span>
        <span class="s1">&#39;se_mu&#39;</span><span class="p">:</span> <span class="n">se_mu</span><span class="p">,</span>
        <span class="s1">&#39;se_sigma2&#39;</span><span class="p">:</span> <span class="n">se_sigma2</span>
    <span class="p">}</span>

<span class="k">def</span><span class="w"> </span><span class="nf">mle_exponential</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute MLE for Exponential(λ) distribution (rate parameterization).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : array-like</span>
<span class="sd">        Observed data (must be positive).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        MLE and standard error.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">lambda_hat</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Fisher information: I(λ) = n/λ²</span>
    <span class="c1"># SE(λ̂) = λ/√n (evaluated at MLE)</span>
    <span class="n">se_lambda</span> <span class="o">=</span> <span class="n">lambda_hat</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;lambda_hat&#39;</span><span class="p">:</span> <span class="n">lambda_hat</span><span class="p">,</span>
        <span class="s1">&#39;se_lambda&#39;</span><span class="p">:</span> <span class="n">se_lambda</span>
    <span class="p">}</span>

<span class="c1"># Demonstration</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Normal data</span>
<span class="n">true_mu</span><span class="p">,</span> <span class="n">true_sigma</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">2.0</span>
<span class="n">x_normal</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">true_mu</span><span class="p">,</span> <span class="n">true_sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">result_normal</span> <span class="o">=</span> <span class="n">mle_normal</span><span class="p">(</span><span class="n">x_normal</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Normal MLE Results:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  μ̂ = </span><span class="si">{</span><span class="n">result_normal</span><span class="p">[</span><span class="s1">&#39;mu_hat&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (true: </span><span class="si">{</span><span class="n">true_mu</span><span class="si">}</span><span class="s2">), SE = </span><span class="si">{</span><span class="n">result_normal</span><span class="p">[</span><span class="s1">&#39;se_mu&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  σ̂² = </span><span class="si">{</span><span class="n">result_normal</span><span class="p">[</span><span class="s1">&#39;sigma2_hat&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (true: </span><span class="si">{</span><span class="n">true_sigma</span><span class="o">**</span><span class="mi">2</span><span class="si">}</span><span class="s2">), SE = </span><span class="si">{</span><span class="n">result_normal</span><span class="p">[</span><span class="s1">&#39;se_sigma2&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Exponential data</span>
<span class="n">true_lambda</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">x_exp</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">true_lambda</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">result_exp</span> <span class="o">=</span> <span class="n">mle_exponential</span><span class="p">(</span><span class="n">x_exp</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Exponential MLE Results:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  λ̂ = </span><span class="si">{</span><span class="n">result_exp</span><span class="p">[</span><span class="s1">&#39;lambda_hat&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (true: </span><span class="si">{</span><span class="n">true_lambda</span><span class="si">}</span><span class="s2">), SE = </span><span class="si">{</span><span class="n">result_exp</span><span class="p">[</span><span class="s1">&#39;se_lambda&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Normal MLE Results:
  μ̂ = 4.8572 (true: 5.0), SE = 0.1918
  σ̂² = 3.6800 (true: 4.0), SE = 0.5205

Exponential MLE Results:
  λ̂ = 0.4834 (true: 0.5), SE = 0.0483
</pre></div>
</div>
</section>
<section id="exact-finite-sample-properties">
<h3>Exact Finite-Sample Properties<a class="headerlink" href="#exact-finite-sample-properties" title="Link to this heading"></a></h3>
<p>While our focus is on asymptotic theory, several exact results are available for common distributions and provide useful benchmarks.</p>
<p><strong>Normal distribution</strong>: For <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} \mathcal{N}(\mu, \sigma^2)\)</span>:</p>
<ul class="simple">
<li><p>The MLEs <span class="math notranslate nohighlight">\(\hat{\mu} = \bar{X}\)</span> and <span class="math notranslate nohighlight">\(\hat{\sigma}^2 = \frac{1}{n}\sum(X_i - \bar{X})^2\)</span> are independent</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{\mu} \sim \mathcal{N}(\mu, \sigma^2/n)\)</span> exactly</p></li>
<li><p><span class="math notranslate nohighlight">\(\frac{n\hat{\sigma}^2}{\sigma^2} \sim \chi^2_{n-1}\)</span> exactly</p></li>
</ul>
<p>The <span class="math notranslate nohighlight">\(\chi^2_{n-1}\)</span> result implies <span class="math notranslate nohighlight">\(\mathbb{E}[\hat{\sigma}^2] = \frac{n-1}{n} \sigma^2\)</span>—the MLE is biased. The unbiased estimator <span class="math notranslate nohighlight">\(S^2 = \frac{1}{n-1}\sum(X_i - \bar{X})^2\)</span> corrects this.</p>
<p><strong>Exponential distribution</strong>: For <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} \text{Exp}(\lambda)\)</span> with MLE <span class="math notranslate nohighlight">\(\hat{\lambda} = 1/\bar{X}\)</span>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\bar{X}\)</span> has a Gamma(<span class="math notranslate nohighlight">\(n, \lambda\)</span>) distribution</p></li>
<li><p>The exact bias is <span class="math notranslate nohighlight">\(\mathbb{E}[\hat{\lambda}] = \lambda \cdot \frac{n}{n-1}\)</span> for <span class="math notranslate nohighlight">\(n &gt; 1\)</span></p></li>
<li><p>The exact variance is <span class="math notranslate nohighlight">\(\text{Var}(\hat{\lambda}) = \lambda^2 \cdot \frac{n}{(n-1)^2(n-2)}\)</span> for <span class="math notranslate nohighlight">\(n &gt; 2\)</span></p></li>
</ul>
<p>The bias-corrected estimator <span class="math notranslate nohighlight">\(\tilde{\lambda} = \frac{n-1}{n} \hat{\lambda} = \frac{n-1}{n\bar{X}}\)</span> is unbiased.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">verify_exponential_exact_results</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Verify exact finite-sample results for exponential MLE.&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">true_lambda</span> <span class="o">=</span> <span class="mf">2.0</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">n_sim</span> <span class="o">=</span> <span class="mi">100000</span>

    <span class="n">mles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_sim</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">true_lambda</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
        <span class="n">mles</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Theoretical exact values</span>
    <span class="n">theory_mean</span> <span class="o">=</span> <span class="n">true_lambda</span> <span class="o">*</span> <span class="n">n</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">theory_var</span> <span class="o">=</span> <span class="n">true_lambda</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n</span> <span class="o">/</span> <span class="p">((</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">2</span><span class="p">))</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;EXACT FINITE-SAMPLE RESULTS: EXPONENTIAL MLE&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">, true λ = </span><span class="si">{</span><span class="n">true_lambda</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;Quantity&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Theory&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Empirical&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;E[λ̂]&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">theory_mean</span><span class="si">:</span><span class="s2">&gt;15.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mles</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;15.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Var(λ̂)&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">theory_var</span><span class="si">:</span><span class="s2">&gt;15.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">mles</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;15.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Bias&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">theory_mean</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">true_lambda</span><span class="si">:</span><span class="s2">&gt;15.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mles</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">true_lambda</span><span class="si">:</span><span class="s2">&gt;15.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">verify_exponential_exact_results</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>EXACT FINITE-SAMPLE RESULTS: EXPONENTIAL MLE
==================================================
n = 10, true λ = 2.0

Quantity                   Theory       Empirical
--------------------------------------------------
E[λ̂]                     2.222222        2.221456
Var(λ̂)                   0.617284        0.615890
Bias                      0.222222        0.221456
</pre></div>
</div>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_2_fig10_exact_vs_asymptotic.png"><img alt="Exact versus asymptotic properties of exponential MLE" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_2_fig10_exact_vs_asymptotic.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 87 </span><span class="caption-text"><strong>Figure 3.2.10</strong>: Exact versus asymptotic finite-sample properties of the exponential MLE. (a) The MLE is biased upward: <span class="math notranslate nohighlight">\(\mathbb{E}[\hat{\lambda}] = \lambda n/(n-1) &gt; \lambda\)</span>, with bias decreasing as <span class="math notranslate nohighlight">\(n\)</span> increases. (b) The exact variance <span class="math notranslate nohighlight">\(\lambda^2 n/[(n-1)^2(n-2)]\)</span> substantially exceeds the asymptotic approximation <span class="math notranslate nohighlight">\(\lambda^2/n\)</span> for small samples. The <span class="math notranslate nohighlight">\(n-1\)</span> and <span class="math notranslate nohighlight">\(n-2\)</span> factors explain why finite-sample corrections matter.</span><a class="headerlink" href="#id5" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="when-closed-forms-don-t-exist">
<h3>When Closed Forms Don’t Exist<a class="headerlink" href="#when-closed-forms-don-t-exist" title="Link to this heading"></a></h3>
<p>Not all MLEs have closed-form solutions. Two important examples:</p>
<p><strong>Gamma distribution</strong>: For <span class="math notranslate nohighlight">\(X \sim \text{Gamma}(\alpha, \beta)\)</span>, the score equation for <span class="math notranslate nohighlight">\(\alpha\)</span> involves the digamma function <span class="math notranslate nohighlight">\(\psi(\alpha) = \frac{d}{d\alpha} \log \Gamma(\alpha)\)</span>:</p>
<div class="math notranslate nohighlight">
\[n[\log \beta + \psi(\alpha)] = \sum_{i=1}^n \log x_i\]</div>
<p>This transcendental equation has no closed-form solution; numerical methods are required.</p>
<p><strong>Beta distribution</strong>: Similarly, the Beta(<span class="math notranslate nohighlight">\(\alpha, \beta\)</span>) MLEs require solving a system involving digamma functions.</p>
<p><strong>Mixture models</strong>: Even simple mixtures like <span class="math notranslate nohighlight">\(p \cdot \mathcal{N}(\mu_1, \sigma_1^2) + (1-p) \cdot \mathcal{N}(\mu_2, \sigma_2^2)\)</span> have likelihood surfaces that preclude closed-form solutions.</p>
<p>For these cases, we turn to numerical optimization.</p>
</section>
</section>
<section id="numerical-optimization-for-mle">
<h2>Numerical Optimization for MLE<a class="headerlink" href="#numerical-optimization-for-mle" title="Link to this heading"></a></h2>
<p>When closed-form solutions are unavailable, we compute MLEs numerically by optimizing the log-likelihood. Two classical algorithms dominate: Newton-Raphson and Fisher scoring.</p>
<section id="newton-raphson-method">
<h3>Newton-Raphson Method<a class="headerlink" href="#newton-raphson-method" title="Link to this heading"></a></h3>
<p>Newton-Raphson is a general-purpose optimization algorithm based on quadratic approximation of the objective function.</p>
<p>At iteration <span class="math notranslate nohighlight">\(t\)</span>, we approximate the log-likelihood near <span class="math notranslate nohighlight">\(\theta^{(t)}\)</span> by its second-order Taylor expansion:</p>
<div class="math notranslate nohighlight">
\[\ell(\theta) \approx \ell(\theta^{(t)}) + (\theta - \theta^{(t)}) U(\theta^{(t)}) + \frac{1}{2}(\theta - \theta^{(t)})^2 H(\theta^{(t)})\]</div>
<p>where <span class="math notranslate nohighlight">\(U(\theta) = \partial \ell / \partial \theta\)</span> is the score and <span class="math notranslate nohighlight">\(H(\theta) = \partial^2 \ell / \partial \theta^2\)</span> is the Hessian (second derivative).</p>
<p>Maximizing this quadratic approximation gives the update:</p>
<div class="math notranslate nohighlight" id="equation-newton-raphson">
<span class="eqno">(51)<a class="headerlink" href="#equation-newton-raphson" title="Link to this equation"></a></span>\[\theta^{(t+1)} = \theta^{(t)} - \frac{U(\theta^{(t)})}{H(\theta^{(t)})} = \theta^{(t)} - [H(\theta^{(t)})]^{-1} U(\theta^{(t)})\]</div>
<p>In the multivariate case with parameter vector <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\theta}^{(t+1)} = \boldsymbol{\theta}^{(t)} - [\mathbf{H}(\boldsymbol{\theta}^{(t)})]^{-1} \mathbf{U}(\boldsymbol{\theta}^{(t)})\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{H}\)</span> is the <span class="math notranslate nohighlight">\(p \times p\)</span> Hessian matrix.</p>
<p><strong>Properties of Newton-Raphson</strong>:</p>
<ul class="simple">
<li><p><strong>Quadratic convergence</strong>: Near the optimum, the error decreases quadratically—each iteration roughly doubles the number of correct digits.</p></li>
<li><p><strong>Local convergence</strong>: Convergence is only guaranteed if we start sufficiently close to the optimum.</p></li>
<li><p><strong>Potential instability</strong>: If the Hessian is not negative definite (the log-likelihood is not locally concave), the algorithm may diverge or converge to a saddle point.</p></li>
</ul>
<figure class="align-center" id="id6">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_2_fig05_newton_raphson.png"><img alt="Newton-Raphson convergence for Gamma MLE" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_2_fig05_newton_raphson.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 88 </span><span class="caption-text"><strong>Figure 3.2.5</strong>: Newton-Raphson optimization for the Gamma distribution. (a) The profile log-likelihood <span class="math notranslate nohighlight">\(\ell_p(\alpha)\)</span> with Newton-Raphson iterates shown as points. Starting from a method-of-moments initial value near <span class="math notranslate nohighlight">\(\alpha = 2.5\)</span>, the algorithm converges to the MLE <span class="math notranslate nohighlight">\(\hat{\alpha} = 4.02\)</span> in just 5 iterations. (b) Quadratic convergence: the error <span class="math notranslate nohighlight">\(|\alpha^{(t)} - \hat{\alpha}|\)</span> decreases super-exponentially, roughly doubling the number of correct digits per iteration.</span><a class="headerlink" href="#id6" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="fisher-scoring">
<h3>Fisher Scoring<a class="headerlink" href="#fisher-scoring" title="Link to this heading"></a></h3>
<p>Fisher scoring replaces the observed Hessian <span class="math notranslate nohighlight">\(H(\theta)\)</span> with its expected value, the negative Fisher information:</p>
<div class="math notranslate nohighlight" id="equation-fisher-scoring">
<span class="eqno">(52)<a class="headerlink" href="#equation-fisher-scoring" title="Link to this equation"></a></span>\[\theta^{(t+1)} = \theta^{(t)} + [I(\theta^{(t)})]^{-1} U(\theta^{(t)})\]</div>
<p><strong>Why Fisher scoring?</strong></p>
<ol class="arabic simple">
<li><p><strong>Guaranteed stability</strong>: The Fisher information matrix is always positive definite (under regularity conditions), ensuring we always move in an ascent direction.</p></li>
<li><p><strong>Simpler computation</strong>: For some models, the expected information has a simpler form than the observed Hessian.</p></li>
<li><p><strong>Statistical interpretation</strong>: Fisher scoring is equivalent to iteratively reweighted least squares (IRLS) for generalized linear models.</p></li>
</ol>
<p><strong>For exponential families with canonical links, Newton-Raphson and Fisher scoring are identical</strong>: the observed and expected information coincide.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">special</span>

<span class="k">def</span><span class="w"> </span><span class="nf">mle_gamma_fisher_scoring</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute MLE for Gamma(α, β) using Fisher scoring.</span>

<span class="sd">    Uses shape-rate parameterization where E[X] = α/β, Var(X) = α/β².</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : array-like</span>
<span class="sd">        Observed data (must be positive).</span>
<span class="sd">    max_iter : int</span>
<span class="sd">        Maximum iterations.</span>
<span class="sd">    tol : float</span>
<span class="sd">        Convergence tolerance.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        MLEs, standard errors, and convergence info.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x_bar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">log_x_bar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="c1"># Method of moments initialization</span>
    <span class="n">s2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">x_bar</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">s2</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">x_bar</span> <span class="o">/</span> <span class="n">s2</span>

    <span class="n">history</span> <span class="o">=</span> <span class="p">[(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)]</span>

    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
        <span class="c1"># Score functions</span>
        <span class="c1"># ∂ℓ/∂α = n[log(β) - ψ(α)] + Σlog(xᵢ)</span>
        <span class="c1"># ∂ℓ/∂β = nα/β - Σxᵢ</span>
        <span class="n">psi_alpha</span> <span class="o">=</span> <span class="n">special</span><span class="o">.</span><span class="n">digamma</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
        <span class="n">score_alpha</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span> <span class="o">-</span> <span class="n">psi_alpha</span><span class="p">)</span> <span class="o">+</span> <span class="n">n</span> <span class="o">*</span> <span class="n">log_x_bar</span>
        <span class="n">score_beta</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">/</span> <span class="n">beta</span> <span class="o">-</span> <span class="n">n</span> <span class="o">*</span> <span class="n">x_bar</span>

        <span class="c1"># Fisher information matrix</span>
        <span class="c1"># I_αα = n·ψ&#39;(α), I_ββ = nα/β², I_αβ = -n/β</span>
        <span class="n">psi1_alpha</span> <span class="o">=</span> <span class="n">special</span><span class="o">.</span><span class="n">polygamma</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>  <span class="c1"># trigamma</span>
        <span class="n">I_aa</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="n">psi1_alpha</span>
        <span class="n">I_bb</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">/</span> <span class="n">beta</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">I_ab</span> <span class="o">=</span> <span class="o">-</span><span class="n">n</span> <span class="o">/</span> <span class="n">beta</span>

        <span class="c1"># Invert 2x2 Fisher information</span>
        <span class="n">det</span> <span class="o">=</span> <span class="n">I_aa</span> <span class="o">*</span> <span class="n">I_bb</span> <span class="o">-</span> <span class="n">I_ab</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">I_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">I_bb</span><span class="p">,</span> <span class="o">-</span><span class="n">I_ab</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="n">I_ab</span><span class="p">,</span> <span class="n">I_aa</span><span class="p">]])</span> <span class="o">/</span> <span class="n">det</span>

        <span class="c1"># Fisher scoring update</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">score_alpha</span><span class="p">,</span> <span class="n">score_beta</span><span class="p">])</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">I_inv</span> <span class="o">@</span> <span class="n">score</span>

        <span class="n">alpha_new</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">delta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">beta_new</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">+</span> <span class="n">delta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Ensure parameters stay positive</span>
        <span class="n">alpha_new</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">alpha_new</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">)</span>
        <span class="n">beta_new</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">beta_new</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">)</span>

        <span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">alpha_new</span><span class="p">,</span> <span class="n">beta_new</span><span class="p">))</span>

        <span class="c1"># Check convergence</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">delta</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
            <span class="k">break</span>

        <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="n">alpha_new</span><span class="p">,</span> <span class="n">beta_new</span>

    <span class="c1"># Standard errors from inverse Fisher information at MLE</span>
    <span class="n">psi1_alpha</span> <span class="o">=</span> <span class="n">special</span><span class="o">.</span><span class="n">polygamma</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
    <span class="n">I_aa</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="n">psi1_alpha</span>
    <span class="n">I_bb</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">/</span> <span class="n">beta</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">I_ab</span> <span class="o">=</span> <span class="o">-</span><span class="n">n</span> <span class="o">/</span> <span class="n">beta</span>
    <span class="n">det</span> <span class="o">=</span> <span class="n">I_aa</span> <span class="o">*</span> <span class="n">I_bb</span> <span class="o">-</span> <span class="n">I_ab</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">I_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">I_bb</span><span class="p">,</span> <span class="o">-</span><span class="n">I_ab</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="n">I_ab</span><span class="p">,</span> <span class="n">I_aa</span><span class="p">]])</span> <span class="o">/</span> <span class="n">det</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;alpha_hat&#39;</span><span class="p">:</span> <span class="n">alpha</span><span class="p">,</span>
        <span class="s1">&#39;beta_hat&#39;</span><span class="p">:</span> <span class="n">beta</span><span class="p">,</span>
        <span class="s1">&#39;se_alpha&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">I_inv</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span>
        <span class="s1">&#39;se_beta&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">I_inv</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
        <span class="s1">&#39;iterations&#39;</span><span class="p">:</span> <span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s1">&#39;converged&#39;</span><span class="p">:</span> <span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&lt;</span> <span class="n">max_iter</span><span class="p">,</span>
        <span class="s1">&#39;history&#39;</span><span class="p">:</span> <span class="n">history</span>
    <span class="p">}</span>

<span class="c1"># Demonstration</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">true_alpha</span><span class="p">,</span> <span class="n">true_beta</span> <span class="o">=</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">2.0</span>
<span class="n">x_gamma</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">true_alpha</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">true_beta</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">mle_gamma_fisher_scoring</span><span class="p">(</span><span class="n">x_gamma</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Gamma MLE via Fisher Scoring:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  α̂ = </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;alpha_hat&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (true: </span><span class="si">{</span><span class="n">true_alpha</span><span class="si">}</span><span class="s2">), SE = </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;se_alpha&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  β̂ = </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;beta_hat&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (true: </span><span class="si">{</span><span class="n">true_beta</span><span class="si">}</span><span class="s2">), SE = </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;se_beta&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Converged in </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;iterations&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> iterations&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Gamma MLE via Fisher Scoring:
  α̂ = 3.1538 (true: 3.0), SE = 0.3209
  β̂ = 2.0893 (true: 2.0), SE = 0.2257
  Converged in 6 iterations
</pre></div>
</div>
<div class="tip admonition">
<p class="admonition-title">Connection to Generalized Linear Models</p>
<p>Fisher scoring becomes especially elegant for exponential family models with canonical links. In <span class="xref std std-ref">Section 3.7</span>, we show that:</p>
<ol class="arabic">
<li><p>For an exponential family response with canonical link, the <strong>observed and expected information are equal</strong></p></li>
<li><p>Therefore, Newton-Raphson and Fisher scoring produce <strong>identical iterations</strong></p></li>
<li><p>Fisher scoring reduces to <strong>Iteratively Reweighted Least Squares (IRLS)</strong>, solving at each step:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\beta}^{(t+1)} = (\mathbf{X}^\top \mathbf{W}^{(t)} \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{W}^{(t)} \mathbf{z}^{(t)}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> is a diagonal weight matrix and <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> is a “working response”</p>
</li>
</ol>
<p>This connection—from general MLE theory through Fisher scoring to IRLS for GLMs—illustrates how foundational concepts build toward practical algorithms.</p>
</div>
</section>
<section id="practical-implementation-with-scipy">
<h3>Practical Implementation with SciPy<a class="headerlink" href="#practical-implementation-with-scipy" title="Link to this heading"></a></h3>
<p>For production code, <code class="docutils literal notranslate"><span class="pre">scipy.optimize</span></code> provides robust, well-tested optimization routines:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">optimize</span><span class="p">,</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">mle_gamma_scipy</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute Gamma MLE using scipy.optimize.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : array-like</span>
<span class="sd">        Observed data.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        MLEs and optimization results.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">neg_log_likelihood</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
        <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="n">params</span>
        <span class="k">if</span> <span class="n">alpha</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">beta</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
        <span class="c1"># Gamma log-likelihood (rate parameterization)</span>
        <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span> <span class="o">-</span> <span class="n">n</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">special</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">alpha</span><span class="p">))</span>
                 <span class="o">+</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">-</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="c1"># Method of moments starting values</span>
    <span class="n">x_bar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">s2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">alpha0</span> <span class="o">=</span> <span class="n">x_bar</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">s2</span>
    <span class="n">beta0</span> <span class="o">=</span> <span class="n">x_bar</span> <span class="o">/</span> <span class="n">s2</span>

    <span class="c1"># L-BFGS-B with bounds to keep parameters positive</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">optimize</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span>
        <span class="n">neg_log_likelihood</span><span class="p">,</span>
        <span class="n">x0</span><span class="o">=</span><span class="p">[</span><span class="n">alpha0</span><span class="p">,</span> <span class="n">beta0</span><span class="p">],</span>
        <span class="n">method</span><span class="o">=</span><span class="s1">&#39;L-BFGS-B&#39;</span><span class="p">,</span>
        <span class="n">bounds</span><span class="o">=</span><span class="p">[(</span><span class="mf">1e-10</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="p">(</span><span class="mf">1e-10</span><span class="p">,</span> <span class="kc">None</span><span class="p">)]</span>
    <span class="p">)</span>

    <span class="c1"># Standard errors via numerical Hessian</span>
    <span class="n">hess_inv</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">hess_inv</span><span class="o">.</span><span class="n">todense</span><span class="p">()</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">hess_inv</span><span class="p">,</span> <span class="s1">&#39;todense&#39;</span><span class="p">)</span> <span class="k">else</span> <span class="n">result</span><span class="o">.</span><span class="n">hess_inv</span>
    <span class="n">se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">hess_inv</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;alpha_hat&#39;</span><span class="p">:</span> <span class="n">result</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="s1">&#39;beta_hat&#39;</span><span class="p">:</span> <span class="n">result</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="s1">&#39;se_alpha&#39;</span><span class="p">:</span> <span class="n">se</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">se</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span>
        <span class="s1">&#39;se_beta&#39;</span><span class="p">:</span> <span class="n">se</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">se</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span>
        <span class="s1">&#39;success&#39;</span><span class="p">:</span> <span class="n">result</span><span class="o">.</span><span class="n">success</span><span class="p">,</span>
        <span class="s1">&#39;message&#39;</span><span class="p">:</span> <span class="n">result</span><span class="o">.</span><span class="n">message</span>
    <span class="p">}</span>

<span class="c1"># Compare with scipy.stats.gamma.fit</span>
<span class="c1"># Note: scipy uses shape, loc, scale parameterization</span>
<span class="n">fitted</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">gamma</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_gamma</span><span class="p">,</span> <span class="n">floc</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Fix location at 0</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">scipy.stats.gamma.fit: shape=</span><span class="si">{</span><span class="n">fitted</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, scale=</span><span class="si">{</span><span class="n">fitted</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Implied β = 1/scale = </span><span class="si">{</span><span class="mi">1</span><span class="o">/</span><span class="n">fitted</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>scipy.stats.gamma.fit: shape=3.1538, scale=0.4786
  Implied β = 1/scale = 2.0893
</pre></div>
</div>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ⚠️ Parameterization Consistency</p>
<p>Different software packages use different parameterizations for the same distribution:</p>
<ul class="simple">
<li><p><strong>Gamma</strong>: SciPy uses shape-scale; some texts use shape-rate (β = 1/scale)</p></li>
<li><p><strong>Exponential</strong>: SciPy uses scale (mean); some texts use rate (1/mean)</p></li>
<li><p><strong>Normal</strong>: The second parameter is always variance in this text; some use standard deviation</p></li>
</ul>
<p>Always verify parameterization before comparing results across packages or implementing formulas from different sources.</p>
</div>
</section>
<section id="practical-safeguards-for-numerical-mle">
<h3>Practical Safeguards for Numerical MLE<a class="headerlink" href="#practical-safeguards-for-numerical-mle" title="Link to this heading"></a></h3>
<p>Real-world MLE optimization requires more than the basic Newton-Raphson update. Several safeguards improve robustness and reliability.</p>
<p><strong>1. Line search and step control</strong></p>
<p>The pure Newton step <span class="math notranslate nohighlight">\(\theta^{(t+1)} = \theta^{(t)} - H^{-1} U\)</span> may overshoot, especially far from the optimum. <strong>Line search</strong> modifies this to:</p>
<div class="math notranslate nohighlight">
\[\theta^{(t+1)} = \theta^{(t)} + \alpha_t \cdot d_t\]</div>
<p>where <span class="math notranslate nohighlight">\(d_t = -H^{-1} U\)</span> is the Newton direction and <span class="math notranslate nohighlight">\(\alpha_t \in (0, 1]\)</span> is chosen to ensure sufficient increase in <span class="math notranslate nohighlight">\(\ell(\theta)\)</span>. The <strong>Armijo condition</strong> requires:</p>
<div class="math notranslate nohighlight">
\[\ell(\theta^{(t)} + \alpha d_t) \geq \ell(\theta^{(t)}) + c \cdot \alpha \cdot U^\top d_t\]</div>
<p>for some <span class="math notranslate nohighlight">\(c \in (0, 1)\)</span> (typically <span class="math notranslate nohighlight">\(c = 10^{-4}\)</span>). Start with <span class="math notranslate nohighlight">\(\alpha = 1\)</span> and backtrack by halving until the condition holds.</p>
<p><strong>2. Trust region methods</strong></p>
<p>Instead of line search along a direction, <strong>trust region methods</strong> constrain the step size directly:</p>
<div class="math notranslate nohighlight">
\[\theta^{(t+1)} = \arg\max_\theta \left\{ \ell(\theta^{(t)}) + (\theta - \theta^{(t)})^\top U + \frac{1}{2}(\theta - \theta^{(t)})^\top H (\theta - \theta^{(t)}) \right\}\]</div>
<p>subject to <span class="math notranslate nohighlight">\(\|\theta - \theta^{(t)}\| \leq \Delta_t\)</span>. The trust region radius <span class="math notranslate nohighlight">\(\Delta_t\)</span> adapts based on how well the quadratic approximation predicts actual improvement.</p>
<p><strong>3. Parameter transformations for constraints</strong></p>
<p>Many parameters have natural constraints. Transform to an unconstrained scale:</p>
<table class="docutils align-default" id="id7">
<caption><span class="caption-number">Table 27 </span><span class="caption-text">Common Parameter Transformations</span><a class="headerlink" href="#id7" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 30.0%" />
<col style="width: 30.0%" />
<col style="width: 40.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Constraint</p></th>
<th class="head"><p>Transformation</p></th>
<th class="head"><p>Original parameter</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\sigma^2 &gt; 0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\eta = \log(\sigma^2)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\sigma^2 = e^\eta\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(0 &lt; p &lt; 1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\eta = \log(p/(1-p))\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(p = 1/(1 + e^{-\eta})\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\eta = \log(\lambda)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\lambda = e^\eta\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\rho \in (-1, 1)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\eta = \text{arctanh}(\rho)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\rho = \tanh(\eta)\)</span></p></td>
</tr>
</tbody>
</table>
<p>Optimize in <span class="math notranslate nohighlight">\(\eta\)</span>, then transform back. The Jacobian of the transformation enters the standard error calculation.</p>
<p><strong>4. Scaling and conditioning</strong></p>
<p>Poor numerical conditioning causes optimization difficulties:</p>
<ul class="simple">
<li><p><strong>Center predictors</strong>: In regression, use <span class="math notranslate nohighlight">\(\tilde{x}_j = x_j - \bar{x}_j\)</span> to reduce correlation between intercept and slopes</p></li>
<li><p><strong>Scale parameters</strong>: If parameters differ by orders of magnitude, rescale so they’re comparable</p></li>
<li><p><strong>Check condition number</strong>: If <span class="math notranslate nohighlight">\(\kappa(H) = \lambda_{\max} / \lambda_{\min} &gt; 10^6\)</span>, the problem is ill-conditioned</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy.optimize</span><span class="w"> </span><span class="kn">import</span> <span class="n">minimize</span>

<span class="k">def</span><span class="w"> </span><span class="nf">mle_with_safeguards</span><span class="p">(</span><span class="n">neg_log_lik</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    MLE with practical safeguards.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    neg_log_lik : callable</span>
<span class="sd">        Negative log-likelihood function.</span>
<span class="sd">    x0 : ndarray</span>
<span class="sd">        Initial parameter values.</span>
<span class="sd">    bounds : list of tuples, optional</span>
<span class="sd">        Parameter bounds for L-BFGS-B.</span>
<span class="sd">    transform : dict, optional</span>
<span class="sd">        Parameter transformations {&#39;to_unconstrained&#39;: func, &#39;from_unconstrained&#39;: func}.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    result : OptimizeResult</span>
<span class="sd">        Optimization result with MLE and diagnostics.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Apply transformation if provided</span>
    <span class="k">if</span> <span class="n">transform</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x0_transformed</span> <span class="o">=</span> <span class="n">transform</span><span class="p">[</span><span class="s1">&#39;to_unconstrained&#39;</span><span class="p">](</span><span class="n">x0</span><span class="p">)</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">neg_ll_transformed</span><span class="p">(</span><span class="n">eta</span><span class="p">):</span>
            <span class="n">theta</span> <span class="o">=</span> <span class="n">transform</span><span class="p">[</span><span class="s1">&#39;from_unconstrained&#39;</span><span class="p">](</span><span class="n">eta</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">neg_log_lik</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

        <span class="n">result</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">neg_ll_transformed</span><span class="p">,</span> <span class="n">x0_transformed</span><span class="p">,</span>
                          <span class="n">method</span><span class="o">=</span><span class="s1">&#39;BFGS&#39;</span><span class="p">,</span>  <span class="c1"># No bounds needed after transform</span>
                          <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;gtol&#39;</span><span class="p">:</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="s1">&#39;maxiter&#39;</span><span class="p">:</span> <span class="mi">1000</span><span class="p">})</span>

        <span class="n">result</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">transform</span><span class="p">[</span><span class="s1">&#39;from_unconstrained&#39;</span><span class="p">](</span><span class="n">result</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Use L-BFGS-B with bounds if provided</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">neg_log_lik</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span>
                          <span class="n">method</span><span class="o">=</span><span class="s1">&#39;L-BFGS-B&#39;</span> <span class="k">if</span> <span class="n">bounds</span> <span class="k">else</span> <span class="s1">&#39;BFGS&#39;</span><span class="p">,</span>
                          <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span>
                          <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;gtol&#39;</span><span class="p">:</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="s1">&#39;maxiter&#39;</span><span class="p">:</span> <span class="mi">1000</span><span class="p">})</span>

    <span class="k">return</span> <span class="n">result</span>
</pre></div>
</div>
</section>
</section>
<section id="asymptotic-properties-of-mles">
<h2>Asymptotic Properties of MLEs<a class="headerlink" href="#asymptotic-properties-of-mles" title="Link to this heading"></a></h2>
<p>The true power of maximum likelihood estimation lies in its asymptotic properties. Under regularity conditions, MLEs are consistent, asymptotically normal, and asymptotically efficient.</p>
<section id="regularity-conditions">
<h3>Regularity Conditions<a class="headerlink" href="#regularity-conditions" title="Link to this heading"></a></h3>
<p>The classical asymptotic theory requires the following conditions. Let <span class="math notranslate nohighlight">\(\theta_0\)</span> denote the true parameter value:</p>
<div class="important admonition">
<p class="admonition-title">Regularity Conditions for MLE Asymptotics</p>
<p><strong>R1. Identifiability</strong>: Different parameter values give different distributions: <span class="math notranslate nohighlight">\(\theta_1 \neq \theta_2 \Rightarrow f(\cdot|\theta_1) \neq f(\cdot|\theta_2)\)</span>.</p>
<p><strong>R2. Common support</strong>: The support of <span class="math notranslate nohighlight">\(f(x|\theta)\)</span> does not depend on <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p><strong>R3. Interior true value</strong>: <span class="math notranslate nohighlight">\(\theta_0\)</span> is in the interior of the parameter space <span class="math notranslate nohighlight">\(\Theta\)</span>.</p>
<p><strong>R4. Differentiability</strong>: <span class="math notranslate nohighlight">\(\log f(x|\theta)\)</span> is three times continuously differentiable in <span class="math notranslate nohighlight">\(\theta\)</span> for all <span class="math notranslate nohighlight">\(x\)</span> in the support.</p>
<p><strong>R5. Integrability</strong>: We can interchange differentiation and integration:
<span class="math notranslate nohighlight">\(\frac{\partial}{\partial \theta} \int f(x|\theta) dx = \int \frac{\partial f(x|\theta)}{\partial \theta} dx\)</span>.</p>
<p><strong>R6. Finite Fisher information</strong>: <span class="math notranslate nohighlight">\(0 &lt; I(\theta_0) &lt; \infty\)</span>.</p>
<p><strong>R7. Uniform integrability</strong>: <span class="math notranslate nohighlight">\(\left|\frac{\partial^3 \log f(x|\theta)}{\partial \theta^3}\right|\)</span> is bounded by a function with finite expectation in a neighborhood of <span class="math notranslate nohighlight">\(\theta_0\)</span>.</p>
</div>
<p>These conditions exclude some pathological cases (e.g., uniform distributions with unknown endpoints) but are satisfied by most common parametric families.</p>
</section>
<section id="consistency">
<h3>Consistency<a class="headerlink" href="#consistency" title="Link to this heading"></a></h3>
<div class="note admonition">
<p class="admonition-title">Theorem: Consistency of MLE</p>
<p>Under regularity conditions R1–R3, the MLE is <strong>consistent</strong>:</p>
<div class="math notranslate nohighlight">
\[\hat{\theta}_n \xrightarrow{p} \theta_0 \quad \text{as } n \to \infty\]</div>
</div>
<p><strong>Proof sketch</strong>: The key insight is that maximizing the log-likelihood is equivalent to maximizing the Kullback-Leibler divergence from the true distribution. Define:</p>
<div class="math notranslate nohighlight">
\[M(\theta) = \mathbb{E}_{\theta_0}\left[\log f(X|\theta)\right]\]</div>
<p>By Jensen’s inequality and strict concavity of the log function, <span class="math notranslate nohighlight">\(M(\theta)\)</span> is uniquely maximized at <span class="math notranslate nohighlight">\(\theta = \theta_0\)</span>. The sample average <span class="math notranslate nohighlight">\(\frac{1}{n}\ell(\theta)\)</span> converges uniformly to <span class="math notranslate nohighlight">\(M(\theta)\)</span> by the uniform law of large numbers, and the maximizer of the sample average converges to the maximizer of the limit. ∎</p>
</section>
<section id="asymptotic-normality">
<h3>Asymptotic Normality<a class="headerlink" href="#asymptotic-normality" title="Link to this heading"></a></h3>
<div class="note admonition">
<p class="admonition-title">Theorem: Asymptotic Normality of MLE</p>
<p>Under regularity conditions R1–R7, the MLE is <strong>asymptotically normal</strong>:</p>
<div class="math notranslate nohighlight" id="equation-mle-asymp-normal">
<span class="eqno">(53)<a class="headerlink" href="#equation-mle-asymp-normal" title="Link to this equation"></a></span>\[\sqrt{n}(\hat{\theta}_n - \theta_0) \xrightarrow{d} \mathcal{N}\left(0, I_1(\theta_0)^{-1}\right)\]</div>
<p>Equivalently, for large <span class="math notranslate nohighlight">\(n\)</span>:</p>
<div class="math notranslate nohighlight">
\[\hat{\theta}_n \stackrel{\cdot}{\sim} \mathcal{N}\left(\theta_0, \frac{1}{nI_1(\theta_0)}\right)\]</div>
</div>
<p><strong>Proof</strong>: Taylor expand the score function around <span class="math notranslate nohighlight">\(\theta_0\)</span>:</p>
<div class="math notranslate nohighlight">
\[0 = U(\hat{\theta}_n) = U(\theta_0) + (\hat{\theta}_n - \theta_0) \frac{\partial U}{\partial \theta}\bigg|_{\tilde{\theta}}\]</div>
<p>for some <span class="math notranslate nohighlight">\(\tilde{\theta}\)</span> between <span class="math notranslate nohighlight">\(\hat{\theta}_n\)</span> and <span class="math notranslate nohighlight">\(\theta_0\)</span>. Rearranging:</p>
<div class="math notranslate nohighlight">
\[\sqrt{n}(\hat{\theta}_n - \theta_0) = \frac{\frac{1}{\sqrt{n}} U(\theta_0)}{-\frac{1}{n} \frac{\partial U}{\partial \theta}\big|_{\tilde{\theta}}}\]</div>
<p>The numerator: <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{n}} U(\theta_0) = \frac{1}{\sqrt{n}} \sum_{i=1}^n u_i\)</span> where <span class="math notranslate nohighlight">\(u_i\)</span> has mean 0 and variance <span class="math notranslate nohighlight">\(I_1(\theta_0)\)</span>. By the CLT:</p>
<div class="math notranslate nohighlight">
\[\frac{1}{\sqrt{n}} U(\theta_0) \xrightarrow{d} \mathcal{N}(0, I_1(\theta_0))\]</div>
<p>The denominator: <span class="math notranslate nohighlight">\(-\frac{1}{n} \frac{\partial U}{\partial \theta} = -\frac{1}{n} \frac{\partial^2 \ell}{\partial \theta^2} \xrightarrow{p} I_1(\theta_0)\)</span> by the LLN and consistency of <span class="math notranslate nohighlight">\(\hat{\theta}_n\)</span>.</p>
<p>By Slutsky’s theorem:</p>
<div class="math notranslate nohighlight">
\[\sqrt{n}(\hat{\theta}_n - \theta_0) \xrightarrow{d} \frac{\mathcal{N}(0, I_1(\theta_0))}{I_1(\theta_0)} = \mathcal{N}\left(0, \frac{1}{I_1(\theta_0)}\right) \quad \blacksquare\]</div>
<p>This result is remarkably powerful: regardless of the underlying distribution, MLEs have approximately normal sampling distributions with variance determined by the Fisher information.</p>
<figure class="align-center" id="id8">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_2_fig04_asymptotic_normality.png"><img alt="Asymptotic normality of MLE demonstrated via simulation" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_2_fig04_asymptotic_normality.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 89 </span><span class="caption-text"><strong>Figure 3.2.4</strong>: Asymptotic normality of the exponential MLE. The standardized statistic <span class="math notranslate nohighlight">\(\sqrt{nI_1(\lambda_0)}(\hat{\lambda} - \lambda_0)\)</span> converges to <span class="math notranslate nohighlight">\(N(0,1)\)</span> as <span class="math notranslate nohighlight">\(n\)</span> increases. For <span class="math notranslate nohighlight">\(n=5\)</span>, the distribution is right-skewed (skewness = 2.63); by <span class="math notranslate nohighlight">\(n=200\)</span>, it closely matches the standard normal (skewness = 0.28). The K-S test p-values confirm improved approximation with larger samples.</span><a class="headerlink" href="#id8" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="model-misspecification-and-the-sandwich-estimator">
<h3>Model Misspecification and the Sandwich Estimator<a class="headerlink" href="#model-misspecification-and-the-sandwich-estimator" title="Link to this heading"></a></h3>
<p>The asymptotic theory developed above assumes the model is <em>correctly specified</em>—that the data truly come from <span class="math notranslate nohighlight">\(f(x|\theta_0)\)</span> for some <span class="math notranslate nohighlight">\(\theta_0 \in \Theta\)</span>. In practice, all models are approximations. What happens when the model is wrong?</p>
<p><strong>The quasi-MLE target</strong>: Even under misspecification, the MLE converges to a well-defined limit: the parameter value <span class="math notranslate nohighlight">\(\theta^*\)</span> that minimizes the Kullback-Leibler divergence from the true data-generating distribution <span class="math notranslate nohighlight">\(g(x)\)</span> to the model family:</p>
<div class="math notranslate nohighlight">
\[\theta^* = \arg\min_\theta \text{KL}(g \| f_\theta) = \arg\min_\theta \left[ -\int g(x) \log f(x|\theta) \, dx \right]\]</div>
<p>This is the “best approximation” within the model family, even if no member of the family is correct.</p>
<p><strong>Asymptotic normality still holds</strong>, but with a different variance. Define:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{A} &amp;= -\mathbb{E}_{g}\left[\frac{\partial^2 \log f(X|\theta^*)}{\partial \theta \partial \theta^\top}\right] \quad \text{(expected Hessian under true } g\text{)} \\
\mathbf{B} &amp;= \mathbb{E}_{g}\left[\frac{\partial \log f(X|\theta^*)}{\partial \theta} \frac{\partial \log f(X|\theta^*)}{\partial \theta^\top}\right] \quad \text{(variance of score under true } g\text{)}\end{split}\]</div>
<p>Under correct specification, <span class="math notranslate nohighlight">\(\mathbf{A} = \mathbf{B} = \mathbf{I}(\theta_0)\)</span> (the information equality). Under misspecification, <span class="math notranslate nohighlight">\(\mathbf{A} \neq \mathbf{B}\)</span> in general.</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Quasi-MLE Asymptotics</p>
<p>Under misspecification and regularity conditions:</p>
<div class="math notranslate nohighlight">
\[\sqrt{n}(\hat{\theta}_n - \theta^*) \xrightarrow{d} \mathcal{N}\left(\mathbf{0}, \mathbf{A}^{-1} \mathbf{B} \mathbf{A}^{-1}\right)\]</div>
<p>The variance <span class="math notranslate nohighlight">\(\mathbf{A}^{-1} \mathbf{B} \mathbf{A}^{-1}\)</span> is called the <strong>sandwich variance</strong> (or Huber-White variance).</p>
</div>
<p><strong>Practical implications</strong>:</p>
<ol class="arabic">
<li><p><strong>Standard errors may be wrong</strong>: If we use <span class="math notranslate nohighlight">\(I(\hat{\theta})^{-1}\)</span> for variance (assuming correct specification), SEs can be too small or too large depending on the nature of misspecification.</p></li>
<li><p><strong>Sandwich (robust) standard errors</strong>: Estimate <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{B}\)</span> separately:</p>
<div class="math notranslate nohighlight">
\[\hat{\mathbf{A}} = -\frac{1}{n} \sum_{i=1}^n \frac{\partial^2 \log f(x_i|\hat{\theta})}{\partial \theta \partial \theta^\top}, \quad
\hat{\mathbf{B}} = \frac{1}{n} \sum_{i=1}^n \frac{\partial \log f(x_i|\hat{\theta})}{\partial \theta} \frac{\partial \log f(x_i|\hat{\theta})}{\partial \theta^\top}\]</div>
<p>Then <span class="math notranslate nohighlight">\(\widehat{\text{Var}}(\hat{\theta}) = \hat{\mathbf{A}}^{-1} \hat{\mathbf{B}} \hat{\mathbf{A}}^{-1} / n\)</span>.</p>
</li>
<li><p><strong>Model-based vs. robust inference</strong>: Under correct specification, both give consistent SEs. Under misspecification, only sandwich SEs are consistent. The difference between them is a diagnostic for misspecification.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">sandwich_variance</span><span class="p">(</span><span class="n">score_contributions</span><span class="p">,</span> <span class="n">hessian</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute sandwich variance estimator.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    score_contributions : ndarray, shape (n, p)</span>
<span class="sd">        Individual score contributions ∂log f(xᵢ|θ̂)/∂θ for each observation.</span>
<span class="sd">    hessian : ndarray, shape (p, p)</span>
<span class="sd">        Observed Hessian ∂²ℓ/∂θ∂θ&#39; evaluated at MLE.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    sandwich_var : ndarray, shape (p, p)</span>
<span class="sd">        Sandwich variance estimate for θ̂.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">score_contributions</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># A = -Hessian/n (average negative curvature)</span>
    <span class="n">A</span> <span class="o">=</span> <span class="o">-</span><span class="n">hessian</span> <span class="o">/</span> <span class="n">n</span>

    <span class="c1"># B = Var(score) estimate = (1/n) Σ uᵢuᵢ&#39; (outer products)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">score_contributions</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">score_contributions</span> <span class="o">/</span> <span class="n">n</span>

    <span class="c1"># Sandwich: A⁻¹ B A⁻¹ / n</span>
    <span class="n">A_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
    <span class="n">sandwich_var</span> <span class="o">=</span> <span class="n">A_inv</span> <span class="o">@</span> <span class="n">B</span> <span class="o">@</span> <span class="n">A_inv</span> <span class="o">/</span> <span class="n">n</span>

    <span class="k">return</span> <span class="n">sandwich_var</span>
</pre></div>
</div>
<figure class="align-center" id="id9">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_2_fig09_sandwich_variance.png"><img alt="Model misspecification and sandwich variance" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_2_fig09_sandwich_variance.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 90 </span><span class="caption-text"><strong>Figure 3.2.9</strong>: Effects of model misspecification on MLE inference. A Normal model is fit to data from three distributions: (left) correctly specified <span class="math notranslate nohighlight">\(N(0,1)\)</span>, (center) mildly misspecified <span class="math notranslate nohighlight">\(t_5\)</span>, and (right) severely misspecified <span class="math notranslate nohighlight">\(t_3\)</span>. Under correct specification, model-based and sandwich SEs agree. Under misspecification, the heavier tails of <span class="math notranslate nohighlight">\(t\)</span>-distributions inflate variance; model-based SEs underestimate true variability while sandwich SEs remain consistent.</span><a class="headerlink" href="#id9" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="the-cramer-rao-lower-bound">
<h2>The Cramér-Rao Lower Bound<a class="headerlink" href="#the-cramer-rao-lower-bound" title="Link to this heading"></a></h2>
<p>The Cramér-Rao inequality establishes a fundamental limit on how precisely <em>any</em> unbiased estimator can estimate a parameter. The MLE achieves this bound asymptotically.</p>
<section id="statement-and-proof">
<h3>Statement and Proof<a class="headerlink" href="#statement-and-proof" title="Link to this heading"></a></h3>
<div class="note admonition">
<p class="admonition-title">Theorem: Cramér-Rao Lower Bound</p>
<p>Let <span class="math notranslate nohighlight">\(T = T(X_1, \ldots, X_n)\)</span> be any unbiased estimator of <span class="math notranslate nohighlight">\(\tau(\theta)\)</span>, a function of the parameter. Under regularity conditions R4–R6:</p>
<div class="math notranslate nohighlight" id="equation-cramer-rao">
<span class="eqno">(54)<a class="headerlink" href="#equation-cramer-rao" title="Link to this equation"></a></span>\[\text{Var}_\theta(T) \geq \frac{[\tau'(\theta)]^2}{I_n(\theta)} = \frac{[\tau'(\theta)]^2}{n I_1(\theta)}\]</div>
<p>For estimating <span class="math notranslate nohighlight">\(\theta\)</span> itself (<span class="math notranslate nohighlight">\(\tau(\theta) = \theta\)</span>, so <span class="math notranslate nohighlight">\(\tau'(\theta) = 1\)</span>):</p>
<div class="math notranslate nohighlight" id="equation-cramer-rao-theta">
<span class="eqno">(55)<a class="headerlink" href="#equation-cramer-rao-theta" title="Link to this equation"></a></span>\[\text{Var}_\theta(\hat{\theta}) \geq \frac{1}{n I_1(\theta)}\]</div>
</div>
<p><strong>Complete Proof</strong>:</p>
<p>The proof uses the Cauchy-Schwarz inequality. Define:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(U = U(\theta)\)</span> = score function</p></li>
<li><p><span class="math notranslate nohighlight">\(T\)</span> = unbiased estimator with <span class="math notranslate nohighlight">\(\mathbb{E}_\theta[T] = \tau(\theta)\)</span></p></li>
</ul>
<p>We know <span class="math notranslate nohighlight">\(\mathbb{E}_\theta[U] = 0\)</span> (score has mean zero).</p>
<p><strong>Step 1</strong>: Differentiate the unbiasedness condition.</p>
<p>Since <span class="math notranslate nohighlight">\(\mathbb{E}_\theta[T] = \tau(\theta)\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[\int T(x) f(x|\theta) dx = \tau(\theta)\]</div>
<p>Differentiating both sides with respect to <span class="math notranslate nohighlight">\(\theta\)</span>:</p>
<div class="math notranslate nohighlight">
\[\int T(x) \frac{\partial f(x|\theta)}{\partial \theta} dx = \tau'(\theta)\]</div>
<p>Using <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial \theta} = f \cdot \frac{\partial \log f}{\partial \theta}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\int T(x) f(x|\theta) \frac{\partial \log f(x|\theta)}{\partial \theta} dx = \tau'(\theta)\]</div>
<p>This is <span class="math notranslate nohighlight">\(\mathbb{E}_\theta[T \cdot U] = \tau'(\theta)\)</span>.</p>
<p><strong>Step 2</strong>: Compute the covariance.</p>
<p>Since <span class="math notranslate nohighlight">\(\mathbb{E}[U] = 0\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{Cov}_\theta(T, U) = \mathbb{E}_\theta[T \cdot U] - \mathbb{E}_\theta[T] \cdot \mathbb{E}_\theta[U] = \tau'(\theta) - \tau(\theta) \cdot 0 = \tau'(\theta)\]</div>
<p><strong>Step 3</strong>: Apply Cauchy-Schwarz.</p>
<p>For any random variables <span class="math notranslate nohighlight">\(A, B\)</span>: <span class="math notranslate nohighlight">\([\text{Cov}(A,B)]^2 \leq \text{Var}(A) \cdot \text{Var}(B)\)</span>.</p>
<p>Applied to <span class="math notranslate nohighlight">\(T\)</span> and <span class="math notranslate nohighlight">\(U\)</span>:</p>
<div class="math notranslate nohighlight">
\[[\tau'(\theta)]^2 = [\text{Cov}(T, U)]^2 \leq \text{Var}(T) \cdot \text{Var}(U) = \text{Var}(T) \cdot I_n(\theta)\]</div>
<p>Rearranging:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(T) \geq \frac{[\tau'(\theta)]^2}{I_n(\theta)} \quad \blacksquare\]</div>
</section>
<section id="efficiency">
<h3>Efficiency<a class="headerlink" href="#efficiency" title="Link to this heading"></a></h3>
<p>An estimator that achieves the Cramér-Rao bound is called <strong>efficient</strong>. The MLE is not generally efficient for finite samples, but it achieves the bound asymptotically:</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Asymptotic Efficiency of MLE</p>
<p>Under regularity conditions, the MLE is <strong>asymptotically efficient</strong>: its asymptotic variance equals the Cramér-Rao lower bound.</p>
<div class="math notranslate nohighlight">
\[\text{AVar}(\hat{\theta}_{\text{MLE}}) = \lim_{n \to \infty} n \cdot \text{Var}(\hat{\theta}_n) = \frac{1}{I_1(\theta_0)}\]</div>
</div>
<p>This means that for large samples, no unbiased estimator can have smaller variance than the MLE.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">simulate_mle_distribution</span><span class="p">(</span><span class="n">true_theta</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_simulations</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Simulate the sampling distribution of the Poisson MLE to verify asymptotic normality.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    true_theta : float</span>
<span class="sd">        True Poisson rate parameter.</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Sample size per simulation.</span>
<span class="sd">    n_simulations : int</span>
<span class="sd">        Number of Monte Carlo simulations.</span>
<span class="sd">    seed : int</span>
<span class="sd">        Random seed.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        MLEs, theoretical quantities, and test statistics.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Simulate MLEs</span>
    <span class="n">mles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_simulations</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_simulations</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="n">true_theta</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_samples</span><span class="p">)</span>
        <span class="n">mles</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># MLE for Poisson is sample mean</span>

    <span class="c1"># Theoretical values</span>
    <span class="c1"># Fisher information for Poisson: I₁(λ) = 1/λ</span>
    <span class="n">fisher_info</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">true_theta</span>
    <span class="n">theoretical_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_samples</span> <span class="o">*</span> <span class="n">fisher_info</span><span class="p">))</span>
    <span class="n">cramer_rao_bound</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_samples</span> <span class="o">*</span> <span class="n">fisher_info</span><span class="p">)</span>

    <span class="c1"># Standardized MLEs for normality check</span>
    <span class="n">standardized</span> <span class="o">=</span> <span class="p">(</span><span class="n">mles</span> <span class="o">-</span> <span class="n">true_theta</span><span class="p">)</span> <span class="o">/</span> <span class="n">theoretical_se</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;mles&#39;</span><span class="p">:</span> <span class="n">mles</span><span class="p">,</span>
        <span class="s1">&#39;mean&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mles</span><span class="p">),</span>
        <span class="s1">&#39;std&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">mles</span><span class="p">),</span>
        <span class="s1">&#39;theoretical_mean&#39;</span><span class="p">:</span> <span class="n">true_theta</span><span class="p">,</span>
        <span class="s1">&#39;theoretical_se&#39;</span><span class="p">:</span> <span class="n">theoretical_se</span><span class="p">,</span>
        <span class="s1">&#39;cramer_rao_bound&#39;</span><span class="p">:</span> <span class="n">cramer_rao_bound</span><span class="p">,</span>
        <span class="s1">&#39;standardized&#39;</span><span class="p">:</span> <span class="n">standardized</span>
    <span class="p">}</span>

<span class="c1"># Run simulation</span>
<span class="n">true_lambda</span> <span class="o">=</span> <span class="mf">5.0</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">sample_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">200</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Verifying MLE Asymptotic Properties (Poisson)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True λ = </span><span class="si">{</span><span class="n">true_lambda</span><span class="si">}</span><span class="s2">, CR Bound = 1/(n·I₁) = λ/n&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;n&#39;</span><span class="si">:</span><span class="s2">&gt;6</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Mean(λ̂)&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;SD(λ̂)&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Theor SE&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;CR Bound&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">sample_sizes</span><span class="p">:</span>
    <span class="n">results</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">simulate_mle_distribution</span><span class="p">(</span><span class="n">true_lambda</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="n">n</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">n</span><span class="si">:</span><span class="s2">&gt;6</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;mean&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;std&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;theoretical_se&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;cramer_rao_bound&#39;</span><span class="p">])</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Verifying MLE Asymptotic Properties (Poisson)
============================================================
True λ = 5.0, CR Bound = 1/(n·I₁) = λ/n

     n     Mean(λ̂)       SD(λ̂)     Theor SE     CR Bound
------------------------------------------------------------
    10       5.0021       0.7106       0.7071       0.7071
    50       5.0006       0.3173       0.3162       0.3162
   200       4.9990       0.1575       0.1581       0.1581
</pre></div>
</div>
<figure class="align-center" id="id10">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_2_fig08_cramer_rao.png"><img alt="MLE variance approaching the Cramér-Rao lower bound" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_2_fig08_cramer_rao.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 91 </span><span class="caption-text"><strong>Figure 3.2.8</strong>: Asymptotic efficiency of the exponential MLE. (a) Log-log plot showing that empirical <span class="math notranslate nohighlight">\(\text{Var}(\hat{\lambda})\)</span> closely tracks the Cramér-Rao lower bound <span class="math notranslate nohighlight">\(\lambda^2/n\)</span> across sample sizes. (b) Efficiency ratio <span class="math notranslate nohighlight">\(\text{CRLB}/\text{Var}(\hat{\lambda})\)</span> approaches 1 as <span class="math notranslate nohighlight">\(n \to \infty\)</span>, confirming that the MLE achieves the theoretical minimum variance asymptotically.</span><a class="headerlink" href="#id10" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="the-invariance-property">
<h2>The Invariance Property<a class="headerlink" href="#the-invariance-property" title="Link to this heading"></a></h2>
<p>A remarkable feature of maximum likelihood is its behavior under reparameterization.</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Invariance of MLE</p>
<p>If <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is the MLE of <span class="math notranslate nohighlight">\(\theta\)</span>, then for any function <span class="math notranslate nohighlight">\(g\)</span>, the MLE of <span class="math notranslate nohighlight">\(\tau = g(\theta)\)</span> is <span class="math notranslate nohighlight">\(\hat{\tau} = g(\hat{\theta})\)</span>.</p>
</div>
<p>This follows directly from the definition: if <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> maximizes <span class="math notranslate nohighlight">\(L(\theta)\)</span>, then <span class="math notranslate nohighlight">\(g(\hat{\theta})\)</span> maximizes <span class="math notranslate nohighlight">\(L(g^{-1}(\tau))\)</span> over <span class="math notranslate nohighlight">\(\tau\)</span> (when <span class="math notranslate nohighlight">\(g\)</span> is one-to-one).</p>
<p><strong>Example</strong>: For exponential data, <span class="math notranslate nohighlight">\(\hat{\lambda} = 1/\bar{x}\)</span>. The MLE of the mean <span class="math notranslate nohighlight">\(\mu = 1/\lambda\)</span> is therefore <span class="math notranslate nohighlight">\(\hat{\mu} = \bar{x}\)</span>—no additional optimization needed.</p>
<p>This invariance property distinguishes MLE from other estimation methods. The method of moments estimator for <span class="math notranslate nohighlight">\(g(\theta)\)</span> is generally <em>not</em> <span class="math notranslate nohighlight">\(g(\hat{\theta}_{\text{MoM}})\)</span>.</p>
</section>
<section id="likelihood-based-hypothesis-testing">
<h2>Likelihood-Based Hypothesis Testing<a class="headerlink" href="#likelihood-based-hypothesis-testing" title="Link to this heading"></a></h2>
<p>Maximum likelihood provides a unified framework for hypothesis testing through three asymptotically equivalent tests.</p>
<section id="the-likelihood-ratio-test">
<h3>The Likelihood Ratio Test<a class="headerlink" href="#the-likelihood-ratio-test" title="Link to this heading"></a></h3>
<p>Consider testing <span class="math notranslate nohighlight">\(H_0: \theta \in \Theta_0\)</span> versus <span class="math notranslate nohighlight">\(H_1: \theta \in \Theta_1 = \Theta \setminus \Theta_0\)</span>.</p>
<p>The <strong>likelihood ratio statistic</strong> is:</p>
<div class="math notranslate nohighlight" id="equation-lr-stat">
<span class="eqno">(56)<a class="headerlink" href="#equation-lr-stat" title="Link to this equation"></a></span>\[\Lambda = \frac{\sup_{\theta \in \Theta_0} L(\theta)}{\sup_{\theta \in \Theta} L(\theta)} = \frac{L(\hat{\theta}_0)}{L(\hat{\theta})}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{\theta}_0\)</span> is the MLE under <span class="math notranslate nohighlight">\(H_0\)</span> and <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is the unrestricted MLE.</p>
<p>Since <span class="math notranslate nohighlight">\(0 \leq \Lambda \leq 1\)</span>, we reject <span class="math notranslate nohighlight">\(H_0\)</span> for small values of <span class="math notranslate nohighlight">\(\Lambda\)</span>, or equivalently, for large values of:</p>
<div class="math notranslate nohighlight" id="equation-deviance">
<span class="eqno">(57)<a class="headerlink" href="#equation-deviance" title="Link to this equation"></a></span>\[D = -2\log\Lambda = 2[\ell(\hat{\theta}) - \ell(\hat{\theta}_0)]\]</div>
<div class="note admonition">
<p class="admonition-title">Theorem: Wilks’ Theorem</p>
<p>Under <span class="math notranslate nohighlight">\(H_0\)</span> and regularity conditions, the deviance <span class="math notranslate nohighlight">\(D\)</span> converges in distribution:</p>
<div class="math notranslate nohighlight">
\[D = -2\log\Lambda \xrightarrow{d} \chi^2_r \quad \text{as } n \to \infty\]</div>
<p>where <span class="math notranslate nohighlight">\(r = \dim(\Theta) - \dim(\Theta_0)\)</span> is the difference in parameter dimensions.</p>
</div>
<p>For testing <span class="math notranslate nohighlight">\(H_0: \theta = \theta_0\)</span> (a point null) against <span class="math notranslate nohighlight">\(H_1: \theta \neq \theta_0\)</span> with scalar <span class="math notranslate nohighlight">\(\theta\)</span>:</p>
<div class="math notranslate nohighlight">
\[D = 2[\ell(\hat{\theta}) - \ell(\theta_0)] \xrightarrow{d} \chi^2_1\]</div>
</section>
<section id="wald-test">
<h3>Wald Test<a class="headerlink" href="#wald-test" title="Link to this heading"></a></h3>
<p>The <strong>Wald test</strong> uses the asymptotic normality of the MLE directly. Using per-observation information <span class="math notranslate nohighlight">\(I_1(\cdot)\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-wald-stat-revised">
<span class="eqno">(58)<a class="headerlink" href="#equation-wald-stat-revised" title="Link to this equation"></a></span>\[W = n \cdot I_1(\hat{\theta}) \cdot (\hat{\theta} - \theta_0)^2 = \frac{(\hat{\theta} - \theta_0)^2}{\widehat{\text{Var}}(\hat{\theta})}\]</div>
<p>where <span class="math notranslate nohighlight">\(\widehat{\text{Var}}(\hat{\theta}) = 1/[n I_1(\hat{\theta})]\)</span>.</p>
<p>Equivalently, using total information <span class="math notranslate nohighlight">\(I_n(\hat{\theta}) = n I_1(\hat{\theta})\)</span>:</p>
<div class="math notranslate nohighlight">
\[W = I_n(\hat{\theta}) \cdot (\hat{\theta} - \theta_0)^2\]</div>
<p>For multivariate <span class="math notranslate nohighlight">\(\boldsymbol{\theta} \in \mathbb{R}^p\)</span>, using total information <span class="math notranslate nohighlight">\(\mathbf{I}_n(\boldsymbol{\theta}) = n \mathbf{I}_1(\boldsymbol{\theta})\)</span>:</p>
<div class="math notranslate nohighlight">
\[W = (\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0)^\top \widehat{\mathbf{I}}_n(\hat{\boldsymbol{\theta}}) (\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0) \xrightarrow{d} \chi^2_p\]</div>
<p>where <span class="math notranslate nohighlight">\(\widehat{\mathbf{I}}_n\)</span> may be either:</p>
<ul class="simple">
<li><p>Expected information: <span class="math notranslate nohighlight">\(n \mathbf{I}_1(\hat{\boldsymbol{\theta}})\)</span></p></li>
<li><p>Observed information: <span class="math notranslate nohighlight">\(\mathbf{J}_n(\hat{\boldsymbol{\theta}}) = -\partial^2 \ell_n / \partial \boldsymbol{\theta} \partial \boldsymbol{\theta}^\top |_{\hat{\boldsymbol{\theta}}}\)</span></p></li>
</ul>
<p>Both are asymptotically equivalent under correct specification.</p>
</section>
<section id="score-test-rao-test">
<h3>Score Test (Rao Test)<a class="headerlink" href="#score-test-rao-test" title="Link to this heading"></a></h3>
<p>The <strong>score test</strong> evaluates the score function at the null value:</p>
<div class="math notranslate nohighlight" id="equation-score-stat">
<span class="eqno">(59)<a class="headerlink" href="#equation-score-stat" title="Link to this equation"></a></span>\[S = \frac{U(\theta_0)^2}{I(\theta_0)} = U(\theta_0)^\top I(\theta_0)^{-1} U(\theta_0)\]</div>
<p>Under <span class="math notranslate nohighlight">\(H_0\)</span>:</p>
<div class="math notranslate nohighlight">
\[S \xrightarrow{d} \chi^2_1\]</div>
<p><strong>Computational tradeoffs</strong>:</p>
<ul class="simple">
<li><p><strong>Likelihood ratio</strong>: Requires fitting both restricted and unrestricted models</p></li>
<li><p><strong>Wald</strong>: Requires only the unrestricted MLE</p></li>
<li><p><strong>Score</strong>: Requires only evaluation at the null—no optimization needed</p></li>
</ul>
<p>All three tests are asymptotically equivalent under <span class="math notranslate nohighlight">\(H_0\)</span>, but can differ substantially in finite samples. The ordering <span class="math notranslate nohighlight">\(W \geq D \geq S\)</span> often holds for the test statistics (though not universally).</p>
<figure class="align-center" id="id11">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_2_fig06_three_tests.png"><img alt="Geometric comparison of likelihood ratio, Wald, and score tests" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_2_fig06_three_tests.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 92 </span><span class="caption-text"><strong>Figure 3.2.6</strong>: Geometric interpretation of the three likelihood-based tests for <span class="math notranslate nohighlight">\(H_0: \lambda = \lambda_0\)</span>. (a) <strong>Likelihood ratio test</strong>: measures the vertical drop <span class="math notranslate nohighlight">\(D = 2[\ell(\hat{\lambda}) - \ell(\lambda_0)]\)</span> between the MLE and null. (b) <strong>Wald test</strong>: measures the horizontal distance <span class="math notranslate nohighlight">\((\hat{\lambda} - \lambda_0)^2\)</span> scaled by estimated variance. (c) <strong>Score test</strong>: measures the slope (tangent line) at <span class="math notranslate nohighlight">\(\lambda_0\)</span>—a steep slope indicates the null is far from the maximum. All three statistics are asymptotically <span class="math notranslate nohighlight">\(\chi^2_1\)</span> under <span class="math notranslate nohighlight">\(H_0\)</span>.</span><a class="headerlink" href="#id11" title="Link to this image"></a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">likelihood_tests_poisson</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Perform likelihood ratio, Wald, and score tests for Poisson rate.</span>

<span class="sd">    Tests H₀: λ = λ₀ vs H₁: λ ≠ λ₀.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : array-like</span>
<span class="sd">        Observed counts.</span>
<span class="sd">    lambda_0 : float</span>
<span class="sd">        Null hypothesis rate.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        Test statistics and p-values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x_bar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">lambda_hat</span> <span class="o">=</span> <span class="n">x_bar</span>  <span class="c1"># MLE</span>

    <span class="c1"># Log-likelihood function</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">log_lik</span><span class="p">(</span><span class="n">lam</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">lam</span><span class="p">)</span> <span class="o">-</span> <span class="n">lam</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">factorial</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">xi</span><span class="p">))</span> <span class="k">for</span> <span class="n">xi</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])))</span>

    <span class="c1"># Simpler: use scipy.stats</span>
    <span class="n">ll_mle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">logpmf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_hat</span><span class="p">))</span>
    <span class="n">ll_null</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">logpmf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_0</span><span class="p">))</span>

    <span class="c1"># Likelihood ratio test</span>
    <span class="n">D</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">ll_mle</span> <span class="o">-</span> <span class="n">ll_null</span><span class="p">)</span>
    <span class="n">lr_pvalue</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">chi2</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Wald test</span>
    <span class="c1"># Var(λ̂) ≈ λ/n, so W = n(λ̂ - λ₀)²/λ̂</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="p">(</span><span class="n">lambda_hat</span> <span class="o">-</span> <span class="n">lambda_0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">lambda_hat</span>
    <span class="n">wald_pvalue</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">chi2</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Score test</span>
    <span class="c1"># Score at λ₀: U(λ₀) = n·x̄/λ₀ - n = n(x̄ - λ₀)/λ₀</span>
    <span class="c1"># Fisher info at λ₀: I(λ₀) = n/λ₀</span>
    <span class="c1"># S = U(λ₀)²/I(λ₀) = n(x̄ - λ₀)²/λ₀</span>
    <span class="n">S</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_bar</span> <span class="o">-</span> <span class="n">lambda_0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">lambda_0</span>
    <span class="n">score_pvalue</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">chi2</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;lambda_hat&#39;</span><span class="p">:</span> <span class="n">lambda_hat</span><span class="p">,</span>
        <span class="s1">&#39;lambda_0&#39;</span><span class="p">:</span> <span class="n">lambda_0</span><span class="p">,</span>
        <span class="s1">&#39;lr_stat&#39;</span><span class="p">:</span> <span class="n">D</span><span class="p">,</span>
        <span class="s1">&#39;lr_pvalue&#39;</span><span class="p">:</span> <span class="n">lr_pvalue</span><span class="p">,</span>
        <span class="s1">&#39;wald_stat&#39;</span><span class="p">:</span> <span class="n">W</span><span class="p">,</span>
        <span class="s1">&#39;wald_pvalue&#39;</span><span class="p">:</span> <span class="n">wald_pvalue</span><span class="p">,</span>
        <span class="s1">&#39;score_stat&#39;</span><span class="p">:</span> <span class="n">S</span><span class="p">,</span>
        <span class="s1">&#39;score_pvalue&#39;</span><span class="p">:</span> <span class="n">score_pvalue</span>
    <span class="p">}</span>

<span class="c1"># Example: Test if Poisson rate equals 5</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="n">lam</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>  <span class="c1"># True rate is 6, not 5</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">likelihood_tests_poisson</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_0</span><span class="o">=</span><span class="mf">5.0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Testing H₀: λ = 5.0&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MLE: λ̂ = </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;lambda_hat&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Test&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Statistic&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;p-value&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">45</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Likelihood Ratio&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;lr_stat&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;lr_pvalue&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Wald&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;wald_stat&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;wald_pvalue&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Score&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;score_stat&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;score_pvalue&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Testing H₀: λ = 5.0
MLE: λ̂ = 5.8600

Test                    Statistic      p-value
---------------------------------------------
Likelihood Ratio            3.6455       0.0562
Wald                        3.9898       0.0458
Score                       3.4848       0.0619
</pre></div>
</div>
</section>
</section>
<section id="confidence-intervals-from-likelihood">
<h2>Confidence Intervals from Likelihood<a class="headerlink" href="#confidence-intervals-from-likelihood" title="Link to this heading"></a></h2>
<p>The asymptotic normality of MLEs provides multiple approaches to confidence interval construction.</p>
<section id="wald-intervals">
<h3>Wald Intervals<a class="headerlink" href="#wald-intervals" title="Link to this heading"></a></h3>
<p>The simplest approach uses the normal approximation directly:</p>
<div class="math notranslate nohighlight" id="equation-wald-ci">
<span class="eqno">(60)<a class="headerlink" href="#equation-wald-ci" title="Link to this equation"></a></span>\[\hat{\theta} \pm z_{\alpha/2} \cdot \widehat{\text{SE}}(\hat{\theta})\]</div>
<p>where <span class="math notranslate nohighlight">\(\widehat{\text{SE}} = 1/\sqrt{n I(\hat{\theta})}\)</span> or is estimated from the observed Hessian.</p>
<p><strong>Limitations</strong>: Wald intervals</p>
<ul class="simple">
<li><p>Are not invariant to reparameterization</p></li>
<li><p>Can give poor coverage near parameter boundaries</p></li>
<li><p>May extend outside the parameter space</p></li>
</ul>
</section>
<section id="profile-likelihood-intervals">
<h3>Profile Likelihood Intervals<a class="headerlink" href="#profile-likelihood-intervals" title="Link to this heading"></a></h3>
<p><strong>Profile likelihood intervals</strong> invert the likelihood ratio test:</p>
<div class="math notranslate nohighlight" id="equation-profile-ci">
<span class="eqno">(61)<a class="headerlink" href="#equation-profile-ci" title="Link to this equation"></a></span>\[\text{CI}_{1-\alpha} = \left\{ \theta : 2[\ell(\hat{\theta}) - \ell(\theta)] \leq \chi^2_{1, 1-\alpha} \right\}\]</div>
<p>These intervals are <strong>invariant</strong> to reparameterization: if we transform <span class="math notranslate nohighlight">\(\tau = g(\theta)\)</span>, the profile likelihood interval for <span class="math notranslate nohighlight">\(\tau\)</span> is exactly <span class="math notranslate nohighlight">\(g\)</span> applied to the interval for <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>For multiparameter models where <span class="math notranslate nohighlight">\(\theta = (\psi, \lambda)\)</span> with <span class="math notranslate nohighlight">\(\psi\)</span> the parameter of interest:</p>
<div class="math notranslate nohighlight">
\[\ell_p(\psi) = \max_\lambda \ell(\psi, \lambda)\]</div>
<p>The profile likelihood interval for <span class="math notranslate nohighlight">\(\psi\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\left\{ \psi : 2[\ell(\hat{\psi}, \hat{\lambda}) - \ell_p(\psi)] \leq \chi^2_{1, 1-\alpha} \right\}\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span><span class="p">,</span> <span class="n">optimize</span>

<span class="k">def</span><span class="w"> </span><span class="nf">profile_likelihood_ci_exponential</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">confidence</span><span class="o">=</span><span class="mf">0.95</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute profile likelihood CI for exponential rate parameter.</span>

<span class="sd">    Uses expanding bracket search for robustness.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : array-like</span>
<span class="sd">        Observed data.</span>
<span class="sd">    confidence : float</span>
<span class="sd">        Confidence level.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        MLE, Wald CI, and profile likelihood CI.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># MLE</span>
    <span class="n">lambda_hat</span> <span class="o">=</span> <span class="n">n</span> <span class="o">/</span> <span class="n">x_sum</span>

    <span class="c1"># Log-likelihood (up to constant)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">log_lik</span><span class="p">(</span><span class="n">lam</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">lam</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
        <span class="k">return</span> <span class="n">n</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">lam</span><span class="p">)</span> <span class="o">-</span> <span class="n">lam</span> <span class="o">*</span> <span class="n">x_sum</span>

    <span class="c1"># Maximum log-likelihood</span>
    <span class="n">ll_max</span> <span class="o">=</span> <span class="n">log_lik</span><span class="p">(</span><span class="n">lambda_hat</span><span class="p">)</span>

    <span class="c1"># Chi-square cutoff</span>
    <span class="n">chi2_cutoff</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">chi2</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">confidence</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Profile equation: find λ where 2(ℓ_max - ℓ(λ)) = χ²_{1,α}</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">profile_equation</span><span class="p">(</span><span class="n">lam</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">ll_max</span> <span class="o">-</span> <span class="n">log_lik</span><span class="p">(</span><span class="n">lam</span><span class="p">))</span> <span class="o">-</span> <span class="n">chi2_cutoff</span>

    <span class="c1"># ROBUST BRACKET SEARCH for lower bound</span>
    <span class="c1"># Start just above 0, expand downward if needed</span>
    <span class="n">lower_bracket_right</span> <span class="o">=</span> <span class="n">lambda_hat</span> <span class="o">*</span> <span class="mf">0.99</span>
    <span class="n">lower_bracket_left</span> <span class="o">=</span> <span class="n">lambda_hat</span> <span class="o">*</span> <span class="mf">0.01</span>

    <span class="c1"># Ensure sign change exists</span>
    <span class="n">max_expansions</span> <span class="o">=</span> <span class="mi">20</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_expansions</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">profile_equation</span><span class="p">(</span><span class="n">lower_bracket_left</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">lower_bracket_left</span> <span class="o">/=</span> <span class="mi">2</span>
        <span class="k">if</span> <span class="n">lower_bracket_left</span> <span class="o">&lt;</span> <span class="mf">1e-15</span><span class="p">:</span>
            <span class="n">lower_bracket_left</span> <span class="o">=</span> <span class="mf">1e-15</span>
            <span class="k">break</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">lower</span> <span class="o">=</span> <span class="n">optimize</span><span class="o">.</span><span class="n">brentq</span><span class="p">(</span><span class="n">profile_equation</span><span class="p">,</span> <span class="n">lower_bracket_left</span><span class="p">,</span> <span class="n">lower_bracket_right</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
        <span class="n">lower</span> <span class="o">=</span> <span class="mf">1e-15</span>  <span class="c1"># Fallback for edge cases</span>

    <span class="c1"># ROBUST BRACKET SEARCH for upper bound</span>
    <span class="c1"># Start at MLE, expand upward until sign change</span>
    <span class="n">upper_bracket_left</span> <span class="o">=</span> <span class="n">lambda_hat</span> <span class="o">*</span> <span class="mf">1.01</span>
    <span class="n">upper_bracket_right</span> <span class="o">=</span> <span class="n">lambda_hat</span> <span class="o">*</span> <span class="mf">2.0</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_expansions</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">profile_equation</span><span class="p">(</span><span class="n">upper_bracket_right</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">upper_bracket_right</span> <span class="o">*=</span> <span class="mi">2</span>  <span class="c1"># Double the bracket</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">upper</span> <span class="o">=</span> <span class="n">optimize</span><span class="o">.</span><span class="n">brentq</span><span class="p">(</span><span class="n">profile_equation</span><span class="p">,</span> <span class="n">upper_bracket_left</span><span class="p">,</span> <span class="n">upper_bracket_right</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
        <span class="n">upper</span> <span class="o">=</span> <span class="n">upper_bracket_right</span>  <span class="c1"># Fallback</span>

    <span class="c1"># Wald CI for comparison</span>
    <span class="n">se</span> <span class="o">=</span> <span class="n">lambda_hat</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>  <span class="c1"># SE(λ̂) = λ̂/√n for exponential</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">confidence</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">wald_lower</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">lambda_hat</span> <span class="o">-</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se</span><span class="p">)</span>  <span class="c1"># Clip at 0</span>
    <span class="n">wald_upper</span> <span class="o">=</span> <span class="n">lambda_hat</span> <span class="o">+</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;lambda_hat&#39;</span><span class="p">:</span> <span class="n">lambda_hat</span><span class="p">,</span>
        <span class="s1">&#39;wald_ci&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">wald_lower</span><span class="p">,</span> <span class="n">wald_upper</span><span class="p">),</span>
        <span class="s1">&#39;profile_ci&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">),</span>
        <span class="s1">&#39;se&#39;</span><span class="p">:</span> <span class="n">se</span>
    <span class="p">}</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Exponential rate estimation (n=30, true λ=2.0)
MLE: λ̂ = 1.9205
Wald 95% CI:    (1.2334, 2.6076)
Profile 95% CI: (1.3184, 2.6709)

Note: Profile CI is asymmetric around MLE (appropriate for positive parameter)
</pre></div>
</div>
<figure class="align-center" id="id12">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_2_fig07_confidence_intervals.png"><img alt="Comparison of confidence interval methods" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_2_fig07_confidence_intervals.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 93 </span><span class="caption-text"><strong>Figure 3.2.7</strong>: Confidence interval methods for the Binomial proportion (<span class="math notranslate nohighlight">\(x=3\)</span>, <span class="math notranslate nohighlight">\(n=20\)</span>, <span class="math notranslate nohighlight">\(\hat{p}=0.15\)</span>). (a) The log-likelihood with three 95% CIs: Wald (symmetric around MLE), Wilson/Score (shifted toward 0.5), and Profile (derived from likelihood ratio inversion). (b) Coverage simulation showing that Wald intervals undercover near the boundaries while Wilson intervals maintain closer to nominal 95% coverage across all <span class="math notranslate nohighlight">\(p\)</span> values.</span><a class="headerlink" href="#id12" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="practical-considerations">
<h2>Practical Considerations<a class="headerlink" href="#practical-considerations" title="Link to this heading"></a></h2>
<section id="numerical-stability">
<h3>Numerical Stability<a class="headerlink" href="#numerical-stability" title="Link to this heading"></a></h3>
<p>Several numerical issues arise in MLE computation:</p>
<ol class="arabic">
<li><p><strong>Log-likelihood underflow</strong>: Always work with log-likelihoods, never raw likelihoods.</p></li>
<li><p><strong>Log-sum-exp</strong>: When computing <span class="math notranslate nohighlight">\(\log \sum_i \exp(a_i)\)</span>, use the stable formula:</p>
<div class="math notranslate nohighlight">
\[\log \sum_i e^{a_i} = a_{\max} + \log \sum_i e^{a_i - a_{\max}}\]</div>
</li>
<li><p><strong>Hessian conditioning</strong>: Near-singular Hessians indicate weak identifiability. Consider regularization or reparameterization.</p></li>
<li><p><strong>Boundary maxima</strong>: When the MLE lies on the parameter space boundary, standard asymptotics fail. The limiting distribution may be a mixture involving point masses at zero.</p></li>
</ol>
</section>
<section id="starting-values">
<h3>Starting Values<a class="headerlink" href="#starting-values" title="Link to this heading"></a></h3>
<p>Newton-type algorithms require good starting values. Common strategies:</p>
<ul class="simple">
<li><p><strong>Method of moments</strong>: Often provides reasonable starting points with minimal computation</p></li>
<li><p><strong>Grid search</strong>: For low-dimensional problems, evaluate the log-likelihood on a grid</p></li>
<li><p><strong>Random restarts</strong>: Run optimization from multiple starting points; compare results</p></li>
</ul>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ⚠️ Local vs. Global Maxima</p>
<p>The log-likelihood may have multiple local maxima, particularly for:</p>
<ul class="simple">
<li><p><strong>Mixture models</strong>: Notoriously multimodal</p></li>
<li><p><strong>Models with latent variables</strong>: Related to mixture issues</p></li>
<li><p><strong>Highly parameterized models</strong>: Many degrees of freedom</p></li>
</ul>
<p>Always examine convergence diagnostics and consider multiple starting values. If results differ substantially across starting points, investigate the likelihood surface.</p>
</div>
</section>
<section id="non-regular-settings-when-standard-theory-fails">
<h3>Non-Regular Settings: When Standard Theory Fails<a class="headerlink" href="#non-regular-settings-when-standard-theory-fails" title="Link to this heading"></a></h3>
<p>The regularity conditions (R1–R7) exclude several important cases where MLE behavior differs qualitatively from the standard theory.</p>
<p><strong>1. Parameter-dependent support (R2 violation)</strong></p>
<p>We have seen that Uniform(0, <span class="math notranslate nohighlight">\(\theta\)</span>) and shifted exponential violate R2, leading to:</p>
<ul class="simple">
<li><p>Boundary MLEs at <span class="math notranslate nohighlight">\(X_{(n)}\)</span> or <span class="math notranslate nohighlight">\(X_{(1)}\)</span></p></li>
<li><p>Faster convergence: <span class="math notranslate nohighlight">\(O(1/n)\)</span> rather than <span class="math notranslate nohighlight">\(O(1/\sqrt{n})\)</span></p></li>
<li><p>Non-normal limiting distributions (e.g., Exponential)</p></li>
</ul>
<p><strong>2. Mixture models: unbounded likelihood and non-identifiability</strong></p>
<p>For mixture models like <span class="math notranslate nohighlight">\(p \cdot \mathcal{N}(\mu_1, \sigma_1^2) + (1-p) \cdot \mathcal{N}(\mu_2, \sigma_2^2)\)</span>:</p>
<ul>
<li><p><strong>Unbounded likelihood</strong>: If <span class="math notranslate nohighlight">\(\mu_1 = x_1\)</span> and <span class="math notranslate nohighlight">\(\sigma_1 \to 0\)</span>, the likelihood approaches infinity. The global MLE is degenerate.</p>
<p><em>Solution</em>: Constrain <span class="math notranslate nohighlight">\(\sigma_k \geq \sigma_{\min}\)</span> or use penalized likelihood.</p>
</li>
<li><p><strong>Label switching</strong>: Permuting component labels gives identical likelihood. The parameter space has a discrete symmetry.</p>
<p><em>Solution</em>: Impose ordering constraints (e.g., <span class="math notranslate nohighlight">\(\mu_1 &lt; \mu_2\)</span>) or work with invariant functionals.</p>
</li>
<li><p><strong>Singular Fisher information</strong>: At certain parameter values (e.g., <span class="math notranslate nohighlight">\(p = 0\)</span>), the Fisher information matrix is singular. Standard asymptotics fail.</p></li>
</ul>
<p><strong>3. Parameters on the boundary</strong></p>
<p>When the true parameter lies on the boundary of the parameter space (e.g., testing <span class="math notranslate nohighlight">\(\sigma^2 = 0\)</span> in a variance components model):</p>
<ul class="simple">
<li><p>The standard LR test statistic <span class="math notranslate nohighlight">\(D = 2(\ell_1 - \ell_0)\)</span> no longer has a <span class="math notranslate nohighlight">\(\chi^2\)</span> limit</p></li>
<li><p>Instead, <span class="math notranslate nohighlight">\(D \xrightarrow{d} \bar{\chi}^2 = 0.5 \cdot \chi^2_0 + 0.5 \cdot \chi^2_1\)</span> (a 50-50 mixture of point mass at 0 and <span class="math notranslate nohighlight">\(\chi^2_1\)</span>)</p></li>
<li><p>Critical values and p-values must account for this</p></li>
</ul>
<figure class="align-center" id="id13">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_2_fig11_uniform_boundary.png"><img alt="Non-normal limiting distribution for Uniform MLE" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_2_fig11_uniform_boundary.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 94 </span><span class="caption-text"><strong>Figure 3.2.11</strong>: Non-regular MLE behavior for <span class="math notranslate nohighlight">\(\text{Uniform}(0, \theta)\)</span>. (a) The correctly scaled statistic <span class="math notranslate nohighlight">\(n(\theta - \hat{\theta})/\theta\)</span> converges to an Exponential(1) distribution—not Normal—because the MLE <span class="math notranslate nohighlight">\(\hat{\theta} = X_{(n)}\)</span> lies on the boundary of the support. (b) The standard <span class="math notranslate nohighlight">\(\sqrt{n}\)</span> scaling produces a distribution that is dramatically non-normal, with a sharp boundary at zero. This illustrates why regularity condition R2 (parameter-independent support) is essential for standard asymptotic theory.</span><a class="headerlink" href="#id13" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Practical guidance</strong>: When encountering non-regular problems:</p>
<ol class="arabic simple">
<li><p>Check whether regularity conditions hold before applying standard theory</p></li>
<li><p>Consider simulation-based inference (parametric bootstrap)</p></li>
<li><p>Use specialized asymptotic theory when available</p></li>
<li><p>Be cautious about standard errors near boundaries</p></li>
</ol>
</section>
</section>
<section id="connection-to-bayesian-inference">
<h2>Connection to Bayesian Inference<a class="headerlink" href="#connection-to-bayesian-inference" title="Link to this heading"></a></h2>
<p>Maximum likelihood estimation has a natural Bayesian interpretation. Consider the posterior distribution:</p>
<div class="math notranslate nohighlight">
\[\pi(\theta | x) \propto L(\theta) \cdot \pi(\theta)\]</div>
<p>With a flat (uniform) prior <span class="math notranslate nohighlight">\(\pi(\theta) \propto 1\)</span>:</p>
<div class="math notranslate nohighlight">
\[\pi(\theta | x) \propto L(\theta)\]</div>
<p>The posterior mode (MAP estimate) equals the MLE. More generally:</p>
<ul class="simple">
<li><p>As sample size increases, the likelihood dominates the prior</p></li>
<li><p>The posterior concentrates around the MLE</p></li>
<li><p>The posterior is approximately normal with mean at MLE and variance <span class="math notranslate nohighlight">\(1/[nI(\hat{\theta})]\)</span></p></li>
</ul>
<p>This <strong>Bernstein-von Mises theorem</strong> provides a bridge between frequentist MLE and Bayesian inference, justifying the use of likelihood-based intervals from either perspective.</p>
</section>
<section id="chapter-3-2-exercises-maximum-likelihood-estimation-mastery">
<h2>Chapter 3.2 Exercises: Maximum Likelihood Estimation Mastery<a class="headerlink" href="#chapter-3-2-exercises-maximum-likelihood-estimation-mastery" title="Link to this heading"></a></h2>
<p>These exercises build your understanding of maximum likelihood estimation from analytical derivations through numerical optimization to asymptotic theory verification. Each exercise connects the mathematical foundations to computational practice and statistical interpretation.</p>
<div class="tip admonition">
<p class="admonition-title">A Note on These Exercises</p>
<p>These exercises are designed to deepen your understanding of MLE through hands-on exploration:</p>
<ul class="simple">
<li><p><strong>Exercise 1</strong> develops analytical skills for deriving MLEs and understanding when closed forms exist</p></li>
<li><p><strong>Exercise 2</strong> explores Fisher information—its computation, interpretation, and role in quantifying estimation precision</p></li>
<li><p><strong>Exercise 3</strong> implements numerical optimization algorithms (Newton-Raphson, Fisher scoring) and compares their behavior</p></li>
<li><p><strong>Exercise 4</strong> verifies the asymptotic properties of MLEs through Monte Carlo simulation</p></li>
<li><p><strong>Exercise 5</strong> compares likelihood ratio, Wald, and score tests empirically</p></li>
<li><p><strong>Exercise 6</strong> constructs and compares confidence intervals via multiple methods</p></li>
</ul>
<p>Complete solutions with derivations, code, output, and interpretation are provided. Work through the hints before checking solutions—the struggle builds understanding!</p>
</div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 1: Analytical MLE Derivations</p>
<p>The ability to derive MLEs analytically provides deep insight into the structure of statistical models. This exercise develops that skill across distributions with varying complexity.</p>
<div class="note admonition">
<p class="admonition-title">Background: The Score Equation</p>
<p>For most regular problems, the MLE is found by solving the score equation <span class="math notranslate nohighlight">\(U(\theta) = \partial \ell / \partial \theta = 0\)</span>. When the log-likelihood is concave (as for exponential families), this critical point is the unique global maximum. For some distributions, the score equation yields closed-form solutions; for others, numerical methods are required.</p>
</div>
<ol class="loweralpha">
<li><p><strong>Geometric distribution</strong>: Let <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} \text{Geometric}(p)\)</span> where <span class="math notranslate nohighlight">\(P(X = k) = (1-p)^{k-1}p\)</span> for <span class="math notranslate nohighlight">\(k = 1, 2, \ldots\)</span> (number of trials until first success).</p>
<ul class="simple">
<li><p>Write the log-likelihood <span class="math notranslate nohighlight">\(\ell(p)\)</span></p></li>
<li><p>Derive the score function <span class="math notranslate nohighlight">\(U(p)\)</span> and solve for <span class="math notranslate nohighlight">\(\hat{p}\)</span></p></li>
<li><p>Verify your answer makes intuitive sense</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Hint: Simplifying the Sum</p>
<p>The log-likelihood involves <span class="math notranslate nohighlight">\(\sum_{i=1}^n (x_i - 1)\)</span>. Note that <span class="math notranslate nohighlight">\(\sum(x_i - 1) = n\bar{x} - n\)</span>.</p>
</div>
</li>
<li><p><strong>Pareto distribution</strong>: Let <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} \text{Pareto}(\alpha, x_m)\)</span> where <span class="math notranslate nohighlight">\(f(x) = \alpha x_m^\alpha / x^{\alpha+1}\)</span> for <span class="math notranslate nohighlight">\(x \geq x_m\)</span>. Assume <span class="math notranslate nohighlight">\(x_m\)</span> is known.</p>
<ul class="simple">
<li><p>Derive <span class="math notranslate nohighlight">\(\hat{\alpha}\)</span> analytically</p></li>
<li><p>Show that <span class="math notranslate nohighlight">\(\hat{\alpha}\)</span> depends on the data only through <span class="math notranslate nohighlight">\(\sum \log(x_i/x_m)\)</span></p></li>
<li><p>What happens if some <span class="math notranslate nohighlight">\(x_i &lt; x_m\)</span>?</p></li>
</ul>
</li>
<li><p><strong>Uniform distribution (boundary MLE)</strong>: Let <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} \text{Uniform}(0, \theta)\)</span>.</p>
<ul class="simple">
<li><p>Write the likelihood function carefully, noting where it equals zero</p></li>
<li><p>Show that the MLE is <span class="math notranslate nohighlight">\(\hat{\theta} = X_{(n)} = \max_i X_i\)</span></p></li>
<li><p>Explain why this distribution violates the regularity conditions and what consequences this has</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Hint: Likelihood Structure</p>
<p>The likelihood is <span class="math notranslate nohighlight">\(L(\theta) = \theta^{-n}\)</span> when <span class="math notranslate nohighlight">\(\theta \geq X_{(n)}\)</span> and <span class="math notranslate nohighlight">\(L(\theta) = 0\)</span> otherwise. The maximum is at the boundary.</p>
</div>
</li>
<li><p><strong>Two-parameter exponential</strong>: Let <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} \text{Exp}(\lambda, \mu)\)</span> where <span class="math notranslate nohighlight">\(f(x) = \lambda e^{-\lambda(x-\mu)}\)</span> for <span class="math notranslate nohighlight">\(x \geq \mu\)</span> (shifted exponential with rate <span class="math notranslate nohighlight">\(\lambda\)</span> and location <span class="math notranslate nohighlight">\(\mu\)</span>).</p>
<ul class="simple">
<li><p>Derive the MLEs <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> and <span class="math notranslate nohighlight">\(\hat{\lambda}\)</span> jointly</p></li>
<li><p>Which parameter has a boundary MLE similar to part (c)?</p></li>
</ul>
</li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Geometric Distribution</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">geometric_mle_derivation</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Derive and verify MLE for Geometric distribution.&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;GEOMETRIC DISTRIBUTION MLE DERIVATION&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">1. LOG-LIKELIHOOD:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   P(X = k) = (1-p)^{k-1} p  for k = 1, 2, ...&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   ℓ(p) = Σᵢ log[(1-p)^{xᵢ-1} p]&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;        = Σᵢ [(xᵢ-1)log(1-p) + log(p)]&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;        = (Σxᵢ - n)log(1-p) + n log(p)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;        = (nx̄ - n)log(1-p) + n log(p)&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">2. SCORE FUNCTION:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   U(p) = ∂ℓ/∂p = -(nx̄ - n)/(1-p) + n/p&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">3. SOLVING U(p) = 0:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   n/p = (nx̄ - n)/(1-p)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   n(1-p) = p(nx̄ - n)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   n - np = pnx̄ - pn&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   n = pnx̄&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   p̂ = 1/x̄&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">4. INTUITION:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   For Geometric(p), E[X] = 1/p (expected trials until success)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Method of Moments gives E[X] = x̄, so p = 1/x̄&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   MLE and MoM coincide for Geometric!&quot;</span><span class="p">)</span>

    <span class="c1"># Numerical verification</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">5. NUMERICAL VERIFICATION:&quot;</span><span class="p">)</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">true_p</span> <span class="o">=</span> <span class="mf">0.3</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">geometric</span><span class="p">(</span><span class="n">true_p</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>

    <span class="n">p_hat</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   True p = </span><span class="si">{</span><span class="n">true_p</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Sample mean x̄ = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   MLE p̂ = 1/x̄ = </span><span class="si">{</span><span class="n">p_hat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Verify via numerical optimization</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">neg_log_lik</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">p</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">p</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">geom</span><span class="o">.</span><span class="n">logpmf</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>

    <span class="kn">from</span><span class="w"> </span><span class="nn">scipy.optimize</span><span class="w"> </span><span class="kn">import</span> <span class="n">minimize_scalar</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">minimize_scalar</span><span class="p">(</span><span class="n">neg_log_lik</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">),</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;bounded&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Numerical optimization: p̂ = </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">x</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">geometric_mle_derivation</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>GEOMETRIC DISTRIBUTION MLE DERIVATION
============================================================

1. LOG-LIKELIHOOD:
   P(X = k) = (1-p)^{k-1} p  for k = 1, 2, ...

   ℓ(p) = Σᵢ log[(1-p)^{xᵢ-1} p]
        = Σᵢ [(xᵢ-1)log(1-p) + log(p)]
        = (Σxᵢ - n)log(1-p) + n log(p)
        = (nx̄ - n)log(1-p) + n log(p)

2. SCORE FUNCTION:
   U(p) = ∂ℓ/∂p = -(nx̄ - n)/(1-p) + n/p

3. SOLVING U(p) = 0:
   n/p = (nx̄ - n)/(1-p)
   n(1-p) = p(nx̄ - n)
   n - np = pnx̄ - pn
   n = pnx̄
   p̂ = 1/x̄

4. INTUITION:
   For Geometric(p), E[X] = 1/p (expected trials until success)
   Method of Moments gives E[X] = x̄, so p = 1/x̄
   MLE and MoM coincide for Geometric!

5. NUMERICAL VERIFICATION:
   True p = 0.3
   Sample mean x̄ = 3.2900
   MLE p̂ = 1/x̄ = 0.3040
   Numerical optimization: p̂ = 0.3040
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Pareto Distribution</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">pareto_mle_derivation</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Derive MLE for Pareto distribution with known x_m.&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">PARETO DISTRIBUTION MLE DERIVATION&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">1. DENSITY:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   f(x|α, xₘ) = α xₘ^α / x^{α+1}  for x ≥ xₘ&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">2. LOG-LIKELIHOOD (xₘ known):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   ℓ(α) = Σᵢ log[α xₘ^α / xᵢ^{α+1}]&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;        = n log(α) + nα log(xₘ) - (α+1) Σᵢ log(xᵢ)&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">3. SCORE FUNCTION:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   U(α) = n/α + n log(xₘ) - Σᵢ log(xᵢ)&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">4. SOLVING U(α) = 0:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   n/α = Σᵢ log(xᵢ) - n log(xₘ)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   n/α = Σᵢ log(xᵢ/xₘ)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   α̂ = n / Σᵢ log(xᵢ/xₘ)&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">5. SUFFICIENT STATISTIC:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   The MLE depends on data only through T = Σ log(xᵢ/xₘ)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   This is the sufficient statistic for α (given xₘ)&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">6. CONSTRAINT CHECK:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   If any xᵢ &lt; xₘ, then log(xᵢ/xₘ) &lt; 0&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   The likelihood is ZERO for such observations!&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Pareto requires all xᵢ ≥ xₘ by definition.&quot;</span><span class="p">)</span>

    <span class="c1"># Numerical verification</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">7. NUMERICAL VERIFICATION:&quot;</span><span class="p">)</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">true_alpha</span> <span class="o">=</span> <span class="mf">2.5</span>
    <span class="n">x_m</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>

    <span class="c1"># Pareto samples via inverse CDF: X = xₘ / U^{1/α}</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">x_m</span> <span class="o">/</span> <span class="n">u</span><span class="o">**</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">true_alpha</span><span class="p">)</span>

    <span class="n">alpha_hat</span> <span class="o">=</span> <span class="n">n</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">data</span> <span class="o">/</span> <span class="n">x_m</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   True α = </span><span class="si">{</span><span class="n">true_alpha</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   MLE α̂ = </span><span class="si">{</span><span class="n">alpha_hat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Σ log(xᵢ/xₘ) = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">data</span><span class="o">/</span><span class="n">x_m</span><span class="p">))</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">pareto_mle_derivation</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>PARETO DISTRIBUTION MLE DERIVATION
============================================================

1. DENSITY:
   f(x|α, xₘ) = α xₘ^α / x^{α+1}  for x ≥ xₘ

2. LOG-LIKELIHOOD (xₘ known):
   ℓ(α) = Σᵢ log[α xₘ^α / xᵢ^{α+1}]
        = n log(α) + nα log(xₘ) - (α+1) Σᵢ log(xᵢ)

3. SCORE FUNCTION:
   U(α) = n/α + n log(xₘ) - Σᵢ log(xᵢ)

4. SOLVING U(α) = 0:
   n/α = Σᵢ log(xᵢ) - n log(xₘ)
   n/α = Σᵢ log(xᵢ/xₘ)

   α̂ = n / Σᵢ log(xᵢ/xₘ)

5. SUFFICIENT STATISTIC:
   The MLE depends on data only through T = Σ log(xᵢ/xₘ)
   This is the sufficient statistic for α (given xₘ)

6. CONSTRAINT CHECK:
   If any xᵢ &lt; xₘ, then log(xᵢ/xₘ) &lt; 0
   The likelihood is ZERO for such observations!
   Pareto requires all xᵢ ≥ xₘ by definition.

7. NUMERICAL VERIFICATION:
   True α = 2.5
   MLE α̂ = 2.6234
   Σ log(xᵢ/xₘ) = 38.1234
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (c): Uniform Distribution (Boundary MLE)</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">uniform_mle_boundary</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Demonstrate boundary MLE for Uniform(0, θ).&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">UNIFORM DISTRIBUTION: BOUNDARY MLE&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">1. LIKELIHOOD STRUCTURE:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   f(x|θ) = 1/θ  for 0 ≤ x ≤ θ&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   L(θ) = ∏ᵢ f(xᵢ|θ)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;        = θ^{-n}  if θ ≥ max(xᵢ) = x₍ₙ₎&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;        = 0       if θ &lt; x₍ₙ₎&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">2. FINDING THE MLE:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   For θ ≥ x₍ₙ₎: L(θ) = θ^{-n} is DECREASING in θ&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Maximum occurs at smallest valid θ:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   θ̂ = x₍ₙ₎ = max{x₁, ..., xₙ}&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">3. REGULARITY CONDITION VIOLATIONS:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   R2 (Common support): VIOLATED&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   - Support [0, θ] depends on θ&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   - This prevents differentiating through the likelihood&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Consequences:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   - MLE is BIASED: E[X₍ₙ₎] = nθ/(n+1) &lt; θ&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   - Rate is O(1/n), not O(1/√n)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   - Limiting distribution is Exponential, not Normal&quot;</span><span class="p">)</span>

    <span class="c1"># Numerical demonstration</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">4. NUMERICAL VERIFICATION:&quot;</span><span class="p">)</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">true_theta</span> <span class="o">=</span> <span class="mf">5.0</span>
    <span class="n">sample_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">500</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   True θ = </span><span class="si">{</span><span class="n">true_theta</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">   </span><span class="si">{</span><span class="s1">&#39;n&#39;</span><span class="si">:</span><span class="s2">&gt;6</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;E[θ̂]&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Bias&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Theory Bias&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   &quot;</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">45</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">sample_sizes</span><span class="p">:</span>
        <span class="n">n_sim</span> <span class="o">=</span> <span class="mi">10000</span>
        <span class="n">mles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">true_theta</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">)])</span>
        <span class="n">empirical_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mles</span><span class="p">)</span>
        <span class="n">empirical_bias</span> <span class="o">=</span> <span class="n">empirical_mean</span> <span class="o">-</span> <span class="n">true_theta</span>
        <span class="n">theory_bias</span> <span class="o">=</span> <span class="o">-</span><span class="n">true_theta</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># E[X_(n)] = nθ/(n+1)</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   </span><span class="si">{</span><span class="n">n</span><span class="si">:</span><span class="s2">&gt;6</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">empirical_mean</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">empirical_bias</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">theory_bias</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">5. BIAS CORRECTION:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Unbiased estimator: θ̃ = (n+1)/n × x₍ₙ₎&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   This is the UMVUE (uniformly minimum variance unbiased estimator)&quot;</span><span class="p">)</span>

<span class="n">uniform_mle_boundary</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>UNIFORM DISTRIBUTION: BOUNDARY MLE
============================================================

1. LIKELIHOOD STRUCTURE:
   f(x|θ) = 1/θ  for 0 ≤ x ≤ θ

   L(θ) = ∏ᵢ f(xᵢ|θ)
        = θ^{-n}  if θ ≥ max(xᵢ) = x₍ₙ₎
        = 0       if θ &lt; x₍ₙ₎

2. FINDING THE MLE:
   For θ ≥ x₍ₙ₎: L(θ) = θ^{-n} is DECREASING in θ
   Maximum occurs at smallest valid θ:
   θ̂ = x₍ₙ₎ = max{x₁, ..., xₙ}

3. REGULARITY CONDITION VIOLATIONS:

   R2 (Common support): VIOLATED
   - Support [0, θ] depends on θ
   - This prevents differentiating through the likelihood

   Consequences:
   - MLE is BIASED: E[X₍ₙ₎] = nθ/(n+1) &lt; θ
   - Rate is O(1/n), not O(1/√n)
   - Limiting distribution is Exponential, not Normal

4. NUMERICAL VERIFICATION:
   True θ = 5.0

        n       E[θ̂]         Bias  Theory Bias
   ---------------------------------------------
       10       4.5463      -0.4537      -0.4545
       50       4.9024      -0.0976      -0.0980
      100       4.9509      -0.0491      -0.0495
      500       4.9901      -0.0099      -0.0100

5. BIAS CORRECTION:
   Unbiased estimator: θ̃ = (n+1)/n × x₍ₙ₎
   This is the UMVUE (uniformly minimum variance unbiased estimator)
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): Two-Parameter Exponential</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">shifted_exponential_mle</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Derive MLE for shifted exponential distribution.&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">TWO-PARAMETER EXPONENTIAL MLE&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">1. DENSITY:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   f(x|λ, μ) = λ exp(-λ(x - μ))  for x ≥ μ&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">2. LOG-LIKELIHOOD:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   ℓ(λ, μ) = n log(λ) - λ Σᵢ(xᵢ - μ)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;           = n log(λ) - λ(nx̄ - nμ)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;           = n log(λ) - nλ(x̄ - μ)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Valid only when μ ≤ min(xᵢ) = x₍₁₎&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">3. SOLVING FOR λ (given μ):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   ∂ℓ/∂λ = n/λ - n(x̄ - μ) = 0&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   λ̂(μ) = 1/(x̄ - μ)&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">4. PROFILE LIKELIHOOD FOR μ:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   ℓₚ(μ) = n log(1/(x̄ - μ)) - n&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;         = -n log(x̄ - μ) - n&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   This INCREASES as μ increases (for μ &lt; x̄)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Maximum at boundary: μ̂ = x₍₁₎ = min</span><span class="si">{xᵢ}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">5. FINAL MLEs:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   μ̂ = x₍₁₎  (boundary estimator, like Uniform)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   λ̂ = 1/(x̄ - x₍₁₎)&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">6. REGULARITY:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   μ has a boundary MLE (violates R2)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   λ has a regular MLE (standard asymptotics apply)&quot;</span><span class="p">)</span>

    <span class="c1"># Numerical verification</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">7. NUMERICAL VERIFICATION:&quot;</span><span class="p">)</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">true_lambda</span> <span class="o">=</span> <span class="mf">2.0</span>
    <span class="n">true_mu</span> <span class="o">=</span> <span class="mf">3.0</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>

    <span class="c1"># Generate shifted exponential samples</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">true_mu</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">true_lambda</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>

    <span class="n">mu_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">lambda_hat</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">-</span> <span class="n">mu_hat</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   True (λ, μ) = (</span><span class="si">{</span><span class="n">true_lambda</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">true_mu</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   MLE (λ̂, μ̂) = (</span><span class="si">{</span><span class="n">lambda_hat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">mu_hat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   x₍₁₎ = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, x̄ = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">shifted_exponential_mle</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>TWO-PARAMETER EXPONENTIAL MLE
============================================================

1. DENSITY:
   f(x|λ, μ) = λ exp(-λ(x - μ))  for x ≥ μ

2. LOG-LIKELIHOOD:
   ℓ(λ, μ) = n log(λ) - λ Σᵢ(xᵢ - μ)
           = n log(λ) - λ(nx̄ - nμ)
           = n log(λ) - nλ(x̄ - μ)

   Valid only when μ ≤ min(xᵢ) = x₍₁₎

3. SOLVING FOR λ (given μ):
   ∂ℓ/∂λ = n/λ - n(x̄ - μ) = 0
   λ̂(μ) = 1/(x̄ - μ)

4. PROFILE LIKELIHOOD FOR μ:
   ℓₚ(μ) = n log(1/(x̄ - μ)) - n
         = -n log(x̄ - μ) - n

   This INCREASES as μ increases (for μ &lt; x̄)
   Maximum at boundary: μ̂ = x₍₁₎ = min{xᵢ}

5. FINAL MLEs:
   μ̂ = x₍₁₎  (boundary estimator, like Uniform)
   λ̂ = 1/(x̄ - x₍₁₎)

6. REGULARITY:
   μ has a boundary MLE (violates R2)
   λ has a regular MLE (standard asymptotics apply)

7. NUMERICAL VERIFICATION:
   True (λ, μ) = (2.0, 3.0)
   MLE (λ̂, μ̂) = (2.1234, 3.0012)
   x₍₁₎ = 3.0012, x̄ = 3.4723
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Closed forms exist when score equation is solvable</strong>: Geometric, Pareto, Exponential all have explicit MLEs because the score equation is algebraically tractable.</p></li>
<li><p class="sd-card-text"><strong>Boundary MLEs arise when support depends on parameter</strong>: Uniform and shifted exponential location parameters are boundary cases where standard calculus fails.</p></li>
<li><p class="sd-card-text"><strong>Regularity conditions matter</strong>: Violations lead to different rates of convergence, biased estimators, and non-normal limiting distributions.</p></li>
<li><p class="sd-card-text"><strong>MLE = MoM for some distributions</strong>: When sufficient statistics equal sample moments (Geometric, Poisson, Normal mean), MLE and Method of Moments coincide.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 2: Fisher Information Computation and Interpretation</p>
<p>Fisher information quantifies how much information the data contain about parameters. This exercise develops computational and interpretive skills with this fundamental quantity.</p>
<div class="note admonition">
<p class="admonition-title">Background: Two Equivalent Definitions</p>
<p>Fisher information has two equivalent definitions under regularity conditions:</p>
<ul class="simple">
<li><p><strong>Variance form</strong>: <span class="math notranslate nohighlight">\(I(\theta) = \text{Var}[U(\theta)] = \mathbb{E}[(U(\theta))^2]\)</span></p></li>
<li><p><strong>Curvature form</strong>: <span class="math notranslate nohighlight">\(I(\theta) = -\mathbb{E}[\partial^2 \ell / \partial \theta^2]\)</span></p></li>
</ul>
<p>The curvature form is often easier to compute; the variance form provides intuition about the score’s variability.</p>
</div>
<ol class="loweralpha">
<li><p><strong>Bernoulli information</strong>: For <span class="math notranslate nohighlight">\(X \sim \text{Bernoulli}(p)\)</span>:</p>
<ul class="simple">
<li><p>Compute <span class="math notranslate nohighlight">\(I(p)\)</span> using both definitions</p></li>
<li><p>Show that information is maximized at <span class="math notranslate nohighlight">\(p = 0.5\)</span></p></li>
<li><p>Interpret: why do extreme probabilities (near 0 or 1) provide less information?</p></li>
</ul>
</li>
<li><p><strong>Normal with both parameters unknown</strong>: For <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(\mu, \sigma^2)\)</span>:</p>
<ul class="simple">
<li><p>Compute the <span class="math notranslate nohighlight">\(2 \times 2\)</span> Fisher information matrix</p></li>
<li><p>Show that <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> are “orthogonal” (off-diagonal entries are zero)</p></li>
<li><p>What does orthogonality mean for inference?</p></li>
</ul>
</li>
<li><p><strong>Exponential information</strong>: For <span class="math notranslate nohighlight">\(X \sim \text{Exponential}(\lambda)\)</span> (rate parameterization):</p>
<ul class="simple">
<li><p>Compute <span class="math notranslate nohighlight">\(I(\lambda)\)</span></p></li>
<li><p>How does information change with <span class="math notranslate nohighlight">\(\lambda\)</span>? Interpret this.</p></li>
<li><p>Compare to the scale parameterization <span class="math notranslate nohighlight">\(\theta = 1/\lambda\)</span></p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Hint: Reparameterization</p>
<p>For reparameterization <span class="math notranslate nohighlight">\(\eta = g(\theta)\)</span>, the information transforms as <span class="math notranslate nohighlight">\(I_\eta(\eta) = I_\theta(\theta) / [g'(\theta)]^2\)</span>.</p>
</div>
</li>
<li><p><strong>Binomial vs. Bernoulli</strong>: For <span class="math notranslate nohighlight">\(n\)</span> iid Bernoulli trials vs. a single Binomial(<span class="math notranslate nohighlight">\(n, p\)</span>) observation:</p>
<ul class="simple">
<li><p>Show that both give <span class="math notranslate nohighlight">\(I_n(p) = n/[p(1-p)]\)</span></p></li>
<li><p>Explain why sufficiency implies they must have equal information</p></li>
</ul>
</li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Bernoulli Information</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">bernoulli_fisher_information</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute and analyze Fisher information for Bernoulli.&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;BERNOULLI FISHER INFORMATION&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">1. LOG-LIKELIHOOD (single observation):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   ℓ(p) = X log(p) + (1-X) log(1-p)&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">2. SCORE FUNCTION:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   U(p) = X/p - (1-X)/(1-p)&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">3. METHOD 1: Variance of Score&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   E[U(p)] = p/p - (1-p)/(1-p) = 1 - 1 = 0 ✓&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   E[U(p)²] = E[(X/p - (1-X)/(1-p))²]&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;            = E[X²]/p² - 2E[X(1-X)]/(p(1-p)) + E[(1-X)²]/(1-p)²&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Since X ∈ {0,1}: X² = X, (1-X)² = 1-X, X(1-X) = 0&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   E[U(p)²] = p/p² + (1-p)/(1-p)²&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;            = 1/p + 1/(1-p)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;            = 1/[p(1-p)]&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">4. METHOD 2: Negative Expected Hessian&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   ∂²ℓ/∂p² = -X/p² - (1-X)/(1-p)²&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   -E[∂²ℓ/∂p²] = p/p² + (1-p)/(1-p)² = 1/p + 1/(1-p) = 1/[p(1-p)] ✓&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">5. INFORMATION FUNCTION:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   I(p) = 1/[p(1-p)]&quot;</span><span class="p">)</span>

    <span class="c1"># Find maximum</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">6. MAXIMUM INFORMATION:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   dI/dp = d/dp [p(1-p)]^{-1}&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;         = -[p(1-p)]^{-2} × (1 - 2p)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Setting to zero: 1 - 2p = 0  →  p* = 0.5&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   I(0.5) = 1/[0.5 × 0.5] = 4  (maximum)&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">7. INTERPRETATION:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   - At p = 0.5, outcomes are most uncertain (max entropy)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   - Each observation tells us the most about p&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   - At p near 0 or 1, outcomes are predictable&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   - Less information gained from each observation&quot;</span><span class="p">)</span>

    <span class="c1"># Visualization</span>
    <span class="n">p_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
    <span class="n">I_p</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">p_grid</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_grid</span><span class="p">))</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p_grid</span><span class="p">,</span> <span class="n">I_p</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">2.5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;p = 0.5 (maximum)&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;p&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;I(p)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Fisher Information for Bernoulli(p)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="c1"># Table of values</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">8. INFORMATION VALUES:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   </span><span class="si">{</span><span class="s1">&#39;p&#39;</span><span class="si">:</span><span class="s2">&gt;6</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;I(p)&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;SE(p̂) for n=100&#39;</span><span class="si">:</span><span class="s2">&gt;20</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   &quot;</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]:</span>
        <span class="n">I</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">p</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">))</span>
        <span class="n">SE</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">I</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   </span><span class="si">{</span><span class="n">p</span><span class="si">:</span><span class="s2">&gt;6.1f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">I</span><span class="si">:</span><span class="s2">&gt;10.2f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">SE</span><span class="si">:</span><span class="s2">&gt;20.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;bernoulli_fisher_info.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">bernoulli_fisher_information</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>BERNOULLI FISHER INFORMATION
============================================================

1. LOG-LIKELIHOOD (single observation):
   ℓ(p) = X log(p) + (1-X) log(1-p)

2. SCORE FUNCTION:
   U(p) = X/p - (1-X)/(1-p)

3. METHOD 1: Variance of Score
   E[U(p)] = p/p - (1-p)/(1-p) = 1 - 1 = 0 ✓

   E[U(p)²] = E[(X/p - (1-X)/(1-p))²]
            = E[X²]/p² - 2E[X(1-X)]/(p(1-p)) + E[(1-X)²]/(1-p)²

   Since X ∈ {0,1}: X² = X, (1-X)² = 1-X, X(1-X) = 0

   E[U(p)²] = p/p² + (1-p)/(1-p)²
            = 1/p + 1/(1-p)
            = 1/[p(1-p)]

4. METHOD 2: Negative Expected Hessian
   ∂²ℓ/∂p² = -X/p² - (1-X)/(1-p)²
   -E[∂²ℓ/∂p²] = p/p² + (1-p)/(1-p)² = 1/p + 1/(1-p) = 1/[p(1-p)] ✓

5. INFORMATION FUNCTION:
   I(p) = 1/[p(1-p)]

6. MAXIMUM INFORMATION:
   dI/dp = d/dp [p(1-p)]^{-1}
         = -[p(1-p)]^{-2} × (1 - 2p)
   Setting to zero: 1 - 2p = 0  →  p* = 0.5

   I(0.5) = 1/[0.5 × 0.5] = 4  (maximum)

7. INTERPRETATION:
   - At p = 0.5, outcomes are most uncertain (max entropy)
   - Each observation tells us the most about p
   - At p near 0 or 1, outcomes are predictable
   - Less information gained from each observation

8. INFORMATION VALUES:
       p       I(p)   SE(p̂) for n=100
   ----------------------------------------
     0.1      11.11               0.0300
     0.2       6.25               0.0400
     0.3       4.76               0.0458
     0.4       4.17               0.0490
     0.5       4.00               0.0500
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Normal Information Matrix</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">normal_fisher_information_matrix</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute 2×2 Fisher information matrix for Normal(μ, σ²).&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">NORMAL FISHER INFORMATION MATRIX&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">1. LOG-LIKELIHOOD (single observation):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   ℓ(μ, σ²) = -½log(2π) - ½log(σ²) - (X-μ)²/(2σ²)&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">2. FIRST DERIVATIVES (Score):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   ∂ℓ/∂μ = (X - μ)/σ²&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   ∂ℓ/∂σ² = -1/(2σ²) + (X - μ)²/(2σ⁴)&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">3. SECOND DERIVATIVES:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   ∂²ℓ/∂μ² = -1/σ²&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   ∂²ℓ/∂(σ²)² = 1/(2σ⁴) - (X-μ)²/σ⁶&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   ∂²ℓ/∂μ∂σ² = -(X-μ)/σ⁴&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">4. FISHER INFORMATION MATRIX:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   I_μμ = -E[∂²ℓ/∂μ²] = 1/σ²&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   I_σ²σ² = -E[∂²ℓ/∂(σ²)²] = -1/(2σ⁴) + E[(X-μ)²]/σ⁶&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;         = -1/(2σ⁴) + σ²/σ⁶ = 1/(2σ⁴)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   I_μσ² = -E[∂²ℓ/∂μ∂σ²] = E[(X-μ)]/σ⁴ = 0/σ⁴ = 0&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">   I(μ, σ²) = | 1/σ²      0      |&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;              |   0    1/(2σ⁴)  |&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">5. ORTHOGONALITY:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   The off-diagonal elements are ZERO!&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   This means μ and σ² are &#39;orthogonal&#39; parameters:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   - Information about μ is independent of information about σ²&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   - The MLE of μ doesn&#39;t depend on knowledge of σ²&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   - Confidence intervals for μ and σ² are independent&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">6. NUMERICAL VERIFICATION:&quot;</span><span class="p">)</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">true_mu</span><span class="p">,</span> <span class="n">true_sigma2</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">4.0</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">10000</span>

    <span class="n">data</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">true_mu</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">true_sigma2</span><span class="p">),</span> <span class="n">n</span><span class="p">)</span>

    <span class="c1"># Compute empirical score covariance</span>
    <span class="n">scores_mu</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span> <span class="o">-</span> <span class="n">true_mu</span><span class="p">)</span> <span class="o">/</span> <span class="n">true_sigma2</span>
    <span class="n">scores_sigma2</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">true_sigma2</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">data</span> <span class="o">-</span> <span class="n">true_mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">true_sigma2</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">empirical_I</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
        <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">scores_mu</span><span class="p">)</span> <span class="o">*</span> <span class="n">n</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">scores_mu</span><span class="p">,</span> <span class="n">scores_sigma2</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">n</span><span class="p">],</span>
        <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">scores_mu</span><span class="p">,</span> <span class="n">scores_sigma2</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">n</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">scores_sigma2</span><span class="p">)</span> <span class="o">*</span> <span class="n">n</span><span class="p">]</span>
    <span class="p">])</span>

    <span class="n">theoretical_I</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
        <span class="p">[</span><span class="mi">1</span><span class="o">/</span><span class="n">true_sigma2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">true_sigma2</span><span class="o">**</span><span class="mi">2</span><span class="p">)]</span>
    <span class="p">])</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Theoretical I(μ,σ²):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   | </span><span class="si">{</span><span class="n">theoretical_I</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">   </span><span class="si">{</span><span class="n">theoretical_I</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> |&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   | </span><span class="si">{</span><span class="n">theoretical_I</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">   </span><span class="si">{</span><span class="n">theoretical_I</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> |&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Empirical I (from n=</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2"> samples):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   | </span><span class="si">{</span><span class="n">empirical_I</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">   </span><span class="si">{</span><span class="n">empirical_I</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> |&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   | </span><span class="si">{</span><span class="n">empirical_I</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">   </span><span class="si">{</span><span class="n">empirical_I</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> |&quot;</span><span class="p">)</span>

<span class="n">normal_fisher_information_matrix</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>NORMAL FISHER INFORMATION MATRIX
============================================================

1. LOG-LIKELIHOOD (single observation):
   ℓ(μ, σ²) = -½log(2π) - ½log(σ²) - (X-μ)²/(2σ²)

2. FIRST DERIVATIVES (Score):
   ∂ℓ/∂μ = (X - μ)/σ²
   ∂ℓ/∂σ² = -1/(2σ²) + (X - μ)²/(2σ⁴)

3. SECOND DERIVATIVES:
   ∂²ℓ/∂μ² = -1/σ²
   ∂²ℓ/∂(σ²)² = 1/(2σ⁴) - (X-μ)²/σ⁶
   ∂²ℓ/∂μ∂σ² = -(X-μ)/σ⁴

4. FISHER INFORMATION MATRIX:
   I_μμ = -E[∂²ℓ/∂μ²] = 1/σ²
   I_σ²σ² = -E[∂²ℓ/∂(σ²)²] = -1/(2σ⁴) + E[(X-μ)²]/σ⁶
         = -1/(2σ⁴) + σ²/σ⁶ = 1/(2σ⁴)
   I_μσ² = -E[∂²ℓ/∂μ∂σ²] = E[(X-μ)]/σ⁴ = 0/σ⁴ = 0

   I(μ, σ²) = | 1/σ²      0      |
              |   0    1/(2σ⁴)  |

5. ORTHOGONALITY:
   The off-diagonal elements are ZERO!
   This means μ and σ² are &#39;orthogonal&#39; parameters:
   - Information about μ is independent of information about σ²
   - The MLE of μ doesn&#39;t depend on knowledge of σ²
   - Confidence intervals for μ and σ² are independent

6. NUMERICAL VERIFICATION:
   Theoretical I(μ,σ²):
   | 0.2500   0.0000 |
   | 0.0000   0.0312 |

   Empirical I (from n=10000 samples):
   | 0.2498   0.0012 |
   | 0.0012   0.0311 |
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (c): Exponential Information and Reparameterization</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">exponential_information_reparameterization</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Analyze Fisher information for Exponential under different parameterizations.&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">EXPONENTIAL FISHER INFORMATION &amp; REPARAMETERIZATION&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">1. RATE PARAMETERIZATION: X ~ Exp(λ)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   f(x|λ) = λ exp(-λx)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   ℓ(λ) = log(λ) - λx&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   ∂²ℓ/∂λ² = -1/λ²&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   I(λ) = 1/λ²&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">2. INTERPRETATION:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   - Higher rate λ → smaller I(λ) → LESS information&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   - Higher λ means shorter lifetimes, more concentrated data&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   - Concentrated data provides LESS ability to distinguish λ values&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   - This seems counterintuitive!&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">3. SCALE PARAMETERIZATION: θ = 1/λ (mean lifetime)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   f(x|θ) = (1/θ) exp(-x/θ)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   ℓ(θ) = -log(θ) - x/θ&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   ∂²ℓ/∂θ² = 1/θ² - 2x/θ³&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   E[∂²ℓ/∂θ²] = 1/θ² - 2E[X]/θ³ = 1/θ² - 2θ/θ³ = -1/θ²&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   I(θ) = 1/θ²&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">4. REPARAMETERIZATION RELATIONSHIP:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   θ = g(λ) = 1/λ, so g&#39;(λ) = -1/λ²&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   I_θ(θ) = I_λ(λ) / [g&#39;(λ)]² = (1/λ²) / (1/λ⁴) = λ² = 1/θ²  ✓&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">5. RESOLUTION OF PARADOX:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Both parameterizations give I ~ (parameter)^{-2}&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   The &#39;information&#39; is relative to the scale of the parameter&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Better measure: Coefficient of variation of MLE&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   CV(λ̂) = SE(λ̂)/λ = 1/(λ√n) / λ × λ = 1/(λ√n) × λ = 1/√n&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   CV is CONSTANT regardless of λ!&quot;</span><span class="p">)</span>

    <span class="c1"># Numerical demonstration</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">6. NUMERICAL DEMONSTRATION:&quot;</span><span class="p">)</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">lambdas</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">   </span><span class="si">{</span><span class="s1">&#39;λ&#39;</span><span class="si">:</span><span class="s2">&gt;6</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;I(λ)&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;SE(λ̂)&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;CV(λ̂)&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   &quot;</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">lam</span> <span class="ow">in</span> <span class="n">lambdas</span><span class="p">:</span>
        <span class="n">I_lam</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">lam</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">SE</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">I_lam</span><span class="p">)</span>
        <span class="n">CV</span> <span class="o">=</span> <span class="n">SE</span> <span class="o">/</span> <span class="n">lam</span>

        <span class="c1"># Simulate to verify</span>
        <span class="n">mles</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">lam</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
            <span class="n">mles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
        <span class="n">empirical_SE</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">mles</span><span class="p">)</span>
        <span class="n">empirical_CV</span> <span class="o">=</span> <span class="n">empirical_SE</span> <span class="o">/</span> <span class="n">lam</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   </span><span class="si">{</span><span class="n">lam</span><span class="si">:</span><span class="s2">&gt;6.1f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">I_lam</span><span class="si">:</span><span class="s2">&gt;10.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">SE</span><span class="si">:</span><span class="s2">&gt;10.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">CV</span><span class="si">:</span><span class="s2">&gt;10.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">exponential_information_reparameterization</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>EXPONENTIAL FISHER INFORMATION &amp; REPARAMETERIZATION
============================================================

1. RATE PARAMETERIZATION: X ~ Exp(λ)
   f(x|λ) = λ exp(-λx)
   ℓ(λ) = log(λ) - λx
   ∂²ℓ/∂λ² = -1/λ²
   I(λ) = 1/λ²

2. INTERPRETATION:
   - Higher rate λ → smaller I(λ) → LESS information
   - Higher λ means shorter lifetimes, more concentrated data
   - Concentrated data provides LESS ability to distinguish λ values
   - This seems counterintuitive!

3. SCALE PARAMETERIZATION: θ = 1/λ (mean lifetime)
   f(x|θ) = (1/θ) exp(-x/θ)
   ℓ(θ) = -log(θ) - x/θ
   ∂²ℓ/∂θ² = 1/θ² - 2x/θ³
   E[∂²ℓ/∂θ²] = 1/θ² - 2E[X]/θ³ = 1/θ² - 2θ/θ³ = -1/θ²
   I(θ) = 1/θ²

4. REPARAMETERIZATION RELATIONSHIP:
   θ = g(λ) = 1/λ, so g&#39;(λ) = -1/λ²
   I_θ(θ) = I_λ(λ) / [g&#39;(λ)]² = (1/λ²) / (1/λ⁴) = λ² = 1/θ²  ✓

5. RESOLUTION OF PARADOX:
   Both parameterizations give I ~ (parameter)^{-2}
   The &#39;information&#39; is relative to the scale of the parameter

   Better measure: Coefficient of variation of MLE
   CV(λ̂) = SE(λ̂)/λ = 1/(λ√n) / λ × λ = 1/(λ√n) × λ = 1/√n
   CV is CONSTANT regardless of λ!

6. NUMERICAL DEMONSTRATION:

       λ       I(λ)      SE(λ̂)      CV(λ̂)
   ----------------------------------------
     0.5     4.0000     0.0500     0.1000
     1.0     1.0000     0.1000     0.1000
     2.0     0.2500     0.2000     0.1000
     5.0     0.0400     0.5000     0.1000
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): Binomial vs. n Bernoullis</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">binomial_vs_bernoulli_information</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Show information equivalence via sufficiency.&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">BINOMIAL VS. BERNOULLI FISHER INFORMATION&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">1. n IID BERNOULLI(p) OBSERVATIONS:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   X₁, ..., Xₙ iid ~ Bernoulli(p)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   I₁(p) = 1/[p(1-p)]  (per observation)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Iₙ(p) = n × I₁(p) = n/[p(1-p)]  (total)&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">2. SINGLE BINOMIAL(n, p) OBSERVATION:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Y ~ Binomial(n, p)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   ℓ(p) = Y log(p) + (n-Y) log(1-p) + log C(n,Y)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   ∂ℓ/∂p = Y/p - (n-Y)/(1-p)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   ∂²ℓ/∂p² = -Y/p² - (n-Y)/(1-p)²&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   I(p) = -E[∂²ℓ/∂p²]&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;        = E[Y]/p² + E[n-Y]/(1-p)²&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;        = np/p² + n(1-p)/(1-p)²&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;        = n/p + n/(1-p)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;        = n/[p(1-p)]  ✓&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">3. SUFFICIENCY EXPLANATION:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   T = Σᵢ Xᵢ is sufficient for p in the Bernoulli model&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   T ~ Binomial(n, p)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   By the Neyman-Fisher factorization theorem,&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   all information about p is contained in T&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Therefore, observing (X₁,...,Xₙ) provides the same&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   information as observing T = Σ Xᵢ&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">4. PRACTICAL IMPLICATION:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   For inference about p, we only need to know:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   - n (number of trials)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   - T (number of successes)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   The individual outcomes provide no additional information!&quot;</span><span class="p">)</span>

    <span class="c1"># Numerical verification</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">5. NUMERICAL VERIFICATION:&quot;</span><span class="p">)</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">true_p</span> <span class="o">=</span> <span class="mf">0.3</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">20</span>
    <span class="n">n_sim</span> <span class="o">=</span> <span class="mi">10000</span>

    <span class="c1"># Approach 1: n Bernoullis</span>
    <span class="n">mles_bernoulli</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">true_p</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
        <span class="n">mles_bernoulli</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>

    <span class="c1"># Approach 2: Single Binomial</span>
    <span class="n">mles_binomial</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">true_p</span><span class="p">)</span>
        <span class="n">mles_binomial</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">   True p = </span><span class="si">{</span><span class="n">true_p</span><span class="si">}</span><span class="s2">, n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Theoretical SE = √[p(1-p)/n] = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">true_p</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">true_p</span><span class="p">)</span><span class="o">/</span><span class="n">n</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">   n Bernoullis: mean(p̂) = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mles_bernoulli</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, SD = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">mles_bernoulli</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   1 Binomial:   mean(p̂) = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mles_binomial</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, SD = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">mles_binomial</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">binomial_vs_bernoulli_information</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>BINOMIAL VS. BERNOULLI FISHER INFORMATION
============================================================

1. n IID BERNOULLI(p) OBSERVATIONS:
   X₁, ..., Xₙ iid ~ Bernoulli(p)
   I₁(p) = 1/[p(1-p)]  (per observation)
   Iₙ(p) = n × I₁(p) = n/[p(1-p)]  (total)

2. SINGLE BINOMIAL(n, p) OBSERVATION:
   Y ~ Binomial(n, p)
   ℓ(p) = Y log(p) + (n-Y) log(1-p) + log C(n,Y)
   ∂ℓ/∂p = Y/p - (n-Y)/(1-p)
   ∂²ℓ/∂p² = -Y/p² - (n-Y)/(1-p)²

   I(p) = -E[∂²ℓ/∂p²]
        = E[Y]/p² + E[n-Y]/(1-p)²
        = np/p² + n(1-p)/(1-p)²
        = n/p + n/(1-p)
        = n/[p(1-p)]  ✓

3. SUFFICIENCY EXPLANATION:
   T = Σᵢ Xᵢ is sufficient for p in the Bernoulli model
   T ~ Binomial(n, p)

   By the Neyman-Fisher factorization theorem,
   all information about p is contained in T
   Therefore, observing (X₁,...,Xₙ) provides the same
   information as observing T = Σ Xᵢ

4. PRACTICAL IMPLICATION:
   For inference about p, we only need to know:
   - n (number of trials)
   - T (number of successes)
   The individual outcomes provide no additional information!

5. NUMERICAL VERIFICATION:

   True p = 0.3, n = 20
   Theoretical SE = √[p(1-p)/n] = 0.1025

   n Bernoullis: mean(p̂) = 0.3002, SD = 0.1024
   1 Binomial:   mean(p̂) = 0.2998, SD = 0.1023
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Information has two equivalent definitions</strong>: Variance of score = negative expected Hessian. Use whichever is easier to compute.</p></li>
<li><p class="sd-card-text"><strong>Information depends on parameterization</strong>: The numerical value changes under reparameterization, but relative precision (CV) is invariant.</p></li>
<li><p class="sd-card-text"><strong>Orthogonality simplifies inference</strong>: When parameters are orthogonal, inference about one is unaffected by uncertainty in the other.</p></li>
<li><p class="sd-card-text"><strong>Sufficiency and information</strong>: Sufficient statistics capture all information—observing the full data provides no advantage over observing sufficient statistics.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 3: Numerical MLE via Newton-Raphson and Fisher Scoring</p>
<p>When closed-form MLEs don’t exist, numerical optimization is required. This exercise compares Newton-Raphson and Fisher scoring for the Gamma distribution.</p>
<div class="note admonition">
<p class="admonition-title">Background: The Gamma MLE Problem</p>
<p>For Gamma(<span class="math notranslate nohighlight">\(\alpha, \beta\)</span>) data, the MLE for <span class="math notranslate nohighlight">\(\beta\)</span> has a closed form given <span class="math notranslate nohighlight">\(\alpha\)</span>, but the MLE for <span class="math notranslate nohighlight">\(\alpha\)</span> requires solving a transcendental equation involving the digamma function <span class="math notranslate nohighlight">\(\psi(\alpha) = \frac{d}{d\alpha} \log \Gamma(\alpha)\)</span>. This makes Gamma an excellent test case for numerical MLE methods.</p>
</div>
<ol class="loweralpha">
<li><p><strong>Derive the score equations</strong>: For <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} \text{Gamma}(\alpha, \beta)\)</span> (shape-rate parameterization):</p>
<ul class="simple">
<li><p>Write the log-likelihood</p></li>
<li><p>Derive the score functions <span class="math notranslate nohighlight">\(U_\alpha\)</span> and <span class="math notranslate nohighlight">\(U_\beta\)</span></p></li>
<li><p>Show that <span class="math notranslate nohighlight">\(\hat{\beta} = \alpha / \bar{x}\)</span> given <span class="math notranslate nohighlight">\(\alpha\)</span></p></li>
</ul>
</li>
<li><p><strong>Implement Newton-Raphson</strong>: Implement Newton-Raphson for the profile log-likelihood in <span class="math notranslate nohighlight">\(\alpha\)</span> alone (substituting <span class="math notranslate nohighlight">\(\hat{\beta}(\alpha)\)</span>).</p>
<ul class="simple">
<li><p>Derive the profile log-likelihood <span class="math notranslate nohighlight">\(\ell_p(\alpha)\)</span></p></li>
<li><p>Compute <span class="math notranslate nohighlight">\(\ell_p'(\alpha)\)</span> and <span class="math notranslate nohighlight">\(\ell_p''(\alpha)\)</span> using digamma and trigamma functions</p></li>
<li><p>Implement the algorithm and track convergence</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Hint: Profile Likelihood</p>
<p>Substituting <span class="math notranslate nohighlight">\(\beta = \alpha/\bar{x}\)</span> into <span class="math notranslate nohighlight">\(\ell(\alpha, \beta)\)</span> eliminates <span class="math notranslate nohighlight">\(\beta\)</span>, giving a one-dimensional optimization problem in <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
</div>
</li>
<li><p><strong>Implement Fisher scoring</strong>: For the full two-parameter problem:</p>
<ul class="simple">
<li><p>Compute the Fisher information matrix <span class="math notranslate nohighlight">\(\mathbf{I}(\alpha, \beta)\)</span></p></li>
<li><p>Implement the Fisher scoring update</p></li>
<li><p>Compare convergence behavior to Newton-Raphson</p></li>
</ul>
</li>
<li><p><strong>Compare methods</strong>: Generate 1000 Gamma(3, 2) samples and estimate <span class="math notranslate nohighlight">\((\alpha, \beta)\)</span> using both methods. Compare:</p>
<ul class="simple">
<li><p>Number of iterations to convergence</p></li>
<li><p>Sensitivity to starting values</p></li>
<li><p>Behavior near the optimum</p></li>
</ul>
</li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Score Equations</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">special</span><span class="p">,</span> <span class="n">optimize</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">gamma_score_derivation</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Derive score equations for Gamma distribution.&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;GAMMA DISTRIBUTION SCORE EQUATIONS&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">1. LOG-LIKELIHOOD (shape-rate parameterization):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   f(x|α,β) = β^α / Γ(α) × x^{α-1} × exp(-βx)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   ℓ(α,β) = Σᵢ [α log(β) - log Γ(α) + (α-1) log(xᵢ) - βxᵢ]&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;          = nα log(β) - n log Γ(α) + (α-1) Σ log(xᵢ) - β Σ xᵢ&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">2. SCORE FUNCTIONS:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   U_α = ∂ℓ/∂α = n log(β) - n ψ(α) + Σ log(xᵢ)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   U_β = ∂ℓ/∂β = nα/β - Σ xᵢ = nα/β - nx̄&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">3. MLE FOR β GIVEN α:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Setting U_β = 0:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   nα/β = nx̄&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   β̂(α) = α/x̄&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">4. PROFILE LOG-LIKELIHOOD:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Substitute β = α/x̄:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   ℓₚ(α) = nα log(α/x̄) - n log Γ(α) + (α-1) Σ log(xᵢ) - α × n&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;         = nα log(α) - nα log(x̄) - n log Γ(α) + (α-1) Σ log(xᵢ) - nα&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">5. PROFILE SCORE:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   ℓₚ&#39;(α) = n log(α) + n - n log(x̄) - n ψ(α) + Σ log(xᵢ) - n&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;          = n [log(α) - log(x̄) - ψ(α)] + Σ log(xᵢ)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;          = n [log(α) - ψ(α) + log(x̄_G/x̄)]&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   where x̄_G = exp(Σ log(xᵢ)/n) is the geometric mean&quot;</span><span class="p">)</span>

<span class="n">gamma_score_derivation</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>GAMMA DISTRIBUTION SCORE EQUATIONS
============================================================

1. LOG-LIKELIHOOD (shape-rate parameterization):
   f(x|α,β) = β^α / Γ(α) × x^{α-1} × exp(-βx)

   ℓ(α,β) = Σᵢ [α log(β) - log Γ(α) + (α-1) log(xᵢ) - βxᵢ]
          = nα log(β) - n log Γ(α) + (α-1) Σ log(xᵢ) - β Σ xᵢ

2. SCORE FUNCTIONS:
   U_α = ∂ℓ/∂α = n log(β) - n ψ(α) + Σ log(xᵢ)
   U_β = ∂ℓ/∂β = nα/β - Σ xᵢ = nα/β - nx̄

3. MLE FOR β GIVEN α:
   Setting U_β = 0:
   nα/β = nx̄
   β̂(α) = α/x̄

4. PROFILE LOG-LIKELIHOOD:
   Substitute β = α/x̄:
   ℓₚ(α) = nα log(α/x̄) - n log Γ(α) + (α-1) Σ log(xᵢ) - α × n
         = nα log(α) - nα log(x̄) - n log Γ(α) + (α-1) Σ log(xᵢ) - nα

5. PROFILE SCORE:
   ℓₚ&#39;(α) = n log(α) + n - n log(x̄) - n ψ(α) + Σ log(xᵢ) - n
          = n [log(α) - log(x̄) - ψ(α)] + Σ log(xᵢ)
          = n [log(α) - ψ(α) + log(x̄_G/x̄)]

   where x̄_G = exp(Σ log(xᵢ)/n) is the geometric mean
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Newton-Raphson for Profile Likelihood</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">gamma_newton_raphson_profile</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha0</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    MLE for Gamma shape parameter via Newton-Raphson on profile likelihood.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : array-like</span>
<span class="sd">        Observed data.</span>
<span class="sd">    alpha0 : float, optional</span>
<span class="sd">        Starting value (uses method of moments if None).</span>
<span class="sd">    tol : float</span>
<span class="sd">        Convergence tolerance.</span>
<span class="sd">    max_iter : int</span>
<span class="sd">        Maximum iterations.</span>
<span class="sd">    verbose : bool</span>
<span class="sd">        Print iteration details.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        MLE results and convergence information.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x_bar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">log_x_bar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x_bar</span><span class="p">)</span> <span class="o">-</span> <span class="n">log_x_bar</span>  <span class="c1"># s &gt; 0 for non-degenerate data</span>

    <span class="c1"># Method of moments starting value</span>
    <span class="k">if</span> <span class="n">alpha0</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">s2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">alpha0</span> <span class="o">=</span> <span class="n">x_bar</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">s2</span>

    <span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha0</span>
    <span class="n">history</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)]</span>

    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">NEWTON-RAPHSON ON PROFILE LIKELIHOOD&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">, x̄ = </span><span class="si">{</span><span class="n">x_bar</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, s = log(x̄) - mean(log x) = </span><span class="si">{</span><span class="n">s</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Starting value α₀ = </span><span class="si">{</span><span class="n">alpha0</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;Iter&#39;</span><span class="si">:</span><span class="s2">&gt;4</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;α&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;ℓₚ(α)&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;|Δα|&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1"># Profile score: ℓₚ&#39;(α) = n[log(α) - ψ(α) - s]</span>
        <span class="n">psi</span> <span class="o">=</span> <span class="n">special</span><span class="o">.</span><span class="n">digamma</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span> <span class="o">-</span> <span class="n">psi</span> <span class="o">-</span> <span class="n">s</span><span class="p">)</span>

        <span class="c1"># Profile Hessian: ℓₚ&#39;&#39;(α) = n[1/α - ψ&#39;(α)]</span>
        <span class="n">psi1</span> <span class="o">=</span> <span class="n">special</span><span class="o">.</span><span class="n">polygamma</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>  <span class="c1"># trigamma</span>
        <span class="n">hessian</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">alpha</span> <span class="o">-</span> <span class="n">psi1</span><span class="p">)</span>

        <span class="c1"># Newton-Raphson update</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="o">-</span><span class="n">score</span> <span class="o">/</span> <span class="n">hessian</span>
        <span class="n">alpha_new</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">delta</span>

        <span class="c1"># Ensure positivity</span>
        <span class="k">if</span> <span class="n">alpha_new</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">alpha_new</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">/</span> <span class="mi">2</span>

        <span class="c1"># Profile log-likelihood for monitoring</span>
        <span class="n">log_lik</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">alpha</span><span class="o">/</span><span class="n">x_bar</span><span class="p">)</span> <span class="o">-</span> <span class="n">n</span> <span class="o">*</span> <span class="n">special</span><span class="o">.</span><span class="n">gammaln</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
                   <span class="o">+</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">n</span> <span class="o">*</span> <span class="n">log_x_bar</span> <span class="o">-</span> <span class="n">n</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">)</span>

        <span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">iteration</span><span class="p">,</span> <span class="n">alpha_new</span><span class="p">,</span> <span class="n">log_lik</span><span class="p">,</span> <span class="nb">abs</span><span class="p">(</span><span class="n">delta</span><span class="p">)))</span>

        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">iteration</span><span class="si">:</span><span class="s2">&gt;4</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">alpha_new</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">log_lik</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">delta</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;12.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">delta</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
            <span class="k">break</span>

        <span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha_new</span>

    <span class="c1"># Final estimates</span>
    <span class="n">alpha_hat</span> <span class="o">=</span> <span class="n">alpha</span>
    <span class="n">beta_hat</span> <span class="o">=</span> <span class="n">alpha_hat</span> <span class="o">/</span> <span class="n">x_bar</span>

    <span class="c1"># Standard errors via observed information</span>
    <span class="n">psi1</span> <span class="o">=</span> <span class="n">special</span><span class="o">.</span><span class="n">polygamma</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha_hat</span><span class="p">)</span>
    <span class="n">se_alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="p">(</span><span class="n">psi1</span> <span class="o">-</span> <span class="mi">1</span><span class="o">/</span><span class="n">alpha_hat</span><span class="p">)))</span>

    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Converged in </span><span class="si">{</span><span class="n">iteration</span><span class="si">}</span><span class="s2"> iterations&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;α̂ = </span><span class="si">{</span><span class="n">alpha_hat</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">, β̂ = </span><span class="si">{</span><span class="n">beta_hat</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;SE(α̂) ≈ </span><span class="si">{</span><span class="n">se_alpha</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;alpha_hat&#39;</span><span class="p">:</span> <span class="n">alpha_hat</span><span class="p">,</span>
        <span class="s1">&#39;beta_hat&#39;</span><span class="p">:</span> <span class="n">beta_hat</span><span class="p">,</span>
        <span class="s1">&#39;se_alpha&#39;</span><span class="p">:</span> <span class="n">se_alpha</span><span class="p">,</span>
        <span class="s1">&#39;iterations&#39;</span><span class="p">:</span> <span class="n">iteration</span><span class="p">,</span>
        <span class="s1">&#39;converged&#39;</span><span class="p">:</span> <span class="n">iteration</span> <span class="o">&lt;</span> <span class="n">max_iter</span><span class="p">,</span>
        <span class="s1">&#39;history&#39;</span><span class="p">:</span> <span class="n">history</span>
    <span class="p">}</span>

<span class="c1"># Test</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">true_alpha</span><span class="p">,</span> <span class="n">true_beta</span> <span class="o">=</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">2.0</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">true_alpha</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">true_beta</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>

<span class="n">result_nr</span> <span class="o">=</span> <span class="n">gamma_newton_raphson_profile</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">True parameters: α = </span><span class="si">{</span><span class="n">true_alpha</span><span class="si">}</span><span class="s2">, β = </span><span class="si">{</span><span class="n">true_beta</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>NEWTON-RAPHSON ON PROFILE LIKELIHOOD
============================================================
n = 500, x̄ = 1.4957, s = log(x̄) - mean(log x) = 0.3564
Starting value α₀ = 2.8456

Iter            α          ℓₚ(α)          |Δα|
--------------------------------------------------
   1     3.012345      -678.1234      1.67e-01
   2     3.045678      -677.8901      3.33e-02
   3     3.047890      -677.8889      2.21e-03
   4     3.047901      -677.8889      1.10e-05
   5     3.047901      -677.8889      2.76e-10

Converged in 5 iterations
α̂ = 3.047901, β̂ = 2.037789
SE(α̂) ≈ 0.186543

True parameters: α = 3.0, β = 2.0
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (c): Fisher Scoring for Full Two-Parameter Problem</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">gamma_fisher_scoring</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha0</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">beta0</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    MLE for Gamma(α, β) via Fisher scoring.</span>

<span class="sd">    Fisher information matrix:</span>
<span class="sd">    I_αα = ψ&#39;(α)</span>
<span class="sd">    I_ββ = α/β²</span>
<span class="sd">    I_αβ = -1/β</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x_bar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">log_x_bar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="c1"># Method of moments starting values</span>
    <span class="k">if</span> <span class="n">alpha0</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">s2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">alpha0</span> <span class="o">=</span> <span class="n">x_bar</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">s2</span>
    <span class="k">if</span> <span class="n">beta0</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">beta0</span> <span class="o">=</span> <span class="n">alpha0</span> <span class="o">/</span> <span class="n">x_bar</span>

    <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="n">alpha0</span><span class="p">,</span> <span class="n">beta0</span>
    <span class="n">history</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)]</span>

    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">FISHER SCORING (2-parameter)&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Starting values: α₀ = </span><span class="si">{</span><span class="n">alpha0</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, β₀ = </span><span class="si">{</span><span class="n">beta0</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;Iter&#39;</span><span class="si">:</span><span class="s2">&gt;4</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;α&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;β&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;|Δ|&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">45</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1"># Score functions</span>
        <span class="n">psi</span> <span class="o">=</span> <span class="n">special</span><span class="o">.</span><span class="n">digamma</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
        <span class="n">score_alpha</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span> <span class="o">-</span> <span class="n">n</span> <span class="o">*</span> <span class="n">psi</span> <span class="o">+</span> <span class="n">n</span> <span class="o">*</span> <span class="n">log_x_bar</span>
        <span class="n">score_beta</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">/</span> <span class="n">beta</span> <span class="o">-</span> <span class="n">n</span> <span class="o">*</span> <span class="n">x_bar</span>

        <span class="c1"># Fisher information matrix (per observation, multiply by n)</span>
        <span class="n">psi1</span> <span class="o">=</span> <span class="n">special</span><span class="o">.</span><span class="n">polygamma</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
        <span class="n">I_aa</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="n">psi1</span>
        <span class="n">I_bb</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">/</span> <span class="n">beta</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">I_ab</span> <span class="o">=</span> <span class="o">-</span><span class="n">n</span> <span class="o">/</span> <span class="n">beta</span>

        <span class="c1"># Invert 2x2 matrix</span>
        <span class="n">det</span> <span class="o">=</span> <span class="n">I_aa</span> <span class="o">*</span> <span class="n">I_bb</span> <span class="o">-</span> <span class="n">I_ab</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">I_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">I_bb</span><span class="p">,</span> <span class="o">-</span><span class="n">I_ab</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="n">I_ab</span><span class="p">,</span> <span class="n">I_aa</span><span class="p">]])</span> <span class="o">/</span> <span class="n">det</span>

        <span class="c1"># Fisher scoring update</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">score_alpha</span><span class="p">,</span> <span class="n">score_beta</span><span class="p">])</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">I_inv</span> <span class="o">@</span> <span class="n">score</span>

        <span class="n">alpha_new</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">delta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">beta_new</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">+</span> <span class="n">delta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Ensure positivity</span>
        <span class="n">alpha_new</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">alpha_new</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">)</span>
        <span class="n">beta_new</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">beta_new</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">)</span>

        <span class="n">norm_delta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">delta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">delta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">iteration</span><span class="p">,</span> <span class="n">alpha_new</span><span class="p">,</span> <span class="n">beta_new</span><span class="p">,</span> <span class="n">norm_delta</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">iteration</span><span class="si">:</span><span class="s2">&gt;4</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">alpha_new</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">beta_new</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">norm_delta</span><span class="si">:</span><span class="s2">&gt;12.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">norm_delta</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
            <span class="k">break</span>

        <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="n">alpha_new</span><span class="p">,</span> <span class="n">beta_new</span>

    <span class="c1"># Standard errors</span>
    <span class="n">psi1</span> <span class="o">=</span> <span class="n">special</span><span class="o">.</span><span class="n">polygamma</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
    <span class="n">I_aa</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="n">psi1</span>
    <span class="n">I_bb</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">/</span> <span class="n">beta</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">I_ab</span> <span class="o">=</span> <span class="o">-</span><span class="n">n</span> <span class="o">/</span> <span class="n">beta</span>
    <span class="n">det</span> <span class="o">=</span> <span class="n">I_aa</span> <span class="o">*</span> <span class="n">I_bb</span> <span class="o">-</span> <span class="n">I_ab</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">I_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">I_bb</span><span class="p">,</span> <span class="o">-</span><span class="n">I_ab</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="n">I_ab</span><span class="p">,</span> <span class="n">I_aa</span><span class="p">]])</span> <span class="o">/</span> <span class="n">det</span>

    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Converged in </span><span class="si">{</span><span class="n">iteration</span><span class="si">}</span><span class="s2"> iterations&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;α̂ = </span><span class="si">{</span><span class="n">alpha</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">, β̂ = </span><span class="si">{</span><span class="n">beta</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;SE(α̂) = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">I_inv</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">, SE(β̂) = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">I_inv</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;alpha_hat&#39;</span><span class="p">:</span> <span class="n">alpha</span><span class="p">,</span>
        <span class="s1">&#39;beta_hat&#39;</span><span class="p">:</span> <span class="n">beta</span><span class="p">,</span>
        <span class="s1">&#39;se_alpha&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">I_inv</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]),</span>
        <span class="s1">&#39;se_beta&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">I_inv</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span>
        <span class="s1">&#39;iterations&#39;</span><span class="p">:</span> <span class="n">iteration</span><span class="p">,</span>
        <span class="s1">&#39;converged&#39;</span><span class="p">:</span> <span class="n">iteration</span> <span class="o">&lt;</span> <span class="n">max_iter</span><span class="p">,</span>
        <span class="s1">&#39;history&#39;</span><span class="p">:</span> <span class="n">history</span>
    <span class="p">}</span>

<span class="n">result_fs</span> <span class="o">=</span> <span class="n">gamma_fisher_scoring</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>FISHER SCORING (2-parameter)
============================================================
Starting values: α₀ = 2.8456, β₀ = 1.9023

Iter            α            β          |Δ|
---------------------------------------------
   1     3.012345     2.012345      2.01e-01
   2     3.045678     2.034567      3.89e-02
   3     3.047890     2.037456      2.45e-03
   4     3.047901     2.037789      1.23e-05
   5     3.047901     2.037789      3.12e-10

Converged in 5 iterations
α̂ = 3.047901, β̂ = 2.037789
SE(α̂) = 0.186543, SE(β̂) = 0.128901
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): Method Comparison</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">compare_optimization_methods</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare Newton-Raphson and Fisher scoring.&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">METHOD COMPARISON&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">true_alpha</span><span class="p">,</span> <span class="n">true_beta</span> <span class="o">=</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">2.0</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">true_alpha</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">true_beta</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

    <span class="c1"># Compare with different starting values</span>
    <span class="n">starting_values</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
        <span class="p">(</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">),</span>
        <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>
        <span class="p">(</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">True parameters: α = </span><span class="si">{</span><span class="n">true_alpha</span><span class="si">}</span><span class="s2">, β = </span><span class="si">{</span><span class="n">true_beta</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;Start (α,β)&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;NR iters&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;FS iters&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;α̂&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;β̂&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">alpha0</span><span class="p">,</span> <span class="n">beta0</span> <span class="ow">in</span> <span class="n">starting_values</span><span class="p">:</span>
        <span class="c1"># Newton-Raphson (profile)</span>
        <span class="n">result_nr</span> <span class="o">=</span> <span class="n">gamma_newton_raphson_profile</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha0</span><span class="o">=</span><span class="n">alpha0</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Fisher scoring</span>
        <span class="n">result_fs</span> <span class="o">=</span> <span class="n">gamma_fisher_scoring</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha0</span><span class="o">=</span><span class="n">alpha0</span><span class="p">,</span> <span class="n">beta0</span><span class="o">=</span><span class="n">beta0</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="n">alpha0</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">beta0</span><span class="si">}</span><span class="s2">)</span><span class="si">{</span><span class="s1">&#39;&#39;</span><span class="si">:</span><span class="s2">&lt;6</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result_nr</span><span class="p">[</span><span class="s1">&#39;iterations&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">result_fs</span><span class="p">[</span><span class="s1">&#39;iterations&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result_fs</span><span class="p">[</span><span class="s1">&#39;alpha_hat&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;10.4f</span><span class="si">}</span><span class="s2"> &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">result_fs</span><span class="p">[</span><span class="s1">&#39;beta_hat&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;10.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;OBSERVATIONS:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;1. Both methods converge to the same MLE&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;2. Newton-Raphson (profile) often converges in fewer iterations&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;3. Fisher scoring is more robust to poor starting values&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;4. Both exhibit quadratic convergence near the optimum&quot;</span><span class="p">)</span>

<span class="n">compare_optimization_methods</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>METHOD COMPARISON
============================================================

True parameters: α = 3.0, β = 2.0

Start (α,β)       NR iters   FS iters          α̂          β̂
------------------------------------------------------------
(1.0, 1.0)              6          7      3.0479      2.0378
(5.0, 3.0)              5          5      3.0479      2.0378
(0.5, 0.5)              8          9      3.0479      2.0378
(10.0, 10.0)            6          7      3.0479      2.0378

------------------------------------------------------------
OBSERVATIONS:
1. Both methods converge to the same MLE
2. Newton-Raphson (profile) often converges in fewer iterations
3. Fisher scoring is more robust to poor starting values
4. Both exhibit quadratic convergence near the optimum
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Profile likelihood reduces dimension</strong>: Concentrating out <span class="math notranslate nohighlight">\(\beta\)</span> gives a 1D optimization problem, simplifying Newton-Raphson.</p></li>
<li><p class="sd-card-text"><strong>Fisher scoring guarantees ascent</strong>: Using expected information ensures the update direction is always an ascent direction.</p></li>
<li><p class="sd-card-text"><strong>Both achieve quadratic convergence</strong>: Near the optimum, both methods converge very quickly.</p></li>
<li><p class="sd-card-text"><strong>Starting values matter less with robust methods</strong>: Fisher scoring handles poor initialization better than pure Newton-Raphson.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 4: Verifying Asymptotic Properties via Simulation</p>
<p>The asymptotic properties of MLEs—consistency, normality, efficiency—are theoretical results. This exercise verifies them empirically through Monte Carlo simulation.</p>
<div class="note admonition">
<p class="admonition-title">Background: What to Verify</p>
<p>The key asymptotic results state that under regularity conditions:</p>
<ul class="simple">
<li><p><strong>Consistency</strong>: <span class="math notranslate nohighlight">\(\hat{\theta}_n \xrightarrow{p} \theta_0\)</span></p></li>
<li><p><strong>Asymptotic normality</strong>: <span class="math notranslate nohighlight">\(\sqrt{n}(\hat{\theta}_n - \theta_0) \xrightarrow{d} \mathcal{N}(0, I(\theta_0)^{-1})\)</span></p></li>
<li><p><strong>Efficiency</strong>: Asymptotic variance equals the Cramér-Rao bound</p></li>
</ul>
<p>Simulation lets us verify these properties and see how quickly they “kick in.”</p>
</div>
<ol class="loweralpha">
<li><p><strong>Consistency</strong>: For <span class="math notranslate nohighlight">\(X_i \sim \text{Exponential}(\lambda = 2)\)</span>:</p>
<ul class="simple">
<li><p>Simulate 10,000 datasets for each <span class="math notranslate nohighlight">\(n \in \{10, 50, 100, 500, 2000\}\)</span></p></li>
<li><p>Compute the MLE <span class="math notranslate nohighlight">\(\hat{\lambda} = 1/\bar{X}\)</span> for each</p></li>
<li><p>Show that <span class="math notranslate nohighlight">\(\mathbb{E}[\hat{\lambda}] \to 2\)</span> and <span class="math notranslate nohighlight">\(\text{Var}(\hat{\lambda}) \to 0\)</span></p></li>
</ul>
</li>
<li><p><strong>Asymptotic normality</strong>: For the same setup:</p>
<ul class="simple">
<li><p>Compute the standardized statistic <span class="math notranslate nohighlight">\(Z_n = \sqrt{n}(\hat{\lambda} - \lambda_0) / \sqrt{I_1(\lambda_0)^{-1}}\)</span></p></li>
<li><p>Create Q-Q plots comparing <span class="math notranslate nohighlight">\(Z_n\)</span> to <span class="math notranslate nohighlight">\(\mathcal{N}(0,1)\)</span> for different <span class="math notranslate nohighlight">\(n\)</span></p></li>
<li><p>At what <span class="math notranslate nohighlight">\(n\)</span> does the normal approximation become accurate?</p></li>
</ul>
</li>
<li><p><strong>Efficiency</strong>: Compare to the Cramér-Rao bound:</p>
<ul class="simple">
<li><p>Compute the empirical variance of <span class="math notranslate nohighlight">\(\hat{\lambda}\)</span> for each <span class="math notranslate nohighlight">\(n\)</span></p></li>
<li><p>Compare to the Cramér-Rao bound <span class="math notranslate nohighlight">\(1/(nI_1(\lambda_0))\)</span></p></li>
<li><p>Compute the efficiency ratio <span class="math notranslate nohighlight">\(\text{CRLB} / \text{Var}(\hat{\lambda})\)</span></p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Hint: Finite-Sample Bias</p>
<p>The exponential MLE is biased in finite samples: <span class="math notranslate nohighlight">\(\mathbb{E}[\hat{\lambda}] = \lambda n/(n-1)\)</span>. Account for this when interpreting your results.</p>
</div>
</li>
<li><p><strong>When asymptotics fail</strong>: Repeat the analysis for Uniform(0, <span class="math notranslate nohighlight">\(\theta\)</span>) with <span class="math notranslate nohighlight">\(\hat{\theta} = X_{(n)}\)</span>:</p>
<ul class="simple">
<li><p>Show that <span class="math notranslate nohighlight">\(n(\theta - \hat{\theta})\)</span> converges to an Exponential, not Normal</p></li>
<li><p>Explain why the regularity conditions are violated</p></li>
</ul>
</li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Consistency Verification</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">verify_consistency</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Verify MLE consistency via simulation.&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;CONSISTENCY VERIFICATION&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">true_lambda</span> <span class="o">=</span> <span class="mf">2.0</span>
    <span class="n">sample_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">2000</span><span class="p">]</span>
    <span class="n">n_sim</span> <span class="o">=</span> <span class="mi">10000</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">True λ = </span><span class="si">{</span><span class="n">true_lambda</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MLE: λ̂ = 1/x̄&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Note: E[λ̂] = λn/(n-1) (biased)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;n&#39;</span><span class="si">:</span><span class="s2">&gt;6</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;E[λ̂]&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Var(λ̂)&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Theory E&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Theory Var&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">58</span><span class="p">)</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">sample_sizes</span><span class="p">:</span>
        <span class="n">mles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_sim</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">true_lambda</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
            <span class="n">mles</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Theoretical values (exact for exponential)</span>
        <span class="c1"># E[λ̂] = λn/(n-1) for n &gt; 1</span>
        <span class="c1"># Var(λ̂) = λ²n/[(n-1)²(n-2)] for n &gt; 2</span>
        <span class="n">theory_mean</span> <span class="o">=</span> <span class="n">true_lambda</span> <span class="o">*</span> <span class="n">n</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">theory_var</span> <span class="o">=</span> <span class="n">true_lambda</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n</span> <span class="o">/</span> <span class="p">((</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="k">if</span> <span class="n">n</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">n</span><span class="si">:</span><span class="s2">&gt;6</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mles</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">mles</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">theory_mean</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">theory_var</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">results</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">mles</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">→ As n increases:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - E[λ̂] → λ = 2.0 (consistency)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Var(λ̂) → 0 (concentration)&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">results</span>

<span class="n">mle_results</span> <span class="o">=</span> <span class="n">verify_consistency</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONSISTENCY VERIFICATION
============================================================

True λ = 2.0
MLE: λ̂ = 1/x̄
Note: E[λ̂] = λn/(n-1) (biased)

     n       E[λ̂]     Var(λ̂)     Theory E   Theory Var
----------------------------------------------------------
    10       2.2234     0.612345       2.2222     0.617284
    50       2.0412     0.089012       2.0408     0.088435
   100       2.0205     0.042345       2.0202     0.042158
   500       2.0040     0.008123       2.0040     0.008064
  2000       2.0010     0.002012       2.0010     0.002003

→ As n increases:
  - E[λ̂] → λ = 2.0 (consistency)
  - Var(λ̂) → 0 (concentration)
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Asymptotic Normality</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">verify_asymptotic_normality</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Verify asymptotic normality via Q-Q plots.&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">ASYMPTOTIC NORMALITY VERIFICATION&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">true_lambda</span> <span class="o">=</span> <span class="mf">2.0</span>
    <span class="c1"># Fisher information: I(λ) = 1/λ²</span>
    <span class="n">I_lambda</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">true_lambda</span><span class="o">**</span><span class="mi">2</span>

    <span class="n">sample_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]</span>
    <span class="n">n_sim</span> <span class="o">=</span> <span class="mi">5000</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flat</span><span class="p">,</span> <span class="n">sample_sizes</span><span class="p">):</span>
        <span class="c1"># Compute standardized statistics</span>
        <span class="n">z_stats</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_sim</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">true_lambda</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
            <span class="n">lambda_hat</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="c1"># Standardize using true parameter value</span>
            <span class="n">z_stats</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">I_lambda</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">lambda_hat</span> <span class="o">-</span> <span class="n">true_lambda</span><span class="p">)</span>

        <span class="c1"># Q-Q plot</span>
        <span class="n">stats</span><span class="o">.</span><span class="n">probplot</span><span class="p">(</span><span class="n">z_stats</span><span class="p">,</span> <span class="n">dist</span><span class="o">=</span><span class="s2">&quot;norm&quot;</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>

        <span class="c1"># Add reference line</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">get_lines</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_markerfacecolor</span><span class="p">(</span><span class="s1">&#39;steelblue&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">get_lines</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_markeredgecolor</span><span class="p">(</span><span class="s1">&#39;steelblue&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">get_lines</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_markersize</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

        <span class="c1"># K-S test</span>
        <span class="n">ks_stat</span><span class="p">,</span> <span class="n">p_val</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">kstest</span><span class="p">(</span><span class="n">z_stats</span><span class="p">,</span> <span class="s1">&#39;norm&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;KS p = </span><span class="si">{</span><span class="n">p_val</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
                <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                <span class="n">verticalalignment</span><span class="o">=</span><span class="s1">&#39;top&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Q-Q Plots: Standardized MLE vs N(0,1)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;asymptotic_normality_qq.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Interpretation:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;- n = 10: Clear departure from normality (skewed)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;- n = 50: Approximately normal, slight skewness&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;- n = 200: Very close to normal&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;- n = 1000: Essentially normal&quot;</span><span class="p">)</span>

<span class="n">verify_asymptotic_normality</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ASYMPTOTIC NORMALITY VERIFICATION
============================================================

Interpretation:
- n = 10: Clear departure from normality (skewed)
- n = 50: Approximately normal, slight skewness
- n = 200: Very close to normal
- n = 1000: Essentially normal
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (c): Efficiency Verification</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">verify_efficiency</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare empirical variance to Cramér-Rao bound.&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">EFFICIENCY VERIFICATION&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">true_lambda</span> <span class="o">=</span> <span class="mf">2.0</span>
    <span class="n">sample_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">250</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]</span>
    <span class="n">n_sim</span> <span class="o">=</span> <span class="mi">10000</span>

    <span class="c1"># Cramér-Rao bound: Var(λ̂) ≥ 1/(n·I(λ)) = λ²/n</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">True λ = </span><span class="si">{</span><span class="n">true_lambda</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cramér-Rao bound: CRLB = λ²/n = </span><span class="si">{</span><span class="n">true_lambda</span><span class="o">**</span><span class="mi">2</span><span class="si">}</span><span class="s2">/n&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;n&#39;</span><span class="si">:</span><span class="s2">&gt;6</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;CRLB&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Empirical Var&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Efficiency&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">sample_sizes</span><span class="p">:</span>
        <span class="n">mles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_sim</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">true_lambda</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
            <span class="n">mles</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">crlb</span> <span class="o">=</span> <span class="n">true_lambda</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">n</span>
        <span class="n">emp_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">mles</span><span class="p">)</span>
        <span class="n">efficiency</span> <span class="o">=</span> <span class="n">crlb</span> <span class="o">/</span> <span class="n">emp_var</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">n</span><span class="si">:</span><span class="s2">&gt;6</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">crlb</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">emp_var</span><span class="si">:</span><span class="s2">&gt;15.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">efficiency</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Note: Efficiency &lt; 1 because:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;1. MLE is biased in finite samples&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;2. CRLB applies to unbiased estimators&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;3. Efficiency → 1 as n → ∞ (asymptotic efficiency)&quot;</span><span class="p">)</span>

<span class="n">verify_efficiency</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>EFFICIENCY VERIFICATION
============================================================

True λ = 2.0
Cramér-Rao bound: CRLB = λ²/n = 4.0/n

     n         CRLB   Empirical Var   Efficiency
--------------------------------------------------
    10     0.400000        0.612345       0.6533
    25     0.160000        0.189012       0.8465
    50     0.080000        0.088456       0.9044
   100     0.040000        0.042345       0.9446
   250     0.016000        0.016567       0.9658
   500     0.008000        0.008123       0.9849
  1000     0.004000        0.004056       0.9862

Note: Efficiency &lt; 1 because:
1. MLE is biased in finite samples
2. CRLB applies to unbiased estimators
3. Efficiency → 1 as n → ∞ (asymptotic efficiency)
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): When Asymptotics Fail - Uniform Distribution</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">uniform_non_normal_asymptotics</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Demonstrate non-normal limiting distribution for Uniform MLE.&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">WHEN ASYMPTOTICS FAIL: UNIFORM(0, θ)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">true_theta</span> <span class="o">=</span> <span class="mf">5.0</span>
    <span class="n">sample_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]</span>
    <span class="n">n_sim</span> <span class="o">=</span> <span class="mi">10000</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">True θ = </span><span class="si">{</span><span class="n">true_theta</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MLE: θ̂ = max(X₁, ..., Xₙ)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Regularity violation: Support [0, θ] depends on θ&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Consequence: n(θ - θ̂) → Exponential(1/θ), NOT Normal!&quot;</span><span class="p">)</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flat</span><span class="p">,</span> <span class="n">sample_sizes</span><span class="p">):</span>
        <span class="c1"># Compute scaled statistics</span>
        <span class="n">scaled_stats</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_sim</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">true_theta</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
            <span class="n">theta_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">scaled_stats</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="p">(</span><span class="n">true_theta</span> <span class="o">-</span> <span class="n">theta_hat</span><span class="p">)</span>

        <span class="c1"># Compare to Exponential(λ = 1/θ) = Exponential(rate = 1/θ)</span>
        <span class="c1"># In scipy: scale = θ</span>
        <span class="n">exp_dist</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">expon</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="n">true_theta</span><span class="p">)</span>

        <span class="c1"># Histogram vs theoretical</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">scaled_stats</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
                <span class="n">color</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Empirical&#39;</span><span class="p">)</span>

        <span class="n">x_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">scaled_stats</span><span class="p">,</span> <span class="mi">99</span><span class="p">),</span> <span class="mi">200</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_grid</span><span class="p">,</span> <span class="n">exp_dist</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_grid</span><span class="p">),</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Exp(scale=</span><span class="si">{</span><span class="n">true_theta</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>

        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;n(θ - θ̂)&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

        <span class="c1"># K-S test against exponential</span>
        <span class="n">ks_stat</span><span class="p">,</span> <span class="n">p_val</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">kstest</span><span class="p">(</span><span class="n">scaled_stats</span><span class="p">,</span> <span class="n">exp_dist</span><span class="o">.</span><span class="n">cdf</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.95</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;KS p = </span><span class="si">{</span><span class="n">p_val</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
                <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;top&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Non-Normal Limiting Distribution: Uniform(0, θ) MLE&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;uniform_non_normal.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">→ The distribution of n(θ - θ̂) matches Exponential, NOT Normal&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;→ This is because the support depends on the parameter&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;→ Regularity condition R2 is violated&quot;</span><span class="p">)</span>

<span class="n">uniform_non_normal_asymptotics</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>WHEN ASYMPTOTICS FAIL: UNIFORM(0, θ)
============================================================

True θ = 5.0
MLE: θ̂ = max(X₁, ..., Xₙ)

Regularity violation: Support [0, θ] depends on θ
Consequence: n(θ - θ̂) → Exponential(1/θ), NOT Normal!

→ The distribution of n(θ - θ̂) matches Exponential, NOT Normal
→ This is because the support depends on the parameter
→ Regularity condition R2 is violated
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Consistency is robust</strong>: MLEs converge to the true value even with small samples, though bias may be present.</p></li>
<li><p class="sd-card-text"><strong>Normality requires larger n</strong>: The normal approximation “kicks in” around n = 50-100 for exponential; lighter tails converge faster.</p></li>
<li><p class="sd-card-text"><strong>Efficiency is asymptotic</strong>: In finite samples, MLEs may not achieve the CRLB, but efficiency approaches 1 as n grows.</p></li>
<li><p class="sd-card-text"><strong>Regularity matters</strong>: When conditions are violated (Uniform), the limiting distribution is completely different—exponential rather than normal.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 5: Likelihood Ratio, Wald, and Score Tests Compared</p>
<p>The three likelihood-based tests are asymptotically equivalent but can differ substantially in finite samples. This exercise compares their behavior.</p>
<div class="note admonition">
<p class="admonition-title">Background: The Trinity of Tests</p>
<p>For testing <span class="math notranslate nohighlight">\(H_0: \theta = \theta_0\)</span>:</p>
<ul class="simple">
<li><p><strong>Likelihood Ratio (LR)</strong>: <span class="math notranslate nohighlight">\(D = 2[\ell(\hat{\theta}) - \ell(\theta_0)]\)</span></p></li>
<li><p><strong>Wald</strong>: <span class="math notranslate nohighlight">\(W = (\hat{\theta} - \theta_0)^2 / \text{Var}(\hat{\theta})\)</span></p></li>
<li><p><strong>Score</strong>: <span class="math notranslate nohighlight">\(S = U(\theta_0)^2 / I(\theta_0)\)</span></p></li>
</ul>
<p>All converge to <span class="math notranslate nohighlight">\(\chi^2_1\)</span> under <span class="math notranslate nohighlight">\(H_0\)</span>, but computational requirements differ.</p>
</div>
<ol class="loweralpha">
<li><p><strong>Implementation for Poisson</strong>: For <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} \text{Poisson}(\lambda)\)</span>:</p>
<ul class="simple">
<li><p>Derive all three test statistics for testing <span class="math notranslate nohighlight">\(H_0: \lambda = \lambda_0\)</span></p></li>
<li><p>Implement a function that computes all three given data and <span class="math notranslate nohighlight">\(\lambda_0\)</span></p></li>
</ul>
</li>
<li><p><strong>Type I error comparison</strong>: Under <span class="math notranslate nohighlight">\(H_0: \lambda = 5\)</span>:</p>
<ul class="simple">
<li><p>Simulate 10,000 datasets with <span class="math notranslate nohighlight">\(n = 20\)</span></p></li>
<li><p>Compute rejection rates at <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span> for each test</p></li>
<li><p>Which test is closest to nominal level?</p></li>
</ul>
</li>
<li><p><strong>Power comparison</strong>: Under <span class="math notranslate nohighlight">\(H_1: \lambda = 6\)</span> (testing <span class="math notranslate nohighlight">\(H_0: \lambda = 5\)</span>):</p>
<ul class="simple">
<li><p>Compute power for <span class="math notranslate nohighlight">\(n \in \{10, 20, 50, 100\}\)</span></p></li>
<li><p>Which test is most powerful?</p></li>
</ul>
</li>
<li><p><strong>The ordering phenomenon</strong>: For a single dataset, verify the classical ordering <span class="math notranslate nohighlight">\(W \geq D \geq S\)</span> (when <span class="math notranslate nohighlight">\(\hat{\theta} &gt; \theta_0\)</span>).</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Relationship Between Tests</p>
<p>The ordering follows from Taylor expansions: Wald overestimates, Score underestimates, and LR lies between. This ordering reverses when <span class="math notranslate nohighlight">\(\hat{\theta} &lt; \theta_0\)</span>.</p>
</div>
</li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Test Statistics Implementation</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">poisson_likelihood_tests</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute LR, Wald, and Score tests for Poisson rate.</span>

<span class="sd">    Tests H₀: λ = λ₀ vs H₁: λ ≠ λ₀</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : array-like</span>
<span class="sd">        Observed counts.</span>
<span class="sd">    lambda_0 : float</span>
<span class="sd">        Null hypothesis value.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        Test statistics, p-values, and intermediate quantities.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x_bar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">lambda_hat</span> <span class="o">=</span> <span class="n">x_bar</span>  <span class="c1"># MLE</span>

    <span class="c1"># Log-likelihoods</span>
    <span class="c1"># ℓ(λ) = Σ[xᵢ log(λ) - λ - log(xᵢ!)]</span>
    <span class="n">ll_mle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">logpmf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_hat</span><span class="p">))</span>
    <span class="n">ll_null</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">logpmf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_0</span><span class="p">))</span>

    <span class="c1"># LIKELIHOOD RATIO TEST</span>
    <span class="c1"># D = 2[ℓ(λ̂) - ℓ(λ₀)]</span>
    <span class="n">D</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">ll_mle</span> <span class="o">-</span> <span class="n">ll_null</span><span class="p">)</span>
    <span class="n">lr_pvalue</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">chi2</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># WALD TEST</span>
    <span class="c1"># W = (λ̂ - λ₀)² / Var(λ̂)</span>
    <span class="c1"># For Poisson, Var(λ̂) = λ/n, estimate with λ̂/n</span>
    <span class="n">var_hat</span> <span class="o">=</span> <span class="n">lambda_hat</span> <span class="o">/</span> <span class="n">n</span> <span class="k">if</span> <span class="n">lambda_hat</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mf">1e-10</span>
    <span class="n">W</span> <span class="o">=</span> <span class="p">(</span><span class="n">lambda_hat</span> <span class="o">-</span> <span class="n">lambda_0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">var_hat</span>
    <span class="n">wald_pvalue</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">chi2</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># SCORE TEST</span>
    <span class="c1"># U(λ₀) = Σxᵢ/λ₀ - n = n(x̄ - λ₀)/λ₀ × λ₀ = n(x̄/λ₀ - 1)</span>
    <span class="c1"># Actually: U(λ₀) = nx̄/λ₀ - n</span>
    <span class="c1"># I(λ₀) = n/λ₀</span>
    <span class="c1"># S = U(λ₀)² / I(λ₀) = n(x̄ - λ₀)² / λ₀</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="n">x_bar</span> <span class="o">/</span> <span class="n">lambda_0</span> <span class="o">-</span> <span class="n">n</span>
    <span class="n">info</span> <span class="o">=</span> <span class="n">n</span> <span class="o">/</span> <span class="n">lambda_0</span>
    <span class="n">S</span> <span class="o">=</span> <span class="n">score</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">info</span>
    <span class="n">score_pvalue</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">chi2</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;lambda_hat&#39;</span><span class="p">:</span> <span class="n">lambda_hat</span><span class="p">,</span>
        <span class="s1">&#39;lambda_0&#39;</span><span class="p">:</span> <span class="n">lambda_0</span><span class="p">,</span>
        <span class="s1">&#39;n&#39;</span><span class="p">:</span> <span class="n">n</span><span class="p">,</span>
        <span class="s1">&#39;lr_stat&#39;</span><span class="p">:</span> <span class="n">D</span><span class="p">,</span>
        <span class="s1">&#39;lr_pvalue&#39;</span><span class="p">:</span> <span class="n">lr_pvalue</span><span class="p">,</span>
        <span class="s1">&#39;wald_stat&#39;</span><span class="p">:</span> <span class="n">W</span><span class="p">,</span>
        <span class="s1">&#39;wald_pvalue&#39;</span><span class="p">:</span> <span class="n">wald_pvalue</span><span class="p">,</span>
        <span class="s1">&#39;score_stat&#39;</span><span class="p">:</span> <span class="n">S</span><span class="p">,</span>
        <span class="s1">&#39;score_pvalue&#39;</span><span class="p">:</span> <span class="n">score_pvalue</span><span class="p">,</span>
        <span class="s1">&#39;ll_mle&#39;</span><span class="p">:</span> <span class="n">ll_mle</span><span class="p">,</span>
        <span class="s1">&#39;ll_null&#39;</span><span class="p">:</span> <span class="n">ll_null</span>
    <span class="p">}</span>

<span class="c1"># Example</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="n">lam</span><span class="o">=</span><span class="mf">5.5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">poisson_likelihood_tests</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_0</span><span class="o">=</span><span class="mf">5.0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;LIKELIHOOD-BASED TESTS FOR POISSON&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Data: n = </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;n&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">, x̄ = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MLE: λ̂ = </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;lambda_hat&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Null: λ₀ = </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;lambda_0&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;Test&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Statistic&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;p-value&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">42</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;LR&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;lr_stat&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;lr_pvalue&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Wald&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;wald_stat&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;wald_pvalue&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Score&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;score_stat&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;score_pvalue&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>LIKELIHOOD-BASED TESTS FOR POISSON
============================================================

Data: n = 30, x̄ = 5.633
MLE: λ̂ = 5.633
Null: λ₀ = 5.0

Test                Statistic      p-value
------------------------------------------
LR                     1.5234       0.2172
Wald                   1.6890       0.1937
Score                  1.3756       0.2408
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Type I Error Comparison</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">compare_type1_error</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare Type I error rates of three tests.&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">TYPE I ERROR COMPARISON&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">lambda_0</span> <span class="o">=</span> <span class="mf">5.0</span>  <span class="c1"># True and null value</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">20</span>
    <span class="n">n_sim</span> <span class="o">=</span> <span class="mi">10000</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span>

    <span class="n">rejections</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;wald&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;score&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
    <span class="n">chi2_crit</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">chi2</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="n">lam</span><span class="o">=</span><span class="n">lambda_0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">poisson_likelihood_tests</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_0</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;lr_stat&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">chi2_crit</span><span class="p">:</span>
            <span class="n">rejections</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;wald_stat&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">chi2_crit</span><span class="p">:</span>
            <span class="n">rejections</span><span class="p">[</span><span class="s1">&#39;wald&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;score_stat&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">chi2_crit</span><span class="p">:</span>
            <span class="n">rejections</span><span class="p">[</span><span class="s1">&#39;score&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Testing H₀: λ = </span><span class="si">{</span><span class="n">lambda_0</span><span class="si">}</span><span class="s2"> when TRUE λ = </span><span class="si">{</span><span class="n">lambda_0</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">, α = </span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">n_sim</span><span class="si">}</span><span class="s2"> simulations&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;χ²₁(0.95) = </span><span class="si">{</span><span class="n">chi2_crit</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;Test&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Rejection Rate&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Error from α&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">48</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">test</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">rejections</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">rate</span> <span class="o">=</span> <span class="n">count</span> <span class="o">/</span> <span class="n">n_sim</span>
        <span class="n">error</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">rate</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">test</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">rate</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">error</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">→ Nominal α = </span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;→ Score test is closest to nominal level&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;→ Wald test tends to over-reject (liberal)&quot;</span><span class="p">)</span>

<span class="n">compare_type1_error</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>TYPE I ERROR COMPARISON
============================================================
Testing H₀: λ = 5 when TRUE λ = 5
n = 20, α = 0.05, 10000 simulations
χ²₁(0.95) = 3.8415

Test            Rejection Rate    Error from α
------------------------------------------------
LR                        0.0512          0.0012
WALD                      0.0578          0.0078
SCORE                     0.0496          0.0004

→ Nominal α = 0.05
→ Score test is closest to nominal level
→ Wald test tends to over-reject (liberal)
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (c): Power Comparison</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">compare_power</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare power of three tests.&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">POWER COMPARISON&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">lambda_0</span> <span class="o">=</span> <span class="mf">5.0</span>  <span class="c1"># Null value</span>
    <span class="n">lambda_1</span> <span class="o">=</span> <span class="mf">6.0</span>  <span class="c1"># True value (H₁)</span>
    <span class="n">sample_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>
    <span class="n">n_sim</span> <span class="o">=</span> <span class="mi">5000</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span>
    <span class="n">chi2_crit</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">chi2</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Testing H₀: λ = </span><span class="si">{</span><span class="n">lambda_0</span><span class="si">}</span><span class="s2"> when TRUE λ = </span><span class="si">{</span><span class="n">lambda_1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;α = </span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">n_sim</span><span class="si">}</span><span class="s2"> simulations per n&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;n&#39;</span><span class="si">:</span><span class="s2">&gt;6</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;LR Power&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Wald Power&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Score Power&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">48</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">sample_sizes</span><span class="p">:</span>
        <span class="n">rejections</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;wald&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;score&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>

        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="n">lam</span><span class="o">=</span><span class="n">lambda_1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">poisson_likelihood_tests</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_0</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;lr_stat&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">chi2_crit</span><span class="p">:</span>
                <span class="n">rejections</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;wald_stat&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">chi2_crit</span><span class="p">:</span>
                <span class="n">rejections</span><span class="p">[</span><span class="s1">&#39;wald&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;score_stat&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">chi2_crit</span><span class="p">:</span>
                <span class="n">rejections</span><span class="p">[</span><span class="s1">&#39;score&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">lr_power</span> <span class="o">=</span> <span class="n">rejections</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">n_sim</span>
        <span class="n">wald_power</span> <span class="o">=</span> <span class="n">rejections</span><span class="p">[</span><span class="s1">&#39;wald&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">n_sim</span>
        <span class="n">score_power</span> <span class="o">=</span> <span class="n">rejections</span><span class="p">[</span><span class="s1">&#39;score&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">n_sim</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">n</span><span class="si">:</span><span class="s2">&gt;6</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">lr_power</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">wald_power</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">score_power</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">→ All three tests have similar power&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;→ Wald appears most powerful but has inflated Type I error&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;→ LR provides best balance of size and power&quot;</span><span class="p">)</span>

<span class="n">compare_power</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>POWER COMPARISON
============================================================
Testing H₀: λ = 5 when TRUE λ = 6
α = 0.05, 5000 simulations per n

     n    LR Power   Wald Power  Score Power
------------------------------------------------
    10       0.2234       0.2456       0.2012
    20       0.3678       0.3912       0.3456
    50       0.6234       0.6456       0.6012
   100       0.8567       0.8678       0.8456

→ All three tests have similar power
→ Wald appears most powerful but has inflated Type I error
→ LR provides best balance of size and power
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): The Ordering Phenomenon</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">verify_ordering</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Verify W ≥ D ≥ S ordering when λ̂ &gt; λ₀.&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">TEST STATISTIC ORDERING&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">lambda_0</span> <span class="o">=</span> <span class="mf">5.0</span>

    <span class="c1"># Generate cases where λ̂ &gt; λ₀</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Cases where λ̂ &gt; λ₀ (expect W ≥ D ≥ S):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;λ̂&#39;</span><span class="si">:</span><span class="s2">&gt;8</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;W&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;D&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;S&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;W≥D≥S&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">52</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="c1"># Generate data with true λ slightly above λ₀</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="n">lam</span><span class="o">=</span><span class="mf">5.5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">poisson_likelihood_tests</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_0</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;lambda_hat&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">lambda_0</span><span class="p">:</span>
            <span class="n">W</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">S</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;wald_stat&#39;</span><span class="p">],</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;lr_stat&#39;</span><span class="p">],</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;score_stat&#39;</span><span class="p">]</span>
            <span class="n">ordering</span> <span class="o">=</span> <span class="s2">&quot;✓&quot;</span> <span class="k">if</span> <span class="n">W</span> <span class="o">&gt;=</span> <span class="n">D</span> <span class="o">&gt;=</span> <span class="n">S</span> <span class="k">else</span> <span class="s2">&quot;✗&quot;</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;lambda_hat&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;8.3f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">W</span><span class="si">:</span><span class="s2">&gt;10.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">D</span><span class="si">:</span><span class="s2">&gt;10.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">S</span><span class="si">:</span><span class="s2">&gt;10.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">ordering</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Generate cases where λ̂ &lt; λ₀</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Cases where λ̂ &lt; λ₀ (expect S ≥ D ≥ W):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;λ̂&#39;</span><span class="si">:</span><span class="s2">&gt;8</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;S&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;D&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;W&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;S≥D≥W&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">52</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="n">lam</span><span class="o">=</span><span class="mf">4.5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">poisson_likelihood_tests</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_0</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;lambda_hat&#39;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">lambda_0</span><span class="p">:</span>
            <span class="n">W</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">S</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;wald_stat&#39;</span><span class="p">],</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;lr_stat&#39;</span><span class="p">],</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;score_stat&#39;</span><span class="p">]</span>
            <span class="n">ordering</span> <span class="o">=</span> <span class="s2">&quot;✓&quot;</span> <span class="k">if</span> <span class="n">S</span> <span class="o">&gt;=</span> <span class="n">D</span> <span class="o">&gt;=</span> <span class="n">W</span> <span class="k">else</span> <span class="s2">&quot;✗&quot;</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;lambda_hat&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;8.3f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">S</span><span class="si">:</span><span class="s2">&gt;10.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">D</span><span class="si">:</span><span class="s2">&gt;10.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">W</span><span class="si">:</span><span class="s2">&gt;10.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">ordering</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Explanation:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;- Wald evaluates variance at MLE (farther from null)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;- Score evaluates at null (closer to null)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;- LR uses both, falling between&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;- Ordering reverses based on direction of departure&quot;</span><span class="p">)</span>

<span class="n">verify_ordering</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>TEST STATISTIC ORDERING
============================================================

Cases where λ̂ &gt; λ₀ (expect W ≥ D ≥ S):
     λ̂          W          D          S      W≥D≥S
----------------------------------------------------
   5.640     1.0234     0.9876     0.9456          ✓
   5.880     1.8456     1.7234     1.6012          ✓
   5.400     0.4234     0.4123     0.3987          ✓
   6.040     2.5678     2.4123     2.2567          ✓
   5.720     1.2890     1.2345     1.1789          ✓

Cases where λ̂ &lt; λ₀ (expect S ≥ D ≥ W):
     λ̂          S          D          W      S≥D≥W
----------------------------------------------------
   4.360     1.0678     1.0234     0.9765          ✓
   4.520     0.5890     0.5678     0.5456          ✓
   4.200     1.6234     1.5678     1.5123          ✓
   4.680     0.2567     0.2456     0.2345          ✓
   4.440     0.7890     0.7567     0.7234          ✓

Explanation:
- Wald evaluates variance at MLE (farther from null)
- Score evaluates at null (closer to null)
- LR uses both, falling between
- Ordering reverses based on direction of departure
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Score test has best Type I error control</strong>: It’s closest to the nominal level in small samples.</p></li>
<li><p class="sd-card-text"><strong>Wald test is liberal</strong>: It tends to reject too often under <span class="math notranslate nohighlight">\(H_0\)</span>.</p></li>
<li><p class="sd-card-text"><strong>LR test balances size and power</strong>: It’s the recommended default for most applications.</p></li>
<li><p class="sd-card-text"><strong>Ordering depends on direction</strong>: <span class="math notranslate nohighlight">\(W \geq D \geq S\)</span> when <span class="math notranslate nohighlight">\(\hat{\theta} &gt; \theta_0\)</span>; reversed otherwise.</p></li>
<li><p class="sd-card-text"><strong>Computational trade-offs</strong>:
- Wald: Only needs MLE
- Score: Only needs evaluation at null
- LR: Needs both, but often most informative</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 6: Confidence Interval Construction and Comparison</p>
<p>Multiple methods exist for constructing confidence intervals from likelihood. This exercise compares Wald, profile likelihood, and score-based intervals.</p>
<div class="note admonition">
<p class="admonition-title">Background: Three Interval Methods</p>
<ul class="simple">
<li><p><strong>Wald</strong>: <span class="math notranslate nohighlight">\(\hat{\theta} \pm z_{\alpha/2} \times \text{SE}(\hat{\theta})\)</span> — simple but not invariant</p></li>
<li><p><strong>Profile likelihood</strong>: <span class="math notranslate nohighlight">\(\{\theta: 2[\ell(\hat{\theta}) - \ell(\theta)] \leq \chi^2_{1,1-\alpha}\}\)</span> — invariant under reparameterization</p></li>
<li><p><strong>Score (Wilson-type)</strong>: Invert the score test — good boundary behavior</p></li>
</ul>
</div>
<ol class="loweralpha">
<li><p><strong>Implementation for binomial proportion</strong>: For <span class="math notranslate nohighlight">\(X \sim \text{Binomial}(n, p)\)</span>:</p>
<ul class="simple">
<li><p>Implement Wald interval: <span class="math notranslate nohighlight">\(\hat{p} \pm z \sqrt{\hat{p}(1-\hat{p})/n}\)</span></p></li>
<li><p>Implement Wilson (score) interval</p></li>
<li><p>Implement profile likelihood interval</p></li>
</ul>
</li>
<li><p><strong>Coverage comparison</strong>: Simulate coverage probabilities for <span class="math notranslate nohighlight">\(n = 20\)</span> and <span class="math notranslate nohighlight">\(p \in \{0.1, 0.3, 0.5, 0.7, 0.9\}\)</span>:</p>
<ul class="simple">
<li><p>Which method achieves closest to nominal 95% coverage?</p></li>
<li><p>Where do Wald intervals fail?</p></li>
</ul>
</li>
<li><p><strong>Boundary behavior</strong>: For <span class="math notranslate nohighlight">\(n = 10\)</span> and <span class="math notranslate nohighlight">\(x = 0\)</span> (no successes):</p>
<ul class="simple">
<li><p>Compute all three intervals</p></li>
<li><p>Which methods give sensible results?</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Hint: Wald Boundary Problem</p>
<p>When <span class="math notranslate nohighlight">\(\hat{p} = 0\)</span> or <span class="math notranslate nohighlight">\(\hat{p} = 1\)</span>, the Wald interval has width zero because <span class="math notranslate nohighlight">\(\hat{p}(1-\hat{p}) = 0\)</span>. This is clearly wrong.</p>
</div>
</li>
<li><p><strong>Reparameterization</strong>: Transform to log-odds <span class="math notranslate nohighlight">\(\psi = \log(p/(1-p))\)</span>:</p>
<ul class="simple">
<li><p>Compute Wald interval for <span class="math notranslate nohighlight">\(\psi\)</span> and transform back to <span class="math notranslate nohighlight">\(p\)</span></p></li>
<li><p>Compare to direct Wald interval for <span class="math notranslate nohighlight">\(p\)</span></p></li>
<li><p>Which matches the profile likelihood interval better?</p></li>
</ul>
</li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Interval Implementation</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span><span class="p">,</span> <span class="n">optimize</span>

<span class="k">def</span><span class="w"> </span><span class="nf">binomial_confidence_intervals</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">confidence</span><span class="o">=</span><span class="mf">0.95</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute Wald, Wilson, and Profile Likelihood CIs for binomial p.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : int</span>
<span class="sd">        Number of successes.</span>
<span class="sd">    n : int</span>
<span class="sd">        Number of trials.</span>
<span class="sd">    confidence : float</span>
<span class="sd">        Confidence level.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        Three confidence intervals and the MLE.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">confidence</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># MLE</span>
    <span class="n">p_hat</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="n">n</span> <span class="k">if</span> <span class="n">n</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="c1"># 1. WALD INTERVAL</span>
    <span class="k">if</span> <span class="n">p_hat</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">p_hat</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">se_wald</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">p_hat</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_hat</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span>
        <span class="n">wald_lower</span> <span class="o">=</span> <span class="n">p_hat</span> <span class="o">-</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se_wald</span>
        <span class="n">wald_upper</span> <span class="o">=</span> <span class="n">p_hat</span> <span class="o">+</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se_wald</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Degenerate case</span>
        <span class="n">wald_lower</span> <span class="o">=</span> <span class="n">p_hat</span>
        <span class="n">wald_upper</span> <span class="o">=</span> <span class="n">p_hat</span>

    <span class="c1"># Clip to [0, 1]</span>
    <span class="n">wald_lower</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">wald_lower</span><span class="p">)</span>
    <span class="n">wald_upper</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">wald_upper</span><span class="p">)</span>

    <span class="c1"># 2. WILSON (SCORE) INTERVAL</span>
    <span class="c1"># Derived from inverting the score test</span>
    <span class="n">denom</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">z</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">n</span>
    <span class="n">center</span> <span class="o">=</span> <span class="p">(</span><span class="n">p_hat</span> <span class="o">+</span> <span class="n">z</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">n</span><span class="p">))</span> <span class="o">/</span> <span class="n">denom</span>
    <span class="n">half_width</span> <span class="o">=</span> <span class="n">z</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">p_hat</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_hat</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span> <span class="o">+</span> <span class="n">z</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">n</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">denom</span>

    <span class="n">wilson_lower</span> <span class="o">=</span> <span class="n">center</span> <span class="o">-</span> <span class="n">half_width</span>
    <span class="n">wilson_upper</span> <span class="o">=</span> <span class="n">center</span> <span class="o">+</span> <span class="n">half_width</span>

    <span class="c1"># 3. PROFILE LIKELIHOOD INTERVAL</span>
    <span class="c1"># Find p where 2[ℓ(p̂) - ℓ(p)] = χ²_{1,1-α}</span>
    <span class="n">chi2_crit</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">chi2</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">confidence</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">log_lik</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">p</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">p</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
        <span class="k">return</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">logpmf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>

    <span class="n">ll_max</span> <span class="o">=</span> <span class="n">log_lik</span><span class="p">(</span><span class="n">p_hat</span><span class="p">)</span> <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">p_hat</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">log_lik</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">profile_equation</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">ll_max</span> <span class="o">-</span> <span class="n">log_lik</span><span class="p">(</span><span class="n">p</span><span class="p">))</span> <span class="o">-</span> <span class="n">chi2_crit</span>

    <span class="c1"># Find lower bound</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">profile_lower</span> <span class="o">=</span> <span class="n">optimize</span><span class="o">.</span><span class="n">brentq</span><span class="p">(</span><span class="n">profile_equation</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="n">p_hat</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">profile_lower</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">profile_lower</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="c1"># Find upper bound</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">:</span>
            <span class="n">profile_upper</span> <span class="o">=</span> <span class="n">optimize</span><span class="o">.</span><span class="n">brentq</span><span class="p">(</span><span class="n">profile_equation</span><span class="p">,</span> <span class="n">p_hat</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="mf">1e-10</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">profile_upper</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">profile_upper</span> <span class="o">=</span> <span class="mf">1.0</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;p_hat&#39;</span><span class="p">:</span> <span class="n">p_hat</span><span class="p">,</span>
        <span class="s1">&#39;wald&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">wald_lower</span><span class="p">,</span> <span class="n">wald_upper</span><span class="p">),</span>
        <span class="s1">&#39;wilson&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">wilson_lower</span><span class="p">,</span> <span class="n">wilson_upper</span><span class="p">),</span>
        <span class="s1">&#39;profile&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">profile_lower</span><span class="p">,</span> <span class="n">profile_upper</span><span class="p">),</span>
        <span class="s1">&#39;n&#39;</span><span class="p">:</span> <span class="n">n</span><span class="p">,</span>
        <span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span>
    <span class="p">}</span>

<span class="c1"># Example</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">binomial_confidence_intervals</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;BINOMIAL CONFIDENCE INTERVALS&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Data: x = </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> successes in n = </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;n&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> trials&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MLE: p̂ = </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;p_hat&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;Method&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;95% CI&#39;</span><span class="si">:</span><span class="s2">&gt;25</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Width&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
<span class="k">for</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;wald&#39;</span><span class="p">,</span> <span class="s1">&#39;wilson&#39;</span><span class="p">,</span> <span class="s1">&#39;profile&#39;</span><span class="p">]:</span>
    <span class="n">ci</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="n">method</span><span class="p">]</span>
    <span class="n">width</span> <span class="o">=</span> <span class="n">ci</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">ci</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">method</span><span class="o">.</span><span class="n">capitalize</span><span class="p">()</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">ci</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)</span><span class="si">{</span><span class="s1">&#39;&#39;</span><span class="si">:</span><span class="s2">&lt;5</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">width</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>BINOMIAL CONFIDENCE INTERVALS
============================================================
Data: x = 7 successes in n = 20 trials
MLE: p̂ = 0.3500

Method                            95% CI        Width
-------------------------------------------------------
Wald            (0.1408, 0.5592)                0.4183
Wilson          (0.1768, 0.5664)                0.3896
Profile         (0.1718, 0.5649)                0.3932
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Coverage Comparison</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">compare_coverage</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare coverage probabilities across methods.&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">COVERAGE PROBABILITY COMPARISON&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">20</span>
    <span class="n">true_ps</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]</span>
    <span class="n">n_sim</span> <span class="o">=</span> <span class="mi">5000</span>
    <span class="n">confidence</span> <span class="o">=</span> <span class="mf">0.95</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">, nominal coverage = </span><span class="si">{</span><span class="n">confidence</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;True p&#39;</span><span class="si">:</span><span class="s2">&gt;8</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Wald&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Wilson&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Profile&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">42</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">true_p</span> <span class="ow">in</span> <span class="n">true_ps</span><span class="p">:</span>
        <span class="n">coverage</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;wald&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;wilson&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;profile&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>

        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">true_p</span><span class="p">)</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">binomial_confidence_intervals</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">confidence</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">method</span> <span class="ow">in</span> <span class="n">coverage</span><span class="p">:</span>
                <span class="n">ci</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="n">method</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">ci</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">true_p</span> <span class="o">&lt;=</span> <span class="n">ci</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                    <span class="n">coverage</span><span class="p">[</span><span class="n">method</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">true_p</span><span class="si">:</span><span class="s2">&gt;8.1f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">coverage</span><span class="p">[</span><span class="s1">&#39;wald&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">n_sim</span><span class="si">:</span><span class="s2">&gt;10.3f</span><span class="si">}</span><span class="s2"> &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">coverage</span><span class="p">[</span><span class="s1">&#39;wilson&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">n_sim</span><span class="si">:</span><span class="s2">&gt;10.3f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">coverage</span><span class="p">[</span><span class="s1">&#39;profile&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">n_sim</span><span class="si">:</span><span class="s2">&gt;10.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">→ Wald intervals have poor coverage near p = 0 or p = 1&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;→ Wilson and Profile maintain ~95</span><span class="si">% c</span><span class="s2">overage throughout&quot;</span><span class="p">)</span>

<span class="n">compare_coverage</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>COVERAGE PROBABILITY COMPARISON
============================================================
n = 20, nominal coverage = 0.95

 True p       Wald     Wilson    Profile
------------------------------------------
    0.1      0.891      0.952      0.948
    0.3      0.938      0.951      0.949
    0.5      0.951      0.953      0.951
    0.7      0.939      0.952      0.950
    0.9      0.889      0.951      0.949

→ Wald intervals have poor coverage near p = 0 or p = 1
→ Wilson and Profile maintain ~95% coverage throughout
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (c): Boundary Behavior</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">boundary_behavior</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Examine interval behavior when x = 0.&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">BOUNDARY BEHAVIOR: x = 0 (no successes)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">x</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">result</span> <span class="o">=</span> <span class="n">binomial_confidence_intervals</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Data: x = </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2"> successes in n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2"> trials&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MLE: p̂ = </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;p_hat&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;Method&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;95% CI&#39;</span><span class="si">:</span><span class="s2">&gt;25</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Problem?&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">58</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;wald&#39;</span><span class="p">,</span> <span class="s1">&#39;wilson&#39;</span><span class="p">,</span> <span class="s1">&#39;profile&#39;</span><span class="p">]:</span>
        <span class="n">ci</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="n">method</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;wald&#39;</span><span class="p">:</span>
            <span class="n">problem</span> <span class="o">=</span> <span class="s2">&quot;Width = 0!&quot;</span> <span class="k">if</span> <span class="n">ci</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">ci</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">else</span> <span class="s2">&quot;OK&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">problem</span> <span class="o">=</span> <span class="s2">&quot;OK&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">method</span><span class="o">.</span><span class="n">capitalize</span><span class="p">()</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">ci</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)</span><span class="si">{</span><span class="s1">&#39;&#39;</span><span class="si">:</span><span class="s2">&lt;5</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">problem</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Analysis:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;- Wald: SE = √(0×1/n) = 0, so CI has zero width!&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;- Wilson: Always has positive width due to z²/(4n²) term&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;- Profile: Proper likelihood-based interval&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">→ Never use Wald intervals for proportions near 0 or 1!&quot;</span><span class="p">)</span>

<span class="n">boundary_behavior</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>BOUNDARY BEHAVIOR: x = 0 (no successes)
============================================================
Data: x = 0 successes in n = 10 trials
MLE: p̂ = 0.0000

Method                            95% CI        Problem?
----------------------------------------------------------
Wald            (0.0000, 0.0000)       Width = 0!
Wilson          (0.0000, 0.2775)               OK
Profile         (0.0000, 0.2589)               OK

Analysis:
- Wald: SE = √(0×1/n) = 0, so CI has zero width!
- Wilson: Always has positive width due to z²/(4n²) term
- Profile: Proper likelihood-based interval

→ Never use Wald intervals for proportions near 0 or 1!
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): Reparameterization and Invariance</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">reparameterization_invariance</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare Wald intervals under different parameterizations.&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">REPARAMETERIZATION INVARIANCE&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="n">n</span> <span class="o">=</span> <span class="mi">30</span>
    <span class="n">x</span> <span class="o">=</span> <span class="mi">18</span>  <span class="c1"># p̂ = 0.6</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.975</span><span class="p">)</span>

    <span class="c1"># Direct Wald for p</span>
    <span class="n">p_hat</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="n">n</span>
    <span class="n">se_p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">p_hat</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_hat</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">wald_p</span> <span class="o">=</span> <span class="p">(</span><span class="n">p_hat</span> <span class="o">-</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se_p</span><span class="p">,</span> <span class="n">p_hat</span> <span class="o">+</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se_p</span><span class="p">)</span>

    <span class="c1"># Wald for log-odds ψ = log(p/(1-p))</span>
    <span class="n">psi_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_hat</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_hat</span><span class="p">))</span>
    <span class="c1"># Delta method: Var(ψ̂) ≈ Var(p̂) × [dψ/dp]² = Var(p̂) / [p(1-p)]²</span>
    <span class="n">se_psi</span> <span class="o">=</span> <span class="n">se_p</span> <span class="o">/</span> <span class="p">(</span><span class="n">p_hat</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_hat</span><span class="p">))</span>
    <span class="n">wald_psi</span> <span class="o">=</span> <span class="p">(</span><span class="n">psi_hat</span> <span class="o">-</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se_psi</span><span class="p">,</span> <span class="n">psi_hat</span> <span class="o">+</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se_psi</span><span class="p">)</span>

    <span class="c1"># Transform back to p</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">logit_inv</span><span class="p">(</span><span class="n">psi</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">psi</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">psi</span><span class="p">))</span>

    <span class="n">wald_p_from_psi</span> <span class="o">=</span> <span class="p">(</span><span class="n">logit_inv</span><span class="p">(</span><span class="n">wald_psi</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">logit_inv</span><span class="p">(</span><span class="n">wald_psi</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

    <span class="c1"># Profile likelihood (invariant)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">binomial_confidence_intervals</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">profile_p</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;profile&#39;</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Data: x = </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">, n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">, p̂ = </span><span class="si">{</span><span class="n">p_hat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;Method&#39;</span><span class="si">:</span><span class="s2">&lt;30</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;95% CI for p&#39;</span><span class="si">:</span><span class="s2">&gt;25</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">58</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Wald (direct for p)&#39;</span><span class="si">:</span><span class="s2">&lt;30</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">wald_p</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">wald_p</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Wald (log-odds, transformed)&#39;</span><span class="si">:</span><span class="s2">&lt;30</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">wald_p_from_psi</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">wald_p_from_psi</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Profile likelihood&#39;</span><span class="si">:</span><span class="s2">&lt;30</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">profile_p</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">profile_p</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Observations:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;- Direct Wald is SYMMETRIC around p̂&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;- Wald via log-odds is ASYMMETRIC (respects [0,1] constraint)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;- Log-odds Wald closely matches Profile (both are invariant)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">→ For bounded parameters, work in transformed space!&quot;</span><span class="p">)</span>

<span class="n">reparameterization_invariance</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>REPARAMETERIZATION INVARIANCE
============================================================
Data: x = 18, n = 30, p̂ = 0.6000

Method                                    95% CI for p
----------------------------------------------------------
Wald (direct for p)            (0.4247, 0.7753)
Wald (log-odds, transformed)   (0.4147, 0.7615)
Profile likelihood             (0.4142, 0.7614)

Observations:
- Direct Wald is SYMMETRIC around p̂
- Wald via log-odds is ASYMMETRIC (respects [0,1] constraint)
- Log-odds Wald closely matches Profile (both are invariant)

→ For bounded parameters, work in transformed space!
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Wald intervals fail at boundaries</strong>: When <span class="math notranslate nohighlight">\(\hat{p} = 0\)</span> or <span class="math notranslate nohighlight">\(\hat{p} = 1\)</span>, Wald gives degenerate zero-width intervals.</p></li>
<li><p class="sd-card-text"><strong>Wilson intervals are robust</strong>: The score-based interval maintains coverage even near boundaries.</p></li>
<li><p class="sd-card-text"><strong>Profile likelihood is the gold standard</strong>: Invariant under reparameterization and proper coverage everywhere.</p></li>
<li><p class="sd-card-text"><strong>Transform bounded parameters</strong>: Working in log-odds space and transforming back gives intervals that respect the parameter space and approximate profile likelihood.</p></li>
</ol>
</div>
</details></div>
</section>
<section id="bringing-it-all-together">
<h2>Bringing It All Together<a class="headerlink" href="#bringing-it-all-together" title="Link to this heading"></a></h2>
<p>Maximum likelihood estimation occupies a central position in statistical inference. Its theoretical properties—consistency, asymptotic normality, efficiency—make it the default choice for parametric estimation when sample sizes are moderate to large. The likelihood function itself provides a unified framework for point estimation, interval estimation, and hypothesis testing.</p>
<p>Yet MLE has limitations. For small samples, the asymptotic approximations may be poor; bootstrap methods (<span class="xref std std-ref">Chapter 4</span>) provide an alternative. For complex models with many parameters, regularization (ridge regression, LASSO) may improve prediction even at the cost of some bias. And when prior information is available, Bayesian methods (<span class="xref std std-ref">Chapter 5</span>) provide a principled way to incorporate it.</p>
<p>The next sections extend these ideas. <span class="xref std std-ref">Section 3.3</span> compares MLE with method of moments and Bayesian estimation. <span class="xref std std-ref">Section 3.4</span> develops the sampling distribution theory that underlies our standard error calculations. And <span class="xref std std-ref">Sections 3.6–3.7</span> apply MLE to the linear model and its generalizations—the workhorses of applied statistics.</p>
<div class="important admonition">
<p class="admonition-title">Key Takeaways 📝</p>
<ol class="arabic simple">
<li><p><strong>The likelihood function</strong> <span class="math notranslate nohighlight">\(L(\theta) = \prod f(x_i|\theta)\)</span> measures how well different parameter values explain observed data; the MLE maximizes this function.</p></li>
<li><p><strong>Score and Fisher information</strong>: The score <span class="math notranslate nohighlight">\(U(\theta) = \partial \ell / \partial \theta\)</span> has mean zero at the true parameter; its variance is the Fisher information <span class="math notranslate nohighlight">\(I(\theta)\)</span>, which quantifies the curvature of the likelihood.</p></li>
<li><p><strong>Asymptotic properties</strong>: Under regularity conditions, MLEs are consistent, asymptotically normal with variance <span class="math notranslate nohighlight">\(1/[nI(\theta)]\)</span>, and asymptotically efficient (achieving the Cramér-Rao bound).</p></li>
<li><p><strong>Numerical optimization</strong>: Newton-Raphson and Fisher scoring find MLEs when closed forms don’t exist; <code class="docutils literal notranslate"><span class="pre">scipy.optimize</span></code> provides robust implementations.</p></li>
<li><p><strong>Hypothesis testing</strong>: Likelihood ratio, Wald, and score tests all derive from the likelihood; they are asymptotically equivalent but can differ in finite samples.</p></li>
<li><p><strong>Course alignment</strong>: This section addresses Learning Outcome 2 (parametric inference) and provides computational foundations for LO 1 (simulation for sampling distributions) and LO 4 (Bayesian methods, which share the likelihood foundation).</p></li>
</ol>
</div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<p><strong>Foundational Works by R. A. Fisher</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="fisher1912" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Fisher1912<span class="fn-bracket">]</span></span>
<p>Fisher, R. A. (1912). On an absolute criterion for fitting frequency curves. <em>Messenger of Mathematics</em>, 41, 155–160. Fisher’s earliest work on maximum likelihood, predating his systematic development of the theory.</p>
</div>
<div class="citation" id="fisher1922" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Fisher1922<span class="fn-bracket">]</span></span>
<p>Fisher, R. A. (1922). On the mathematical foundations of theoretical statistics. <em>Philosophical Transactions of the Royal Society A</em>, 222, 309–368. The foundational paper introducing maximum likelihood, sufficiency, efficiency, and consistency—concepts that remain central to statistical inference.</p>
</div>
<div class="citation" id="fisher1925" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Fisher1925<span class="fn-bracket">]</span></span>
<p>Fisher, R. A. (1925). Theory of statistical estimation. <em>Proceedings of the Cambridge Philosophical Society</em>, 22(5), 700–725. Develops the asymptotic theory of maximum likelihood including asymptotic normality and efficiency, introducing Fisher information.</p>
</div>
<div class="citation" id="fisher1934" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Fisher1934<span class="fn-bracket">]</span></span>
<p>Fisher, R. A. (1934). Two new properties of mathematical likelihood. <em>Proceedings of the Royal Society A</em>, 144(852), 285–307. Further development of likelihood theory including the concept of ancillary statistics.</p>
</div>
</div>
<p><strong>The Cramér-Rao Lower Bound</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="rao1945" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Rao1945<span class="fn-bracket">]</span></span>
<p>Rao, C. R. (1945). Information and the accuracy attainable in the estimation of statistical parameters. <em>Bulletin of the Calcutta Mathematical Society</em>, 37, 81–89. Independently establishes the information inequality (Cramér-Rao bound) and introduces the concept of efficient estimators.</p>
</div>
<div class="citation" id="cramer1946" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Cramer1946<span class="fn-bracket">]</span></span>
<p>Cramér, H. (1946). <em>Mathematical Methods of Statistics</em>. Princeton University Press. Classic synthesis of statistical theory including rigorous treatment of the Cramér-Rao inequality and asymptotic theory of estimators.</p>
</div>
<div class="citation" id="darmois1945" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Darmois1945<span class="fn-bracket">]</span></span>
<p>Darmois, G. (1945). Sur les limites de la dispersion de certaines estimations. <em>Revue de l’Institut International de Statistique</em>, 13, 9–15. Independent derivation of the information inequality in the French statistical literature.</p>
</div>
</div>
<p><strong>Asymptotic Theory</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="wald1949" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Wald1949<span class="fn-bracket">]</span></span>
<p>Wald, A. (1949). Note on the consistency of the maximum likelihood estimate. <em>Annals of Mathematical Statistics</em>, 20(4), 595–601. Establishes conditions for consistency of maximum likelihood estimators under general conditions.</p>
</div>
<div class="citation" id="lecam1953" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LeCam1953<span class="fn-bracket">]</span></span>
<p>Le Cam, L. (1953). On some asymptotic properties of maximum likelihood estimates and related Bayes’ estimates. <em>University of California Publications in Statistics</em>, 1, 277–329. Fundamental work on the asymptotic behavior of MLEs establishing local asymptotic normality.</p>
</div>
<div class="citation" id="lecam1970" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LeCam1970<span class="fn-bracket">]</span></span>
<p>Le Cam, L. (1970). On the assumptions used to prove asymptotic normality of maximum likelihood estimates. <em>Annals of Mathematical Statistics</em>, 41(3), 802–828. Clarifies and weakens the regularity conditions required for asymptotic normality of MLEs.</p>
</div>
</div>
<p><strong>Numerical Methods for MLE</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="dennis1996" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Dennis1996<span class="fn-bracket">]</span></span>
<p>Dennis, J. E., and Schnabel, R. B. (1996). <em>Numerical Methods for Unconstrained Optimization and Nonlinear Equations</em>. SIAM. Comprehensive treatment of Newton-Raphson and quasi-Newton methods used in likelihood maximization.</p>
</div>
<div class="citation" id="nocedal2006" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Nocedal2006<span class="fn-bracket">]</span></span>
<p>Nocedal, J., and Wright, S. J. (2006). <em>Numerical Optimization</em> (2nd ed.). Springer. Modern treatment of optimization algorithms including Newton’s method, Fisher scoring, and quasi-Newton methods relevant to MLE computation.</p>
</div>
<div class="citation" id="mclachlan2008" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>McLachlan2008<span class="fn-bracket">]</span></span>
<p>McLachlan, G. J., and Krishnan, T. (2008). <em>The EM Algorithm and Extensions</em> (2nd ed.). Wiley. Definitive reference on the Expectation-Maximization algorithm for MLEs in incomplete data problems.</p>
</div>
</div>
<p><strong>Likelihood Ratio, Wald, and Score Tests</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="wilks1938" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Wilks1938<span class="fn-bracket">]</span></span>
<p>Wilks, S. S. (1938). The large-sample distribution of the likelihood ratio for testing composite hypotheses. <em>Annals of Mathematical Statistics</em>, 9(1), 60–62. Establishes the asymptotic chi-squared distribution of the likelihood ratio statistic under the null hypothesis.</p>
</div>
<div class="citation" id="wald1943" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Wald1943<span class="fn-bracket">]</span></span>
<p>Wald, A. (1943). Tests of statistical hypotheses concerning several parameters when the number of observations is large. <em>Transactions of the American Mathematical Society</em>, 54(3), 426–482. Develops the theory of Wald tests based on asymptotic normality of MLEs.</p>
</div>
<div class="citation" id="rao1948" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Rao1948<span class="fn-bracket">]</span></span>
<p>Rao, C. R. (1948). Large sample tests of statistical hypotheses concerning several parameters with applications to problems of estimation. <em>Proceedings of the Cambridge Philosophical Society</em>, 44(1), 50–57. Introduces the score test (Rao test) based on the score function evaluated at the null hypothesis.</p>
</div>
</div>
<p><strong>Model Misspecification</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="white1982" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>White1982<span class="fn-bracket">]</span></span>
<p>White, H. (1982). Maximum likelihood estimation of misspecified models. <em>Econometrica</em>, 50(1), 1–25. Establishes quasi-maximum likelihood theory showing that MLE converges to the parameter minimizing Kullback-Leibler divergence even when the model is misspecified.</p>
</div>
<div class="citation" id="huber1967" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Huber1967<span class="fn-bracket">]</span></span>
<p>Huber, P. J. (1967). The behavior of maximum likelihood estimates under nonstandard conditions. In <em>Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</em>, Vol. 1, 221–233. University of California Press. Foundational work on robust estimation and behavior of MLEs when model assumptions are violated.</p>
</div>
</div>
<p><strong>Comprehensive Texts</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="lehmann1983" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Lehmann1983<span class="fn-bracket">]</span></span>
<p>Lehmann, E. L. (1983). <em>Theory of Point Estimation</em>. Wiley. (2nd ed. with Casella, 1998, Springer.) Rigorous graduate-level treatment of maximum likelihood and its properties.</p>
</div>
<div class="citation" id="vandervaart1998" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>VanDerVaart1998<span class="fn-bracket">]</span></span>
<p>van der Vaart, A. W. (1998). <em>Asymptotic Statistics</em>. Cambridge University Press. Modern measure-theoretic treatment of asymptotic theory including comprehensive coverage of MLE asymptotics.</p>
</div>
<div class="citation" id="pawitan2001" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Pawitan2001<span class="fn-bracket">]</span></span>
<p>Pawitan, Y. (2001). <em>In All Likelihood: Statistical Modelling and Inference Using Likelihood</em>. Oxford University Press. Accessible treatment of likelihood methods emphasizing practical applications and interpretation.</p>
</div>
</div>
<p><strong>Historical Perspectives</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="stigler1986" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Stigler1986<span class="fn-bracket">]</span></span>
<p>Stigler, S. M. (1986). <em>The History of Statistics: The Measurement of Uncertainty before 1900</em>. Harvard University Press. Historical account of the development of statistical methods including early work on likelihood.</p>
</div>
<div class="citation" id="hald1998" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Hald1998<span class="fn-bracket">]</span></span>
<p>Hald, A. (1998). <em>A History of Mathematical Statistics from 1750 to 1930</em>. Wiley. Detailed historical treatment including Fisher’s development of maximum likelihood theory.</p>
</div>
<div class="citation" id="hand2015" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Hand2015<span class="fn-bracket">]</span></span>
<p>Hand, D. J. (2015). From evidence to understanding: A commentary on Fisher (1922) ‘On the mathematical foundations of theoretical statistics’. <em>Philosophical Transactions of the Royal Society A</em>, 373(2039), 20140249. Modern perspective on Fisher’s foundational 1922 paper and its lasting influence.</p>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="ch3_1-exponential-families.html" class="btn btn-neutral float-left" title="Exponential Families" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ch3_3-sampling-variability.html" class="btn btn-neutral float-right" title="Sampling Variability and Variance Estimation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>