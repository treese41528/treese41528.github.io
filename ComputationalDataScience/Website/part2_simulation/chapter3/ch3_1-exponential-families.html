

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Exponential Families &mdash; STAT 418</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=b32ea9d2" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT418/Website/part2_simulation/chapter3/ch3_1-exponential-families.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=30646c52"></script>
      <script>let toggleHintShow = 'Show solution';</script>
      <script>let toggleHintHide = 'Hide solution';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>window.MathJax = {"options": {"enableMenu": true, "menuOptions": {"settings": {"enrich": true, "speech": true, "braille": true, "collapsible": true, "assistiveMml": false}}}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
      <script src="../../_static/custom.js?v=8718e0ab"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Maximum Likelihood Estimation" href="ch3_2-maximum-likelihood-estimation.html" />
    <link rel="prev" title="Chapter 3: Parametric Inference and Likelihood Methods" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Course Content</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../part1_foundations/index.html">Part I: Foundations of Probability and Computation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part1_foundations/chapter1/index.html">Chapter 1: Statistical Paradigms and Core Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html">Paradigms of Probability and Statistical Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#the-mathematical-foundation-kolmogorov-s-axioms">The Mathematical Foundation: Kolmogorov’s Axioms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#interpretations-of-probability">Interpretations of Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#statistical-inference-paradigms">Statistical Inference Paradigms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#historical-and-philosophical-debates">Historical and Philosophical Debates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#looking-ahead-our-course-focus">Looking Ahead: Our Course Focus</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html">Probability Distributions: Theory and Computation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#from-abstract-foundations-to-concrete-tools">From Abstract Foundations to Concrete Tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#the-python-ecosystem-for-probability">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#introduction-why-probability-distributions-matter">Introduction: Why Probability Distributions Matter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#id1">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#continuous-distributions">Continuous Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#additional-important-distributions">Additional Important Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#summary-and-practical-guidelines">Summary and Practical Guidelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#conclusion">Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html">Python Random Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#from-mathematical-distributions-to-computational-samples">From Mathematical Distributions to Computational Samples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-python-ecosystem-at-a-glance">The Python Ecosystem at a Glance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#understanding-pseudo-random-number-generation">Understanding Pseudo-Random Number Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-standard-library-random-module">The Standard Library: <code class="docutils literal notranslate"><span class="pre">random</span></code> Module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#numpy-fast-vectorized-random-sampling">NumPy: Fast Vectorized Random Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#scipy-stats-the-complete-statistical-toolkit">SciPy Stats: The Complete Statistical Toolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#bringing-it-all-together-library-selection-guide">Bringing It All Together: Library Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#looking-ahead-from-random-numbers-to-monte-carlo-methods">Looking Ahead: From Random Numbers to Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html">Chapter 1 Summary: Foundations in Place</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#the-three-pillars-of-chapter-1">The Three Pillars of Chapter 1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#what-lies-ahead-the-road-to-simulation">What Lies Ahead: The Road to Simulation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#chapter-1-exercises-synthesis-problems">Chapter 1 Exercises: Synthesis Problems</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Part II: Simulation-Based Methods</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../chapter2/index.html">Chapter 2: Monte Carlo Simulation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html">Monte Carlo Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#the-historical-development-of-monte-carlo-methods">The Historical Development of Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#the-core-principle-expectation-as-integration">The Core Principle: Expectation as Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#theoretical-foundations">Theoretical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#variance-estimation-and-confidence-intervals">Variance Estimation and Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#comparison-with-deterministic-methods">Comparison with Deterministic Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#sample-size-determination">Sample Size Determination</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#convergence-diagnostics-and-monitoring">Convergence Diagnostics and Monitoring</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#chapter-2-1-exercises-monte-carlo-fundamentals-mastery">Chapter 2.1 Exercises: Monte Carlo Fundamentals Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html">Uniform Random Variates</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#why-uniform-the-universal-currency-of-randomness">Why Uniform? The Universal Currency of Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#the-paradox-of-computational-randomness">The Paradox of Computational Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#chaotic-dynamical-systems-an-instructive-failure">Chaotic Dynamical Systems: An Instructive Failure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#linear-congruential-generators">Linear Congruential Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#shift-register-generators">Shift-Register Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#the-kiss-generator-combining-strategies">The KISS Generator: Combining Strategies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#modern-generators-mersenne-twister-and-pcg">Modern Generators: Mersenne Twister and PCG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#statistical-testing-of-random-number-generators">Statistical Testing of Random Number Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#chapter-2-2-exercises-uniform-random-variates-mastery">Chapter 2.2 Exercises: Uniform Random Variates Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html">Inverse CDF Method</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#mathematical-foundations">Mathematical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#continuous-distributions-with-closed-form-inverses">Continuous Distributions with Closed-Form Inverses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#numerical-inversion">Numerical Inversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#mixed-distributions">Mixed Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#chapter-2-3-exercises-inverse-cdf-method-mastery">Chapter 2.3 Exercises: Inverse CDF Method Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html">Transformation Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#why-transformation-methods">Why Transformation Methods?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-boxmuller-transform">The Box–Muller Transform</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-polar-marsaglia-method">The Polar (Marsaglia) Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#method-comparison-boxmuller-vs-polar-vs-ziggurat">Method Comparison: Box–Muller vs Polar vs Ziggurat</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-ziggurat-algorithm">The Ziggurat Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-clt-approximation-historical">The CLT Approximation (Historical)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#distributions-derived-from-the-normal">Distributions Derived from the Normal</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#multivariate-normal-generation">Multivariate Normal Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#implementation-guidance">Implementation Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#chapter-2-4-exercises-transformation-methods-mastery">Chapter 2.4 Exercises: Transformation Methods Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html">Rejection Sampling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-dartboard-intuition">The Dartboard Intuition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-accept-reject-algorithm">The Accept-Reject Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#efficiency-analysis">Efficiency Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#choosing-the-proposal-distribution">Choosing the Proposal Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-squeeze-principle">The Squeeze Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#geometric-example-sampling-from-the-unit-disk">Geometric Example: Sampling from the Unit Disk</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#limitations-and-the-curse-of-dimensionality">Limitations and the Curse of Dimensionality</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#connections-to-other-methods">Connections to Other Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#chapter-2-5-exercises-rejection-sampling-mastery">Chapter 2.5 Exercises: Rejection Sampling Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html">Variance Reduction Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#the-variance-reduction-paradigm">The Variance Reduction Paradigm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#importance-sampling">Importance Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#control-variates">Control Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#antithetic-variates">Antithetic Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#stratified-sampling">Stratified Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#common-random-numbers">Common Random Numbers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#conditional-monte-carlo-raoblackwellization">Conditional Monte Carlo (Rao–Blackwellization)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#combining-variance-reduction-techniques">Combining Variance Reduction Techniques</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#chapter-2-6-exercises-variance-reduction-mastery">Chapter 2.6 Exercises: Variance Reduction Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html">Chapter Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#the-complete-monte-carlo-workflow">The Complete Monte Carlo Workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#quick-reference-tables">Quick Reference Tables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#common-pitfalls-checklist">Common Pitfalls Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#connections-to-later-chapters">Connections to Later Chapters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#learning-outcomes-checklist">Learning Outcomes Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#further-reading-optimization-and-missing-data">Further Reading: Optimization and Missing Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Chapter 3: Parametric Inference and Likelihood Methods</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Exponential Families</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#historical-origins-from-scattered-results-to-unified-theory">Historical Origins: From Scattered Results to Unified Theory</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-canonical-exponential-family">The Canonical Exponential Family</a></li>
<li class="toctree-l4"><a class="reference internal" href="#converting-familiar-distributions">Converting Familiar Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-log-partition-function-a-moment-generating-machine">The Log-Partition Function: A Moment-Generating Machine</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sufficiency-capturing-all-parameter-information">Sufficiency: Capturing All Parameter Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="#minimal-sufficiency-and-completeness">Minimal Sufficiency and Completeness</a></li>
<li class="toctree-l4"><a class="reference internal" href="#conjugate-priors-and-bayesian-inference">Conjugate Priors and Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="#exponential-dispersion-models-and-glms">Exponential Dispersion Models and GLMs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#chapter-3-1-exercises-exponential-families-mastery">Chapter 3.1 Exercises: Exponential Families Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html">Maximum Likelihood Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#the-likelihood-function">The Likelihood Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#the-score-function">The Score Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#fisher-information">Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#closed-form-maximum-likelihood-estimators">Closed-Form Maximum Likelihood Estimators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#numerical-optimization-for-mle">Numerical Optimization for MLE</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#asymptotic-properties-of-mles">Asymptotic Properties of MLEs</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#the-cramer-rao-lower-bound">The Cramér-Rao Lower Bound</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#the-invariance-property">The Invariance Property</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#likelihood-based-hypothesis-testing">Likelihood-Based Hypothesis Testing</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#confidence-intervals-from-likelihood">Confidence Intervals from Likelihood</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#connection-to-bayesian-inference">Connection to Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#chapter-3-2-exercises-maximum-likelihood-estimation-mastery">Chapter 3.2 Exercises: Maximum Likelihood Estimation Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch3_3-sampling-variability.html">Sampling Variability and Variance Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#statistical-estimators-and-their-properties">Statistical Estimators and Their Properties</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#sampling-distributions">Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#the-delta-method">The Delta Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#variance-estimation-methods">Variance Estimation Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#applications-and-worked-examples">Applications and Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch3_4-linear-models.html">Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#matrix-calculus-foundations">Matrix Calculus Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#the-linear-model">The Linear Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#ordinary-least-squares-the-calculus-approach">Ordinary Least Squares: The Calculus Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#ordinary-least-squares-the-geometric-approach">Ordinary Least Squares: The Geometric Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#properties-of-the-ols-estimator">Properties of the OLS Estimator</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#the-gauss-markov-theorem">The Gauss-Markov Theorem</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#estimating-the-error-variance">Estimating the Error Variance</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#distributional-results-under-normality">Distributional Results Under Normality</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#diagnostics-and-model-checking">Diagnostics and Model Checking</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#numerical-stability-qr-decomposition">Numerical Stability: QR Decomposition</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#model-selection-and-information-criteria">Model Selection and Information Criteria</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#regularization-ridge-and-lasso">Regularization: Ridge and LASSO</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#chapter-3-4-exercises-linear-models-mastery">Chapter 3.4 Exercises: Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch3_5-generalized-linear-models.html">Generalized Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#historical-context-unification-of-regression-methods">Historical Context: Unification of Regression Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#the-glm-framework-three-components">The GLM Framework: Three Components</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#score-equations-and-fisher-information">Score Equations and Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#iteratively-reweighted-least-squares">Iteratively Reweighted Least Squares</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#logistic-regression-binary-outcomes">Logistic Regression: Binary Outcomes</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#poisson-regression-count-data">Poisson Regression: Count Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#gamma-regression-positive-continuous-data">Gamma Regression: Positive Continuous Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#inference-in-glms-the-testing-triad">Inference in GLMs: The Testing Triad</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#model-diagnostics">Model Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#model-comparison-and-selection">Model Comparison and Selection</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#quasi-likelihood-and-robust-inference">Quasi-Likelihood and Robust Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#further-reading">Further Reading</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#chapter-3-5-exercises-generalized-linear-models-mastery">Chapter 3.5 Exercises: Generalized Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch3_6-chapter-summary.html">Chapter Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#the-parametric-inference-pipeline">The Parametric Inference Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#the-five-pillars-of-chapter-3">The Five Pillars of Chapter 3</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#quick-reference-core-formulas">Quick Reference: Core Formulas</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#connections-to-future-material">Connections to Future Material</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#practical-guidance">Practical Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter4/index.html">Chapter 4: Resampling Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html">The Sampling Distribution Problem</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#the-fundamental-target-sampling-distributions">The Fundamental Target: Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#historical-development-the-quest-for-sampling-distributions">Historical Development: The Quest for Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#three-routes-to-the-sampling-distribution">Three Routes to the Sampling Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#when-asymptotics-fail-motivating-the-bootstrap">When Asymptotics Fail: Motivating the Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#the-plug-in-principle-theoretical-foundation">The Plug-In Principle: Theoretical Foundation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#computational-perspective-bootstrap-as-monte-carlo">Computational Perspective: Bootstrap as Monte Carlo</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#chapter-4-1-exercises">Chapter 4.1 Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html">The Empirical Distribution and Plug-in Principle</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#the-empirical-cumulative-distribution-function">The Empirical Cumulative Distribution Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#convergence-of-the-empirical-cdf">Convergence of the Empirical CDF</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#parameters-as-statistical-functionals">Parameters as Statistical Functionals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#when-the-plug-in-principle-fails">When the Plug-in Principle Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#the-bootstrap-idea-in-one-sentence">The Bootstrap Idea in One Sentence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#computational-implementation">Computational Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#section-4-2-exercises-ecdf-and-plug-in-mastery">Section 4.2 Exercises: ECDF and Plug-in Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html">The Nonparametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#the-bootstrap-principle">The Bootstrap Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-standard-errors">Bootstrap Standard Errors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-bias-estimation">Bootstrap Bias Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-confidence-intervals">Bootstrap Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-for-regression">Bootstrap for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-diagnostics">Bootstrap Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#when-bootstrap-fails">When Bootstrap Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html">Section 4.4: The Parametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#the-parametric-bootstrap-principle">The Parametric Bootstrap Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#location-scale-families">Location-Scale Families</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#parametric-bootstrap-for-regression">Parametric Bootstrap for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#confidence-intervals">Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#model-checking-and-validation">Model Checking and Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#when-parametric-bootstrap-fails">When Parametric Bootstrap Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#parametric-vs-nonparametric-a-decision-framework">Parametric vs. Nonparametric: A Decision Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part3_bayesian/index.html">Part III: Bayesian Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part3_bayesian/index.html#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html">Bayesian Philosophy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Part II: Simulation-Based Methods</a></li>
          <li class="breadcrumb-item"><a href="index.html">Chapter 3: Parametric Inference and Likelihood Methods</a></li>
      <li class="breadcrumb-item active">Exponential Families</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/part2_simulation/chapter3/ch3_1-exponential-families.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="exponential-families">
<span id="ch3-1-exponential-families"></span><h1>Exponential Families<a class="headerlink" href="#exponential-families" title="Link to this heading"></a></h1>
<p>In Chapter 1, we encountered probability distributions as individual entities: the Bernoulli for binary outcomes, the Poisson for rare events, the Normal for measurement errors, the Gamma for waiting times. Each distribution had its own parameterization, its own moment formulas, its own special properties. We derived means and variances separately for each, proved limit theorems case by case, and implemented each distribution independently in Python. This approach, while pedagogically sound, obscures a profound truth: <strong>most distributions used in statistical practice are special cases of a single unifying framework</strong>.</p>
<p>That framework is the <strong>exponential family</strong>—a class of distributions that encompasses the Bernoulli, Binomial, Poisson, Normal, Exponential, Gamma, Beta, Multinomial, and many others under one elegant mathematical structure. Understanding this unification is not mere mathematical aesthetics; it has immediate practical consequences:</p>
<ol class="arabic simple">
<li><p><strong>Unified inference algorithms</strong>: Maximum likelihood estimation, Fisher scoring, and the iteratively reweighted least squares (IRLS) algorithm work identically for <em>all</em> exponential family members. Write the algorithm once, apply it everywhere.</p></li>
<li><p><strong>Automatic sufficiency</strong>: Exponential families come with natural sufficient statistics—functions of the data that capture all information about parameters. No need to derive sufficiency distribution by distribution.</p></li>
<li><p><strong>Moments from a single function</strong>: The mean, variance, and all higher moments of the sufficient statistic can be extracted by differentiating one function (the log-partition function). This remarkable property eliminates tedious case-by-case moment derivations.</p></li>
<li><p><strong>Conjugate Bayesian inference</strong>: Exponential families have natural conjugate priors, making Bayesian updating computationally tractable and analytically elegant.</p></li>
<li><p><strong>Generalized Linear Models</strong>: The entire theory of GLMs—logistic regression, Poisson regression, Gamma regression—rests on the exponential family foundation. Understanding exponential families means understanding how and why GLMs work.</p></li>
</ol>
<p>This chapter begins our study of <strong>parametric inference</strong>: given data, how do we learn about the parameters of a probability model? The exponential family provides the scaffolding for everything that follows. Section 3.2 develops maximum likelihood estimation, which takes a particularly elegant form for exponential families. Section 3.7 builds generalized linear models atop the exponential dispersion family structure introduced here.</p>
<div class="important admonition">
<p class="admonition-title">Road Map 🧭</p>
<ul class="simple">
<li><p><strong>Understand</strong>: The canonical exponential family form and why it matters for statistical inference</p></li>
<li><p><strong>Develop</strong>: Ability to convert common distributions to exponential family form and extract their components</p></li>
<li><p><strong>Implement</strong>: Python code to work with exponential family representations programmatically</p></li>
<li><p><strong>Evaluate</strong>: Distinguish full-rank versus curved exponential families and understand their inferential consequences</p></li>
</ul>
</div>
<section id="historical-origins-from-scattered-results-to-unified-theory">
<h2>Historical Origins: From Scattered Results to Unified Theory<a class="headerlink" href="#historical-origins-from-scattered-results-to-unified-theory" title="Link to this heading"></a></h2>
<p>The exponential family did not emerge from a single discovery but rather from the gradual recognition that disparate results shared common structure. The story begins with <strong>Ronald A. Fisher</strong> in the 1920s, who noticed that certain distributions possessed a remarkable property: a finite-dimensional statistic could capture all information about the parameters regardless of sample size.</p>
<section id="fisher-s-notion-of-sufficiency">
<h3>Fisher’s Notion of Sufficiency<a class="headerlink" href="#fisher-s-notion-of-sufficiency" title="Link to this heading"></a></h3>
<p>In his seminal 1922 paper “On the Mathematical Foundations of Theoretical Statistics,” Fisher introduced the concept of <strong>sufficiency</strong>. He observed that for normally distributed data with unknown mean <span class="math notranslate nohighlight">\(\mu\)</span>, the sample mean <span class="math notranslate nohighlight">\(\bar{X}\)</span> contains all information about <span class="math notranslate nohighlight">\(\mu\)</span>—knowing the individual observations <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span> provides no additional information beyond <span class="math notranslate nohighlight">\(\bar{X}\)</span>. Similarly, for Poisson data, the sample sum <span class="math notranslate nohighlight">\(\sum X_i\)</span> is sufficient for the rate parameter <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<p>But which distributions possess sufficient statistics? Fisher’s factorization theorem (proved rigorously by Jerzy Neyman in 1935) answered this question, and the answer pointed directly at the exponential family: <strong>a distribution has a fixed-dimension sufficient statistic (independent of sample size) if and only if it belongs to the exponential family</strong>.</p>
</section>
<section id="the-koopman-pitman-darmois-theorem">
<h3>The Koopman-Pitman-Darmois Theorem<a class="headerlink" href="#the-koopman-pitman-darmois-theorem" title="Link to this heading"></a></h3>
<p>In 1935-1936, three mathematicians—Bernard Koopman, E.J.G. Pitman, and Georges Darmois—independently proved a remarkable characterization theorem. Consider distributions with the property that, for any sample size <span class="math notranslate nohighlight">\(n\)</span>, there exists a sufficient statistic of fixed dimension <span class="math notranslate nohighlight">\(k\)</span> (not growing with <span class="math notranslate nohighlight">\(n\)</span>). The Koopman-Pitman-Darmois theorem states that <strong>under mild regularity conditions, such distributions must be exponential families</strong>.</p>
<p>This theorem explains why exponential families dominate statistical practice: they are essentially the <em>only</em> distributions admitting tractable sufficient statistics. Nature could have given us distributions where inference grows arbitrarily complex with sample size. Instead, the distributions that arise naturally—binomial, Poisson, normal, gamma—all belong to the exponential family and enjoy fixed-dimensional sufficiency.</p>
</section>
<section id="modern-formalization">
<h3>Modern Formalization<a class="headerlink" href="#modern-formalization" title="Link to this heading"></a></h3>
<p>The measure-theoretic formalization of exponential families was completed in the 1950s and 1960s, with major contributions from Erich Lehmann, Lawrence Brown, and Ole Barndorff-Nielsen. Today, the exponential family serves as the foundation for:</p>
<ul class="simple">
<li><p>Classical parametric inference (point estimation, hypothesis testing)</p></li>
<li><p>Generalized linear models (Nelder &amp; Wedderburn, 1972)</p></li>
<li><p>Bayesian conjugate analysis</p></li>
<li><p>Information geometry (Amari, 1985)</p></li>
<li><p>Modern machine learning (exponential family embeddings, variational inference)</p></li>
</ul>
</section>
</section>
<section id="the-canonical-exponential-family">
<h2>The Canonical Exponential Family<a class="headerlink" href="#the-canonical-exponential-family" title="Link to this heading"></a></h2>
<p>We now present the mathematical definition of the exponential family. The framework may seem abstract at first, but we will immediately connect it to the familiar distributions from Chapter 1.</p>
<section id="definition-and-components">
<h3>Definition and Components<a class="headerlink" href="#definition-and-components" title="Link to this heading"></a></h3>
<div class="note admonition">
<p class="admonition-title">Definition: Canonical Exponential Family</p>
<p>A <strong>canonical s-parameter exponential family</strong> is a collection of probability distributions with densities (or probability mass functions) of the form:</p>
<div class="math notranslate nohighlight" id="equation-exp-family-def">
<span class="eqno">(31)<a class="headerlink" href="#equation-exp-family-def" title="Link to this equation"></a></span>\[p_\eta(x) = h(x) \exp\left\{ \eta^T T(x) - A(\eta) \right\}\]</div>
<p>with respect to a dominating measure <span class="math notranslate nohighlight">\(\mu\)</span> (Lebesgue measure for continuous distributions, counting measure for discrete). The components are:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(T: \mathcal{X} \to \mathbb{R}^s\)</span> — the <strong>sufficient statistic</strong>, a function mapping observations to an s-dimensional vector</p></li>
<li><p><span class="math notranslate nohighlight">\(\eta \in \Xi \subseteq \mathbb{R}^s\)</span> — the <strong>natural parameter</strong> (or canonical parameter)</p></li>
<li><p><span class="math notranslate nohighlight">\(A: \Xi \to \mathbb{R}\)</span> — the <strong>log-partition function</strong> (or cumulant function), ensuring the density integrates to one</p></li>
<li><p><span class="math notranslate nohighlight">\(h: \mathcal{X} \to [0, \infty)\)</span> — the <strong>carrier density</strong> (or base measure density), independent of <span class="math notranslate nohighlight">\(\eta\)</span></p></li>
</ul>
</div>
<p>The representation <a class="reference internal" href="#equation-exp-family-def">(31)</a> may look unfamiliar, but every distribution from Chapter 1 can be written in this form. The genius of the exponential family is that once a distribution is expressed canonically, all its inferential properties follow from general theorems—no case-by-case analysis needed.</p>
</section>
<section id="the-natural-parameter-space">
<h3>The Natural Parameter Space<a class="headerlink" href="#the-natural-parameter-space" title="Link to this heading"></a></h3>
<p>The <strong>natural parameter space</strong> <span class="math notranslate nohighlight">\(\Xi\)</span> is the set of all <span class="math notranslate nohighlight">\(\eta\)</span> values for which the density is well-defined:</p>
<div class="math notranslate nohighlight">
\[\Xi = \left\{ \eta \in \mathbb{R}^s : A(\eta) = \log \int h(x) \exp\left\{ \eta^T T(x) \right\} d\mu(x) &lt; \infty \right\}\]</div>
<p>A fundamental property is that <strong>the natural parameter space is always convex</strong>. This follows from Hölder’s inequality:</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Convexity of Natural Parameter Space</p>
<p>For any exponential family, the natural parameter space <span class="math notranslate nohighlight">\(\Xi\)</span> is a convex set.</p>
<p><strong>Proof</strong>: Let <span class="math notranslate nohighlight">\(\eta_1, \eta_2 \in \Xi\)</span> and <span class="math notranslate nohighlight">\(\lambda \in (0,1)\)</span>. We must show <span class="math notranslate nohighlight">\(\lambda \eta_1 + (1-\lambda)\eta_2 \in \Xi\)</span>. Using Hölder’s inequality with exponents <span class="math notranslate nohighlight">\(1/\lambda\)</span> and <span class="math notranslate nohighlight">\(1/(1-\lambda)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\int h(x) e^{(\lambda \eta_1 + (1-\lambda)\eta_2)^T T(x)} d\mu
&amp;= \int h(x)^\lambda h(x)^{1-\lambda} e^{\lambda \eta_1^T T(x)} e^{(1-\lambda)\eta_2^T T(x)} d\mu \\
&amp;\leq \left( \int h(x) e^{\eta_1^T T(x)} d\mu \right)^\lambda \left( \int h(x) e^{\eta_2^T T(x)} d\mu \right)^{1-\lambda} \\
&amp;= e^{\lambda A(\eta_1)} \cdot e^{(1-\lambda) A(\eta_2)} &lt; \infty\end{split}\]</div>
<p>Therefore <span class="math notranslate nohighlight">\(A(\lambda \eta_1 + (1-\lambda)\eta_2) &lt; \infty\)</span>, proving <span class="math notranslate nohighlight">\(\lambda \eta_1 + (1-\lambda)\eta_2 \in \Xi\)</span>. ∎</p>
</div>
<p>The convexity of <span class="math notranslate nohighlight">\(\Xi\)</span> has profound computational consequences: optimization over the natural parameter space has no local minima, and the log-likelihood function is concave.</p>
<p>The conceptual power of the exponential family becomes apparent when we visualize how diverse distributions—discrete and continuous, bounded and unbounded—all conform to the same mathematical template. Figure 3.1 displays four canonical examples: the Bernoulli, Poisson, Normal, and Exponential distributions. Despite their apparent differences, each panel shows the same underlying structure with its specific natural parameter <span class="math notranslate nohighlight">\(\eta\)</span>, sufficient statistic <span class="math notranslate nohighlight">\(T(x)\)</span>, and log-partition function <span class="math notranslate nohighlight">\(A(\eta)\)</span>.</p>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_1_fig01_exponential_family_unification.png"><img alt="Four distributions (Bernoulli, Poisson, Normal, Exponential) shown with their exponential family components" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_1_fig01_exponential_family_unification.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 78 </span><span class="caption-text"><strong>Figure 3.1</strong>: The Exponential Family Unifies Diverse Distributions. Four apparently different distributions—Bernoulli for binary outcomes, Poisson for counts, Normal for continuous measurements, and Exponential for waiting times—all share the canonical form <span class="math notranslate nohighlight">\(p_\eta(x) = h(x)\exp\{\eta^T T(x) - A(\eta)\}\)</span>. Each panel displays the distribution for several parameter values and lists its exponential family components.</span><a class="headerlink" href="#id1" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="converting-familiar-distributions">
<h2>Converting Familiar Distributions<a class="headerlink" href="#converting-familiar-distributions" title="Link to this heading"></a></h2>
<p>Let us now verify that the distributions from Chapter 1 fit the exponential family template. For each, we identify the natural parameter <span class="math notranslate nohighlight">\(\eta\)</span>, sufficient statistic <span class="math notranslate nohighlight">\(T(x)\)</span>, log-partition function <span class="math notranslate nohighlight">\(A(\eta)\)</span>, and carrier density <span class="math notranslate nohighlight">\(h(x)\)</span>.</p>
<section id="bernoulli-distribution">
<h3>Bernoulli Distribution<a class="headerlink" href="#bernoulli-distribution" title="Link to this heading"></a></h3>
<p>The Bernoulli(<span class="math notranslate nohighlight">\(p\)</span>) distribution has PMF:</p>
<div class="math notranslate nohighlight">
\[P(X = x) = p^x (1-p)^{1-x} = (1-p) \cdot \left(\frac{p}{1-p}\right)^x \quad \text{for } x \in \{0, 1\}\]</div>
<p>Taking the exponential form:</p>
<div class="math notranslate nohighlight">
\[P(X = x) = \exp\left\{ x \log\frac{p}{1-p} + \log(1-p) \right\}\]</div>
<p>Comparing with <a class="reference internal" href="#equation-exp-family-def">(31)</a>:</p>
<ul class="simple">
<li><p><strong>Natural parameter</strong>: <span class="math notranslate nohighlight">\(\eta = \log\frac{p}{1-p}\)</span> (the log-odds)</p></li>
<li><p><strong>Sufficient statistic</strong>: <span class="math notranslate nohighlight">\(T(x) = x\)</span></p></li>
<li><p><strong>Log-partition function</strong>: <span class="math notranslate nohighlight">\(A(\eta) = -\log(1-p) = \log(1 + e^\eta)\)</span></p></li>
<li><p><strong>Carrier density</strong>: <span class="math notranslate nohighlight">\(h(x) = 1\)</span></p></li>
</ul>
<p>The natural parameter <span class="math notranslate nohighlight">\(\eta\)</span> ranges over all of <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>, with <span class="math notranslate nohighlight">\(p = e^\eta/(1+e^\eta)\)</span> (the logistic function). The log-partition function <span class="math notranslate nohighlight">\(A(\eta) = \log(1+e^\eta)\)</span> is known as the <strong>softplus function</strong> in machine learning.</p>
</section>
<section id="poisson-distribution">
<h3>Poisson Distribution<a class="headerlink" href="#poisson-distribution" title="Link to this heading"></a></h3>
<p>The Poisson(<span class="math notranslate nohighlight">\(\lambda\)</span>) distribution has PMF:</p>
<div class="math notranslate nohighlight">
\[P(X = x) = \frac{\lambda^x e^{-\lambda}}{x!} = \frac{1}{x!} \exp\left\{ x \log\lambda - \lambda \right\}\]</div>
<p>Comparing with <a class="reference internal" href="#equation-exp-family-def">(31)</a>:</p>
<ul class="simple">
<li><p><strong>Natural parameter</strong>: <span class="math notranslate nohighlight">\(\eta = \log\lambda\)</span></p></li>
<li><p><strong>Sufficient statistic</strong>: <span class="math notranslate nohighlight">\(T(x) = x\)</span></p></li>
<li><p><strong>Log-partition function</strong>: <span class="math notranslate nohighlight">\(A(\eta) = \lambda = e^\eta\)</span></p></li>
<li><p><strong>Carrier density</strong>: <span class="math notranslate nohighlight">\(h(x) = 1/x!\)</span></p></li>
</ul>
<p>The natural parameter space is <span class="math notranslate nohighlight">\(\Xi = \mathbb{R}\)</span> (since <span class="math notranslate nohighlight">\(e^\eta &gt; 0\)</span> for all <span class="math notranslate nohighlight">\(\eta\)</span>).</p>
</section>
<section id="normal-distribution">
<h3>Normal Distribution<a class="headerlink" href="#normal-distribution" title="Link to this heading"></a></h3>
<p>The Normal(<span class="math notranslate nohighlight">\(\mu, \sigma^2\)</span>) distribution has PDF:</p>
<div class="math notranslate nohighlight">
\[f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left\{ -\frac{(x-\mu)^2}{2\sigma^2} \right\}\]</div>
<p>Expanding the quadratic:</p>
<div class="math notranslate nohighlight">
\[f(x) = \frac{1}{\sqrt{2\pi}} \exp\left\{ \frac{\mu}{\sigma^2} x - \frac{1}{2\sigma^2} x^2 - \frac{\mu^2}{2\sigma^2} - \frac{1}{2}\log\sigma^2 \right\}\]</div>
<p>This is a <strong>two-parameter</strong> exponential family:</p>
<ul class="simple">
<li><p><strong>Natural parameters</strong>: <span class="math notranslate nohighlight">\(\eta = (\eta_1, \eta_2) = \left(\frac{\mu}{\sigma^2}, -\frac{1}{2\sigma^2}\right)\)</span></p></li>
<li><p><strong>Sufficient statistic</strong>: <span class="math notranslate nohighlight">\(T(x) = (x, x^2)\)</span></p></li>
<li><p><strong>Log-partition function</strong>: <span class="math notranslate nohighlight">\(A(\eta) = -\frac{\eta_1^2}{4\eta_2} + \frac{1}{2}\log\left(-\frac{\pi}{\eta_2}\right)\)</span></p></li>
<li><p><strong>Carrier density</strong>: <span class="math notranslate nohighlight">\(h(x) = 1\)</span></p></li>
</ul>
<p>Note that we need <span class="math notranslate nohighlight">\(\eta_2 &lt; 0\)</span> for the variance to be positive, so <span class="math notranslate nohighlight">\(\Xi = \mathbb{R} \times (-\infty, 0)\)</span>.</p>
</section>
<section id="exponential-distribution">
<h3>Exponential Distribution<a class="headerlink" href="#exponential-distribution" title="Link to this heading"></a></h3>
<p>The Exponential(<span class="math notranslate nohighlight">\(\lambda\)</span>) distribution (rate parameterization) has PDF:</p>
<div class="math notranslate nohighlight">
\[f(x) = \lambda e^{-\lambda x} = \exp\left\{ -\lambda x + \log\lambda \right\} \quad \text{for } x \geq 0\]</div>
<p>Comparing with <a class="reference internal" href="#equation-exp-family-def">(31)</a>:</p>
<ul class="simple">
<li><p><strong>Natural parameter</strong>: <span class="math notranslate nohighlight">\(\eta = -\lambda\)</span></p></li>
<li><p><strong>Sufficient statistic</strong>: <span class="math notranslate nohighlight">\(T(x) = x\)</span></p></li>
<li><p><strong>Log-partition function</strong>: <span class="math notranslate nohighlight">\(A(\eta) = -\log(-\eta) = -\log\lambda\)</span></p></li>
<li><p><strong>Carrier density</strong>: <span class="math notranslate nohighlight">\(h(x) = \mathbf{1}_{x \geq 0}\)</span></p></li>
</ul>
<p>The natural parameter space is <span class="math notranslate nohighlight">\(\Xi = (-\infty, 0)\)</span>.</p>
</section>
<section id="gamma-distribution">
<h3>Gamma Distribution<a class="headerlink" href="#gamma-distribution" title="Link to this heading"></a></h3>
<p>The Gamma(<span class="math notranslate nohighlight">\(\alpha, \beta\)</span>) distribution (shape-rate parameterization) has PDF:</p>
<div class="math notranslate nohighlight">
\[f(x) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-\beta x} \quad \text{for } x &gt; 0\]</div>
<p>Rewriting:</p>
<div class="math notranslate nohighlight">
\[f(x) = \frac{x^{\alpha-1}}{\Gamma(\alpha)} \exp\left\{ -\beta x + \alpha\log\beta \right\}\]</div>
<p>This is a <strong>two-parameter</strong> family, but the shape <span class="math notranslate nohighlight">\(\alpha\)</span> appears in both the carrier density and the log-partition function. If we treat <span class="math notranslate nohighlight">\(\alpha\)</span> as known:</p>
<ul class="simple">
<li><p><strong>Natural parameter</strong>: <span class="math notranslate nohighlight">\(\eta = -\beta\)</span></p></li>
<li><p><strong>Sufficient statistic</strong>: <span class="math notranslate nohighlight">\(T(x) = x\)</span></p></li>
<li><p><strong>Log-partition function</strong>: <span class="math notranslate nohighlight">\(A(\eta) = \alpha\log(-1/\eta) = -\alpha\log(-\eta)\)</span></p></li>
<li><p><strong>Carrier density</strong>: <span class="math notranslate nohighlight">\(h(x) = x^{\alpha-1}/\Gamma(\alpha)\)</span></p></li>
</ul>
<p>The natural parameter space is <span class="math notranslate nohighlight">\(\Xi = (-\infty, 0)\)</span>.</p>
</section>
<section id="complete-exponential-family-table">
<h3>Complete Exponential Family Table<a class="headerlink" href="#complete-exponential-family-table" title="Link to this heading"></a></h3>
<p>The following table summarizes the exponential family representations of common distributions:</p>
<table class="docutils align-default" id="id2">
<caption><span class="caption-number">Table 23 </span><span class="caption-text">Exponential Family Representations</span><a class="headerlink" href="#id2" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 25.0%" />
<col style="width: 15.0%" />
<col style="width: 25.0%" />
<col style="width: 15.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Distribution</p></th>
<th class="head"><p>Natural Parameter <span class="math notranslate nohighlight">\(\eta\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(T(x)\)</span></p></th>
<th class="head"><p>Log-Partition <span class="math notranslate nohighlight">\(A(\eta)\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(h(x)\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Bernoulli(<span class="math notranslate nohighlight">\(p\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\log\frac{p}{1-p}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\log(1 + e^\eta)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Binomial(<span class="math notranslate nohighlight">\(n,p\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\log\frac{p}{1-p}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n\log(1 + e^\eta)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\binom{n}{x}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Poisson(<span class="math notranslate nohighlight">\(\lambda\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\log\lambda\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(e^\eta\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1/x!\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Exponential(<span class="math notranslate nohighlight">\(\lambda\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(-\lambda\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\log(-\eta)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{1}_{x \geq 0}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Normal(<span class="math notranslate nohighlight">\(\mu,\sigma^2\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\left(\frac{\mu}{\sigma^2}, -\frac{1}{2\sigma^2}\right)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((x, x^2)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\frac{\eta_1^2}{4\eta_2} + \frac{1}{2}\log\left(-\frac{\pi}{\eta_2}\right)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Gamma(<span class="math notranslate nohighlight">\(\alpha,\beta\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(-\beta\)</span> (<span class="math notranslate nohighlight">\(\alpha\)</span> fixed)</p></td>
<td><p><span class="math notranslate nohighlight">\(x\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\alpha\log(-\eta)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{x^{\alpha-1}}{\Gamma(\alpha)}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Beta(<span class="math notranslate nohighlight">\(\alpha,\beta\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\((\alpha-1, \beta-1)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((\log x, \log(1-x))\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\log B(\eta_1+1, \eta_2+1)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Multinomial(<span class="math notranslate nohighlight">\(n,\pi\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\log\frac{\pi_i}{\pi_K}\)</span> for <span class="math notranslate nohighlight">\(i&lt;K\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((x_1,\ldots,x_{K-1})\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n\log\left(1 + \sum_{i=1}^{K-1} e^{\eta_i}\right)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{n!}{\prod x_i!}\)</span></p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="the-log-partition-function-a-moment-generating-machine">
<h2>The Log-Partition Function: A Moment-Generating Machine<a class="headerlink" href="#the-log-partition-function-a-moment-generating-machine" title="Link to this heading"></a></h2>
<p>The log-partition function <span class="math notranslate nohighlight">\(A(\eta)\)</span> is far more than a normalizing constant—it is a <strong>complete summary of the distribution’s moments</strong>. This remarkable property allows us to compute means, variances, and higher moments by simple differentiation, avoiding the integral calculations that were necessary in Chapter 1.</p>
<section id="first-derivative-the-mean">
<h3>First Derivative: The Mean<a class="headerlink" href="#first-derivative-the-mean" title="Link to this heading"></a></h3>
<div class="note admonition">
<p class="admonition-title">Theorem: Mean of Sufficient Statistic</p>
<p>For an exponential family with log-partition function <span class="math notranslate nohighlight">\(A(\eta)\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-mean-from-a">
<span class="eqno">(32)<a class="headerlink" href="#equation-mean-from-a" title="Link to this equation"></a></span>\[\nabla A(\eta) = \mathbb{E}_\eta[T(X)]\]</div>
<p>That is, the gradient of <span class="math notranslate nohighlight">\(A\)</span> equals the expected value of the sufficient statistic.</p>
</div>
<p><strong>Proof</strong>: By definition of the log-partition function:</p>
<div class="math notranslate nohighlight">
\[e^{A(\eta)} = \int h(x) \exp\left\{ \eta^T T(x) \right\} d\mu(x)\]</div>
<p>Differentiating both sides with respect to <span class="math notranslate nohighlight">\(\eta_j\)</span>:</p>
<div class="math notranslate nohighlight">
\[e^{A(\eta)} \frac{\partial A}{\partial \eta_j} = \int h(x) T_j(x) \exp\left\{ \eta^T T(x) \right\} d\mu(x)\]</div>
<p>The right side equals <span class="math notranslate nohighlight">\(e^{A(\eta)} \mathbb{E}_\eta[T_j(X)]\)</span>, since:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}_\eta[T_j(X)] = \int T_j(x) \cdot h(x) \exp\left\{ \eta^T T(x) - A(\eta) \right\} d\mu(x)\]</div>
<p>Dividing both sides by <span class="math notranslate nohighlight">\(e^{A(\eta)}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial A}{\partial \eta_j}(\eta) = \mathbb{E}_\eta[T_j(X)]\]</div>
<p>In vector notation: <span class="math notranslate nohighlight">\(\nabla A(\eta) = \mathbb{E}_\eta[T(X)]\)</span>. ∎</p>
<div class="note admonition">
<p class="admonition-title">Technical Note 📚</p>
<p>The interchange of differentiation and integration is justified by dominated convergence on the interior of <span class="math notranslate nohighlight">\(\Xi\)</span>. For a rigorous treatment, see Theorem 2.4 in Keener’s <em>Theoretical Statistics</em> or Brown (1986).</p>
</div>
</section>
<section id="second-derivative-the-variance">
<h3>Second Derivative: The Variance<a class="headerlink" href="#second-derivative-the-variance" title="Link to this heading"></a></h3>
<div class="note admonition">
<p class="admonition-title">Theorem: Variance of Sufficient Statistic</p>
<p>For an exponential family with log-partition function <span class="math notranslate nohighlight">\(A(\eta)\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-var-from-a">
<span class="eqno">(33)<a class="headerlink" href="#equation-var-from-a" title="Link to this equation"></a></span>\[\nabla^2 A(\eta) = \text{Var}_\eta(T(X))\]</div>
<p>That is, the Hessian matrix of <span class="math notranslate nohighlight">\(A\)</span> equals the variance-covariance matrix of the sufficient statistic.</p>
</div>
<p><strong>Proof</strong>: Differentiating <a class="reference internal" href="#equation-mean-from-a">(32)</a> again with respect to <span class="math notranslate nohighlight">\(\eta_k\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial^2 A}{\partial \eta_j \partial \eta_k} = \frac{\partial}{\partial \eta_k} \mathbb{E}_\eta[T_j(X)]\]</div>
<p>Computing the right side requires differentiating the expectation. After calculation (similar to the mean derivation):</p>
<div class="math notranslate nohighlight">
\[\frac{\partial^2 A}{\partial \eta_j \partial \eta_k} = \mathbb{E}_\eta[T_j(X) T_k(X)] - \mathbb{E}_\eta[T_j(X)] \mathbb{E}_\eta[T_k(X)] = \text{Cov}_\eta(T_j(X), T_k(X))\]</div>
<p>In matrix notation: <span class="math notranslate nohighlight">\(\nabla^2 A(\eta) = \text{Var}_\eta(T(X))\)</span>. ∎</p>
<p>This result has a profound consequence: <strong>the Hessian of the log-partition function is always positive semi-definite</strong> (since it’s a variance matrix), which means <strong>A(η) is convex</strong>. Combined with the convexity of the natural parameter space, this makes exponential family inference computationally tractable.</p>
<div class="note admonition">
<p class="admonition-title">Example 💡 Poisson Moments from the Log-Partition Function</p>
<p><strong>Given</strong>: Poisson(<span class="math notranslate nohighlight">\(\lambda\)</span>) with natural parameter <span class="math notranslate nohighlight">\(\eta = \log\lambda\)</span> and log-partition <span class="math notranslate nohighlight">\(A(\eta) = e^\eta\)</span>.</p>
<p><strong>Find</strong>: <span class="math notranslate nohighlight">\(\mathbb{E}[X]\)</span> and <span class="math notranslate nohighlight">\(\text{Var}(X)\)</span> using derivatives of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p><strong>Solution</strong>:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[X] = \frac{dA}{d\eta} = \frac{d}{d\eta} e^\eta = e^\eta = \lambda\]</div>
<div class="math notranslate nohighlight">
\[\text{Var}(X) = \frac{d^2A}{d\eta^2} = \frac{d^2}{d\eta^2} e^\eta = e^\eta = \lambda\]</div>
<p>We recover the familiar result <span class="math notranslate nohighlight">\(\mathbb{E}[X] = \text{Var}(X) = \lambda\)</span> for the Poisson, derived in Chapter 1 through direct summation. The exponential family approach required only differentiation!</p>
</div>
<p>Figure 3.2 visualizes this remarkable property for the Poisson distribution. Panel (a) shows the log-partition function <span class="math notranslate nohighlight">\(A(\eta) = e^\eta\)</span> as a function of the natural parameter. Panels (b) and (c) demonstrate that its first derivative gives the mean and its second derivative gives the variance, with simulated samples confirming the theoretical curves.</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_1_fig02_log_partition_moments.png"><img alt="Log-partition function for Poisson showing mean and variance from derivatives" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_1_fig02_log_partition_moments.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 79 </span><span class="caption-text"><strong>Figure 3.2</strong>: The Log-Partition Function as Moment Generator (Poisson). For the Poisson distribution with <span class="math notranslate nohighlight">\(A(\eta) = e^\eta\)</span>, (a) shows the log-partition function, (b) demonstrates that <span class="math notranslate nohighlight">\(A'(\eta) = e^\eta = \lambda = \mathbb{E}[X]\)</span>, and (c) shows <span class="math notranslate nohighlight">\(A''(\eta) = e^\eta = \lambda = \text{Var}(X)\)</span>. Red circles mark theoretical values; orange/blue squares show sample estimates from 5,000 simulated observations, confirming the theory.</span><a class="headerlink" href="#id3" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>The same principle applies to continuous distributions. Figure 3.3 demonstrates the moment-from-derivative property for the Exponential distribution, where the functional forms of <span class="math notranslate nohighlight">\(A(\eta)\)</span> and its derivatives differ but the fundamental relationship remains identical.</p>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_1_fig02b_log_partition_exponential.png"><img alt="Log-partition function for Exponential distribution showing mean and variance from derivatives" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_1_fig02b_log_partition_exponential.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 80 </span><span class="caption-text"><strong>Figure 3.3</strong>: The Log-Partition Function as Moment Generator (Exponential). For the Exponential distribution with natural parameter <span class="math notranslate nohighlight">\(\eta = -\lambda\)</span> and <span class="math notranslate nohighlight">\(A(\eta) = -\log(-\eta)\)</span>, the derivatives yield <span class="math notranslate nohighlight">\(A'(\eta) = -1/\eta = 1/\lambda = \mathbb{E}[X]\)</span> and <span class="math notranslate nohighlight">\(A''(\eta) = 1/\eta^2 = 1/\lambda^2 = \text{Var}(X)\)</span>. This continuous example complements the discrete Poisson case, demonstrating that the moment-from-derivatives property is universal across all exponential family members.</span><a class="headerlink" href="#id4" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="connection-to-fisher-information">
<h3>Connection to Fisher Information<a class="headerlink" href="#connection-to-fisher-information" title="Link to this heading"></a></h3>
<p>A crucial connection emerges: the Hessian of the log-partition function equals not only the variance of the sufficient statistic but also the <strong>Fisher information</strong>. Recall from the likelihood theory preview in Chapter 1 that Fisher information measures the curvature of the log-likelihood.</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Fisher Information Equality</p>
<p>For a canonical exponential family:</p>
<div class="math notranslate nohighlight" id="equation-fisher-info-eq">
<span class="eqno">(34)<a class="headerlink" href="#equation-fisher-info-eq" title="Link to this equation"></a></span>\[I(\eta) = \nabla^2 A(\eta) = \text{Var}_\eta(T(X))\]</div>
<p>where <span class="math notranslate nohighlight">\(I(\eta)\)</span> is the Fisher information matrix.</p>
</div>
<p>This equality means that:</p>
<ul class="simple">
<li><p><strong>Highly variable sufficient statistics imply high Fisher information</strong>: when <span class="math notranslate nohighlight">\(T(X)\)</span> varies substantially, the data contains much information about <span class="math notranslate nohighlight">\(\eta\)</span>.</p></li>
<li><p><strong>The log-likelihood is always concave</strong>: since <span class="math notranslate nohighlight">\(\nabla^2 A(\eta) \geq 0\)</span> (positive semi-definite), the log-likelihood <span class="math notranslate nohighlight">\(\ell(\eta) = \eta^T T(x) - A(\eta)\)</span> has <span class="math notranslate nohighlight">\(\nabla^2 \ell = -\nabla^2 A \leq 0\)</span>.</p></li>
</ul>
<p>The concavity of the log-likelihood guarantees that maximum likelihood estimation has no local optima—any critical point is a global maximum. This computational convenience is a hallmark of exponential families.</p>
</section>
</section>
<section id="sufficiency-capturing-all-parameter-information">
<h2>Sufficiency: Capturing All Parameter Information<a class="headerlink" href="#sufficiency-capturing-all-parameter-information" title="Link to this heading"></a></h2>
<p>One of the most important properties of exponential families is that the sufficient statistic <span class="math notranslate nohighlight">\(T(X)\)</span> contains <strong>all information</strong> about the parameter <span class="math notranslate nohighlight">\(\eta\)</span>. To make this precise, we need the concept of sufficiency.</p>
<section id="the-neyman-fisher-factorization-theorem">
<h3>The Neyman-Fisher Factorization Theorem<a class="headerlink" href="#the-neyman-fisher-factorization-theorem" title="Link to this heading"></a></h3>
<div class="note admonition">
<p class="admonition-title">Definition: Sufficient Statistic</p>
<p>A statistic <span class="math notranslate nohighlight">\(T(X)\)</span> is <strong>sufficient</strong> for parameter <span class="math notranslate nohighlight">\(\theta\)</span> if the conditional distribution of <span class="math notranslate nohighlight">\(X\)</span> given <span class="math notranslate nohighlight">\(T(X)\)</span> does not depend on <span class="math notranslate nohighlight">\(\theta\)</span>. Intuitively, once we know <span class="math notranslate nohighlight">\(T(X)\)</span>, we have extracted all information about <span class="math notranslate nohighlight">\(\theta\)</span> from the data.</p>
</div>
<p>The Neyman-Fisher factorization theorem provides a practical criterion for sufficiency:</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Neyman-Fisher Factorization</p>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> have density <span class="math notranslate nohighlight">\(f(x|\theta)\)</span> with respect to a dominating measure. The statistic <span class="math notranslate nohighlight">\(T(X)\)</span> is sufficient for <span class="math notranslate nohighlight">\(\theta\)</span> if and only if the density can be factored as:</p>
<div class="math notranslate nohighlight" id="equation-factorization">
<span class="eqno">(35)<a class="headerlink" href="#equation-factorization" title="Link to this equation"></a></span>\[f(x|\theta) = g(T(x), \theta) \cdot h(x)\]</div>
<p>where <span class="math notranslate nohighlight">\(g\)</span> depends on <span class="math notranslate nohighlight">\(x\)</span> only through <span class="math notranslate nohighlight">\(T(x)\)</span>, and <span class="math notranslate nohighlight">\(h\)</span> does not depend on <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
</div>
<p><strong>Proof (⟸ direction, discrete case)</strong>: Assume the factorization holds. For the conditional probability:</p>
<div class="math notranslate nohighlight">
\[P(X = x | T = t) = \frac{P(X = x, T(X) = t)}{P(T = t)}\]</div>
<p>If <span class="math notranslate nohighlight">\(T(x) \neq t\)</span>, the numerator is zero. If <span class="math notranslate nohighlight">\(T(x) = t\)</span>:</p>
<div class="math notranslate nohighlight">
\[P(X = x | T = t) = \frac{g(t, \theta) h(x)}{\sum_{y: T(y) = t} g(t, \theta) h(y)} = \frac{h(x)}{\sum_{y: T(y) = t} h(y)}\]</div>
<p>This expression does not depend on <span class="math notranslate nohighlight">\(\theta\)</span>, proving sufficiency. ∎</p>
<p><strong>Proof (⟹ direction)</strong>: Assume <span class="math notranslate nohighlight">\(T\)</span> is sufficient. Then <span class="math notranslate nohighlight">\(P(X = x | T = t)\)</span> is independent of <span class="math notranslate nohighlight">\(\theta\)</span>. We can write:</p>
<div class="math notranslate nohighlight">
\[f(x|\theta) = P(X = x | T = T(x)) \cdot P(T = T(x) | \theta) = h(x) \cdot g(T(x), \theta)\]</div>
<p>where <span class="math notranslate nohighlight">\(h(x) = P(X = x | T = T(x))\)</span> (independent of <span class="math notranslate nohighlight">\(\theta\)</span>) and <span class="math notranslate nohighlight">\(g(T(x), \theta) = P(T = T(x) | \theta)\)</span>. ∎</p>
</section>
<section id="sufficiency-in-exponential-families">
<h3>Sufficiency in Exponential Families<a class="headerlink" href="#sufficiency-in-exponential-families" title="Link to this heading"></a></h3>
<p>For exponential families, sufficiency is immediate:</p>
<div class="note admonition">
<p class="admonition-title">Corollary: Exponential Family Sufficiency</p>
<p>In an exponential family with density <span class="math notranslate nohighlight">\(p_\eta(x) = h(x) \exp\{\eta^T T(x) - A(\eta)\}\)</span>, the natural sufficient statistic <span class="math notranslate nohighlight">\(T(X)\)</span> is sufficient for <span class="math notranslate nohighlight">\(\eta\)</span>.</p>
</div>
<p><strong>Proof</strong>: The density factors as <span class="math notranslate nohighlight">\(g(T(x), \eta) \cdot h(x)\)</span> where <span class="math notranslate nohighlight">\(g(t, \eta) = \exp\{\eta^T t - A(\eta)\}\)</span> depends on <span class="math notranslate nohighlight">\(x\)</span> only through <span class="math notranslate nohighlight">\(T(x)\)</span>. By the Neyman-Fisher theorem, <span class="math notranslate nohighlight">\(T(X)\)</span> is sufficient. ∎</p>
<p>This result explains why exponential families are so prevalent in statistical practice: they automatically provide sufficient statistics, enabling dimension reduction without information loss.</p>
<p>The practical impact of sufficiency is illustrated in Figure 3.4, which shows two Poisson datasets with very different individual observations but identical sufficient statistics. Dataset A has varied counts (3, 5, 2, 8, 7, 5, 6, 4) while Dataset B is uniform (all 5s), yet both yield <span class="math notranslate nohighlight">\(T(X) = \sum x_i = 40\)</span>. As the right panel demonstrates, the likelihood function—and hence all inference about <span class="math notranslate nohighlight">\(\lambda\)</span>—depends only on this sum. The MLE is <span class="math notranslate nohighlight">\(\hat{\lambda} = 5\)</span> regardless of which dataset we observed.</p>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_1_fig03_sufficient_statistics.png"><img alt="Two datasets with same sufficient statistic yield identical inference" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_1_fig03_sufficient_statistics.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 81 </span><span class="caption-text"><strong>Figure 3.4</strong>: Sufficient Statistics Enable Data Reduction Without Information Loss. (a) Two datasets with identical sufficient statistic <span class="math notranslate nohighlight">\(T(X) = \sum x_i = 40\)</span>: Dataset A (blue) has varied values while Dataset B (magenta) is uniform. (b) The likelihood function depends only on <span class="math notranslate nohighlight">\(T(X)\)</span>, so both datasets yield identical inference with MLE <span class="math notranslate nohighlight">\(\hat{\lambda} = 5\)</span>. This illustrates the Neyman-Fisher factorization: <span class="math notranslate nohighlight">\(L(\lambda|x) = g(T(x), \lambda) \cdot h(x)\)</span>.</span><a class="headerlink" href="#id5" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="minimal-sufficiency-and-completeness">
<h2>Minimal Sufficiency and Completeness<a class="headerlink" href="#minimal-sufficiency-and-completeness" title="Link to this heading"></a></h2>
<p>While many statistics can be sufficient (e.g., the entire sample is always sufficient), we seek the <strong>minimal sufficient statistic</strong>—the most reduced summary that retains all parameter information.</p>
<section id="minimal-sufficiency">
<h3>Minimal Sufficiency<a class="headerlink" href="#minimal-sufficiency" title="Link to this heading"></a></h3>
<div class="note admonition">
<p class="admonition-title">Definition: Minimal Sufficiency</p>
<p>A sufficient statistic <span class="math notranslate nohighlight">\(T(X)\)</span> is <strong>minimal sufficient</strong> if it is a function of every other sufficient statistic. Equivalently, <span class="math notranslate nohighlight">\(T\)</span> achieves maximal data reduction while preserving sufficiency.</p>
</div>
<p>For exponential families, the natural sufficient statistic is typically minimal sufficient.</p>
</section>
<section id="completeness">
<h3>Completeness<a class="headerlink" href="#completeness" title="Link to this heading"></a></h3>
<p>A stronger property than minimal sufficiency is <strong>completeness</strong>, which ensures uniqueness of unbiased estimators:</p>
<div class="note admonition">
<p class="admonition-title">Definition: Completeness</p>
<p>A family of distributions <span class="math notranslate nohighlight">\(\{P_\theta : \theta \in \Theta\}\)</span> is <strong>complete</strong> if:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}_\theta[g(T)] = 0 \text{ for all } \theta \in \Theta \implies g(T) = 0 \text{ almost surely}\]</div>
<p>Intuitively, the only function of <span class="math notranslate nohighlight">\(T\)</span> with zero expectation everywhere is the zero function.</p>
</div>
<div class="note admonition">
<p class="admonition-title">Theorem: Completeness of Full-Rank Exponential Families</p>
<p>An exponential family is <strong>full-rank</strong> if the natural parameter space <span class="math notranslate nohighlight">\(\Xi\)</span> contains an <span class="math notranslate nohighlight">\(s\)</span>-dimensional open set (where <span class="math notranslate nohighlight">\(s\)</span> is the dimension of <span class="math notranslate nohighlight">\(\eta\)</span>). Full-rank exponential families have complete sufficient statistics.</p>
</div>
<p>The distinction between full-rank and <strong>curved</strong> exponential families matters:</p>
<ul class="simple">
<li><p><strong>Full-rank</strong>: <span class="math notranslate nohighlight">\(\text{dim}(\eta) = s\)</span> parameters can vary independently</p></li>
<li><p><strong>Curved</strong>: <span class="math notranslate nohighlight">\(\text{dim}(\theta) &lt; \text{dim}(\eta)\)</span>; parameters are constrained to a lower-dimensional manifold</p></li>
</ul>
<p>Example: The <span class="math notranslate nohighlight">\(N(\mu, \mu^2)\)</span> distribution (variance equals squared mean) is a curved exponential family. The two-dimensional sufficient statistic <span class="math notranslate nohighlight">\((X, X^2)\)</span> is not complete because <span class="math notranslate nohighlight">\(\eta_1\)</span> and <span class="math notranslate nohighlight">\(\eta_2\)</span> are constrained.</p>
</section>
<section id="the-lehmann-scheffe-theorem">
<h3>The Lehmann-Scheffé Theorem<a class="headerlink" href="#the-lehmann-scheffe-theorem" title="Link to this heading"></a></h3>
<p>Completeness enables a powerful result connecting sufficiency to optimal estimation:</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Lehmann-Scheffé</p>
<p>Let <span class="math notranslate nohighlight">\(T\)</span> be a complete sufficient statistic for <span class="math notranslate nohighlight">\(\theta\)</span>. If <span class="math notranslate nohighlight">\(g(T)\)</span> is an unbiased estimator of <span class="math notranslate nohighlight">\(\tau(\theta)\)</span>, then <span class="math notranslate nohighlight">\(g(T)\)</span> is the <strong>uniformly minimum variance unbiased estimator (UMVUE)</strong> of <span class="math notranslate nohighlight">\(\tau(\theta)\)</span>.</p>
</div>
<p><strong>Proof sketch</strong>: By the Rao-Blackwell theorem, conditioning any unbiased estimator on a sufficient statistic reduces variance. By completeness, there is at most one unbiased estimator based on <span class="math notranslate nohighlight">\(T\)</span>. Therefore, <span class="math notranslate nohighlight">\(g(T)\)</span> must have the smallest variance among all unbiased estimators. ∎</p>
<p>This theorem provides a recipe for finding optimal estimators: (1) find a complete sufficient statistic, (2) find any unbiased estimator, (3) condition on the sufficient statistic.</p>
</section>
</section>
<section id="conjugate-priors-and-bayesian-inference">
<h2>Conjugate Priors and Bayesian Inference<a class="headerlink" href="#conjugate-priors-and-bayesian-inference" title="Link to this heading"></a></h2>
<p>Exponential families possess natural <strong>conjugate priors</strong>—prior distributions that, when combined with the likelihood, yield posteriors in the same family. This conjugacy makes Bayesian updating analytically tractable.</p>
<section id="the-conjugate-prior-structure">
<h3>The Conjugate Prior Structure<a class="headerlink" href="#the-conjugate-prior-structure" title="Link to this heading"></a></h3>
<p>For an exponential family likelihood:</p>
<div class="math notranslate nohighlight">
\[L(\eta | x_1, \ldots, x_n) \propto \exp\left\{ \eta^T \sum_{i=1}^n T(x_i) - n A(\eta) \right\}\]</div>
<p>The natural conjugate prior has the form:</p>
<div class="math notranslate nohighlight" id="equation-conjugate-prior">
<span class="eqno">(36)<a class="headerlink" href="#equation-conjugate-prior" title="Link to this equation"></a></span>\[\pi(\eta | \alpha, \nu) \propto \exp\left\{ \eta^T \alpha - \nu A(\eta) \right\}\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha \in \mathbb{R}^s\)</span> and <span class="math notranslate nohighlight">\(\nu &gt; 0\)</span> are hyperparameters.</p>
</section>
<section id="bayesian-updating">
<h3>Bayesian Updating<a class="headerlink" href="#bayesian-updating" title="Link to this heading"></a></h3>
<p>When we observe data <span class="math notranslate nohighlight">\(x_1, \ldots, x_n\)</span>, the posterior is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\pi(\eta | x_1, \ldots, x_n) &amp;\propto L(\eta | x_1, \ldots, x_n) \cdot \pi(\eta | \alpha, \nu) \\
&amp;\propto \exp\left\{ \eta^T \left(\sum_{i=1}^n T(x_i) + \alpha\right) - (n + \nu) A(\eta) \right\}\end{split}\]</div>
<p>This is the same functional form as the prior <a class="reference internal" href="#equation-conjugate-prior">(36)</a> with updated hyperparameters:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\alpha' &amp;= \alpha + \sum_{i=1}^n T(x_i) \\
\nu' &amp;= \nu + n\end{split}\]</div>
<p>The interpretation is elegant:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\nu\)</span> represents the “number of pseudo-observations” from the prior</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span> represents the “pseudo-sufficient statistic” from those observations</p></li>
<li><p>The posterior combines prior pseudo-data with observed data</p></li>
</ul>
<div class="note admonition">
<p class="admonition-title">Example 💡 Beta-Binomial Conjugacy</p>
<p><strong>Setup</strong>: Bernoulli likelihood with Beta(<span class="math notranslate nohighlight">\(\alpha, \beta\)</span>) prior on <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p><strong>Observation</strong>: In <span class="math notranslate nohighlight">\(n\)</span> trials, observe <span class="math notranslate nohighlight">\(k\)</span> successes.</p>
<p><strong>Posterior</strong>: Beta(<span class="math notranslate nohighlight">\(\alpha + k, \beta + n - k\)</span>)</p>
<p>The prior “pseudo-observations” are <span class="math notranslate nohighlight">\(\alpha - 1\)</span> successes and <span class="math notranslate nohighlight">\(\beta - 1\)</span> failures. The posterior adds the observed <span class="math notranslate nohighlight">\(k\)</span> successes and <span class="math notranslate nohighlight">\(n-k\)</span> failures.</p>
</div>
<p>Figure 3.5 visualizes the Beta-Binomial conjugate updating process as data accumulates. Starting from a weakly informative Beta(2, 2) prior centered at 0.5, we observe Bernoulli trials from a process with true probability <span class="math notranslate nohighlight">\(p = 0.7\)</span>. With each batch of observations, the posterior concentrates around the true value, demonstrating how prior beliefs are updated by likelihood evidence. The conjugate structure ensures that the posterior remains in the Beta family at every stage—no numerical integration required.</p>
<figure class="align-center" id="id6">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_1_fig04_conjugate_updating.png"><img alt="Beta-Binomial conjugate prior updating as data accumulates" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_1_fig04_conjugate_updating.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 82 </span><span class="caption-text"><strong>Figure 3.5</strong>: Conjugate Prior Updating in Action. Starting from a Beta(2, 2) prior (panel a), the posterior evolves as Bernoulli observations accumulate. Each panel shows the posterior (solid) overlaid on the prior (dashed) after n = 5, 10, 15, 20, and 30 observations from a Bernoulli(<span class="math notranslate nohighlight">\(p = 0.7\)</span>) process. The posterior mean (orange dashed) converges toward the true value (red dotted) as the posterior concentrates. Conjugacy ensures the posterior is always Beta(<span class="math notranslate nohighlight">\(\alpha + k\)</span>, <span class="math notranslate nohighlight">\(\beta + n - k\)</span>), enabling analytic updating without numerical integration.</span><a class="headerlink" href="#id6" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="exponential-dispersion-models-and-glms">
<h2>Exponential Dispersion Models and GLMs<a class="headerlink" href="#exponential-dispersion-models-and-glms" title="Link to this heading"></a></h2>
<p>The exponential family framework extends to <strong>exponential dispersion models</strong>, which form the foundation for generalized linear models (GLMs). This extension will be crucial in Section 3.7.</p>
<section id="exponential-dispersion-form">
<h3>Exponential Dispersion Form<a class="headerlink" href="#exponential-dispersion-form" title="Link to this heading"></a></h3>
<p>An exponential dispersion model has density:</p>
<div class="math notranslate nohighlight" id="equation-edm-form">
<span class="eqno">(37)<a class="headerlink" href="#equation-edm-form" title="Link to this equation"></a></span>\[f(y | \theta, \phi) = \exp\left\{ \frac{y\theta - b(\theta)}{\phi} + c(y, \phi) \right\}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\theta\)</span> is the <strong>canonical parameter</strong> (natural parameter)</p></li>
<li><p><span class="math notranslate nohighlight">\(\phi &gt; 0\)</span> is the <strong>dispersion parameter</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(b(\theta)\)</span> is the <strong>cumulant function</strong></p></li>
</ul>
<p>This form is equivalent to the exponential family <a class="reference internal" href="#equation-exp-family-def">(31)</a> with <span class="math notranslate nohighlight">\(\eta = \theta/\phi\)</span> and specific choices of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(h\)</span>.</p>
</section>
<section id="mean-and-variance-functions">
<h3>Mean and Variance Functions<a class="headerlink" href="#mean-and-variance-functions" title="Link to this heading"></a></h3>
<p>The mean and variance have elegant expressions in terms of <span class="math notranslate nohighlight">\(b(\theta)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbb{E}[Y] &amp;= \mu = b'(\theta) \\
\text{Var}(Y) &amp;= \phi \cdot b''(\theta) = \phi \cdot V(\mu)\end{split}\]</div>
<p>The function <span class="math notranslate nohighlight">\(V(\mu) = b''(\theta)\)</span> is called the <strong>variance function</strong>—it expresses how the variance depends on the mean. Each exponential dispersion family has a characteristic variance function:</p>
<table class="docutils align-default" id="id7">
<caption><span class="caption-number">Table 24 </span><span class="caption-text">Variance Functions for Common Distributions</span><a class="headerlink" href="#id7" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 30.0%" />
<col style="width: 30.0%" />
<col style="width: 40.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Distribution</p></th>
<th class="head"><p>Variance Function <span class="math notranslate nohighlight">\(V(\mu)\)</span></p></th>
<th class="head"><p>Var(<span class="math notranslate nohighlight">\(Y\)</span>)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Normal</p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\phi\)</span> (constant)</p></td>
</tr>
<tr class="row-odd"><td><p>Poisson</p></td>
<td><p><span class="math notranslate nohighlight">\(\mu\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\phi \mu\)</span> (proportional to mean)</p></td>
</tr>
<tr class="row-even"><td><p>Binomial</p></td>
<td><p><span class="math notranslate nohighlight">\(\mu(1-\mu)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\phi \mu(1-\mu)\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Gamma</p></td>
<td><p><span class="math notranslate nohighlight">\(\mu^2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\phi \mu^2\)</span> (CV constant)</p></td>
</tr>
<tr class="row-even"><td><p>Inverse Gaussian</p></td>
<td><p><span class="math notranslate nohighlight">\(\mu^3\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\phi \mu^3\)</span></p></td>
</tr>
</tbody>
</table>
</section>
<section id="the-canonical-link">
<h3>The Canonical Link<a class="headerlink" href="#the-canonical-link" title="Link to this heading"></a></h3>
<p>The <strong>canonical link</strong> function connects the mean <span class="math notranslate nohighlight">\(\mu\)</span> to the linear predictor in a GLM by setting <span class="math notranslate nohighlight">\(g(\mu) = \theta\)</span>, where <span class="math notranslate nohighlight">\(\theta\)</span> is the canonical parameter. Since <span class="math notranslate nohighlight">\(\mu = b'(\theta)\)</span>, the canonical link is:</p>
<div class="math notranslate nohighlight">
\[g = (b')^{-1}\]</div>
<p>For example:</p>
<ul class="simple">
<li><p>Normal: <span class="math notranslate nohighlight">\(\mu = \theta\)</span>, so <span class="math notranslate nohighlight">\(g(\mu) = \mu\)</span> (identity link)</p></li>
<li><p>Bernoulli: <span class="math notranslate nohighlight">\(\mu = e^\theta/(1+e^\theta)\)</span>, so <span class="math notranslate nohighlight">\(g(\mu) = \log(\mu/(1-\mu))\)</span> (logit link)</p></li>
<li><p>Poisson: <span class="math notranslate nohighlight">\(\mu = e^\theta\)</span>, so <span class="math notranslate nohighlight">\(g(\mu) = \log\mu\)</span> (log link)</p></li>
</ul>
<p>The canonical link has special computational properties: observed and expected Fisher information coincide, and the log-likelihood is guaranteed concave.</p>
</section>
</section>
<section id="python-implementation">
<h2>Python Implementation<a class="headerlink" href="#python-implementation" title="Link to this heading"></a></h2>
<p>Let us implement an exponential family framework in Python that can convert between natural and standard parameterizations and compute moments from the log-partition function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.misc</span><span class="w"> </span><span class="kn">import</span> <span class="n">derivative</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Dict</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">class</span><span class="w"> </span><span class="nc">ExponentialFamily</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Represents a one-parameter exponential family distribution.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    name : str</span>
<span class="sd">        Name of the distribution</span>
<span class="sd">    natural_to_standard : callable</span>
<span class="sd">        Converts natural parameter eta to standard parameter(s)</span>
<span class="sd">    standard_to_natural : callable</span>
<span class="sd">        Converts standard parameter(s) to natural parameter eta</span>
<span class="sd">    log_partition : callable</span>
<span class="sd">        The log-partition function A(eta)</span>
<span class="sd">    carrier_density : callable</span>
<span class="sd">        The carrier density h(x)</span>
<span class="sd">    sufficient_stat : callable</span>
<span class="sd">        The sufficient statistic T(x)</span>
<span class="sd">    support : tuple</span>
<span class="sd">        The support of the distribution (min, max)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">natural_to_standard</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
                 <span class="n">standard_to_natural</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">log_partition</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
                 <span class="n">carrier_density</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">sufficient_stat</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
                 <span class="n">support</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">natural_to_standard</span> <span class="o">=</span> <span class="n">natural_to_standard</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">standard_to_natural</span> <span class="o">=</span> <span class="n">standard_to_natural</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">log_partition</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">carrier_density</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">T</span> <span class="o">=</span> <span class="n">sufficient_stat</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">support</span> <span class="o">=</span> <span class="n">support</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">mean_from_A</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eta</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">dx</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute E[T(X)] as the derivative of A(eta).</span>

<span class="sd">        Uses numerical differentiation to illustrate the theorem.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">derivative</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">A</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">dx</span><span class="o">=</span><span class="n">dx</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">variance_from_A</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eta</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">dx</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute Var(T(X)) as the second derivative of A(eta).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">derivative</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">A</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">dx</span><span class="o">=</span><span class="n">dx</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">log_density</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">eta</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute log p_eta(x) = log h(x) + eta * T(x) - A(eta).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">support</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">or</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">support</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">+</span> <span class="n">eta</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">T</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">A</span><span class="p">(</span><span class="n">eta</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">density</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">eta</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute the density p_eta(x).&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_density</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">eta</span><span class="p">))</span>


<span class="c1"># Define common exponential families</span>

<span class="c1"># Bernoulli(p): eta = log(p/(1-p)), A(eta) = log(1 + exp(eta))</span>
<span class="n">bernoulli_ef</span> <span class="o">=</span> <span class="n">ExponentialFamily</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Bernoulli&quot;</span><span class="p">,</span>
    <span class="n">natural_to_standard</span><span class="o">=</span><span class="k">lambda</span> <span class="n">eta</span><span class="p">:</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">eta</span><span class="p">)),</span>  <span class="c1"># p = sigmoid(eta)</span>
    <span class="n">standard_to_natural</span><span class="o">=</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)),</span>       <span class="c1"># eta = logit(p)</span>
    <span class="n">log_partition</span><span class="o">=</span><span class="k">lambda</span> <span class="n">eta</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">eta</span><span class="p">)),</span>       <span class="c1"># softplus</span>
    <span class="n">carrier_density</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">sufficient_stat</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
    <span class="n">support</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Poisson(lambda): eta = log(lambda), A(eta) = exp(eta)</span>
<span class="n">poisson_ef</span> <span class="o">=</span> <span class="n">ExponentialFamily</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Poisson&quot;</span><span class="p">,</span>
    <span class="n">natural_to_standard</span><span class="o">=</span><span class="k">lambda</span> <span class="n">eta</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">eta</span><span class="p">),</span>             <span class="c1"># lambda = exp(eta)</span>
    <span class="n">standard_to_natural</span><span class="o">=</span><span class="k">lambda</span> <span class="n">lam</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">lam</span><span class="p">),</span>             <span class="c1"># eta = log(lambda)</span>
    <span class="n">log_partition</span><span class="o">=</span><span class="k">lambda</span> <span class="n">eta</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">eta</span><span class="p">),</span>
    <span class="n">carrier_density</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">factorial</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">x</span> <span class="o">==</span> <span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">sufficient_stat</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
    <span class="n">support</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Exponential(lambda): eta = -lambda, A(eta) = -log(-eta)</span>
<span class="n">exponential_ef</span> <span class="o">=</span> <span class="n">ExponentialFamily</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Exponential&quot;</span><span class="p">,</span>
    <span class="n">natural_to_standard</span><span class="o">=</span><span class="k">lambda</span> <span class="n">eta</span><span class="p">:</span> <span class="o">-</span><span class="n">eta</span><span class="p">,</span>                    <span class="c1"># lambda = -eta</span>
    <span class="n">standard_to_natural</span><span class="o">=</span><span class="k">lambda</span> <span class="n">lam</span><span class="p">:</span> <span class="o">-</span><span class="n">lam</span><span class="p">,</span>                    <span class="c1"># eta = -lambda</span>
    <span class="n">log_partition</span><span class="o">=</span><span class="k">lambda</span> <span class="n">eta</span><span class="p">:</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="o">-</span><span class="n">eta</span><span class="p">),</span>
    <span class="n">carrier_density</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">sufficient_stat</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
    <span class="n">support</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
<span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">demonstrate_moment_derivation</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Demonstrate that moments can be derived from the log-partition function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;EXPONENTIAL FAMILY: MOMENTS FROM LOG-PARTITION FUNCTION&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>

    <span class="c1"># Poisson example</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Poisson Distribution ---&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">lam</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">]:</span>
        <span class="n">eta</span> <span class="o">=</span> <span class="n">poisson_ef</span><span class="o">.</span><span class="n">standard_to_natural</span><span class="p">(</span><span class="n">lam</span><span class="p">)</span>

        <span class="c1"># From log-partition function</span>
        <span class="n">mean_A</span> <span class="o">=</span> <span class="n">poisson_ef</span><span class="o">.</span><span class="n">mean_from_A</span><span class="p">(</span><span class="n">eta</span><span class="p">)</span>
        <span class="n">var_A</span> <span class="o">=</span> <span class="n">poisson_ef</span><span class="o">.</span><span class="n">variance_from_A</span><span class="p">(</span><span class="n">eta</span><span class="p">)</span>

        <span class="c1"># From scipy (ground truth)</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="n">lam</span><span class="p">)</span>
        <span class="n">mean_scipy</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">var_scipy</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">var</span><span class="p">()</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">lambda = </span><span class="si">{</span><span class="n">lam</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean from A&#39;(eta):     </span><span class="si">{</span><span class="n">mean_A</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean from scipy:       </span><span class="si">{</span><span class="n">mean_scipy</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Variance from A&#39;&#39;(eta): </span><span class="si">{</span><span class="n">var_A</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Variance from scipy:   </span><span class="si">{</span><span class="n">var_scipy</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Bernoulli example</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Bernoulli Distribution ---&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]:</span>
        <span class="n">eta</span> <span class="o">=</span> <span class="n">bernoulli_ef</span><span class="o">.</span><span class="n">standard_to_natural</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

        <span class="c1"># From log-partition function</span>
        <span class="n">mean_A</span> <span class="o">=</span> <span class="n">bernoulli_ef</span><span class="o">.</span><span class="n">mean_from_A</span><span class="p">(</span><span class="n">eta</span><span class="p">)</span>
        <span class="n">var_A</span> <span class="o">=</span> <span class="n">bernoulli_ef</span><span class="o">.</span><span class="n">variance_from_A</span><span class="p">(</span><span class="n">eta</span><span class="p">)</span>

        <span class="c1"># Theoretical values</span>
        <span class="n">mean_theory</span> <span class="o">=</span> <span class="n">p</span>
        <span class="n">var_theory</span> <span class="o">=</span> <span class="n">p</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">p = </span><span class="si">{</span><span class="n">p</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean from A&#39;(eta):     </span><span class="si">{</span><span class="n">mean_A</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean (theoretical):    </span><span class="si">{</span><span class="n">mean_theory</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Variance from A&#39;&#39;(eta): </span><span class="si">{</span><span class="n">var_A</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Variance (theoretical): </span><span class="si">{</span><span class="n">var_theory</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Exponential example</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Exponential Distribution ---&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">lam</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]:</span>
        <span class="n">eta</span> <span class="o">=</span> <span class="n">exponential_ef</span><span class="o">.</span><span class="n">standard_to_natural</span><span class="p">(</span><span class="n">lam</span><span class="p">)</span>

        <span class="c1"># From log-partition function</span>
        <span class="n">mean_A</span> <span class="o">=</span> <span class="n">exponential_ef</span><span class="o">.</span><span class="n">mean_from_A</span><span class="p">(</span><span class="n">eta</span><span class="p">)</span>
        <span class="n">var_A</span> <span class="o">=</span> <span class="n">exponential_ef</span><span class="o">.</span><span class="n">variance_from_A</span><span class="p">(</span><span class="n">eta</span><span class="p">)</span>

        <span class="c1"># Theoretical values (for rate parameterization)</span>
        <span class="n">mean_theory</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">lam</span>
        <span class="n">var_theory</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">lam</span><span class="o">**</span><span class="mi">2</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">lambda = </span><span class="si">{</span><span class="n">lam</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean from A&#39;(eta):     </span><span class="si">{</span><span class="n">mean_A</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean (theoretical):    </span><span class="si">{</span><span class="n">mean_theory</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Variance from A&#39;&#39;(eta): </span><span class="si">{</span><span class="n">var_A</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Variance (theoretical): </span><span class="si">{</span><span class="n">var_theory</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="c1"># Run demonstration</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">demonstrate_moment_derivation</span><span class="p">()</span>
</pre></div>
</div>
<div class="tip admonition">
<p class="admonition-title">Example Output</p>
<p>Running the demonstration produces:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>======================================================================
EXPONENTIAL FAMILY: MOMENTS FROM LOG-PARTITION FUNCTION
======================================================================

--- Poisson Distribution ---

lambda = 1.0:
  Mean from A&#39;(eta):     1.000000
  Mean from scipy:       1.000000
  Variance from A&#39;&#39;(eta): 1.000000
  Variance from scipy:   1.000000

lambda = 5.0:
  Mean from A&#39;(eta):     5.000000
  Mean from scipy:       5.000000
  Variance from A&#39;&#39;(eta): 5.000000
  Variance from scipy:   5.000000
</pre></div>
</div>
<p>The numerical derivatives of <span class="math notranslate nohighlight">\(A(\eta)\)</span> recover the theoretical moments exactly, verifying the exponential family moment theorems.</p>
</div>
</section>
<section id="practical-considerations">
<h2>Practical Considerations<a class="headerlink" href="#practical-considerations" title="Link to this heading"></a></h2>
<section id="numerical-stability">
<h3>Numerical Stability<a class="headerlink" href="#numerical-stability" title="Link to this heading"></a></h3>
<p>When working with exponential families computationally, several numerical issues arise:</p>
<ol class="arabic">
<li><p><strong>Log-sum-exp stability</strong>: The log-partition function <span class="math notranslate nohighlight">\(A(\eta) = \log \int h(x) e^{\eta^T T(x)} d\mu\)</span> can overflow. Use the log-sum-exp trick:</p>
<div class="math notranslate nohighlight">
\[\log \sum_i e^{a_i} = m + \log \sum_i e^{a_i - m}\]</div>
<p>where <span class="math notranslate nohighlight">\(m = \max_i a_i\)</span>.</p>
</li>
<li><p><strong>Natural vs. mean parameterization</strong>: The natural parameter <span class="math notranslate nohighlight">\(\eta\)</span> is convenient for theoretical analysis, but the mean parameter <span class="math notranslate nohighlight">\(\mu = \nabla A(\eta)\)</span> is often more interpretable. GLM software typically reports mean parameters.</p></li>
<li><p><strong>Boundary behavior</strong>: At the boundary of <span class="math notranslate nohighlight">\(\Xi\)</span>, the log-partition function may approach infinity. For example, in logistic regression with separable data, <span class="math notranslate nohighlight">\(|\eta| \to \infty\)</span> as coefficients diverge.</p></li>
</ol>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ⚠️</p>
<p><strong>Parameterization confusion</strong>: Different sources use different parameterizations for the same distribution. The normal distribution may be parameterized by <span class="math notranslate nohighlight">\((\mu, \sigma^2)\)</span>, <span class="math notranslate nohighlight">\((\mu, \sigma)\)</span>, <span class="math notranslate nohighlight">\((\mu, \tau)\)</span> where <span class="math notranslate nohighlight">\(\tau = 1/\sigma^2\)</span> is precision, or the natural parameters <span class="math notranslate nohighlight">\((\eta_1, \eta_2)\)</span>. Always verify which convention is in use before applying formulas.</p>
</div>
</section>
<section id="when-to-use-exponential-family-framework">
<h3>When to Use Exponential Family Framework<a class="headerlink" href="#when-to-use-exponential-family-framework" title="Link to this heading"></a></h3>
<p>The exponential family framework is most valuable when:</p>
<ul class="simple">
<li><p><strong>Developing general algorithms</strong>: Write MLE, IRLS, or Bayesian updating once for all exponential families</p></li>
<li><p><strong>Deriving theoretical properties</strong>: Sufficiency, Fisher information, and asymptotic theory follow from general results</p></li>
<li><p><strong>Building GLMs</strong>: The exponential dispersion model structure is essential for GLM inference</p></li>
</ul>
<p>For routine calculations with a single distribution, using scipy.stats directly is often more convenient.</p>
</section>
</section>
<section id="chapter-3-1-exercises-exponential-families-mastery">
<h2>Chapter 3.1 Exercises: Exponential Families Mastery<a class="headerlink" href="#chapter-3-1-exercises-exponential-families-mastery" title="Link to this heading"></a></h2>
<p>These exercises progressively build your understanding of exponential families, from recognizing and converting distributions to canonical form through exploiting the log-partition function for inference and connecting to conjugate priors. Each exercise connects theory, implementation, and practical considerations that appear throughout statistical modeling.</p>
<div class="tip admonition">
<p class="admonition-title">A Note on These Exercises</p>
<p>These exercises are designed to deepen your understanding of exponential families through hands-on derivation and implementation:</p>
<ul class="simple">
<li><p><strong>Exercise 1</strong> reinforces core concepts: converting distributions to canonical form and identifying natural parameters</p></li>
<li><p><strong>Exercise 2</strong> develops facility with the log-partition function’s remarkable properties for computing moments</p></li>
<li><p><strong>Exercise 3</strong> explores sufficiency and the Neyman-Fisher factorization theorem</p></li>
<li><p><strong>Exercise 4</strong> connects exponential families to Bayesian inference through conjugate priors</p></li>
<li><p><strong>Exercise 5</strong> distinguishes full-rank from curved exponential families</p></li>
<li><p><strong>Exercise 6</strong> synthesizes the material into a computational toolkit for unified inference</p></li>
</ul>
<p>Complete solutions with derivations, code, output, and interpretation are provided. Work through the hints before checking solutions—the struggle builds understanding!</p>
</div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 1: Converting to Canonical Exponential Family Form</p>
<p>The <strong>geometric distribution</strong> models the number of failures before the first success in a sequence of independent Bernoulli trials. With success probability <span class="math notranslate nohighlight">\(p \in (0, 1)\)</span>, the PMF is <span class="math notranslate nohighlight">\(P(X = k) = (1-p)^k p\)</span> for <span class="math notranslate nohighlight">\(k = 0, 1, 2, \ldots\)</span>.</p>
<div class="note admonition">
<p class="admonition-title">Background: Why Canonical Form Matters</p>
<p>Converting a distribution to canonical exponential family form reveals its essential structure: the natural parameter <span class="math notranslate nohighlight">\(\eta\)</span>, sufficient statistic <span class="math notranslate nohighlight">\(T(x)\)</span>, and log-partition function <span class="math notranslate nohighlight">\(A(\eta)\)</span>. Once in this form, we can read off moments from derivatives of <span class="math notranslate nohighlight">\(A\)</span>, identify conjugate priors automatically, and apply unified computational methods across all exponential families.</p>
</div>
<ol class="loweralpha">
<li><p><strong>Canonical form derivation</strong>: Express the geometric PMF in the canonical exponential family form:</p>
<div class="math notranslate nohighlight">
\[f(x|\eta) = h(x) \exp\bigl(\eta \cdot T(x) - A(\eta)\bigr)\]</div>
<p>Identify <span class="math notranslate nohighlight">\(\eta(p)\)</span>, <span class="math notranslate nohighlight">\(T(x)\)</span>, <span class="math notranslate nohighlight">\(h(x)\)</span>, and <span class="math notranslate nohighlight">\(A(\eta)\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Taking Logarithms</p>
<p>Start by taking <span class="math notranslate nohighlight">\(\log P(X = k) = k \log(1-p) + \log p\)</span>. Group terms to identify what multiplies <span class="math notranslate nohighlight">\(k\)</span> (that’s <span class="math notranslate nohighlight">\(\eta\)</span>) and what remains as a function of <span class="math notranslate nohighlight">\(p\)</span> alone (that becomes <span class="math notranslate nohighlight">\(-A(\eta)\)</span> after reparametrization).</p>
</div>
</li>
<li><p><strong>Natural parameter space</strong>: Determine the natural parameter space <span class="math notranslate nohighlight">\(\Xi = \{\eta : A(\eta) &lt; \infty\}\)</span>. What values of <span class="math notranslate nohighlight">\(\eta\)</span> correspond to valid probabilities <span class="math notranslate nohighlight">\(p \in (0, 1)\)</span>?</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Existence of Normalizing Constant</p>
<p>The log-partition function <span class="math notranslate nohighlight">\(A(\eta)\)</span> is finite exactly when the sum <span class="math notranslate nohighlight">\(\sum_{k=0}^{\infty} e^{\eta k}\)</span> converges. This is a geometric series—when does it converge?</p>
</div>
</li>
<li><p><strong>Moment computation via</strong> <span class="math notranslate nohighlight">\(A(\eta)\)</span>: Verify that <span class="math notranslate nohighlight">\(A'(\eta) = \mathbb{E}[T(X)]\)</span> and <span class="math notranslate nohighlight">\(A''(\eta) = \text{Var}(T(X))\)</span> by computing derivatives and comparing to the known mean and variance of the geometric distribution.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Known Moments</p>
<p>For <span class="math notranslate nohighlight">\(X \sim \text{Geometric}(p)\)</span> (counting failures before first success), <span class="math notranslate nohighlight">\(\mathbb{E}[X] = (1-p)/p\)</span> and <span class="math notranslate nohighlight">\(\text{Var}(X) = (1-p)/p^2\)</span>. Express these in terms of <span class="math notranslate nohighlight">\(\eta\)</span>.</p>
</div>
</li>
<li><p><strong>Numerical verification</strong>: Implement a function that generates geometric samples and verifies the moment formulas empirically for <span class="math notranslate nohighlight">\(p = 0.3\)</span>.</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Canonical Form Derivation</strong></p>
<p class="sd-card-text">Starting from <span class="math notranslate nohighlight">\(P(X = k) = (1-p)^k p\)</span>:</p>
<div class="math notranslate nohighlight">
\[\log P(X = k) = k \log(1-p) + \log p\]</div>
<p class="sd-card-text">Identifying components:</p>
<ul class="simple">
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(\eta = \log(1-p)\)</span> (natural parameter)</p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(T(x) = x\)</span> (sufficient statistic)</p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(h(x) = 1\)</span> (base measure, since <span class="math notranslate nohighlight">\(x \in \{0, 1, 2, \ldots\}\)</span>)</p></li>
</ul>
<p class="sd-card-text">For the log-partition function, we need <span class="math notranslate nohighlight">\(\log p\)</span> in terms of <span class="math notranslate nohighlight">\(\eta\)</span>:</p>
<div class="math notranslate nohighlight">
\[\eta = \log(1-p) \implies 1-p = e^{\eta} \implies p = 1 - e^{\eta}\]</div>
<p class="sd-card-text">Therefore:</p>
<div class="math notranslate nohighlight">
\[\log p = \log(1 - e^{\eta})\]</div>
<p class="sd-card-text">The canonical form is:</p>
<div class="math notranslate nohighlight">
\[P(X = k | \eta) = \exp\bigl(\eta \cdot k - A(\eta)\bigr)\]</div>
<p class="sd-card-text">where <span class="math notranslate nohighlight">\(A(\eta) = -\log(1 - e^{\eta})\)</span>.</p>
<p class="sd-card-text"><strong>Part (b): Natural Parameter Space</strong></p>
<p class="sd-card-text">The log-partition function <span class="math notranslate nohighlight">\(A(\eta) = -\log(1 - e^{\eta})\)</span> is finite when <span class="math notranslate nohighlight">\(1 - e^{\eta} &gt; 0\)</span>, i.e., when <span class="math notranslate nohighlight">\(e^{\eta} &lt; 1\)</span>, which requires <span class="math notranslate nohighlight">\(\eta &lt; 0\)</span>.</p>
<p class="sd-card-text">Therefore: <span class="math notranslate nohighlight">\(\Xi = (-\infty, 0)\)</span></p>
<p class="sd-card-text">Mapping back to <span class="math notranslate nohighlight">\(p\)</span>: when <span class="math notranslate nohighlight">\(\eta \in (-\infty, 0)\)</span>, we have <span class="math notranslate nohighlight">\(e^{\eta} \in (0, 1)\)</span>, so <span class="math notranslate nohighlight">\(p = 1 - e^{\eta} \in (0, 1)\)</span>. The natural parameter space exactly corresponds to valid probability values.</p>
<p class="sd-card-text"><strong>Part (c): Moment Computation</strong></p>
<p class="sd-card-text">Computing <span class="math notranslate nohighlight">\(A'(\eta)\)</span>:</p>
<div class="math notranslate nohighlight">
\[A'(\eta) = \frac{d}{d\eta}\bigl[-\log(1 - e^{\eta})\bigr] = \frac{e^{\eta}}{1 - e^{\eta}}\]</div>
<p class="sd-card-text">Since <span class="math notranslate nohighlight">\(e^{\eta} = 1-p\)</span>:</p>
<div class="math notranslate nohighlight">
\[A'(\eta) = \frac{1-p}{1-(1-p)} = \frac{1-p}{p}\]</div>
<p class="sd-card-text">This equals <span class="math notranslate nohighlight">\(\mathbb{E}[X]\)</span> for the geometric distribution. ✓</p>
<p class="sd-card-text">Computing <span class="math notranslate nohighlight">\(A''(\eta)\)</span>:</p>
<div class="math notranslate nohighlight">
\[A''(\eta) = \frac{d}{d\eta}\left[\frac{e^{\eta}}{1 - e^{\eta}}\right] = \frac{e^{\eta}(1-e^{\eta}) + e^{2\eta}}{(1-e^{\eta})^2} = \frac{e^{\eta}}{(1-e^{\eta})^2}\]</div>
<p class="sd-card-text">In terms of <span class="math notranslate nohighlight">\(p\)</span>:</p>
<div class="math notranslate nohighlight">
\[A''(\eta) = \frac{1-p}{p^2}\]</div>
<p class="sd-card-text">This equals <span class="math notranslate nohighlight">\(\text{Var}(X)\)</span> for the geometric distribution. ✓</p>
<p class="sd-card-text"><strong>Part (d): Numerical Verification</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">verify_geometric_exponential_family</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Verify exponential family properties for geometric distribution.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Natural parameter</span>
    <span class="n">eta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span>

    <span class="c1"># Log-partition function and derivatives</span>
    <span class="n">A_eta</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">eta</span><span class="p">))</span>
    <span class="n">A_prime</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">eta</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">eta</span><span class="p">))</span>  <span class="c1"># E[X]</span>
    <span class="n">A_double_prime</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">eta</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">eta</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span>  <span class="c1"># Var(X)</span>

    <span class="c1"># Known formulas</span>
    <span class="n">theoretical_mean</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span> <span class="o">/</span> <span class="n">p</span>
    <span class="n">theoretical_var</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span> <span class="o">/</span> <span class="n">p</span><span class="o">**</span><span class="mi">2</span>

    <span class="c1"># Monte Carlo verification</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">geometric</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>  <span class="c1"># NumPy counts trials, we want failures</span>
    <span class="n">mc_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
    <span class="n">mc_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;GEOMETRIC DISTRIBUTION: EXPONENTIAL FAMILY VERIFICATION&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Parameter: p = </span><span class="si">{</span><span class="n">p</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Natural parameter: η = log(1-p) = </span><span class="si">{</span><span class="n">eta</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Natural parameter space: Ξ = (-∞, 0)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Quantity&#39;</span><span class="si">:</span><span class="s2">&lt;25</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Theory&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;A(η) deriv&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Monte Carlo&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;E[X]&#39;</span><span class="si">:</span><span class="s2">&lt;25</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">theoretical_mean</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">A_prime</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">mc_mean</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Var(X)&#39;</span><span class="si">:</span><span class="s2">&lt;25</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">theoretical_var</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">A_double_prime</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">mc_var</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Log-partition A(η) = -log(1 - e^η) = </span><span class="si">{</span><span class="n">A_eta</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">verify_geometric_exponential_family</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>GEOMETRIC DISTRIBUTION: EXPONENTIAL FAMILY VERIFICATION
=================================================================
Parameter: p = 0.3
Natural parameter: η = log(1-p) = -0.356675
Natural parameter space: Ξ = (-∞, 0)

Quantity                       Theory   A(η) deriv  Monte Carlo
-----------------------------------------------------------------
E[X]                         2.333333     2.333333     2.329960
Var(X)                       7.777778     7.777778     7.765498

Log-partition A(η) = -log(1 - e^η) = 1.203973
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Natural parameter</strong>: The transformation <span class="math notranslate nohighlight">\(\eta = \log(1-p)\)</span> maps <span class="math notranslate nohighlight">\(p \in (0,1)\)</span> to <span class="math notranslate nohighlight">\(\eta \in (-\infty, 0)\)</span>.</p></li>
<li><p class="sd-card-text"><strong>Sufficient statistic</strong>: <span class="math notranslate nohighlight">\(T(X) = X\)</span> is the identity—the sample itself is sufficient.</p></li>
<li><p class="sd-card-text"><strong>Moment computation</strong>: The log-partition function <span class="math notranslate nohighlight">\(A(\eta)\)</span> encodes all moments through its derivatives, eliminating the need for separate calculations.</p></li>
<li><p class="sd-card-text"><strong>Verification</strong>: Monte Carlo estimates match theoretical values within sampling error, confirming correctness of the exponential family representation.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 2: The Log-Partition Theorem for Multiparameter Families</p>
<p>The <strong>normal distribution</strong> <span class="math notranslate nohighlight">\(\mathcal{N}(\mu, \sigma^2)\)</span> is a two-parameter exponential family. This exercise explores how the gradient and Hessian of the log-partition function encode moments for multiparameter families.</p>
<div class="note admonition">
<p class="admonition-title">Background: Multiparameter Exponential Families</p>
<p>For a <span class="math notranslate nohighlight">\(k\)</span>-parameter exponential family with natural parameters <span class="math notranslate nohighlight">\(\boldsymbol{\eta} = (\eta_1, \ldots, \eta_k)\)</span> and sufficient statistics <span class="math notranslate nohighlight">\(\mathbf{T}(x) = (T_1(x), \ldots, T_k(x))\)</span>, the fundamental theorem generalizes: <span class="math notranslate nohighlight">\(\nabla A(\boldsymbol{\eta}) = \mathbb{E}[\mathbf{T}(X)]\)</span> and <span class="math notranslate nohighlight">\(\nabla^2 A(\boldsymbol{\eta}) = \text{Cov}(\mathbf{T}(X))\)</span>. The Hessian being a covariance matrix guarantees that <span class="math notranslate nohighlight">\(A\)</span> is convex.</p>
</div>
<ol class="loweralpha">
<li><p><strong>Two-parameter form</strong>: The normal density can be written as:</p>
<div class="math notranslate nohighlight">
\[f(x|\mu, \sigma^2) \propto \exp\left(\frac{\mu}{\sigma^2} x - \frac{1}{2\sigma^2} x^2 - \frac{\mu^2}{2\sigma^2} - \frac{1}{2}\log\sigma^2\right)\]</div>
<p>Identify the natural parameters <span class="math notranslate nohighlight">\(\eta_1, \eta_2\)</span> and sufficient statistics <span class="math notranslate nohighlight">\(T_1(x), T_2(x)\)</span>. Express <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> in terms of <span class="math notranslate nohighlight">\(\eta_1, \eta_2\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Grouping Terms</p>
<p>The natural parameters are the coefficients of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(x^2\)</span> in the exponent. Be careful with signs—one natural parameter will be negative for valid <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
</div>
</li>
<li><p><strong>Log-partition function</strong>: Derive <span class="math notranslate nohighlight">\(A(\eta_1, \eta_2)\)</span> by completing the square or direct calculation. Verify that it takes the form:</p>
<div class="math notranslate nohighlight">
\[A(\eta_1, \eta_2) = -\frac{\eta_1^2}{4\eta_2} - \frac{1}{2}\log(-2\eta_2)\]</div>
<div class="tip admonition">
<p class="admonition-title">Hint: Normalization Constant</p>
<p>The log-partition function equals <span class="math notranslate nohighlight">\(\log \int \exp(\eta_1 x + \eta_2 x^2) dx\)</span>. Complete the square in the exponent and use the Gaussian integral formula.</p>
</div>
</li>
<li><p><strong>Gradient verification</strong>: Compute <span class="math notranslate nohighlight">\(\nabla A = (\partial A/\partial \eta_1, \partial A/\partial \eta_2)\)</span> and verify:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\partial A/\partial \eta_1 = \mathbb{E}[X] = \mu\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\partial A/\partial \eta_2 = \mathbb{E}[X^2] = \mu^2 + \sigma^2\)</span></p></li>
</ul>
</li>
<li><p><strong>Hessian and Fisher information</strong>: Compute the Hessian <span class="math notranslate nohighlight">\(\nabla^2 A\)</span> and verify it equals <span class="math notranslate nohighlight">\(\text{Cov}(T_1(X), T_2(X))\)</span>. Show this matrix is positive definite, confirming <span class="math notranslate nohighlight">\(A\)</span> is convex.</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Two-Parameter Form</strong></p>
<p class="sd-card-text">From the normal density exponent:</p>
<div class="math notranslate nohighlight">
\[\frac{\mu}{\sigma^2} x - \frac{1}{2\sigma^2} x^2\]</div>
<p class="sd-card-text">We identify:</p>
<ul class="simple">
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(\eta_1 = \mu/\sigma^2\)</span> (coefficient of <span class="math notranslate nohighlight">\(x\)</span>)</p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(\eta_2 = -1/(2\sigma^2)\)</span> (coefficient of <span class="math notranslate nohighlight">\(x^2\)</span>)</p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(T_1(x) = x\)</span></p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(T_2(x) = x^2\)</span></p></li>
</ul>
<p class="sd-card-text">Solving for the original parameters:</p>
<div class="math notranslate nohighlight">
\[\sigma^2 = -\frac{1}{2\eta_2}, \quad \mu = \eta_1 \sigma^2 = -\frac{\eta_1}{2\eta_2}\]</div>
<p class="sd-card-text"><strong>Natural parameter space</strong>: <span class="math notranslate nohighlight">\(\eta_2 &lt; 0\)</span> (for <span class="math notranslate nohighlight">\(\sigma^2 &gt; 0\)</span>), <span class="math notranslate nohighlight">\(\eta_1 \in \mathbb{R}\)</span>.</p>
<p class="sd-card-text"><strong>Part (b): Log-Partition Function</strong></p>
<div class="math notranslate nohighlight">
\[A(\eta_1, \eta_2) = \log \int_{-\infty}^{\infty} \exp(\eta_1 x + \eta_2 x^2) dx\]</div>
<p class="sd-card-text">Complete the square:</p>
<div class="math notranslate nohighlight">
\[\eta_1 x + \eta_2 x^2 = \eta_2 \left(x + \frac{\eta_1}{2\eta_2}\right)^2 - \frac{\eta_1^2}{4\eta_2}\]</div>
<p class="sd-card-text">The integral becomes:</p>
<div class="math notranslate nohighlight">
\[\exp\left(-\frac{\eta_1^2}{4\eta_2}\right) \int \exp\left(\eta_2\left(x + \frac{\eta_1}{2\eta_2}\right)^2\right) dx\]</div>
<p class="sd-card-text">Since <span class="math notranslate nohighlight">\(\eta_2 &lt; 0\)</span>, let <span class="math notranslate nohighlight">\(-\eta_2 = 1/(2\sigma^2)\)</span>, giving:</p>
<div class="math notranslate nohighlight">
\[\int \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right) dx = \sqrt{2\pi\sigma^2} = \sqrt{\frac{\pi}{-\eta_2}}\]</div>
<p class="sd-card-text">Therefore:</p>
<div class="math notranslate nohighlight">
\[A(\eta_1, \eta_2) = -\frac{\eta_1^2}{4\eta_2} + \frac{1}{2}\log\left(\frac{\pi}{-\eta_2}\right) = -\frac{\eta_1^2}{4\eta_2} - \frac{1}{2}\log(-2\eta_2) + \frac{1}{2}\log(2\pi)\]</div>
<p class="sd-card-text">Dropping the constant <span class="math notranslate nohighlight">\(\frac{1}{2}\log(2\pi)\)</span> (absorbed into <span class="math notranslate nohighlight">\(h(x)\)</span>):</p>
<div class="math notranslate nohighlight">
\[A(\eta_1, \eta_2) = -\frac{\eta_1^2}{4\eta_2} - \frac{1}{2}\log(-2\eta_2)\]</div>
<p class="sd-card-text"><strong>Part (c): Gradient Verification</strong></p>
<div class="math notranslate nohighlight">
\[\frac{\partial A}{\partial \eta_1} = -\frac{2\eta_1}{4\eta_2} = -\frac{\eta_1}{2\eta_2} = \mu \quad \checkmark\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial A}{\partial \eta_2} = \frac{\eta_1^2}{4\eta_2^2} - \frac{1}{2} \cdot \frac{-2}{-2\eta_2} = \frac{\eta_1^2}{4\eta_2^2} + \frac{1}{2\eta_2}\]</div>
<p class="sd-card-text">Converting to <span class="math notranslate nohighlight">\(\mu, \sigma^2\)</span>:</p>
<div class="math notranslate nohighlight">
\[= \frac{\mu^2/\sigma^4}{1/\sigma^4} + \frac{1}{-1/\sigma^2} = \mu^2 + \sigma^2 = \mathbb{E}[X^2] \quad \checkmark\]</div>
<p class="sd-card-text"><strong>Part (d): Hessian and Fisher Information</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">normal_exponential_family_analysis</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma_sq</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Analyze normal distribution as 2-parameter exponential family.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma_sq</span><span class="p">)</span>

    <span class="c1"># Natural parameters</span>
    <span class="n">eta1</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">/</span> <span class="n">sigma_sq</span>
    <span class="n">eta2</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma_sq</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;NORMAL DISTRIBUTION: 2-PARAMETER EXPONENTIAL FAMILY&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Parameters: μ = </span><span class="si">{</span><span class="n">mu</span><span class="si">}</span><span class="s2">, σ² = </span><span class="si">{</span><span class="n">sigma_sq</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Natural parameters: η₁ = </span><span class="si">{</span><span class="n">eta1</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">, η₂ = </span><span class="si">{</span><span class="n">eta2</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>

    <span class="c1"># Gradient of A (theoretical)</span>
    <span class="n">grad_A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
        <span class="o">-</span><span class="n">eta1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">eta2</span><span class="p">),</span>  <span class="c1"># = μ</span>
        <span class="n">eta1</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">eta2</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">eta2</span><span class="p">)</span>  <span class="c1"># = μ² + σ²</span>
    <span class="p">])</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;GRADIENT ∇A(η) = E[T(X)]:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  ∂A/∂η₁ = </span><span class="si">{</span><span class="n">grad_A</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (should be μ = </span><span class="si">{</span><span class="n">mu</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  ∂A/∂η₂ = </span><span class="si">{</span><span class="n">grad_A</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (should be μ² + σ² = </span><span class="si">{</span><span class="n">mu</span><span class="o">**</span><span class="mi">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">sigma_sq</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>

    <span class="c1"># Hessian of A (theoretical)</span>
    <span class="n">H11</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">eta2</span><span class="p">)</span>  <span class="c1"># = σ²</span>
    <span class="n">H12</span> <span class="o">=</span> <span class="n">eta1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">eta2</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># = 2μσ²</span>
    <span class="n">H22</span> <span class="o">=</span> <span class="o">-</span><span class="n">eta1</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">eta2</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">eta2</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># = 2σ⁴ + 2μ²σ²</span>

    <span class="n">hessian_A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">H11</span><span class="p">,</span> <span class="n">H12</span><span class="p">],</span> <span class="p">[</span><span class="n">H12</span><span class="p">,</span> <span class="n">H22</span><span class="p">]])</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;HESSIAN ∇²A(η) = Cov(T(X)):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  ∂²A/∂η₁² = Var(X) = </span><span class="si">{</span><span class="n">H11</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (should be σ² = </span><span class="si">{</span><span class="n">sigma_sq</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  ∂²A/∂η₁∂η₂ = Cov(X, X²) = </span><span class="si">{</span><span class="n">H12</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  ∂²A/∂η₂² = Var(X²) = </span><span class="si">{</span><span class="n">H22</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>

    <span class="c1"># Monte Carlo verification</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="n">T1</span> <span class="o">=</span> <span class="n">X</span>
    <span class="n">T2</span> <span class="o">=</span> <span class="n">X</span><span class="o">**</span><span class="mi">2</span>

    <span class="n">mc_cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">T1</span><span class="p">,</span> <span class="n">T2</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MONTE CARLO VERIFICATION (n = </span><span class="si">{:,}</span><span class="s2">):&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_samples</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Var(X) = </span><span class="si">{</span><span class="n">mc_cov</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="n">H11</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Cov(X, X²) = </span><span class="si">{</span><span class="n">mc_cov</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="n">H12</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Var(X²) = </span><span class="si">{</span><span class="n">mc_cov</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="n">H22</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>

    <span class="c1"># Positive definiteness check</span>
    <span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvalsh</span><span class="p">(</span><span class="n">hessian_A</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;CONVEXITY CHECK:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Eigenvalues of ∇²A: </span><span class="si">{</span><span class="n">eigenvalues</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Positive definite: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">eigenvalues</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">normal_exponential_family_analysis</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">sigma_sq</span><span class="o">=</span><span class="mf">3.0</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>NORMAL DISTRIBUTION: 2-PARAMETER EXPONENTIAL FAMILY
=================================================================
Parameters: μ = 2.0, σ² = 3.0

Natural parameters: η₁ = 0.666667, η₂ = -0.166667

GRADIENT ∇A(η) = E[T(X)]:
  ∂A/∂η₁ = 2.000000 (should be μ = 2.0)
  ∂A/∂η₂ = 7.000000 (should be μ² + σ² = 7.0)

HESSIAN ∇²A(η) = Cov(T(X)):
  ∂²A/∂η₁² = Var(X) = 3.000000 (should be σ² = 3.0)
  ∂²A/∂η₁∂η₂ = Cov(X, X²) = 12.000000
  ∂²A/∂η₂² = Var(X²) = 60.000000

MONTE CARLO VERIFICATION (n = 100,000):
  Var(X) = 3.010849 (theory: 3.000000)
  Cov(X, X²) = 12.135051 (theory: 12.000000)
  Var(X²) = 60.586389 (theory: 60.000000)

CONVEXITY CHECK:
  Eigenvalues of ∇²A: [ 2.99352249 60.00647751]
  Positive definite: True
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Two sufficient statistics</strong>: The normal family requires <span class="math notranslate nohighlight">\((X, X^2)\)</span> to capture information about both <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p></li>
<li><p class="sd-card-text"><strong>Gradient = means</strong>: <span class="math notranslate nohighlight">\(\nabla A\)</span> gives <span class="math notranslate nohighlight">\((\mathbb{E}[X], \mathbb{E}[X^2])\)</span>, from which we recover <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p></li>
<li><p class="sd-card-text"><strong>Hessian = covariance</strong>: The Hessian <span class="math notranslate nohighlight">\(\nabla^2 A\)</span> is exactly the covariance matrix of the sufficient statistics.</p></li>
<li><p class="sd-card-text"><strong>Guaranteed convexity</strong>: Since <span class="math notranslate nohighlight">\(\nabla^2 A = \text{Cov}(\mathbf{T})\)</span> is a covariance matrix, it is always positive semidefinite, ensuring <span class="math notranslate nohighlight">\(A\)</span> is convex.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 3: Sufficiency and the Neyman-Fisher Factorization</p>
<p>The <strong>Poisson distribution</strong> provides a clean example of how exponential families yield minimal sufficient statistics that achieve dramatic dimension reduction.</p>
<div class="note admonition">
<p class="admonition-title">Background: Why Sufficiency Matters</p>
<p>A statistic <span class="math notranslate nohighlight">\(T(\mathbf{X})\)</span> is <strong>sufficient</strong> for <span class="math notranslate nohighlight">\(\theta\)</span> if it captures all information in the data about <span class="math notranslate nohighlight">\(\theta\)</span>. For exponential families, the sufficient statistic is precisely the inner product <span class="math notranslate nohighlight">\(\boldsymbol{\eta}^\top \mathbf{T}(\mathbf{x})\)</span>—no other function of the data can improve inference. This enables massive data compression without information loss.</p>
</div>
<ol class="loweralpha">
<li><p><strong>Factorization proof</strong>: For i.i.d. observations <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \sim \text{Poisson}(\lambda)\)</span>, use the Neyman-Fisher factorization theorem to prove that <span class="math notranslate nohighlight">\(T = \sum_{i=1}^n X_i\)</span> is sufficient for <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Joint PMF</p>
<p>Write the joint PMF <span class="math notranslate nohighlight">\(P(X_1 = x_1, \ldots, X_n = x_n | \lambda)\)</span> and factor it as <span class="math notranslate nohighlight">\(g(T, \lambda) \cdot h(\mathbf{x})\)</span> where <span class="math notranslate nohighlight">\(h\)</span> doesn’t depend on <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
</div>
</li>
<li><p><strong>Information equivalence</strong>: Generate two datasets with <span class="math notranslate nohighlight">\(n = 1000\)</span> observations each but the same sum <span class="math notranslate nohighlight">\(T\)</span>. Verify they yield identical likelihood functions by plotting <span class="math notranslate nohighlight">\(L(\lambda)\)</span> for both.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Conditional Distribution</p>
<p>Given <span class="math notranslate nohighlight">\(T = t\)</span>, the individual observations are conditionally multinomial with equal probabilities. You can generate a dataset with a specific sum by distributing <span class="math notranslate nohighlight">\(t\)</span> “events” uniformly across <span class="math notranslate nohighlight">\(n\)</span> bins.</p>
</div>
</li>
<li><p><strong>Dimension reduction</strong>: For <span class="math notranslate nohighlight">\(n = 1000\)</span> observations from <span class="math notranslate nohighlight">\(\text{Poisson}(5)\)</span>, compare storage/computation requirements using the full dataset versus the sufficient statistic. How much compression is achieved?</p></li>
<li><p><strong>Counter-example exploration</strong>: The Uniform(<span class="math notranslate nohighlight">\(0, \theta\)</span>) distribution is NOT a regular exponential family. Generate <span class="math notranslate nohighlight">\(n = 100\)</span> samples and show that <span class="math notranslate nohighlight">\(T = \sum X_i\)</span> is NOT sufficient, but <span class="math notranslate nohighlight">\(T = \max(X_i)\)</span> is.</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Factorization Proof</strong></p>
<p class="sd-card-text">The joint PMF for i.i.d. Poisson observations:</p>
<div class="math notranslate nohighlight">
\[P(\mathbf{X} = \mathbf{x} | \lambda) = \prod_{i=1}^n \frac{\lambda^{x_i} e^{-\lambda}}{x_i!} = \frac{\lambda^{\sum_i x_i} e^{-n\lambda}}{\prod_i x_i!}\]</div>
<p class="sd-card-text">Factoring:</p>
<div class="math notranslate nohighlight">
\[= \underbrace{\lambda^T e^{-n\lambda}}_{g(T, \lambda)} \cdot \underbrace{\frac{1}{\prod_i x_i!}}_{h(\mathbf{x})}\]</div>
<p class="sd-card-text">where <span class="math notranslate nohighlight">\(T = \sum_{i=1}^n x_i\)</span>. Since the PMF factors into a function of <span class="math notranslate nohighlight">\((T, \lambda)\)</span> and a function of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> alone, by Neyman-Fisher, <span class="math notranslate nohighlight">\(T\)</span> is sufficient for <span class="math notranslate nohighlight">\(\lambda\)</span>. ∎</p>
<p class="sd-card-text"><strong>Parts (b)-(d): Implementation</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">sufficiency_demonstration</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">lambda_true</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Demonstrate sufficiency for Poisson distribution.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;SUFFICIENCY DEMONSTRATION: POISSON DISTRIBUTION&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>

    <span class="c1"># Part (b): Information equivalence</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">PART (b): INFORMATION EQUIVALENCE&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>

    <span class="c1"># Dataset 1: Original Poisson sample</span>
    <span class="n">X1</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="n">lambda_true</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">T</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X1</span><span class="p">)</span>

    <span class="c1"># Dataset 2: Same sum, different individual values</span>
    <span class="c1"># Distribute T events uniformly across n bins</span>
    <span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
        <span class="n">X2</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dataset 1: n=</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">, sum=</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X1</span><span class="p">)</span><span class="si">}</span><span class="s2">, mean=</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X1</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dataset 2: n=</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">, sum=</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X2</span><span class="p">)</span><span class="si">}</span><span class="s2">, mean=</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X2</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Same sufficient statistic T = </span><span class="si">{</span><span class="n">T</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Compute log-likelihoods</span>
    <span class="n">lambda_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">poisson_loglik</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">lam</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Log-likelihood for Poisson sample.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">lam</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">*</span> <span class="n">lam</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">factorial</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)))</span>

    <span class="c1"># Note: The factorial terms cancel when comparing, so we use simplified form</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">poisson_loglik_simple</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">lam</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Simplified log-likelihood using sufficient statistic.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">T</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">lam</span><span class="p">)</span> <span class="o">-</span> <span class="n">n</span> <span class="o">*</span> <span class="n">lam</span>

    <span class="n">ll1</span> <span class="o">=</span> <span class="p">[</span><span class="n">poisson_loglik_simple</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X1</span><span class="p">),</span> <span class="n">n</span><span class="p">,</span> <span class="n">lam</span><span class="p">)</span> <span class="k">for</span> <span class="n">lam</span> <span class="ow">in</span> <span class="n">lambda_grid</span><span class="p">]</span>
    <span class="n">ll2</span> <span class="o">=</span> <span class="p">[</span><span class="n">poisson_loglik_simple</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X2</span><span class="p">),</span> <span class="n">n</span><span class="p">,</span> <span class="n">lam</span><span class="p">)</span> <span class="k">for</span> <span class="n">lam</span> <span class="ow">in</span> <span class="n">lambda_grid</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Max likelihood difference: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ll1</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ll2</span><span class="p">)))</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Likelihoods are IDENTICAL (as guaranteed by sufficiency)&quot;</span><span class="p">)</span>

    <span class="c1"># Part (c): Dimension reduction</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;PART (c): DIMENSION REDUCTION&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>

    <span class="n">bytes_full</span> <span class="o">=</span> <span class="n">X1</span><span class="o">.</span><span class="n">nbytes</span>
    <span class="n">bytes_sufficient</span> <span class="o">=</span> <span class="mi">8</span>  <span class="c1"># One 64-bit integer for the sum</span>
    <span class="n">compression_ratio</span> <span class="o">=</span> <span class="n">bytes_full</span> <span class="o">/</span> <span class="n">bytes_sufficient</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Full dataset: </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2"> integers = </span><span class="si">{</span><span class="n">bytes_full</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> bytes&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sufficient statistic: 1 integer = </span><span class="si">{</span><span class="n">bytes_sufficient</span><span class="si">}</span><span class="s2"> bytes&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Compression ratio: </span><span class="si">{</span><span class="n">compression_ratio</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Information loss: ZERO (by sufficiency theorem)&quot;</span><span class="p">)</span>

    <span class="c1"># Part (d): Uniform distribution counter-example</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;PART (d): UNIFORM(0, θ) - NOT REGULAR EXPONENTIAL FAMILY&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>

    <span class="n">theta_true</span> <span class="o">=</span> <span class="mf">10.0</span>
    <span class="n">n_unif</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">X_unif</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">theta_true</span><span class="p">,</span> <span class="n">n_unif</span><span class="p">)</span>

    <span class="n">T_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X_unif</span><span class="p">)</span>
    <span class="n">T_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">X_unif</span><span class="p">)</span>

    <span class="c1"># Generate another sample with same sum but different max</span>
    <span class="c1"># (This shows T_sum is not sufficient)</span>
    <span class="n">X_unif2</span> <span class="o">=</span> <span class="n">X_unif</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="c1"># Swap some values to change max while preserving sum approximately</span>
    <span class="n">X_unif2</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">X_unif2</span><span class="p">)]</span> <span class="o">=</span> <span class="n">X_unif2</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">X_unif2</span><span class="p">)]</span> <span class="o">*</span> <span class="mf">0.9</span>
    <span class="n">adjustment</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X_unif</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X_unif2</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_unif</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_unif</span><span class="p">)</span> <span class="o">!=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">X_unif</span><span class="p">)</span>
    <span class="n">X_unif2</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">+=</span> <span class="n">adjustment</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dataset 1: sum = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X_unif</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, max = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">X_unif</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dataset 2: sum = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X_unif2</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, max = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">X_unif2</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Likelihood for Uniform(0, θ)</span>
    <span class="c1"># L(θ) = (1/θ)^n if max(x_i) ≤ θ, else 0</span>
    <span class="c1"># MLE = max(x_i)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">For Uniform(0, θ):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  MLE = max(X) = </span><span class="si">{</span><span class="n">T_max</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (true θ = </span><span class="si">{</span><span class="n">theta_true</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  The SUM is NOT sufficient (different max → different MLE)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  The MAX is sufficient (Neyman-Fisher: L ∝ θ^</span><span class="si">{</span><span class="o">-</span><span class="n">n</span><span class="si">}</span><span class="s2"> · I(θ ≥ max))&quot;</span><span class="p">)</span>

<span class="n">sufficiency_demonstration</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>SUFFICIENCY DEMONSTRATION: POISSON DISTRIBUTION
=================================================================

PART (b): INFORMATION EQUIVALENCE
----------------------------------------
Dataset 1: n=1000, sum=4976, mean=4.976
Dataset 2: n=1000, sum=4976, mean=4.976
Same sufficient statistic T = 4976

Max likelihood difference: 0.00e+00
Likelihoods are IDENTICAL (as guaranteed by sufficiency)

-----------------------------------------------------------------
PART (c): DIMENSION REDUCTION
----------------------------------------
Full dataset: 1000 integers = 8,000 bytes
Sufficient statistic: 1 integer = 8 bytes
Compression ratio: 1000x
Information loss: ZERO (by sufficiency theorem)

-----------------------------------------------------------------
PART (d): UNIFORM(0, θ) - NOT REGULAR EXPONENTIAL FAMILY
----------------------------------------
Dataset 1: sum = 502.8648, max = 9.9616
Dataset 2: sum = 502.8648, max = 8.9654

For Uniform(0, θ):
  MLE = max(X) = 9.9616 (true θ = 10.0)
  The SUM is NOT sufficient (different max → different MLE)
  The MAX is sufficient (Neyman-Fisher: L ∝ θ^{-n} · I(θ ≥ max))
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Lossless compression</strong>: For Poisson with <span class="math notranslate nohighlight">\(n = 1000\)</span>, we compress from 1000 values to 1 value with zero information loss.</p></li>
<li><p class="sd-card-text"><strong>Likelihood depends only on T</strong>: Two datasets with identical <span class="math notranslate nohighlight">\(T\)</span> have identical likelihoods for all <span class="math notranslate nohighlight">\(\lambda\)</span>—the individual values are irrelevant.</p></li>
<li><p class="sd-card-text"><strong>Exponential family structure</strong>: Sufficiency follows directly from the exponential family form where <span class="math notranslate nohighlight">\(\eta = \log\lambda\)</span> and <span class="math notranslate nohighlight">\(T(x) = x\)</span>.</p></li>
<li><p class="sd-card-text"><strong>Non-exponential families differ</strong>: For Uniform(<span class="math notranslate nohighlight">\(0, \theta\)</span>), the maximum (not sum) is sufficient because the likelihood depends on whether <span class="math notranslate nohighlight">\(\theta \geq \max(x_i)\)</span>.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 4: Conjugate Prior Construction and Bayesian Updating</p>
<p>The <strong>Gamma-Poisson</strong> conjugate pair illustrates how exponential families enable closed-form Bayesian inference.</p>
<div class="note admonition">
<p class="admonition-title">Background: Conjugate Priors</p>
<p>For an exponential family likelihood, a conjugate prior exists that maintains the same functional form after updating with data. This enables sequential updating: the posterior from one observation becomes the prior for the next. The conjugate prior for an exponential family with natural parameter <span class="math notranslate nohighlight">\(\eta\)</span> takes the form <span class="math notranslate nohighlight">\(\pi(\eta) \propto \exp(\nu \eta - \tau A(\eta))\)</span>, where <span class="math notranslate nohighlight">\((\nu, \tau)\)</span> are hyperparameters interpretable as “prior data.”</p>
</div>
<p>Consider observations <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} \text{Poisson}(\lambda)\)</span>.</p>
<ol class="loweralpha">
<li><p><strong>Conjugate prior derivation</strong>: The Gamma(<span class="math notranslate nohighlight">\(\alpha, \beta\)</span>) prior on <span class="math notranslate nohighlight">\(\lambda\)</span> has density <span class="math notranslate nohighlight">\(\pi(\lambda) \propto \lambda^{\alpha-1} e^{-\beta\lambda}\)</span>. Show this is conjugate to the Poisson likelihood by deriving the posterior distribution.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Combine Terms</p>
<p>Write the posterior <span class="math notranslate nohighlight">\(\pi(\lambda | \mathbf{x}) \propto L(\lambda) \pi(\lambda)\)</span> and collect powers of <span class="math notranslate nohighlight">\(\lambda\)</span> and coefficients of <span class="math notranslate nohighlight">\(e^{-\lambda}\)</span>.</p>
</div>
</li>
<li><p><strong>Prior interpretation</strong>: Show that the Gamma(<span class="math notranslate nohighlight">\(\alpha, \beta\)</span>) prior can be interpreted as having observed <span class="math notranslate nohighlight">\(\alpha - 1\)</span> total events in <span class="math notranslate nohighlight">\(\beta\)</span> prior observations. How does the prior sample size affect posterior inference?</p></li>
<li><p><strong>Sequential updating</strong>: Implement Bayesian updating that processes data in three batches. Start with Gamma(2, 1) prior and observe:</p>
<ul class="simple">
<li><p>Batch 1: <span class="math notranslate nohighlight">\(\mathbf{x}_1 = (3, 5, 2)\)</span> (n=3)</p></li>
<li><p>Batch 2: <span class="math notranslate nohighlight">\(\mathbf{x}_2 = (4, 6, 3, 5)\)</span> (n=4)</p></li>
<li><p>Batch 3: <span class="math notranslate nohighlight">\(\mathbf{x}_3 = (5, 4, 5)\)</span> (n=3)</p></li>
</ul>
<p>Show that processing sequentially yields the same posterior as processing all data at once.</p>
</li>
<li><p><strong>Posterior predictive</strong>: Derive and compute the posterior predictive distribution <span class="math notranslate nohighlight">\(P(X_{n+1} = k | \mathbf{x})\)</span>. Show it is Negative Binomial.</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Conjugate Prior Derivation</strong></p>
<p class="sd-card-text">Likelihood:</p>
<div class="math notranslate nohighlight">
\[L(\lambda | \mathbf{x}) \propto \lambda^{\sum x_i} e^{-n\lambda}\]</div>
<p class="sd-card-text">Prior:</p>
<div class="math notranslate nohighlight">
\[\pi(\lambda) \propto \lambda^{\alpha-1} e^{-\beta\lambda}\]</div>
<p class="sd-card-text">Posterior:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\pi(\lambda | \mathbf{x}) &amp;\propto L(\lambda | \mathbf{x}) \cdot \pi(\lambda) \\
&amp;\propto \lambda^{\sum x_i} e^{-n\lambda} \cdot \lambda^{\alpha-1} e^{-\beta\lambda} \\
&amp;= \lambda^{(\alpha + \sum x_i) - 1} e^{-(\beta + n)\lambda}\end{split}\]</div>
<p class="sd-card-text">This is the kernel of a Gamma(<span class="math notranslate nohighlight">\(\alpha + \sum x_i, \beta + n\)</span>) distribution.</p>
<p class="sd-card-text"><strong>Conjugacy confirmed</strong>: Gamma prior + Poisson likelihood = Gamma posterior.</p>
<p class="sd-card-text"><strong>Part (b): Prior Interpretation</strong></p>
<p class="sd-card-text">The posterior parameters are:</p>
<ul class="simple">
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(\alpha' = \alpha + \sum x_i = \alpha + T\)</span></p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(\beta' = \beta + n\)</span></p></li>
</ul>
<p class="sd-card-text">Interpretation: The prior Gamma(<span class="math notranslate nohighlight">\(\alpha, \beta\)</span>) acts as if we had already observed <span class="math notranslate nohighlight">\(T_0 = \alpha\)</span> total events in <span class="math notranslate nohighlight">\(n_0 = \beta\)</span> pseudo-observations, giving a prior mean <span class="math notranslate nohighlight">\(\mathbb{E}[\lambda] = \alpha/\beta = T_0/n_0\)</span>.</p>
<p class="sd-card-text">The posterior mean is:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[\lambda | \mathbf{x}] = \frac{\alpha + T}{\beta + n} = \frac{\beta}{\beta + n} \cdot \frac{\alpha}{\beta} + \frac{n}{\beta + n} \cdot \frac{T}{n}\]</div>
<p class="sd-card-text">This is a weighted average of prior mean and sample mean, with weights proportional to sample sizes.</p>
<p class="sd-card-text"><strong>Parts (c)-(d): Implementation</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">gamma_poisson_bayesian_updating</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Demonstrate sequential Bayesian updating with conjugate prior.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;GAMMA-POISSON CONJUGATE BAYESIAN UPDATING&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>

    <span class="c1"># Prior: Gamma(α=2, β=1)</span>
    <span class="n">alpha_0</span><span class="p">,</span> <span class="n">beta_0</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span>

    <span class="c1"># Data batches</span>
    <span class="n">batch1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="n">batch2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
    <span class="n">batch3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
    <span class="n">all_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">batch1</span><span class="p">,</span> <span class="n">batch2</span><span class="p">,</span> <span class="n">batch3</span><span class="p">])</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prior: Gamma(</span><span class="si">{</span><span class="n">alpha_0</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">beta_0</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Prior mean: E[λ] = </span><span class="si">{</span><span class="n">alpha_0</span><span class="o">/</span><span class="n">beta_0</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Prior variance: Var(λ) = </span><span class="si">{</span><span class="n">alpha_0</span><span class="o">/</span><span class="n">beta_0</span><span class="o">**</span><span class="mi">2</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>

    <span class="c1"># Sequential updating</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;SEQUENTIAL UPDATING:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>

    <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="n">alpha_0</span><span class="p">,</span> <span class="n">beta_0</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="n">batch1</span><span class="p">,</span> <span class="n">batch2</span><span class="p">,</span> <span class="n">batch3</span><span class="p">],</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">T</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">alpha_new</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">T</span>
        <span class="n">beta_new</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">+</span> <span class="n">n</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Batch </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: data = </span><span class="si">{</span><span class="n">batch</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  T = </span><span class="si">{</span><span class="n">T</span><span class="si">}</span><span class="s2">, n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Posterior: Gamma(</span><span class="si">{</span><span class="n">alpha_new</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">beta_new</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Posterior mean: E[λ|data] = </span><span class="si">{</span><span class="n">alpha_new</span><span class="o">/</span><span class="n">beta_new</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">()</span>

        <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="n">alpha_new</span><span class="p">,</span> <span class="n">beta_new</span>

    <span class="n">sequential_alpha</span><span class="p">,</span> <span class="n">sequential_beta</span> <span class="o">=</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span>

    <span class="c1"># All-at-once updating</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ALL-AT-ONCE UPDATING:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>
    <span class="n">T_all</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">all_data</span><span class="p">)</span>
    <span class="n">n_all</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_data</span><span class="p">)</span>
    <span class="n">batch_alpha</span> <span class="o">=</span> <span class="n">alpha_0</span> <span class="o">+</span> <span class="n">T_all</span>
    <span class="n">batch_beta</span> <span class="o">=</span> <span class="n">beta_0</span> <span class="o">+</span> <span class="n">n_all</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;All data: T = </span><span class="si">{</span><span class="n">T_all</span><span class="si">}</span><span class="s2">, n = </span><span class="si">{</span><span class="n">n_all</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Posterior: Gamma(</span><span class="si">{</span><span class="n">batch_alpha</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">batch_beta</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Posterior mean: E[λ|data] = </span><span class="si">{</span><span class="n">batch_alpha</span><span class="o">/</span><span class="n">batch_beta</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;VERIFICATION:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Sequential: Gamma(</span><span class="si">{</span><span class="n">sequential_alpha</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">sequential_beta</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  All-at-once: Gamma(</span><span class="si">{</span><span class="n">batch_alpha</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">batch_beta</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Match: </span><span class="si">{</span><span class="n">sequential_alpha</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">batch_alpha</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">sequential_beta</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">batch_beta</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Part (d): Posterior predictive</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;PART (d): POSTERIOR PREDICTIVE DISTRIBUTION&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>

    <span class="c1"># X_{n+1} | data ~ NegBinom(r=α&#39;, p=β&#39;/(β&#39;+1))</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">batch_alpha</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">batch_beta</span> <span class="o">/</span> <span class="p">(</span><span class="n">batch_beta</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Posterior: Gamma(α&#39;=</span><span class="si">{</span><span class="n">batch_alpha</span><span class="si">}</span><span class="s2">, β&#39;=</span><span class="si">{</span><span class="n">batch_beta</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Posterior predictive: X_</span><span class="se">{{</span><span class="s2">n+1</span><span class="se">}}</span><span class="s2"> | data ~ NegBinom(r=</span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="s2">, p=</span><span class="si">{</span><span class="n">p</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>

    <span class="c1"># Verify via Monte Carlo</span>
    <span class="n">n_mc</span> <span class="o">=</span> <span class="mi">100_000</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

    <span class="c1"># Draw λ from posterior, then X from Poisson(λ)</span>
    <span class="n">lambda_samples</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">batch_alpha</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="n">batch_beta</span><span class="p">,</span> <span class="n">n_mc</span><span class="p">)</span>
    <span class="n">x_pred_samples</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="n">lambda_samples</span><span class="p">)</span>

    <span class="c1"># Compare to Negative Binomial</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Posterior Predictive Distribution:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;k&#39;</span><span class="si">:</span><span class="s2">&gt;5</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;MC Prob&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;NegBinom&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Difference&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">45</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">mc_prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x_pred_samples</span> <span class="o">==</span> <span class="n">k</span><span class="p">)</span>
        <span class="n">nb_prob</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">nbinom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">k</span><span class="si">:</span><span class="s2">&gt;5</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">mc_prob</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">nb_prob</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">mc_prob</span><span class="o">-</span><span class="n">nb_prob</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Posterior predictive mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x_pred_samples</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Negative Binomial mean: </span><span class="si">{</span><span class="n">r</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">)</span><span class="o">/</span><span class="n">p</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;(= α&#39;/β&#39; = </span><span class="si">{</span><span class="n">batch_alpha</span><span class="o">/</span><span class="n">batch_beta</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> = posterior mean of λ)&quot;</span><span class="p">)</span>

<span class="n">gamma_poisson_bayesian_updating</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>GAMMA-POISSON CONJUGATE BAYESIAN UPDATING
=================================================================
Prior: Gamma(2, 1)
  Prior mean: E[λ] = 2.0000
  Prior variance: Var(λ) = 2.0000

SEQUENTIAL UPDATING:
----------------------------------------
Batch 1: data = [3 5 2]
  T = 10, n = 3
  Posterior: Gamma(12, 4)
  Posterior mean: E[λ|data] = 3.0000

Batch 2: data = [4 6 3 5]
  T = 18, n = 4
  Posterior: Gamma(30, 8)
  Posterior mean: E[λ|data] = 3.7500

Batch 3: data = [5 4 5]
  T = 14, n = 3
  Posterior: Gamma(44, 11)
  Posterior mean: E[λ|data] = 4.0000

ALL-AT-ONCE UPDATING:
----------------------------------------
All data: T = 42, n = 10
Posterior: Gamma(44, 11)
Posterior mean: E[λ|data] = 4.0000

VERIFICATION:
  Sequential: Gamma(44, 11)
  All-at-once: Gamma(44, 11)
  Match: True

-----------------------------------------------------------------
PART (d): POSTERIOR PREDICTIVE DISTRIBUTION
----------------------------------------
Posterior: Gamma(α&#39;=44, β&#39;=11)
Posterior predictive: X_{n+1} | data ~ NegBinom(r=44, p=0.9167)

Posterior Predictive Distribution:
    k      MC Prob     NegBinom   Difference
---------------------------------------------
    0       0.0093       0.0092       0.0001
    1       0.0360       0.0366       0.0006
    2       0.0720       0.0729       0.0009
    3       0.0985       0.0970       0.0015
    4       0.1084       0.1090       0.0006
    5       0.1093       0.1090       0.0003
    6       0.0982       0.0999       0.0017
    7       0.0859       0.0857       0.0002
    8       0.0698       0.0696       0.0002
    9       0.0533       0.0541       0.0008

Posterior predictive mean: 4.0035
Negative Binomial mean: 4.0000
(= α&#39;/β&#39; = 4.0000 = posterior mean of λ)
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Conjugacy enables closed-form updating</strong>: Each batch update is a simple addition to hyperparameters.</p></li>
<li><p class="sd-card-text"><strong>Order doesn’t matter</strong>: Sequential processing yields identical results to batch processing—a consequence of sufficient statistics.</p></li>
<li><p class="sd-card-text"><strong>Prior interpretation</strong>: Gamma(<span class="math notranslate nohighlight">\(\alpha, \beta\)</span>) represents <span class="math notranslate nohighlight">\(\alpha\)</span> pseudo-events in <span class="math notranslate nohighlight">\(\beta\)</span> pseudo-observations.</p></li>
<li><p class="sd-card-text"><strong>Posterior predictive</strong>: Integrating out uncertainty in <span class="math notranslate nohighlight">\(\lambda\)</span> yields a Negative Binomial, which has heavier tails than Poisson—reflecting parameter uncertainty.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 5: Full-Rank versus Curved Exponential Families</p>
<p>The normal family <span class="math notranslate nohighlight">\(\mathcal{N}(\theta, \theta^2)\)</span>, where the variance equals the square of the mean, is a <strong>curved exponential family</strong>—a one-dimensional subfamily of the two-dimensional full-rank normal family.</p>
<div class="note admonition">
<p class="admonition-title">Background: Curved Families</p>
<p>A <strong>curved exponential family</strong> is obtained when the natural parameter vector <span class="math notranslate nohighlight">\(\boldsymbol{\eta}\)</span> is constrained to lie on a lower-dimensional manifold. The constraint reduces the dimension of the parameter space but introduces complications: sufficient statistics may not be complete, and standard exponential family theory requires modification.</p>
</div>
<p>Consider <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(\theta, \theta^2)\)</span> for <span class="math notranslate nohighlight">\(\theta &gt; 0\)</span>.</p>
<ol class="loweralpha">
<li><p><strong>Constraint identification</strong>: Express this family in the full exponential family form for <span class="math notranslate nohighlight">\(\mathcal{N}(\mu, \sigma^2)\)</span> with natural parameters <span class="math notranslate nohighlight">\((\eta_1, \eta_2)\)</span>. What constraint relates <span class="math notranslate nohighlight">\(\eta_1\)</span> and <span class="math notranslate nohighlight">\(\eta_2\)</span> when <span class="math notranslate nohighlight">\(\sigma^2 = \mu^2\)</span>?</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Parameter Mapping</p>
<p>For <span class="math notranslate nohighlight">\(\mathcal{N}(\mu, \sigma^2)\)</span>: <span class="math notranslate nohighlight">\(\eta_1 = \mu/\sigma^2\)</span> and <span class="math notranslate nohighlight">\(\eta_2 = -1/(2\sigma^2)\)</span>. With <span class="math notranslate nohighlight">\(\sigma^2 = \mu^2 = \theta^2\)</span>, express both in terms of <span class="math notranslate nohighlight">\(\theta\)</span> and eliminate <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
</div>
</li>
<li><p><strong>Geometric interpretation</strong>: In the <span class="math notranslate nohighlight">\((\eta_1, \eta_2)\)</span> plane, the full-rank normal family fills a half-plane (<span class="math notranslate nohighlight">\(\eta_2 &lt; 0\)</span>). Sketch the constraint curve from part (a) and verify it’s a parabola.</p></li>
<li><p><strong>Non-completeness</strong>: For a full exponential family, the sufficient statistic is complete, meaning no non-trivial function of <span class="math notranslate nohighlight">\(T\)</span> has expectation zero for all parameter values. Show that for <span class="math notranslate nohighlight">\(\mathcal{N}(\theta, \theta^2)\)</span>, the statistic <span class="math notranslate nohighlight">\(T = (X, X^2)\)</span> is NOT complete by finding a function <span class="math notranslate nohighlight">\(g(T)\)</span> with <span class="math notranslate nohighlight">\(\mathbb{E}_\theta[g(T)] = 0\)</span> for all <span class="math notranslate nohighlight">\(\theta &gt; 0\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Exploiting the Constraint</p>
<p>If <span class="math notranslate nohighlight">\(\mathbb{E}[X] = \theta\)</span> and <span class="math notranslate nohighlight">\(\text{Var}(X) = \theta^2\)</span>, then <span class="math notranslate nohighlight">\(\mathbb{E}[X^2] = \theta^2 + \theta^2 = 2\theta^2\)</span>. Can you construct <span class="math notranslate nohighlight">\(g(X, X^2)\)</span> such that <span class="math notranslate nohighlight">\(\mathbb{E}[g] = 0\)</span>?</p>
</div>
</li>
<li><p><strong>MLE comparison</strong>: Generate <span class="math notranslate nohighlight">\(n = 100\)</span> samples from <span class="math notranslate nohighlight">\(\mathcal{N}(3, 9)\)</span>. Compare:</p>
<ul class="simple">
<li><p>Full-rank MLE: <span class="math notranslate nohighlight">\((\hat{\mu}, \hat{\sigma}^2) = (\bar{X}, S^2)\)</span></p></li>
<li><p>Constrained MLE: <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> satisfying the constraint</p></li>
</ul>
</li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Constraint Identification</strong></p>
<p class="sd-card-text">For <span class="math notranslate nohighlight">\(\mathcal{N}(\mu, \sigma^2)\)</span>:</p>
<ul class="simple">
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(\eta_1 = \mu/\sigma^2\)</span></p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(\eta_2 = -1/(2\sigma^2)\)</span></p></li>
</ul>
<p class="sd-card-text">With constraint <span class="math notranslate nohighlight">\(\sigma^2 = \mu^2 = \theta^2\)</span>:</p>
<ul class="simple">
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(\eta_1 = \theta/\theta^2 = 1/\theta\)</span></p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(\eta_2 = -1/(2\theta^2)\)</span></p></li>
</ul>
<p class="sd-card-text">Eliminating <span class="math notranslate nohighlight">\(\theta\)</span>:</p>
<div class="math notranslate nohighlight">
\[\theta = 1/\eta_1 \implies \eta_2 = -\frac{1}{2(1/\eta_1)^2} = -\frac{\eta_1^2}{2}\]</div>
<p class="sd-card-text"><strong>Constraint</strong>: <span class="math notranslate nohighlight">\(\eta_2 = -\eta_1^2/2\)</span> (a parabola opening downward).</p>
<p class="sd-card-text"><strong>Part (b): Geometric Interpretation</strong></p>
<ul class="simple">
<li><p class="sd-card-text">Full-rank family: <span class="math notranslate nohighlight">\(\{(\eta_1, \eta_2) : \eta_1 \in \mathbb{R}, \eta_2 &lt; 0\}\)</span> (lower half-plane)</p></li>
<li><p class="sd-card-text">Curved family: <span class="math notranslate nohighlight">\(\{(\eta_1, -\eta_1^2/2) : \eta_1 \neq 0\}\)</span> (parabola in lower half-plane)</p></li>
</ul>
<p class="sd-card-text">For <span class="math notranslate nohighlight">\(\theta &gt; 0\)</span>: <span class="math notranslate nohighlight">\(\eta_1 = 1/\theta &gt; 0\)</span>, so we trace the right branch.
For <span class="math notranslate nohighlight">\(\theta &lt; 0\)</span>: <span class="math notranslate nohighlight">\(\eta_1 = 1/\theta &lt; 0\)</span> (left branch, if we extended the family).</p>
<p class="sd-card-text"><strong>Part (c): Non-Completeness</strong></p>
<p class="sd-card-text">For <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(\theta, \theta^2)\)</span>:</p>
<ul class="simple">
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(\mathbb{E}[X] = \theta\)</span></p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(\mathbb{E}[X^2] = \text{Var}(X) + (\mathbb{E}[X])^2 = \theta^2 + \theta^2 = 2\theta^2\)</span></p></li>
</ul>
<p class="sd-card-text">Consider <span class="math notranslate nohighlight">\(g(X, X^2) = X^2 - 2X^2 = -X^2\)</span>… no, that doesn’t work.</p>
<p class="sd-card-text">Try: <span class="math notranslate nohighlight">\(g(X) = X^2 - 2(\mathbb{E}[X])^2\)</span>… but we need <span class="math notranslate nohighlight">\(g\)</span> that doesn’t depend on <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p class="sd-card-text">The key: <span class="math notranslate nohighlight">\(\mathbb{E}[X^2] = 2\mathbb{E}[X]^2\)</span> under the constraint.</p>
<p class="sd-card-text">Let <span class="math notranslate nohighlight">\(g(x, x^2) = x^2 - 2\)</span> … no.</p>
<p class="sd-card-text">Actually, we need: <span class="math notranslate nohighlight">\(\mathbb{E}[X^2 - 2\theta^2] = 0\)</span>. But <span class="math notranslate nohighlight">\(\theta\)</span> appears.</p>
<p class="sd-card-text">The correct approach: <span class="math notranslate nohighlight">\(\mathbb{E}[X^2] = 2\theta^2\)</span> and <span class="math notranslate nohighlight">\(\mathbb{E}[X^4] = 3\theta^4 + 6\theta^4 = 3\sigma^4 + 6\mu^2\sigma^2 = 3\theta^4 + 6\theta^4 = 9\theta^4\)</span> (using <span class="math notranslate nohighlight">\(\mathbb{E}[X^4] = 3\sigma^4 + 6\mu^2\sigma^2 + \mu^4\)</span>).</p>
<p class="sd-card-text">So <span class="math notranslate nohighlight">\(\mathbb{E}[X^4] = 9\theta^4 = 9(\theta^2)^2 = 9(\mathbb{E}[X^2]/2)^2 = 9\mathbb{E}[X^2]^2/4\)</span>.</p>
<p class="sd-card-text">Therefore <span class="math notranslate nohighlight">\(\mathbb{E}[X^4 - (9/4)(X^2)^2]\)</span>… still not independent of <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p class="sd-card-text"><strong>Correct construction</strong>: Use <span class="math notranslate nohighlight">\(g(X) = X^2 - 2X \cdot |X|\)</span> (informal) or more rigorously, note that completeness fails because there exists a non-trivial unbiased estimator of zero.</p>
<p class="sd-card-text">For simplicity, consider: Since <span class="math notranslate nohighlight">\(\text{Var}(X) = \mu^2\)</span>, we have <span class="math notranslate nohighlight">\(\mathbb{E}[(X-\mu)^2] = \mu^2\)</span>, so <span class="math notranslate nohighlight">\(\mathbb{E}[X^2 - 2\mu X + \mu^2] = \mu^2\)</span>, giving <span class="math notranslate nohighlight">\(\mathbb{E}[X^2] - 2\mu^2 + \mu^2 = \mu^2\)</span>, thus <span class="math notranslate nohighlight">\(\mathbb{E}[X^2] = 2\mu^2\)</span>.</p>
<p class="sd-card-text">The function <span class="math notranslate nohighlight">\(g(X, X^2) = X^2 - 2X^2 = -X^2\)</span> doesn’t work. Let’s verify numerically.</p>
<p class="sd-card-text"><strong>Parts (c)-(d): Implementation</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">optimize</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">curved_exponential_family_analysis</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Analyze N(θ, θ²) as a curved exponential family.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;CURVED EXPONENTIAL FAMILY: N(θ, θ²)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>

    <span class="c1"># Part (a)-(b): Constraint visualization</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">PART (a)-(b): CONSTRAINT IN NATURAL PARAMETER SPACE&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>

    <span class="c1"># Full-rank: (η₁, η₂) with η₂ &lt; 0</span>
    <span class="c1"># Curved: η₂ = -η₁²/2</span>

    <span class="n">eta1_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">eta2_constraint</span> <span class="o">=</span> <span class="o">-</span><span class="n">eta1_range</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mi">2</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Full-rank family: {(η₁, η₂) : η₁ ∈ ℝ, η₂ &lt; 0}&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Curved family constraint: η₂ = -η₁²/2&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;For θ &gt; 0: η₁ = 1/θ &gt; 0, η₂ = -1/(2θ²) &lt; 0&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Example: θ = 2 → η₁ = 0.5, η₂ = -0.125&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;         θ = 3 → η₁ = 0.333, η₂ = -0.056&quot;</span><span class="p">)</span>

    <span class="c1"># Part (c): Non-completeness demonstration</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;PART (c): NON-COMPLETENESS OF SUFFICIENT STATISTIC&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>

    <span class="c1"># For N(θ, θ²):</span>
    <span class="c1"># E[X] = θ</span>
    <span class="c1"># E[X²] = Var(X) + E[X]² = θ² + θ² = 2θ²</span>
    <span class="c1"># E[X³] = 3θ·θ² + θ³ = 4θ³ (using E[X³] = μ³ + 3μσ²)</span>
    <span class="c1"># Actually E[X³] = E[(μ + σZ)³] = μ³ + 3μσ² for Z~N(0,1)</span>
    <span class="c1">#                = θ³ + 3θ·θ² = 4θ³</span>

    <span class="c1"># So E[X³] = 4θ³ and E[X]³ = θ³</span>
    <span class="c1"># Thus E[X³] = 4·E[X]³</span>
    <span class="c1"># Consider g(X) = X³ - 4(sample mean)³... but this depends on all data</span>

    <span class="c1"># For non-completeness, we show T = (X, X²) doesn&#39;t uniquely determine θ</span>
    <span class="c1"># through its expectation alone - we need a function g with E[g(T)] = 0.</span>

    <span class="c1"># Key insight: E[X²] = 2·E[X]² under the constraint</span>
    <span class="c1"># So g(x) = x² - 2·(some function)... but we need g to not depend on θ</span>

    <span class="c1"># Actually, the non-completeness is subtle. Let&#39;s verify via simulation.</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Under constraint σ² = μ²:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  E[X] = θ&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  E[X²] = 2θ²&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  E[X⁴] = 3σ⁴ + 6μ²σ² + μ⁴ = 3θ⁴ + 6θ⁴ + θ⁴ = 10θ⁴&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;For full-rank N(μ, σ²), T = (ΣX, ΣX²) is complete.&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;For curved N(θ, θ²), the same T is NOT complete because&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;the parameter space is restricted.&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Non-completeness means: ∃g with E[g(T)] = 0 ∀θ, g ≢ 0&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Example: g(x) = x³ - 4x·|x|² fails because it&#39;s not measurable&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;         w.r.t. T = (x, x²) alone.&quot;</span><span class="p">)</span>

    <span class="c1"># Part (d): MLE comparison</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;PART (d): MLE COMPARISON&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>

    <span class="n">theta_true</span> <span class="o">=</span> <span class="mf">3.0</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

    <span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">theta_true</span><span class="p">,</span> <span class="n">theta_true</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>

    <span class="c1"># Full-rank MLE</span>
    <span class="n">mu_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">sigma2_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># MLE uses n, not n-1</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True parameters: θ = </span><span class="si">{</span><span class="n">theta_true</span><span class="si">}</span><span class="s2">, so μ = </span><span class="si">{</span><span class="n">theta_true</span><span class="si">}</span><span class="s2">, σ² = </span><span class="si">{</span><span class="n">theta_true</span><span class="o">**</span><span class="mi">2</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample size: n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Full-rank MLE (unconstrained):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  μ̂ = X̄ = </span><span class="si">{</span><span class="n">mu_hat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  σ̂² = S² = </span><span class="si">{</span><span class="n">sigma2_hat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Note: σ̂²/μ̂² = </span><span class="si">{</span><span class="n">sigma2_hat</span><span class="o">/</span><span class="n">mu_hat</span><span class="o">**</span><span class="mi">2</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (should be 1 if constraint holds)&quot;</span><span class="p">)</span>

    <span class="c1"># Constrained MLE: maximize L(θ) subject to σ² = θ²</span>
    <span class="c1"># Log-likelihood: ℓ(θ) = -n/2·log(2π) - n/2·log(θ²) - Σ(xᵢ - θ)²/(2θ²)</span>
    <span class="c1">#                      = -n·log(θ) - Σ(xᵢ - θ)²/(2θ²) + const</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">neg_log_lik_constrained</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">theta</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
        <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">n</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">X</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">theta</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">result</span> <span class="o">=</span> <span class="n">optimize</span><span class="o">.</span><span class="n">minimize_scalar</span><span class="p">(</span>
        <span class="n">neg_log_lik_constrained</span><span class="p">,</span>
        <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,),</span>
        <span class="n">bounds</span><span class="o">=</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
        <span class="n">method</span><span class="o">=</span><span class="s1">&#39;bounded&#39;</span>
    <span class="p">)</span>
    <span class="n">theta_hat_constrained</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">x</span>

    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Constrained MLE (σ² = θ²):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  θ̂ = </span><span class="si">{</span><span class="n">theta_hat_constrained</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Implied μ̂ = </span><span class="si">{</span><span class="n">theta_hat_constrained</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Implied σ̂² = </span><span class="si">{</span><span class="n">theta_hat_constrained</span><span class="o">**</span><span class="mi">2</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Verify via first-order condition</span>
    <span class="c1"># dℓ/dθ = -n/θ + Σ(xᵢ - θ)·(1/θ² + (xᵢ-θ)/θ³)... messy</span>
    <span class="c1"># Numerically verify</span>

    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Comparison:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Full-rank: (μ̂, σ̂²) = (</span><span class="si">{</span><span class="n">mu_hat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">sigma2_hat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Constrained: θ̂ = </span><span class="si">{</span><span class="n">theta_hat_constrained</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  True: θ = </span><span class="si">{</span><span class="n">theta_true</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Check if constraint satisfied for full-rank MLE</span>
    <span class="n">constraint_error</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">sigma2_hat</span> <span class="o">-</span> <span class="n">mu_hat</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Constraint violation (full-rank): |σ̂² - μ̂²| = </span><span class="si">{</span><span class="n">constraint_error</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">curved_exponential_family_analysis</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CURVED EXPONENTIAL FAMILY: N(θ, θ²)
=================================================================

PART (a)-(b): CONSTRAINT IN NATURAL PARAMETER SPACE
----------------------------------------
Full-rank family: {(η₁, η₂) : η₁ ∈ ℝ, η₂ &lt; 0}
Curved family constraint: η₂ = -η₁²/2

For θ &gt; 0: η₁ = 1/θ &gt; 0, η₂ = -1/(2θ²) &lt; 0
Example: θ = 2 → η₁ = 0.5, η₂ = -0.125
         θ = 3 → η₁ = 0.333, η₂ = -0.056

-----------------------------------------------------------------
PART (c): NON-COMPLETENESS OF SUFFICIENT STATISTIC
----------------------------------------
Under constraint σ² = μ²:
  E[X] = θ
  E[X²] = 2θ²
  E[X⁴] = 3σ⁴ + 6μ²σ² + μ⁴ = 3θ⁴ + 6θ⁴ + θ⁴ = 10θ⁴

For full-rank N(μ, σ²), T = (ΣX, ΣX²) is complete.
For curved N(θ, θ²), the same T is NOT complete because
the parameter space is restricted.

Non-completeness means: ∃g with E[g(T)] = 0 ∀θ, g ≢ 0
Example: g(x) = x³ - 4x·|x|² fails because it&#39;s not measurable
         w.r.t. T = (x, x²) alone.

-----------------------------------------------------------------
PART (d): MLE COMPARISON
----------------------------------------
True parameters: θ = 3, so μ = 3, σ² = 9
Sample size: n = 100

Full-rank MLE (unconstrained):
  μ̂ = X̄ = 2.9174
  σ̂² = S² = 8.9498
  Note: σ̂²/μ̂² = 1.0518 (should be 1 if constraint holds)

Constrained MLE (σ² = θ²):
  θ̂ = 2.9581
  Implied μ̂ = 2.9581
  Implied σ̂² = 8.7504

Comparison:
  Full-rank: (μ̂, σ̂²) = (2.9174, 8.9498)
  Constrained: θ̂ = 2.9581
  True: θ = 3.0000

Constraint violation (full-rank): |σ̂² - μ̂²| = 0.4386
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Curved vs full-rank</strong>: The constraint <span class="math notranslate nohighlight">\(\eta_2 = -\eta_1^2/2\)</span> restricts the parameter space to a one-dimensional curve within the two-dimensional half-plane.</p></li>
<li><p class="sd-card-text"><strong>Dimension reduction</strong>: The curved family has one free parameter (<span class="math notranslate nohighlight">\(\theta\)</span>) instead of two (<span class="math notranslate nohighlight">\(\mu, \sigma^2\)</span>), reflecting the constraint.</p></li>
<li><p class="sd-card-text"><strong>Non-completeness</strong>: The sufficient statistic <span class="math notranslate nohighlight">\((X, X^2)\)</span> is complete for the full-rank family but NOT for the curved family—the constraint introduces dependencies.</p></li>
<li><p class="sd-card-text"><strong>MLE differs</strong>: The constrained MLE (<span class="math notranslate nohighlight">\(\hat{\theta} = 2.96\)</span>) differs from simply taking the full-rank MLE and imposing the constraint afterward.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 6: Building an Exponential Family Computational Toolkit</p>
<p>This exercise synthesizes the material by building a unified Python class for exponential family inference.</p>
<div class="note admonition">
<p class="admonition-title">Background: Unified Inference</p>
<p>Once a distribution is recognized as an exponential family, identical algorithms can compute MLEs (solve <span class="math notranslate nohighlight">\(A'(\eta) = \bar{T}\)</span>), Fisher information (<span class="math notranslate nohighlight">\(A''(\eta)\)</span>), confidence intervals, and more. A well-designed toolkit eliminates redundant implementation across different distributions.</p>
</div>
<ol class="loweralpha simple">
<li><p><strong>Class design</strong>: Implement an <code class="docutils literal notranslate"><span class="pre">ExponentialFamily</span></code> class with methods for:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">log_partition(eta)</span></code>: Compute <span class="math notranslate nohighlight">\(A(\eta)\)</span></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sufficient_statistic(x)</span></code>: Compute <span class="math notranslate nohighlight">\(T(x)\)</span></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mean(eta)</span></code>: Compute <span class="math notranslate nohighlight">\(\mathbb{E}[T(X)] = A'(\eta)\)</span></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">variance(eta)</span></code>: Compute <span class="math notranslate nohighlight">\(\text{Var}(T(X)) = A''(\eta)\)</span></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mle(data)</span></code>: Find <span class="math notranslate nohighlight">\(\hat{\eta}\)</span> solving <span class="math notranslate nohighlight">\(A'(\hat{\eta}) = \bar{T}\)</span></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">fisher_info(eta)</span></code>: Return <span class="math notranslate nohighlight">\(I(\eta) = A''(\eta)\)</span></p></li>
</ul>
</li>
<li><p><strong>Poisson implementation</strong>: Create a <code class="docutils literal notranslate"><span class="pre">PoissonFamily</span></code> subclass and verify it reproduces known results for data <span class="math notranslate nohighlight">\(\mathbf{x} = (3, 5, 2, 4, 6)\)</span>.</p></li>
<li><p><strong>Normal (known variance) implementation</strong>: Create a <code class="docutils literal notranslate"><span class="pre">NormalKnownVariance</span></code> subclass for <span class="math notranslate nohighlight">\(\mathcal{N}(\mu, \sigma_0^2)\)</span> with known <span class="math notranslate nohighlight">\(\sigma_0^2\)</span> and verify for sample data.</p></li>
<li><p><strong>Unified inference</strong>: Use your toolkit to compute MLEs, standard errors, and 95% confidence intervals for both families with the same code structure.</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">optimize</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">abc</span><span class="w"> </span><span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>

<span class="k">class</span><span class="w"> </span><span class="nc">ExponentialFamily</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Abstract base class for exponential family distributions.</span>

<span class="sd">    Canonical form: f(x|η) = h(x) exp(η·T(x) - A(η))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">log_partition</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eta</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute A(η), the log-partition function.&quot;&quot;&quot;</span>
        <span class="k">pass</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">sufficient_statistic</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute T(x), the sufficient statistic.&quot;&quot;&quot;</span>
        <span class="k">pass</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">natural_to_canonical</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eta</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convert natural parameter η to canonical parameter.&quot;&quot;&quot;</span>
        <span class="k">pass</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">canonical_to_natural</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convert canonical parameter to natural parameter η.&quot;&quot;&quot;</span>
        <span class="k">pass</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute E[T(X)] = A&#39;(η) via numerical differentiation.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_partition</span><span class="p">(</span><span class="n">eta</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_partition</span><span class="p">(</span><span class="n">eta</span> <span class="o">-</span> <span class="n">eps</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">eps</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">variance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute Var(T(X)) = A&#39;&#39;(η) via numerical differentiation.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_partition</span><span class="p">(</span><span class="n">eta</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">log_partition</span><span class="p">(</span><span class="n">eta</span><span class="p">)</span> <span class="o">+</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">log_partition</span><span class="p">(</span><span class="n">eta</span> <span class="o">-</span> <span class="n">eps</span><span class="p">))</span> <span class="o">/</span> <span class="n">eps</span><span class="o">**</span><span class="mi">2</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">fisher_info</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Fisher information I(η) = A&#39;&#39;(η).&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">variance</span><span class="p">(</span><span class="n">eta</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">mle</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Find MLE by solving A&#39;(η̂) = T̄.</span>

<span class="sd">        For exponential families, this is equivalent to moment matching.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">T_bar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">sufficient_statistic</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">])</span>

        <span class="c1"># Solve A&#39;(η) = T_bar</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">objective</span><span class="p">(</span><span class="n">eta</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">eta</span><span class="p">)</span> <span class="o">-</span> <span class="n">T_bar</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

        <span class="c1"># Initial guess from method of moments</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">optimize</span><span class="o">.</span><span class="n">minimize_scalar</span><span class="p">(</span><span class="n">objective</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;bounded&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span><span class="o">.</span><span class="n">x</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">confidence_interval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute (1-α) confidence interval for η.&quot;&quot;&quot;</span>
        <span class="n">eta_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mle</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">fisher</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fisher_info</span><span class="p">(</span><span class="n">eta_hat</span><span class="p">)</span>
        <span class="n">se</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">fisher</span><span class="p">)</span>

        <span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">eta_hat</span> <span class="o">-</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se</span><span class="p">,</span> <span class="n">eta_hat</span> <span class="o">+</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">PoissonFamily</span><span class="p">(</span><span class="n">ExponentialFamily</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Poisson(λ) in exponential family form.</span>

<span class="sd">    η = log(λ), T(x) = x, A(η) = exp(η) = λ</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">log_partition</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eta</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">eta</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sufficient_statistic</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">natural_to_canonical</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eta</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;η → λ&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">eta</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">canonical_to_natural</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lam</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;λ → η&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">lam</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;A&#39;(η) = exp(η) = λ (exact)&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">eta</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">variance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;A&#39;&#39;(η) = exp(η) = λ (exact)&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">eta</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">NormalKnownVariance</span><span class="p">(</span><span class="n">ExponentialFamily</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    N(μ, σ₀²) with known variance σ₀² in exponential family form.</span>

<span class="sd">    η = μ/σ₀², T(x) = x, A(η) = σ₀²η²/2</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sigma_sq</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma_sq</span> <span class="o">=</span> <span class="n">sigma_sq</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">log_partition</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eta</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma_sq</span> <span class="o">*</span> <span class="n">eta</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mi">2</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sufficient_statistic</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">natural_to_canonical</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eta</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;η → μ&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">eta</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma_sq</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">canonical_to_natural</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;μ → η&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">mu</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma_sq</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;A&#39;(η) = σ₀²η = μ (exact)&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma_sq</span> <span class="o">*</span> <span class="n">eta</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">variance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;A&#39;&#39;(η) = σ₀² (exact, doesn&#39;t depend on η)&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma_sq</span>


<span class="k">def</span><span class="w"> </span><span class="nf">unified_inference_demo</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Demonstrate unified inference across exponential families.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;EXPONENTIAL FAMILY UNIFIED INFERENCE TOOLKIT&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>

    <span class="c1"># Poisson data</span>
    <span class="n">poisson_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
    <span class="n">poisson</span> <span class="o">=</span> <span class="n">PoissonFamily</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">POISSON FAMILY:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Data: </span><span class="si">{</span><span class="n">poisson_data</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample mean T̄ = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">poisson_data</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">eta_hat_poisson</span> <span class="o">=</span> <span class="n">poisson</span><span class="o">.</span><span class="n">mle</span><span class="p">(</span><span class="n">poisson_data</span><span class="p">)</span>
    <span class="n">lambda_hat</span> <span class="o">=</span> <span class="n">poisson</span><span class="o">.</span><span class="n">natural_to_canonical</span><span class="p">(</span><span class="n">eta_hat_poisson</span><span class="p">)</span>
    <span class="n">fisher_poisson</span> <span class="o">=</span> <span class="n">poisson</span><span class="o">.</span><span class="n">fisher_info</span><span class="p">(</span><span class="n">eta_hat_poisson</span><span class="p">)</span>
    <span class="n">ci_eta</span> <span class="o">=</span> <span class="n">poisson</span><span class="o">.</span><span class="n">confidence_interval</span><span class="p">(</span><span class="n">poisson_data</span><span class="p">)</span>
    <span class="n">ci_lambda</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">ci_eta</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">ci_eta</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">MLE:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  η̂ = log(λ̂) = </span><span class="si">{</span><span class="n">eta_hat_poisson</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  λ̂ = exp(η̂) = </span><span class="si">{</span><span class="n">lambda_hat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  (Compare: X̄ = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">poisson_data</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Fisher information: I(η̂) = A&#39;&#39;(η̂) = </span><span class="si">{</span><span class="n">fisher_poisson</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Standard error (η): </span><span class="si">{</span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">poisson_data</span><span class="p">)</span><span class="o">*</span><span class="n">fisher_poisson</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% CI for η: (</span><span class="si">{</span><span class="n">ci_eta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_eta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% CI for λ: (</span><span class="si">{</span><span class="n">ci_lambda</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_lambda</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

    <span class="c1"># Normal with known variance</span>
    <span class="n">sigma_sq_known</span> <span class="o">=</span> <span class="mf">4.0</span>
    <span class="n">normal_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.1</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">,</span> <span class="mf">2.9</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">,</span> <span class="mf">2.7</span><span class="p">,</span> <span class="mf">3.8</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">])</span>
    <span class="n">normal</span> <span class="o">=</span> <span class="n">NormalKnownVariance</span><span class="p">(</span><span class="n">sigma_sq_known</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;NORMAL FAMILY (known σ² = 4):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Data: </span><span class="si">{</span><span class="n">normal_data</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample mean T̄ = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">normal_data</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">eta_hat_normal</span> <span class="o">=</span> <span class="n">normal</span><span class="o">.</span><span class="n">mle</span><span class="p">(</span><span class="n">normal_data</span><span class="p">)</span>
    <span class="n">mu_hat</span> <span class="o">=</span> <span class="n">normal</span><span class="o">.</span><span class="n">natural_to_canonical</span><span class="p">(</span><span class="n">eta_hat_normal</span><span class="p">)</span>
    <span class="n">fisher_normal</span> <span class="o">=</span> <span class="n">normal</span><span class="o">.</span><span class="n">fisher_info</span><span class="p">(</span><span class="n">eta_hat_normal</span><span class="p">)</span>
    <span class="n">ci_eta_normal</span> <span class="o">=</span> <span class="n">normal</span><span class="o">.</span><span class="n">confidence_interval</span><span class="p">(</span><span class="n">normal_data</span><span class="p">)</span>
    <span class="n">ci_mu</span> <span class="o">=</span> <span class="p">(</span><span class="n">normal</span><span class="o">.</span><span class="n">natural_to_canonical</span><span class="p">(</span><span class="n">ci_eta_normal</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
             <span class="n">normal</span><span class="o">.</span><span class="n">natural_to_canonical</span><span class="p">(</span><span class="n">ci_eta_normal</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">MLE:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  η̂ = μ̂/σ² = </span><span class="si">{</span><span class="n">eta_hat_normal</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  μ̂ = η̂·σ² = </span><span class="si">{</span><span class="n">mu_hat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  (Compare: X̄ = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">normal_data</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Fisher information: I(η̂) = A&#39;&#39;(η̂) = </span><span class="si">{</span><span class="n">fisher_normal</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Standard error (η): </span><span class="si">{</span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">normal_data</span><span class="p">)</span><span class="o">*</span><span class="n">fisher_normal</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Standard error (μ): </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma_sq_known</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">normal_data</span><span class="p">))</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% CI for η: (</span><span class="si">{</span><span class="n">ci_eta_normal</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_eta_normal</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% CI for μ: (</span><span class="si">{</span><span class="n">ci_mu</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_mu</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

    <span class="c1"># Verify theoretical SE for μ</span>
    <span class="n">theoretical_se_mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma_sq_known</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">normal_data</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Theoretical SE(μ̂) = σ/√n = </span><span class="si">{</span><span class="n">theoretical_se_mu</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;KEY INSIGHT: Same algorithm, different families!&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The exponential family structure enables unified inference.&quot;</span><span class="p">)</span>

<span class="n">unified_inference_demo</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>EXPONENTIAL FAMILY UNIFIED INFERENCE TOOLKIT
=================================================================

POISSON FAMILY:
----------------------------------------
Data: [3 5 2 4 6]
Sample mean T̄ = 4.0000

MLE:
  η̂ = log(λ̂) = 1.3863
  λ̂ = exp(η̂) = 4.0000
  (Compare: X̄ = 4.0000)

Fisher information: I(η̂) = A&#39;&#39;(η̂) = 4.0000
Standard error (η): 0.2236
95% CI for η: (0.9481, 1.8245)
95% CI for λ: (2.5808, 6.2018)

=================================================================
NORMAL FAMILY (known σ² = 4):
----------------------------------------
Data: [2.1 3.5 1.8 2.9 3.2 2.5 3.1 2.7 3.8 2.3]
Sample mean T̄ = 2.7900

MLE:
  η̂ = μ̂/σ² = 0.6975
  μ̂ = η̂·σ² = 2.7900
  (Compare: X̄ = 2.7900)

Fisher information: I(η̂) = A&#39;&#39;(η̂) = 4.0000
Standard error (η): 0.1581
Standard error (μ): 0.6325
95% CI for η: (0.3876, 1.0074)
95% CI for μ: (1.5504, 4.0296)

Theoretical SE(μ̂) = σ/√n = 0.6325

=================================================================
KEY INSIGHT: Same algorithm, different families!
The exponential family structure enables unified inference.
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Unified interface</strong>: Both Poisson and Normal inference use identical method calls—<code class="docutils literal notranslate"><span class="pre">mle()</span></code>, <code class="docutils literal notranslate"><span class="pre">fisher_info()</span></code>, <code class="docutils literal notranslate"><span class="pre">confidence_interval()</span></code>—despite different underlying mathematics.</p></li>
<li><p class="sd-card-text"><strong>MLE solves</strong> <span class="math notranslate nohighlight">\(A'(\hat{\eta}) = \bar{T}\)</span>: For both families, the MLE is found by matching the expected sufficient statistic to its sample value.</p></li>
<li><p class="sd-card-text"><strong>Fisher information from</strong> <span class="math notranslate nohighlight">\(A''\)</span>: The variance of the MLE follows directly from the log-partition function’s second derivative.</p></li>
<li><p class="sd-card-text"><strong>Natural vs canonical parameters</strong>: The toolkit handles both parameterizations, enabling reporting in whichever is more interpretable.</p></li>
<li><p class="sd-card-text"><strong>Extensibility</strong>: Adding new exponential families only requires implementing <code class="docutils literal notranslate"><span class="pre">log_partition</span></code>, <code class="docutils literal notranslate"><span class="pre">sufficient_statistic</span></code>, and the parameter transformations—the inference machinery is inherited.</p></li>
</ol>
</div>
</details></div>
</section>
<section id="bringing-it-all-together">
<h2>Bringing It All Together<a class="headerlink" href="#bringing-it-all-together" title="Link to this heading"></a></h2>
<p>These exercises have taken you through the core concepts and practical skills of exponential families:</p>
<ol class="arabic simple">
<li><p><strong>Canonical form conversion</strong> (Exercise 1): Any exponential family distribution can be written in the form <span class="math notranslate nohighlight">\(f(x|\eta) = h(x)\exp(\eta \cdot T(x) - A(\eta))\)</span>, enabling unified treatment.</p></li>
<li><p><strong>Log-partition power</strong> (Exercise 2): The function <span class="math notranslate nohighlight">\(A(\eta)\)</span> encodes all moments—first derivatives give means, second derivatives give variances and covariances.</p></li>
<li><p><strong>Sufficiency and compression</strong> (Exercise 3): The sufficient statistic <span class="math notranslate nohighlight">\(T(x)\)</span> captures all information about the parameter, enabling massive data reduction without information loss.</p></li>
<li><p><strong>Conjugate priors</strong> (Exercise 4): Exponential families admit closed-form Bayesian updating through conjugate priors, with hyperparameters interpretable as prior pseudo-data.</p></li>
<li><p><strong>Full-rank vs curved</strong> (Exercise 5): Constraints on natural parameters create curved subfamilies where standard results require modification.</p></li>
<li><p><strong>Unified computation</strong> (Exercise 6): A single computational framework serves all exponential families—the structure dictates the algorithms.</p></li>
</ol>
<p>The next sections develop maximum likelihood theory, where exponential family structure provides analytical tractability and computational efficiency that general families lack.</p>
<div class="important admonition">
<p class="admonition-title">Key Takeaways 📝</p>
<ol class="arabic simple">
<li><p><strong>Core concept</strong>: The exponential family unifies most common distributions under a single framework <span class="math notranslate nohighlight">\(p_\eta(x) = h(x)\exp\{\eta^T T(x) - A(\eta)\}\)</span>, with natural parameter <span class="math notranslate nohighlight">\(\eta\)</span>, sufficient statistic <span class="math notranslate nohighlight">\(T(x)\)</span>, and log-partition function <span class="math notranslate nohighlight">\(A(\eta)\)</span>.</p></li>
<li><p><strong>Computational insight</strong>: All moments of <span class="math notranslate nohighlight">\(T(X)\)</span> come from derivatives of <span class="math notranslate nohighlight">\(A(\eta)\)</span>—the mean is <span class="math notranslate nohighlight">\(\nabla A(\eta)\)</span> and the variance is <span class="math notranslate nohighlight">\(\nabla^2 A(\eta)\)</span>. This Hessian also equals the Fisher information, guaranteeing concave log-likelihoods.</p></li>
<li><p><strong>Inferential power</strong>: The sufficient statistic <span class="math notranslate nohighlight">\(T(X)\)</span> captures all parameter information (Neyman-Fisher factorization). For full-rank families, <span class="math notranslate nohighlight">\(T(X)\)</span> is complete, enabling UMVUE construction via Lehmann-Scheffé.</p></li>
<li><p><strong>Course connections</strong>: This framework provides the foundation for MLE (Section 3.2, where the score equation becomes <span class="math notranslate nohighlight">\(\nabla A(\eta) = \bar{T}\)</span>), GLMs (Section 3.7, built on exponential dispersion models), and Bayesian inference (Chapter 5, using conjugate priors). [LO 1, 2]</p></li>
</ol>
</div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<p><strong>Foundational Works on Exponential Families</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="koopman1936" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Koopman1936<span class="fn-bracket">]</span></span>
<p>Koopman, B. O. (1936). On distributions admitting a sufficient statistic. <em>Transactions of the American Mathematical Society</em>, 39(3), 399–409. One of the three independent proofs establishing that exponential families are characterized by fixed-dimensional sufficient statistics.</p>
</div>
<div class="citation" id="pitman1936" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Pitman1936<span class="fn-bracket">]</span></span>
<p>Pitman, E. J. G. (1936). Sufficient statistics and intrinsic accuracy. <em>Proceedings of the Cambridge Philosophical Society</em>, 32(4), 567–579. Establishes the Pitman-Koopman-Darmois theorem connecting sufficiency to exponential family structure.</p>
</div>
<div class="citation" id="darmois1935" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Darmois1935<span class="fn-bracket">]</span></span>
<p>Darmois, G. (1935). Sur les lois de probabilité à estimation exhaustive. <em>Comptes Rendus de l’Académie des Sciences</em>, 200, 1265–1266. The French contribution to the simultaneous independent discovery of the exponential family characterization theorem.</p>
</div>
<div class="citation" id="brown1986" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Brown1986<span class="fn-bracket">]</span></span>
<p>Brown, L. D. (1986). <em>Fundamentals of Statistical Exponential Families with Applications in Statistical Decision Theory</em>. Institute of Mathematical Statistics Lecture Notes–Monograph Series, Vol. 9. The definitive mathematical reference on exponential families, covering canonical parameters, convexity, completeness, and decision-theoretic applications.</p>
</div>
<div class="citation" id="barndorffnielsen1978" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>BarndorffNielsen1978<span class="fn-bracket">]</span></span>
<p>Barndorff-Nielsen, O. E. (1978). <em>Information and Exponential Families in Statistical Theory</em>. Wiley. Comprehensive treatment of exponential families emphasizing information geometry and statistical inference.</p>
</div>
</div>
<p><strong>Sufficiency and the Neyman-Fisher Factorization</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="fisher1922" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Fisher1922<span class="fn-bracket">]</span></span>
<p>Fisher, R. A. (1922). On the mathematical foundations of theoretical statistics. <em>Philosophical Transactions of the Royal Society A</em>, 222, 309–368. Introduces sufficiency, maximum likelihood, and efficiency—foundational concepts for understanding exponential families.</p>
</div>
<div class="citation" id="neyman1935" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Neyman1935<span class="fn-bracket">]</span></span>
<p>Neyman, J. (1935). Sur un teorema concernente le cosidette statistiche sufficienti. <em>Giornale dell’Istituto Italiano degli Attuari</em>, 6, 320–334. Provides the rigorous proof of the factorization theorem characterizing sufficient statistics.</p>
</div>
<div class="citation" id="halmos1949" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Halmos1949<span class="fn-bracket">]</span></span>
<p>Halmos, P. R., and Savage, L. J. (1949). Application of the Radon-Nikodym theorem to the theory of sufficient statistics. <em>Annals of Mathematical Statistics</em>, 20(2), 225–241. Extends sufficiency theory to the measure-theoretic setting essential for modern treatments.</p>
</div>
</div>
<p><strong>Completeness and UMVUE Theory</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="lehmannscheffe1950" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LehmannScheffe1950<span class="fn-bracket">]</span></span>
<p>Lehmann, E. L., and Scheffé, H. (1950). Completeness, similar regions, and unbiased estimation: Part I. <em>Sankhyā</em>, 10(4), 305–340. Establishes the theory of complete sufficient statistics and their role in constructing uniformly minimum variance unbiased estimators.</p>
</div>
<div class="citation" id="lehmanncasella1998" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LehmannCasella1998<span class="fn-bracket">]</span></span>
<p>Lehmann, E. L., and Casella, G. (1998). <em>Theory of Point Estimation</em> (2nd ed.). Springer. Graduate-level treatment of point estimation theory including comprehensive coverage of exponential families and completeness.</p>
</div>
</div>
<p><strong>Conjugate Priors and Bayesian Analysis</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="raiffa1961" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Raiffa1961<span class="fn-bracket">]</span></span>
<p>Raiffa, H., and Schlaifer, R. (1961). <em>Applied Statistical Decision Theory</em>. Harvard Business School. Introduces conjugate prior families and their computational advantages for Bayesian inference in exponential families.</p>
</div>
<div class="citation" id="diaconis1979" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Diaconis1979<span class="fn-bracket">]</span></span>
<p>Diaconis, P., and Ylvisaker, D. (1979). Conjugate priors for exponential families. <em>Annals of Statistics</em>, 7(2), 269–281. Characterizes conjugate priors for exponential families and establishes their interpretation as pseudo-observations.</p>
</div>
</div>
<p><strong>Information Geometry</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="amari1985" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Amari1985<span class="fn-bracket">]</span></span>
<p>Amari, S. (1985). <em>Differential-Geometrical Methods in Statistics</em>. Lecture Notes in Statistics, Vol. 28. Springer. Develops the differential geometric structure of exponential families, introducing concepts like α-connections and dual affine structures.</p>
</div>
<div class="citation" id="amarinagaoka2000" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>AmariNagaoka2000<span class="fn-bracket">]</span></span>
<p>Amari, S., and Nagaoka, H. (2000). <em>Methods of Information Geometry</em>. Oxford University Press. Modern treatment of information geometry with applications to statistics, machine learning, and neural networks.</p>
</div>
</div>
<p><strong>Modern Applications</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="mccullaghnelder1989" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>McCullaghNelder1989<span class="fn-bracket">]</span></span>
<p>McCullagh, P., and Nelder, J. A. (1989). <em>Generalized Linear Models</em> (2nd ed.). Chapman and Hall. The standard reference for GLMs, built on the exponential dispersion family foundation developed from exponential families.</p>
</div>
<div class="citation" id="wainwright2008" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Wainwright2008<span class="fn-bracket">]</span></span>
<p>Wainwright, M. J., and Jordan, M. I. (2008). Graphical models, exponential families, and variational inference. <em>Foundations and Trends in Machine Learning</em>, 1(1–2), 1–305. Comprehensive survey connecting exponential families to modern machine learning methods including graphical models and variational inference.</p>
</div>
</div>
<p><strong>Textbook Treatments</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="casellaberger2002" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CasellaBerger2002<span class="fn-bracket">]</span></span>
<p>Casella, G., and Berger, R. L. (2002). <em>Statistical Inference</em> (2nd ed.). Duxbury Press. Chapter 3 provides an accessible introduction to sufficiency and exponential families suitable for advanced undergraduates.</p>
</div>
<div class="citation" id="keener2010" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Keener2010<span class="fn-bracket">]</span></span>
<p>Keener, R. W. (2010). <em>Theoretical Statistics: Topics for a Core Course</em>. Springer. Modern measure-theoretic approach to exponential families with clear proofs and examples.</p>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Chapter 3: Parametric Inference and Likelihood Methods" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ch3_2-maximum-likelihood-estimation.html" class="btn btn-neutral float-right" title="Maximum Likelihood Estimation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>