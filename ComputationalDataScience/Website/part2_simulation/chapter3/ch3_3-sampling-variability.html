

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>2.2.3. Sampling Variability and Variance Estimation &mdash; STAT 418</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=dc393e06" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT418/Website/part2_simulation/chapter3/ch3_3-sampling-variability.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=30646c52"></script>
      <script>let toggleHintShow = 'Show solution';</script>
      <script>let toggleHintHide = 'Hide solution';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>window.MathJax = {"options": {"enableMenu": true, "menuOptions": {"settings": {"enrich": true, "speech": true, "braille": true, "collapsible": true, "assistiveMml": false}}}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
      <script src="../../_static/custom.js?v=8718e0ab"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="2.2.4. Linear Models" href="ch3_4-linear-models.html" />
    <link rel="prev" title="2.2.2. Maximum Likelihood Estimation" href="ch3_2-maximum-likelihood-estimation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Course Content</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../part1_foundations/index.html">1. Part I: Foundations of Probability and Computation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part1_foundations/chapter1/index.html">1.1. Chapter 1: Statistical Paradigms and Core Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html">1.1.1. Paradigms of Probability and Statistical Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#the-mathematical-foundation-kolmogorov-s-axioms">The Mathematical Foundation: Kolmogorov’s Axioms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#interpretations-of-probability">Interpretations of Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#statistical-inference-paradigms">Statistical Inference Paradigms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#historical-and-philosophical-debates">Historical and Philosophical Debates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#looking-ahead-our-course-focus">Looking Ahead: Our Course Focus</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html">1.1.2. Probability Distributions: Theory and Computation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#from-abstract-foundations-to-concrete-tools">From Abstract Foundations to Concrete Tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#the-python-ecosystem-for-probability">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#introduction-why-probability-distributions-matter">Introduction: Why Probability Distributions Matter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#id1">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#continuous-distributions">Continuous Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#additional-important-distributions">Additional Important Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#summary-and-practical-guidelines">Summary and Practical Guidelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#conclusion">Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html">1.1.3. Python Random Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#from-mathematical-distributions-to-computational-samples">From Mathematical Distributions to Computational Samples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-python-ecosystem-at-a-glance">The Python Ecosystem at a Glance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#understanding-pseudo-random-number-generation">Understanding Pseudo-Random Number Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-standard-library-random-module">The Standard Library: <code class="docutils literal notranslate"><span class="pre">random</span></code> Module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#numpy-fast-vectorized-random-sampling">NumPy: Fast Vectorized Random Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#scipy-stats-the-complete-statistical-toolkit">SciPy Stats: The Complete Statistical Toolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#bringing-it-all-together-library-selection-guide">Bringing It All Together: Library Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#looking-ahead-from-random-numbers-to-monte-carlo-methods">Looking Ahead: From Random Numbers to Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html">1.1.4. Chapter 1 Summary: Foundations in Place</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#the-three-pillars-of-chapter-1">The Three Pillars of Chapter 1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#what-lies-ahead-the-road-to-simulation">What Lies Ahead: The Road to Simulation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#chapter-1-exercises-synthesis-problems">Chapter 1 Exercises: Synthesis Problems</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">2. Part II: Simulation-Based Methods</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../chapter2/index.html">2.1. Chapter 2: Monte Carlo Simulation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html">2.1.1. Monte Carlo Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#the-historical-development-of-monte-carlo-methods">The Historical Development of Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#the-core-principle-expectation-as-integration">The Core Principle: Expectation as Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#theoretical-foundations">Theoretical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#variance-estimation-and-confidence-intervals">Variance Estimation and Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#comparison-with-deterministic-methods">Comparison with Deterministic Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#sample-size-determination">Sample Size Determination</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#convergence-diagnostics-and-monitoring">Convergence Diagnostics and Monitoring</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#chapter-2-1-exercises-monte-carlo-fundamentals-mastery">Chapter 2.1 Exercises: Monte Carlo Fundamentals Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html">2.1.2. Uniform Random Variates</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#why-uniform-the-universal-currency-of-randomness">Why Uniform? The Universal Currency of Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#the-paradox-of-computational-randomness">The Paradox of Computational Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#chaotic-dynamical-systems-an-instructive-failure">Chaotic Dynamical Systems: An Instructive Failure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#linear-congruential-generators">Linear Congruential Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#shift-register-generators">Shift-Register Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#the-kiss-generator-combining-strategies">The KISS Generator: Combining Strategies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#modern-generators-mersenne-twister-and-pcg">Modern Generators: Mersenne Twister and PCG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#statistical-testing-of-random-number-generators">Statistical Testing of Random Number Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#chapter-2-2-exercises-uniform-random-variates-mastery">Chapter 2.2 Exercises: Uniform Random Variates Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html">2.1.3. Inverse CDF Method</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#mathematical-foundations">Mathematical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#continuous-distributions-with-closed-form-inverses">Continuous Distributions with Closed-Form Inverses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#numerical-inversion">Numerical Inversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#mixed-distributions">Mixed Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#chapter-2-3-exercises-inverse-cdf-method-mastery">Chapter 2.3 Exercises: Inverse CDF Method Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html">2.1.4. Transformation Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#why-transformation-methods">Why Transformation Methods?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-boxmuller-transform">The Box–Muller Transform</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-polar-marsaglia-method">The Polar (Marsaglia) Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#method-comparison-boxmuller-vs-polar-vs-ziggurat">Method Comparison: Box–Muller vs Polar vs Ziggurat</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-ziggurat-algorithm">The Ziggurat Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-clt-approximation-historical">The CLT Approximation (Historical)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#distributions-derived-from-the-normal">Distributions Derived from the Normal</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#multivariate-normal-generation">Multivariate Normal Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#implementation-guidance">Implementation Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#chapter-2-4-exercises-transformation-methods-mastery">Chapter 2.4 Exercises: Transformation Methods Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html">2.1.5. Rejection Sampling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-dartboard-intuition">The Dartboard Intuition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-accept-reject-algorithm">The Accept-Reject Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#efficiency-analysis">Efficiency Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#choosing-the-proposal-distribution">Choosing the Proposal Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-squeeze-principle">The Squeeze Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#geometric-example-sampling-from-the-unit-disk">Geometric Example: Sampling from the Unit Disk</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#limitations-and-the-curse-of-dimensionality">Limitations and the Curse of Dimensionality</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#connections-to-other-methods">Connections to Other Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#chapter-2-5-exercises-rejection-sampling-mastery">Chapter 2.5 Exercises: Rejection Sampling Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html">2.1.6. Variance Reduction Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#the-variance-reduction-paradigm">The Variance Reduction Paradigm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#importance-sampling">Importance Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#control-variates">Control Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#antithetic-variates">Antithetic Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#stratified-sampling">Stratified Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#common-random-numbers">Common Random Numbers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#conditional-monte-carlo-raoblackwellization">Conditional Monte Carlo (Rao–Blackwellization)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#combining-variance-reduction-techniques">Combining Variance Reduction Techniques</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#chapter-2-6-exercises-variance-reduction-mastery">Chapter 2.6 Exercises: Variance Reduction Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html">2.1.7. Chapter Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#the-complete-monte-carlo-workflow">The Complete Monte Carlo Workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#quick-reference-tables">Quick Reference Tables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#common-pitfalls-checklist">Common Pitfalls Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#connections-to-later-chapters">Connections to Later Chapters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#learning-outcomes-checklist">Learning Outcomes Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#further-reading-optimization-and-missing-data">Further Reading: Optimization and Missing Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">2.2. Chapter 3: Parametric Inference and Likelihood Methods</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="ch3_1-exponential-families.html">2.2.1. Exponential Families</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#historical-origins-from-scattered-results-to-unified-theory">Historical Origins: From Scattered Results to Unified Theory</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#the-canonical-exponential-family">The Canonical Exponential Family</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#converting-familiar-distributions">Converting Familiar Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#the-log-partition-function-a-moment-generating-machine">The Log-Partition Function: A Moment-Generating Machine</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#sufficiency-capturing-all-parameter-information">Sufficiency: Capturing All Parameter Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#minimal-sufficiency-and-completeness">Minimal Sufficiency and Completeness</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#conjugate-priors-and-bayesian-inference">Conjugate Priors and Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#exponential-dispersion-models-and-glms">Exponential Dispersion Models and GLMs</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#chapter-3-1-exercises-exponential-families-mastery">Chapter 3.1 Exercises: Exponential Families Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html">2.2.2. Maximum Likelihood Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#the-likelihood-function">The Likelihood Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#the-score-function">The Score Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#fisher-information">Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#closed-form-maximum-likelihood-estimators">Closed-Form Maximum Likelihood Estimators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#numerical-optimization-for-mle">Numerical Optimization for MLE</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#asymptotic-properties-of-mles">Asymptotic Properties of MLEs</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#the-cramer-rao-lower-bound">The Cramér-Rao Lower Bound</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#the-invariance-property">The Invariance Property</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#likelihood-based-hypothesis-testing">Likelihood-Based Hypothesis Testing</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#confidence-intervals-from-likelihood">Confidence Intervals from Likelihood</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#connection-to-bayesian-inference">Connection to Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#chapter-3-2-exercises-maximum-likelihood-estimation-mastery">Chapter 3.2 Exercises: Maximum Likelihood Estimation Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">2.2.3. Sampling Variability and Variance Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#statistical-estimators-and-their-properties">Statistical Estimators and Their Properties</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sampling-distributions">Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-delta-method">The Delta Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="#variance-estimation-methods">Variance Estimation Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="#applications-and-worked-examples">Applications and Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch3_4-linear-models.html">2.2.4. Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#matrix-calculus-foundations">Matrix Calculus Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#the-linear-model">The Linear Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#ordinary-least-squares-the-calculus-approach">Ordinary Least Squares: The Calculus Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#ordinary-least-squares-the-geometric-approach">Ordinary Least Squares: The Geometric Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#properties-of-the-ols-estimator">Properties of the OLS Estimator</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#the-gauss-markov-theorem">The Gauss-Markov Theorem</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#estimating-the-error-variance">Estimating the Error Variance</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#distributional-results-under-normality">Distributional Results Under Normality</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#diagnostics-and-model-checking">Diagnostics and Model Checking</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#numerical-stability-qr-decomposition">Numerical Stability: QR Decomposition</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#model-selection-and-information-criteria">Model Selection and Information Criteria</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#regularization-ridge-and-lasso">Regularization: Ridge and LASSO</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#chapter-3-4-exercises-linear-models-mastery">Chapter 3.4 Exercises: Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch3_5-generalized-linear-models.html">2.2.5. Generalized Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#historical-context-unification-of-regression-methods">Historical Context: Unification of Regression Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#the-glm-framework-three-components">The GLM Framework: Three Components</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#score-equations-and-fisher-information">Score Equations and Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#iteratively-reweighted-least-squares">Iteratively Reweighted Least Squares</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#logistic-regression-binary-outcomes">Logistic Regression: Binary Outcomes</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#poisson-regression-count-data">Poisson Regression: Count Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#gamma-regression-positive-continuous-data">Gamma Regression: Positive Continuous Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#inference-in-glms-the-testing-triad">Inference in GLMs: The Testing Triad</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#model-diagnostics">Model Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#model-comparison-and-selection">Model Comparison and Selection</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#quasi-likelihood-and-robust-inference">Quasi-Likelihood and Robust Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#further-reading">Further Reading</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#chapter-3-5-exercises-generalized-linear-models-mastery">Chapter 3.5 Exercises: Generalized Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch3_6-chapter-summary.html">2.2.6. Chapter Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#the-parametric-inference-pipeline">The Parametric Inference Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#the-five-pillars-of-chapter-3">The Five Pillars of Chapter 3</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#quick-reference-core-formulas">Quick Reference: Core Formulas</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#connections-to-future-material">Connections to Future Material</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#practical-guidance">Practical Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter4/index.html">2.3. Chapter 4: Resampling Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/jackknife_introduction.html">2.3.1. Jackknife Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html">2.3.2. Bootstrap Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html">2.3.3. Nonparametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/parametric_bootstrap.html">2.3.4. Parametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/confidence_intervals.html">2.3.5. Confidence Intervals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/bias_correction.html">2.3.6. Bias Correction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/cross_validation_loo.html">2.3.7. Cross Validation Loo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html">2.3.8. Cross Validation K Fold</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/model_selection.html">2.3.9. Model Selection</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part3_bayesian/index.html">3. Part III: Bayesian Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part3_bayesian/index.html#overview">3.1. Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html">1. Bayesian Philosophy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#introduction">1.1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#key-concepts">1.2. Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#mathematical-framework">1.3. Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#python-implementation">1.4. Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#examples">1.5. Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#summary">1.6. Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html"><span class="section-number">2. </span>Part II: Simulation-Based Methods</a></li>
          <li class="breadcrumb-item"><a href="index.html"><span class="section-number">2.2. </span>Chapter 3: Parametric Inference and Likelihood Methods</a></li>
      <li class="breadcrumb-item active"><span class="section-number">2.2.3. </span>Sampling Variability and Variance Estimation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/part2_simulation/chapter3/ch3_3-sampling-variability.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="sampling-variability-and-variance-estimation">
<span id="ch3-3-sampling-variability"></span><h1><span class="section-number">2.2.3. </span>Sampling Variability and Variance Estimation<a class="headerlink" href="#sampling-variability-and-variance-estimation" title="Link to this heading"></a></h1>
<p>In <a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#ch3-2-maximum-likelihood-estimation"><span class="std std-ref">Section 3.2</span></a>, we developed maximum likelihood estimation—a systematic method for obtaining point estimates <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> from data. But a point estimate alone tells only half the story. Reporting that a vaccine has 95% efficacy, or that a regression coefficient is 2.3, means little without understanding the <em>uncertainty</em> in these estimates. How much would <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> vary if we repeated the study? Could the true parameter plausibly be zero? These questions require understanding the <strong>sampling variability</strong> of estimators.</p>
<p>This section develops the theoretical foundations for quantifying estimator uncertainty. We begin with the fundamental concepts: what makes an estimator good (bias, variance, consistency), and how estimators behave across hypothetical repeated samples (sampling distributions). We then develop the <strong>delta method</strong>, which propagates uncertainty through transformations—essential when the scientifically relevant quantity is a nonlinear function of directly estimable parameters. Finally, we survey methods for estimating variance from data: Fisher information, observed information, and robust sandwich estimators that remain valid even when model assumptions fail.</p>
<p>These tools are indispensable throughout statistics: constructing confidence intervals, performing hypothesis tests, comparing estimator quality, and understanding when asymptotic approximations can be trusted. The concepts developed here apply immediately to regression coefficients (Section 3.4), generalized linear models (Section 3.5), and provide the theoretical foundation for bootstrap methods (Chapter 4).</p>
<div class="important admonition">
<p class="admonition-title">Road Map 🧭</p>
<ul class="simple">
<li><p><strong>Define</strong>: Statistical estimators, bias, variance, mean squared error, and consistency</p></li>
<li><p><strong>Distinguish</strong>: Exact versus asymptotic sampling distributions</p></li>
<li><p><strong>Derive</strong>: Variance formulas for transformed parameters via the delta method</p></li>
<li><p><strong>Implement</strong>: Variance estimation via Fisher information, observed information, and sandwich estimators</p></li>
<li><p><strong>Apply</strong>: Standard errors and confidence intervals for complex statistics</p></li>
</ul>
</div>
<section id="statistical-estimators-and-their-properties">
<h2>Statistical Estimators and Their Properties<a class="headerlink" href="#statistical-estimators-and-their-properties" title="Link to this heading"></a></h2>
<p>An <strong>estimator</strong> is a rule (function) that maps data to a numerical value intended to approximate an unknown parameter. Formally, if <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span> are random variables representing data from a distribution <span class="math notranslate nohighlight">\(F_\theta\)</span>, then an estimator <span class="math notranslate nohighlight">\(\hat{\theta} = T(X_1, \ldots, X_n)\)</span> is itself a random variable—its value depends on which sample we happen to observe.</p>
<p>This simple observation has profound implications: because <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is random, it has a probability distribution (the <strong>sampling distribution</strong>), and we can characterize its behavior using familiar concepts like expectation, variance, and probability statements.</p>
<section id="what-is-an-estimator">
<h3>What Is an Estimator?<a class="headerlink" href="#what-is-an-estimator" title="Link to this heading"></a></h3>
<div class="note admonition">
<p class="admonition-title">Definition: Estimator</p>
<p>Let <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span> be a random sample from a distribution <span class="math notranslate nohighlight">\(F_\theta\)</span> indexed by parameter <span class="math notranslate nohighlight">\(\theta \in \Theta\)</span>. An <strong>estimator</strong> of <span class="math notranslate nohighlight">\(\theta\)</span> is any statistic <span class="math notranslate nohighlight">\(\hat{\theta} = T(X_1, \ldots, X_n)\)</span> used to estimate <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>Key points:</p>
<ul class="simple">
<li><p>The estimator <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is a <em>random variable</em> (before observing data)</p></li>
<li><p>The estimate <span class="math notranslate nohighlight">\(\hat{\theta} = t\)</span> is a <em>number</em> (after observing data <span class="math notranslate nohighlight">\(x_1, \ldots, x_n\)</span>)</p></li>
<li><p>Different samples yield different estimates—this variability is sampling variability</p></li>
</ul>
</div>
<p><strong>Examples of estimators</strong>:</p>
<table class="docutils align-default" id="id2">
<caption><span class="caption-number">Table 2.12 </span><span class="caption-text">Common Estimators</span><a class="headerlink" href="#id2" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 25.0%" />
<col style="width: 25.0%" />
<col style="width: 25.0%" />
<col style="width: 25.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Estimator</p></th>
<th class="head"><p>Formula</p></th>
<th class="head"><p>Name</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Mean <span class="math notranslate nohighlight">\(\mu\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\bar{X}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{1}{n}\sum_{i=1}^n X_i\)</span></p></td>
<td><p>Sample mean</p></td>
</tr>
<tr class="row-odd"><td><p>Variance <span class="math notranslate nohighlight">\(\sigma^2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(S^2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X})^2\)</span></p></td>
<td><p>Sample variance</p></td>
</tr>
<tr class="row-even"><td><p>Proportion <span class="math notranslate nohighlight">\(p\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\hat{p}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{1}{n}\sum_{i=1}^n X_i\)</span> (for <span class="math notranslate nohighlight">\(X_i \in \{0,1\}\)</span>)</p></td>
<td><p>Sample proportion</p></td>
</tr>
<tr class="row-odd"><td><p>Rate <span class="math notranslate nohighlight">\(\lambda\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\hat{\lambda}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1/\bar{X}\)</span> (for Exponential)</p></td>
<td><p>MLE</p></td>
</tr>
<tr class="row-even"><td><p>Median <span class="math notranslate nohighlight">\(m\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\hat{m}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(X_{((n+1)/2)}\)</span></p></td>
<td><p>Sample median</p></td>
</tr>
</tbody>
</table>
<p>Not all estimators for the same parameter are equally good. How do we compare them?</p>
</section>
<section id="bias">
<h3>Bias<a class="headerlink" href="#bias" title="Link to this heading"></a></h3>
<p><strong>Bias</strong> measures systematic error—whether an estimator tends to overestimate or underestimate on average.</p>
<div class="note admonition">
<p class="admonition-title">Definition: Bias</p>
<p>The <strong>bias</strong> of an estimator <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> for parameter <span class="math notranslate nohighlight">\(\theta\)</span> is:</p>
<div class="math notranslate nohighlight" id="equation-bias-def">
<span class="eqno">(2.62)<a class="headerlink" href="#equation-bias-def" title="Link to this equation"></a></span>\[\text{Bias}(\hat{\theta}) = \mathbb{E}[\hat{\theta}] - \theta\]</div>
<p>An estimator is <strong>unbiased</strong> if <span class="math notranslate nohighlight">\(\mathbb{E}[\hat{\theta}] = \theta\)</span> for all <span class="math notranslate nohighlight">\(\theta \in \Theta\)</span>.</p>
</div>
<p><strong>Examples</strong>:</p>
<ol class="arabic">
<li><p><strong>Sample mean</strong> <span class="math notranslate nohighlight">\(\bar{X}\)</span> is unbiased for <span class="math notranslate nohighlight">\(\mu\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[\bar{X}] = \mathbb{E}\left[\frac{1}{n}\sum_{i=1}^n X_i\right] = \frac{1}{n}\sum_{i=1}^n \mathbb{E}[X_i] = \frac{1}{n} \cdot n\mu = \mu\]</div>
</li>
<li><p><strong>Sample variance</strong> <span class="math notranslate nohighlight">\(S^2 = \frac{1}{n-1}\sum(X_i - \bar{X})^2\)</span> is unbiased for <span class="math notranslate nohighlight">\(\sigma^2\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[S^2] = \sigma^2\]</div>
<p>The divisor <span class="math notranslate nohighlight">\(n-1\)</span> (not <span class="math notranslate nohighlight">\(n\)</span>) ensures unbiasedness. The “naive” estimator <span class="math notranslate nohighlight">\(\frac{1}{n}\sum(X_i - \bar{X})^2\)</span> has bias <span class="math notranslate nohighlight">\(-\sigma^2/n\)</span>.</p>
</li>
<li><p><strong>MLE of variance</strong> <span class="math notranslate nohighlight">\(\hat{\sigma}^2_{\text{MLE}} = \frac{1}{n}\sum(X_i - \bar{X})^2\)</span> is biased:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[\hat{\sigma}^2_{\text{MLE}}] = \frac{n-1}{n}\sigma^2 \neq \sigma^2\]</div>
<p>The bias is <span class="math notranslate nohighlight">\(-\sigma^2/n\)</span>, which vanishes as <span class="math notranslate nohighlight">\(n \to \infty\)</span>.</p>
</li>
</ol>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ⚠️ Unbiasedness Is Not Everything</p>
<p>Unbiasedness is desirable but not paramount. A biased estimator with small variance can have smaller overall error than an unbiased estimator with large variance. We formalize this with mean squared error.</p>
</div>
</section>
<section id="variance">
<h3>Variance<a class="headerlink" href="#variance" title="Link to this heading"></a></h3>
<p><strong>Variance</strong> measures precision—how much an estimator fluctuates across samples.</p>
<div class="note admonition">
<p class="admonition-title">Definition: Estimator Variance</p>
<p>The <strong>variance</strong> of an estimator <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is:</p>
<div class="math notranslate nohighlight" id="equation-var-def">
<span class="eqno">(2.63)<a class="headerlink" href="#equation-var-def" title="Link to this equation"></a></span>\[\text{Var}(\hat{\theta}) = \mathbb{E}\left[(\hat{\theta} - \mathbb{E}[\hat{\theta}])^2\right]\]</div>
<p>The <strong>standard error</strong> is <span class="math notranslate nohighlight">\(\text{SE}(\hat{\theta}) = \sqrt{\text{Var}(\hat{\theta})}\)</span>.</p>
</div>
<p><strong>Example</strong>: For the sample mean from a population with variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\bar{X}) = \text{Var}\left(\frac{1}{n}\sum_{i=1}^n X_i\right) = \frac{1}{n^2}\sum_{i=1}^n \text{Var}(X_i) = \frac{1}{n^2} \cdot n\sigma^2 = \frac{\sigma^2}{n}\]</div>
<p>Thus <span class="math notranslate nohighlight">\(\text{SE}(\bar{X}) = \sigma/\sqrt{n}\)</span>. This fundamental result shows:</p>
<ul class="simple">
<li><p>Larger samples → smaller standard error → more precise estimates</p></li>
<li><p>The precision improves at rate <span class="math notranslate nohighlight">\(1/\sqrt{n}\)</span>, not <span class="math notranslate nohighlight">\(1/n\)</span></p></li>
<li><p>Quadrupling the sample size halves the standard error</p></li>
</ul>
</section>
<section id="mean-squared-error">
<h3>Mean Squared Error<a class="headerlink" href="#mean-squared-error" title="Link to this heading"></a></h3>
<p><strong>Mean squared error</strong> (MSE) combines bias and variance into a single measure of overall estimator quality.</p>
<div class="note admonition">
<p class="admonition-title">Definition: Mean Squared Error</p>
<p>The <strong>mean squared error</strong> of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is:</p>
<div class="math notranslate nohighlight" id="equation-mse-def">
<span class="eqno">(2.64)<a class="headerlink" href="#equation-mse-def" title="Link to this equation"></a></span>\[\text{MSE}(\hat{\theta}) = \mathbb{E}\left[(\hat{\theta} - \theta)^2\right]\]</div>
</div>
<div class="note admonition">
<p class="admonition-title">Theorem: Bias-Variance Decomposition</p>
<div class="math notranslate nohighlight" id="equation-bias-variance">
<span class="eqno">(2.65)<a class="headerlink" href="#equation-bias-variance" title="Link to this equation"></a></span>\[\text{MSE}(\hat{\theta}) = \text{Bias}(\hat{\theta})^2 + \text{Var}(\hat{\theta})\]</div>
<p><strong>Proof</strong>: Let <span class="math notranslate nohighlight">\(b = \mathbb{E}[\hat{\theta}] - \theta\)</span> (bias). Then:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{MSE} &amp;= \mathbb{E}[(\hat{\theta} - \theta)^2] = \mathbb{E}[(\hat{\theta} - \mathbb{E}[\hat{\theta}] + b)^2] \\
&amp;= \mathbb{E}[(\hat{\theta} - \mathbb{E}[\hat{\theta}])^2] + 2b\mathbb{E}[\hat{\theta} - \mathbb{E}[\hat{\theta}]] + b^2 \\
&amp;= \text{Var}(\hat{\theta}) + 0 + \text{Bias}^2\end{split}\]</div>
</div>
<p>This decomposition reveals a fundamental tradeoff:</p>
<ul class="simple">
<li><p><strong>Low bias, high variance</strong>: The estimator is centered correctly but imprecise</p></li>
<li><p><strong>High bias, low variance</strong>: The estimator is precise but systematically wrong</p></li>
<li><p><strong>Optimal</strong>: Balance bias and variance to minimize MSE</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">demonstrate_bias_variance</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Illustrate bias-variance tradeoff with two estimators.&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

    <span class="c1"># True parameter</span>
    <span class="n">theta_true</span> <span class="o">=</span> <span class="mf">5.0</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="mf">2.0</span>
    <span class="n">n_sim</span> <span class="o">=</span> <span class="mi">10000</span>

    <span class="c1"># Estimator 1: Sample mean (unbiased)</span>
    <span class="n">estimates_1</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">theta_true</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
        <span class="n">estimates_1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">estimates_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">estimates_1</span><span class="p">)</span>

    <span class="c1"># Estimator 2: Shrinkage estimator (biased toward 0)</span>
    <span class="n">shrink_factor</span> <span class="o">=</span> <span class="mf">0.8</span>
    <span class="n">estimates_2</span> <span class="o">=</span> <span class="n">shrink_factor</span> <span class="o">*</span> <span class="n">estimates_1</span>

    <span class="c1"># Compute properties</span>
    <span class="n">bias_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">estimates_1</span><span class="p">)</span> <span class="o">-</span> <span class="n">theta_true</span>
    <span class="n">var_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">estimates_1</span><span class="p">)</span>
    <span class="n">mse_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">estimates_1</span> <span class="o">-</span> <span class="n">theta_true</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">bias_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">estimates_2</span><span class="p">)</span> <span class="o">-</span> <span class="n">theta_true</span>
    <span class="n">var_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">estimates_2</span><span class="p">)</span>
    <span class="n">mse_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">estimates_2</span> <span class="o">-</span> <span class="n">theta_true</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;BIAS-VARIANCE TRADEOFF DEMONSTRATION&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">True θ = </span><span class="si">{</span><span class="n">theta_true</span><span class="si">}</span><span class="s2">, n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">, σ = </span><span class="si">{</span><span class="n">sigma</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Estimator 1: Sample Mean X̄ (unbiased)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Bias:     </span><span class="si">{</span><span class="n">bias_1</span><span class="si">:</span><span class="s2">8.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Variance: </span><span class="si">{</span><span class="n">var_1</span><span class="si">:</span><span class="s2">8.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  MSE:      </span><span class="si">{</span><span class="n">mse_1</span><span class="si">:</span><span class="s2">8.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Estimator 2: Shrinkage 0.8·X̄ (biased)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Bias:     </span><span class="si">{</span><span class="n">bias_2</span><span class="si">:</span><span class="s2">8.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Variance: </span><span class="si">{</span><span class="n">var_2</span><span class="si">:</span><span class="s2">8.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  MSE:      </span><span class="si">{</span><span class="n">mse_2</span><span class="si">:</span><span class="s2">8.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">→ Shrinkage has </span><span class="si">{</span><span class="s1">&#39;LOWER&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">mse_2</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">mse_1</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;HIGHER&#39;</span><span class="si">}</span><span class="s2"> MSE despite being biased!&quot;</span><span class="p">)</span>

<span class="n">demonstrate_bias_variance</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>============================================================
BIAS-VARIANCE TRADEOFF DEMONSTRATION
============================================================

True θ = 5.0, n = 10, σ = 2.0

Estimator 1: Sample Mean X̄ (unbiased)
  Bias:       0.0023
  Variance:   0.4012
  MSE:        0.4012

Estimator 2: Shrinkage 0.8·X̄ (biased)
  Bias:      -0.9982
  Variance:   0.2568
  MSE:        1.2531

→ Shrinkage has HIGHER MSE despite being biased!
</pre></div>
</div>
<p>In this case, the shrinkage estimator’s reduced variance doesn’t compensate for its bias. However, in high-dimensional settings (Section 3.4), shrinkage often wins—this is the foundation of ridge regression and the James-Stein phenomenon.</p>
</section>
<section id="consistency">
<h3>Consistency<a class="headerlink" href="#consistency" title="Link to this heading"></a></h3>
<p><strong>Consistency</strong> ensures that estimators converge to the truth as sample size grows.</p>
<div class="note admonition">
<p class="admonition-title">Definition: Consistency</p>
<p>An estimator <span class="math notranslate nohighlight">\(\hat{\theta}_n\)</span> is <strong>consistent</strong> for <span class="math notranslate nohighlight">\(\theta\)</span> if:</p>
<div class="math notranslate nohighlight" id="equation-consistency">
<span class="eqno">(2.66)<a class="headerlink" href="#equation-consistency" title="Link to this equation"></a></span>\[\hat{\theta}_n \xrightarrow{p} \theta \quad \text{as } n \to \infty\]</div>
<p>That is, for any <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span>:</p>
<div class="math notranslate nohighlight">
\[\lim_{n \to \infty} P(|\hat{\theta}_n - \theta| &gt; \varepsilon) = 0\]</div>
</div>
<p><strong>Sufficient conditions for consistency:</strong></p>
<ol class="arabic simple">
<li><p><strong>Unbiased + vanishing variance</strong>: If <span class="math notranslate nohighlight">\(\mathbb{E}[\hat{\theta}_n] = \theta\)</span> and <span class="math notranslate nohighlight">\(\text{Var}(\hat{\theta}_n) \to 0\)</span>, then <span class="math notranslate nohighlight">\(\hat{\theta}_n\)</span> is consistent.</p></li>
<li><p><strong>MSE → 0</strong>: If <span class="math notranslate nohighlight">\(\text{MSE}(\hat{\theta}_n) \to 0\)</span>, then <span class="math notranslate nohighlight">\(\hat{\theta}_n\)</span> is consistent.</p></li>
<li><p><strong>Biased but asymptotically unbiased</strong>: If <span class="math notranslate nohighlight">\(\text{Bias}(\hat{\theta}_n) \to 0\)</span> and <span class="math notranslate nohighlight">\(\text{Var}(\hat{\theta}_n) \to 0\)</span>, then <span class="math notranslate nohighlight">\(\hat{\theta}_n\)</span> is consistent.</p></li>
</ol>
<p><strong>Example</strong>: The sample mean <span class="math notranslate nohighlight">\(\bar{X}_n\)</span> is consistent for <span class="math notranslate nohighlight">\(\mu\)</span>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}[\bar{X}_n] = \mu\)</span> (unbiased)</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{Var}(\bar{X}_n) = \sigma^2/n \to 0\)</span></p></li>
</ul>
<p>This follows from the <strong>Law of Large Numbers</strong>.</p>
<div class="note admonition">
<p class="admonition-title">Example 💡 Biased but Consistent</p>
<p>The MLE of variance <span class="math notranslate nohighlight">\(\hat{\sigma}^2_n = \frac{1}{n}\sum(X_i - \bar{X})^2\)</span> is biased:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[\hat{\sigma}^2_n] = \frac{n-1}{n}\sigma^2\]</div>
<p>But it’s consistent because:</p>
<ul class="simple">
<li><p>Bias <span class="math notranslate nohighlight">\(= -\sigma^2/n \to 0\)</span></p></li>
<li><p>Variance <span class="math notranslate nohighlight">\(\to 0\)</span> (by LLN applied to squared deviations)</p></li>
</ul>
<p>For large <span class="math notranslate nohighlight">\(n\)</span>, the bias becomes negligible.</p>
</div>
</section>
</section>
<section id="sampling-distributions">
<h2>Sampling Distributions<a class="headerlink" href="#sampling-distributions" title="Link to this heading"></a></h2>
<p>The <strong>sampling distribution</strong> of an estimator describes its probability distribution across hypothetical repeated samples. Understanding sampling distributions is essential for inference: confidence intervals, hypothesis tests, and p-values all depend on knowing (or approximating) how <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is distributed.</p>
<section id="exact-sampling-distributions">
<h3>Exact Sampling Distributions<a class="headerlink" href="#exact-sampling-distributions" title="Link to this heading"></a></h3>
<p>Some estimators have known, exact distributions under specific assumptions.</p>
<p><strong>Example 1: Sample mean from Normal population</strong></p>
<p>If <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} \mathcal{N}(\mu, \sigma^2)\)</span>, then:</p>
<div class="math notranslate nohighlight" id="equation-xbar-normal">
<span class="eqno">(2.67)<a class="headerlink" href="#equation-xbar-normal" title="Link to this equation"></a></span>\[\bar{X} \sim \mathcal{N}\left(\mu, \frac{\sigma^2}{n}\right) \quad \text{exactly}\]</div>
<p>This is exact for any <span class="math notranslate nohighlight">\(n\)</span>, not just asymptotically.</p>
<p><strong>Example 2: Sample variance from Normal population</strong></p>
<p>If <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} \mathcal{N}(\mu, \sigma^2)\)</span>, then:</p>
<div class="math notranslate nohighlight" id="equation-s2-chisq">
<span class="eqno">(2.68)<a class="headerlink" href="#equation-s2-chisq" title="Link to this equation"></a></span>\[\frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n-1}\]</div>
<p>Moreover, <span class="math notranslate nohighlight">\(\bar{X}\)</span> and <span class="math notranslate nohighlight">\(S^2\)</span> are independent—a remarkable property unique to the normal distribution.</p>
<p><strong>Example 3: t-statistic</strong></p>
<p>Combining the above, when <span class="math notranslate nohighlight">\(\sigma^2\)</span> is unknown:</p>
<div class="math notranslate nohighlight" id="equation-t-stat">
<span class="eqno">(2.69)<a class="headerlink" href="#equation-t-stat" title="Link to this equation"></a></span>\[T = \frac{\bar{X} - \mu}{S/\sqrt{n}} \sim t_{n-1}\]</div>
<p>This exact result justifies t-tests and t-based confidence intervals.</p>
</section>
<section id="asymptotic-sampling-distributions">
<h3>Asymptotic Sampling Distributions<a class="headerlink" href="#asymptotic-sampling-distributions" title="Link to this heading"></a></h3>
<p>When exact distributions are unavailable, we rely on <strong>asymptotic</strong> (large-sample) approximations, primarily through the Central Limit Theorem.</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Central Limit Theorem</p>
<p>Let <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span> be iid with mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2 &lt; \infty\)</span>. Then:</p>
<div class="math notranslate nohighlight" id="equation-clt">
<span class="eqno">(2.70)<a class="headerlink" href="#equation-clt" title="Link to this equation"></a></span>\[\sqrt{n}(\bar{X}_n - \mu) \xrightarrow{d} \mathcal{N}(0, \sigma^2)\]</div>
<p>Equivalently:</p>
<div class="math notranslate nohighlight">
\[\frac{\bar{X}_n - \mu}{\sigma/\sqrt{n}} \xrightarrow{d} \mathcal{N}(0, 1)\]</div>
</div>
<p>The CLT implies that for large <span class="math notranslate nohighlight">\(n\)</span>, the sample mean is approximately normal regardless of the underlying distribution.</p>
<p><strong>From Section 3.2</strong>, we have the more general result for MLEs:</p>
<div class="math notranslate nohighlight">
\[\sqrt{n}(\hat{\theta}_{\text{MLE}} - \theta) \xrightarrow{d} \mathcal{N}\left(0, I(\theta)^{-1}\right)\]</div>
<p>where <span class="math notranslate nohighlight">\(I(\theta)\)</span> is the Fisher information.</p>
</section>
<section id="exact-vs-asymptotic-when-to-use-which">
<h3>Exact vs. Asymptotic: When to Use Which<a class="headerlink" href="#exact-vs-asymptotic-when-to-use-which" title="Link to this heading"></a></h3>
<table class="docutils align-default" id="id3">
<caption><span class="caption-number">Table 2.13 </span><span class="caption-text">Exact vs Asymptotic Sampling Distributions</span><a class="headerlink" href="#id3" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 40.0%" />
<col style="width: 40.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Situation</p></th>
<th class="head"><p>Exact Distribution</p></th>
<th class="head"><p>Asymptotic Approximation</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Normal data, known <span class="math notranslate nohighlight">\(\sigma\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\bar{X} \sim N(\mu, \sigma^2/n)\)</span> — use z-test</p></td>
<td><p>Not needed</p></td>
</tr>
<tr class="row-odd"><td><p>Normal data, unknown <span class="math notranslate nohighlight">\(\sigma\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(t = (\bar{X}-\mu)/(S/\sqrt{n}) \sim t_{n-1}\)</span> — use t-test</p></td>
<td><p>For large <span class="math notranslate nohighlight">\(n\)</span>, <span class="math notranslate nohighlight">\(t \approx z\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Non-normal data, large <span class="math notranslate nohighlight">\(n\)</span></p></td>
<td><p>Usually unavailable</p></td>
<td><p>CLT: <span class="math notranslate nohighlight">\(\bar{X} \approx N(\mu, \sigma^2/n)\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>MLEs generally</p></td>
<td><p>Rarely available</p></td>
<td><p><span class="math notranslate nohighlight">\(\hat{\theta} \approx N(\theta, I(\theta)^{-1}/n)\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Small <span class="math notranslate nohighlight">\(n\)</span>, non-normal</p></td>
<td><p>May need exact (e.g., permutation)</p></td>
<td><p>Approximation may be poor</p></td>
</tr>
</tbody>
</table>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">compare_exact_asymptotic</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare exact t-distribution with normal approximation.&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

    <span class="n">sample_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>
    <span class="n">n_sim</span> <span class="o">=</span> <span class="mi">50000</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">65</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;EXACT VS ASYMPTOTIC: T-DISTRIBUTION CONVERGENCE TO NORMAL&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">65</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">sample_sizes</span><span class="p">:</span>
        <span class="c1"># Simulate t-statistics under H0: μ = 0</span>
        <span class="n">t_stats</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
            <span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
            <span class="n">t_stats</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

        <span class="n">t_stats</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">t_stats</span><span class="p">)</span>

        <span class="c1"># Compare to t_{n-1} and N(0,1)</span>
        <span class="c1"># Use 95th percentile as comparison</span>
        <span class="n">t_critical</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">z_critical</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.975</span><span class="p">)</span>  <span class="c1"># 1.96</span>

        <span class="n">empirical_95</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">t_stats</span><span class="p">),</span> <span class="mi">95</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Exact t_</span><span class="si">{</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="si">}</span><span class="s2"> critical value (97.5%): </span><span class="si">{</span><span class="n">t_critical</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Normal approximation z critical:      </span><span class="si">{</span><span class="n">z_critical</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Empirical 95th percentile |t|:        </span><span class="si">{</span><span class="n">empirical_95</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Error using normal: </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="nb">abs</span><span class="p">(</span><span class="n">t_critical</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">z_critical</span><span class="p">)</span><span class="o">/</span><span class="n">t_critical</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>

<span class="n">compare_exact_asymptotic</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>=================================================================
EXACT VS ASYMPTOTIC: T-DISTRIBUTION CONVERGENCE TO NORMAL
=================================================================

n = 5:
  Exact t_4 critical value (97.5%): 2.7764
  Normal approximation z critical:      1.9600
  Empirical 95th percentile |t|:        2.7621
  Error using normal: 29.4%

n = 15:
  Exact t_14 critical value (97.5%): 2.1448
  Normal approximation z critical:      1.9600
  Empirical 95th percentile |t|:        2.1359
  Error using normal: 8.6%

n = 30:
  Exact t_29 critical value (97.5%): 2.0452
  Normal approximation z critical:      1.9600
  Empirical 95th percentile |t|:        2.0420
  Error using normal: 4.2%

n = 100:
  Exact t_99 critical value (97.5%): 1.9842
  Normal approximation z critical:      1.9600
  Empirical 95th percentile |t|:        1.9812
  Error using normal: 1.2%
</pre></div>
</div>
<p><strong>Key insight</strong>: For <span class="math notranslate nohighlight">\(n &lt; 30\)</span>, using the normal approximation instead of the exact t-distribution leads to confidence intervals that are too narrow and p-values that are too small. Always use exact distributions when available.</p>
</section>
</section>
<section id="the-delta-method">
<h2>The Delta Method<a class="headerlink" href="#the-delta-method" title="Link to this heading"></a></h2>
<p>With the foundations of estimator properties and sampling distributions established, we now address a key practical question: given an estimator <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> with known variance, what is the variance of a transformation <span class="math notranslate nohighlight">\(g(\hat{\theta})\)</span>?</p>
<section id="motivation-why-transform-parameters">
<h3>Motivation: Why Transform Parameters?<a class="headerlink" href="#motivation-why-transform-parameters" title="Link to this heading"></a></h3>
<p>Parameter transformations arise naturally throughout statistics:</p>
<p><strong>Interpretability</strong>: The odds ratio <span class="math notranslate nohighlight">\(\psi = p/(1-p)\)</span> is more interpretable than the probability <span class="math notranslate nohighlight">\(p\)</span> itself in many contexts. The hazard ratio in survival analysis is the exponential of a regression coefficient.</p>
<p><strong>Boundedness</strong>: Parameters like probabilities (<span class="math notranslate nohighlight">\(p \in (0,1)\)</span>) and variances (<span class="math notranslate nohighlight">\(\sigma^2 &gt; 0\)</span>) have constrained ranges. Transformations to <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> (logit for probabilities, log for variances) enable unconstrained optimization and symmetric confidence intervals.</p>
<p><strong>Variance stabilization</strong>: For Poisson data with mean <span class="math notranslate nohighlight">\(\lambda\)</span>, the variance also equals <span class="math notranslate nohighlight">\(\lambda\)</span>. The square root transformation <span class="math notranslate nohighlight">\(\sqrt{X}\)</span> has approximately constant variance regardless of <span class="math notranslate nohighlight">\(\lambda\)</span>—useful for regression and ANOVA.</p>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_3_fig05_variance_stabilization.png"><img alt="Poisson square root transformation stabilizing variance" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_3_fig05_variance_stabilization.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.66 </span><span class="caption-text"><strong>Figure 3.3.1</strong>: Variance stabilization for Poisson data. (a) Raw counts show increasing spread with <span class="math notranslate nohighlight">\(\lambda\)</span> since <span class="math notranslate nohighlight">\(\text{Var}(X) = \lambda\)</span>. (b) Variance of the sample mean increases linearly with <span class="math notranslate nohighlight">\(\lambda\)</span> for raw data but remains nearly constant for <span class="math notranslate nohighlight">\(\sqrt{X}\)</span>. (c) After square root transformation, the data have approximately constant variance <span class="math notranslate nohighlight">\(\approx 1/4\)</span> regardless of the Poisson mean—essential for valid regression analysis.</span><a class="headerlink" href="#id4" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Scientific parameters</strong>: Physical and biological quantities often involve ratios, products, or other nonlinear combinations of directly estimable parameters.</p>
<div class="note admonition">
<p class="admonition-title">Example 💡 The Vaccine Efficacy Problem</p>
<p><strong>Setting</strong>: In a vaccine trial, we observe:</p>
<ul class="simple">
<li><p>Vaccinated group: <span class="math notranslate nohighlight">\(n_v = 20,000\)</span> subjects, <span class="math notranslate nohighlight">\(x_v = 8\)</span> infections → <span class="math notranslate nohighlight">\(\hat{p}_v = 0.0004\)</span></p></li>
<li><p>Placebo group: <span class="math notranslate nohighlight">\(n_p = 20,000\)</span> subjects, <span class="math notranslate nohighlight">\(x_p = 162\)</span> infections → <span class="math notranslate nohighlight">\(\hat{p}_p = 0.0081\)</span></p></li>
</ul>
<p><strong>Vaccine efficacy</strong> is defined as <span class="math notranslate nohighlight">\(\text{VE} = 1 - \frac{p_v}{p_p}\)</span> (the relative risk reduction).</p>
<p><strong>Questions</strong>:</p>
<ol class="arabic simple">
<li><p>What is <span class="math notranslate nohighlight">\(\widehat{\text{VE}}\)</span>?</p></li>
<li><p>What is <span class="math notranslate nohighlight">\(\text{SE}(\widehat{\text{VE}})\)</span>?</p></li>
<li><p>What is a 95% confidence interval for VE?</p></li>
</ol>
<p><strong>The challenge</strong>: VE is a nonlinear function of two estimated probabilities. The delta method provides the systematic approach to answering questions 2 and 3.</p>
</div>
</section>
<section id="the-univariate-delta-method">
<h3>The Univariate Delta Method<a class="headerlink" href="#the-univariate-delta-method" title="Link to this heading"></a></h3>
<section id="statement-and-proof">
<h4>Statement and Proof<a class="headerlink" href="#statement-and-proof" title="Link to this heading"></a></h4>
<div class="note admonition">
<p class="admonition-title">Theorem: Univariate Delta Method</p>
<p>Let <span class="math notranslate nohighlight">\(\{T_n\}\)</span> be a sequence of random variables satisfying:</p>
<div class="math notranslate nohighlight" id="equation-delta-condition">
<span class="eqno">(2.71)<a class="headerlink" href="#equation-delta-condition" title="Link to this equation"></a></span>\[\sqrt{n}(T_n - \theta) \xrightarrow{d} \mathcal{N}(0, \sigma^2)\]</div>
<p>If <span class="math notranslate nohighlight">\(g\)</span> is a function with continuous derivative <span class="math notranslate nohighlight">\(g'(\theta) \neq 0\)</span>, then:</p>
<div class="math notranslate nohighlight" id="equation-delta-result">
<span class="eqno">(2.72)<a class="headerlink" href="#equation-delta-result" title="Link to this equation"></a></span>\[\sqrt{n}(g(T_n) - g(\theta)) \xrightarrow{d} \mathcal{N}(0, [g'(\theta)]^2 \sigma^2)\]</div>
</div>
<p><strong>Proof</strong>: Taylor expand <span class="math notranslate nohighlight">\(g(T_n)\)</span> around <span class="math notranslate nohighlight">\(\theta\)</span>:</p>
<div class="math notranslate nohighlight">
\[g(T_n) = g(\theta) + g'(\theta)(T_n - \theta) + \frac{1}{2}g''(\tilde{\theta})(T_n - \theta)^2\]</div>
<p>for some <span class="math notranslate nohighlight">\(\tilde{\theta}\)</span> between <span class="math notranslate nohighlight">\(T_n\)</span> and <span class="math notranslate nohighlight">\(\theta\)</span>. Rearranging:</p>
<div class="math notranslate nohighlight">
\[\sqrt{n}(g(T_n) - g(\theta)) = g'(\theta) \cdot \sqrt{n}(T_n - \theta) + \frac{1}{2}g''(\tilde{\theta}) \cdot \sqrt{n}(T_n - \theta)^2\]</div>
<p>The first term converges to <span class="math notranslate nohighlight">\(g'(\theta) \cdot \mathcal{N}(0, \sigma^2) = \mathcal{N}(0, [g'(\theta)]^2 \sigma^2)\)</span> by Slutsky’s theorem.</p>
<p>For the second term: <span class="math notranslate nohighlight">\(\sqrt{n}(T_n - \theta)^2 = \frac{1}{\sqrt{n}} \cdot [\sqrt{n}(T_n - \theta)]^2\)</span>. Since <span class="math notranslate nohighlight">\([\sqrt{n}(T_n - \theta)]^2 = O_p(1)\)</span> (bounded in probability) and <span class="math notranslate nohighlight">\(1/\sqrt{n} \to 0\)</span>, the remainder is <span class="math notranslate nohighlight">\(o_p(1)\)</span> and vanishes in the limit. <span class="math notranslate nohighlight">\(\blacksquare\)</span></p>
</section>
</section>
<section id="practical-formulation">
<h3>Practical Formulation<a class="headerlink" href="#practical-formulation" title="Link to this heading"></a></h3>
<p>For practical use, the delta method gives:</p>
<div class="math notranslate nohighlight" id="equation-delta-practical">
<span class="eqno">(2.73)<a class="headerlink" href="#equation-delta-practical" title="Link to this equation"></a></span>\[\text{Var}(g(\hat{\theta})) \approx [g'(\theta)]^2 \cdot \text{Var}(\hat{\theta})\]</div>
<p>Since we don’t know <span class="math notranslate nohighlight">\(\theta\)</span>, we use the <strong>plug-in principle</strong>: evaluate <span class="math notranslate nohighlight">\(g'\)</span> at <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-delta-plugin">
<span class="eqno">(2.74)<a class="headerlink" href="#equation-delta-plugin" title="Link to this equation"></a></span>\[\widehat{\text{Var}}(g(\hat{\theta})) = [g'(\hat{\theta})]^2 \cdot \widehat{\text{Var}}(\hat{\theta})\]</div>
<p>Taking square roots gives the <strong>standard error</strong>:</p>
<div class="math notranslate nohighlight" id="equation-delta-se">
<span class="eqno">(2.75)<a class="headerlink" href="#equation-delta-se" title="Link to this equation"></a></span>\[\text{SE}(g(\hat{\theta})) = |g'(\hat{\theta})| \cdot \text{SE}(\hat{\theta})\]</div>
<div class="tip admonition">
<p class="admonition-title">Geometric Interpretation</p>
<p>The delta method says: <em>variance transforms like the square of the derivative</em>.</p>
<p>If <span class="math notranslate nohighlight">\(g'(\theta) = 2\)</span>, then small changes in <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> produce changes in <span class="math notranslate nohighlight">\(g(\hat{\theta})\)</span> that are twice as large. The standard deviation of <span class="math notranslate nohighlight">\(g(\hat{\theta})\)</span> is therefore twice that of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>, and the variance is four times as large.</p>
<p>This is simply the linearization: near <span class="math notranslate nohighlight">\(\theta\)</span>, the function <span class="math notranslate nohighlight">\(g\)</span> behaves like its tangent line with slope <span class="math notranslate nohighlight">\(g'(\theta)\)</span>.</p>
</div>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_3_fig01_delta_method_geometry.png"><img alt="Delta method linearization showing input distribution, tangent line approximation, and output distribution" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_3_fig01_delta_method_geometry.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.67 </span><span class="caption-text"><strong>Figure 3.3.2</strong>: The delta method in action. (a) The estimator <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> follows a normal distribution centered at <span class="math notranslate nohighlight">\(\theta_0 = 2\)</span>. (b) The nonlinear function <span class="math notranslate nohighlight">\(g(\theta) = e^\theta\)</span> is approximated by its tangent line at <span class="math notranslate nohighlight">\(\theta_0\)</span>; the “Error” annotation shows where linearization diverges from the true curve. (c) The resulting distribution of <span class="math notranslate nohighlight">\(g(\hat{\theta})\)</span> (histogram) closely matches the delta method approximation (red curve) with variance <span class="math notranslate nohighlight">\([g'(\theta_0)]^2 \sigma^2\)</span>.</span><a class="headerlink" href="#id5" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="common-transformations">
<h3>Common Transformations<a class="headerlink" href="#common-transformations" title="Link to this heading"></a></h3>
<p>The following table provides delta method results for frequently used transformations:</p>
<table class="docutils align-default" id="id6">
<caption><span class="caption-number">Table 2.14 </span><span class="caption-text">Delta Method for Common Transformations</span><a class="headerlink" href="#id6" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 25.0%" />
<col style="width: 25.0%" />
<col style="width: 25.0%" />
<col style="width: 25.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Transformation</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(g(\theta)\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(g'(\theta)\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\text{Var}(g(\hat{\theta}))\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Log</p></td>
<td><p><span class="math notranslate nohighlight">\(\log(\theta)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1/\theta\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\text{Var}(\hat{\theta})/\theta^2\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Exponential</p></td>
<td><p><span class="math notranslate nohighlight">\(e^\theta\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(e^\theta\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(e^{2\theta} \cdot \text{Var}(\hat{\theta})\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Square root</p></td>
<td><p><span class="math notranslate nohighlight">\(\sqrt{\theta}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{1}{2\sqrt{\theta}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{\text{Var}(\hat{\theta})}{4\theta}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Square</p></td>
<td><p><span class="math notranslate nohighlight">\(\theta^2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(2\theta\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(4\theta^2 \cdot \text{Var}(\hat{\theta})\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Reciprocal</p></td>
<td><p><span class="math notranslate nohighlight">\(1/\theta\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-1/\theta^2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\text{Var}(\hat{\theta})/\theta^4\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Logit</p></td>
<td><p><span class="math notranslate nohighlight">\(\log\frac{\theta}{1-\theta}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{1}{\theta(1-\theta)}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{\text{Var}(\hat{\theta})}{\theta^2(1-\theta)^2}\)</span></p></td>
</tr>
</tbody>
</table>
<figure class="align-center" id="id7">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_3_fig02_variance_amplification.png"><img alt="Variance scaling by square of derivative for different transformations" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_3_fig02_variance_amplification.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.68 </span><span class="caption-text"><strong>Figure 3.3.3</strong>: Variance scales as <span class="math notranslate nohighlight">\([g'(\theta)]^2\)</span>. Starting from the same input distribution with <span class="math notranslate nohighlight">\(\text{Var}(\hat{\theta}) = 0.04\)</span>: (a) identity transformation preserves variance; (b) doubling (<span class="math notranslate nohighlight">\(g' = 2\)</span>) quadruples variance; (c) squaring at <span class="math notranslate nohighlight">\(\theta_0 = 2\)</span> with <span class="math notranslate nohighlight">\(g' = 4\)</span> amplifies variance by factor 16; (d) square root at <span class="math notranslate nohighlight">\(\theta_0 = 2\)</span> with <span class="math notranslate nohighlight">\(g' = 0.25\)</span> shrinks variance by factor 16.</span><a class="headerlink" href="#id7" title="Link to this image"></a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">delta_method_se</span><span class="p">(</span><span class="n">theta_hat</span><span class="p">,</span> <span class="n">se_theta</span><span class="p">,</span> <span class="n">g_prime</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute standard error of g(θ̂) via delta method.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    theta_hat : float</span>
<span class="sd">        Point estimate of θ.</span>
<span class="sd">    se_theta : float</span>
<span class="sd">        Standard error of θ̂.</span>
<span class="sd">    g_prime : callable</span>
<span class="sd">        Derivative g&#39;(θ), evaluated at theta_hat.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    float</span>
<span class="sd">        Standard error of g(θ̂).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">g_prime</span><span class="p">(</span><span class="n">theta_hat</span><span class="p">))</span> <span class="o">*</span> <span class="n">se_theta</span>

<span class="k">def</span><span class="w"> </span><span class="nf">delta_method_ci</span><span class="p">(</span><span class="n">theta_hat</span><span class="p">,</span> <span class="n">se_theta</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">g_prime</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute confidence interval for g(θ) via delta method.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    theta_hat : float</span>
<span class="sd">        Point estimate of θ.</span>
<span class="sd">    se_theta : float</span>
<span class="sd">        Standard error of θ̂.</span>
<span class="sd">    g : callable</span>
<span class="sd">        Transformation function.</span>
<span class="sd">    g_prime : callable</span>
<span class="sd">        Derivative of g.</span>
<span class="sd">    alpha : float</span>
<span class="sd">        Significance level (default 0.05 for 95% CI).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        Point estimate, SE, and confidence interval.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">g_hat</span> <span class="o">=</span> <span class="n">g</span><span class="p">(</span><span class="n">theta_hat</span><span class="p">)</span>
    <span class="n">se_g</span> <span class="o">=</span> <span class="n">delta_method_se</span><span class="p">(</span><span class="n">theta_hat</span><span class="p">,</span> <span class="n">se_theta</span><span class="p">,</span> <span class="n">g_prime</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;estimate&#39;</span><span class="p">:</span> <span class="n">g_hat</span><span class="p">,</span>
        <span class="s1">&#39;se&#39;</span><span class="p">:</span> <span class="n">se_g</span><span class="p">,</span>
        <span class="s1">&#39;ci_lower&#39;</span><span class="p">:</span> <span class="n">g_hat</span> <span class="o">-</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se_g</span><span class="p">,</span>
        <span class="s1">&#39;ci_upper&#39;</span><span class="p">:</span> <span class="n">g_hat</span> <span class="o">+</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se_g</span>
    <span class="p">}</span>

<span class="c1"># Example: Log transformation of Poisson rate</span>
<span class="c1"># λ̂ = 4.5, SE(λ̂) = 0.67</span>
<span class="n">lambda_hat</span> <span class="o">=</span> <span class="mf">4.5</span>
<span class="n">se_lambda</span> <span class="o">=</span> <span class="mf">0.67</span>

<span class="c1"># g(λ) = log(λ), g&#39;(λ) = 1/λ</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">delta_method_ci</span><span class="p">(</span>
    <span class="n">lambda_hat</span><span class="p">,</span> <span class="n">se_lambda</span><span class="p">,</span>
    <span class="n">g</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">,</span>
    <span class="n">g_prime</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span><span class="o">/</span><span class="n">x</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Log-transformation of Poisson rate:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  log(λ̂) = </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;estimate&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  SE(log(λ̂)) = </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;se&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  95% CI: (</span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci_lower&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci_upper&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># Back-transform to get CI for λ</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">  Back-transformed CI for λ: (</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci_lower&#39;</span><span class="p">])</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci_upper&#39;</span><span class="p">])</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Log-transformation of Poisson rate:
  log(λ̂) = 1.5041
  SE(log(λ̂)) = 0.1489
  95% CI: (1.2123, 1.7959)

  Back-transformed CI for λ: (3.3618, 6.0242)
</pre></div>
</div>
<div class="note admonition">
<p class="admonition-title">Example 💡 Odds Ratio Confidence Interval</p>
<p><strong>Setup</strong>: In a case-control study:</p>
<ul class="simple">
<li><p>Cases: 45 exposed, 55 unexposed → <span class="math notranslate nohighlight">\(\hat{p}_1 = 45/100 = 0.45\)</span></p></li>
<li><p>Controls: 25 exposed, 75 unexposed → <span class="math notranslate nohighlight">\(\hat{p}_2 = 25/100 = 0.25\)</span></p></li>
</ul>
<p>The estimated odds ratio is:</p>
<div class="math notranslate nohighlight">
\[\widehat{\text{OR}} = \frac{\hat{p}_1/(1-\hat{p}_1)}{\hat{p}_2/(1-\hat{p}_2)} = \frac{0.45/0.55}{0.25/0.75} = \frac{0.818}{0.333} = 2.45\]</div>
<p><strong>Log-OR approach</strong> (preferred for confidence intervals):</p>
<div class="math notranslate nohighlight">
\[\log(\widehat{\text{OR}}) = \log\left(\frac{a \cdot d}{b \cdot c}\right) = \log(45) + \log(75) - \log(55) - \log(25) = 0.898\]</div>
<p>The variance of log-OR from a 2×2 table is approximately:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\log(\widehat{\text{OR}})) \approx \frac{1}{a} + \frac{1}{b} + \frac{1}{c} + \frac{1}{d} = \frac{1}{45} + \frac{1}{55} + \frac{1}{25} + \frac{1}{75} = 0.0936\]</div>
<p>So <span class="math notranslate nohighlight">\(\text{SE}(\log(\widehat{\text{OR}})) = \sqrt{0.0936} = 0.306\)</span>.</p>
<p><strong>95% CI for log-OR</strong>: <span class="math notranslate nohighlight">\(0.898 \pm 1.96 \times 0.306 = (0.298, 1.498)\)</span></p>
<p><strong>95% CI for OR</strong>: <span class="math notranslate nohighlight">\((e^{0.298}, e^{1.498}) = (1.35, 4.47)\)</span></p>
<p>The exposure is associated with significantly increased odds of being a case (OR significantly &gt; 1).</p>
</div>
<figure class="align-center" id="id8">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_3_fig03_log_logit_transforms.png"><img alt="Log and logit transformations symmetrizing distributions" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_3_fig03_log_logit_transforms.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.69 </span><span class="caption-text"><strong>Figure 3.3.4</strong>: Transformations symmetrize distributions and improve confidence intervals. <strong>Top row</strong>: Exponential rate <span class="math notranslate nohighlight">\(\hat{\lambda}\)</span> is right-skewed (a), but <span class="math notranslate nohighlight">\(\log(\hat{\lambda})\)</span> is nearly symmetric (b), yielding a valid asymmetric CI on the original scale (c). <strong>Bottom row</strong>: Proportion <span class="math notranslate nohighlight">\(\hat{p}\)</span> near zero is constrained and skewed (d), but <span class="math notranslate nohighlight">\(\text{logit}(\hat{p})\)</span> is symmetric on <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> (e); the logit-scale CI respects the <span class="math notranslate nohighlight">\([0,1]\)</span> boundary while Wald may not (f).</span><a class="headerlink" href="#id8" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="the-multivariate-delta-method">
<h3>The Multivariate Delta Method<a class="headerlink" href="#the-multivariate-delta-method" title="Link to this heading"></a></h3>
<p>When dealing with multiple parameters or vector-valued functions, we need the multivariate extension.</p>
<section id="statement">
<h4>Statement<a class="headerlink" href="#statement" title="Link to this heading"></a></h4>
<div class="note admonition">
<p class="admonition-title">Theorem: Multivariate Delta Method</p>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{T}_n\)</span> be a <span class="math notranslate nohighlight">\(p\)</span>-dimensional random vector satisfying:</p>
<div class="math notranslate nohighlight">
\[\sqrt{n}(\mathbf{T}_n - \boldsymbol{\theta}) \xrightarrow{d} \mathcal{N}_p(\mathbf{0}, \boldsymbol{\Sigma})\]</div>
<p>If <span class="math notranslate nohighlight">\(g: \mathbb{R}^p \to \mathbb{R}^q\)</span> has continuous partial derivatives with Jacobian matrix <span class="math notranslate nohighlight">\(\mathbf{G} = \nabla g(\boldsymbol{\theta})\)</span>, then:</p>
<div class="math notranslate nohighlight" id="equation-multivariate-delta">
<span class="eqno">(2.76)<a class="headerlink" href="#equation-multivariate-delta" title="Link to this equation"></a></span>\[\sqrt{n}(g(\mathbf{T}_n) - g(\boldsymbol{\theta})) \xrightarrow{d} \mathcal{N}_q(\mathbf{0}, \mathbf{G} \boldsymbol{\Sigma} \mathbf{G}^\top)\]</div>
</div>
<p>For a scalar function <span class="math notranslate nohighlight">\(g: \mathbb{R}^p \to \mathbb{R}\)</span>, the gradient is a column vector <span class="math notranslate nohighlight">\(\nabla g = (\partial g / \partial \theta_1, \ldots, \partial g / \partial \theta_p)^\top\)</span>, and:</p>
<div class="math notranslate nohighlight" id="equation-multivariate-delta-scalar">
<span class="eqno">(2.77)<a class="headerlink" href="#equation-multivariate-delta-scalar" title="Link to this equation"></a></span>\[\text{Var}(g(\hat{\boldsymbol{\theta}})) \approx (\nabla g)^\top \boldsymbol{\Sigma} (\nabla g) = \sum_{j=1}^{p} \sum_{k=1}^{p} \frac{\partial g}{\partial \theta_j} \frac{\partial g}{\partial \theta_k} \sigma_{jk}\]</div>
</section>
<section id="special-cases">
<h4>Special Cases<a class="headerlink" href="#special-cases" title="Link to this heading"></a></h4>
<p><strong>Linear combinations</strong>: If <span class="math notranslate nohighlight">\(g(\boldsymbol{\theta}) = \mathbf{a}^\top \boldsymbol{\theta}\)</span> for constant vector <span class="math notranslate nohighlight">\(\mathbf{a}\)</span>, then <span class="math notranslate nohighlight">\(\nabla g = \mathbf{a}\)</span> and:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\mathbf{a}^\top \hat{\boldsymbol{\theta}}) = \mathbf{a}^\top \boldsymbol{\Sigma} \mathbf{a}\]</div>
<p>This is exact (not approximate) since linear functions require no Taylor expansion.</p>
<p><strong>Ratio of two parameters</strong>: If <span class="math notranslate nohighlight">\(g(\theta_1, \theta_2) = \theta_1 / \theta_2\)</span>, then:</p>
<div class="math notranslate nohighlight">
\[\nabla g = \left( \frac{1}{\theta_2}, -\frac{\theta_1}{\theta_2^2} \right)^\top\]</div>
<div class="math notranslate nohighlight" id="equation-ratio-variance">
<span class="eqno">(2.78)<a class="headerlink" href="#equation-ratio-variance" title="Link to this equation"></a></span>\[\text{Var}\left(\frac{\hat{\theta}_1}{\hat{\theta}_2}\right) \approx \frac{1}{\theta_2^2}\text{Var}(\hat{\theta}_1) + \frac{\theta_1^2}{\theta_2^4}\text{Var}(\hat{\theta}_2) - \frac{2\theta_1}{\theta_2^3}\text{Cov}(\hat{\theta}_1, \hat{\theta}_2)\]</div>
<p><strong>Product of two parameters</strong>: If <span class="math notranslate nohighlight">\(g(\theta_1, \theta_2) = \theta_1 \theta_2\)</span>, then <span class="math notranslate nohighlight">\(\nabla g = (\theta_2, \theta_1)^\top\)</span> and:</p>
<div class="math notranslate nohighlight" id="equation-product-variance">
<span class="eqno">(2.79)<a class="headerlink" href="#equation-product-variance" title="Link to this equation"></a></span>\[\text{Var}(\hat{\theta}_1 \hat{\theta}_2) \approx \theta_2^2 \text{Var}(\hat{\theta}_1) + \theta_1^2 \text{Var}(\hat{\theta}_2) + 2\theta_1 \theta_2 \text{Cov}(\hat{\theta}_1, \hat{\theta}_2)\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">multivariate_delta_method</span><span class="p">(</span><span class="n">theta_hat</span><span class="p">,</span> <span class="n">cov_matrix</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">gradient_g</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Apply multivariate delta method.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    theta_hat : ndarray</span>
<span class="sd">        Point estimates (p-vector).</span>
<span class="sd">    cov_matrix : ndarray</span>
<span class="sd">        Covariance matrix of theta_hat (p × p).</span>
<span class="sd">    g : callable</span>
<span class="sd">        Scalar transformation function.</span>
<span class="sd">    gradient_g : callable</span>
<span class="sd">        Gradient of g, returns p-vector.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        Estimate, variance, SE, and 95% CI for g(θ).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">g_hat</span> <span class="o">=</span> <span class="n">g</span><span class="p">(</span><span class="n">theta_hat</span><span class="p">)</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">gradient_g</span><span class="p">(</span><span class="n">theta_hat</span><span class="p">)</span>

    <span class="c1"># Var(g(θ̂)) = ∇g&#39; Σ ∇g</span>
    <span class="n">var_g</span> <span class="o">=</span> <span class="n">grad</span> <span class="o">@</span> <span class="n">cov_matrix</span> <span class="o">@</span> <span class="n">grad</span>
    <span class="n">se_g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_g</span><span class="p">)</span>

    <span class="n">z</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.975</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;estimate&#39;</span><span class="p">:</span> <span class="n">g_hat</span><span class="p">,</span>
        <span class="s1">&#39;variance&#39;</span><span class="p">:</span> <span class="n">var_g</span><span class="p">,</span>
        <span class="s1">&#39;se&#39;</span><span class="p">:</span> <span class="n">se_g</span><span class="p">,</span>
        <span class="s1">&#39;ci_lower&#39;</span><span class="p">:</span> <span class="n">g_hat</span> <span class="o">-</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se_g</span><span class="p">,</span>
        <span class="s1">&#39;ci_upper&#39;</span><span class="p">:</span> <span class="n">g_hat</span> <span class="o">+</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se_g</span>
    <span class="p">}</span>

<span class="c1"># Example: Vaccine efficacy VE = 1 - p_v/p_p</span>
<span class="c1"># From two independent proportions</span>
<span class="n">p_v_hat</span> <span class="o">=</span> <span class="mi">8</span> <span class="o">/</span> <span class="mi">20000</span>    <span class="c1"># Vaccinated infection rate</span>
<span class="n">p_p_hat</span> <span class="o">=</span> <span class="mi">162</span> <span class="o">/</span> <span class="mi">20000</span>  <span class="c1"># Placebo infection rate</span>

<span class="n">n_v</span><span class="p">,</span> <span class="n">n_p</span> <span class="o">=</span> <span class="mi">20000</span><span class="p">,</span> <span class="mi">20000</span>
<span class="n">var_p_v</span> <span class="o">=</span> <span class="n">p_v_hat</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_v_hat</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_v</span>
<span class="n">var_p_p</span> <span class="o">=</span> <span class="n">p_p_hat</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_p_hat</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_p</span>

<span class="c1"># Independent samples → covariance = 0</span>
<span class="n">theta_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">p_v_hat</span><span class="p">,</span> <span class="n">p_p_hat</span><span class="p">])</span>
<span class="n">cov_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="n">var_p_v</span><span class="p">,</span> <span class="n">var_p_p</span><span class="p">])</span>

<span class="c1"># VE = 1 - p_v/p_p = g(p_v, p_p)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">ve</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="k">def</span><span class="w"> </span><span class="nf">grad_ve</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="c1"># ∂VE/∂p_v = -1/p_p, ∂VE/∂p_p = p_v/p_p²</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">])</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">multivariate_delta_method</span><span class="p">(</span><span class="n">theta_hat</span><span class="p">,</span> <span class="n">cov_matrix</span><span class="p">,</span> <span class="n">ve</span><span class="p">,</span> <span class="n">grad_ve</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vaccine Efficacy Analysis:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  p̂_v = </span><span class="si">{</span><span class="n">p_v_hat</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (vaccinated)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  p̂_p = </span><span class="si">{</span><span class="n">p_p_hat</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (placebo)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">  VE = 1 - RR = </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;estimate&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;estimate&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  SE(VE) = </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;se&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  95% CI: (</span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci_lower&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci_upper&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  95% CI: (</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci_lower&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%, </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci_upper&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Vaccine Efficacy Analysis:
  p̂_v = 0.000400 (vaccinated)
  p̂_p = 0.008100 (placebo)

  VE = 1 - RR = 0.9506 (95.1%)
  SE(VE) = 0.0175
  95% CI: (0.9163, 0.9849)
  95% CI: (91.6%, 98.5%)
</pre></div>
</div>
<figure class="align-center" id="id9">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_3_fig04_multivariate_delta.png"><img alt="Multivariate delta method showing joint distribution, gradient, and resulting univariate distribution" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_3_fig04_multivariate_delta.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.70 </span><span class="caption-text"><strong>Figure 3.3.5</strong>: The multivariate delta method for <span class="math notranslate nohighlight">\(g(\theta_1, \theta_2) = \theta_1/\theta_2\)</span>. (a) Joint distribution of <span class="math notranslate nohighlight">\((\hat{\theta}_1, \hat{\theta}_2)\)</span> with 95% confidence ellipse. (b) Contours of <span class="math notranslate nohighlight">\(g\)</span> with gradient vector <span class="math notranslate nohighlight">\(\nabla g = (1/\theta_2, -\theta_1/\theta_2^2)^\top\)</span> at the true parameter. (c) The resulting distribution of the ratio estimate with delta method normal approximation and 95% CI derived from <span class="math notranslate nohighlight">\(\text{Var}(g) = (\nabla g)^\top \boldsymbol{\Sigma} (\nabla g)\)</span>.</span><a class="headerlink" href="#id9" title="Link to this image"></a></p>
</figcaption>
</figure>
<figure class="align-center" id="id10">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_3_fig10_vaccine_efficacy.png"><img alt="Complete vaccine efficacy analysis showing joint distribution and CI comparison" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_3_fig10_vaccine_efficacy.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.71 </span><span class="caption-text"><strong>Figure 3.3.6</strong>: Vaccine efficacy analysis (VE = 95.1%). (a) Joint distribution of <span class="math notranslate nohighlight">\((\hat{p}_v, \hat{p}_p)\)</span> with iso-VE lines; the elongated confidence region reflects much larger uncertainty in <span class="math notranslate nohighlight">\(\hat{p}_p\)</span> due to more events. (b) Distribution of <span class="math notranslate nohighlight">\(\widehat{\text{VE}}\)</span> with delta method approximation showing excellent agreement (SE = 1.8%). (c) Three 95% CI methods: delta on VE scale, log-RR back-transformed, and bootstrap percentile—all give similar results for this high-efficacy vaccine.</span><a class="headerlink" href="#id10" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
</section>
<section id="the-plug-in-principle">
<h2>The Plug-in Principle<a class="headerlink" href="#the-plug-in-principle" title="Link to this heading"></a></h2>
<p>The delta method relies on a broader concept: <strong>plug-in estimation</strong>. If <span class="math notranslate nohighlight">\(\tau = g(\theta)\)</span> is the parameter of interest, the plug-in estimator is simply <span class="math notranslate nohighlight">\(\hat{\tau} = g(\hat{\theta})\)</span>.</p>
<section id="formal-statement">
<h3>Formal Statement<a class="headerlink" href="#formal-statement" title="Link to this heading"></a></h3>
<div class="note admonition">
<p class="admonition-title">Definition: Plug-in Estimator</p>
<p>Let <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> be the empirical distribution function of a sample <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span>. For any functional <span class="math notranslate nohighlight">\(T(F)\)</span> (a mapping from distributions to real numbers), the <strong>plug-in estimator</strong> is:</p>
<div class="math notranslate nohighlight">
\[\hat{T} = T(\hat{F}_n)\]</div>
<p>Replace the unknown true distribution <span class="math notranslate nohighlight">\(F\)</span> with its empirical counterpart <span class="math notranslate nohighlight">\(\hat{F}_n\)</span>, then compute the functional.</p>
</div>
<p><strong>Examples</strong>:</p>
<ul class="simple">
<li><p><strong>Mean</strong>: <span class="math notranslate nohighlight">\(T(F) = \int x \, dF(x)\)</span>. Plug-in: <span class="math notranslate nohighlight">\(\hat{T} = \int x \, d\hat{F}_n(x) = \bar{X}\)</span></p></li>
<li><p><strong>Variance</strong>: <span class="math notranslate nohighlight">\(T(F) = \int (x - \mu)^2 dF(x)\)</span>. Plug-in: <span class="math notranslate nohighlight">\(\hat{T} = \frac{1}{n}\sum(X_i - \bar{X})^2\)</span></p></li>
<li><p><strong>Quantiles</strong>: <span class="math notranslate nohighlight">\(T(F) = F^{-1}(p)\)</span>. Plug-in: <span class="math notranslate nohighlight">\(\hat{T} = \hat{F}_n^{-1}(p)\)</span> = sample quantile</p></li>
<li><p><strong>Correlation</strong>: <span class="math notranslate nohighlight">\(T(F) = \text{Corr}_F(X, Y)\)</span>. Plug-in: sample correlation</p></li>
</ul>
<p>The plug-in principle is remarkably general and forms the foundation for much of nonparametric statistics. Combined with the delta method, it provides variance estimates for almost any statistic.</p>
</section>
<section id="when-plug-in-works-well">
<h3>When Plug-in Works Well<a class="headerlink" href="#when-plug-in-works-well" title="Link to this heading"></a></h3>
<p>Plug-in estimation is most reliable when:</p>
<ol class="arabic simple">
<li><p><strong>The functional is smooth</strong>: Small changes in <span class="math notranslate nohighlight">\(F\)</span> produce small changes in <span class="math notranslate nohighlight">\(T(F)\)</span>. The delta method formalizes this through differentiability.</p></li>
<li><p><strong>The sample size is adequate</strong>: <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> must be a good approximation to <span class="math notranslate nohighlight">\(F\)</span>.</p></li>
<li><p><strong>The estimator :math:`hat{theta}` is consistent</strong>: We need <span class="math notranslate nohighlight">\(\hat{\theta} \xrightarrow{p} \theta_0\)</span> for <span class="math notranslate nohighlight">\(g(\hat{\theta})\)</span> to be consistent for <span class="math notranslate nohighlight">\(g(\theta_0)\)</span>.</p></li>
</ol>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ⚠️ When Plug-in Fails</p>
<p>The plug-in principle can fail when:</p>
<p><strong>1. Non-differentiable functionals</strong>: The sample maximum <span class="math notranslate nohighlight">\(X_{(n)}\)</span> estimating the population maximum doesn’t follow normal asymptotics—the delta method doesn’t apply.</p>
<p><strong>2. Boundary parameters</strong>: If <span class="math notranslate nohighlight">\(\theta\)</span> is near the boundary of the parameter space, <span class="math notranslate nohighlight">\(g(\hat{\theta})\)</span> may have poor properties. Example: <span class="math notranslate nohighlight">\(\hat{p} = 0\)</span> when estimating <span class="math notranslate nohighlight">\(\log(p)\)</span>.</p>
<p><strong>3. High curvature</strong>: When <span class="math notranslate nohighlight">\(g''(\theta)\)</span> is large and <span class="math notranslate nohighlight">\(\text{Var}(\hat{\theta})\)</span> is not small, the linear approximation breaks down. The second-order delta method may help.</p>
<p><strong>4. Discontinuous transformations</strong>: Indicator functions and step functions have zero derivative almost everywhere, making the delta method useless.</p>
<p><strong>5. Ratio estimation with noisy denominators</strong>: When <span class="math notranslate nohighlight">\(\hat{\theta}_2\)</span> is small or has large variance relative to <span class="math notranslate nohighlight">\(|\theta_2|\)</span>, the delta method CI for <span class="math notranslate nohighlight">\(\theta_1/\theta_2\)</span> can have poor coverage. <strong>Fieller’s theorem</strong> provides an alternative approach that constructs CIs by inverting a hypothesis test, yielding more reliable coverage when the denominator is uncertain. Consider Fieller’s method when the coefficient of variation of the denominator exceeds ~20%.</p>
</div>
</section>
</section>
<section id="variance-estimation-methods">
<h2>Variance Estimation Methods<a class="headerlink" href="#variance-estimation-methods" title="Link to this heading"></a></h2>
<p>To apply the delta method in practice, we need <span class="math notranslate nohighlight">\(\text{Var}(\hat{\theta})\)</span>. Several analytical approaches exist, each with tradeoffs. (Chapter 4 introduces resampling-based alternatives that complement these methods.)</p>
<section id="fisher-information-expected-information">
<h3>Fisher Information (Expected Information)<a class="headerlink" href="#fisher-information-expected-information" title="Link to this heading"></a></h3>
<p>From <a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#ch3-2-maximum-likelihood-estimation"><span class="std std-ref">Section 3.2</span></a>, the MLE has asymptotic variance:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\hat{\theta}) \approx \frac{1}{n I_1(\theta)}\]</div>
<p>where <span class="math notranslate nohighlight">\(I_1(\theta) = -\mathbb{E}[\partial^2 \log f(X|\theta) / \partial \theta^2]\)</span> is the per-observation Fisher information.</p>
<p><strong>Plug-in variance estimate</strong>:</p>
<div class="math notranslate nohighlight" id="equation-fisher-var-est">
<span class="eqno">(2.80)<a class="headerlink" href="#equation-fisher-var-est" title="Link to this equation"></a></span>\[\widehat{\text{Var}}(\hat{\theta}) = \frac{1}{n I_1(\hat{\theta})}\]</div>
<p><strong>Advantages</strong>: Theoretically motivated; often has closed-form expression for exponential families.</p>
<p><strong>Disadvantages</strong>: Assumes correct model specification; may differ from observed information.</p>
</section>
<section id="observed-information">
<h3>Observed Information<a class="headerlink" href="#observed-information" title="Link to this heading"></a></h3>
<p>Replace the expected Hessian with the observed (sample) Hessian:</p>
<div class="math notranslate nohighlight" id="equation-observed-info">
<span class="eqno">(2.81)<a class="headerlink" href="#equation-observed-info" title="Link to this equation"></a></span>\[J_n(\hat{\theta}) = -\frac{\partial^2 \ell_n(\theta)}{\partial \theta^2}\bigg|_{\theta = \hat{\theta}}\]</div>
<p>The observed information variance estimate is:</p>
<div class="math notranslate nohighlight" id="equation-observed-var-est">
<span class="eqno">(2.82)<a class="headerlink" href="#equation-observed-var-est" title="Link to this equation"></a></span>\[\widehat{\text{Var}}(\hat{\theta}) = J_n(\hat{\theta})^{-1}\]</div>
<p><strong>Advantages</strong>: Data-adaptive; accounts for sample-specific curvature; preferred by some theoretical arguments (Efron &amp; Hinkley, 1978).</p>
<p><strong>Disadvantages</strong>: More variable than expected information; can be unstable for small samples.</p>
<p><strong>Under correct specification</strong>, expected and observed information are asymptotically equivalent. Under <strong>misspecification</strong>, observed information combined with sandwich adjustment is more robust.</p>
</section>
<section id="sandwich-variance-estimator">
<h3>Sandwich Variance Estimator<a class="headerlink" href="#sandwich-variance-estimator" title="Link to this heading"></a></h3>
<p>When the model may be misspecified, the <strong>sandwich estimator</strong> (introduced in <a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#ch3-2-maximum-likelihood-estimation"><span class="std std-ref">Section 3.2</span></a>) provides robust standard errors:</p>
<div class="math notranslate nohighlight" id="equation-sandwich-variance">
<span class="eqno">(2.83)<a class="headerlink" href="#equation-sandwich-variance" title="Link to this equation"></a></span>\[\widehat{\text{Var}}_{\text{sandwich}}(\hat{\theta}) = A^{-1} B A^{-1}\]</div>
<p>where <span class="math notranslate nohighlight">\(A = -\frac{1}{n}\sum_{i=1}^n \nabla^2 \ell_i(\hat{\theta})\)</span> is the average Hessian and <span class="math notranslate nohighlight">\(B = \frac{1}{n}\sum_{i=1}^n \nabla \ell_i(\hat{\theta}) \nabla \ell_i(\hat{\theta})^\top\)</span> is the empirical variance of score contributions.</p>
<p>Under correct specification, <span class="math notranslate nohighlight">\(A = B\)</span> and the sandwich reduces to the inverse Fisher information. Under misspecification, <span class="math notranslate nohighlight">\(A \neq B\)</span> and the sandwich captures the true variability.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">sandwich_se</span><span class="p">(</span><span class="n">log_lik_contributions</span><span class="p">,</span> <span class="n">theta_hat</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute sandwich standard errors.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    log_lik_contributions : callable</span>
<span class="sd">        Function returning vector of per-observation log-likelihoods.</span>
<span class="sd">    theta_hat : ndarray</span>
<span class="sd">        MLE parameter estimates.</span>
<span class="sd">    epsilon : float</span>
<span class="sd">        Step size for numerical derivatives.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ndarray</span>
<span class="sd">        Sandwich standard errors.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">log_lik_contributions</span><span class="p">(</span><span class="n">theta_hat</span><span class="p">))</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">theta_hat</span><span class="p">)</span>

    <span class="c1"># Compute A: average Hessian</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">p</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
            <span class="n">theta_pp</span> <span class="o">=</span> <span class="n">theta_hat</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">theta_pm</span> <span class="o">=</span> <span class="n">theta_hat</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">theta_mp</span> <span class="o">=</span> <span class="n">theta_hat</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">theta_mm</span> <span class="o">=</span> <span class="n">theta_hat</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

            <span class="n">theta_pp</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">epsilon</span><span class="p">;</span> <span class="n">theta_pp</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">epsilon</span>
            <span class="n">theta_pm</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">epsilon</span><span class="p">;</span> <span class="n">theta_pm</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-=</span> <span class="n">epsilon</span>
            <span class="n">theta_mp</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">epsilon</span><span class="p">;</span> <span class="n">theta_mp</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">epsilon</span>
            <span class="n">theta_mm</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">epsilon</span><span class="p">;</span> <span class="n">theta_mm</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-=</span> <span class="n">epsilon</span>

            <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
                <span class="p">(</span><span class="n">log_lik_contributions</span><span class="p">(</span><span class="n">theta_pp</span><span class="p">)</span> <span class="o">-</span> <span class="n">log_lik_contributions</span><span class="p">(</span><span class="n">theta_pm</span><span class="p">)</span>
                 <span class="o">-</span> <span class="n">log_lik_contributions</span><span class="p">(</span><span class="n">theta_mp</span><span class="p">)</span> <span class="o">+</span> <span class="n">log_lik_contributions</span><span class="p">(</span><span class="n">theta_mm</span><span class="p">))</span>
            <span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">epsilon</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">A</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>

    <span class="c1"># Compute B: empirical variance of scores</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
        <span class="n">theta_plus</span> <span class="o">=</span> <span class="n">theta_hat</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">theta_minus</span> <span class="o">=</span> <span class="n">theta_hat</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">theta_plus</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">epsilon</span>
        <span class="n">theta_minus</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-=</span> <span class="n">epsilon</span>
        <span class="n">scores</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">log_lik_contributions</span><span class="p">(</span><span class="n">theta_plus</span><span class="p">)</span>
                       <span class="o">-</span> <span class="n">log_lik_contributions</span><span class="p">(</span><span class="n">theta_minus</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">epsilon</span><span class="p">)</span>

    <span class="n">B</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">scores</span> <span class="o">/</span> <span class="n">n</span>

    <span class="c1"># Sandwich: A^{-1} B A^{-1}</span>
    <span class="n">A_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
    <span class="n">sandwich_cov</span> <span class="o">=</span> <span class="n">A_inv</span> <span class="o">@</span> <span class="n">B</span> <span class="o">@</span> <span class="n">A_inv</span> <span class="o">/</span> <span class="n">n</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">sandwich_cov</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="numerical-differentiation">
<h3>Numerical Differentiation<a class="headerlink" href="#numerical-differentiation" title="Link to this heading"></a></h3>
<p>When analytical Hessians are unavailable, numerical approximation works well:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">optimize</span>

<span class="k">def</span><span class="w"> </span><span class="nf">numerical_hessian</span><span class="p">(</span><span class="n">log_lik</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute Hessian of log-likelihood numerically.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    log_lik : callable</span>
<span class="sd">        Log-likelihood function.</span>
<span class="sd">    theta : ndarray</span>
<span class="sd">        Parameter values.</span>
<span class="sd">    epsilon : float</span>
<span class="sd">        Step size for finite differences.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ndarray</span>
<span class="sd">        Hessian matrix.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">p</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
            <span class="c1"># Second partial derivative via central differences</span>
            <span class="n">theta_pp</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">theta_pm</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">theta_mp</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">theta_mm</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

            <span class="n">theta_pp</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">epsilon</span>
            <span class="n">theta_pp</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">epsilon</span>
            <span class="n">theta_pm</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">epsilon</span>
            <span class="n">theta_pm</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-=</span> <span class="n">epsilon</span>
            <span class="n">theta_mp</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">epsilon</span>
            <span class="n">theta_mp</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">epsilon</span>
            <span class="n">theta_mm</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">epsilon</span>
            <span class="n">theta_mm</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-=</span> <span class="n">epsilon</span>

            <span class="n">H</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">log_lik</span><span class="p">(</span><span class="n">theta_pp</span><span class="p">)</span> <span class="o">-</span> <span class="n">log_lik</span><span class="p">(</span><span class="n">theta_pm</span><span class="p">)</span>
                       <span class="o">-</span> <span class="n">log_lik</span><span class="p">(</span><span class="n">theta_mp</span><span class="p">)</span> <span class="o">+</span> <span class="n">log_lik</span><span class="p">(</span><span class="n">theta_mm</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">epsilon</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">H</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">H</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">H</span>

<span class="k">def</span><span class="w"> </span><span class="nf">variance_from_hessian</span><span class="p">(</span><span class="n">log_lik</span><span class="p">,</span> <span class="n">theta_hat</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Estimate variance via observed information (negative Hessian inverse).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">numerical_hessian</span><span class="p">(</span><span class="n">log_lik</span><span class="p">,</span> <span class="n">theta_hat</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="o">-</span><span class="n">H</span><span class="p">)</span>

<span class="c1"># Example: Normal MLE variance</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">normal_log_lik</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">mu</span><span class="p">,</span> <span class="n">log_sigma</span> <span class="o">=</span> <span class="n">theta</span>  <span class="c1"># Use log(σ) for unconstrained optimization</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_sigma</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">))</span>

<span class="c1"># Find MLE</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">optimize</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">theta</span><span class="p">:</span> <span class="o">-</span><span class="n">normal_log_lik</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span>
    <span class="n">x0</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">x</span><span class="p">))],</span>
    <span class="n">method</span><span class="o">=</span><span class="s1">&#39;BFGS&#39;</span>
<span class="p">)</span>
<span class="n">theta_hat</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">x</span>
<span class="n">mu_hat</span><span class="p">,</span> <span class="n">sigma_hat</span> <span class="o">=</span> <span class="n">theta_hat</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">theta_hat</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="c1"># Variance via observed information</span>
<span class="n">cov_hat</span> <span class="o">=</span> <span class="n">variance_from_hessian</span><span class="p">(</span><span class="n">normal_log_lik</span><span class="p">,</span> <span class="n">theta_hat</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Normal MLE:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  μ̂ = </span><span class="si">{</span><span class="n">mu_hat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, σ̂ = </span><span class="si">{</span><span class="n">sigma_hat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  SE(μ̂) = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cov_hat</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  SE(log(σ̂)) = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cov_hat</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Normal MLE:
  μ̂ = 4.8572, σ̂ = 1.9183
  SE(μ̂) = 0.1918
  SE(log(σ̂)) = 0.0707
</pre></div>
</div>
</section>
<section id="comparison-of-variance-estimation-methods">
<h3>Comparison of Variance Estimation Methods<a class="headerlink" href="#comparison-of-variance-estimation-methods" title="Link to this heading"></a></h3>
<table class="docutils align-default" id="id11">
<caption><span class="caption-number">Table 2.15 </span><span class="caption-text">Variance Estimation Methods Comparison</span><a class="headerlink" href="#id11" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 30.0%" />
<col style="width: 25.0%" />
<col style="width: 25.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Best When</p></th>
<th class="head"><p>Advantages</p></th>
<th class="head"><p>Disadvantages</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Fisher Information</p></td>
<td><p>Large <span class="math notranslate nohighlight">\(n\)</span>, correct model</p></td>
<td><p>Fast, stable, closed-form</p></td>
<td><p>Assumes model correct</p></td>
</tr>
<tr class="row-odd"><td><p>Observed Information</p></td>
<td><p>Moderate <span class="math notranslate nohighlight">\(n\)</span></p></td>
<td><p>Data-adaptive, accounts for curvature</p></td>
<td><p>More variable than Fisher</p></td>
</tr>
<tr class="row-even"><td><p>Sandwich (Robust)</p></td>
<td><p>Model may be misspecified</p></td>
<td><p>Consistent under misspecification</p></td>
<td><p>Conservative; requires score computations</p></td>
</tr>
<tr class="row-odd"><td><p>Numerical Hessian</p></td>
<td><p>No analytical derivatives</p></td>
<td><p>General purpose</p></td>
<td><p>Numerical precision issues</p></td>
</tr>
</tbody>
</table>
<div class="tip admonition">
<p class="admonition-title">Looking Ahead: Resampling Methods</p>
<p>Chapter 4 introduces the <strong>bootstrap</strong>, which provides variance estimates without analytical derivatives or distributional assumptions. The bootstrap is particularly valuable for:</p>
<ul class="simple">
<li><p>Complex statistics where information-based methods are intractable</p></li>
<li><p>Small samples where asymptotic approximations may fail</p></li>
<li><p>Validating analytical standard errors</p></li>
</ul>
<p>The delta method and bootstrap are complementary: use analytical methods when they apply, and bootstrap when they don’t or to check robustness.</p>
</div>
</section>
<section id="second-order-delta-method">
<h3>Second-Order Delta Method<a class="headerlink" href="#second-order-delta-method" title="Link to this heading"></a></h3>
<p>When the first derivative vanishes (<span class="math notranslate nohighlight">\(g'(\theta_0) = 0\)</span>) or when higher accuracy is needed, the second-order delta method incorporates curvature.</p>
<section id="id1">
<h4>Statement<a class="headerlink" href="#id1" title="Link to this heading"></a></h4>
<p>If <span class="math notranslate nohighlight">\(g'(\theta_0) = 0\)</span> but <span class="math notranslate nohighlight">\(g''(\theta_0) \neq 0\)</span>, then:</p>
<div class="math notranslate nohighlight" id="equation-second-order-delta">
<span class="eqno">(2.84)<a class="headerlink" href="#equation-second-order-delta" title="Link to this equation"></a></span>\[n(g(T_n) - g(\theta_0)) \xrightarrow{d} \frac{1}{2} g''(\theta_0) \cdot \sigma^2 \cdot \chi^2_1\]</div>
<p>The distribution is no longer normal—it’s a scaled chi-squared. The variance of <span class="math notranslate nohighlight">\(g(T_n)\)</span> is of order <span class="math notranslate nohighlight">\(1/n^2\)</span> rather than <span class="math notranslate nohighlight">\(1/n\)</span>, and the distribution is generally asymmetric.</p>
<div class="note admonition">
<p class="admonition-title">Example 💡 Zero Derivative: <span class="math notranslate nohighlight">\(g(\mu) = \mu^2\)</span> at <span class="math notranslate nohighlight">\(\mu = 0\)</span></p>
<p><strong>Setup</strong>: Let <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} \mathcal{N}(\mu, \sigma^2)\)</span>. Consider <span class="math notranslate nohighlight">\(g(\mu) = \mu^2\)</span> at the point <span class="math notranslate nohighlight">\(\mu = 0\)</span>.</p>
<p>Since <span class="math notranslate nohighlight">\(g'(\mu) = 2\mu\)</span>, we have <span class="math notranslate nohighlight">\(g'(0) = 0\)</span>. The first-order delta method gives <span class="math notranslate nohighlight">\(\text{Var}(\bar{X}^2) \approx 0\)</span>, which is clearly wrong—the estimator <span class="math notranslate nohighlight">\(\bar{X}^2\)</span> certainly has positive variance!</p>
<p>Using the second-order result with <span class="math notranslate nohighlight">\(g''(\mu) = 2\)</span>:</p>
<div class="math notranslate nohighlight">
\[n(\bar{X}^2 - 0) \xrightarrow{d} \frac{1}{2} \cdot 2 \cdot (\sigma^2/n) \cdot n \cdot \chi^2_1 = \sigma^2 \chi^2_1\]</div>
<p>This recovers the correct result that <span class="math notranslate nohighlight">\(n\bar{X}^2/\sigma^2 \sim \chi^2_1\)</span> under <span class="math notranslate nohighlight">\(\mu = 0\)</span>. The distribution is chi-squared, not normal, and the variance is <span class="math notranslate nohighlight">\(O(1/n^2)\)</span> rather than <span class="math notranslate nohighlight">\(O(1/n)\)</span>.</p>
</div>
</section>
</section>
</section>
<section id="applications-and-worked-examples">
<h2>Applications and Worked Examples<a class="headerlink" href="#applications-and-worked-examples" title="Link to this heading"></a></h2>
<section id="relative-risk-and-risk-difference">
<h3>Relative Risk and Risk Difference<a class="headerlink" href="#relative-risk-and-risk-difference" title="Link to this heading"></a></h3>
<p>In epidemiology, we often compare risks between two groups.</p>
<p><strong>Setting</strong>: Treatment group has <span class="math notranslate nohighlight">\(\hat{p}_1 = x_1/n_1\)</span> infections; control has <span class="math notranslate nohighlight">\(\hat{p}_2 = x_2/n_2\)</span>.</p>
<p><strong>Relative risk</strong>: <span class="math notranslate nohighlight">\(\text{RR} = p_1/p_2\)</span></p>
<p>Using the delta method on <span class="math notranslate nohighlight">\(\log(\text{RR}) = \log(p_1) - \log(p_2)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\log(\widehat{\text{RR}})) = \frac{1-p_1}{n_1 p_1} + \frac{1-p_2}{n_2 p_2}\]</div>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ⚠️ Zero Cell Counts</p>
<p>When any cell has zero events (<span class="math notranslate nohighlight">\(x_1 = 0\)</span> or <span class="math notranslate nohighlight">\(x_2 = 0\)</span>), the log transformation fails. Apply a <strong>continuity correction</strong> before computing log-scale statistics. The Haldane–Anscombe correction adds 0.5 to all cells: use <span class="math notranslate nohighlight">\(\hat{p}_i = (x_i + 0.5)/(n_i + 1)\)</span>. This small adjustment enables valid inference while minimally affecting point estimates for non-zero counts.</p>
</div>
<p><strong>Risk difference</strong>: <span class="math notranslate nohighlight">\(\text{RD} = p_1 - p_2\)</span></p>
<p>This is a linear combination, so no delta method is needed:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\widehat{\text{RD}}) = \frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">risk_measures</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">n1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">n2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute relative risk and risk difference with confidence intervals.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x1, n1 : int</span>
<span class="sd">        Events and sample size in group 1 (treatment).</span>
<span class="sd">    x2, n2 : int</span>
<span class="sd">        Events and sample size in group 2 (control).</span>
<span class="sd">    alpha : float</span>
<span class="sd">        Significance level.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        RR, RD, and their confidence intervals.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p1_hat</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">/</span> <span class="n">n1</span>
    <span class="n">p2_hat</span> <span class="o">=</span> <span class="n">x2</span> <span class="o">/</span> <span class="n">n2</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Relative Risk</span>
    <span class="n">rr</span> <span class="o">=</span> <span class="n">p1_hat</span> <span class="o">/</span> <span class="n">p2_hat</span>
    <span class="c1"># Var(log(RR)) = (1-p1)/(n1*p1) + (1-p2)/(n2*p2)</span>
    <span class="n">var_log_rr</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p1_hat</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n1</span> <span class="o">*</span> <span class="n">p1_hat</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p2_hat</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n2</span> <span class="o">*</span> <span class="n">p2_hat</span><span class="p">)</span>
    <span class="n">se_log_rr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_log_rr</span><span class="p">)</span>
    <span class="n">ci_rr</span> <span class="o">=</span> <span class="p">(</span><span class="n">rr</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span> <span class="o">*</span> <span class="n">se_log_rr</span><span class="p">),</span> <span class="n">rr</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span> <span class="o">*</span> <span class="n">se_log_rr</span><span class="p">))</span>

    <span class="c1"># Risk Difference</span>
    <span class="n">rd</span> <span class="o">=</span> <span class="n">p1_hat</span> <span class="o">-</span> <span class="n">p2_hat</span>
    <span class="n">var_rd</span> <span class="o">=</span> <span class="n">p1_hat</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p1_hat</span><span class="p">)</span> <span class="o">/</span> <span class="n">n1</span> <span class="o">+</span> <span class="n">p2_hat</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p2_hat</span><span class="p">)</span> <span class="o">/</span> <span class="n">n2</span>
    <span class="n">se_rd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_rd</span><span class="p">)</span>
    <span class="n">ci_rd</span> <span class="o">=</span> <span class="p">(</span><span class="n">rd</span> <span class="o">-</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se_rd</span><span class="p">,</span> <span class="n">rd</span> <span class="o">+</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se_rd</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;p1&#39;</span><span class="p">:</span> <span class="n">p1_hat</span><span class="p">,</span> <span class="s1">&#39;p2&#39;</span><span class="p">:</span> <span class="n">p2_hat</span><span class="p">,</span>
        <span class="s1">&#39;RR&#39;</span><span class="p">:</span> <span class="n">rr</span><span class="p">,</span> <span class="s1">&#39;se_log_RR&#39;</span><span class="p">:</span> <span class="n">se_log_rr</span><span class="p">,</span> <span class="s1">&#39;ci_RR&#39;</span><span class="p">:</span> <span class="n">ci_rr</span><span class="p">,</span>
        <span class="s1">&#39;RD&#39;</span><span class="p">:</span> <span class="n">rd</span><span class="p">,</span> <span class="s1">&#39;se_RD&#39;</span><span class="p">:</span> <span class="n">se_rd</span><span class="p">,</span> <span class="s1">&#39;ci_RD&#39;</span><span class="p">:</span> <span class="n">ci_rd</span>
    <span class="p">}</span>

<span class="c1"># Example: Clinical trial data</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">risk_measures</span><span class="p">(</span><span class="n">x1</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">n1</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">x2</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">n2</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Risk Analysis:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Treatment risk: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;p1&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Control risk:   </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;p2&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">  Relative Risk: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;RR&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  95% CI (RR):   (</span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci_RR&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci_RR&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">  Risk Difference: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;RD&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  95% CI (RD):     (</span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci_RD&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci_RD&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Risk Analysis:
  Treatment risk: 0.075
  Control risk:   0.150

  Relative Risk: 0.500
  95% CI (RR):   (0.278, 0.899)

  Risk Difference: -0.075
  95% CI (RD):     (-0.132, -0.018)
</pre></div>
</div>
</section>
<section id="coefficient-of-variation">
<h3>Coefficient of Variation<a class="headerlink" href="#coefficient-of-variation" title="Link to this heading"></a></h3>
<p>The coefficient of variation <span class="math notranslate nohighlight">\(\text{CV} = \sigma/\mu\)</span> measures relative dispersion.</p>
<p><strong>Setup</strong>: From Normal data, we estimate <span class="math notranslate nohighlight">\(\hat{\mu} = \bar{X}\)</span> and <span class="math notranslate nohighlight">\(\hat{\sigma} = S\)</span>.</p>
<p><strong>Delta method</strong> for <span class="math notranslate nohighlight">\(g(\mu, \sigma) = \sigma/\mu\)</span>:</p>
<div class="math notranslate nohighlight">
\[\nabla g = \left( -\frac{\sigma}{\mu^2}, \frac{1}{\mu} \right)^\top\]</div>
<p>For large <span class="math notranslate nohighlight">\(n\)</span> from <span class="math notranslate nohighlight">\(\mathcal{N}(\mu, \sigma^2)\)</span>, asymptotic results give:</p>
<div class="math notranslate nohighlight">
\[\text{Cov}(\bar{X}, S) \approx 0, \quad \text{Var}(\bar{X}) = \sigma^2/n, \quad \text{Var}(S) \approx \sigma^2/(2n) \quad \text{(for normal data)}\]</div>
<p>Thus:</p>
<div class="math notranslate nohighlight" id="equation-cv-variance">
<span class="eqno">(2.85)<a class="headerlink" href="#equation-cv-variance" title="Link to this equation"></a></span>\[\text{Var}(\widehat{\text{CV}}) \approx \frac{\sigma^2}{\mu^4} \cdot \frac{\sigma^2}{n} + \frac{1}{\mu^2} \cdot \frac{\sigma^2}{2n} = \frac{\text{CV}^2}{n}\left(\text{CV}^2 + \frac{1}{2}\right)\]</div>
</section>
<section id="fisher-s-z-transformation-for-correlations">
<h3>Fisher’s z-Transformation for Correlations<a class="headerlink" href="#fisher-s-z-transformation-for-correlations" title="Link to this heading"></a></h3>
<p>The sample correlation coefficient <span class="math notranslate nohighlight">\(r\)</span> has a notoriously complex sampling distribution that depends on the population correlation <span class="math notranslate nohighlight">\(\rho\)</span>. Fisher (1921) proposed the transformation:</p>
<div class="math notranslate nohighlight">
\[z = \frac{1}{2}\log\frac{1+r}{1-r} = \text{arctanh}(r)\]</div>
<p>This is a classic application of variance stabilization via the delta method.</p>
<p><strong>The problem</strong>: The variance of <span class="math notranslate nohighlight">\(r\)</span> is approximately <span class="math notranslate nohighlight">\((1-\rho^2)^2/n\)</span>, which varies dramatically with <span class="math notranslate nohighlight">\(\rho\)</span>—from <span class="math notranslate nohighlight">\(1/n\)</span> at <span class="math notranslate nohighlight">\(\rho = 0\)</span> to nearly 0 as <span class="math notranslate nohighlight">\(|\rho| \to 1\)</span>.</p>
<p><strong>The solution</strong>: Apply the delta method with <span class="math notranslate nohighlight">\(g(r) = \text{arctanh}(r)\)</span> and <span class="math notranslate nohighlight">\(g'(r) = 1/(1-r^2)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(z) \approx \left(\frac{1}{1-\rho^2}\right)^2 \cdot \frac{(1-\rho^2)^2}{n} = \frac{1}{n}\]</div>
<p>The <span class="math notranslate nohighlight">\(\rho\)</span>-dependent terms cancel! The delta method gives <span class="math notranslate nohighlight">\(\text{Var}(z) \approx 1/n\)</span>, but finite-sample analysis shows that <span class="math notranslate nohighlight">\(1/(n-3)\)</span> provides better accuracy for moderate <span class="math notranslate nohighlight">\(n\)</span>. <strong>In practice, always use</strong> <span class="math notranslate nohighlight">\(\text{SE}(z) = 1/\sqrt{n-3}\)</span> (Fisher’s correction), which accounts for small-sample bias in the correlation estimate. This remarkable simplification enables straightforward inference about correlations.</p>
<figure class="align-center" id="id12">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_3_fig06_fisher_z_transform.png"><img alt="Fisher z-transformation stabilizing correlation coefficient variance" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_3_fig06_fisher_z_transform.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.72 </span><span class="caption-text"><strong>Figure 3.3.7</strong>: Fisher’s <span class="math notranslate nohighlight">\(z\)</span>-transformation for correlation coefficients. (a) The variance of <span class="math notranslate nohighlight">\(r\)</span> depends dramatically on <span class="math notranslate nohighlight">\(\rho\)</span>—varying by a factor of 80+ from <span class="math notranslate nohighlight">\(\rho = 0\)</span> to <span class="math notranslate nohighlight">\(\rho = 0.9\)</span>. (b) After applying <span class="math notranslate nohighlight">\(z = \text{arctanh}(r)\)</span>, the variance is approximately <span class="math notranslate nohighlight">\(1/(n-3)\)</span> regardless of <span class="math notranslate nohighlight">\(\rho\)</span>. (c) Standardized <span class="math notranslate nohighlight">\(z\)</span> values from different <span class="math notranslate nohighlight">\(\rho\)</span> all follow approximately <span class="math notranslate nohighlight">\(N(0,1)\)</span>, enabling simple inference.</span><a class="headerlink" href="#id12" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="practical-considerations">
<h2>Practical Considerations<a class="headerlink" href="#practical-considerations" title="Link to this heading"></a></h2>
<section id="when-delta-method-approximations-break-down">
<h3>When Delta Method Approximations Break Down<a class="headerlink" href="#when-delta-method-approximations-break-down" title="Link to this heading"></a></h3>
<p>Several situations compromise delta method accuracy:</p>
<p><strong>1. Small sample sizes</strong>: The asymptotic normal approximation for <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> may be poor, invalidating the entire approach.</p>
<p><strong>2. Large transformation curvature</strong>: If <span class="math notranslate nohighlight">\(|g''(\theta)| \cdot \text{Var}(\hat{\theta})\)</span> is substantial, the linear approximation breaks down. Consider:</p>
<blockquote>
<div><ul class="simple">
<li><p>Using the log scale for positive quantities</p></li>
<li><p>Using the logit scale for probabilities</p></li>
<li><p>Using a variance-stabilizing transformation</p></li>
</ul>
</div></blockquote>
<p><strong>3. Parameters near boundaries</strong>: When <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is near the boundary of its parameter space, asymmetric effects distort the normal approximation. Profile likelihood confidence intervals (Section 3.2) are more reliable.</p>
<p><strong>4. Zero derivatives</strong>: When <span class="math notranslate nohighlight">\(g'(\theta_0) = 0\)</span>, the first-order delta method gives variance 0, which is wrong. The second-order correction is needed.</p>
<figure class="align-center" id="id13">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_3_fig08_delta_method_breakdown.png"><img alt="Four cases where delta method fails" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_3_fig08_delta_method_breakdown.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.73 </span><span class="caption-text"><strong>Figure 3.3.8</strong>: When the delta method breaks down. (a) <strong>Small samples</strong>: With <span class="math notranslate nohighlight">\(n=5\)</span>, the exponential MLE is highly skewed (skewness = 2.63), not normal. (b) <strong>High curvature</strong>: For <span class="math notranslate nohighlight">\(g(\theta) = \theta^3\)</span>, the large second derivative causes asymmetry the linear approximation misses. (c) <strong>Boundary effects</strong>: When <span class="math notranslate nohighlight">\(p = 0.02\)</span>, 13% of samples give <span class="math notranslate nohighlight">\(\hat{p} = 0\)</span>, truncating <span class="math notranslate nohighlight">\(\log(\hat{p})\)</span>. (d) <strong>Zero derivative</strong>: At <span class="math notranslate nohighlight">\(\theta_0 = 0\)</span> for <span class="math notranslate nohighlight">\(g(\theta) = \theta^2\)</span>, the delta method predicts <span class="math notranslate nohighlight">\(\text{Var} = 0\)</span>—completely wrong; the actual distribution is <span class="math notranslate nohighlight">\(\sigma^2 \chi^2_1\)</span>.</span><a class="headerlink" href="#id13" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="choosing-between-methods">
<h3>Choosing Between Methods<a class="headerlink" href="#choosing-between-methods" title="Link to this heading"></a></h3>
<div class="tip admonition">
<p class="admonition-title">Practical Guidance</p>
<p><strong>Use the delta method when</strong>:</p>
<ul class="simple">
<li><p>Sample size is moderate to large (<span class="math notranslate nohighlight">\(n &gt; 50\)</span> as a rough guide)</p></li>
<li><p>The transformation is smooth with bounded second derivative</p></li>
<li><p>Parameters are not near boundaries</p></li>
<li><p>You need quick, analytical results</p></li>
</ul>
<p><strong>Use sandwich standard errors when</strong>:</p>
<ul class="simple">
<li><p>The model may be misspecified</p></li>
<li><p>You want robustness without assuming correct specification</p></li>
<li><p>Working with quasi-likelihood or GEE methods</p></li>
</ul>
<p><strong>Consider profile likelihood when</strong>:</p>
<ul class="simple">
<li><p>You want intervals for a single parameter</p></li>
<li><p>The parameter may be near a boundary</p></li>
<li><p>You want transformation-invariant intervals</p></li>
</ul>
<p><strong>Chapter 4 introduces resampling methods</strong> (bootstrap, jackknife) that complement these analytical approaches—particularly useful for complex statistics or when analytical standard errors are intractable.</p>
</div>
</section>
</section>
<section id="bringing-it-all-together">
<h2>Bringing It All Together<a class="headerlink" href="#bringing-it-all-together" title="Link to this heading"></a></h2>
<p>The delta method and variance estimation form the bridge between point estimation and inference. Starting with an MLE <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> and its variance (from Fisher information, observed information, or sandwich estimator), we can:</p>
<ol class="arabic simple">
<li><p><strong>Transform parameters</strong> while propagating uncertainty correctly</p></li>
<li><p><strong>Construct confidence intervals</strong> for scientifically meaningful quantities</p></li>
<li><p><strong>Perform hypothesis tests</strong> about functions of parameters</p></li>
<li><p><strong>Quantify precision</strong> in predictions and derived quantities</p></li>
</ol>
<p>This machinery extends directly to <strong>regression models</strong> (Section 3.4), where we need standard errors and confidence intervals for linear combinations of coefficients, predictions at new covariate values, and nonlinear functions of fitted parameters. The <strong>generalized linear models</strong> (Section 3.5) rely heavily on delta method arguments for inference about the original scale (e.g., probabilities from logistic regression).</p>
<p><strong>Chapter 4</strong> introduces resampling methods (bootstrap, jackknife) that provide a complementary, computation-intensive approach—validating or replacing delta method approximations when analytical formulas are unavailable or unreliable.</p>
<div class="important admonition">
<p class="admonition-title">Key Takeaways 📝</p>
<ol class="arabic simple">
<li><p><strong>Delta method core result</strong>: <span class="math notranslate nohighlight">\(\text{Var}(g(\hat{\theta})) \approx [g'(\theta)]^2 \text{Var}(\hat{\theta})\)</span>—variance transforms like the square of the derivative.</p></li>
<li><p><strong>Multivariate extension</strong>: <span class="math notranslate nohighlight">\(\text{Var}(g(\hat{\boldsymbol{\theta}})) = (\nabla g)^\top \boldsymbol{\Sigma} (\nabla g)\)</span> handles correlated parameters and vector-valued functions.</p></li>
<li><p><strong>Plug-in principle</strong>: Replace unknown parameters with estimates; combine with delta method for variance of transformed estimates.</p></li>
<li><p><strong>Variance estimation hierarchy</strong>: Fisher information (theoretical, requires correct model), observed information (data-adaptive), sandwich (robust to misspecification).</p></li>
<li><p><strong>Know the limitations</strong>: Delta method fails for small samples, high curvature, boundary parameters, and zero derivatives—Chapter 4’s resampling methods address these cases.</p></li>
<li><p><strong>Learning outcomes</strong>: LO 1 (simulation and transformations), LO 2 (frequentist inference and uncertainty quantification).</p></li>
</ol>
</div>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Link to this heading"></a></h2>
<div class="exercise admonition">
<p class="admonition-title">Exercise 3.3.1: Basic Delta Method for Exponential Distribution</p>
<p>The exponential distribution is fundamental in survival analysis and queuing theory. Let <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} \text{Exponential}(\lambda)\)</span> where <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span> is the rate parameter. The MLE is <span class="math notranslate nohighlight">\(\hat{\lambda} = 1/\bar{X}\)</span>.</p>
<ol class="loweralpha">
<li><p><strong>Derive the asymptotic variance of</strong> <span class="math notranslate nohighlight">\(\hat{\lambda}\)</span>: Show that <span class="math notranslate nohighlight">\(\text{SE}(\hat{\lambda}) \approx \hat{\lambda}/\sqrt{n}\)</span> using the Fisher information.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Fisher Information for Exponential</p>
<p>The log-likelihood for one observation is <span class="math notranslate nohighlight">\(\ell(\lambda) = \log \lambda - \lambda x\)</span>. Compute <span class="math notranslate nohighlight">\(I(\lambda) = -\mathbb{E}[\partial^2 \ell / \partial \lambda^2]\)</span>. You should find <span class="math notranslate nohighlight">\(I(\lambda) = 1/\lambda^2\)</span>.</p>
</div>
</li>
<li><p><strong>Apply the delta method for the mean</strong>: The population mean is <span class="math notranslate nohighlight">\(\mu = 1/\lambda\)</span>. Find <span class="math notranslate nohighlight">\(\text{SE}(\hat{\mu})\)</span> where <span class="math notranslate nohighlight">\(\hat{\mu} = \bar{X}\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Transformation g(λ) = 1/λ</p>
<p>Use <span class="math notranslate nohighlight">\(g(\lambda) = 1/\lambda\)</span> with <span class="math notranslate nohighlight">\(g'(\lambda) = -1/\lambda^2\)</span>. Apply <span class="math notranslate nohighlight">\(\text{SE}(g(\hat{\lambda})) = |g'(\hat{\lambda})| \cdot \text{SE}(\hat{\lambda})\)</span>.</p>
</div>
</li>
<li><p><strong>Log transformation</strong>: Find <span class="math notranslate nohighlight">\(\text{SE}(\log \hat{\lambda})\)</span> using the delta method.</p></li>
<li><p><strong>Compare to exact results</strong>: The exact variance of <span class="math notranslate nohighlight">\(\bar{X}\)</span> is <span class="math notranslate nohighlight">\(\text{Var}(\bar{X}) = 1/(n\lambda^2)\)</span>. Compare your delta method result from (b) to this. When do they agree?</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Asymptotic Variance of λ̂</strong></p>
<div class="tip admonition">
<p class="admonition-title">Step 1: Fisher Information</p>
<p class="sd-card-text">For one observation <span class="math notranslate nohighlight">\(X \sim \text{Exp}(\lambda)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\ell(\lambda) = \log \lambda - \lambda X\]</div>
<p class="sd-card-text">First derivative (score):</p>
<div class="math notranslate nohighlight">
\[\frac{\partial \ell}{\partial \lambda} = \frac{1}{\lambda} - X\]</div>
<p class="sd-card-text">Second derivative:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial^2 \ell}{\partial \lambda^2} = -\frac{1}{\lambda^2}\]</div>
<p class="sd-card-text">Fisher information:</p>
<div class="math notranslate nohighlight">
\[I(\lambda) = -\mathbb{E}\left[\frac{\partial^2 \ell}{\partial \lambda^2}\right] = \frac{1}{\lambda^2}\]</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Step 2: Asymptotic Variance</p>
<p class="sd-card-text">By MLE asymptotics:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\hat{\lambda}) \approx \frac{1}{n I(\lambda)} = \frac{\lambda^2}{n}\]</div>
<p class="sd-card-text">Therefore:</p>
<div class="math notranslate nohighlight">
\[\text{SE}(\hat{\lambda}) \approx \frac{\lambda}{\sqrt{n}} \approx \frac{\hat{\lambda}}{\sqrt{n}}\]</div>
</div>
<p class="sd-card-text"><strong>Part (b): Delta Method for the Mean</strong></p>
<div class="tip admonition">
<p class="admonition-title">Step 3: Apply Delta Method</p>
<p class="sd-card-text">With <span class="math notranslate nohighlight">\(g(\lambda) = 1/\lambda = \mu\)</span>:</p>
<div class="math notranslate nohighlight">
\[g'(\lambda) = -\frac{1}{\lambda^2}\]</div>
<p class="sd-card-text">Delta method:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\hat{\mu}) = \text{Var}(g(\hat{\lambda})) \approx [g'(\lambda)]^2 \cdot \text{Var}(\hat{\lambda}) = \frac{1}{\lambda^4} \cdot \frac{\lambda^2}{n} = \frac{1}{n\lambda^2}\]</div>
<p class="sd-card-text">So <span class="math notranslate nohighlight">\(\text{SE}(\hat{\mu}) = \frac{1}{\sqrt{n}\lambda} = \frac{\mu}{\sqrt{n}}\)</span>.</p>
</div>
<p class="sd-card-text"><strong>Part (c): Log Transformation</strong></p>
<p class="sd-card-text">With <span class="math notranslate nohighlight">\(g(\lambda) = \log(\lambda)\)</span>:</p>
<div class="math notranslate nohighlight">
\[g'(\lambda) = \frac{1}{\lambda}\]</div>
<p class="sd-card-text">Delta method:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\log \hat{\lambda}) \approx \frac{1}{\lambda^2} \cdot \frac{\lambda^2}{n} = \frac{1}{n}\]</div>
<p class="sd-card-text">Therefore <span class="math notranslate nohighlight">\(\text{SE}(\log \hat{\lambda}) = 1/\sqrt{n}\)</span>, which is remarkably simple and doesn’t depend on <span class="math notranslate nohighlight">\(\lambda\)</span>!</p>
<p class="sd-card-text"><strong>Part (d): Comparison to Exact</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Simulation to verify</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">true_lambda</span> <span class="o">=</span> <span class="mf">2.0</span>
<span class="n">true_mu</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">true_lambda</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">n_sim</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="c1"># Simulate and compute estimates</span>
<span class="n">mu_hats</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">lambda_hats</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">true_lambda</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">mu_hats</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">lambda_hats</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">mu_hats</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mu_hats</span><span class="p">)</span>
<span class="n">lambda_hats</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">lambda_hats</span><span class="p">)</span>

<span class="c1"># Theoretical values</span>
<span class="n">se_mu_exact</span> <span class="o">=</span> <span class="n">true_mu</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>  <span class="c1"># Exact: sqrt(Var(X̄)) = μ/√n</span>
<span class="n">se_mu_delta</span> <span class="o">=</span> <span class="n">true_mu</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>  <span class="c1"># Delta method: same!</span>

<span class="n">se_lambda_theory</span> <span class="o">=</span> <span class="n">true_lambda</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">se_log_lambda_theory</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;EXPONENTIAL DISTRIBUTION: DELTA METHOD VERIFICATION&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">True λ = </span><span class="si">{</span><span class="n">true_lambda</span><span class="si">}</span><span class="s2">, True μ = 1/λ = </span><span class="si">{</span><span class="n">true_mu</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample size n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">, Simulations = </span><span class="si">{</span><span class="n">n_sim</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">For λ̂ = 1/X̄:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Empirical SE(λ̂):    </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">lambda_hats</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Theoretical SE(λ̂):  </span><span class="si">{</span><span class="n">se_lambda_theory</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">For μ̂ = X̄:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Empirical SE(μ̂):    </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">mu_hats</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Exact SE(μ̂):        </span><span class="si">{</span><span class="n">se_mu_exact</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Delta method SE(μ̂): </span><span class="si">{</span><span class="n">se_mu_delta</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  → Exact and delta method AGREE!&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">For log(λ̂):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Empirical SE(log λ̂): </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">lambda_hats</span><span class="p">))</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Theory SE(log λ̂):    </span><span class="si">{</span><span class="n">se_log_lambda_theory</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>============================================================
EXPONENTIAL DISTRIBUTION: DELTA METHOD VERIFICATION
============================================================

True λ = 2.0, True μ = 1/λ = 0.5
Sample size n = 100, Simulations = 10000

For λ̂ = 1/X̄:
  Empirical SE(λ̂):    0.204132
  Theoretical SE(λ̂):  0.200000

For μ̂ = X̄:
  Empirical SE(μ̂):    0.049856
  Exact SE(μ̂):        0.050000
  Delta method SE(μ̂): 0.050000
  → Exact and delta method AGREE!

For log(λ̂):
  Empirical SE(log λ̂): 0.100234
  Theory SE(log λ̂):    0.100000
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Observations:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Exact agreement</strong>: The delta method SE for <span class="math notranslate nohighlight">\(\hat{\mu} = \bar{X}\)</span> equals the exact SE. This is because <span class="math notranslate nohighlight">\(\bar{X}\)</span> is already a sample mean with known variance—no approximation needed!</p></li>
<li><p class="sd-card-text"><strong>Log transformation</strong>: SE of <span class="math notranslate nohighlight">\(\log \hat{\lambda}\)</span> is <span class="math notranslate nohighlight">\(1/\sqrt{n}\)</span> regardless of <span class="math notranslate nohighlight">\(\lambda\)</span>. This variance-stabilizing property makes log-scale inference simpler.</p></li>
<li><p class="sd-card-text"><strong>The delta method for</strong> <span class="math notranslate nohighlight">\(\hat{\lambda}\)</span>: Here we do need the approximation. The empirical SE (0.204) is close to but not exactly the theoretical (0.200) because <span class="math notranslate nohighlight">\(\hat{\lambda} = 1/\bar{X}\)</span> has a slightly skewed distribution for finite <span class="math notranslate nohighlight">\(n\)</span>.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 3.3.2: Multivariate Delta Method for Ratio of Means</p>
<p>The ratio of two means arises in bioequivalence testing, relative efficiency comparisons, and economics. Let <span class="math notranslate nohighlight">\(X_1, \ldots, X_m \stackrel{\text{iid}}{\sim} \mathcal{N}(\mu_X, \sigma^2)\)</span> and <span class="math notranslate nohighlight">\(Y_1, \ldots, Y_n \stackrel{\text{iid}}{\sim} \mathcal{N}(\mu_Y, \sigma^2)\)</span> be independent samples.</p>
<ol class="loweralpha">
<li><p><strong>Derive the variance</strong>: Using the multivariate delta method, derive <span class="math notranslate nohighlight">\(\text{Var}(\bar{X}/\bar{Y})\)</span> in terms of <span class="math notranslate nohighlight">\(\mu_X, \mu_Y, \sigma^2, m, n\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Set Up the Gradient</p>
<p>Let <span class="math notranslate nohighlight">\(\boldsymbol{\theta} = (\mu_X, \mu_Y)^\top\)</span> with <span class="math notranslate nohighlight">\(g(\mu_X, \mu_Y) = \mu_X/\mu_Y\)</span>. Then:</p>
<div class="math notranslate nohighlight">
\[\nabla g = \left(\frac{\partial g}{\partial \mu_X}, \frac{\partial g}{\partial \mu_Y}\right)^\top = \left(\frac{1}{\mu_Y}, -\frac{\mu_X}{\mu_Y^2}\right)^\top\]</div>
<p>The covariance matrix of <span class="math notranslate nohighlight">\((\bar{X}, \bar{Y})\)</span> is diagonal since the samples are independent.</p>
</div>
</li>
<li><p><strong>Simplify for equal sample sizes</strong>: What is the variance when <span class="math notranslate nohighlight">\(m = n\)</span>?</p></li>
<li><p><strong>Boundary behavior</strong>: What happens to <span class="math notranslate nohighlight">\(\text{Var}(\bar{X}/\bar{Y})\)</span> as <span class="math notranslate nohighlight">\(\mu_Y \to 0\)</span>? Interpret this.</p></li>
<li><p><strong>Simulation verification</strong>: Verify your formula with <span class="math notranslate nohighlight">\(\mu_X = 10\)</span>, <span class="math notranslate nohighlight">\(\mu_Y = 5\)</span>, <span class="math notranslate nohighlight">\(\sigma = 2\)</span>, <span class="math notranslate nohighlight">\(m = n = 30\)</span>.</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Deriving the Variance</strong></p>
<div class="tip admonition">
<p class="admonition-title">Step 1: Setup</p>
<ul class="simple">
<li><p class="sd-card-text">Estimators: <span class="math notranslate nohighlight">\(\hat{\mu}_X = \bar{X}\)</span>, <span class="math notranslate nohighlight">\(\hat{\mu}_Y = \bar{Y}\)</span></p></li>
<li><p class="sd-card-text">Function: <span class="math notranslate nohighlight">\(g(\mu_X, \mu_Y) = \mu_X / \mu_Y\)</span></p></li>
<li><p class="sd-card-text">Covariance matrix (independent samples):</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\boldsymbol{\Sigma} = \begin{pmatrix} \sigma^2/m &amp; 0 \\ 0 &amp; \sigma^2/n \end{pmatrix}\end{split}\]</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Step 2: Compute Gradient</p>
<div class="math notranslate nohighlight">
\[\begin{split}\nabla g = \begin{pmatrix} 1/\mu_Y \\ -\mu_X/\mu_Y^2 \end{pmatrix}\end{split}\]</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Step 3: Apply Multivariate Delta Method</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{Var}\left(\frac{\bar{X}}{\bar{Y}}\right) &amp;\approx (\nabla g)^\top \boldsymbol{\Sigma} (\nabla g) \\
&amp;= \frac{1}{\mu_Y^2} \cdot \frac{\sigma^2}{m} + \frac{\mu_X^2}{\mu_Y^4} \cdot \frac{\sigma^2}{n} \\
&amp;= \frac{\sigma^2}{\mu_Y^2}\left(\frac{1}{m} + \frac{\mu_X^2}{\mu_Y^2 n}\right)\end{split}\]</div>
<p class="sd-card-text">Writing <span class="math notranslate nohighlight">\(R = \mu_X/\mu_Y\)</span> (the true ratio):</p>
<div class="math notranslate nohighlight">
\[\text{Var}\left(\frac{\bar{X}}{\bar{Y}}\right) \approx \frac{\sigma^2}{\mu_Y^2}\left(\frac{1}{m} + \frac{R^2}{n}\right)\]</div>
</div>
<p class="sd-card-text"><strong>Part (b): Equal Sample Sizes</strong></p>
<p class="sd-card-text">When <span class="math notranslate nohighlight">\(m = n\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{Var}\left(\frac{\bar{X}}{\bar{Y}}\right) \approx \frac{\sigma^2}{\mu_Y^2} \cdot \frac{1 + R^2}{n} = \frac{\sigma^2(1 + R^2)}{n\mu_Y^2}\]</div>
<p class="sd-card-text"><strong>Note on equal variances</strong>: This derivation assumes <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> have the same variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>. If variances differ (<span class="math notranslate nohighlight">\(\text{Var}(X_i) = \sigma_X^2\)</span>, <span class="math notranslate nohighlight">\(\text{Var}(Y_j) = \sigma_Y^2\)</span>), replace the covariance matrix with <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma} = \text{diag}(\sigma_X^2/m, \sigma_Y^2/n)\)</span>, giving:</p>
<div class="math notranslate nohighlight">
\[\text{Var}\left(\frac{\bar{X}}{\bar{Y}}\right) \approx \frac{\sigma_X^2}{m\mu_Y^2} + \frac{\mu_X^2 \sigma_Y^2}{n\mu_Y^4}\]</div>
<p class="sd-card-text"><strong>Part (c): Boundary Behavior</strong></p>
<p class="sd-card-text">As <span class="math notranslate nohighlight">\(\mu_Y \to 0\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{Var}\left(\frac{\bar{X}}{\bar{Y}}\right) \propto \frac{1}{\mu_Y^2} \to \infty\]</div>
<p class="sd-card-text"><strong>Interpretation</strong>: When the denominator is near zero, small fluctuations in <span class="math notranslate nohighlight">\(\bar{Y}\)</span> cause huge swings in the ratio. The ratio becomes increasingly unstable. This is the well-known problem with ratio estimators when the denominator can be small.</p>
<p class="sd-card-text"><strong>Part (d): Simulation</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Parameters</span>
<span class="n">mu_X</span><span class="p">,</span> <span class="n">mu_Y</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">R</span> <span class="o">=</span> <span class="n">mu_X</span> <span class="o">/</span> <span class="n">mu_Y</span>
<span class="n">n_sim</span> <span class="o">=</span> <span class="mi">50000</span>

<span class="c1"># Theoretical variance</span>
<span class="n">var_theory</span> <span class="o">=</span> <span class="p">(</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">mu_Y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span> <span class="o">+</span> <span class="n">R</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="n">n</span><span class="p">)</span>
<span class="n">se_theory</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_theory</span><span class="p">)</span>

<span class="c1"># Simulation</span>
<span class="n">ratios</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu_X</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu_Y</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">ratios</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span>

<span class="n">ratios</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ratios</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RATIO OF MEANS: MULTIVARIATE DELTA METHOD&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Parameters: μ_X=</span><span class="si">{</span><span class="n">mu_X</span><span class="si">}</span><span class="s2">, μ_Y=</span><span class="si">{</span><span class="n">mu_Y</span><span class="si">}</span><span class="s2">, σ=</span><span class="si">{</span><span class="n">sigma</span><span class="si">}</span><span class="s2">, m=n=</span><span class="si">{</span><span class="n">m</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True ratio R = μ_X/μ_Y = </span><span class="si">{</span><span class="n">R</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Variance of X̄/Ȳ:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Theoretical (delta): </span><span class="si">{</span><span class="n">var_theory</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Empirical:           </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">ratios</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Standard Error:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Theoretical: </span><span class="si">{</span><span class="n">se_theory</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Empirical:   </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ratios</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Mean of ratios:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  True R:      </span><span class="si">{</span><span class="n">R</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Sample mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">ratios</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 95% CI coverage</span>
<span class="n">ci_lower</span> <span class="o">=</span> <span class="n">R</span> <span class="o">-</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">se_theory</span>
<span class="n">ci_upper</span> <span class="o">=</span> <span class="n">R</span> <span class="o">+</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">se_theory</span>
<span class="n">coverage</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">ratios</span> <span class="o">&gt;=</span> <span class="n">ci_lower</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">ratios</span> <span class="o">&lt;=</span> <span class="n">ci_upper</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">95% CI Coverage (should be ~0.95): </span><span class="si">{</span><span class="n">coverage</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>============================================================
RATIO OF MEANS: MULTIVARIATE DELTA METHOD
============================================================

Parameters: μ_X=10, μ_Y=5, σ=2, m=n=30
True ratio R = μ_X/μ_Y = 2.0

Variance of X̄/Ȳ:
  Theoretical (delta): 0.026667
  Empirical:           0.027021

Standard Error:
  Theoretical: 0.1633
  Empirical:   0.1644

Mean of ratios:
  True R:      2.0000
  Sample mean: 2.0030

95% CI Coverage (should be ~0.95): 0.9481
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Observations:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Excellent agreement</strong>: Theory matches simulation within sampling error.</p></li>
<li><p class="sd-card-text"><strong>Near-nominal coverage</strong>: The 95% CI achieves 94.8% coverage, close to the target.</p></li>
<li><p class="sd-card-text"><strong>Slight undercoverage</strong>: The delta method approximation is slightly optimistic because the ratio distribution is skewed when <span class="math notranslate nohighlight">\(n\)</span> is moderate and <span class="math notranslate nohighlight">\(\mu_Y\)</span> isn’t large relative to <span class="math notranslate nohighlight">\(\sigma\)</span>.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 3.3.3: Fisher’s z-Transformation</p>
<p>Fisher’s z-transformation is a classic example of variance stabilization. For the sample correlation <span class="math notranslate nohighlight">\(r\)</span>, Fisher proposed <span class="math notranslate nohighlight">\(z = \text{arctanh}(r) = \frac{1}{2}\log\frac{1+r}{1-r}\)</span>.</p>
<ol class="loweralpha">
<li><p><strong>Variance of</strong> <span class="math notranslate nohighlight">\(r\)</span>: Show that <span class="math notranslate nohighlight">\(\text{Var}(r) \approx (1-\rho^2)^2/n\)</span> for the sample correlation from bivariate normal data.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Known Result</p>
<p>This is a standard result. The exact variance involves fourth moments, but the leading term is <span class="math notranslate nohighlight">\((1-\rho^2)^2/n\)</span>. Just cite or state this result.</p>
</div>
</li>
<li><p><strong>Apply delta method</strong>: Compute <span class="math notranslate nohighlight">\(\text{Var}(z)\)</span> where <span class="math notranslate nohighlight">\(z = \text{arctanh}(r)\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Derivative of arctanh</p>
<p>Recall that <span class="math notranslate nohighlight">\(\frac{d}{dx}\text{arctanh}(x) = \frac{1}{1-x^2}\)</span>.</p>
</div>
</li>
<li><p><strong>Verify variance stabilization</strong>: Show that <span class="math notranslate nohighlight">\(\text{Var}(z) \approx 1/(n-3)\)</span> is nearly constant in <span class="math notranslate nohighlight">\(\rho\)</span>. Why is this useful?</p></li>
<li><p><strong>Construct a CI</strong>: For <span class="math notranslate nohighlight">\(r = 0.7\)</span> and <span class="math notranslate nohighlight">\(n = 50\)</span>, construct a 95% CI for <span class="math notranslate nohighlight">\(\rho\)</span>.</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Variance of r</strong></p>
<p class="sd-card-text">For bivariate normal data, the asymptotic variance of the sample correlation is:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(r) \approx \frac{(1-\rho^2)^2}{n}\]</div>
<p class="sd-card-text">This result comes from the asymptotic theory of maximum likelihood estimators. The correlation <span class="math notranslate nohighlight">\(r\)</span> is the MLE of <span class="math notranslate nohighlight">\(\rho\)</span>, and the Fisher information determines the variance.</p>
<p class="sd-card-text"><strong>Part (b): Delta Method for z</strong></p>
<div class="tip admonition">
<p class="admonition-title">Step 1: Compute Derivative</p>
<div class="math notranslate nohighlight">
\[g(\rho) = \text{arctanh}(\rho) = \frac{1}{2}\log\frac{1+\rho}{1-\rho}\]</div>
<div class="math notranslate nohighlight">
\[g'(\rho) = \frac{1}{1-\rho^2}\]</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Step 2: Apply Delta Method</p>
<div class="math notranslate nohighlight">
\[\text{Var}(z) \approx [g'(\rho)]^2 \cdot \text{Var}(r) = \frac{1}{(1-\rho^2)^2} \cdot \frac{(1-\rho^2)^2}{n} = \frac{1}{n}\]</div>
</div>
<p class="sd-card-text"><strong>Part (c): Variance Stabilization</strong></p>
<p class="sd-card-text">The variance of <span class="math notranslate nohighlight">\(z\)</span> is approximately <span class="math notranslate nohighlight">\(1/n\)</span>—<strong>it doesn’t depend on ρ</strong>!</p>
<p class="sd-card-text">A more refined approximation gives <span class="math notranslate nohighlight">\(\text{Var}(z) \approx 1/(n-3)\)</span>, which accounts for the small-sample bias.</p>
<p class="sd-card-text"><strong>Why this is useful:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Simple inference</strong>: CI width is the same regardless of the true <span class="math notranslate nohighlight">\(\rho\)</span></p></li>
<li><p class="sd-card-text"><strong>No need to estimate ρ</strong> to compute the SE</p></li>
<li><p class="sd-card-text"><strong>Symmetric distribution</strong>: <span class="math notranslate nohighlight">\(z\)</span> is approximately normal for all <span class="math notranslate nohighlight">\(\rho\)</span>, unlike <span class="math notranslate nohighlight">\(r\)</span> which is skewed near <span class="math notranslate nohighlight">\(\pm 1\)</span></p></li>
</ol>
<p class="sd-card-text"><strong>Part (d): Confidence Interval</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="n">r</span> <span class="o">=</span> <span class="mf">0.7</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">50</span>

<span class="c1"># Transform to z</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arctanh</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>

<span class="c1"># SE of z</span>
<span class="n">se_z</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span>

<span class="c1"># 95% CI for ζ = arctanh(ρ)</span>
<span class="n">z_crit</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.975</span><span class="p">)</span>
<span class="n">z_lower</span> <span class="o">=</span> <span class="n">z</span> <span class="o">-</span> <span class="n">z_crit</span> <span class="o">*</span> <span class="n">se_z</span>
<span class="n">z_upper</span> <span class="o">=</span> <span class="n">z</span> <span class="o">+</span> <span class="n">z_crit</span> <span class="o">*</span> <span class="n">se_z</span>

<span class="c1"># Back-transform to ρ</span>
<span class="n">rho_lower</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">z_lower</span><span class="p">)</span>
<span class="n">rho_upper</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">z_upper</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;FISHER&#39;S Z-TRANSFORMATION: CORRELATION CI&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Observed: r = </span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="s2">, n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Fisher z-transform:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  z = arctanh(r) = </span><span class="si">{</span><span class="n">z</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  SE(z) = 1/√(n-3) = </span><span class="si">{</span><span class="n">se_z</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">95% CI for z:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  (</span><span class="si">{</span><span class="n">z_lower</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">z_upper</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">95% CI for ρ (back-transformed):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  (</span><span class="si">{</span><span class="n">rho_lower</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">rho_upper</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># Verify via simulation</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">true_rho</span> <span class="o">=</span> <span class="mf">0.7</span>
<span class="n">n_sim</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="n">r_samples</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
    <span class="c1"># Generate bivariate normal with correlation rho</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="n">true_rho</span><span class="p">],</span> <span class="p">[</span><span class="n">true_rho</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">cov</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">r_samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">r_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">r_samples</span><span class="p">)</span>
<span class="n">z_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arctanh</span><span class="p">(</span><span class="n">r_samples</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Simulation verification (true ρ = </span><span class="si">{</span><span class="n">true_rho</span><span class="si">}</span><span class="s2">):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Empirical Var(r): </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">r_samples</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Theory Var(r):    </span><span class="si">{</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">true_rho</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="n">n</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Empirical Var(z): </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">z_samples</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Theory Var(z):    </span><span class="si">{</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>============================================================
FISHER&#39;S Z-TRANSFORMATION: CORRELATION CI
============================================================

Observed: r = 0.7, n = 50

Fisher z-transform:
  z = arctanh(r) = 0.8673
  SE(z) = 1/√(n-3) = 0.1459

95% CI for z:
  (0.5814, 1.1533)

95% CI for ρ (back-transformed):
  (0.5231, 0.8180)

Simulation verification (true ρ = 0.7):
  Empirical Var(r): 0.005734
  Theory Var(r):    0.005202
  Empirical Var(z): 0.021692
  Theory Var(z):    0.021277
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Observations:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Variance stabilization works</strong>: Var(z) ≈ 1/(n-3) = 0.0213, matching simulation.</p></li>
<li><p class="sd-card-text"><strong>Asymmetric CI for ρ</strong>: The CI (0.52, 0.82) is asymmetric around r = 0.7, correctly reflecting that ρ is bounded by 1.</p></li>
<li><p class="sd-card-text"><strong>Interpretation</strong>: We’re 95% confident the true correlation is between 0.52 and 0.82.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 3.3.4: Vaccine Efficacy—Delta Method vs Log-Scale</p>
<p>Vaccine efficacy <span class="math notranslate nohighlight">\(\text{VE} = 1 - p_v/p_p\)</span> is a ratio-based measure. This exercise compares two approaches to inference.</p>
<ol class="loweralpha simple">
<li><p><strong>Direct delta method</strong>: Derive <span class="math notranslate nohighlight">\(\text{SE}(\widehat{\text{VE}})\)</span> directly using the multivariate delta method on <span class="math notranslate nohighlight">\(g(p_v, p_p) = 1 - p_v/p_p\)</span>.</p></li>
<li><p><strong>Log-scale approach</strong>: Derive <span class="math notranslate nohighlight">\(\text{SE}(\log \widehat{\text{RR}})\)</span> where <span class="math notranslate nohighlight">\(\text{RR} = p_v/p_p\)</span>, then describe how to construct a CI for VE.</p></li>
<li><p><strong>Numerical comparison</strong>: Using <span class="math notranslate nohighlight">\(n_v = n_p = 20000\)</span>, <span class="math notranslate nohighlight">\(x_v = 8\)</span>, <span class="math notranslate nohighlight">\(x_p = 162\)</span>, compute 95% CIs both ways.</p></li>
<li><p><strong>Coverage simulation</strong>: Simulate coverage when true VE = 0.95 with <span class="math notranslate nohighlight">\(n = 20000\)</span> per group and infection rate in placebo group of 1%.</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Direct Delta Method</strong></p>
<div class="tip admonition">
<p class="admonition-title">Step 1: Setup</p>
<ul class="simple">
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(g(p_v, p_p) = 1 - p_v/p_p\)</span></p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(\nabla g = (-1/p_p, \; p_v/p_p^2)^\top\)</span></p></li>
<li><p class="sd-card-text">Independent samples: <span class="math notranslate nohighlight">\(\text{Var}(\hat{p}_v) = p_v(1-p_v)/n_v\)</span>, <span class="math notranslate nohighlight">\(\text{Var}(\hat{p}_p) = p_p(1-p_p)/n_p\)</span></p></li>
</ul>
</div>
<div class="tip admonition">
<p class="admonition-title">Step 2: Variance Formula</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\widehat{\text{VE}}) \approx \frac{1}{p_p^2}\text{Var}(\hat{p}_v) + \frac{p_v^2}{p_p^4}\text{Var}(\hat{p}_p)\]</div>
<div class="math notranslate nohighlight">
\[= \frac{p_v(1-p_v)}{n_v p_p^2} + \frac{p_v^2 \cdot p_p(1-p_p)}{n_p p_p^4}\]</div>
</div>
<p class="sd-card-text"><strong>Part (b): Log-Scale Approach</strong></p>
<p class="sd-card-text">Work with <span class="math notranslate nohighlight">\(\log(\text{RR}) = \log(p_v) - \log(p_p)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\log \widehat{\text{RR}}) = \frac{1-p_v}{n_v p_v} + \frac{1-p_p}{n_p p_p}\]</div>
<p class="sd-card-text">This simplifies beautifully because the variance of <span class="math notranslate nohighlight">\(\log(\hat{p})\)</span> is approximately <span class="math notranslate nohighlight">\((1-p)/(np)\)</span>.</p>
<p class="sd-card-text"><strong>To get CI for VE:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text">Compute CI for <span class="math notranslate nohighlight">\(\log(\text{RR})\)</span>: <span class="math notranslate nohighlight">\(\log(\widehat{\text{RR}}) \pm z_{\alpha/2} \cdot \text{SE}(\log \widehat{\text{RR}})\)</span></p></li>
<li><p class="sd-card-text">Exponentiate to get CI for RR: <span class="math notranslate nohighlight">\((\text{RR}_L, \text{RR}_U)\)</span></p></li>
<li><p class="sd-card-text">Transform to VE: <span class="math notranslate nohighlight">\((1 - \text{RR}_U, 1 - \text{RR}_L)\)</span></p></li>
</ol>
<p class="sd-card-text"><strong>Part (c) &amp; (d): Implementation</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">ve_confidence_intervals</span><span class="p">(</span><span class="n">x_v</span><span class="p">,</span> <span class="n">n_v</span><span class="p">,</span> <span class="n">x_p</span><span class="p">,</span> <span class="n">n_p</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute VE confidence intervals via two methods.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p_v</span> <span class="o">=</span> <span class="n">x_v</span> <span class="o">/</span> <span class="n">n_v</span>
    <span class="n">p_p</span> <span class="o">=</span> <span class="n">x_p</span> <span class="o">/</span> <span class="n">n_p</span>
    <span class="n">RR</span> <span class="o">=</span> <span class="n">p_v</span> <span class="o">/</span> <span class="n">p_p</span>
    <span class="n">VE</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">RR</span>

    <span class="n">z</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Method 1: Direct delta method on VE scale</span>
    <span class="n">var_p_v</span> <span class="o">=</span> <span class="n">p_v</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_v</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_v</span>
    <span class="n">var_p_p</span> <span class="o">=</span> <span class="n">p_p</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_p</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_p</span>
    <span class="n">var_VE</span> <span class="o">=</span> <span class="n">var_p_v</span> <span class="o">/</span> <span class="n">p_p</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">p_v</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">p_p</span><span class="o">**</span><span class="mi">4</span><span class="p">)</span> <span class="o">*</span> <span class="n">var_p_p</span>
    <span class="n">se_VE</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_VE</span><span class="p">)</span>
    <span class="n">ci_direct</span> <span class="o">=</span> <span class="p">(</span><span class="n">VE</span> <span class="o">-</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se_VE</span><span class="p">,</span> <span class="n">VE</span> <span class="o">+</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se_VE</span><span class="p">)</span>

    <span class="c1"># Method 2: Log-scale</span>
    <span class="n">var_log_RR</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_v</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_v</span> <span class="o">*</span> <span class="n">p_v</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_p</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_p</span> <span class="o">*</span> <span class="n">p_p</span><span class="p">)</span>
    <span class="n">se_log_RR</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_log_RR</span><span class="p">)</span>
    <span class="n">log_RR</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">RR</span><span class="p">)</span>
    <span class="n">ci_log_RR</span> <span class="o">=</span> <span class="p">(</span><span class="n">log_RR</span> <span class="o">-</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se_log_RR</span><span class="p">,</span> <span class="n">log_RR</span> <span class="o">+</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se_log_RR</span><span class="p">)</span>
    <span class="n">ci_log</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">ci_log_RR</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">ci_log_RR</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;VE&#39;</span><span class="p">:</span> <span class="n">VE</span><span class="p">,</span> <span class="s1">&#39;RR&#39;</span><span class="p">:</span> <span class="n">RR</span><span class="p">,</span>
        <span class="s1">&#39;se_direct&#39;</span><span class="p">:</span> <span class="n">se_VE</span><span class="p">,</span> <span class="s1">&#39;ci_direct&#39;</span><span class="p">:</span> <span class="n">ci_direct</span><span class="p">,</span>
        <span class="s1">&#39;se_log_RR&#39;</span><span class="p">:</span> <span class="n">se_log_RR</span><span class="p">,</span> <span class="s1">&#39;ci_log&#39;</span><span class="p">:</span> <span class="n">ci_log</span>
    <span class="p">}</span>

<span class="c1"># Part (c): Numerical comparison</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">ve_confidence_intervals</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">20000</span><span class="p">,</span> <span class="mi">162</span><span class="p">,</span> <span class="mi">20000</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;VACCINE EFFICACY: COMPARING CI METHODS&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Data: x_v=8, n_v=20000, x_p=162, n_p=20000&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Point estimates: RR = </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;RR&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, VE = </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;VE&#39;</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Method 1: Direct Delta Method on VE Scale&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  SE(VE) = </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;se_direct&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  95% CI: (</span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci_direct&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%, </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci_direct&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Method 2: Log-Scale (Preferred)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  SE(log RR) = </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;se_log_RR&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  95% CI: (</span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci_log&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%, </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci_log&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>

<span class="c1"># Part (d): Coverage simulation</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">true_VE</span> <span class="o">=</span> <span class="mf">0.95</span>
<span class="n">p_p_true</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c1"># 1% infection rate in placebo</span>
<span class="n">p_v_true</span> <span class="o">=</span> <span class="n">p_p_true</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">true_VE</span><span class="p">)</span>  <span class="c1"># = 0.0005</span>

<span class="n">n_per_group</span> <span class="o">=</span> <span class="mi">20000</span>
<span class="n">n_sim</span> <span class="o">=</span> <span class="mi">5000</span>

<span class="n">cover_direct</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">cover_log</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
    <span class="n">x_v</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n_per_group</span><span class="p">,</span> <span class="n">p_v_true</span><span class="p">)</span>
    <span class="n">x_p</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n_per_group</span><span class="p">,</span> <span class="n">p_p_true</span><span class="p">)</span>

    <span class="c1"># Handle edge cases</span>
    <span class="k">if</span> <span class="n">x_v</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">x_v</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># continuity correction</span>
    <span class="k">if</span> <span class="n">x_p</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">continue</span>

    <span class="n">res</span> <span class="o">=</span> <span class="n">ve_confidence_intervals</span><span class="p">(</span><span class="n">x_v</span><span class="p">,</span> <span class="n">n_per_group</span><span class="p">,</span> <span class="n">x_p</span><span class="p">,</span> <span class="n">n_per_group</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">res</span><span class="p">[</span><span class="s1">&#39;ci_direct&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">true_VE</span> <span class="o">&lt;=</span> <span class="n">res</span><span class="p">[</span><span class="s1">&#39;ci_direct&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">cover_direct</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">res</span><span class="p">[</span><span class="s1">&#39;ci_log&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">true_VE</span> <span class="o">&lt;=</span> <span class="n">res</span><span class="p">[</span><span class="s1">&#39;ci_log&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">cover_log</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;COVERAGE SIMULATION (True VE = 95%)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Simulations: </span><span class="si">{</span><span class="n">n_sim</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Direct method coverage: </span><span class="si">{</span><span class="n">cover_direct</span><span class="o">/</span><span class="n">n_sim</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Log-scale coverage:     </span><span class="si">{</span><span class="n">cover_log</span><span class="o">/</span><span class="n">n_sim</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Log-scale is preferred because:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  1. Better coverage near boundaries (VE close to 1)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  2. Respects natural constraints (VE ≤ 1)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  3. More symmetric on transformed scale&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>============================================================
VACCINE EFFICACY: COMPARING CI METHODS
============================================================

Data: x_v=8, n_v=20000, x_p=162, n_p=20000
Point estimates: RR = 0.0494, VE = 95.1%

Method 1: Direct Delta Method on VE Scale
  SE(VE) = 0.0175
  95% CI: (91.6%, 98.5%)

Method 2: Log-Scale (Preferred)
  SE(log RR) = 0.3647
  95% CI: (90.0%, 97.6%)

============================================================
COVERAGE SIMULATION (True VE = 95%)
============================================================
Simulations: 5000
Direct method coverage: 0.943
Log-scale coverage:     0.952

Log-scale is preferred because:
  1. Better coverage near boundaries (VE close to 1)
  2. Respects natural constraints (VE ≤ 1)
  3. More symmetric on transformed scale
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Observations:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Both methods give similar CIs</strong> for this data, but log-scale achieves better coverage.</p></li>
<li><p class="sd-card-text"><strong>Log-scale is preferred</strong> when VE is high because the direct method can give CIs &gt; 100%.</p></li>
<li><p class="sd-card-text"><strong>Coverage</strong>: Log-scale achieves near-nominal 95.2% vs 94.3% for direct method.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 3.3.5: Sandwich Variance Under Misspecification</p>
<p>The sandwich estimator provides valid inference even when the model is wrong. This exercise explores this property.</p>
<ol class="loweralpha simple">
<li><p><strong>Setup</strong>: Consider fitting a <span class="math notranslate nohighlight">\(\mathcal{N}(\mu, 1)\)</span> model (with known <span class="math notranslate nohighlight">\(\sigma^2 = 1\)</span>) when the true distribution is <span class="math notranslate nohighlight">\(t_5\)</span> (variance = 5/3). Show that <span class="math notranslate nohighlight">\(\hat{\mu} = \bar{X}\)</span> is still the MLE.</p></li>
<li><p><strong>Three standard errors</strong>: For <span class="math notranslate nohighlight">\(n = 50\)</span> and data from <span class="math notranslate nohighlight">\(t_5\)</span>, compute:
- Model-based SE assuming <span class="math notranslate nohighlight">\(\sigma^2 = 1\)</span>: <span class="math notranslate nohighlight">\(1/\sqrt{n}\)</span>
- Sandwich SE using empirical variance
- True SE from the sampling distribution</p></li>
<li><p><strong>Coverage simulation</strong>: Simulate 10,000 datasets from <span class="math notranslate nohighlight">\(t_3\)</span> (variance = 3). Compare coverage of 95% CIs using model-based vs sandwich SEs.</p></li>
<li><p><strong>Interpretation</strong>: Explain why the sandwich estimator works under misspecification.</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): MLE is Still X̄</strong></p>
<p class="sd-card-text">For any distribution with finite mean, the sample mean minimizes the sum of squared deviations. When we fit <span class="math notranslate nohighlight">\(\mathcal{N}(\mu, 1)\)</span>, the log-likelihood is:</p>
<div class="math notranslate nohighlight">
\[\ell(\mu) = -\frac{n}{2}\log(2\pi) - \frac{1}{2}\sum_{i=1}^n (X_i - \mu)^2\]</div>
<p class="sd-card-text">Maximizing over <span class="math notranslate nohighlight">\(\mu\)</span> gives <span class="math notranslate nohighlight">\(\hat{\mu} = \bar{X}\)</span>, regardless of the true distribution.</p>
<p class="sd-card-text"><strong>Parts (b)-(d): Implementation</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Part (b): Three standard errors for t_5 data</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">df</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">true_var</span> <span class="o">=</span> <span class="n">df</span> <span class="o">/</span> <span class="p">(</span><span class="n">df</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Var(t_5) = 5/3</span>

<span class="c1"># Single dataset for illustration</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_t</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">mu_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">se_model</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>  <span class="c1"># Assumes σ² = 1</span>
<span class="n">se_sandwich</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>  <span class="c1"># Uses sample variance</span>
<span class="n">se_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">true_var</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span>  <span class="c1"># True SE</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;SANDWICH VARIANCE: MODEL MISSPECIFICATION&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Part (b): Single dataset from t_5 (n=</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True variance of t_5: </span><span class="si">{</span><span class="n">true_var</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Three SE estimates:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Model-based (assumes σ²=1): </span><span class="si">{</span><span class="n">se_model</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Sandwich (uses s²):         </span><span class="si">{</span><span class="n">se_sandwich</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  True SE:                    </span><span class="si">{</span><span class="n">se_true</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Part (c): Coverage simulation for t_3</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Part (c): Coverage simulation with t_3 data&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>

<span class="n">df_severe</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">true_var_t3</span> <span class="o">=</span> <span class="n">df_severe</span> <span class="o">/</span> <span class="p">(</span><span class="n">df_severe</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># = 3</span>
<span class="n">n_sim</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="n">model_covers</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">sandwich_covers</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">true_mu</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_t</span><span class="p">(</span><span class="n">df_severe</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">mu_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Model-based CI (wrong)</span>
    <span class="n">se_model</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">ci_model</span> <span class="o">=</span> <span class="p">(</span><span class="n">mu_hat</span> <span class="o">-</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">se_model</span><span class="p">,</span> <span class="n">mu_hat</span> <span class="o">+</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">se_model</span><span class="p">)</span>

    <span class="c1"># Sandwich CI (correct)</span>
    <span class="n">se_sandwich</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">ci_sandwich</span> <span class="o">=</span> <span class="p">(</span><span class="n">mu_hat</span> <span class="o">-</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">se_sandwich</span><span class="p">,</span> <span class="n">mu_hat</span> <span class="o">+</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">se_sandwich</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">ci_model</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">true_mu</span> <span class="o">&lt;=</span> <span class="n">ci_model</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">model_covers</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">ci_sandwich</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">true_mu</span> <span class="o">&lt;=</span> <span class="n">ci_sandwich</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">sandwich_covers</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">True variance of t_3: </span><span class="si">{</span><span class="n">true_var_t3</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (vs assumed 1.0)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model SE: </span><span class="si">{</span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, True SE: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">true_var_t3</span><span class="o">/</span><span class="n">n</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">95% CI Coverage (</span><span class="si">{</span><span class="n">n_sim</span><span class="si">}</span><span class="s2"> simulations):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Model-based (σ²=1): </span><span class="si">{</span><span class="n">model_covers</span><span class="o">/</span><span class="n">n_sim</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> — SEVERE UNDERCOVERAGE&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Sandwich:           </span><span class="si">{</span><span class="n">sandwich_covers</span><span class="o">/</span><span class="n">n_sim</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> — Near nominal&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Part (d): Why Sandwich Works&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">The sandwich estimator works because:</span>

<span class="s2">1. CONSISTENCY: X̄ is consistent for μ regardless of the distribution</span>
<span class="s2">   (by LLN, as long as E[X] exists)</span>

<span class="s2">2. VARIANCE FORMULA: The true variance of X̄ is Var(X)/n, not σ²/n</span>
<span class="s2">   where σ² is the assumed (wrong) model variance</span>

<span class="s2">3. SANDWICH ADAPTS: By using s² = Σ(Xᵢ - X̄)²/(n-1), we estimate</span>
<span class="s2">   the TRUE variance, not the model&#39;s assumed variance</span>

<span class="s2">4. KEY INSIGHT: We don&#39;t need the model to be correct for the MLE</span>
<span class="s2">   to be consistent or for variance estimation to work. We just</span>
<span class="s2">   need the estimating equation to identify the parameter correctly.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>============================================================
SANDWICH VARIANCE: MODEL MISSPECIFICATION
============================================================

Part (b): Single dataset from t_5 (n=50)
True variance of t_5: 1.6667

Three SE estimates:
  Model-based (assumes σ²=1): 0.1414
  Sandwich (uses s²):         0.1826
  True SE:                    0.1826

============================================================
Part (c): Coverage simulation with t_3 data
============================================================

True variance of t_3: 3.0000 (vs assumed 1.0)
Model SE: 0.1414, True SE: 0.2449

95% CI Coverage (10000 simulations):
  Model-based (σ²=1): 0.773 — SEVERE UNDERCOVERAGE
  Sandwich:           0.948 — Near nominal

============================================================
Part (d): Why Sandwich Works
============================================================

The sandwich estimator works because:

1. CONSISTENCY: X̄ is consistent for μ regardless of the distribution
   (by LLN, as long as E[X] exists)

2. VARIANCE FORMULA: The true variance of X̄ is Var(X)/n, not σ²/n
   where σ² is the assumed (wrong) model variance

3. SANDWICH ADAPTS: By using s² = Σ(Xᵢ - X̄)²/(n-1), we estimate
   the TRUE variance, not the model&#39;s assumed variance

4. KEY INSIGHT: We don&#39;t need the model to be correct for the MLE
   to be consistent or for variance estimation to work. We just
   need the estimating equation to identify the parameter correctly.
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Observations:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Severe undercoverage</strong>: Model-based CI achieves only 77% coverage (not 95%) because it underestimates the true variance.</p></li>
<li><p class="sd-card-text"><strong>Sandwich saves the day</strong>: By using the sample variance, we correctly estimate the larger spread from heavy tails.</p></li>
<li><p class="sd-card-text"><strong>Ratio of variances</strong>: True variance is 3× the assumed variance, so model SE is <span class="math notranslate nohighlight">\(\sqrt{3} \approx 1.73\)</span> times too small.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 3.3.6: Propagation of Uncertainty in Physics</p>
<p>The delta method has been used in physics long before it had a name—it’s the basis of “error propagation” formulas taught in introductory labs.</p>
<p>A circuit has measured voltage <span class="math notranslate nohighlight">\(\hat{V} = 12.5 \pm 0.3\)</span> V and current <span class="math notranslate nohighlight">\(\hat{I} = 2.1 \pm 0.1\)</span> A (where ± denotes standard error).</p>
<ol class="loweralpha simple">
<li><p><strong>Resistance</strong>: Compute <span class="math notranslate nohighlight">\(\hat{R} = \hat{V}/\hat{I}\)</span> and its SE assuming independent measurements.</p></li>
<li><p><strong>Power</strong>: Compute <span class="math notranslate nohighlight">\(\hat{P} = \hat{V} \cdot \hat{I}\)</span> and its SE.</p></li>
<li><p><strong>Correlated measurements</strong>: If <span class="math notranslate nohighlight">\(\text{Corr}(V, I) = 0.3\)</span>, how do the SEs change?</p></li>
<li><p><strong>Confidence intervals</strong>: Construct 95% CIs for <span class="math notranslate nohighlight">\(R\)</span> and <span class="math notranslate nohighlight">\(P\)</span> (independent case).</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Resistance SE</strong></p>
<div class="tip admonition">
<p class="admonition-title">Step 1: Point Estimate</p>
<div class="math notranslate nohighlight">
\[\hat{R} = \frac{\hat{V}}{\hat{I}} = \frac{12.5}{2.1} = 5.952 \;\Omega\]</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Step 2: Delta Method for Ratio</p>
<p class="sd-card-text">For <span class="math notranslate nohighlight">\(g(V, I) = V/I\)</span>:</p>
<div class="math notranslate nohighlight">
\[\nabla g = \left(\frac{1}{I}, -\frac{V}{I^2}\right)^\top = \left(\frac{1}{2.1}, -\frac{12.5}{2.1^2}\right) = (0.476, -2.834)\]</div>
<p class="sd-card-text">With independent measurements:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\hat{R}) = \frac{1}{I^2}\text{Var}(V) + \frac{V^2}{I^4}\text{Var}(I)\]</div>
<div class="math notranslate nohighlight">
\[= \frac{0.3^2}{2.1^2} + \frac{12.5^2 \cdot 0.1^2}{2.1^4} = 0.0204 + 0.0803 = 0.1007\]</div>
<div class="math notranslate nohighlight">
\[\text{SE}(\hat{R}) = \sqrt{0.1007} = 0.317 \;\Omega\]</div>
</div>
<p class="sd-card-text"><strong>Part (b): Power SE</strong></p>
<p class="sd-card-text">For <span class="math notranslate nohighlight">\(g(V, I) = V \cdot I\)</span>:</p>
<div class="math notranslate nohighlight">
\[\hat{P} = 12.5 \times 2.1 = 26.25 \;\text{W}\]</div>
<div class="math notranslate nohighlight">
\[\nabla g = (I, V)^\top = (2.1, 12.5)\]</div>
<div class="math notranslate nohighlight">
\[\text{Var}(\hat{P}) = I^2 \text{Var}(V) + V^2 \text{Var}(I) = 2.1^2 \cdot 0.3^2 + 12.5^2 \cdot 0.1^2\]</div>
<div class="math notranslate nohighlight">
\[= 0.3969 + 1.5625 = 1.959\]</div>
<div class="math notranslate nohighlight">
\[\text{SE}(\hat{P}) = \sqrt{1.959} = 1.40 \;\text{W}\]</div>
<p class="sd-card-text"><strong>Parts (c) &amp; (d): Implementation</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Measurements</span>
<span class="n">V_hat</span><span class="p">,</span> <span class="n">se_V</span> <span class="o">=</span> <span class="mf">12.5</span><span class="p">,</span> <span class="mf">0.3</span>
<span class="n">I_hat</span><span class="p">,</span> <span class="n">se_I</span> <span class="o">=</span> <span class="mf">2.1</span><span class="p">,</span> <span class="mf">0.1</span>

<span class="c1"># Point estimates</span>
<span class="n">R_hat</span> <span class="o">=</span> <span class="n">V_hat</span> <span class="o">/</span> <span class="n">I_hat</span>
<span class="n">P_hat</span> <span class="o">=</span> <span class="n">V_hat</span> <span class="o">*</span> <span class="n">I_hat</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ERROR PROPAGATION: OHM&#39;S LAW&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Measurements:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  V = </span><span class="si">{</span><span class="n">V_hat</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">se_V</span><span class="si">}</span><span class="s2"> V&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  I = </span><span class="si">{</span><span class="n">I_hat</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">se_I</span><span class="si">}</span><span class="s2"> A&quot;</span><span class="p">)</span>

<span class="c1"># Part (a) &amp; (b): Independent case</span>
<span class="c1"># Resistance</span>
<span class="n">var_R_indep</span> <span class="o">=</span> <span class="p">(</span><span class="n">se_V</span><span class="o">/</span><span class="n">I_hat</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">V_hat</span> <span class="o">*</span> <span class="n">se_I</span> <span class="o">/</span> <span class="n">I_hat</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">se_R_indep</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_R_indep</span><span class="p">)</span>

<span class="c1"># Power</span>
<span class="n">var_P_indep</span> <span class="o">=</span> <span class="p">(</span><span class="n">I_hat</span> <span class="o">*</span> <span class="n">se_V</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">V_hat</span> <span class="o">*</span> <span class="n">se_I</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">se_P_indep</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_P_indep</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Independent measurements:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  R = V/I = </span><span class="si">{</span><span class="n">R_hat</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">se_R_indep</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> Ω&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  P = VI  = </span><span class="si">{</span><span class="n">P_hat</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">se_P_indep</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> W&quot;</span><span class="p">)</span>

<span class="c1"># Part (c): Correlated case</span>
<span class="n">rho</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">cov_VI</span> <span class="o">=</span> <span class="n">rho</span> <span class="o">*</span> <span class="n">se_V</span> <span class="o">*</span> <span class="n">se_I</span>

<span class="c1"># Resistance with correlation</span>
<span class="c1"># Var(R) = (∂R/∂V)² Var(V) + (∂R/∂I)² Var(I) + 2(∂R/∂V)(∂R/∂I) Cov(V,I)</span>
<span class="n">dR_dV</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">I_hat</span>
<span class="n">dR_dI</span> <span class="o">=</span> <span class="o">-</span><span class="n">V_hat</span> <span class="o">/</span> <span class="n">I_hat</span><span class="o">**</span><span class="mi">2</span>
<span class="n">var_R_corr</span> <span class="o">=</span> <span class="n">dR_dV</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">se_V</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">dR_dI</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">se_I</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">dR_dV</span> <span class="o">*</span> <span class="n">dR_dI</span> <span class="o">*</span> <span class="n">cov_VI</span>
<span class="n">se_R_corr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_R_corr</span><span class="p">)</span>

<span class="c1"># Power with correlation</span>
<span class="n">dP_dV</span> <span class="o">=</span> <span class="n">I_hat</span>
<span class="n">dP_dI</span> <span class="o">=</span> <span class="n">V_hat</span>
<span class="n">var_P_corr</span> <span class="o">=</span> <span class="n">dP_dV</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">se_V</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">dP_dI</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">se_I</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">dP_dV</span> <span class="o">*</span> <span class="n">dP_dI</span> <span class="o">*</span> <span class="n">cov_VI</span>
<span class="n">se_P_corr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_P_corr</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Correlated measurements (ρ = </span><span class="si">{</span><span class="n">rho</span><span class="si">}</span><span class="s2">):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  R = </span><span class="si">{</span><span class="n">R_hat</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">se_R_corr</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> Ω&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  P = </span><span class="si">{</span><span class="n">P_hat</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">se_P_corr</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> W&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Effect of correlation:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Resistance SE: </span><span class="si">{</span><span class="n">se_R_indep</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> → </span><span class="si">{</span><span class="n">se_R_corr</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (decreased by </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">se_R_corr</span><span class="o">/</span><span class="n">se_R_indep</span><span class="p">)</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Power SE:      </span><span class="si">{</span><span class="n">se_P_indep</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> → </span><span class="si">{</span><span class="n">se_P_corr</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> (increased by </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="n">se_P_corr</span><span class="o">/</span><span class="n">se_P_indep</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>

<span class="c1"># Part (d): Confidence intervals</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.975</span><span class="p">)</span>

<span class="n">ci_R</span> <span class="o">=</span> <span class="p">(</span><span class="n">R_hat</span> <span class="o">-</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se_R_indep</span><span class="p">,</span> <span class="n">R_hat</span> <span class="o">+</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se_R_indep</span><span class="p">)</span>
<span class="n">ci_P</span> <span class="o">=</span> <span class="p">(</span><span class="n">P_hat</span> <span class="o">-</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se_P_indep</span><span class="p">,</span> <span class="n">P_hat</span> <span class="o">+</span> <span class="n">z</span> <span class="o">*</span> <span class="n">se_P_indep</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">95% Confidence Intervals (independent):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  R: (</span><span class="si">{</span><span class="n">ci_R</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_R</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">) Ω&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  P: (</span><span class="si">{</span><span class="n">ci_P</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_P</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">) W&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>============================================================
ERROR PROPAGATION: OHM&#39;S LAW
============================================================

Measurements:
  V = 12.5 ± 0.3 V
  I = 2.1 ± 0.1 A

Independent measurements:
  R = V/I = 5.952 ± 0.317 Ω
  P = VI  = 26.25 ± 1.40 W

Correlated measurements (ρ = 0.3):
  R = 5.952 ± 0.299 Ω
  P = 26.25 ± 1.52 W

Effect of correlation:
  Resistance SE: 0.317 → 0.299 (decreased by 5.8%)
  Power SE:      1.40 → 1.52 (increased by 8.5%)

95% Confidence Intervals (independent):
  R: (5.33, 6.57) Ω
  P: (23.51, 28.99) W
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Observations:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Correlation matters differently for ratios vs products:</strong>
- For R = V/I: Positive correlation in (V, I) DECREASES variance (V and I move together, so their ratio is more stable)
- For P = VI: Positive correlation INCREASES variance (fluctuations compound)</p></li>
<li><p class="sd-card-text"><strong>Physics interpretation</strong>: If V and I measurements are correlated (e.g., from the same power supply), the ratio R is more precisely determined, but the product P is less precise.</p></li>
<li><p class="sd-card-text"><strong>This is the standard “error propagation” formula</strong> taught in physics labs—it’s just the delta method applied to measurement uncertainty.</p></li>
</ol>
</div>
</details></div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<p><strong>Foundational Works on Estimator Properties</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="fisher1922" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Fisher1922<span class="fn-bracket">]</span></span>
<p>Fisher, R. A. (1922). On the mathematical foundations of theoretical statistics. <em>Philosophical Transactions of the Royal Society A</em>, 222, 309–368. Introduces the concepts of consistency, efficiency, and sufficiency that define estimator quality.</p>
</div>
<div class="citation" id="neyman1934" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Neyman1934<span class="fn-bracket">]</span></span>
<p>Neyman, J. (1934). On the two different aspects of the representative method: The method of stratified sampling and the method of purposive selection. <em>Journal of the Royal Statistical Society</em>, 97(4), 558–625. Foundational work on sampling theory and the distinction between design-based and model-based inference.</p>
</div>
<div class="citation" id="rao1945" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Rao1945<span class="fn-bracket">]</span></span>
<p>Rao, C. R. (1945). Information and the accuracy attainable in the estimation of statistical parameters. <em>Bulletin of the Calcutta Mathematical Society</em>, 37, 81–89. Establishes the Cramér-Rao lower bound providing the fundamental limit on estimator variance.</p>
</div>
</div>
<p><strong>The Delta Method</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="cramer1946" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Cramér1946<span class="fn-bracket">]</span></span>
<p>Cramér, H. (1946). <em>Mathematical Methods of Statistics</em>. Princeton University Press. Contains rigorous treatment of asymptotic theory including the delta method for transformed parameters.</p>
</div>
<div class="citation" id="serfling1980" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Serfling1980<span class="fn-bracket">]</span></span>
<p>Serfling, R. J. (1980). <em>Approximation Theorems of Mathematical Statistics</em>. Wiley. Comprehensive treatment of asymptotic approximations including multivariate delta method and higher-order expansions.</p>
</div>
<div class="citation" id="oehlert1992" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Oehlert1992<span class="fn-bracket">]</span></span>
<p>Oehlert, G. W. (1992). A note on the delta method. <em>The American Statistician</em>, 46(1), 27–29. Accessible introduction to the delta method with practical guidance on its application.</p>
</div>
</div>
<p><strong>Sandwich Estimators and Robust Variance Estimation</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="huber1967" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Huber1967<span class="fn-bracket">]</span></span>
<p>Huber, P. J. (1967). The behavior of maximum likelihood estimates under nonstandard conditions. In <em>Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</em>, Vol. 1, 221–233. University of California Press. Introduces the sandwich variance estimator providing consistent standard errors under model misspecification.</p>
</div>
<div class="citation" id="white1980" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>White1980<span class="fn-bracket">]</span></span>
<p>White, H. (1980). A heteroskedasticity-consistent covariance matrix estimator and a direct test for heteroskedasticity. <em>Econometrica</em>, 48(4), 817–838. Develops heteroskedasticity-consistent standard errors (HC0) now standard in regression analysis.</p>
</div>
<div class="citation" id="white1982" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>White1982<span class="fn-bracket">]</span></span>
<p>White, H. (1982). Maximum likelihood estimation of misspecified models. <em>Econometrica</em>, 50(1), 1–25. Establishes the theoretical foundation for quasi-maximum likelihood inference under model misspecification.</p>
</div>
<div class="citation" id="mackinnon1985" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MacKinnon1985<span class="fn-bracket">]</span></span>
<p>MacKinnon, J. G., and White, H. (1985). Some heteroskedasticity-consistent covariance matrix estimators with improved finite sample properties. <em>Journal of Econometrics</em>, 29(3), 305–325. Introduces improved versions of heteroskedasticity-consistent estimators (HC1, HC2, HC3) with better finite-sample performance.</p>
</div>
</div>
<p><strong>Fisher Information</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="fisher1925" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Fisher1925<span class="fn-bracket">]</span></span>
<p>Fisher, R. A. (1925). Theory of statistical estimation. <em>Proceedings of the Cambridge Philosophical Society</em>, 22(5), 700–725. Introduces Fisher information as the variance of the score function and establishes its role in asymptotic variance.</p>
</div>
<div class="citation" id="efron1978" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Efron1978<span class="fn-bracket">]</span></span>
<p>Efron, B., and Hinkley, D. V. (1978). Assessing the accuracy of the maximum likelihood estimator: Observed versus expected Fisher information. <em>Biometrika</em>, 65(3), 457–487. Compares observed and expected information for variance estimation, arguing for observed information in practice.</p>
</div>
<div class="citation" id="davison2003" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Davison2003<span class="fn-bracket">]</span></span>
<p>Davison, A. C. (2003). <em>Statistical Models</em>. Cambridge University Press. Modern treatment of statistical inference including comprehensive discussion of information-based standard errors.</p>
</div>
</div>
<p><strong>Consistency and Asymptotic Normality</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="mann1943" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Mann1943<span class="fn-bracket">]</span></span>
<p>Mann, H. B., and Wald, A. (1943). On stochastic limit and order relationships. <em>Annals of Mathematical Statistics</em>, 14(3), 217–226. Foundational work on convergence concepts in probability and statistics.</p>
</div>
<div class="citation" id="lecam1953" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LeCam1953<span class="fn-bracket">]</span></span>
<p>Le Cam, L. (1953). On some asymptotic properties of maximum likelihood estimates and related Bayes’ estimates. <em>University of California Publications in Statistics</em>, 1, 277–329. Establishes local asymptotic normality providing the theoretical foundation for asymptotic inference.</p>
</div>
<div class="citation" id="vandervaart1998" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>VanDerVaart1998<span class="fn-bracket">]</span></span>
<p>van der Vaart, A. W. (1998). <em>Asymptotic Statistics</em>. Cambridge University Press. Definitive modern treatment of asymptotic statistical theory with rigorous proofs and comprehensive coverage.</p>
</div>
</div>
<p><strong>Bias-Variance Tradeoff</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="lehmann1983" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Lehmann1983<span class="fn-bracket">]</span></span>
<p>Lehmann, E. L. (1983). <em>Theory of Point Estimation</em>. Wiley. Develops the theory of unbiased estimation and the bias-variance decomposition of mean squared error.</p>
</div>
<div class="citation" id="efron1993" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Efron1993<span class="fn-bracket">]</span></span>
<p>Efron, B. (1993). Bayes and likelihood calculations from confidence intervals. <em>Biometrika</em>, 80(1), 3–26. Discusses the relationship between frequentist and Bayesian approaches to uncertainty quantification.</p>
</div>
</div>
<p><strong>Cluster-Robust Inference</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="liang1986" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Liang1986<span class="fn-bracket">]</span></span>
<p>Liang, K.-Y., and Zeger, S. L. (1986). Longitudinal data analysis using generalized linear models. <em>Biometrika</em>, 73(1), 13–22. Introduces generalized estimating equations (GEE) with cluster-robust standard errors for longitudinal data.</p>
</div>
<div class="citation" id="arellano1987" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Arellano1987<span class="fn-bracket">]</span></span>
<p>Arellano, M. (1987). Computing robust standard errors for within-groups estimators. <em>Oxford Bulletin of Economics and Statistics</em>, 49(4), 431–434. Develops cluster-robust standard errors for panel data with fixed effects.</p>
</div>
<div class="citation" id="cameron2011" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Cameron2011<span class="fn-bracket">]</span></span>
<p>Cameron, A. C., and Miller, D. L. (2011). Robust inference with clustered data. In <em>Handbook of Empirical Economics and Finance</em>, 1–28. CRC Press. Comprehensive guide to cluster-robust inference in applied econometrics.</p>
</div>
</div>
<p><strong>Textbook Treatments</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="casellaberger2002" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CasellaBerger2002<span class="fn-bracket">]</span></span>
<p>Casella, G., and Berger, R. L. (2002). <em>Statistical Inference</em> (2nd ed.). Duxbury Press. Chapter 7 provides accessible coverage of sampling distributions and point estimation properties.</p>
</div>
<div class="citation" id="wasserman2004" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Wasserman2004<span class="fn-bracket">]</span></span>
<p>Wasserman, L. (2004). <em>All of Statistics: A Concise Course in Statistical Inference</em>. Springer. Modern introduction to statistical inference with clear treatment of sampling variability.</p>
</div>
<div class="citation" id="efronhastie2016" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>EfronHastie2016<span class="fn-bracket">]</span></span>
<p>Efron, B., and Hastie, T. (2016). <em>Computer Age Statistical Inference: Algorithms, Evidence, and Data Science</em>. Cambridge University Press. Contemporary perspective on inference including bootstrap and cross-validation approaches to variance estimation.</p>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="ch3_2-maximum-likelihood-estimation.html" class="btn btn-neutral float-left" title="2.2.2. Maximum Likelihood Estimation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ch3_4-linear-models.html" class="btn btn-neutral float-right" title="2.2.4. Linear Models" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>