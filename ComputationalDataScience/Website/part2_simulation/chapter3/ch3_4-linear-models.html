

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Linear Models &mdash; STAT 418</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=3d0abd52" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT418/Website/part2_simulation/chapter3/ch3_4-linear-models.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=30646c52"></script>
      <script>let toggleHintShow = 'Show solution';</script>
      <script>let toggleHintHide = 'Hide solution';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>window.MathJax = {"options": {"enableMenu": true, "menuOptions": {"settings": {"enrich": true, "speech": true, "braille": true, "collapsible": true, "assistiveMml": false}}}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
      <script src="../../_static/custom.js?v=8718e0ab"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Generalized Linear Models" href="ch3_5-generalized-linear-models.html" />
    <link rel="prev" title="Sampling Variability and Variance Estimation" href="ch3_3-sampling-variability.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Course Content</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../Homework/index.html">Homework Assignments</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Homework/index.html#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#assignment-policies">Assignment Policies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#recommended-workflow">Recommended Workflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#submission-requirements">Submission Requirements</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Homework/index.html#assignments-by-chapter">Assignments by Chapter</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#part-i-foundations">Part I: Foundations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Homework/hw1_distributional_relationships.html">Homework 1: Distributional Relationships and Computational Foundations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Homework/index.html#tips-for-success">Tips for Success</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#mathematical-derivations">Mathematical Derivations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#computational-verification">Computational Verification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#common-pitfalls">Common Pitfalls</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part1_foundations/index.html">Part I: Foundations of Probability and Computation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part1_foundations/chapter1/index.html">Chapter 1: Statistical Paradigms and Core Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html">Paradigms of Probability and Statistical Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#the-mathematical-foundation-kolmogorov-s-axioms">The Mathematical Foundation: Kolmogorov’s Axioms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#interpretations-of-probability">Interpretations of Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#statistical-inference-paradigms">Statistical Inference Paradigms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#historical-and-philosophical-debates">Historical and Philosophical Debates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#looking-ahead-our-course-focus">Looking Ahead: Our Course Focus</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html">Probability Distributions: Theory and Computation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#from-abstract-foundations-to-concrete-tools">From Abstract Foundations to Concrete Tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#the-python-ecosystem-for-probability">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#introduction-why-probability-distributions-matter">Introduction: Why Probability Distributions Matter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#id1">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#continuous-distributions">Continuous Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#additional-important-distributions">Additional Important Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#summary-and-practical-guidelines">Summary and Practical Guidelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#conclusion">Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html">Python Random Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#from-mathematical-distributions-to-computational-samples">From Mathematical Distributions to Computational Samples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-python-ecosystem-at-a-glance">The Python Ecosystem at a Glance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#understanding-pseudo-random-number-generation">Understanding Pseudo-Random Number Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-standard-library-random-module">The Standard Library: <code class="docutils literal notranslate"><span class="pre">random</span></code> Module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#numpy-fast-vectorized-random-sampling">NumPy: Fast Vectorized Random Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#scipy-stats-the-complete-statistical-toolkit">SciPy Stats: The Complete Statistical Toolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#bringing-it-all-together-library-selection-guide">Bringing It All Together: Library Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#looking-ahead-from-random-numbers-to-monte-carlo-methods">Looking Ahead: From Random Numbers to Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html">Chapter 1 Summary: Foundations in Place</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#the-three-pillars-of-chapter-1">The Three Pillars of Chapter 1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#what-lies-ahead-the-road-to-simulation">What Lies Ahead: The Road to Simulation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#chapter-1-exercises-synthesis-problems">Chapter 1 Exercises: Synthesis Problems</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Part II: Simulation-Based Methods</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../chapter2/index.html">Chapter 2: Monte Carlo Simulation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html">Monte Carlo Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#the-historical-development-of-monte-carlo-methods">The Historical Development of Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#the-core-principle-expectation-as-integration">The Core Principle: Expectation as Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#theoretical-foundations">Theoretical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#variance-estimation-and-confidence-intervals">Variance Estimation and Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#comparison-with-deterministic-methods">Comparison with Deterministic Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#sample-size-determination">Sample Size Determination</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#convergence-diagnostics-and-monitoring">Convergence Diagnostics and Monitoring</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#chapter-2-1-exercises-monte-carlo-fundamentals-mastery">Chapter 2.1 Exercises: Monte Carlo Fundamentals Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html">Uniform Random Variates</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#why-uniform-the-universal-currency-of-randomness">Why Uniform? The Universal Currency of Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#the-paradox-of-computational-randomness">The Paradox of Computational Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#chaotic-dynamical-systems-an-instructive-failure">Chaotic Dynamical Systems: An Instructive Failure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#linear-congruential-generators">Linear Congruential Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#shift-register-generators">Shift-Register Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#the-kiss-generator-combining-strategies">The KISS Generator: Combining Strategies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#modern-generators-mersenne-twister-and-pcg">Modern Generators: Mersenne Twister and PCG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#statistical-testing-of-random-number-generators">Statistical Testing of Random Number Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#chapter-2-2-exercises-uniform-random-variates-mastery">Chapter 2.2 Exercises: Uniform Random Variates Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html">Inverse CDF Method</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#mathematical-foundations">Mathematical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#continuous-distributions-with-closed-form-inverses">Continuous Distributions with Closed-Form Inverses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#numerical-inversion">Numerical Inversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#mixed-distributions">Mixed Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#chapter-2-3-exercises-inverse-cdf-method-mastery">Chapter 2.3 Exercises: Inverse CDF Method Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html">Transformation Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#why-transformation-methods">Why Transformation Methods?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-boxmuller-transform">The Box–Muller Transform</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-polar-marsaglia-method">The Polar (Marsaglia) Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#method-comparison-boxmuller-vs-polar-vs-ziggurat">Method Comparison: Box–Muller vs Polar vs Ziggurat</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-ziggurat-algorithm">The Ziggurat Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-clt-approximation-historical">The CLT Approximation (Historical)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#distributions-derived-from-the-normal">Distributions Derived from the Normal</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#multivariate-normal-generation">Multivariate Normal Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#implementation-guidance">Implementation Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#chapter-2-4-exercises-transformation-methods-mastery">Chapter 2.4 Exercises: Transformation Methods Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html">Rejection Sampling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-dartboard-intuition">The Dartboard Intuition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-accept-reject-algorithm">The Accept-Reject Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#efficiency-analysis">Efficiency Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#choosing-the-proposal-distribution">Choosing the Proposal Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-squeeze-principle">The Squeeze Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#geometric-example-sampling-from-the-unit-disk">Geometric Example: Sampling from the Unit Disk</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#limitations-and-the-curse-of-dimensionality">Limitations and the Curse of Dimensionality</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#connections-to-other-methods">Connections to Other Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#chapter-2-5-exercises-rejection-sampling-mastery">Chapter 2.5 Exercises: Rejection Sampling Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html">Variance Reduction Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#the-variance-reduction-paradigm">The Variance Reduction Paradigm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#importance-sampling">Importance Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#control-variates">Control Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#antithetic-variates">Antithetic Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#stratified-sampling">Stratified Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#common-random-numbers">Common Random Numbers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#conditional-monte-carlo-raoblackwellization">Conditional Monte Carlo (Rao–Blackwellization)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#combining-variance-reduction-techniques">Combining Variance Reduction Techniques</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#chapter-2-6-exercises-variance-reduction-mastery">Chapter 2.6 Exercises: Variance Reduction Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html">Chapter Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#the-complete-monte-carlo-workflow">The Complete Monte Carlo Workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#quick-reference-tables">Quick Reference Tables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#common-pitfalls-checklist">Common Pitfalls Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#connections-to-later-chapters">Connections to Later Chapters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#learning-outcomes-checklist">Learning Outcomes Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#further-reading-optimization-and-missing-data">Further Reading: Optimization and Missing Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Chapter 3: Parametric Inference and Likelihood Methods</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="ch3_1-exponential-families.html">Exponential Families</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#historical-origins-from-scattered-results-to-unified-theory">Historical Origins: From Scattered Results to Unified Theory</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#the-canonical-exponential-family">The Canonical Exponential Family</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#converting-familiar-distributions">Converting Familiar Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#the-log-partition-function-a-moment-generating-machine">The Log-Partition Function: A Moment-Generating Machine</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#sufficiency-capturing-all-parameter-information">Sufficiency: Capturing All Parameter Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#minimal-sufficiency-and-completeness">Minimal Sufficiency and Completeness</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#conjugate-priors-and-bayesian-inference">Conjugate Priors and Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#exponential-dispersion-models-and-glms">Exponential Dispersion Models and GLMs</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#chapter-3-1-exercises-exponential-families-mastery">Chapter 3.1 Exercises: Exponential Families Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html">Maximum Likelihood Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#the-likelihood-function">The Likelihood Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#the-score-function">The Score Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#fisher-information">Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#closed-form-maximum-likelihood-estimators">Closed-Form Maximum Likelihood Estimators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#numerical-optimization-for-mle">Numerical Optimization for MLE</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#asymptotic-properties-of-mles">Asymptotic Properties of MLEs</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#the-cramer-rao-lower-bound">The Cramér-Rao Lower Bound</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#the-invariance-property">The Invariance Property</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#likelihood-based-hypothesis-testing">Likelihood-Based Hypothesis Testing</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#confidence-intervals-from-likelihood">Confidence Intervals from Likelihood</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#connection-to-bayesian-inference">Connection to Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#chapter-3-2-exercises-maximum-likelihood-estimation-mastery">Chapter 3.2 Exercises: Maximum Likelihood Estimation Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch3_3-sampling-variability.html">Sampling Variability and Variance Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#statistical-estimators-and-their-properties">Statistical Estimators and Their Properties</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#sampling-distributions">Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#the-delta-method">The Delta Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#variance-estimation-methods">Variance Estimation Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#applications-and-worked-examples">Applications and Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#matrix-calculus-foundations">Matrix Calculus Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-linear-model">The Linear Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#ordinary-least-squares-the-calculus-approach">Ordinary Least Squares: The Calculus Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="#ordinary-least-squares-the-geometric-approach">Ordinary Least Squares: The Geometric Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="#properties-of-the-ols-estimator">Properties of the OLS Estimator</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-gauss-markov-theorem">The Gauss-Markov Theorem</a></li>
<li class="toctree-l4"><a class="reference internal" href="#estimating-the-error-variance">Estimating the Error Variance</a></li>
<li class="toctree-l4"><a class="reference internal" href="#distributional-results-under-normality">Distributional Results Under Normality</a></li>
<li class="toctree-l4"><a class="reference internal" href="#diagnostics-and-model-checking">Diagnostics and Model Checking</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="#numerical-stability-qr-decomposition">Numerical Stability: QR Decomposition</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-selection-and-information-criteria">Model Selection and Information Criteria</a></li>
<li class="toctree-l4"><a class="reference internal" href="#regularization-ridge-and-lasso">Regularization: Ridge and LASSO</a></li>
<li class="toctree-l4"><a class="reference internal" href="#chapter-3-4-exercises-linear-models-mastery">Chapter 3.4 Exercises: Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch3_5-generalized-linear-models.html">Generalized Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#historical-context-unification-of-regression-methods">Historical Context: Unification of Regression Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#the-glm-framework-three-components">The GLM Framework: Three Components</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#score-equations-and-fisher-information">Score Equations and Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#iteratively-reweighted-least-squares">Iteratively Reweighted Least Squares</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#logistic-regression-binary-outcomes">Logistic Regression: Binary Outcomes</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#poisson-regression-count-data">Poisson Regression: Count Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#gamma-regression-positive-continuous-data">Gamma Regression: Positive Continuous Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#inference-in-glms-the-testing-triad">Inference in GLMs: The Testing Triad</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#model-diagnostics">Model Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#model-comparison-and-selection">Model Comparison and Selection</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#quasi-likelihood-and-robust-inference">Quasi-Likelihood and Robust Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#further-reading">Further Reading</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#chapter-3-5-exercises-generalized-linear-models-mastery">Chapter 3.5 Exercises: Generalized Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch3_6-chapter-summary.html">Chapter Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#the-parametric-inference-pipeline">The Parametric Inference Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#the-five-pillars-of-chapter-3">The Five Pillars of Chapter 3</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#quick-reference-core-formulas">Quick Reference: Core Formulas</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#connections-to-future-material">Connections to Future Material</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#practical-guidance">Practical Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_6-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter4/index.html">Chapter 4: Resampling Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html">The Sampling Distribution Problem</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#the-fundamental-target-sampling-distributions">The Fundamental Target: Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#historical-development-the-quest-for-sampling-distributions">Historical Development: The Quest for Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#three-routes-to-the-sampling-distribution">Three Routes to the Sampling Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#when-asymptotics-fail-motivating-the-bootstrap">When Asymptotics Fail: Motivating the Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#the-plug-in-principle-theoretical-foundation">The Plug-In Principle: Theoretical Foundation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#computational-perspective-bootstrap-as-monte-carlo">Computational Perspective: Bootstrap as Monte Carlo</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#chapter-4-1-exercises">Chapter 4.1 Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html">The Empirical Distribution and Plug-in Principle</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#the-empirical-cumulative-distribution-function">The Empirical Cumulative Distribution Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#convergence-of-the-empirical-cdf">Convergence of the Empirical CDF</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#parameters-as-statistical-functionals">Parameters as Statistical Functionals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#when-the-plug-in-principle-fails">When the Plug-in Principle Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#the-bootstrap-idea-in-one-sentence">The Bootstrap Idea in One Sentence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#computational-implementation">Computational Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#section-4-2-exercises-ecdf-and-plug-in-mastery">Section 4.2 Exercises: ECDF and Plug-in Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html">The Nonparametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#the-bootstrap-principle">The Bootstrap Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-standard-errors">Bootstrap Standard Errors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-bias-estimation">Bootstrap Bias Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-confidence-intervals">Bootstrap Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-for-regression">Bootstrap for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-diagnostics">Bootstrap Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#when-bootstrap-fails">When Bootstrap Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html">Section 4.4: The Parametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#the-parametric-bootstrap-principle">The Parametric Bootstrap Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#location-scale-families">Location-Scale Families</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#parametric-bootstrap-for-regression">Parametric Bootstrap for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#confidence-intervals">Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#model-checking-and-validation">Model Checking and Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#when-parametric-bootstrap-fails">When Parametric Bootstrap Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#parametric-vs-nonparametric-a-decision-framework">Parametric vs. Nonparametric: A Decision Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html">Section 4.5: Jackknife Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#historical-context-and-motivation">Historical Context and Motivation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#the-delete-1-jackknife">The Delete-1 Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#jackknife-bias-estimation">Jackknife Bias Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#the-delete-d-jackknife">The Delete-<span class="math notranslate nohighlight">\(d\)</span> Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#jackknife-versus-bootstrap">Jackknife versus Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#the-infinitesimal-jackknife">The Infinitesimal Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part3_bayesian/index.html">Part III: Bayesian Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part3_bayesian/index.html#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html">Bayesian Philosophy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Part II: Simulation-Based Methods</a></li>
          <li class="breadcrumb-item"><a href="index.html">Chapter 3: Parametric Inference and Likelihood Methods</a></li>
      <li class="breadcrumb-item active">Linear Models</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/part2_simulation/chapter3/ch3_4-linear-models.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="linear-models">
<span id="ch3-4-linear-models"></span><h1>Linear Models<a class="headerlink" href="#linear-models" title="Link to this heading"></a></h1>
<p>Linear regression is arguably the most important statistical technique ever developed. From its origins in early 19th century astronomy—where Gauss and Legendre independently derived least squares to predict planetary orbits—to its modern role as the foundation of machine learning, linear regression has shaped how we understand relationships in data. Every data scientist must master this tool, not merely as a button to click, but as a mathematical framework to understand deeply.</p>
<p>This section develops linear regression from first principles. We begin with the <strong>matrix calculus</strong> needed to derive estimators—building up the rules for differentiating vectors, matrices, and quadratic forms. We then derive the ordinary least squares (OLS) estimator via two complementary approaches: the <strong>calculus approach</strong> (minimizing sum of squared errors) and the <strong>geometric approach</strong> (projection onto the column space). These perspectives illuminate different aspects of the same solution.</p>
<p>With the estimator in hand, we prove the <strong>Gauss-Markov theorem</strong>—that OLS is the Best Linear Unbiased Estimator (BLUE) under specific conditions. We examine each condition carefully, understanding what happens when it fails. We then develop the <strong>distributional theory</strong> under normality, enabling t-tests, F-tests, and confidence intervals. Finally, we address <strong>diagnostics and remedies</strong> for when assumptions don’t hold.</p>
<div class="important admonition">
<p class="admonition-title">Road Map 🧭</p>
<ul class="simple">
<li><p><strong>Develop</strong>: Matrix calculus rules for differentiating vectors, quadratic forms, and matrix products</p></li>
<li><p><strong>Derive</strong>: OLS estimator via calculus (normal equations) and geometry (projection)</p></li>
<li><p><strong>Prove</strong>: Gauss-Markov theorem establishing OLS as BLUE under classical assumptions</p></li>
<li><p><strong>Apply</strong>: Distributional results for inference: t-tests, F-tests, confidence intervals</p></li>
<li><p><strong>Evaluate</strong>: Diagnostics for assumption violations and remedial measures</p></li>
</ul>
</div>
<section id="matrix-calculus-foundations">
<h2>Matrix Calculus Foundations<a class="headerlink" href="#matrix-calculus-foundations" title="Link to this heading"></a></h2>
<p>Before deriving the least squares estimator, we need tools for differentiating expressions involving vectors and matrices. This section builds up the necessary calculus systematically, starting from scalar derivatives and extending to the multivariate case.</p>
<section id="why-matrix-calculus">
<h3>Why Matrix Calculus?<a class="headerlink" href="#why-matrix-calculus" title="Link to this heading"></a></h3>
<p>In scalar calculus, finding the minimum of <span class="math notranslate nohighlight">\(f(x) = (y - x)^2\)</span> is straightforward: take the derivative, set it to zero, solve for <span class="math notranslate nohighlight">\(x\)</span>. With <span class="math notranslate nohighlight">\(n\)</span> observations and <span class="math notranslate nohighlight">\(p\)</span> parameters, our objective becomes:</p>
<div class="math notranslate nohighlight">
\[f(\boldsymbol{\beta}) = \sum_{i=1}^n (y_i - \mathbf{x}_i^\top \boldsymbol{\beta})^2 = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^\top(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})\]</div>
<p>Minimizing this requires differentiating with respect to a <strong>vector</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>. Matrix calculus provides the systematic framework for such operations.</p>
</section>
<section id="notation-conventions">
<h3>Notation Conventions<a class="headerlink" href="#notation-conventions" title="Link to this heading"></a></h3>
<p>We adopt the following conventions throughout:</p>
<ul class="simple">
<li><p><strong>Vectors</strong> are column vectors, denoted by bold lowercase: <span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y}, \boldsymbol{\beta}\)</span></p></li>
<li><p><strong>Matrices</strong> are denoted by bold uppercase: <span class="math notranslate nohighlight">\(\mathbf{X}, \mathbf{A}, \boldsymbol{\Sigma}\)</span></p></li>
<li><p><strong>Scalars</strong> are italic lowercase: <span class="math notranslate nohighlight">\(a, b, \sigma^2\)</span></p></li>
<li><p><strong>Transpose</strong>: <span class="math notranslate nohighlight">\(\mathbf{x}^\top\)</span> (row vector), <span class="math notranslate nohighlight">\(\mathbf{A}^\top\)</span></p></li>
<li><p><strong>Dimensions</strong>: <span class="math notranslate nohighlight">\(\mathbf{y} \in \mathbb{R}^n\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\beta} \in \mathbb{R}^p\)</span>, <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{n \times p}\)</span></p></li>
</ul>
<p>The <strong>gradient</strong> of a scalar function <span class="math notranslate nohighlight">\(f: \mathbb{R}^p \to \mathbb{R}\)</span> with respect to vector <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> is:</p>
<div class="math notranslate nohighlight" id="equation-gradient-def">
<span class="eqno">(86)<a class="headerlink" href="#equation-gradient-def" title="Link to this equation"></a></span>\[\begin{split}\nabla_{\boldsymbol{\beta}} f = \frac{\partial f}{\partial \boldsymbol{\beta}} =
\begin{pmatrix}
\frac{\partial f}{\partial \beta_1} \\
\frac{\partial f}{\partial \beta_2} \\
\vdots \\
\frac{\partial f}{\partial \beta_p}
\end{pmatrix} \in \mathbb{R}^p\end{split}\]</div>
<p>This is a column vector of the same dimension as <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>.</p>
</section>
<section id="scalar-by-vector-derivatives-building-the-rules">
<h3>Scalar-by-Vector Derivatives: Building the Rules<a class="headerlink" href="#scalar-by-vector-derivatives-building-the-rules" title="Link to this heading"></a></h3>
<p>We develop the key differentiation rules step by step, starting from component-wise analysis.</p>
<p><strong>Rule 1: Linear function</strong> <span class="math notranslate nohighlight">\(f(\boldsymbol{\beta}) = \mathbf{a}^\top \boldsymbol{\beta}\)</span> where <span class="math notranslate nohighlight">\(\mathbf{a} \in \mathbb{R}^p\)</span> is constant.</p>
<p><em>Component form</em>:</p>
<div class="math notranslate nohighlight">
\[f(\boldsymbol{\beta}) = \mathbf{a}^\top \boldsymbol{\beta} = \sum_{j=1}^p a_j \beta_j\]</div>
<p>Taking the partial derivative with respect to <span class="math notranslate nohighlight">\(\beta_k\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial f}{\partial \beta_k} = \frac{\partial}{\partial \beta_k} \sum_{j=1}^p a_j \beta_j = a_k\]</div>
<p>Assembling all partials into a vector:</p>
<div class="math notranslate nohighlight" id="equation-linear-deriv">
<span class="eqno">(87)<a class="headerlink" href="#equation-linear-deriv" title="Link to this equation"></a></span>\[\boxed{\frac{\partial}{\partial \boldsymbol{\beta}} (\mathbf{a}^\top \boldsymbol{\beta}) = \mathbf{a}}\]</div>
<p>By symmetry of the dot product, <span class="math notranslate nohighlight">\(\mathbf{a}^\top \boldsymbol{\beta} = \boldsymbol{\beta}^\top \mathbf{a}\)</span>, so:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \boldsymbol{\beta}} (\boldsymbol{\beta}^\top \mathbf{a}) = \mathbf{a}\]</div>
<p><strong>Rule 2: Quadratic form</strong> <span class="math notranslate nohighlight">\(f(\boldsymbol{\beta}) = \boldsymbol{\beta}^\top \mathbf{A} \boldsymbol{\beta}\)</span> where <span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{p \times p}\)</span> is constant.</p>
<p><em>Component form</em>:</p>
<div class="math notranslate nohighlight">
\[f(\boldsymbol{\beta}) = \boldsymbol{\beta}^\top \mathbf{A} \boldsymbol{\beta} = \sum_{i=1}^p \sum_{j=1}^p \beta_i A_{ij} \beta_j\]</div>
<p>Taking the partial derivative with respect to <span class="math notranslate nohighlight">\(\beta_k\)</span> requires identifying all terms containing <span class="math notranslate nohighlight">\(\beta_k\)</span>:</p>
<ul class="simple">
<li><p>Terms where <span class="math notranslate nohighlight">\(i = k\)</span> (varying <span class="math notranslate nohighlight">\(j\)</span>): <span class="math notranslate nohighlight">\(\sum_{j=1}^p \beta_k A_{kj} \beta_j\)</span></p></li>
<li><p>Terms where <span class="math notranslate nohighlight">\(j = k\)</span> (varying <span class="math notranslate nohighlight">\(i\)</span>): <span class="math notranslate nohighlight">\(\sum_{i=1}^p \beta_i A_{ik} \beta_k\)</span></p></li>
</ul>
<p>For the first group:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \beta_k} \sum_{j=1}^p \beta_k A_{kj} \beta_j = \sum_{j=1}^p A_{kj} \beta_j = [\mathbf{A}\boldsymbol{\beta}]_k\]</div>
<p>For the second group:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \beta_k} \sum_{i=1}^p \beta_i A_{ik} \beta_k = \sum_{i=1}^p A_{ik} \beta_i = [\mathbf{A}^\top\boldsymbol{\beta}]_k\]</div>
<p>Combining:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial f}{\partial \beta_k} = [\mathbf{A}\boldsymbol{\beta}]_k + [\mathbf{A}^\top\boldsymbol{\beta}]_k\]</div>
<p>Assembling into vector form:</p>
<div class="math notranslate nohighlight" id="equation-quadratic-deriv">
<span class="eqno">(88)<a class="headerlink" href="#equation-quadratic-deriv" title="Link to this equation"></a></span>\[\boxed{\frac{\partial}{\partial \boldsymbol{\beta}} (\boldsymbol{\beta}^\top \mathbf{A} \boldsymbol{\beta}) = (\mathbf{A} + \mathbf{A}^\top)\boldsymbol{\beta}}\]</div>
<p><strong>Special case</strong>: If <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is symmetric (<span class="math notranslate nohighlight">\(\mathbf{A} = \mathbf{A}^\top\)</span>):</p>
<div class="math notranslate nohighlight" id="equation-quadratic-symmetric">
<span class="eqno">(89)<a class="headerlink" href="#equation-quadratic-symmetric" title="Link to this equation"></a></span>\[\boxed{\frac{\partial}{\partial \boldsymbol{\beta}} (\boldsymbol{\beta}^\top \mathbf{A} \boldsymbol{\beta}) = 2\mathbf{A}\boldsymbol{\beta} \quad \text{(when } \mathbf{A} = \mathbf{A}^\top)}\]</div>
<p>This is the result we’ll use most often, since <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{X}\)</span> is always symmetric.</p>
<p><strong>Rule 3: Matrix-vector product</strong> <span class="math notranslate nohighlight">\(f(\boldsymbol{\beta}) = \mathbf{A}\boldsymbol{\beta}\)</span> where the result is a vector, not a scalar.</p>
<p>Here <span class="math notranslate nohighlight">\(f: \mathbb{R}^p \to \mathbb{R}^n\)</span>, so the derivative is a matrix called the <strong>Jacobian</strong>:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial (\mathbf{A}\boldsymbol{\beta})}{\partial \boldsymbol{\beta}^\top} = \mathbf{A} \in \mathbb{R}^{n \times p}\]</div>
<p>The <span class="math notranslate nohighlight">\((i, j)\)</span> element is <span class="math notranslate nohighlight">\(\frac{\partial [\mathbf{A}\boldsymbol{\beta}]_i}{\partial \beta_j} = A_{ij}\)</span>.</p>
</section>
<section id="summary-of-key-rules">
<h3>Summary of Key Rules<a class="headerlink" href="#summary-of-key-rules" title="Link to this heading"></a></h3>
<table class="docutils align-default" id="id1">
<caption><span class="caption-number">Table 32 </span><span class="caption-text">Matrix Calculus Rules for Scalar Functions</span><a class="headerlink" href="#id1" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 40.0%" />
<col style="width: 35.0%" />
<col style="width: 25.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Expression <span class="math notranslate nohighlight">\(f(\boldsymbol{\beta})\)</span></p></th>
<th class="head"><p>Derivative <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial \boldsymbol{\beta}}\)</span></p></th>
<th class="head"><p>Condition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\mathbf{a}^\top \boldsymbol{\beta}\)</span> or <span class="math notranslate nohighlight">\(\boldsymbol{\beta}^\top \mathbf{a}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{a}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{a}\)</span> constant</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\boldsymbol{\beta}^\top \mathbf{A} \boldsymbol{\beta}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((\mathbf{A} + \mathbf{A}^\top)\boldsymbol{\beta}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{A}\)</span> constant</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\boldsymbol{\beta}^\top \mathbf{A} \boldsymbol{\beta}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(2\mathbf{A}\boldsymbol{\beta}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{A}\)</span> symmetric</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-2\mathbf{X}^\top(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})\)</span></p></td>
<td><p>See derivation below</p></td>
</tr>
</tbody>
</table>
</section>
<section id="the-chain-rule-for-vectors">
<h3>The Chain Rule for Vectors<a class="headerlink" href="#the-chain-rule-for-vectors" title="Link to this heading"></a></h3>
<p>For composite functions, we need the chain rule. If <span class="math notranslate nohighlight">\(f: \mathbb{R}^n \to \mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{g}: \mathbb{R}^p \to \mathbb{R}^n\)</span>, then for <span class="math notranslate nohighlight">\(h(\boldsymbol{\beta}) = f(\mathbf{g}(\boldsymbol{\beta}))\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-chain-rule">
<span class="eqno">(90)<a class="headerlink" href="#equation-chain-rule" title="Link to this equation"></a></span>\[\frac{\partial h}{\partial \boldsymbol{\beta}} = \left(\frac{\partial \mathbf{g}}{\partial \boldsymbol{\beta}^\top}\right)^\top \frac{\partial f}{\partial \mathbf{g}}\]</div>
<p>This is the matrix form of the familiar chain rule: derivative of outer function times derivative of inner function.</p>
</section>
</section>
<section id="the-linear-model">
<h2>The Linear Model<a class="headerlink" href="#the-linear-model" title="Link to this heading"></a></h2>
<p>With matrix calculus tools in hand, we now formalize the linear regression model.</p>
<section id="model-specification">
<h3>Model Specification<a class="headerlink" href="#model-specification" title="Link to this heading"></a></h3>
<p>We observe <span class="math notranslate nohighlight">\(n\)</span> data points <span class="math notranslate nohighlight">\((y_i, \mathbf{x}_i)\)</span> where <span class="math notranslate nohighlight">\(y_i \in \mathbb{R}\)</span> is the <strong>response</strong> (dependent variable) and <span class="math notranslate nohighlight">\(\mathbf{x}_i \in \mathbb{R}^p\)</span> is the vector of <strong>predictors</strong> (independent variables, features, covariates).</p>
<p>The <strong>linear model</strong> posits:</p>
<div class="math notranslate nohighlight" id="equation-linear-model">
<span class="eqno">(91)<a class="headerlink" href="#equation-linear-model" title="Link to this equation"></a></span>\[y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_{p-1} x_{i,p-1} + \varepsilon_i, \quad i = 1, \ldots, n\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\beta_0, \beta_1, \ldots, \beta_{p-1}\)</span> are unknown <strong>parameters</strong> (coefficients)</p></li>
<li><p><span class="math notranslate nohighlight">\(\varepsilon_i\)</span> is the <strong>error term</strong> (disturbance, noise)</p></li>
</ul>
<p><strong>Matrix notation</strong>: Define the <strong>design matrix</strong> <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and <strong>response vector</strong> <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{X} = \begin{pmatrix}
1 &amp; x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1,p-1} \\
1 &amp; x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2,p-1} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{n,p-1}
\end{pmatrix} \in \mathbb{R}^{n \times p}, \quad
\mathbf{y} = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix} \in \mathbb{R}^n\end{split}\]</div>
<p>The first column of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is all ones (for the intercept). The parameter vector is <span class="math notranslate nohighlight">\(\boldsymbol{\beta} = (\beta_0, \beta_1, \ldots, \beta_{p-1})^\top \in \mathbb{R}^p\)</span>.</p>
<p>The model in matrix form:</p>
<div class="math notranslate nohighlight" id="equation-matrix-model">
<span class="eqno">(92)<a class="headerlink" href="#equation-matrix-model" title="Link to this equation"></a></span>\[\boxed{\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}}\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon} = (\varepsilon_1, \ldots, \varepsilon_n)^\top\)</span>.</p>
</section>
<section id="what-linear-means">
<h3>What “Linear” Means<a class="headerlink" href="#what-linear-means" title="Link to this heading"></a></h3>
<p>The model is <strong>linear in the parameters</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>, not necessarily in the predictors. All of the following are linear models:</p>
<div class="math notranslate nohighlight">
\[\begin{split}y &amp;= \beta_0 + \beta_1 x + \varepsilon &amp; \text{(simple linear regression)} \\
y &amp;= \beta_0 + \beta_1 x + \beta_2 x^2 + \varepsilon &amp; \text{(polynomial regression)} \\
y &amp;= \beta_0 + \beta_1 \log(x) + \varepsilon &amp; \text{(log transformation)} \\
y &amp;= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + \varepsilon &amp; \text{(interaction term)}\end{split}\]</div>
<p>Each involves <span class="math notranslate nohighlight">\(y\)</span> as a linear combination of <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>. The following is <strong>not</strong> a linear model:</p>
<div class="math notranslate nohighlight">
\[y = \beta_0 e^{\beta_1 x} + \varepsilon\]</div>
<p>Here <span class="math notranslate nohighlight">\(y\)</span> is nonlinear in <span class="math notranslate nohighlight">\(\beta_1\)</span>.</p>
</section>
<section id="interpreting-coefficients">
<h3>Interpreting Coefficients<a class="headerlink" href="#interpreting-coefficients" title="Link to this heading"></a></h3>
<p>Before diving into estimation, let’s clarify what the coefficients mean:</p>
<p><strong>Partial slopes</strong>: In multiple regression, <span class="math notranslate nohighlight">\(\beta_j\)</span> represents the expected change in <span class="math notranslate nohighlight">\(y\)</span> for a one-unit increase in <span class="math notranslate nohighlight">\(x_j\)</span>, <strong>holding all other predictors fixed</strong>. This “ceteris paribus” interpretation is fundamental.</p>
<p><strong>Correlation vs. causation</strong>: The OLS coefficient <span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span> measures the <strong>association</strong> between <span class="math notranslate nohighlight">\(x_j\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, not necessarily a causal effect. To interpret <span class="math notranslate nohighlight">\(\beta_j\)</span> causally, we need:</p>
<ol class="arabic simple">
<li><p>No omitted confounders (variables that affect both <span class="math notranslate nohighlight">\(x_j\)</span> and <span class="math notranslate nohighlight">\(y\)</span>)</p></li>
<li><p>No reverse causation (<span class="math notranslate nohighlight">\(y\)</span> doesn’t cause <span class="math notranslate nohighlight">\(x_j\)</span>)</p></li>
<li><p>No measurement error in predictors</p></li>
</ol>
<p>These are strong assumptions rarely satisfied in observational data. Experimental design (randomization) or quasi-experimental methods (instrumental variables, regression discontinuity) are needed for causal claims.</p>
<p><strong>Effect of correlation among predictors</strong>: When predictors are correlated, “holding other predictors fixed” may describe a scenario rarely observed in the data. The coefficient <span class="math notranslate nohighlight">\(\beta_j\)</span> still has a well-defined mathematical interpretation, but its practical meaning becomes murky. High multicollinearity inflates standard errors, making individual coefficients imprecise even when the overall model predicts well.</p>
</section>
<section id="assumptions-the-classical-linear-model">
<h3>Assumptions: The Classical Linear Model<a class="headerlink" href="#assumptions-the-classical-linear-model" title="Link to this heading"></a></h3>
<p>The linear model <span class="math notranslate nohighlight">\(\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}\)</span> comes with assumptions about the errors <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon}\)</span>. We distinguish two levels:</p>
<p><strong>Gauss-Markov Assumptions</strong> (for OLS to be BLUE):</p>
<div class="note admonition">
<p class="admonition-title">Assumption (GM1): Linearity</p>
<p>The true relationship is <span class="math notranslate nohighlight">\(\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}\)</span> for some <span class="math notranslate nohighlight">\(\boldsymbol{\beta} \in \mathbb{R}^p\)</span>.</p>
</div>
<div class="note admonition">
<p class="admonition-title">Assumption (GM2): Full Rank</p>
<p>The design matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> has full column rank: <span class="math notranslate nohighlight">\(\text{rank}(\mathbf{X}) = p\)</span>.</p>
<p>Equivalently, <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{X}\)</span> is invertible. This requires <span class="math notranslate nohighlight">\(n \geq p\)</span> and no exact multicollinearity among predictors.</p>
</div>
<div class="note admonition">
<p class="admonition-title">Assumption (GM3): Strict Exogeneity</p>
<p>The errors have zero conditional mean given <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[\boldsymbol{\varepsilon} | \mathbf{X}] = \mathbf{0}\]</div>
<p>This implies <span class="math notranslate nohighlight">\(\mathbb{E}[\varepsilon_i | \mathbf{X}] = 0\)</span> for each <span class="math notranslate nohighlight">\(i\)</span>. Predictors are uncorrelated with errors.</p>
</div>
<div class="note admonition">
<p class="admonition-title">Assumption (GM4): Homoskedastic and Uncorrelated Errors</p>
<p>The errors are homoskedastic and uncorrelated:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\boldsymbol{\varepsilon} | \mathbf{X}) = \sigma^2 \mathbf{I}_n\]</div>
<p>This means:</p>
<ul class="simple">
<li><p><strong>Homoskedasticity</strong>: <span class="math notranslate nohighlight">\(\text{Var}(\varepsilon_i | \mathbf{X}) = \sigma^2\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> (constant variance)</p></li>
<li><p><strong>No autocorrelation</strong>: <span class="math notranslate nohighlight">\(\text{Cov}(\varepsilon_i, \varepsilon_j | \mathbf{X}) = 0\)</span> for <span class="math notranslate nohighlight">\(i \neq j\)</span></p></li>
</ul>
<p>(This condition is sometimes called “spherical errors” because the error covariance is a scaled identity matrix.)</p>
</div>
<p><strong>Normality Assumption</strong> (for exact inference):</p>
<div class="note admonition">
<p class="admonition-title">Assumption (GM5): Normal Errors</p>
<p>The errors follow a multivariate normal distribution:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\varepsilon} | \mathbf{X} \sim \mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I}_n)\]</div>
<p>This implies GM3 and GM4, plus enables exact t-tests and F-tests. Note that this is specifically the <strong>spherical normal</strong> model <span class="math notranslate nohighlight">\(\mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I})\)</span>, not merely “some normal covariance”—normality alone does not guarantee homoskedasticity or independence.</p>
</div>
<p>The assumptions form a hierarchy:</p>
<ul class="simple">
<li><p><strong>GM1-GM2</strong>: OLS is well-defined (unique solution exists)</p></li>
<li><p><strong>GM1-GM4</strong>: OLS is BLUE (Gauss-Markov theorem applies)</p></li>
<li><p><strong>GM1-GM5</strong>: Exact finite-sample inference (t, F distributions)</p></li>
</ul>
</section>
</section>
<section id="ordinary-least-squares-the-calculus-approach">
<h2>Ordinary Least Squares: The Calculus Approach<a class="headerlink" href="#ordinary-least-squares-the-calculus-approach" title="Link to this heading"></a></h2>
<p>The <strong>Ordinary Least Squares (OLS)</strong> estimator minimizes the sum of squared residuals:</p>
<div class="math notranslate nohighlight" id="equation-rss-def">
<span class="eqno">(93)<a class="headerlink" href="#equation-rss-def" title="Link to this equation"></a></span>\[\text{RSS}(\boldsymbol{\beta}) = \sum_{i=1}^n (y_i - \mathbf{x}_i^\top \boldsymbol{\beta})^2 = \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^\top(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})\]</div>
<p>We find <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> that minimizes RSS by taking the derivative and setting it to zero.</p>
<section id="expanding-the-objective-function">
<h3>Expanding the Objective Function<a class="headerlink" href="#expanding-the-objective-function" title="Link to this heading"></a></h3>
<p>First, expand the quadratic form:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{RSS}(\boldsymbol{\beta}) &amp;= (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^\top(\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) \\
&amp;= \mathbf{y}^\top\mathbf{y} - \mathbf{y}^\top\mathbf{X}\boldsymbol{\beta} - \boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{y} + \boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{X}\boldsymbol{\beta}\end{split}\]</div>
<p>Since <span class="math notranslate nohighlight">\(\mathbf{y}^\top\mathbf{X}\boldsymbol{\beta}\)</span> is a scalar, it equals its transpose <span class="math notranslate nohighlight">\(\boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{y}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-rss-expanded">
<span class="eqno">(94)<a class="headerlink" href="#equation-rss-expanded" title="Link to this equation"></a></span>\[\text{RSS}(\boldsymbol{\beta}) = \mathbf{y}^\top\mathbf{y} - 2\boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{y} + \boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{X}\boldsymbol{\beta}\]</div>
</section>
<section id="taking-the-derivative">
<h3>Taking the Derivative<a class="headerlink" href="#taking-the-derivative" title="Link to this heading"></a></h3>
<p>Using our matrix calculus rules:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\frac{\partial}{\partial \boldsymbol{\beta}}(\mathbf{y}^\top\mathbf{y}) = \mathbf{0}\)</span> (constant in <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(\frac{\partial}{\partial \boldsymbol{\beta}}(2\boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{y}) = 2\mathbf{X}^\top\mathbf{y}\)</span> (Rule 1: linear form with <span class="math notranslate nohighlight">\(\mathbf{a} = \mathbf{X}^\top\mathbf{y}\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(\frac{\partial}{\partial \boldsymbol{\beta}}(\boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{X}\boldsymbol{\beta}) = 2\mathbf{X}^\top\mathbf{X}\boldsymbol{\beta}\)</span> (Rule 2: quadratic form with symmetric <span class="math notranslate nohighlight">\(\mathbf{A} = \mathbf{X}^\top\mathbf{X}\)</span>)</p></li>
</ul>
<p>Combining:</p>
<div class="math notranslate nohighlight" id="equation-rss-gradient">
<span class="eqno">(95)<a class="headerlink" href="#equation-rss-gradient" title="Link to this equation"></a></span>\[\frac{\partial \text{RSS}}{\partial \boldsymbol{\beta}} = -2\mathbf{X}^\top\mathbf{y} + 2\mathbf{X}^\top\mathbf{X}\boldsymbol{\beta} = 2\mathbf{X}^\top(\mathbf{X}\boldsymbol{\beta} - \mathbf{y})\]</div>
</section>
<section id="the-normal-equations">
<h3>The Normal Equations<a class="headerlink" href="#the-normal-equations" title="Link to this heading"></a></h3>
<p>Setting the gradient to zero:</p>
<div class="math notranslate nohighlight">
\[\mathbf{X}^\top\mathbf{X}\boldsymbol{\beta} - \mathbf{X}^\top\mathbf{y} = \mathbf{0}\]</div>
<p>Rearranging:</p>
<div class="math notranslate nohighlight" id="equation-normal-equations">
<span class="eqno">(96)<a class="headerlink" href="#equation-normal-equations" title="Link to this equation"></a></span>\[\boxed{\mathbf{X}^\top\mathbf{X}\hat{\boldsymbol{\beta}} = \mathbf{X}^\top\mathbf{y}}\]</div>
<p>These are the <strong>normal equations</strong>. The name comes from the fact that the residual vector <span class="math notranslate nohighlight">\(\mathbf{y} - \mathbf{X}\hat{\boldsymbol{\beta}}\)</span> is <strong>normal (perpendicular)</strong> to the column space of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>—a fact we’ll prove geometrically.</p>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_4_fig02_normal_equations.png"><img alt="Two approaches to deriving normal equations: calculus and geometry" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_4_fig02_normal_equations.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 103 </span><span class="caption-text"><strong>Figure 3.4.2</strong>: The normal equations derived two ways. (a) The calculus approach: minimize RSS by setting the gradient to zero. (b) The geometric approach: the residual <span class="math notranslate nohighlight">\(\mathbf{e}\)</span> must be perpendicular to <span class="math notranslate nohighlight">\(C(\mathbf{X})\)</span>, leading to <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{e} = \mathbf{0}\)</span>.</span><a class="headerlink" href="#id2" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="solving-for-the-ols-estimator">
<h3>Solving for the OLS Estimator<a class="headerlink" href="#solving-for-the-ols-estimator" title="Link to this heading"></a></h3>
<p>Under assumption GM2 (<span class="math notranslate nohighlight">\(\text{rank}(\mathbf{X}) = p\)</span>), the matrix <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{X}\)</span> is invertible. Multiplying both sides by <span class="math notranslate nohighlight">\((\mathbf{X}^\top\mathbf{X})^{-1}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-ols-estimator">
<span class="eqno">(97)<a class="headerlink" href="#equation-ols-estimator" title="Link to this equation"></a></span>\[\boxed{\hat{\boldsymbol{\beta}} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}}\]</div>
<p>This is the <strong>OLS estimator</strong>—the central result of regression analysis.</p>
</section>
<section id="verifying-the-minimum">
<h3>Verifying the Minimum<a class="headerlink" href="#verifying-the-minimum" title="Link to this heading"></a></h3>
<p>We found a critical point. To confirm it’s a minimum (not maximum or saddle), examine the second derivative (Hessian):</p>
<div class="math notranslate nohighlight">
\[\frac{\partial^2 \text{RSS}}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^\top} = 2\mathbf{X}^\top\mathbf{X}\]</div>
<p>Since <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{X}\)</span> is positive semi-definite (for any <span class="math notranslate nohighlight">\(\mathbf{v}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{v}^\top\mathbf{X}^\top\mathbf{X}\mathbf{v} = \|\mathbf{X}\mathbf{v}\|^2 \geq 0\)</span>), and positive definite when <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> has full column rank, the Hessian is positive definite. Therefore, the critical point is indeed a <strong>global minimum</strong>.</p>
<div class="note admonition">
<p class="admonition-title">Example 💡 Simple Linear Regression by Hand</p>
<p>Consider simple regression <span class="math notranslate nohighlight">\(y = \beta_0 + \beta_1 x + \varepsilon\)</span> with <span class="math notranslate nohighlight">\(n = 4\)</span> observations:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{X} = \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; 2 \\ 1 &amp; 3 \\ 1 &amp; 4 \end{pmatrix}, \quad
\mathbf{y} = \begin{pmatrix} 2.1 \\ 3.9 \\ 6.2 \\ 7.8 \end{pmatrix}\end{split}\]</div>
<p><strong>Step 1</strong>: Compute <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{X}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{X}^\top\mathbf{X} = \begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 2 &amp; 3 &amp; 4 \end{pmatrix}
\begin{pmatrix} 1 &amp; 1 \\ 1 &amp; 2 \\ 1 &amp; 3 \\ 1 &amp; 4 \end{pmatrix}
= \begin{pmatrix} 4 &amp; 10 \\ 10 &amp; 30 \end{pmatrix}\end{split}\]</div>
<p><strong>Step 2</strong>: Compute <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{y}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{X}^\top\mathbf{y} = \begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 2 &amp; 3 &amp; 4 \end{pmatrix}
\begin{pmatrix} 2.1 \\ 3.9 \\ 6.2 \\ 7.8 \end{pmatrix}
= \begin{pmatrix} 20.0 \\ 58.1 \end{pmatrix}\end{split}\]</div>
<p><strong>Step 3</strong>: Compute <span class="math notranslate nohighlight">\((\mathbf{X}^\top\mathbf{X})^{-1}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}(\mathbf{X}^\top\mathbf{X})^{-1} = \frac{1}{4 \cdot 30 - 10 \cdot 10}
\begin{pmatrix} 30 &amp; -10 \\ -10 &amp; 4 \end{pmatrix}
= \frac{1}{20}\begin{pmatrix} 30 &amp; -10 \\ -10 &amp; 4 \end{pmatrix}
= \begin{pmatrix} 1.5 &amp; -0.5 \\ -0.5 &amp; 0.2 \end{pmatrix}\end{split}\]</div>
<p><strong>Step 4</strong>: Compute <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\hat{\boldsymbol{\beta}} = \begin{pmatrix} 1.5 &amp; -0.5 \\ -0.5 &amp; 0.2 \end{pmatrix}
\begin{pmatrix} 20.0 \\ 58.1 \end{pmatrix}
= \begin{pmatrix} 30 - 29.05 \\ -10 + 11.62 \end{pmatrix}
= \begin{pmatrix} 0.95 \\ 1.62 \end{pmatrix}\end{split}\]</div>
<p>The fitted line is <span class="math notranslate nohighlight">\(\hat{y} = 0.95 + 1.62x\)</span>.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.1</span><span class="p">,</span> <span class="mf">3.9</span><span class="p">,</span> <span class="mf">6.2</span><span class="p">,</span> <span class="mf">7.8</span><span class="p">])</span>

<span class="c1"># OLS via normal equations</span>
<span class="n">XtX</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span>
<span class="n">Xty</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span>
<span class="n">beta_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">XtX</span><span class="p">,</span> <span class="n">Xty</span><span class="p">)</span>  <span class="c1"># More stable than explicit inverse</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X&#39;X =&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">XtX</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">X&#39;y = </span><span class="si">{</span><span class="n">Xty</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">OLS estimates: β̂₀ = </span><span class="si">{</span><span class="n">beta_hat</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, β̂₁ = </span><span class="si">{</span><span class="n">beta_hat</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Verify with fitted values</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta_hat</span>
<span class="n">residuals</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span>
<span class="n">RSS</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">residuals</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Fitted values: </span><span class="si">{</span><span class="n">y_hat</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Residuals: </span><span class="si">{</span><span class="n">residuals</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;RSS = </span><span class="si">{</span><span class="n">RSS</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>X&#39;X =
[[ 4 10]
 [10 30]]

X&#39;y = [20.  58.1]

OLS estimates: β̂₀ = 0.9500, β̂₁ = 1.6200

Fitted values: [2.57 4.19 5.81 7.43]
Residuals: [-0.47 -0.29  0.39  0.37]
RSS = 0.5460
</pre></div>
</div>
</section>
</section>
<section id="ordinary-least-squares-the-geometric-approach">
<h2>Ordinary Least Squares: The Geometric Approach<a class="headerlink" href="#ordinary-least-squares-the-geometric-approach" title="Link to this heading"></a></h2>
<p>The calculus approach found the OLS estimator by optimization. The geometric approach reveals <strong>why</strong> the normal equations work and provides deeper insight into what OLS does.</p>
<section id="column-space-and-projection">
<h3>Column Space and Projection<a class="headerlink" href="#column-space-and-projection" title="Link to this heading"></a></h3>
<p>Consider the design matrix <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{n \times p}\)</span>. Its <strong>column space</strong> (or range) is:</p>
<div class="math notranslate nohighlight" id="equation-column-space">
<span class="eqno">(98)<a class="headerlink" href="#equation-column-space" title="Link to this equation"></a></span>\[\mathcal{C}(\mathbf{X}) = \{\mathbf{X}\boldsymbol{\beta} : \boldsymbol{\beta} \in \mathbb{R}^p\} \subseteq \mathbb{R}^n\]</div>
<p>This is the set of all linear combinations of the columns of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>—a <span class="math notranslate nohighlight">\(p\)</span>-dimensional subspace of <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> (assuming full column rank).</p>
<p>The model says <span class="math notranslate nohighlight">\(\mathbb{E}[\mathbf{y}] = \mathbf{X}\boldsymbol{\beta}\)</span>, so the expected response lies in the column space. The observed <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> typically does <strong>not</strong> lie in <span class="math notranslate nohighlight">\(\mathcal{C}(\mathbf{X})\)</span> because of errors.</p>
<p><strong>Key insight</strong>: OLS finds the point in <span class="math notranslate nohighlight">\(\mathcal{C}(\mathbf{X})\)</span> closest to <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>.</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_4_fig01_ols_geometry.png"><img alt="Geometric interpretation of OLS as orthogonal projection onto a hyperplane" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_4_fig01_ols_geometry.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 104 </span><span class="caption-text"><strong>Figure 3.4.1</strong>: OLS as orthogonal projection. The observed <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> lives in <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>, while the column space <span class="math notranslate nohighlight">\(C(\mathbf{X})\)</span> is a p-dimensional hyperplane. The fitted values <span class="math notranslate nohighlight">\(\hat{\mathbf{y}} = \mathbf{X}\hat{\boldsymbol{\beta}}\)</span> are the closest point to <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> in this subspace. The residual <span class="math notranslate nohighlight">\(\mathbf{e} = \mathbf{y} - \hat{\mathbf{y}}\)</span> is perpendicular to the entire column space.</span><a class="headerlink" href="#id3" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="the-orthogonality-principle">
<h3>The Orthogonality Principle<a class="headerlink" href="#the-orthogonality-principle" title="Link to this heading"></a></h3>
<p>The closest point to <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> in a subspace is the <strong>orthogonal projection</strong>. The projection <span class="math notranslate nohighlight">\(\hat{\mathbf{y}}\)</span> satisfies:</p>
<div class="math notranslate nohighlight">
\[\mathbf{y} - \hat{\mathbf{y}} \perp \mathcal{C}(\mathbf{X})\]</div>
<p>This means the residual <span class="math notranslate nohighlight">\(\mathbf{e} = \mathbf{y} - \hat{\mathbf{y}}\)</span> is orthogonal to every vector in the column space. Since any vector in <span class="math notranslate nohighlight">\(\mathcal{C}(\mathbf{X})\)</span> can be written as <span class="math notranslate nohighlight">\(\mathbf{X}\mathbf{a}\)</span> for some <span class="math notranslate nohighlight">\(\mathbf{a} \in \mathbb{R}^p\)</span>:</p>
<div class="math notranslate nohighlight">
\[(\mathbf{X}\mathbf{a})^\top (\mathbf{y} - \mathbf{X}\hat{\boldsymbol{\beta}}) = 0 \quad \text{for all } \mathbf{a} \in \mathbb{R}^p\]</div>
<p>This requires:</p>
<div class="math notranslate nohighlight">
\[\mathbf{a}^\top \mathbf{X}^\top (\mathbf{y} - \mathbf{X}\hat{\boldsymbol{\beta}}) = 0 \quad \text{for all } \mathbf{a}\]</div>
<p>The only way this holds for all <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> is if:</p>
<div class="math notranslate nohighlight">
\[\mathbf{X}^\top (\mathbf{y} - \mathbf{X}\hat{\boldsymbol{\beta}}) = \mathbf{0}\]</div>
<p>Rearranging:</p>
<div class="math notranslate nohighlight">
\[\mathbf{X}^\top\mathbf{X}\hat{\boldsymbol{\beta}} = \mathbf{X}^\top\mathbf{y}\]</div>
<p>We’ve recovered the normal equations geometrically! The name “normal equations” now makes sense: they express that residuals are <strong>normal</strong> (perpendicular) to the column space.</p>
</section>
<section id="the-hat-matrix">
<h3>The Hat Matrix<a class="headerlink" href="#the-hat-matrix" title="Link to this heading"></a></h3>
<p>The <strong>hat matrix</strong> (or projection matrix) projects <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> onto <span class="math notranslate nohighlight">\(\mathcal{C}(\mathbf{X})\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-hat-matrix">
<span class="eqno">(99)<a class="headerlink" href="#equation-hat-matrix" title="Link to this equation"></a></span>\[\mathbf{H} = \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\]</div>
<p>The fitted values are:</p>
<div class="math notranslate nohighlight">
\[\hat{\mathbf{y}} = \mathbf{X}\hat{\boldsymbol{\beta}} = \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y} = \mathbf{H}\mathbf{y}\]</div>
<p>The name “hat matrix” comes from putting a hat on <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> to get <span class="math notranslate nohighlight">\(\hat{\mathbf{y}}\)</span>.</p>
<p><strong>Properties of the hat matrix</strong>:</p>
<ol class="arabic">
<li><p><strong>Symmetric</strong>: <span class="math notranslate nohighlight">\(\mathbf{H}^\top = \mathbf{H}\)</span></p>
<p><em>Proof</em>: <span class="math notranslate nohighlight">\(\mathbf{H}^\top = [\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top]^\top = \mathbf{X}[(\mathbf{X}^\top\mathbf{X})^{-1}]^\top\mathbf{X}^\top = \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top = \mathbf{H}\)</span></p>
</li>
<li><p><strong>Idempotent</strong>: <span class="math notranslate nohighlight">\(\mathbf{H}^2 = \mathbf{H}\)</span> (projecting twice gives the same result)</p>
<p><em>Proof</em>: <span class="math notranslate nohighlight">\(\mathbf{H}^2 = \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top = \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top = \mathbf{H}\)</span></p>
</li>
<li><p><strong>Trace</strong>: <span class="math notranslate nohighlight">\(\text{tr}(\mathbf{H}) = p\)</span></p>
<p><em>Proof</em>: Using <span class="math notranslate nohighlight">\(\text{tr}(\mathbf{AB}) = \text{tr}(\mathbf{BA})\)</span>:
<span class="math notranslate nohighlight">\(\text{tr}(\mathbf{H}) = \text{tr}(\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top) = \text{tr}((\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{X}) = \text{tr}(\mathbf{I}_p) = p\)</span></p>
</li>
<li><p><strong>Leverage</strong>: The diagonal elements <span class="math notranslate nohighlight">\(h_{ii} \in [0, 1]\)</span> measure how much observation <span class="math notranslate nohighlight">\(i\)</span> influences its own fitted value.</p></li>
</ol>
<p>The residual vector can also be written using a projection matrix:</p>
<div class="math notranslate nohighlight">
\[\mathbf{e} = \mathbf{y} - \hat{\mathbf{y}} = (\mathbf{I} - \mathbf{H})\mathbf{y} = \mathbf{M}\mathbf{y}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{M} = \mathbf{I} - \mathbf{H}\)</span> projects onto the <strong>orthogonal complement</strong> of <span class="math notranslate nohighlight">\(\mathcal{C}(\mathbf{X})\)</span>.</p>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_4_fig04_hat_matrix.png"><img alt="The hat matrix visualized as a projection operator" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_4_fig04_hat_matrix.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 105 </span><span class="caption-text"><strong>Figure 3.4.3</strong>: The hat matrix <span class="math notranslate nohighlight">\(\mathbf{H} = \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\)</span> “puts a hat on y.” (a) Conceptual view: H transforms observed y into fitted ŷ. (b) Matrix visualization showing larger diagonal elements (leverage) for extreme observations. (c) The projection: H maps each <span class="math notranslate nohighlight">\(y_i\)</span> to <span class="math notranslate nohighlight">\(\hat{y}_i\)</span>.</span><a class="headerlink" href="#id4" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="rank-pseudo-inverse-and-when-solutions-exist">
<h3>Rank, Pseudo-Inverse, and When Solutions Exist<a class="headerlink" href="#rank-pseudo-inverse-and-when-solutions-exist" title="Link to this heading"></a></h3>
<p>When <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> has full column rank (<span class="math notranslate nohighlight">\(\text{rank}(\mathbf{X}) = p\)</span>), the solution is unique:</p>
<div class="math notranslate nohighlight">
\[\hat{\boldsymbol{\beta}} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}\]</div>
<p>The matrix <span class="math notranslate nohighlight">\((\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\)</span> is the <strong>left pseudo-inverse</strong> of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, denoted <span class="math notranslate nohighlight">\(\mathbf{X}^+\)</span>. It satisfies <span class="math notranslate nohighlight">\(\mathbf{X}^+\mathbf{X} = \mathbf{I}_p\)</span>.</p>
<p><strong>What if</strong> <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> <strong>doesn’t have full rank?</strong></p>
<p>When <span class="math notranslate nohighlight">\(\text{rank}(\mathbf{X}) = r &lt; p\)</span> (multicollinearity), <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{X}\)</span> is singular and not invertible. The normal equations <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{X}\boldsymbol{\beta} = \mathbf{X}^\top\mathbf{y}\)</span> still have solutions, but <strong>infinitely many</strong>—the solution set is a <span class="math notranslate nohighlight">\((p-r)\)</span>-dimensional affine subspace.</p>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_4_fig12_multicollinearity.png"><img alt="Multicollinearity effects on coefficient estimation" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_4_fig12_multicollinearity.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 106 </span><span class="caption-text"><strong>Figure 3.4.12</strong>: Multicollinearity inflates standard errors. (a) VIF grows rapidly as correlation approaches 1: <span class="math notranslate nohighlight">\(\text{VIF} = 1/(1-\rho^2)\)</span>. (b) With high correlation (<span class="math notranslate nohighlight">\(\rho = 0.95\)</span>), the sampling distribution of <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> is much wider than with uncorrelated predictors. (c) The joint confidence region becomes an elongated ellipse when predictors are correlated.</span><a class="headerlink" href="#id5" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>In this case, we use the <strong>Moore-Penrose pseudo-inverse</strong> <span class="math notranslate nohighlight">\(\mathbf{X}^+\)</span>, which selects the minimum-norm solution:</p>
<div class="math notranslate nohighlight">
\[\hat{\boldsymbol{\beta}} = \mathbf{X}^+ \mathbf{y}\]</div>
<p>This solution has the smallest <span class="math notranslate nohighlight">\(\|\boldsymbol{\beta}\|\)</span> among all solutions to the normal equations.</p>
<p><strong>In practice</strong>: Software like <code class="docutils literal notranslate"><span class="pre">numpy.linalg.lstsq</span></code> and <code class="docutils literal notranslate"><span class="pre">scipy.linalg.lstsq</span></code> use SVD-based methods that handle rank deficiency automatically.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Example with exact multicollinearity</span>
<span class="c1"># x3 = x1 + x2 (perfect linear dependence)</span>
<span class="n">X_singular</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>

<span class="c1"># Check rank</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Matrix shape: </span><span class="si">{</span><span class="n">X_singular</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Rank: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="n">X_singular</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;X&#39;X is singular: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">X_singular</span><span class="o">.</span><span class="n">T</span><span class="w"> </span><span class="o">@</span><span class="w"> </span><span class="n">X_singular</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># lstsq handles rank deficiency via SVD</span>
<span class="n">beta_hat</span><span class="p">,</span> <span class="n">residuals</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X_singular</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Minimum-norm solution: </span><span class="si">{</span><span class="n">beta_hat</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Singular values: </span><span class="si">{</span><span class="n">s</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Matrix shape: (4, 4)
Rank: 3
X&#39;X is singular: True

Minimum-norm solution: [-0.  0.5 1.5 0. ]
Singular values: [5.52 1.73 0.71 0.  ]
</pre></div>
</div>
</section>
</section>
<section id="properties-of-the-ols-estimator">
<h2>Properties of the OLS Estimator<a class="headerlink" href="#properties-of-the-ols-estimator" title="Link to this heading"></a></h2>
<p>Having derived the OLS estimator, we now examine its statistical properties.</p>
<div class="tip admonition">
<p class="admonition-title">Convention: Fixed Design</p>
<p>Throughout this section, we treat <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> as <strong>fixed</strong> (non-random) and condition all statements on <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. Expectations, variances, and distributions are conditional on <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. This “fixed design” viewpoint is standard in classical regression theory. When <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is random, results hold conditional on the observed <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p>
</div>
<section id="unbiasedness">
<h3>Unbiasedness<a class="headerlink" href="#unbiasedness" title="Link to this heading"></a></h3>
<div class="note admonition">
<p class="admonition-title">Theorem: OLS is Unbiased</p>
<p>Under assumptions GM1-GM3, the OLS estimator is unbiased:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[\hat{\boldsymbol{\beta}} | \mathbf{X}] = \boldsymbol{\beta}\]</div>
</div>
<p><strong>Proof</strong>:</p>
<p>Starting from <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}\)</span> and substituting <span class="math notranslate nohighlight">\(\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\hat{\boldsymbol{\beta}} &amp;= (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top(\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}) \\
&amp;= (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{X}\boldsymbol{\beta} + (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\boldsymbol{\varepsilon} \\
&amp;= \boldsymbol{\beta} + (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\boldsymbol{\varepsilon}\end{split}\]</div>
<p>Taking expectations conditional on <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[\hat{\boldsymbol{\beta}} | \mathbf{X}] = \boldsymbol{\beta} + (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top \mathbb{E}[\boldsymbol{\varepsilon} | \mathbf{X}] = \boldsymbol{\beta} + (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top \mathbf{0} = \boldsymbol{\beta}\]</div>
<p>where we used GM3: <span class="math notranslate nohighlight">\(\mathbb{E}[\boldsymbol{\varepsilon} | \mathbf{X}] = \mathbf{0}\)</span>. ∎</p>
</section>
<section id="variance-covariance-matrix">
<h3>Variance-Covariance Matrix<a class="headerlink" href="#variance-covariance-matrix" title="Link to this heading"></a></h3>
<div class="note admonition">
<p class="admonition-title">Theorem: Variance of OLS Estimator</p>
<p>Under assumptions GM1-GM4:</p>
<div class="math notranslate nohighlight" id="equation-ols-variance">
<span class="eqno">(100)<a class="headerlink" href="#equation-ols-variance" title="Link to this equation"></a></span>\[\text{Var}(\hat{\boldsymbol{\beta}} | \mathbf{X}) = \sigma^2 (\mathbf{X}^\top\mathbf{X})^{-1}\]</div>
</div>
<p><strong>Proof</strong>:</p>
<p>From above, <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\boldsymbol{\varepsilon}\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{A} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\)</span>. Then:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{Var}(\hat{\boldsymbol{\beta}} | \mathbf{X}) &amp;= \text{Var}(\mathbf{A}\boldsymbol{\varepsilon} | \mathbf{X}) \\
&amp;= \mathbf{A} \text{Var}(\boldsymbol{\varepsilon} | \mathbf{X}) \mathbf{A}^\top \\
&amp;= \mathbf{A} (\sigma^2 \mathbf{I}_n) \mathbf{A}^\top \\
&amp;= \sigma^2 \mathbf{A} \mathbf{A}^\top\end{split}\]</div>
<p>Now compute <span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{A}^\top\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{A}\mathbf{A}^\top &amp;= (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top \mathbf{X}[(\mathbf{X}^\top\mathbf{X})^{-1}]^\top \\
&amp;= (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1} \\
&amp;= (\mathbf{X}^\top\mathbf{X})^{-1}\end{split}\]</div>
<p>Therefore:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\hat{\boldsymbol{\beta}} | \mathbf{X}) = \sigma^2 (\mathbf{X}^\top\mathbf{X})^{-1}\]</div>
<p>∎</p>
<p><strong>Interpreting the variance formula</strong>:</p>
<p>The variance of the <span class="math notranslate nohighlight">\(j\)</span>-th coefficient is:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\hat{\beta}_j | \mathbf{X}) = \sigma^2 [(\mathbf{X}^\top\mathbf{X})^{-1}]_{jj}\]</div>
<p>The standard error is:</p>
<div class="math notranslate nohighlight">
\[\text{SE}(\hat{\beta}_j) = \sigma \sqrt{[(\mathbf{X}^\top\mathbf{X})^{-1}]_{jj}}\]</div>
<p>In practice, we replace <span class="math notranslate nohighlight">\(\sigma\)</span> with its estimate <span class="math notranslate nohighlight">\(s\)</span> (see below).</p>
</section>
</section>
<section id="the-gauss-markov-theorem">
<h2>The Gauss-Markov Theorem<a class="headerlink" href="#the-gauss-markov-theorem" title="Link to this heading"></a></h2>
<p>The Gauss-Markov theorem is one of the most important results in regression theory. It establishes OLS as the <strong>Best Linear Unbiased Estimator</strong> (BLUE).</p>
<section id="what-blue-means">
<h3>What BLUE Means<a class="headerlink" href="#what-blue-means" title="Link to this heading"></a></h3>
<p>Let’s unpack each word:</p>
<ul class="simple">
<li><p><strong>Estimator</strong>: A function of the data that produces an estimate of the parameter</p></li>
<li><p><strong>Unbiased</strong>: <span class="math notranslate nohighlight">\(\mathbb{E}[\hat{\boldsymbol{\beta}}] = \boldsymbol{\beta}\)</span> (hits the target on average)</p></li>
<li><p><strong>Linear</strong>: The estimator is a linear function of <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>: <span class="math notranslate nohighlight">\(\tilde{\boldsymbol{\beta}} = \mathbf{C}\mathbf{y}\)</span> for some matrix <span class="math notranslate nohighlight">\(\mathbf{C}\)</span></p></li>
<li><p><strong>Best</strong>: Has the smallest variance among all linear unbiased estimators</p></li>
</ul>
<p><strong>Why focus on linear estimators?</strong></p>
<ol class="arabic simple">
<li><p><strong>Tractability</strong>: Linear estimators have simple variance formulas</p></li>
<li><p><strong>Sufficiency</strong>: For normal errors, linear estimators can be optimal even among nonlinear ones</p></li>
<li><p><strong>Robustness considerations</strong>: Nonlinear estimators may be more efficient under specific distributional assumptions but fail badly when those assumptions are wrong</p></li>
</ol>
</section>
<section id="the-theorem-statement">
<h3>The Theorem Statement<a class="headerlink" href="#the-theorem-statement" title="Link to this heading"></a></h3>
<div class="note admonition">
<p class="admonition-title">Theorem: Gauss-Markov</p>
<p>Under assumptions GM1-GM4 (linearity, full rank, strict exogeneity, spherical errors), the OLS estimator <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}\)</span> is BLUE:</p>
<p>For any other linear unbiased estimator <span class="math notranslate nohighlight">\(\tilde{\boldsymbol{\beta}} = \mathbf{C}\mathbf{y}\)</span> of <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\tilde{\boldsymbol{\beta}}) - \text{Var}(\hat{\boldsymbol{\beta}}) \text{ is positive semi-definite}\]</div>
<p>In particular, for any coefficient:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\tilde{\beta}_j) \geq \text{Var}(\hat{\beta}_j)\]</div>
<p>OLS achieves the minimum variance among all linear unbiased estimators.</p>
</div>
</section>
<section id="complete-proof">
<h3>Complete Proof<a class="headerlink" href="#complete-proof" title="Link to this heading"></a></h3>
<p><strong>Step 1: Characterize linear unbiased estimators</strong></p>
<p>Any linear estimator has the form <span class="math notranslate nohighlight">\(\tilde{\boldsymbol{\beta}} = \mathbf{C}\mathbf{y}\)</span> for some <span class="math notranslate nohighlight">\(p \times n\)</span> matrix <span class="math notranslate nohighlight">\(\mathbf{C}\)</span>.</p>
<p>For <span class="math notranslate nohighlight">\(\tilde{\boldsymbol{\beta}}\)</span> to be unbiased:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[\tilde{\boldsymbol{\beta}}] = \mathbf{C}\mathbb{E}[\mathbf{y}] = \mathbf{C}\mathbf{X}\boldsymbol{\beta} = \boldsymbol{\beta} \quad \text{for all } \boldsymbol{\beta}\]</div>
<p>This requires <span class="math notranslate nohighlight">\(\mathbf{C}\mathbf{X} = \mathbf{I}_p\)</span>.</p>
<p><strong>Step 2: Write any linear unbiased estimator as OLS plus something</strong></p>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{A} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\)</span> be the OLS matrix, so <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}} = \mathbf{A}\mathbf{y}\)</span>.</p>
<p>Define <span class="math notranslate nohighlight">\(\mathbf{D} = \mathbf{C} - \mathbf{A}\)</span>. Then any linear unbiased estimator can be written:</p>
<div class="math notranslate nohighlight">
\[\tilde{\boldsymbol{\beta}} = \mathbf{C}\mathbf{y} = (\mathbf{A} + \mathbf{D})\mathbf{y}\]</div>
<p>Since both <span class="math notranslate nohighlight">\(\mathbf{C}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> satisfy <span class="math notranslate nohighlight">\(\mathbf{C}\mathbf{X} = \mathbf{I}_p\)</span> and <span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{X} = \mathbf{I}_p\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbf{D}\mathbf{X} = \mathbf{C}\mathbf{X} - \mathbf{A}\mathbf{X} = \mathbf{I}_p - \mathbf{I}_p = \mathbf{0}\]</div>
<p><strong>Step 3: Compute the variance of the alternative estimator</strong></p>
<p>Conditional on <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, the variance of any linear estimator <span class="math notranslate nohighlight">\(\tilde{\boldsymbol{\beta}} = \mathbf{C}\mathbf{y}\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{Var}(\tilde{\boldsymbol{\beta}} | \mathbf{X}) &amp;= \text{Var}((\mathbf{A} + \mathbf{D})\mathbf{y} | \mathbf{X}) \\
&amp;= (\mathbf{A} + \mathbf{D}) \text{Var}(\mathbf{y} | \mathbf{X}) (\mathbf{A} + \mathbf{D})^\top \\
&amp;= \sigma^2 (\mathbf{A} + \mathbf{D})(\mathbf{A} + \mathbf{D})^\top \\
&amp;= \sigma^2 (\mathbf{A}\mathbf{A}^\top + \mathbf{A}\mathbf{D}^\top + \mathbf{D}\mathbf{A}^\top + \mathbf{D}\mathbf{D}^\top)\end{split}\]</div>
<p><strong>Step 4: Show the cross-terms vanish</strong></p>
<p>Compute <span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{D}^\top\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbf{A}\mathbf{D}^\top = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top \mathbf{D}^\top\]</div>
<p>Now, <span class="math notranslate nohighlight">\(\mathbf{D}\mathbf{X} = \mathbf{0}\)</span> implies <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{D}^\top = \mathbf{0}\)</span>. Therefore:</p>
<div class="math notranslate nohighlight">
\[\mathbf{A}\mathbf{D}^\top = (\mathbf{X}^\top\mathbf{X})^{-1} \cdot \mathbf{0} = \mathbf{0}\]</div>
<p>Similarly, <span class="math notranslate nohighlight">\(\mathbf{D}\mathbf{A}^\top = (\mathbf{A}\mathbf{D}^\top)^\top = \mathbf{0}\)</span>.</p>
<p><strong>Step 5: Conclude</strong></p>
<div class="math notranslate nohighlight">
\[\text{Var}(\tilde{\boldsymbol{\beta}}) = \sigma^2 (\mathbf{A}\mathbf{A}^\top + \mathbf{D}\mathbf{D}^\top) = \text{Var}(\hat{\boldsymbol{\beta}}) + \sigma^2 \mathbf{D}\mathbf{D}^\top\]</div>
<p>Since <span class="math notranslate nohighlight">\(\mathbf{D}\mathbf{D}^\top\)</span> is positive semi-definite (for any <span class="math notranslate nohighlight">\(\mathbf{v}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{v}^\top\mathbf{D}\mathbf{D}^\top\mathbf{v} = \|\mathbf{D}^\top\mathbf{v}\|^2 \geq 0\)</span>):</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\tilde{\boldsymbol{\beta}}) - \text{Var}(\hat{\boldsymbol{\beta}}) = \sigma^2 \mathbf{D}\mathbf{D}^\top \succeq \mathbf{0}\]</div>
<p>with equality if and only if <span class="math notranslate nohighlight">\(\mathbf{D} = \mathbf{0}\)</span>, i.e., <span class="math notranslate nohighlight">\(\tilde{\boldsymbol{\beta}} = \hat{\boldsymbol{\beta}}\)</span>. ∎</p>
</section>
<section id="what-happens-when-assumptions-fail">
<h3>What Happens When Assumptions Fail<a class="headerlink" href="#what-happens-when-assumptions-fail" title="Link to this heading"></a></h3>
<p>Each Gauss-Markov assumption is critical. Here’s what breaks when each fails:</p>
<table class="docutils align-default" id="id6">
<caption><span class="caption-number">Table 33 </span><span class="caption-text">Consequences of Gauss-Markov Assumption Violations</span><a class="headerlink" href="#id6" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 15.0%" />
<col style="width: 25.0%" />
<col style="width: 30.0%" />
<col style="width: 30.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Assumption</p></th>
<th class="head"><p>Violation Example</p></th>
<th class="head"><p>Consequence for OLS</p></th>
<th class="head"><p>Remedy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>GM1</strong> Linearity</p></td>
<td><p>True model is <span class="math notranslate nohighlight">\(y = \beta_0 + \beta_1 x^2 + \varepsilon\)</span> but we fit <span class="math notranslate nohighlight">\(y = \beta_0 + \beta_1 x\)</span></p></td>
<td><p>OLS is <strong>biased</strong> and <strong>inconsistent</strong>; estimates wrong quantity</p></td>
<td><p>Transform predictors, add polynomial terms, use nonlinear models</p></td>
</tr>
<tr class="row-odd"><td><p><strong>GM2</strong> Full Rank</p></td>
<td><p>Perfect multicollinearity: <span class="math notranslate nohighlight">\(x_3 = 2x_1 + x_2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{X}\)</span> is singular; OLS not unique</p></td>
<td><p>Remove redundant predictors, use regularization (ridge)</p></td>
</tr>
<tr class="row-even"><td><p><strong>GM3</strong> Exogeneity</p></td>
<td><p>Omitted variable correlated with included predictor</p></td>
<td><p>OLS is <strong>biased</strong> and <strong>inconsistent</strong></p></td>
<td><p>Include omitted variable, use instrumental variables, fixed effects</p></td>
</tr>
<tr class="row-odd"><td><p><strong>GM4a</strong> Homoskedasticity</p></td>
<td><p>Variance increases with <span class="math notranslate nohighlight">\(x\)</span>: <span class="math notranslate nohighlight">\(\text{Var}(\varepsilon_i) = \sigma^2 x_i^2\)</span></p></td>
<td><p>OLS unbiased but <strong>not efficient</strong>; SEs wrong</p></td>
<td><p>WLS if form known, robust (sandwich) SEs otherwise</p></td>
</tr>
<tr class="row-even"><td><p><strong>GM4b</strong> No autocorrelation</p></td>
<td><p>Time series: <span class="math notranslate nohighlight">\(\varepsilon_t = \rho\varepsilon_{t-1} + u_t\)</span></p></td>
<td><p>OLS unbiased but <strong>not efficient</strong>; SEs wrong</p></td>
<td><p>GLS, Newey-West SEs, time series models</p></td>
</tr>
</tbody>
</table>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ⚠️ Endogeneity</p>
<p>The most serious violation is <strong>GM3</strong> (exogeneity): the requirement that <span class="math notranslate nohighlight">\(\mathbb{E}[\varepsilon_i | \mathbf{X}] = 0\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>. This “conditional mean zero” condition says that knowing <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> tells you nothing about the expected error.</p>
<p>When <span class="math notranslate nohighlight">\(\mathbb{E}[\varepsilon | \mathbf{X}] \neq \mathbf{0}\)</span>:</p>
<ul class="simple">
<li><p>OLS is <strong>biased</strong> even in large samples (inconsistent)</p></li>
<li><p>No amount of data fixes the problem</p></li>
<li><p><strong>Robust standard errors do not help</strong>: They correct for heteroskedasticity, not bias. If your coefficients are biased, having more precise standard errors for the wrong value doesn’t help!</p></li>
</ul>
<p>Common causes: omitted variables correlated with included predictors, measurement error in <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, simultaneity (reverse causation).</p>
<p>Detection is difficult since we don’t observe <span class="math notranslate nohighlight">\(\varepsilon\)</span>. Economic theory, domain knowledge, and careful study design are essential. Remedies include instrumental variables and natural experiments.</p>
</div>
</section>
</section>
<section id="estimating-the-error-variance">
<h2>Estimating the Error Variance<a class="headerlink" href="#estimating-the-error-variance" title="Link to this heading"></a></h2>
<p>The variance formula <span class="math notranslate nohighlight">\(\text{Var}(\hat{\boldsymbol{\beta}}) = \sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}\)</span> depends on the unknown <span class="math notranslate nohighlight">\(\sigma^2\)</span>. We need to estimate it.</p>
<section id="the-residual-sum-of-squares">
<h3>The Residual Sum of Squares<a class="headerlink" href="#the-residual-sum-of-squares" title="Link to this heading"></a></h3>
<p>Define the <strong>residuals</strong> as <span class="math notranslate nohighlight">\(\mathbf{e} = \mathbf{y} - \hat{\mathbf{y}} = \mathbf{y} - \mathbf{X}\hat{\boldsymbol{\beta}}\)</span>.</p>
<p>The <strong>residual sum of squares</strong> (RSS) is:</p>
<div class="math notranslate nohighlight">
\[\text{RSS} = \mathbf{e}^\top\mathbf{e} = \sum_{i=1}^n e_i^2 = \sum_{i=1}^n (y_i - \hat{y}_i)^2\]</div>
<p>A natural estimator of <span class="math notranslate nohighlight">\(\sigma^2\)</span> might be <span class="math notranslate nohighlight">\(\text{RSS}/n\)</span>. However, this is <strong>biased</strong> because residuals underestimate the true errors.</p>
</section>
<section id="the-unbiased-estimator">
<h3>The Unbiased Estimator<a class="headerlink" href="#the-unbiased-estimator" title="Link to this heading"></a></h3>
<div class="note admonition">
<p class="admonition-title">Theorem: Unbiased Estimator of σ²</p>
<p>Under GM1-GM4:</p>
<div class="math notranslate nohighlight" id="equation-s2-estimator">
<span class="eqno">(101)<a class="headerlink" href="#equation-s2-estimator" title="Link to this equation"></a></span>\[s^2 = \frac{\text{RSS}}{n - p} = \frac{\mathbf{e}^\top\mathbf{e}}{n - p}\]</div>
<p>is an unbiased estimator of <span class="math notranslate nohighlight">\(\sigma^2\)</span>: <span class="math notranslate nohighlight">\(\mathbb{E}[s^2] = \sigma^2\)</span>.</p>
</div>
<p><strong>Proof sketch</strong>:</p>
<p>The residuals can be written as <span class="math notranslate nohighlight">\(\mathbf{e} = \mathbf{M}\mathbf{y} = \mathbf{M}(\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}) = \mathbf{M}\boldsymbol{\varepsilon}\)</span> where <span class="math notranslate nohighlight">\(\mathbf{M} = \mathbf{I} - \mathbf{H}\)</span>.</p>
<p>(Note: <span class="math notranslate nohighlight">\(\mathbf{M}\mathbf{X}\boldsymbol{\beta} = (\mathbf{I} - \mathbf{H})\mathbf{X}\boldsymbol{\beta} = \mathbf{X}\boldsymbol{\beta} - \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{X}\boldsymbol{\beta} = \mathbf{X}\boldsymbol{\beta} - \mathbf{X}\boldsymbol{\beta} = \mathbf{0}\)</span>)</p>
<p>Then:</p>
<div class="math notranslate nohighlight">
\[\text{RSS} = \mathbf{e}^\top\mathbf{e} = \boldsymbol{\varepsilon}^\top\mathbf{M}^\top\mathbf{M}\boldsymbol{\varepsilon} = \boldsymbol{\varepsilon}^\top\mathbf{M}\boldsymbol{\varepsilon}\]</div>
<p>since <span class="math notranslate nohighlight">\(\mathbf{M}\)</span> is idempotent (<span class="math notranslate nohighlight">\(\mathbf{M}^2 = \mathbf{M}\)</span>) and symmetric.</p>
<p>Taking expectations and using the trace formula for quadratic forms:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[\text{RSS}] = \mathbb{E}[\boldsymbol{\varepsilon}^\top\mathbf{M}\boldsymbol{\varepsilon}] = \text{tr}(\mathbf{M} \cdot \sigma^2\mathbf{I}) = \sigma^2 \text{tr}(\mathbf{M})\]</div>
<p>Since <span class="math notranslate nohighlight">\(\text{tr}(\mathbf{M}) = \text{tr}(\mathbf{I} - \mathbf{H}) = n - \text{tr}(\mathbf{H}) = n - p\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[\text{RSS}] = \sigma^2 (n - p)\]</div>
<p>Therefore <span class="math notranslate nohighlight">\(\mathbb{E}[\text{RSS}/(n-p)] = \sigma^2\)</span>. ∎</p>
<p><strong>Degrees of freedom interpretation</strong>: We estimate <span class="math notranslate nohighlight">\(p\)</span> parameters from <span class="math notranslate nohighlight">\(n\)</span> observations, leaving <span class="math notranslate nohighlight">\(n - p\)</span> degrees of freedom for estimating error variance.</p>
<figure class="align-center" id="id7">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_4_fig06_degrees_of_freedom.png"><img alt="Degrees of freedom visualized showing why we divide by n-p" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_4_fig06_degrees_of_freedom.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 107 </span><span class="caption-text"><strong>Figure 3.4.5</strong>: Degrees of freedom explained. (a) With n observations, p are “used up” estimating <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>, leaving n-p to estimate <span class="math notranslate nohighlight">\(\sigma^2\)</span>. (b) Simulation confirms <span class="math notranslate nohighlight">\(s^2 = \text{RSS}/(n-p)\)</span> is unbiased while <span class="math notranslate nohighlight">\(\text{RSS}/n\)</span> underestimates <span class="math notranslate nohighlight">\(\sigma^2\)</span>. (c) Under normality, <span class="math notranslate nohighlight">\(\text{RSS}/\sigma^2 \sim \chi^2_{n-p}\)</span>.</span><a class="headerlink" href="#id7" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="standard-errors-of-coefficients">
<h3>Standard Errors of Coefficients<a class="headerlink" href="#standard-errors-of-coefficients" title="Link to this heading"></a></h3>
<p>The estimated variance-covariance matrix of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\widehat{\text{Var}}(\hat{\boldsymbol{\beta}}) = s^2 (\mathbf{X}^\top\mathbf{X})^{-1}\]</div>
<p>The standard error of the <span class="math notranslate nohighlight">\(j\)</span>-th coefficient is:</p>
<div class="math notranslate nohighlight" id="equation-se-formula">
<span class="eqno">(102)<a class="headerlink" href="#equation-se-formula" title="Link to this equation"></a></span>\[\text{SE}(\hat{\beta}_j) = s \sqrt{[(\mathbf{X}^\top\mathbf{X})^{-1}]_{jj}}\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">ols_with_inference</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Perform OLS regression with inference.</span>

<span class="sd">    Note: This function uses explicit matrix formulas for pedagogical clarity.</span>
<span class="sd">    For production code, use np.linalg.lstsq() or scipy.linalg.lstsq() which</span>
<span class="sd">    are numerically more stable (see Numerical Stability section).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict with beta_hat, se, t_stats, p_values, R2, s2</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># OLS estimates (formula-aligned; use np.linalg.lstsq for production)</span>
    <span class="n">XtX</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span>
    <span class="n">XtX_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">XtX</span><span class="p">)</span>  <span class="c1"># Needed for standard errors</span>
    <span class="n">beta_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">XtX</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># More stable than inv() @ y</span>

    <span class="c1"># Fitted values and residuals</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta_hat</span>
    <span class="n">residuals</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span>

    <span class="c1"># Variance estimation</span>
    <span class="n">RSS</span> <span class="o">=</span> <span class="n">residuals</span> <span class="o">@</span> <span class="n">residuals</span>
    <span class="n">s2</span> <span class="o">=</span> <span class="n">RSS</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s2</span><span class="p">)</span>

    <span class="c1"># Standard errors</span>
    <span class="n">var_beta</span> <span class="o">=</span> <span class="n">s2</span> <span class="o">*</span> <span class="n">XtX_inv</span>
    <span class="n">se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">var_beta</span><span class="p">))</span>

    <span class="c1"># t-statistics and p-values</span>
    <span class="n">t_stats</span> <span class="o">=</span> <span class="n">beta_hat</span> <span class="o">/</span> <span class="n">se</span>
    <span class="n">p_values</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">t_stats</span><span class="p">),</span> <span class="n">df</span><span class="o">=</span><span class="n">n</span><span class="o">-</span><span class="n">p</span><span class="p">))</span>

    <span class="c1"># R-squared</span>
    <span class="n">TSS</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">R2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">RSS</span> <span class="o">/</span> <span class="n">TSS</span>
    <span class="n">R2_adj</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">R2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;beta_hat&#39;</span><span class="p">:</span> <span class="n">beta_hat</span><span class="p">,</span>
        <span class="s1">&#39;se&#39;</span><span class="p">:</span> <span class="n">se</span><span class="p">,</span>
        <span class="s1">&#39;t_stats&#39;</span><span class="p">:</span> <span class="n">t_stats</span><span class="p">,</span>
        <span class="s1">&#39;p_values&#39;</span><span class="p">:</span> <span class="n">p_values</span><span class="p">,</span>
        <span class="s1">&#39;R2&#39;</span><span class="p">:</span> <span class="n">R2</span><span class="p">,</span>
        <span class="s1">&#39;R2_adj&#39;</span><span class="p">:</span> <span class="n">R2_adj</span><span class="p">,</span>
        <span class="s1">&#39;s2&#39;</span><span class="p">:</span> <span class="n">s2</span><span class="p">,</span>
        <span class="s1">&#39;s&#39;</span><span class="p">:</span> <span class="n">s</span><span class="p">,</span>
        <span class="s1">&#39;residuals&#39;</span><span class="p">:</span> <span class="n">residuals</span><span class="p">,</span>
        <span class="s1">&#39;y_hat&#39;</span><span class="p">:</span> <span class="n">y_hat</span>
    <span class="p">}</span>

<span class="c1"># Example: mtcars-style data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">hp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>  <span class="c1"># horsepower</span>
<span class="n">wt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>  <span class="c1"># weight (1000 lbs)</span>
<span class="n">mpg</span> <span class="o">=</span> <span class="mi">40</span> <span class="o">-</span> <span class="mf">0.03</span> <span class="o">*</span> <span class="n">hp</span> <span class="o">-</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">wt</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">hp</span><span class="p">,</span> <span class="n">wt</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">mpg</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">ols_with_inference</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;OLS REGRESSION RESULTS&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Sample size: n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">, Predictors: p = </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;R² = </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;R2&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Adjusted R² = </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;R2_adj&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Residual SE: s = </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;s&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;Coefficient&#39;</span><span class="si">:</span><span class="s2">&lt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Estimate&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Std Error&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;t-stat&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;p-value&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">54</span><span class="p">)</span>
<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Intercept&#39;</span><span class="p">,</span> <span class="s1">&#39;hp&#39;</span><span class="p">,</span> <span class="s1">&#39;wt&#39;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">se</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;beta_hat&#39;</span><span class="p">],</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;se&#39;</span><span class="p">],</span>
                               <span class="n">results</span><span class="p">[</span><span class="s1">&#39;t_stats&#39;</span><span class="p">],</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;p_values&#39;</span><span class="p">]):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">:</span><span class="s2">&lt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">b</span><span class="si">:</span><span class="s2">&gt;10.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">se</span><span class="si">:</span><span class="s2">&gt;10.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">t</span><span class="si">:</span><span class="s2">&gt;10.2f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">p</span><span class="si">:</span><span class="s2">&gt;10.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>============================================================
OLS REGRESSION RESULTS
============================================================

Sample size: n = 32, Predictors: p = 3
R² = 0.8826, Adjusted R² = 0.8745
Residual SE: s = 1.9734

Coefficient     Estimate  Std Error     t-stat    p-value
------------------------------------------------------
Intercept        40.4873     1.5872      25.51     0.0000
hp               -0.0313     0.0048      -6.47     0.0000
wt               -3.9056     0.4397      -8.88     0.0000
</pre></div>
</div>
</section>
</section>
<section id="distributional-results-under-normality">
<h2>Distributional Results Under Normality<a class="headerlink" href="#distributional-results-under-normality" title="Link to this heading"></a></h2>
<p>Under the full normal linear model (GM1-GM5), we have exact distributional results enabling precise inference.</p>
<section id="distribution-of-the-ols-estimator">
<h3>Distribution of the OLS Estimator<a class="headerlink" href="#distribution-of-the-ols-estimator" title="Link to this heading"></a></h3>
<div class="note admonition">
<p class="admonition-title">Theorem: Normality of OLS</p>
<p>Under GM1-GM5:</p>
<div class="math notranslate nohighlight">
\[\hat{\boldsymbol{\beta}} | \mathbf{X} \sim \mathcal{N}\left(\boldsymbol{\beta}, \sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}\right)\]</div>
</div>
<p><strong>Proof</strong>: Since <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}\)</span> with <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \sigma^2\mathbf{I})\)</span>:</p>
<p><span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> is a linear transformation of normal <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon}\)</span>, hence normal. Mean and variance computed earlier. ∎</p>
</section>
<section id="distribution-of-s2">
<h3>Distribution of s²<a class="headerlink" href="#distribution-of-s2" title="Link to this heading"></a></h3>
<div class="note admonition">
<p class="admonition-title">Theorem: Chi-Square Distribution of RSS</p>
<p>Under GM1-GM5:</p>
<div class="math notranslate nohighlight">
\[\frac{(n-p)s^2}{\sigma^2} = \frac{\text{RSS}}{\sigma^2} \sim \chi^2_{n-p}\]</div>
<p>Moreover, <span class="math notranslate nohighlight">\(s^2\)</span> and <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> are independent.</p>
</div>
<p><strong>Proof sketch</strong>: RSS can be written as <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon}^\top\mathbf{M}\boldsymbol{\varepsilon}/\sigma^2\)</span> where <span class="math notranslate nohighlight">\(\mathbf{M} = \mathbf{I} - \mathbf{H}\)</span> is idempotent with rank <span class="math notranslate nohighlight">\(n - p\)</span>. By Cochran’s theorem, this is <span class="math notranslate nohighlight">\(\chi^2_{n-p}\)</span>.</p>
<p>The independence follows from the fact that <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> depends on <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon}\)</span> through <span class="math notranslate nohighlight">\((\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\boldsymbol{\varepsilon}\)</span> and <span class="math notranslate nohighlight">\(s^2\)</span> through <span class="math notranslate nohighlight">\(\mathbf{M}\boldsymbol{\varepsilon}\)</span>. Since <span class="math notranslate nohighlight">\((\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top \cdot \sigma^2 \mathbf{M} = \mathbf{0}\)</span> (orthogonality), these linear forms of a multivariate normal are independent. ∎</p>
</section>
<section id="t-tests-for-individual-coefficients">
<h3>t-Tests for Individual Coefficients<a class="headerlink" href="#t-tests-for-individual-coefficients" title="Link to this heading"></a></h3>
<p>Combining the above results:</p>
<div class="math notranslate nohighlight">
\[\frac{\hat{\beta}_j - \beta_j}{\text{SE}(\hat{\beta}_j)} = \frac{\hat{\beta}_j - \beta_j}{s\sqrt{[(\mathbf{X}^\top\mathbf{X})^{-1}]_{jj}}} \sim t_{n-p}\]</div>
<p>This enables <strong>t-tests</strong> for individual coefficients:</p>
<ul class="simple">
<li><p><strong>Null hypothesis</strong>: <span class="math notranslate nohighlight">\(H_0: \beta_j = \beta_{j,0}\)</span> (often <span class="math notranslate nohighlight">\(\beta_{j,0} = 0\)</span>)</p></li>
<li><p><strong>Test statistic</strong>: <span class="math notranslate nohighlight">\(t = \frac{\hat{\beta}_j - \beta_{j,0}}{\text{SE}(\hat{\beta}_j)}\)</span></p></li>
<li><p><strong>Decision</strong>: Reject at level <span class="math notranslate nohighlight">\(\alpha\)</span> if <span class="math notranslate nohighlight">\(|t| &gt; t_{n-p, 1-\alpha/2}\)</span></p></li>
</ul>
<p><strong>Confidence interval</strong> for <span class="math notranslate nohighlight">\(\beta_j\)</span>:</p>
<div class="math notranslate nohighlight">
\[\hat{\beta}_j \pm t_{n-p, 1-\alpha/2} \cdot \text{SE}(\hat{\beta}_j)\]</div>
<figure class="align-center" id="id8">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_4_fig11_confidence_vs_prediction.png"><img alt="Confidence intervals vs prediction intervals showing the key difference" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_4_fig11_confidence_vs_prediction.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 108 </span><span class="caption-text"><strong>Figure 3.4.7</strong>: Confidence interval (for the mean) vs. prediction interval (for new Y). (a) The CI (narrow band) captures uncertainty in <span class="math notranslate nohighlight">\(E[Y|x]\)</span>, while the PI (wide band) also includes the irreducible error variance. (b) The formulas differ by the “1” inside the square root—representing <span class="math notranslate nohighlight">\(\sigma^2\)</span> for a new observation. (c) Both intervals have hyperbolic shape, widening away from <span class="math notranslate nohighlight">\(\bar{x}\)</span>, but the PI is offset by a constant amount.</span><a class="headerlink" href="#id8" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="f-tests-for-nested-models">
<h3>F-Tests for Nested Models<a class="headerlink" href="#f-tests-for-nested-models" title="Link to this heading"></a></h3>
<p>To test whether a subset of coefficients is jointly zero, use the <strong>F-test</strong>.</p>
<p><strong>Setup</strong>: Compare:
- <strong>Restricted model</strong> (R): <span class="math notranslate nohighlight">\(q\)</span> constraints, <span class="math notranslate nohighlight">\(\text{RSS}_R\)</span>
- <strong>Full model</strong> (F): no constraints, <span class="math notranslate nohighlight">\(\text{RSS}_F\)</span></p>
<div class="note admonition">
<p class="admonition-title">Theorem: F-Test for Nested Models</p>
<p>Under <span class="math notranslate nohighlight">\(H_0\)</span> (constraints are true) and normality:</p>
<div class="math notranslate nohighlight">
\[F = \frac{(\text{RSS}_R - \text{RSS}_F) / q}{\text{RSS}_F / (n - p)} \sim F_{q, n-p}\]</div>
</div>
<p><strong>Example</strong>: Testing <span class="math notranslate nohighlight">\(H_0: \beta_2 = \beta_3 = 0\)</span> (joint significance of two predictors).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">f_test_nested</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X_full</span><span class="p">,</span> <span class="n">X_reduced</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    F-test comparing full model to reduced (nested) model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">p_full</span> <span class="o">=</span> <span class="n">X_full</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">p_reduced</span> <span class="o">=</span> <span class="n">X_reduced</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">p_full</span> <span class="o">-</span> <span class="n">p_reduced</span>  <span class="c1"># number of restrictions</span>

    <span class="c1"># Fit both models</span>
    <span class="n">beta_full</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X_full</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="kc">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">beta_reduced</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X_reduced</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="kc">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">RSS_full</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">X_full</span> <span class="o">@</span> <span class="n">beta_full</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">RSS_reduced</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">X_reduced</span> <span class="o">@</span> <span class="n">beta_reduced</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># F statistic</span>
    <span class="n">F</span> <span class="o">=</span> <span class="p">((</span><span class="n">RSS_reduced</span> <span class="o">-</span> <span class="n">RSS_full</span><span class="p">)</span> <span class="o">/</span> <span class="n">q</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">RSS_full</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">p_full</span><span class="p">))</span>
    <span class="n">p_value</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">f</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="n">p_full</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;F&#39;</span><span class="p">:</span> <span class="n">F</span><span class="p">,</span> <span class="s1">&#39;df1&#39;</span><span class="p">:</span> <span class="n">q</span><span class="p">,</span> <span class="s1">&#39;df2&#39;</span><span class="p">:</span> <span class="n">n</span> <span class="o">-</span> <span class="n">p_full</span><span class="p">,</span> <span class="s1">&#39;p_value&#39;</span><span class="p">:</span> <span class="n">p_value</span><span class="p">,</span>
            <span class="s1">&#39;RSS_full&#39;</span><span class="p">:</span> <span class="n">RSS_full</span><span class="p">,</span> <span class="s1">&#39;RSS_reduced&#39;</span><span class="p">:</span> <span class="n">RSS_reduced</span><span class="p">}</span>

<span class="c1"># Example: test if both hp and wt are needed</span>
<span class="n">X_full</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">hp</span><span class="p">,</span> <span class="n">wt</span><span class="p">])</span>
<span class="n">X_reduced</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)])</span>  <span class="c1"># intercept only</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">f_test_nested</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X_full</span><span class="p">,</span> <span class="n">X_reduced</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">F-test: Are hp and wt jointly significant?&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;F(</span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;df1&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;df2&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">) = </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;F&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;p-value = </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;p_value&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="diagnostics-and-model-checking">
<h2>Diagnostics and Model Checking<a class="headerlink" href="#diagnostics-and-model-checking" title="Link to this heading"></a></h2>
<p>The validity of regression inference depends on assumptions. <strong>Diagnostics</strong> help detect violations.</p>
<section id="residual-analysis">
<h3>Residual Analysis<a class="headerlink" href="#residual-analysis" title="Link to this heading"></a></h3>
<p><strong>Types of residuals</strong>:</p>
<ol class="arabic">
<li><p><strong>Raw residuals</strong>: <span class="math notranslate nohighlight">\(e_i = y_i - \hat{y}_i\)</span></p></li>
<li><p><strong>Standardized residuals</strong>: <span class="math notranslate nohighlight">\(r_i = \frac{e_i}{s\sqrt{1 - h_{ii}}}\)</span></p>
<p>These have approximately constant variance under homoskedasticity.</p>
</li>
<li><p><strong>Studentized (externally)</strong>: <span class="math notranslate nohighlight">\(t_i = \frac{e_i}{s_{(i)}\sqrt{1 - h_{ii}}} \sim t_{n-p-1}\)</span></p>
<p>where <span class="math notranslate nohighlight">\(s_{(i)}\)</span> is computed without observation <span class="math notranslate nohighlight">\(i\)</span>. Follows exact t-distribution.</p>
</li>
</ol>
<p><strong>Diagnostic plots</strong>:</p>
<ul class="simple">
<li><p><strong>Residuals vs. Fitted</strong>: Check linearity (should be random scatter) and homoskedasticity (constant spread)</p></li>
<li><p><strong>Q-Q plot of residuals</strong>: Check normality (should follow 45° line)</p></li>
<li><p><strong>Residuals vs. each predictor</strong>: Check functional form</p></li>
<li><p><strong>Scale-Location plot</strong>: <span class="math notranslate nohighlight">\(\sqrt{|r_i|}\)</span> vs. fitted for homoskedasticity</p></li>
</ul>
<figure class="align-center" id="id9">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_4_fig07_assumption_violations.png"><img alt="Gallery of assumption violations showing nonlinearity, heteroskedasticity, non-normality, and autocorrelation" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_4_fig07_assumption_violations.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 109 </span><span class="caption-text"><strong>Figure 3.4.6</strong>: Assumption violations gallery. Top: data patterns; Bottom: diagnostic plots. Nonlinearity shows curved residual pattern. Heteroskedasticity produces a fan shape. Non-normality creates heavy tails in Q-Q plots. Autocorrelation shows serial correlation in <span class="math notranslate nohighlight">\(e_t\)</span> vs <span class="math notranslate nohighlight">\(e_{t+1}\)</span> plots.</span><a class="headerlink" href="#id9" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="leverage-and-influence">
<h3>Leverage and Influence<a class="headerlink" href="#leverage-and-influence" title="Link to this heading"></a></h3>
<p>Not all observations affect the fit equally.</p>
<p><strong>Leverage</strong>: <span class="math notranslate nohighlight">\(h_{ii} = [\mathbf{H}]_{ii}\)</span> measures how far <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> is from the center of the predictor space.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(h_{ii} \in [1/n, 1]\)</span> (from idempotence and hat matrix properties)</p></li>
<li><p>Average leverage: <span class="math notranslate nohighlight">\(\bar{h} = p/n\)</span></p></li>
<li><p>High leverage if <span class="math notranslate nohighlight">\(h_{ii} &gt; 2p/n\)</span> or <span class="math notranslate nohighlight">\(&gt; 3p/n\)</span></p></li>
</ul>
<figure class="align-center" id="id10">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_4_fig05_leverage.png"><img alt="Leverage intuition showing why extreme x values matter" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_4_fig05_leverage.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 110 </span><span class="caption-text"><strong>Figure 3.4.4</strong>: Leverage measures how far observation <span class="math notranslate nohighlight">\(x_i\)</span> is from <span class="math notranslate nohighlight">\(\bar{x}\)</span>. (a) Points far from center have high leverage (red = high, blue = low). (b) High-leverage points can dramatically influence the regression line. (c) The leverage formula <span class="math notranslate nohighlight">\(h_{ii} = 1/n + (x_i - \bar{x})^2/S_{xx}\)</span> is U-shaped in <span class="math notranslate nohighlight">\(x\)</span>.</span><a class="headerlink" href="#id10" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Influence measures</strong>:</p>
<ul>
<li><p><strong>Cook’s Distance</strong>: Combines leverage and residual size</p>
<div class="math notranslate nohighlight">
\[D_i = \frac{r_i^2}{p} \cdot \frac{h_{ii}}{1 - h_{ii}}\]</div>
<p>Large if <span class="math notranslate nohighlight">\(D_i &gt; 4/n\)</span> or <span class="math notranslate nohighlight">\(&gt; 1\)</span></p>
</li>
<li><p><strong>DFFITS</strong>: Scaled difference in fitted value when <span class="math notranslate nohighlight">\(i\)</span> deleted</p></li>
<li><p><strong>DFBETAS</strong>: Change in each coefficient when <span class="math notranslate nohighlight">\(i\)</span> deleted</p></li>
</ul>
</section>
<section id="robust-standard-errors">
<h3>Robust Standard Errors<a class="headerlink" href="#robust-standard-errors" title="Link to this heading"></a></h3>
<p>When homoskedasticity fails but we still want valid inference, use <strong>robust (sandwich) standard errors</strong>:</p>
<div class="math notranslate nohighlight">
\[\widehat{\text{Var}}_{\text{robust}}(\hat{\boldsymbol{\beta}}) = (\mathbf{X}^\top\mathbf{X})^{-1} \mathbf{X}^\top \hat{\boldsymbol{\Omega}} \mathbf{X} (\mathbf{X}^\top\mathbf{X})^{-1}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Omega}} = \text{diag}(e_1^2, \ldots, e_n^2)\)</span> estimates the heteroskedastic structure.</p>
<p><strong>HC variants</strong> (heteroskedasticity-consistent):</p>
<ul class="simple">
<li><p><strong>HC0</strong>: <span class="math notranslate nohighlight">\(\hat{\Omega}_{ii} = e_i^2\)</span> (White’s original)</p></li>
<li><p><strong>HC1</strong>: <span class="math notranslate nohighlight">\(\hat{\Omega}_{ii} = \frac{n}{n-p} e_i^2\)</span> (degrees-of-freedom correction)</p></li>
<li><p><strong>HC2</strong>: <span class="math notranslate nohighlight">\(\hat{\Omega}_{ii} = \frac{e_i^2}{1 - h_{ii}}\)</span> (leverage adjustment)</p></li>
<li><p><strong>HC3</strong>: <span class="math notranslate nohighlight">\(\hat{\Omega}_{ii} = \frac{e_i^2}{(1 - h_{ii})^2}\)</span> (jackknife-like, best for small samples)</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">robust_se</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">residuals</span><span class="p">,</span> <span class="n">hc_type</span><span class="o">=</span><span class="s1">&#39;HC1&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute heteroskedasticity-robust standard errors.</span>

<span class="sd">    Note: This implementation forms the full n×n hat matrix H, which is</span>
<span class="sd">    O(n²) in memory and time. For large n, compute leverages h_ii directly</span>
<span class="sd">    without forming H: h_ii = x_i&#39; (X&#39;X)^{-1} x_i for each row.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array, shape (n, p)</span>
<span class="sd">        Design matrix</span>
<span class="sd">    residuals : array, shape (n,)</span>
<span class="sd">        OLS residuals</span>
<span class="sd">    hc_type : str</span>
<span class="sd">        &#39;HC0&#39;, &#39;HC1&#39;, &#39;HC2&#39;, or &#39;HC3&#39;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">XtX_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span>

    <span class="c1"># Leverage values (O(n²) - for large n, compute row-by-row instead)</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">XtX_inv</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">H</span><span class="p">)</span>

    <span class="c1"># Construct Omega based on HC type</span>
    <span class="n">e2</span> <span class="o">=</span> <span class="n">residuals</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">if</span> <span class="n">hc_type</span> <span class="o">==</span> <span class="s1">&#39;HC0&#39;</span><span class="p">:</span>
        <span class="n">omega</span> <span class="o">=</span> <span class="n">e2</span>
    <span class="k">elif</span> <span class="n">hc_type</span> <span class="o">==</span> <span class="s1">&#39;HC1&#39;</span><span class="p">:</span>
        <span class="n">omega</span> <span class="o">=</span> <span class="n">e2</span> <span class="o">*</span> <span class="n">n</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">hc_type</span> <span class="o">==</span> <span class="s1">&#39;HC2&#39;</span><span class="p">:</span>
        <span class="n">omega</span> <span class="o">=</span> <span class="n">e2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">h</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">hc_type</span> <span class="o">==</span> <span class="s1">&#39;HC3&#39;</span><span class="p">:</span>
        <span class="n">omega</span> <span class="o">=</span> <span class="n">e2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">h</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown HC type: </span><span class="si">{</span><span class="n">hc_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Sandwich formula</span>
    <span class="n">meat</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">omega</span><span class="p">)</span> <span class="o">@</span> <span class="n">X</span>
    <span class="n">sandwich</span> <span class="o">=</span> <span class="n">XtX_inv</span> <span class="o">@</span> <span class="n">meat</span> <span class="o">@</span> <span class="n">XtX_inv</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">sandwich</span><span class="p">))</span>
</pre></div>
</div>
</section>
</section>
<section id="bringing-it-all-together">
<h2>Bringing It All Together<a class="headerlink" href="#bringing-it-all-together" title="Link to this heading"></a></h2>
<p>Linear regression is both a statistical tool and a conceptual framework that illuminates fundamental ideas in inference. We’ve seen:</p>
<p><strong>The calculus approach</strong> reveals OLS as the solution to an optimization problem—minimizing squared residuals leads to the normal equations <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{X}\hat{\boldsymbol{\beta}} = \mathbf{X}^\top\mathbf{y}\)</span>.</p>
<p><strong>The geometric approach</strong> shows OLS as projection—<span class="math notranslate nohighlight">\(\hat{\mathbf{y}}\)</span> is the orthogonal projection of <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> onto the column space of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. The residual <span class="math notranslate nohighlight">\(\mathbf{e}\)</span> is perpendicular to all predictors.</p>
<p><strong>The Gauss-Markov theorem</strong> establishes that under classical assumptions (linearity, exogeneity, spherical errors), OLS achieves minimum variance among all linear unbiased estimators. This optimality result has driven the dominance of OLS in applied work.</p>
<p><strong>Distributional theory</strong> under normality enables exact t-tests and F-tests. Without normality, asymptotic approximations still provide valid inference for large samples.</p>
<p><strong>Diagnostics</strong> reveal whether assumptions hold. When they fail, remedies range from robust standard errors to alternative estimators.</p>
<div class="tip admonition">
<p class="admonition-title">Looking Ahead</p>
<p>Section 3.5 extends these ideas to <strong>Generalized Linear Models (GLMs)</strong>—logistic regression, Poisson regression, and more. GLMs handle non-normal responses while maintaining the interpretability and computational tractability of the linear model framework. The IRLS algorithm for fitting GLMs is essentially Fisher scoring—Newton-Raphson with expected information—applied to the same exponential family likelihoods we met in Section 3.1.</p>
</div>
<div class="important admonition">
<p class="admonition-title">Key Takeaways 📝</p>
<ol class="arabic simple">
<li><p><strong>Matrix calculus foundation</strong>: The gradient of <span class="math notranslate nohighlight">\(\boldsymbol{\beta}^\top\mathbf{A}\boldsymbol{\beta}\)</span> is <span class="math notranslate nohighlight">\(2\mathbf{A}\boldsymbol{\beta}\)</span> (for symmetric <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>), enabling systematic derivation of least squares estimators.</p></li>
<li><p><strong>Two perspectives on OLS</strong>: Calculus (minimize RSS) and geometry (project onto column space) yield the same normal equations <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{X}\hat{\boldsymbol{\beta}} = \mathbf{X}^\top\mathbf{y}\)</span>.</p></li>
<li><p><strong>Gauss-Markov theorem</strong>: Under linearity, full rank, exogeneity, and spherical errors, OLS is BLUE—the best linear unbiased estimator. The complete proof uses the <span class="math notranslate nohighlight">\(\mathbf{D}\)</span> matrix decomposition.</p></li>
<li><p><strong>Inference requires variance estimation</strong>: The unbiased estimator <span class="math notranslate nohighlight">\(s^2 = \text{RSS}/(n-p)\)</span> replaces <span class="math notranslate nohighlight">\(\sigma^2\)</span> in standard error formulas. Under normality, t-tests and F-tests are exact.</p></li>
<li><p><strong>Diagnostics are essential</strong>: Residual plots, leverage measures, and Cook’s distance detect assumption violations. Robust standard errors (HC0-HC3) provide valid inference under heteroskedasticity.</p></li>
<li><p><strong>Outcome alignment</strong>: This section addresses Learning Outcomes on implementing regression methods, understanding estimator properties (bias, variance, efficiency), and applying diagnostics—skills essential for Sections 3.5 (GLMs), Chapter 4 (bootstrap for regression), and Chapter 5 (Bayesian linear models).</p></li>
</ol>
</div>
</section>
<section id="numerical-stability-qr-decomposition">
<h2>Numerical Stability: QR Decomposition<a class="headerlink" href="#numerical-stability-qr-decomposition" title="Link to this heading"></a></h2>
<p>The formula <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}\)</span> is mathematically correct but <strong>numerically problematic</strong>. Computing <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{X}\)</span> and inverting it can lose precision due to the condition number.</p>
<section id="the-condition-number-problem">
<h3>The Condition Number Problem<a class="headerlink" href="#the-condition-number-problem" title="Link to this heading"></a></h3>
<p>The <strong>condition number</strong> of a matrix measures sensitivity to numerical errors:</p>
<div class="math notranslate nohighlight">
\[\kappa(\mathbf{A}) = \|\mathbf{A}\| \cdot \|\mathbf{A}^{-1}\|\]</div>
<p>For the product <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{X}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\kappa(\mathbf{X}^\top\mathbf{X}) = \kappa(\mathbf{X})^2\]</div>
<p>This <strong>squaring</strong> of the condition number is catastrophic. If <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> has condition number <span class="math notranslate nohighlight">\(10^4\)</span> (not unusual with poorly scaled predictors), then <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{X}\)</span> has condition number <span class="math notranslate nohighlight">\(10^8\)</span>, potentially losing 8 digits of precision.</p>
<figure class="align-center" id="id11">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_4_fig13_numerical_stability.png"><img alt="Numerical stability comparison showing normal equations vs QR decomposition" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_4_fig13_numerical_stability.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 111 </span><span class="caption-text"><strong>Figure 3.4.8</strong>: Numerical stability matters. (a) Normal equations square the condition number: <span class="math notranslate nohighlight">\(\kappa(\mathbf{X}^\top\mathbf{X}) = \kappa(\mathbf{X})^2\)</span>. (b) Theoretical error bounds show QR decomposition with <span class="math notranslate nohighlight">\(O(\kappa \cdot \epsilon)\)</span> error vastly outperforms normal equations with <span class="math notranslate nohighlight">\(O(\kappa^2 \cdot \epsilon)\)</span> error. (c) Practical guidelines: use QR/SVD-based methods (like <code class="docutils literal notranslate"><span class="pre">np.linalg.lstsq</span></code>) when <span class="math notranslate nohighlight">\(\kappa &gt; 10^3\)</span>.</span><a class="headerlink" href="#id11" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="qr-decomposition-solution">
<h3>QR Decomposition Solution<a class="headerlink" href="#qr-decomposition-solution" title="Link to this heading"></a></h3>
<p>The <strong>QR decomposition</strong> factorizes <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> as:</p>
<div class="math notranslate nohighlight">
\[\mathbf{X} = \mathbf{Q}\mathbf{R}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{Q} \in \mathbb{R}^{n \times p}\)</span> has orthonormal columns: <span class="math notranslate nohighlight">\(\mathbf{Q}^\top\mathbf{Q} = \mathbf{I}_p\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{R} \in \mathbb{R}^{p \times p}\)</span> is upper triangular</p></li>
</ul>
<p><strong>Solving the normal equations via QR</strong>:</p>
<p>Substitute into <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{X}\hat{\boldsymbol{\beta}} = \mathbf{X}^\top\mathbf{y}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbf{R}^\top\mathbf{Q}^\top\mathbf{Q}\mathbf{R}\hat{\boldsymbol{\beta}} = \mathbf{R}^\top\mathbf{Q}^\top\mathbf{y}\]</div>
<p>Since <span class="math notranslate nohighlight">\(\mathbf{Q}^\top\mathbf{Q} = \mathbf{I}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{R}^\top\)</span> is invertible:</p>
<div class="math notranslate nohighlight">
\[\mathbf{R}\hat{\boldsymbol{\beta}} = \mathbf{Q}^\top\mathbf{y}\]</div>
<p>This triangular system is solved by <strong>back-substitution</strong>—no matrix inversion needed!</p>
<p><strong>Advantages</strong>:</p>
<ol class="arabic simple">
<li><p>Condition number is <span class="math notranslate nohighlight">\(\kappa(\mathbf{R}) = \kappa(\mathbf{X})\)</span>, not squared</p></li>
<li><p>No explicit computation of <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{X}\)</span></p></li>
<li><p>Back-substitution is <span class="math notranslate nohighlight">\(O(p^2)\)</span>, highly stable</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">ols_qr</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute OLS via QR decomposition (numerically stable).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">Q</span><span class="p">,</span> <span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">Qty</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span>
    <span class="n">beta_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">Qty</span><span class="p">)</span>  <span class="c1"># Back-substitution</span>
    <span class="k">return</span> <span class="n">beta_hat</span>

<span class="k">def</span><span class="w"> </span><span class="nf">demonstrate_condition_number</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Show the condition number problem.&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>

    <span class="c1"># Poorly conditioned design matrix (highly correlated predictors)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>  <span class="c1"># Nearly collinear!</span>

    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">])</span>
    <span class="n">y</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x1</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">x2</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>

    <span class="c1"># Condition numbers</span>
    <span class="n">kappa_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">kappa_XtX</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;CONDITION NUMBER COMPARISON&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">κ(X):     </span><span class="si">{</span><span class="n">kappa_X</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;κ(X&#39;X):   </span><span class="si">{</span><span class="n">kappa_XtX</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ratio:    </span><span class="si">{</span><span class="n">kappa_XtX</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">kappa_X</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2"> (should be ≈ κ(X))&quot;</span><span class="p">)</span>

    <span class="c1"># Compare methods</span>
    <span class="c1"># Method 1: Normal equations (unstable)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">beta_normal</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Normal equations: β̂ = </span><span class="si">{</span><span class="n">beta_normal</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">except</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">LinAlgError</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Normal equations: FAILED (singular matrix)&quot;</span><span class="p">)</span>

    <span class="c1"># Method 2: QR decomposition (stable)</span>
    <span class="n">beta_qr</span> <span class="o">=</span> <span class="n">ols_qr</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;QR decomposition: β̂ = </span><span class="si">{</span><span class="n">beta_qr</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Method 3: lstsq (uses SVD)</span>
    <span class="n">beta_lstsq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="kc">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;lstsq (SVD):      β̂ = </span><span class="si">{</span><span class="n">beta_lstsq</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">demonstrate_condition_number</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>============================================================
CONDITION NUMBER COMPARISON
============================================================

κ(X):     1.05e+04
κ(X&#39;X):   1.11e+08
Ratio:    1.05e+04 (should be ≈ κ(X))

Normal equations: β̂ = [-0.73  1.58  3.42]
QR decomposition: β̂ = [-0.73  1.58  3.42]
lstsq (SVD):      β̂ = [-0.73  1.58  3.42]
</pre></div>
</div>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ⚠️ Never Compute (X’X)⁻¹ Explicitly</p>
<p>When writing your own regression code:</p>
<ul class="simple">
<li><p><strong>Don’t do this</strong>: <code class="docutils literal notranslate"><span class="pre">np.linalg.inv(X.T</span> <span class="pre">&#64;</span> <span class="pre">X)</span> <span class="pre">&#64;</span> <span class="pre">X.T</span> <span class="pre">&#64;</span> <span class="pre">y</span></code></p></li>
<li><p><strong>Do this instead</strong>: <code class="docutils literal notranslate"><span class="pre">np.linalg.lstsq(X,</span> <span class="pre">y,</span> <span class="pre">rcond=None)[0]</span></code></p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">lstsq</span></code> function uses SVD internally, which is even more stable than QR and handles rank-deficient matrices gracefully.</p>
</div>
</section>
</section>
<section id="model-selection-and-information-criteria">
<h2>Model Selection and Information Criteria<a class="headerlink" href="#model-selection-and-information-criteria" title="Link to this heading"></a></h2>
<p>With multiple candidate models, how do we choose? <strong>Information criteria</strong> balance goodness-of-fit against complexity.</p>
<section id="the-bias-variance-tradeoff">
<h3>The Bias-Variance Tradeoff<a class="headerlink" href="#the-bias-variance-tradeoff" title="Link to this heading"></a></h3>
<p>Consider adding a predictor to a model:</p>
<ul class="simple">
<li><p><strong>RSS decreases</strong> (or stays the same)—the fit improves or is unchanged</p></li>
<li><p><strong>Variance increases</strong>—more parameters to estimate</p></li>
</ul>
<p>This is the classic bias-variance tradeoff:</p>
<div class="math notranslate nohighlight">
\[\text{MSE}(\hat{y}) = \text{Bias}^2 + \text{Variance}\]</div>
<p>A complex model has low bias (fits the data well) but high variance (overfits to noise). A simple model has higher bias but lower variance.</p>
</section>
<section id="r-squared-and-adjusted-r-squared">
<h3>R-Squared and Adjusted R-Squared<a class="headerlink" href="#r-squared-and-adjusted-r-squared" title="Link to this heading"></a></h3>
<p><strong>R-squared</strong> measures the proportion of variance explained:</p>
<div class="math notranslate nohighlight">
\[R^2 = 1 - \frac{\text{RSS}}{\text{TSS}} = 1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}\]</div>
<figure class="align-center" id="id12">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_4_fig03_regression_decomposition.png"><img alt="Regression decomposition showing TSS = ESS + RSS" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_4_fig03_regression_decomposition.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 112 </span><span class="caption-text"><strong>Figure 3.4.13</strong>: The fundamental decomposition TSS = ESS + RSS. (a) For each point, the deviation from <span class="math notranslate nohighlight">\(\bar{y}\)</span> splits into explained (ESS) and residual (RSS) components. (b) The equation: Total Sum of Squares = Explained Sum of Squares + Residual Sum of Squares. (c) <span class="math notranslate nohighlight">\(R^2 = \text{ESS}/\text{TSS}\)</span> is the proportion explained.</span><a class="headerlink" href="#id12" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Problem</strong>: <span class="math notranslate nohighlight">\(R^2\)</span> never decreases when adding predictors, even useless ones.</p>
<p><strong>Adjusted R-squared</strong> penalizes for model complexity:</p>
<div class="math notranslate nohighlight">
\[R^2_{\text{adj}} = 1 - \frac{\text{RSS}/(n-p)}{\text{TSS}/(n-1)} = 1 - (1 - R^2)\frac{n-1}{n-p}\]</div>
<p>This can decrease when adding a predictor that doesn’t improve the fit enough to justify its inclusion.</p>
</section>
<section id="information-criteria">
<h3>Information Criteria<a class="headerlink" href="#information-criteria" title="Link to this heading"></a></h3>
<p><strong>Akaike Information Criterion (AIC)</strong>:</p>
<div class="math notranslate nohighlight">
\[\text{AIC} = n \ln(\text{RSS}/n) + 2p\]</div>
<p><strong>Bayesian Information Criterion (BIC)</strong>:</p>
<div class="math notranslate nohighlight">
\[\text{BIC} = n \ln(\text{RSS}/n) + p \ln(n)\]</div>
<p>These are the AIC and BIC formulas for <strong>Gaussian linear regression</strong>, dropping additive constants that cancel when comparing models fit to the same response <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>. For general likelihoods, use <span class="math notranslate nohighlight">\(\text{AIC} = -2\ln L + 2p\)</span> and <span class="math notranslate nohighlight">\(\text{BIC} = -2\ln L + p\ln n\)</span>.</p>
<p>Both penalize complexity, but BIC penalizes more heavily for large <span class="math notranslate nohighlight">\(n\)</span>.</p>
<p><strong>Usage</strong>: Lower is better. Compare models by their AIC or BIC values.</p>
<table class="docutils align-default" id="id13">
<caption><span class="caption-number">Table 34 </span><span class="caption-text">Model Selection Criteria</span><a class="headerlink" href="#id13" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 30.0%" />
<col style="width: 25.0%" />
<col style="width: 25.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Criterion</p></th>
<th class="head"><p>Formula</p></th>
<th class="head"><p>Penalty</p></th>
<th class="head"><p>Best for</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(R^2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1 - \text{RSS}/\text{TSS}\)</span></p></td>
<td><p>None (always increases)</p></td>
<td><p>Never for selection</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(R^2_{\text{adj}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1 - (1-R^2)\frac{n-1}{n-p}\)</span></p></td>
<td><p>Mild</p></td>
<td><p>Quick comparison</p></td>
</tr>
<tr class="row-even"><td><p>AIC</p></td>
<td><p><span class="math notranslate nohighlight">\(n\ln(\text{RSS}/n) + 2p\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(2p\)</span></p></td>
<td><p>Prediction</p></td>
</tr>
<tr class="row-odd"><td><p>BIC</p></td>
<td><p><span class="math notranslate nohighlight">\(n\ln(\text{RSS}/n) + p\ln(n)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(p\ln(n)\)</span></p></td>
<td><p>Consistent selection</p></td>
</tr>
</tbody>
</table>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">model_selection_criteria</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute model selection criteria.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y : array</span>
<span class="sd">        Observed values</span>
<span class="sd">    y_hat : array</span>
<span class="sd">        Fitted values</span>
<span class="sd">    p : int</span>
<span class="sd">        Number of parameters (including intercept)</span>
<span class="sd">    n : int</span>
<span class="sd">        Sample size</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">RSS</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">TSS</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">R2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">RSS</span> <span class="o">/</span> <span class="n">TSS</span>
    <span class="n">R2_adj</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">R2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span>

    <span class="n">AIC</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">RSS</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">p</span>
    <span class="n">BIC</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">RSS</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span> <span class="o">+</span> <span class="n">p</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;R2&#39;</span><span class="p">:</span> <span class="n">R2</span><span class="p">,</span> <span class="s1">&#39;R2_adj&#39;</span><span class="p">:</span> <span class="n">R2_adj</span><span class="p">,</span> <span class="s1">&#39;AIC&#39;</span><span class="p">:</span> <span class="n">AIC</span><span class="p">,</span> <span class="s1">&#39;BIC&#39;</span><span class="p">:</span> <span class="n">BIC</span><span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="regularization-ridge-and-lasso">
<h2>Regularization: Ridge and LASSO<a class="headerlink" href="#regularization-ridge-and-lasso" title="Link to this heading"></a></h2>
<p>When predictors are highly correlated or <span class="math notranslate nohighlight">\(p\)</span> is large relative to <span class="math notranslate nohighlight">\(n\)</span>, OLS can be unstable or overfit. <strong>Regularization</strong> shrinks coefficients toward zero, trading bias for reduced variance.</p>
<section id="ridge-regression">
<h3>Ridge Regression<a class="headerlink" href="#ridge-regression" title="Link to this heading"></a></h3>
<p><strong>Ridge regression</strong> adds an <span class="math notranslate nohighlight">\(L_2\)</span> penalty:</p>
<div class="math notranslate nohighlight">
\[\hat{\boldsymbol{\beta}}_{\text{ridge}} = \arg\min_{\boldsymbol{\beta}} \left\{ \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda \|\boldsymbol{\beta}\|^2 \right\}\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda \geq 0\)</span> is the regularization parameter.</p>
<p><strong>Closed-form solution</strong>:</p>
<div class="math notranslate nohighlight" id="equation-ridge-solution">
<span class="eqno">(103)<a class="headerlink" href="#equation-ridge-solution" title="Link to this equation"></a></span>\[\hat{\boldsymbol{\beta}}_{\text{ridge}} = (\mathbf{X}^\top\mathbf{X} + \lambda\mathbf{I})^{-1}\mathbf{X}^\top\mathbf{y}\]</div>
<p><strong>Key properties</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Bias-variance tradeoff</strong>: Ridge is biased (<span class="math notranslate nohighlight">\(\mathbb{E}[\hat{\boldsymbol{\beta}}_{\text{ridge}}] \neq \boldsymbol{\beta}\)</span>) but can have lower MSE than OLS</p></li>
<li><p><strong>Shrinkage</strong>: Coefficients shrink toward zero as <span class="math notranslate nohighlight">\(\lambda \to \infty\)</span></p></li>
<li><p><strong>Handles multicollinearity</strong>: Adding <span class="math notranslate nohighlight">\(\lambda\mathbf{I}\)</span> makes <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{X} + \lambda\mathbf{I}\)</span> invertible even if <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{X}\)</span> is singular</p></li>
<li><p><strong>No variable selection</strong>: All coefficients remain nonzero</p></li>
</ol>
<figure class="align-center" id="id14">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_4_fig08_bias_variance.png"><img alt="The bias-variance tradeoff in regularization" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_4_fig08_bias_variance.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 113 </span><span class="caption-text"><strong>Figure 3.4.9</strong>: The bias-variance tradeoff. (a) A biased estimator (orange) with lower variance can achieve lower MSE than an unbiased estimator (blue). (b) As regularization <span class="math notranslate nohighlight">\(\lambda\)</span> increases, variance decreases but bias increases—there’s an optimal <span class="math notranslate nohighlight">\(\lambda\)</span>. (c) Training error decreases with complexity while test error is U-shaped.</span><a class="headerlink" href="#id14" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="lasso">
<h3>LASSO<a class="headerlink" href="#lasso" title="Link to this heading"></a></h3>
<p><strong>LASSO</strong> (Least Absolute Shrinkage and Selection Operator) uses an <span class="math notranslate nohighlight">\(L_1\)</span> penalty:</p>
<div class="math notranslate nohighlight">
\[\hat{\boldsymbol{\beta}}_{\text{LASSO}} = \arg\min_{\boldsymbol{\beta}} \left\{ \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda \|\boldsymbol{\beta}\|_1 \right\}\]</div>
<p>where <span class="math notranslate nohighlight">\(\|\boldsymbol{\beta}\|_1 = \sum_j |\beta_j|\)</span>.</p>
<p><strong>Key properties</strong>:</p>
<ol class="arabic simple">
<li><p><strong>No closed form</strong>: Must use numerical optimization (coordinate descent)</p></li>
<li><p><strong>Variable selection</strong>: Sets some coefficients exactly to zero (sparse solutions)</p></li>
<li><p><strong>Geometry</strong>: The <span class="math notranslate nohighlight">\(L_1\)</span> constraint has corners at the axes, making exact zeros likely</p></li>
</ol>
<figure class="align-center" id="id15">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_4_fig09_l1_l2_geometry.png"><img alt="L1 vs L2 geometry showing why LASSO produces sparse solutions" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_4_fig09_l1_l2_geometry.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 114 </span><span class="caption-text"><strong>Figure 3.4.10</strong>: L1 vs L2 geometry. (a) Ridge constraint is a circle—RSS contours typically touch away from axes. (b) LASSO constraint is a diamond—contours often touch at corners where <span class="math notranslate nohighlight">\(\beta_j = 0\)</span>. (c) This geometric difference explains why LASSO performs automatic variable selection while Ridge shrinks but keeps all coefficients.</span><a class="headerlink" href="#id15" title="Link to this image"></a></p>
</figcaption>
</figure>
<figure class="align-center" id="id16">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_4_fig10_regularization_paths.png"><img alt="Regularization paths showing how coefficients shrink with lambda" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter3/ch3_4_fig10_regularization_paths.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 115 </span><span class="caption-text"><strong>Figure 3.4.11</strong>: Regularization paths. (a) Ridge shrinks coefficients smoothly toward zero as <span class="math notranslate nohighlight">\(\lambda\)</span> increases—all remain nonzero. (b) LASSO drives coefficients to exactly zero at different <span class="math notranslate nohighlight">\(\lambda\)</span> values, providing a variable selection path.</span><a class="headerlink" href="#id16" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="elastic-net">
<h3>Elastic Net<a class="headerlink" href="#elastic-net" title="Link to this heading"></a></h3>
<p><strong>Elastic Net</strong> combines <span class="math notranslate nohighlight">\(L_1\)</span> and <span class="math notranslate nohighlight">\(L_2\)</span> penalties:</p>
<div class="math notranslate nohighlight">
\[\hat{\boldsymbol{\beta}}_{\text{EN}} = \arg\min_{\boldsymbol{\beta}} \left\{ \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda_1 \|\boldsymbol{\beta}\|_1 + \lambda_2 \|\boldsymbol{\beta}\|^2 \right\}\]</div>
<p>This combines the variable selection of LASSO with the stability of ridge for correlated predictors.</p>
<div class="warning admonition">
<p class="admonition-title">Practical Considerations for Regularization</p>
<ol class="arabic simple">
<li><p><strong>Intercept handling</strong>: Typically, we center <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> and standardize predictors, and we <strong>do not penalize the intercept</strong> <span class="math notranslate nohighlight">\(\beta_0\)</span>. Most software handles this automatically.</p></li>
<li><p><strong>Scaling matters</strong>: Always report whether predictors were standardized. The penalty <span class="math notranslate nohighlight">\(\lambda\)</span> depends on the scale of predictors—a coefficient for income in dollars behaves very differently from income in thousands of dollars.</p></li>
<li><p><strong>Cross-validation</strong>: Choose <span class="math notranslate nohighlight">\(\lambda\)</span> by cross-validation to balance fit and complexity.</p></li>
</ol>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">Ridge</span><span class="p">,</span> <span class="n">Lasso</span><span class="p">,</span> <span class="n">ElasticNet</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="k">def</span><span class="w"> </span><span class="nf">compare_regularization</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare OLS, Ridge, LASSO, and Elastic Net.&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

    <span class="c1"># Generate data with correlated predictors</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">50</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
    <span class="c1"># Add correlation structure</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="n">X</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.9</span> <span class="o">*</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span>

    <span class="c1"># True coefficients: only first 5 are nonzero</span>
    <span class="n">beta_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="n">beta_true</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">]</span>

    <span class="n">y</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta_true</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>

    <span class="c1"># Standardize for fair comparison</span>
    <span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
    <span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="c1"># Fit models</span>
    <span class="n">ols_beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="kc">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="n">lasso</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="n">lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="n">enet</span> <span class="o">=</span> <span class="n">ElasticNet</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">enet</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;REGULARIZATION COMPARISON&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Data: n=</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">, p=</span><span class="si">{</span><span class="n">p</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">beta_true</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="si">}</span><span class="s2"> true nonzero coefficients&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;Method&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Nonzero&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;MSE (coeffs)&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">40</span><span class="p">)</span>

    <span class="n">methods</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;OLS&#39;</span><span class="p">,</span> <span class="n">ols_beta</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;Ridge&#39;</span><span class="p">,</span> <span class="n">ridge</span><span class="o">.</span><span class="n">coef_</span><span class="p">),</span>
               <span class="p">(</span><span class="s1">&#39;LASSO&#39;</span><span class="p">,</span> <span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;Elastic Net&#39;</span><span class="p">,</span> <span class="n">enet</span><span class="o">.</span><span class="n">coef_</span><span class="p">)]</span>

    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">coef</span> <span class="ow">in</span> <span class="n">methods</span><span class="p">:</span>
        <span class="n">nonzero</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">coef</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">1e-6</span><span class="p">)</span>
        <span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">coef</span> <span class="o">-</span> <span class="n">beta_true</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">nonzero</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">mse</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">→ LASSO and Elastic Net achieve sparsity (fewer nonzero coefficients)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;→ Ridge shrinks but keeps all coefficients nonzero&quot;</span><span class="p">)</span>

<span class="n">compare_regularization</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>============================================================
REGULARIZATION COMPARISON
============================================================

Data: n=100, p=50, 5 true nonzero coefficients

Method              Nonzero   MSE (coeffs)
----------------------------------------
OLS                      50          0.0892
Ridge                    50          0.0734
LASSO                    12          0.0418
Elastic Net              14          0.0456

→ LASSO and Elastic Net achieve sparsity (fewer nonzero coefficients)
→ Ridge shrinks but keeps all coefficients nonzero
</pre></div>
</div>
</section>
<section id="choosing-the-regularization-parameter">
<h3>Choosing the Regularization Parameter<a class="headerlink" href="#choosing-the-regularization-parameter" title="Link to this heading"></a></h3>
<p>The parameter <span class="math notranslate nohighlight">\(\lambda\)</span> controls the amount of shrinkage. Too small → overfitting; too large → underfitting.</p>
<p><strong>Cross-validation</strong> is the standard approach:</p>
<ol class="arabic simple">
<li><p>Split data into <span class="math notranslate nohighlight">\(K\)</span> folds</p></li>
<li><p>For each candidate <span class="math notranslate nohighlight">\(\lambda\)</span>:
- Train on <span class="math notranslate nohighlight">\(K-1\)</span> folds, predict on held-out fold
- Average prediction error across folds</p></li>
<li><p>Choose <span class="math notranslate nohighlight">\(\lambda\)</span> with lowest CV error</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">RidgeCV</span><span class="p">,</span> <span class="n">LassoCV</span>

<span class="c1"># Ridge with built-in CV</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">ridge_cv</span> <span class="o">=</span> <span class="n">RidgeCV</span><span class="p">(</span><span class="n">alphas</span><span class="o">=</span><span class="n">alphas</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">ridge_cv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ridge optimal λ: </span><span class="si">{</span><span class="n">ridge_cv</span><span class="o">.</span><span class="n">alpha_</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># LASSO with built-in CV</span>
<span class="n">lasso_cv</span> <span class="o">=</span> <span class="n">LassoCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">lasso_cv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;LASSO optimal λ: </span><span class="si">{</span><span class="n">lasso_cv</span><span class="o">.</span><span class="n">alpha_</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="chapter-3-4-exercises-linear-models-mastery">
<h2>Chapter 3.4 Exercises: Linear Models Mastery<a class="headerlink" href="#chapter-3-4-exercises-linear-models-mastery" title="Link to this heading"></a></h2>
<p>These exercises build your understanding of linear regression from matrix calculus foundations through the Gauss-Markov theorem to practical diagnostics. Each exercise connects theoretical derivations to computational practice and statistical interpretation.</p>
<div class="tip admonition">
<p class="admonition-title">A Note on These Exercises</p>
<p>These exercises are designed to deepen your understanding of linear models through hands-on exploration:</p>
<ul class="simple">
<li><p><strong>Exercise 1</strong> develops matrix calculus skills essential for deriving OLS estimators</p></li>
<li><p><strong>Exercise 2</strong> explores the geometric interpretation of regression as projection</p></li>
<li><p><strong>Exercise 3</strong> identifies Gauss-Markov assumption violations and their consequences</p></li>
<li><p><strong>Exercise 4</strong> provides a complete proof of Gauss-Markov for a single coefficient</p></li>
<li><p><strong>Exercise 5</strong> verifies theoretical properties computationally via Monte Carlo simulation</p></li>
<li><p><strong>Exercise 6</strong> compares model-based and robust standard errors under heteroskedasticity</p></li>
</ul>
<p>Complete solutions with derivations, code, output, and interpretation are provided. Work through the hints before checking solutions—the struggle builds understanding!</p>
</div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 1: Matrix Calculus Derivations</p>
<p>The matrix calculus rules developed in this section are the foundation for deriving OLS estimators. This exercise builds fluency with these essential tools.</p>
<div class="note admonition">
<p class="admonition-title">Background: Why Matrix Calculus?</p>
<p>When optimizing functions of vectors and matrices (like RSS as a function of <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>), we need systematic rules for computing gradients. Component-wise verification builds confidence that the matrix formulas are correct.</p>
</div>
<ol class="loweralpha simple">
<li><p><strong>Linear form</strong>: Verify by component-wise differentiation that <span class="math notranslate nohighlight">\(\frac{\partial}{\partial \mathbf{x}}(\mathbf{a}^\top\mathbf{x}) = \mathbf{a}\)</span> for <span class="math notranslate nohighlight">\(\mathbf{a}, \mathbf{x} \in \mathbb{R}^3\)</span>.</p></li>
<li><p><strong>Cross term</strong>: Derive <span class="math notranslate nohighlight">\(\frac{\partial}{\partial \boldsymbol{\beta}}(\mathbf{y}^\top\mathbf{X}\boldsymbol{\beta})\)</span> directly from the definition of gradient.</p></li>
<li><p><strong>Quadratic form</strong>: For symmetric <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>, show that <span class="math notranslate nohighlight">\(\frac{\partial}{\partial \mathbf{x}}(\mathbf{x}^\top\mathbf{A}\mathbf{x}) = 2\mathbf{A}\mathbf{x}\)</span> by expanding in components.</p></li>
<li><p><strong>RSS gradient</strong>: Derive the OLS gradient <span class="math notranslate nohighlight">\(\frac{\partial \text{RSS}}{\partial \boldsymbol{\beta}} = 2\mathbf{X}^\top(\mathbf{X}\boldsymbol{\beta} - \mathbf{y})\)</span> using the chain rule.</p></li>
</ol>
<div class="tip admonition">
<p class="admonition-title">Hint</p>
<p>For part (c), write out <span class="math notranslate nohighlight">\(\mathbf{x}^\top\mathbf{A}\mathbf{x} = \sum_i\sum_j x_i A_{ij} x_j\)</span> and differentiate with respect to <span class="math notranslate nohighlight">\(x_k\)</span>. Remember that terms containing <span class="math notranslate nohighlight">\(x_k\)</span> appear both when <span class="math notranslate nohighlight">\(i = k\)</span> and when <span class="math notranslate nohighlight">\(j = k\)</span>.</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Component-wise verification for linear function</strong></p>
<p class="sd-card-text">Let <span class="math notranslate nohighlight">\(\mathbf{a} = (a_1, a_2, a_3)^\top\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1, x_2, x_3)^\top\)</span>.</p>
<div class="math notranslate nohighlight">
\[f(\mathbf{x}) = \mathbf{a}^\top\mathbf{x} = a_1 x_1 + a_2 x_2 + a_3 x_3\]</div>
<p class="sd-card-text">Partial derivatives:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial f}{\partial x_1} = a_1, \quad
\frac{\partial f}{\partial x_2} = a_2, \quad
\frac{\partial f}{\partial x_3} = a_3\]</div>
<p class="sd-card-text">Therefore:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\nabla_\mathbf{x} f = \begin{pmatrix} a_1 \\ a_2 \\ a_3 \end{pmatrix} = \mathbf{a} \quad \checkmark\end{split}\]</div>
<p class="sd-card-text"><strong>Part (b): Gradient of y’Xβ</strong></p>
<p class="sd-card-text"><span class="math notranslate nohighlight">\(f(\boldsymbol{\beta}) = \mathbf{y}^\top\mathbf{X}\boldsymbol{\beta}\)</span> is a scalar. Write <span class="math notranslate nohighlight">\(\mathbf{c} = \mathbf{X}^\top\mathbf{y}\)</span>, so <span class="math notranslate nohighlight">\(f = \mathbf{c}^\top\boldsymbol{\beta} = \sum_j c_j \beta_j\)</span>.</p>
<div class="math notranslate nohighlight">
\[\frac{\partial f}{\partial \beta_k} = c_k = [\mathbf{X}^\top\mathbf{y}]_k\]</div>
<p class="sd-card-text">Therefore:</p>
<div class="math notranslate nohighlight">
\[\nabla_{\boldsymbol{\beta}} (\mathbf{y}^\top\mathbf{X}\boldsymbol{\beta}) = \mathbf{X}^\top\mathbf{y}\]</div>
<p class="sd-card-text"><strong>Part (c): Quadratic form with symmetric A</strong></p>
<p class="sd-card-text">Let <span class="math notranslate nohighlight">\(f(\mathbf{x}) = \mathbf{x}^\top\mathbf{A}\mathbf{x} = \sum_i\sum_j x_i A_{ij} x_j\)</span>.</p>
<p class="sd-card-text">For partial with respect to <span class="math notranslate nohighlight">\(x_k\)</span>:</p>
<ul class="simple">
<li><p class="sd-card-text">Terms where <span class="math notranslate nohighlight">\(i = k\)</span>: <span class="math notranslate nohighlight">\(\sum_j x_k A_{kj} x_j\)</span> → derivative is <span class="math notranslate nohighlight">\(\sum_j A_{kj} x_j = [\mathbf{A}\mathbf{x}]_k\)</span></p></li>
<li><p class="sd-card-text">Terms where <span class="math notranslate nohighlight">\(j = k\)</span>: <span class="math notranslate nohighlight">\(\sum_i x_i A_{ik} x_k\)</span> → derivative is <span class="math notranslate nohighlight">\(\sum_i A_{ik} x_i = [\mathbf{A}^\top\mathbf{x}]_k\)</span></p></li>
</ul>
<p class="sd-card-text">Total: <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x_k} = [\mathbf{A}\mathbf{x}]_k + [\mathbf{A}^\top\mathbf{x}]_k\)</span></p>
<p class="sd-card-text">For symmetric <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>: <span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{x} = \mathbf{A}^\top\mathbf{x}\)</span>, so:</p>
<div class="math notranslate nohighlight">
\[\nabla f = 2\mathbf{A}\mathbf{x} \quad \checkmark\]</div>
<p class="sd-card-text"><strong>Part (d): RSS gradient via chain rule</strong></p>
<p class="sd-card-text">Let <span class="math notranslate nohighlight">\(\mathbf{r} = \mathbf{y} - \mathbf{X}\boldsymbol{\beta}\)</span> (residual vector). Then RSS <span class="math notranslate nohighlight">\(= \mathbf{r}^\top\mathbf{r}\)</span>.</p>
<p class="sd-card-text"><strong>Chain rule</strong>: <span class="math notranslate nohighlight">\(\frac{\partial \text{RSS}}{\partial \boldsymbol{\beta}} = \frac{\partial \mathbf{r}}{\partial \boldsymbol{\beta}^\top}^\top \frac{\partial (\mathbf{r}^\top\mathbf{r})}{\partial \mathbf{r}}\)</span></p>
<ul class="simple">
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(\frac{\partial (\mathbf{r}^\top\mathbf{r})}{\partial \mathbf{r}} = 2\mathbf{r}\)</span> (gradient of <span class="math notranslate nohighlight">\(\|\mathbf{r}\|^2\)</span>)</p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(\frac{\partial \mathbf{r}}{\partial \boldsymbol{\beta}^\top} = \frac{\partial (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})}{\partial \boldsymbol{\beta}^\top} = -\mathbf{X}\)</span></p></li>
</ul>
<p class="sd-card-text">Therefore:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial \text{RSS}}{\partial \boldsymbol{\beta}} = (-\mathbf{X})^\top (2\mathbf{r}) = -2\mathbf{X}^\top\mathbf{r} = 2\mathbf{X}^\top(\mathbf{X}\boldsymbol{\beta} - \mathbf{y}) \quad \checkmark\]</div>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 2: Geometry of Simple Regression</p>
<p>The geometric perspective reveals regression as projection onto the column space of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. This exercise develops that intuition for simple linear regression.</p>
<div class="note admonition">
<p class="admonition-title">Background: Projection Interpretation</p>
<p>In simple regression <span class="math notranslate nohighlight">\(y = \beta_0 + \beta_1 x + \varepsilon\)</span>, the design matrix <span class="math notranslate nohighlight">\(\mathbf{X} = [\mathbf{1} \; \mathbf{x}]\)</span> spans a 2-dimensional subspace of <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>. The fitted values <span class="math notranslate nohighlight">\(\hat{\mathbf{y}}\)</span> are the orthogonal projection of <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> onto this plane.</p>
</div>
<ol class="loweralpha simple">
<li><p>Show that the design matrix has columns <span class="math notranslate nohighlight">\(\mathbf{1}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, and verify that <span class="math notranslate nohighlight">\(\hat{\beta}_1 = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2}\)</span>.</p></li>
<li><p>Verify geometrically that the residual vector is orthogonal to both <span class="math notranslate nohighlight">\(\mathbf{1}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. What do these orthogonality conditions imply about the residuals?</p></li>
<li><p>Show that the regression line passes through the point <span class="math notranslate nohighlight">\((\bar{x}, \bar{y})\)</span>.</p></li>
<li><p>Derive the formula for <span class="math notranslate nohighlight">\(R^2\)</span> as the squared correlation between <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> and <span class="math notranslate nohighlight">\(\hat{\mathbf{y}}\)</span> using vector inner products.</p></li>
</ol>
<div class="tip admonition">
<p class="admonition-title">Hint</p>
<p>For part (b), the condition <span class="math notranslate nohighlight">\(\mathbf{1}^\top \mathbf{e} = 0\)</span> means <span class="math notranslate nohighlight">\(\sum e_i = 0\)</span> (residuals sum to zero). The condition <span class="math notranslate nohighlight">\(\mathbf{x}^\top \mathbf{e} = 0\)</span> means <span class="math notranslate nohighlight">\(\sum x_i e_i = 0\)</span>.</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): OLS formula for simple regression</strong></p>
<p class="sd-card-text">The design matrix is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{X} = \begin{pmatrix} 1 &amp; x_1 \\ 1 &amp; x_2 \\ \vdots &amp; \vdots \\ 1 &amp; x_n \end{pmatrix} = [\mathbf{1} \; \mathbf{x}]\end{split}\]</div>
<p class="sd-card-text">From the normal equations <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{X}\hat{\boldsymbol{\beta}} = \mathbf{X}^\top\mathbf{y}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{pmatrix} n &amp; \sum x_i \\ \sum x_i &amp; \sum x_i^2 \end{pmatrix} \begin{pmatrix} \hat{\beta}_0 \\ \hat{\beta}_1 \end{pmatrix} = \begin{pmatrix} \sum y_i \\ \sum x_i y_i \end{pmatrix}\end{split}\]</div>
<p class="sd-card-text">Solving (multiply first equation by <span class="math notranslate nohighlight">\(\bar{x}\)</span> and subtract from second):</p>
<div class="math notranslate nohighlight">
\[\hat{\beta}_1 = \frac{\sum x_i y_i - n\bar{x}\bar{y}}{\sum x_i^2 - n\bar{x}^2} = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2}\]</div>
<p class="sd-card-text"><strong>Part (b): Orthogonality conditions</strong></p>
<p class="sd-card-text">The normal equations <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{e} = \mathbf{0}\)</span> give:</p>
<ul class="simple">
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(\mathbf{1}^\top\mathbf{e} = \sum_i e_i = 0\)</span> → <strong>Residuals sum to zero</strong></p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(\mathbf{x}^\top\mathbf{e} = \sum_i x_i e_i = 0\)</span> → <strong>Residuals uncorrelated with predictor</strong></p></li>
</ul>
<p class="sd-card-text"><strong>Geometric interpretation</strong>: The residual vector <span class="math notranslate nohighlight">\(\mathbf{e}\)</span> is perpendicular to the plane spanned by <span class="math notranslate nohighlight">\(\mathbf{1}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. Since <span class="math notranslate nohighlight">\(\hat{\mathbf{y}} = \mathbf{X}\hat{\boldsymbol{\beta}}\)</span> lies in this plane, <span class="math notranslate nohighlight">\(\mathbf{e} = \mathbf{y} - \hat{\mathbf{y}}\)</span> is the perpendicular from <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> to the plane—the shortest distance.</p>
<p class="sd-card-text"><strong>Part (c): Regression line through</strong> <span class="math notranslate nohighlight">\((\bar{x}, \bar{y})\)</span></p>
<p class="sd-card-text">From <span class="math notranslate nohighlight">\(\sum e_i = 0\)</span>:</p>
<div class="math notranslate nohighlight">
\[\sum(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0\]</div>
<p class="sd-card-text">This gives <span class="math notranslate nohighlight">\(n\bar{y} - n\hat{\beta}_0 - \hat{\beta}_1 n\bar{x} = 0\)</span>, so:</p>
<div class="math notranslate nohighlight">
\[\hat{\beta}_0 = \bar{y} - \hat{\beta}_1\bar{x}\]</div>
<p class="sd-card-text">Therefore the fitted value at <span class="math notranslate nohighlight">\(x = \bar{x}\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\hat{y}|_{x=\bar{x}} = \hat{\beta}_0 + \hat{\beta}_1\bar{x} = (\bar{y} - \hat{\beta}_1\bar{x}) + \hat{\beta}_1\bar{x} = \bar{y}\]</div>
<p class="sd-card-text">The regression line passes through the <strong>centroid</strong> <span class="math notranslate nohighlight">\((\bar{x}, \bar{y})\)</span>.</p>
<p class="sd-card-text"><strong>Part (d): R² as squared correlation</strong></p>
<p class="sd-card-text">Define centered vectors: <span class="math notranslate nohighlight">\(\tilde{\mathbf{y}} = \mathbf{y} - \bar{y}\mathbf{1}\)</span> and <span class="math notranslate nohighlight">\(\tilde{\hat{\mathbf{y}}} = \hat{\mathbf{y}} - \bar{y}\mathbf{1}\)</span>.</p>
<p class="sd-card-text">The correlation between <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> and <span class="math notranslate nohighlight">\(\hat{\mathbf{y}}\)</span> is:</p>
<div class="math notranslate nohighlight">
\[r_{y,\hat{y}} = \frac{\tilde{\mathbf{y}}^\top\tilde{\hat{\mathbf{y}}}}{\|\tilde{\mathbf{y}}\| \cdot \|\tilde{\hat{\mathbf{y}}}\|}\]</div>
<p class="sd-card-text">Since <span class="math notranslate nohighlight">\(\mathbf{e} \perp \hat{\mathbf{y}}\)</span> and <span class="math notranslate nohighlight">\(\bar{e} = 0\)</span>:</p>
<div class="math notranslate nohighlight">
\[\tilde{\mathbf{y}}^\top\tilde{\hat{\mathbf{y}}} = (\tilde{\hat{\mathbf{y}}} + \mathbf{e})^\top\tilde{\hat{\mathbf{y}}} = \|\tilde{\hat{\mathbf{y}}}\|^2\]</div>
<p class="sd-card-text">Therefore:</p>
<div class="math notranslate nohighlight">
\[r_{y,\hat{y}}^2 = \frac{\|\tilde{\hat{\mathbf{y}}}\|^4}{\|\tilde{\mathbf{y}}\|^2 \cdot \|\tilde{\hat{\mathbf{y}}}\|^2} = \frac{\|\tilde{\hat{\mathbf{y}}}\|^2}{\|\tilde{\mathbf{y}}\|^2} = \frac{\text{ESS}}{\text{TSS}} = R^2\]</div>
<p class="sd-card-text"><strong>Key insight</strong>: <span class="math notranslate nohighlight">\(R^2\)</span> is the squared correlation between observed and fitted values, which equals the proportion of variance explained.</p>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 3: Gauss-Markov Assumptions</p>
<p>Understanding which assumptions are violated—and the consequences—is essential for applied regression. This exercise builds diagnostic intuition.</p>
<div class="note admonition">
<p class="admonition-title">Background: The Assumption Hierarchy</p>
<p>The Gauss-Markov assumptions are: <strong>GM1</strong> (linearity), <strong>GM2</strong> (full rank), <strong>GM3</strong> (exogeneity: <span class="math notranslate nohighlight">\(\mathbb{E}[\varepsilon|\mathbf{X}] = \mathbf{0}\)</span>), and <strong>GM4</strong> (spherical errors: <span class="math notranslate nohighlight">\(\text{Var}(\varepsilon|\mathbf{X}) = \sigma^2\mathbf{I}\)</span>). Each violation has specific consequences for OLS.</p>
</div>
<p>For each scenario, identify which Gauss-Markov assumption is violated and describe the consequences.</p>
<ol class="loweralpha simple">
<li><p>A researcher fits <span class="math notranslate nohighlight">\(y = \beta_0 + \beta_1 x + \varepsilon\)</span> but the true relationship is <span class="math notranslate nohighlight">\(y = \beta_0 + \beta_1 \log(x) + \varepsilon\)</span>.</p></li>
<li><p>In a regression with three predictors, the third predictor is defined as <span class="math notranslate nohighlight">\(x_3 = 2x_1 - x_2\)</span>.</p></li>
<li><p>An economist studies savings rates but omits “income,” which is correlated with education (included) and directly affects savings.</p></li>
<li><p>In a study of housing prices, the variance of residuals is proportional to house size.</p></li>
<li><p>Monthly unemployment data shows residuals that are positively correlated across consecutive months.</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Wrong functional form</strong></p>
<p class="sd-card-text"><strong>Violation</strong>: GM1 (Linearity)</p>
<p class="sd-card-text">The true model is <span class="math notranslate nohighlight">\(y = \beta_0 + \beta_1\log(x) + \varepsilon\)</span>, but we’re fitting <span class="math notranslate nohighlight">\(y = \gamma_0 + \gamma_1 x + \varepsilon\)</span>.</p>
<p class="sd-card-text"><strong>Consequences</strong>:</p>
<ul class="simple">
<li><p class="sd-card-text">OLS estimates <span class="math notranslate nohighlight">\(\hat{\gamma}_0, \hat{\gamma}_1\)</span> are <strong>biased</strong> and <strong>inconsistent</strong></p></li>
<li><p class="sd-card-text">They estimate the “best linear approximation” to a nonlinear relationship</p></li>
<li><p class="sd-card-text">Residual plots will show systematic curvature (nonlinearity pattern)</p></li>
<li><p class="sd-card-text">Predictions will be systematically wrong, especially at extreme <span class="math notranslate nohighlight">\(x\)</span> values</p></li>
</ul>
<p class="sd-card-text"><strong>Remedy</strong>: Transform predictors (<span class="math notranslate nohighlight">\(\log(x)\)</span>), add polynomial terms, or use nonlinear models.</p>
<p class="sd-card-text"><strong>Part (b): Perfect multicollinearity</strong></p>
<p class="sd-card-text"><strong>Violation</strong>: GM2 (Full Rank)</p>
<p class="sd-card-text">If <span class="math notranslate nohighlight">\(x_3 = 2x_1 - x_2\)</span>, the third column of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is a linear combination of the first two.</p>
<p class="sd-card-text"><strong>Consequences</strong>:</p>
<ul class="simple">
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{X}\)</span> is <strong>singular</strong> (not invertible)</p></li>
<li><p class="sd-card-text">OLS has <strong>infinitely many solutions</strong>—the parameter vector is not identified</p></li>
<li><p class="sd-card-text">Software will fail or drop one predictor automatically</p></li>
</ul>
<p class="sd-card-text"><strong>Remedy</strong>: Remove redundant predictor(s) or use regularization (ridge regression).</p>
<p class="sd-card-text"><strong>Part (c): Omitted variable bias</strong></p>
<p class="sd-card-text"><strong>Violation</strong>: GM3 (Exogeneity/Strict Exogeneity)</p>
<p class="sd-card-text">Income affects both education (parents with higher income can afford more education) and savings (direct effect). By omitting income, <span class="math notranslate nohighlight">\(\mathbb{E}[\varepsilon|\text{education}] \neq 0\)</span>.</p>
<p class="sd-card-text"><strong>Consequences</strong>:</p>
<ul class="simple">
<li><p class="sd-card-text">OLS coefficient on education is <strong>biased</strong> and <strong>inconsistent</strong></p></li>
<li><p class="sd-card-text">The bias direction depends on signs: if income ↔ education (positive) and income → savings (positive), education coefficient is biased <strong>upward</strong></p></li>
<li><p class="sd-card-text">Omitted variable bias formula: <span class="math notranslate nohighlight">\(\text{bias}(\hat{\beta}_{\text{edu}}) = \beta_{\text{inc}} \times \delta\)</span> where <span class="math notranslate nohighlight">\(\delta\)</span> is the regression coefficient of income on education</p></li>
</ul>
<p class="sd-card-text"><strong>Remedy</strong>: Include income as a control, use instrumental variables, or acknowledge the limitation.</p>
<p class="sd-card-text"><strong>Part (d): Heteroskedasticity</strong></p>
<p class="sd-card-text"><strong>Violation</strong>: GM4 (Spherical Errors—specifically homoskedasticity)</p>
<p class="sd-card-text"><span class="math notranslate nohighlight">\(\text{Var}(\varepsilon_i) = \sigma^2 \cdot (\text{house size}_i)\)</span>, not constant.</p>
<p class="sd-card-text"><strong>Consequences</strong>:</p>
<ul class="simple">
<li><p class="sd-card-text">OLS is still <strong>unbiased</strong> and <strong>consistent</strong></p></li>
<li><p class="sd-card-text">OLS is <strong>no longer BLUE</strong>—more efficient estimators exist (WLS)</p></li>
<li><p class="sd-card-text">Standard errors are <strong>wrong</strong> (typically too small for large houses)</p></li>
<li><p class="sd-card-text">t-tests and F-tests have incorrect size (Type I error ≠ α)</p></li>
</ul>
<p class="sd-card-text"><strong>Remedy</strong>: Use robust (HC) standard errors, or weighted least squares with <span class="math notranslate nohighlight">\(w_i \propto 1/\text{size}_i\)</span>.</p>
<p class="sd-card-text"><strong>Part (e): Autocorrelation</strong></p>
<p class="sd-card-text"><strong>Violation</strong>: GM4 (Spherical Errors—specifically no autocorrelation)</p>
<p class="sd-card-text"><span class="math notranslate nohighlight">\(\text{Cov}(\varepsilon_t, \varepsilon_{t-1}) &gt; 0\)</span> for consecutive months.</p>
<p class="sd-card-text"><strong>Consequences</strong>:</p>
<ul class="simple">
<li><p class="sd-card-text">OLS is still <strong>unbiased</strong> and <strong>consistent</strong></p></li>
<li><p class="sd-card-text">OLS is <strong>not BLUE</strong></p></li>
<li><p class="sd-card-text">Standard errors are <strong>underestimated</strong> (positive autocorrelation clusters errors)</p></li>
<li><p class="sd-card-text">Durbin-Watson test will detect this (DW &lt; 2 indicates positive autocorrelation)</p></li>
</ul>
<p class="sd-card-text"><strong>Remedy</strong>: Use Newey-West (HAC) standard errors, or GLS/Cochrane-Orcutt for efficiency.</p>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 4: Complete BLUE Proof</p>
<p>The Gauss-Markov theorem establishes OLS as the Best Linear Unbiased Estimator. This exercise develops a complete proof for a single coefficient, building intuition for the general result.</p>
<div class="note admonition">
<p class="admonition-title">Background: The BLUE Property</p>
<p>BLUE means OLS achieves the minimum variance among all linear unbiased estimators. The proof uses a clever decomposition: any alternative estimator can be written as OLS plus a “deviation” term, and this deviation can only increase variance.</p>
</div>
<ol class="loweralpha simple">
<li><p>Let <span class="math notranslate nohighlight">\(\tilde{\beta}_1 = \mathbf{c}^\top\mathbf{y}\)</span> be any linear unbiased estimator of <span class="math notranslate nohighlight">\(\beta_1\)</span> in simple regression. Show that unbiasedness requires <span class="math notranslate nohighlight">\(\mathbf{c}^\top\mathbf{X} = (0, 1)\)</span>.</p></li>
<li><p>Write <span class="math notranslate nohighlight">\(\mathbf{c} = \mathbf{a}_1 + \mathbf{d}\)</span> where <span class="math notranslate nohighlight">\(\mathbf{a}_1\)</span> is the second row of <span class="math notranslate nohighlight">\((\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\)</span>. Show that <span class="math notranslate nohighlight">\(\mathbf{d}^\top\mathbf{X} = \mathbf{0}\)</span>.</p></li>
<li><p>Show that <span class="math notranslate nohighlight">\(\text{Var}(\tilde{\beta}_1) = \text{Var}(\hat{\beta}_1) + \sigma^2\mathbf{d}^\top\mathbf{d}\)</span>.</p></li>
<li><p>Conclude that <span class="math notranslate nohighlight">\(\text{Var}(\tilde{\beta}_1) \geq \text{Var}(\hat{\beta}_1)\)</span> with equality iff <span class="math notranslate nohighlight">\(\mathbf{d} = \mathbf{0}\)</span>.</p></li>
</ol>
<div class="tip admonition">
<p class="admonition-title">Hint</p>
<p>The key insight in part (c) is that the cross-term <span class="math notranslate nohighlight">\(\mathbf{a}_1^\top\mathbf{d} = 0\)</span> because <span class="math notranslate nohighlight">\(\mathbf{d}\)</span> is orthogonal to the column space of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{a}_1\)</span> lies in that column space.</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Unbiasedness condition</strong></p>
<p class="sd-card-text">In simple regression, <span class="math notranslate nohighlight">\(\mathbf{X} = [\mathbf{1} \; \mathbf{x}]\)</span> (n×2). For <span class="math notranslate nohighlight">\(\tilde{\beta}_1 = \mathbf{c}^\top\mathbf{y}\)</span> to be unbiased for <span class="math notranslate nohighlight">\(\beta_1\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbb{E}[\tilde{\beta}_1] = \mathbf{c}^\top\mathbb{E}[\mathbf{y}] = \mathbf{c}^\top\mathbf{X}\boldsymbol{\beta} = \mathbf{c}^\top[\mathbf{1} \; \mathbf{x}]\begin{pmatrix}\beta_0 \\ \beta_1\end{pmatrix} = (\mathbf{c}^\top\mathbf{1})\beta_0 + (\mathbf{c}^\top\mathbf{x})\beta_1\end{split}\]</div>
<p class="sd-card-text">For this to equal <span class="math notranslate nohighlight">\(\beta_1\)</span> for all <span class="math notranslate nohighlight">\((\beta_0, \beta_1)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbf{c}^\top\mathbf{1} = 0, \quad \mathbf{c}^\top\mathbf{x} = 1\]</div>
<p class="sd-card-text">Equivalently, <span class="math notranslate nohighlight">\(\mathbf{c}^\top\mathbf{X} = (0, 1)\)</span>.</p>
<p class="sd-card-text"><strong>Part (b): Decomposition</strong></p>
<p class="sd-card-text">Let <span class="math notranslate nohighlight">\(\mathbf{a}_1\)</span> be the second row of <span class="math notranslate nohighlight">\((\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\)</span>, so <span class="math notranslate nohighlight">\(\hat{\beta}_1 = \mathbf{a}_1^\top\mathbf{y}\)</span> is the OLS estimator.</p>
<p class="sd-card-text">Since OLS is unbiased: <span class="math notranslate nohighlight">\(\mathbf{a}_1^\top\mathbf{X} = (0, 1)\)</span>.</p>
<p class="sd-card-text">Define <span class="math notranslate nohighlight">\(\mathbf{d} = \mathbf{c} - \mathbf{a}_1\)</span>. Then:</p>
<div class="math notranslate nohighlight">
\[\mathbf{d}^\top\mathbf{X} = \mathbf{c}^\top\mathbf{X} - \mathbf{a}_1^\top\mathbf{X} = (0, 1) - (0, 1) = (0, 0)\]</div>
<p class="sd-card-text">So <span class="math notranslate nohighlight">\(\mathbf{d}^\top\mathbf{X} = \mathbf{0}\)</span>.</p>
<p class="sd-card-text"><strong>Part (c): Variance calculation</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{Var}(\tilde{\beta}_1) &amp;= \text{Var}(\mathbf{c}^\top\mathbf{y}) = \mathbf{c}^\top\text{Var}(\mathbf{y})\mathbf{c} = \sigma^2\mathbf{c}^\top\mathbf{c} \\
&amp;= \sigma^2(\mathbf{a}_1 + \mathbf{d})^\top(\mathbf{a}_1 + \mathbf{d}) \\
&amp;= \sigma^2(\mathbf{a}_1^\top\mathbf{a}_1 + 2\mathbf{a}_1^\top\mathbf{d} + \mathbf{d}^\top\mathbf{d})\end{split}\]</div>
<p class="sd-card-text">Now, <span class="math notranslate nohighlight">\(\mathbf{a}_1 = [(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top]^\top_{\cdot,2}\)</span> (second column of <span class="math notranslate nohighlight">\(\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\)</span>).</p>
<p class="sd-card-text">Since <span class="math notranslate nohighlight">\(\mathbf{d}^\top\mathbf{X} = \mathbf{0}\)</span>, we have <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{d} = \mathbf{0}\)</span>.</p>
<p class="sd-card-text">Using the explicit form of <span class="math notranslate nohighlight">\(\mathbf{a}_1\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbf{a}_1 = \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{e}_2\]</div>
<p class="sd-card-text">where <span class="math notranslate nohighlight">\(\mathbf{e}_2 = (0, 1)^\top\)</span>. Then:</p>
<div class="math notranslate nohighlight">
\[\mathbf{a}_1^\top\mathbf{d} = \mathbf{e}_2^\top(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{d} = \mathbf{e}_2^\top(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{0} = 0\]</div>
<p class="sd-card-text">Therefore:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\tilde{\beta}_1) = \sigma^2\mathbf{a}_1^\top\mathbf{a}_1 + \sigma^2\mathbf{d}^\top\mathbf{d} = \text{Var}(\hat{\beta}_1) + \sigma^2\|\mathbf{d}\|^2\]</div>
<p class="sd-card-text"><strong>Part (d): Conclusion</strong></p>
<p class="sd-card-text">Since <span class="math notranslate nohighlight">\(\|\mathbf{d}\|^2 \geq 0\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\tilde{\beta}_1) \geq \text{Var}(\hat{\beta}_1)\]</div>
<p class="sd-card-text">with equality if and only if <span class="math notranslate nohighlight">\(\mathbf{d} = \mathbf{0}\)</span>, i.e., <span class="math notranslate nohighlight">\(\mathbf{c} = \mathbf{a}_1\)</span>.</p>
<p class="sd-card-text">This proves <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> is the <strong>Best</strong> (minimum variance) <strong>Linear Unbiased Estimator</strong>.</p>
<p class="sd-card-text"><strong>Key Insight</strong>: Any deviation <span class="math notranslate nohighlight">\(\mathbf{d}\)</span> from the OLS weights must be orthogonal to <span class="math notranslate nohighlight">\(C(\mathbf{X})\)</span> to preserve unbiasedness. But such deviations can only <em>add</em> variance, never reduce it. OLS is the unique minimum-variance choice.</p>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 5: Computational Practice</p>
<p>Theory becomes concrete through computation. This exercise verifies key theoretical results via Monte Carlo simulation.</p>
<div class="note admonition">
<p class="admonition-title">Background: Why Simulate?</p>
<p>Simulation provides empirical verification of theoretical formulas. When theory says <span class="math notranslate nohighlight">\(\mathbb{E}[\hat{\beta}] = \beta\)</span> and <span class="math notranslate nohighlight">\(\text{Var}(\hat{\beta}) = \sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}\)</span>, simulation confirms these hold—building confidence and catching errors.</p>
</div>
<p>Using Python, verify key theoretical results.</p>
<ol class="loweralpha simple">
<li><p>Generate data from <span class="math notranslate nohighlight">\(y = 2 + 3x + \varepsilon\)</span> with <span class="math notranslate nohighlight">\(\varepsilon \sim N(0, 4)\)</span>, <span class="math notranslate nohighlight">\(n = 50\)</span>. Fit OLS and verify <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}} \approx (2, 3)\)</span>.</p></li>
<li><p>Compute the hat matrix <span class="math notranslate nohighlight">\(\mathbf{H}\)</span> and verify: <span class="math notranslate nohighlight">\(\mathbf{H} = \mathbf{H}^\top\)</span>, <span class="math notranslate nohighlight">\(\mathbf{H}^2 = \mathbf{H}\)</span>, <span class="math notranslate nohighlight">\(\text{tr}(\mathbf{H}) = p\)</span>.</p></li>
<li><p>Verify that <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{e} = \mathbf{0}\)</span> (residuals orthogonal to predictors).</p></li>
<li><p>Simulate 10,000 datasets and verify that <span class="math notranslate nohighlight">\(\mathbb{E}[\hat{\beta}_1] = 3\)</span> and <span class="math notranslate nohighlight">\(\text{Var}(\hat{\beta}_1) \approx \sigma^2[(\mathbf{X}^\top\mathbf{X})^{-1}]_{22}\)</span>.</p></li>
<li><p>Verify that <span class="math notranslate nohighlight">\(s^2 = \text{RSS}/(n-p)\)</span> is unbiased for <span class="math notranslate nohighlight">\(\sigma^2 = 4\)</span>.</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">exercise_3_4_5</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Complete solution to Exercise 3.4.5.&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

    <span class="c1"># Parameters</span>
    <span class="n">beta_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="mf">2.0</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">50</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;EXERCISE 3.4.5: COMPUTATIONAL VERIFICATION&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>

    <span class="c1"># Part (a): Single dataset fit</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Part (a): Single Dataset OLS ---&quot;</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">x</span><span class="p">])</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta_true</span> <span class="o">+</span> <span class="n">eps</span>

    <span class="n">beta_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="kc">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True β = </span><span class="si">{</span><span class="n">beta_true</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;OLS β̂ = [</span><span class="si">{</span><span class="n">beta_hat</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">beta_hat</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>

    <span class="c1"># Part (b): Hat matrix properties</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Part (b): Hat Matrix Properties ---&quot;</span><span class="p">)</span>
    <span class="n">XtX_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">XtX_inv</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;H = H&#39; (symmetric):     </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">H</span><span class="p">,</span><span class="w"> </span><span class="n">H</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;H² = H (idempotent):    </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">H</span><span class="w"> </span><span class="o">@</span><span class="w"> </span><span class="n">H</span><span class="p">,</span><span class="w"> </span><span class="n">H</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;tr(H) = p = </span><span class="si">{</span><span class="n">p</span><span class="si">}</span><span class="s2">:        tr(H) = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">H</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Part (c): Orthogonality of residuals</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Part (c): Residual Orthogonality ---&quot;</span><span class="p">)</span>
    <span class="n">e</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta_hat</span>
    <span class="n">Xte</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">e</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;X&#39;e = [</span><span class="si">{</span><span class="n">Xte</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">Xte</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;||X&#39;e|| ≈ 0: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">Xte</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Parts (d) &amp; (e): Simulation study</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Parts (d) &amp; (e): Simulation Study (10,000 reps) ---&quot;</span><span class="p">)</span>
    <span class="n">n_sim</span> <span class="o">=</span> <span class="mi">10000</span>

    <span class="n">beta1_hats</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">s2_estimates</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Use FIXED X for simulation</span>
    <span class="n">x_fixed</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">X_fixed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">x_fixed</span><span class="p">])</span>
    <span class="n">XtX_inv_fixed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X_fixed</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X_fixed</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
        <span class="n">eps_sim</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
        <span class="n">y_sim</span> <span class="o">=</span> <span class="n">X_fixed</span> <span class="o">@</span> <span class="n">beta_true</span> <span class="o">+</span> <span class="n">eps_sim</span>

        <span class="n">beta_sim</span> <span class="o">=</span> <span class="n">XtX_inv_fixed</span> <span class="o">@</span> <span class="n">X_fixed</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y_sim</span>
        <span class="n">beta1_hats</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">beta_sim</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

        <span class="n">resid</span> <span class="o">=</span> <span class="n">y_sim</span> <span class="o">-</span> <span class="n">X_fixed</span> <span class="o">@</span> <span class="n">beta_sim</span>
        <span class="n">s2_estimates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">resid</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">p</span><span class="p">))</span>

    <span class="n">beta1_hats</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">beta1_hats</span><span class="p">)</span>
    <span class="n">s2_estimates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">s2_estimates</span><span class="p">)</span>

    <span class="c1"># Theoretical variance</span>
    <span class="n">var_theory</span> <span class="o">=</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">XtX_inv_fixed</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">For β̂₁ (slope):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  E[β̂₁] ≈ β₁ = </span><span class="si">{</span><span class="n">beta_true</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;    Sample mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">beta1_hats</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Var(β̂₁) = σ²[(X&#39;X)⁻¹]₂₂ = </span><span class="si">{</span><span class="n">var_theory</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;    Sample var:  </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">beta1_hats</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">For s² (variance estimator):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  E[s²] = σ² = </span><span class="si">{</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;    Sample mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">s2_estimates</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">exercise_3_4_5</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>============================================================
EXERCISE 3.4.5: COMPUTATIONAL VERIFICATION
============================================================

--- Part (a): Single Dataset OLS ---
True β = [2. 3.]
OLS β̂ = [2.119, 2.972]

--- Part (b): Hat Matrix Properties ---
H = H&#39; (symmetric):     True
H² = H (idempotent):    True
tr(H) = p = 2:        tr(H) = 2.000000

--- Part (c): Residual Orthogonality ---
X&#39;e = [1.42e-14, -3.55e-14]
||X&#39;e|| ≈ 0: True

--- Parts (d) &amp; (e): Simulation Study (10,000 reps) ---

For β̂₁ (slope):
  E[β̂₁] ≈ β₁ = 3.0:
    Sample mean: 3.0002
  Var(β̂₁) = σ²[(X&#39;X)⁻¹]₂₂ = 0.035294:
    Sample var:  0.035183

For s² (variance estimator):
  E[s²] = σ² = 4.0:
    Sample mean: 4.0024
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Hat matrix properties confirmed</strong>: <span class="math notranslate nohighlight">\(\mathbf{H}\)</span> is symmetric, idempotent, and has trace equal to <span class="math notranslate nohighlight">\(p\)</span> (number of parameters).</p></li>
<li><p class="sd-card-text"><strong>Orthogonality verified</strong>: <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{e} \approx \mathbf{0}\)</span> to machine precision—the residuals are exactly orthogonal to the column space.</p></li>
<li><p class="sd-card-text"><strong>Unbiasedness confirmed</strong>: Over 10,000 simulations, <span class="math notranslate nohighlight">\(\bar{\hat{\beta}}_1 = 3.0002 \approx \beta_1 = 3.0\)</span>.</p></li>
<li><p class="sd-card-text"><strong>Variance formula confirmed</strong>: Empirical variance matches <span class="math notranslate nohighlight">\(\sigma^2[(\mathbf{X}^\top\mathbf{X})^{-1}]_{22}\)</span> closely.</p></li>
<li><p class="sd-card-text"><strong>s² is unbiased</strong>: <span class="math notranslate nohighlight">\(\bar{s}^2 = 4.0024 \approx \sigma^2 = 4.0\)</span>.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 6: Robust Standard Errors</p>
<p>When heteroskedasticity is present, model-based standard errors are wrong. This exercise compares them to robust alternatives via simulation.</p>
<div class="note admonition">
<p class="admonition-title">Background: The Heteroskedasticity Problem</p>
<p>Under heteroskedasticity, <span class="math notranslate nohighlight">\(\text{Var}(\varepsilon_i) = \sigma_i^2\)</span> varies across observations. OLS remains unbiased but the formula <span class="math notranslate nohighlight">\(\text{Var}(\hat{\boldsymbol{\beta}}) = \sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}\)</span> is wrong. Robust (sandwich) standard errors correct this.</p>
</div>
<ol class="loweralpha simple">
<li><p>Generate heteroskedastic data: <span class="math notranslate nohighlight">\(y_i = 2 + 3x_i + \varepsilon_i\)</span> where <span class="math notranslate nohighlight">\(\varepsilon_i \sim N(0, (0.5 + x_i)^2)\)</span>.</p></li>
<li><p>Fit OLS and compute standard errors using both model-based and HC3 formulas.</p></li>
<li><p>Construct 95% CIs for <span class="math notranslate nohighlight">\(\beta_1\)</span> using both SE methods. Assess coverage via simulation (10,000 replications). Which achieves closer to 95% coverage?</p></li>
<li><p>Explain why the model-based SE is inappropriate here and how HC3 corrects the problem.</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Parts (a)-(c): Implementation and Coverage Simulation</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">robust_se_hc3</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">residuals</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute HC3 robust standard errors.&quot;&quot;&quot;</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">XtX_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">XtX_inv</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">H</span><span class="p">)</span>
    <span class="n">omega</span> <span class="o">=</span> <span class="n">residuals</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">h</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">meat</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">omega</span><span class="p">)</span> <span class="o">@</span> <span class="n">X</span>
    <span class="n">sandwich</span> <span class="o">=</span> <span class="n">XtX_inv</span> <span class="o">@</span> <span class="n">meat</span> <span class="o">@</span> <span class="n">XtX_inv</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">sandwich</span><span class="p">))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">coverage_simulation</span><span class="p">(</span><span class="n">n_sim</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare coverage of model-based vs HC3 standard errors.&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="n">beta_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>

    <span class="n">model_covers</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">hc3_covers</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sim</span><span class="p">):</span>
        <span class="c1"># Generate heteroskedastic data</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">+</span> <span class="n">x</span>  <span class="c1"># SD increases with x</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">beta_true</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">beta_true</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">eps</span>

        <span class="c1"># Design matrix</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">x</span><span class="p">])</span>

        <span class="c1"># OLS fit</span>
        <span class="n">XtX_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span>
        <span class="n">beta_hat</span> <span class="o">=</span> <span class="n">XtX_inv</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span>
        <span class="n">residuals</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta_hat</span>

        <span class="c1"># Model-based SE (assumes homoskedasticity)</span>
        <span class="n">s2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">residuals</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">se_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">XtX_inv</span><span class="p">))</span>

        <span class="c1"># HC3 robust SE</span>
        <span class="n">se_hc3</span> <span class="o">=</span> <span class="n">robust_se_hc3</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">residuals</span><span class="p">)</span>

        <span class="c1"># 95% CI for beta_1 (slope)</span>
        <span class="n">t_crit</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">ci_model</span> <span class="o">=</span> <span class="p">(</span><span class="n">beta_hat</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">t_crit</span> <span class="o">*</span> <span class="n">se_model</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                    <span class="n">beta_hat</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">t_crit</span> <span class="o">*</span> <span class="n">se_model</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">ci_hc3</span> <span class="o">=</span> <span class="p">(</span><span class="n">beta_hat</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">t_crit</span> <span class="o">*</span> <span class="n">se_hc3</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                  <span class="n">beta_hat</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">t_crit</span> <span class="o">*</span> <span class="n">se_hc3</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">ci_model</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">beta_true</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">ci_model</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">model_covers</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">ci_hc3</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">beta_true</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">ci_hc3</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">hc3_covers</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">model_covers</span> <span class="o">/</span> <span class="n">n_sim</span><span class="p">,</span> <span class="n">hc3_covers</span> <span class="o">/</span> <span class="n">n_sim</span>

<span class="c1"># Run simulation</span>
<span class="n">cov_model</span><span class="p">,</span> <span class="n">cov_hc3</span> <span class="o">=</span> <span class="n">coverage_simulation</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;HETEROSKEDASTICITY: MODEL-BASED VS ROBUST SE&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">95% CI Coverage for β₁ (10,000 simulations):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Model-based SE: </span><span class="si">{</span><span class="n">cov_model</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (target: 0.950)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  HC3 robust SE:  </span><span class="si">{</span><span class="n">cov_hc3</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (target: 0.950)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Model-based </span><span class="si">{</span><span class="s1">&#39;UNDERCOVERED&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">cov_model</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mf">0.93</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;OK&#39;</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;HC3 </span><span class="si">{</span><span class="s1">&#39;achieves nominal coverage&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="mf">0.94</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">cov_hc3</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="mf">0.96</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;slightly off&#39;</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>============================================================
HETEROSKEDASTICITY: MODEL-BASED VS ROBUST SE
============================================================

95% CI Coverage for β₁ (10,000 simulations):
  Model-based SE: 0.912 (target: 0.950)
  HC3 robust SE:  0.948 (target: 0.950)

Model-based UNDERCOVERED
HC3 achieves nominal coverage
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): Explanation</strong></p>
<p class="sd-card-text">The model-based SE assumes <span class="math notranslate nohighlight">\(\text{Var}(\varepsilon_i) = \sigma^2\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>. When variance increases with <span class="math notranslate nohighlight">\(x\)</span>, this assumption fails.</p>
<p class="sd-card-text"><strong>Why model-based SE fails</strong>: The formula <span class="math notranslate nohighlight">\(s^2(\mathbf{X}^\top\mathbf{X})^{-1}\)</span> uses a pooled <span class="math notranslate nohighlight">\(s^2\)</span> that averages across all observations. This underestimates variance where <span class="math notranslate nohighlight">\(x\)</span> is large (variance actually higher) and overestimates where <span class="math notranslate nohighlight">\(x\)</span> is small. Since high-<span class="math notranslate nohighlight">\(x\)</span> observations carry more weight in determining <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span>, the net effect is that model-based SE is <strong>too small</strong>, leading to CIs that are too narrow and systematic undercoverage.</p>
<p class="sd-card-text"><strong>How HC3 corrects</strong>: The sandwich formula uses observation-specific variance estimates:</p>
<div class="math notranslate nohighlight">
\[\widehat{\text{Var}}(\hat{\boldsymbol{\beta}}) = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\hat{\boldsymbol{\Omega}}\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\]</div>
<p class="sd-card-text">where <span class="math notranslate nohighlight">\(\hat{\Omega}_{ii} = e_i^2/(1-h_{ii})^2\)</span> for HC3.</p>
<ul class="simple">
<li><p class="sd-card-text">Each observation contributes its own squared residual</p></li>
<li><p class="sd-card-text">The <span class="math notranslate nohighlight">\((1-h_{ii})^2\)</span> denominator inflates residuals for high-leverage points (which tend to have smaller residuals even when variance is large)</p></li>
<li><p class="sd-card-text">This provides a conservative correction that achieves approximately correct coverage even under heteroskedasticity</p></li>
</ul>
<p class="sd-card-text"><strong>Key Insight</strong>: Robust standard errors <strong>do not change the point estimates</strong>—OLS <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> is still unbiased. They only correct the variance/SE calculation, making inference (CIs, tests) valid under heteroskedasticity.</p>
</div>
</details></div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<p><strong>Historical Origins of Least Squares</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="legendre1805" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Legendre1805<span class="fn-bracket">]</span></span>
<p>Legendre, A. M. (1805). <em>Nouvelles méthodes pour la détermination des orbites des comètes</em>. Firmin Didot, Paris. Appendix contains the first published account of the method of least squares, developed for fitting parabolic orbits to comet observations.</p>
</div>
<div class="citation" id="gauss1809" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Gauss1809<span class="fn-bracket">]</span></span>
<p>Gauss, C. F. (1809). <em>Theoria Motus Corporum Coelestium in Sectionibus Conicis Solem Ambientium</em>. Perthes and Besser, Hamburg. Contains Gauss’s probabilistic justification of least squares under normally distributed errors, applied to predicting the orbit of the asteroid Ceres.</p>
</div>
<div class="citation" id="gauss1821" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Gauss1821<span class="fn-bracket">]</span></span>
<p>Gauss, C. F. (1821–1823). <em>Theoria Combinationis Observationum Erroribus Minimis Obnoxiae</em>. Commentationes Societatis Regiae Scientiarum Gottingensis. English translation by G. W. Stewart (1995), SIAM. Proves that least squares minimizes variance among linear unbiased estimators without assuming normality—the original Gauss-Markov theorem.</p>
</div>
<div class="citation" id="stigler1981" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Stigler1981<span class="fn-bracket">]</span></span>
<p>Stigler, S. M. (1981). Gauss and the invention of least squares. <em>Annals of Statistics</em>, 9(3), 465–474. Historical analysis of the priority dispute between Gauss and Legendre over the invention of least squares.</p>
</div>
</div>
<p><strong>The Gauss-Markov Theorem</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="markov1900" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Markov1900<span class="fn-bracket">]</span></span>
<p>Markov, A. A. (1900). <em>Wahrscheinlichkeitsrechnung</em>. Teubner, Leipzig. Contains Markov’s rediscovery and generalization of Gauss’s optimality result for least squares.</p>
</div>
<div class="citation" id="aitken1935" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Aitken1935<span class="fn-bracket">]</span></span>
<p>Aitken, A. C. (1935). On least squares and linear combination of observations. <em>Proceedings of the Royal Society of Edinburgh</em>, 55, 42–48. Generalizes least squares to handle correlated errors through the weighted least squares estimator (generalized least squares).</p>
</div>
<div class="citation" id="plackett1950" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Plackett1950<span class="fn-bracket">]</span></span>
<p>Plackett, R. L. (1950). Some theorems in least squares. <em>Biometrika</em>, 37(1/2), 149–157. Clarifies and extends the Gauss-Markov theorem with modern notation.</p>
</div>
<div class="citation" id="kruskal1968" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Kruskal1968<span class="fn-bracket">]</span></span>
<p>Kruskal, W. (1968). When are Gauss-Markov and least squares estimators identical? A coordinate-free approach. <em>Annals of Mathematical Statistics</em>, 39(1), 70–75. Geometric perspective on the Gauss-Markov theorem using coordinate-free notation.</p>
</div>
</div>
<p><strong>Distributional Theory and Inference</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="student1908" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Student1908<span class="fn-bracket">]</span></span>
<p>Student [Gosset, W. S.] (1908). The probable error of a mean. <em>Biometrika</em>, 6(1), 1–25. Introduces the t-distribution for inference on means with estimated variance—fundamental for regression coefficient testing.</p>
</div>
<div class="citation" id="fisher1925" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Fisher1925<span class="fn-bracket">]</span></span>
<p>Fisher, R. A. (1925). <em>Statistical Methods for Research Workers</em>. Oliver and Boyd. Develops the F-test for comparing nested models and analysis of variance for regression.</p>
</div>
<div class="citation" id="cochran1934" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Cochran1934<span class="fn-bracket">]</span></span>
<p>Cochran, W. G. (1934). The distribution of quadratic forms in a normal system, with applications to the analysis of covariance. <em>Mathematical Proceedings of the Cambridge Philosophical Society</em>, 30(2), 178–191. Cochran’s theorem on distributions of quadratic forms, fundamental for understanding F-tests and ANOVA decompositions.</p>
</div>
<div class="citation" id="scheffe1959" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Scheffe1959<span class="fn-bracket">]</span></span>
<p>Scheffé, H. (1959). <em>The Analysis of Variance</em>. Wiley. Comprehensive treatment of linear models from the ANOVA perspective with rigorous distributional theory.</p>
</div>
</div>
<p><strong>Matrix Approach to Linear Models</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="graybill1961" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Graybill1961<span class="fn-bracket">]</span></span>
<p>Graybill, F. A. (1961). <em>An Introduction to Linear Statistical Models</em>. McGraw-Hill. Early systematic treatment of linear models using matrix notation.</p>
</div>
<div class="citation" id="searle1971" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Searle1971<span class="fn-bracket">]</span></span>
<p>Searle, S. R. (1971). <em>Linear Models</em>. Wiley. Definitive reference on the matrix theory of linear models including rank-deficient designs.</p>
</div>
<div class="citation" id="rao1973" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Rao1973<span class="fn-bracket">]</span></span>
<p>Rao, C. R. (1973). <em>Linear Statistical Inference and Its Applications</em> (2nd ed.). Wiley. Graduate-level treatment emphasizing linear algebra foundations of statistical inference.</p>
</div>
</div>
<p><strong>Diagnostics and Model Checking</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="cook1977" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Cook1977<span class="fn-bracket">]</span></span>
<p>Cook, R. D. (1977). Detection of influential observation in linear regression. <em>Technometrics</em>, 19(1), 15–18. Introduces Cook’s distance for identifying influential observations.</p>
</div>
<div class="citation" id="belsley1980" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Belsley1980<span class="fn-bracket">]</span></span>
<p>Belsley, D. A., Kuh, E., and Welsch, R. E. (1980). <em>Regression Diagnostics: Identifying Influential Data and Sources of Collinearity</em>. Wiley. Comprehensive treatment of regression diagnostics including condition numbers and influence measures.</p>
</div>
<div class="citation" id="cook1982" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Cook1982<span class="fn-bracket">]</span></span>
<p>Cook, R. D., and Weisberg, S. (1982). <em>Residuals and Influence in Regression</em>. Chapman and Hall. Monograph on diagnostic methods including added-variable plots and influence functions.</p>
</div>
</div>
<p><strong>Heteroskedasticity and Robust Methods</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="white1980" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>White1980<span class="fn-bracket">]</span></span>
<p>White, H. (1980). A heteroskedasticity-consistent covariance matrix estimator and a direct test for heteroskedasticity. <em>Econometrica</em>, 48(4), 817–838. Introduces heteroskedasticity-robust standard errors for regression coefficients.</p>
</div>
<div class="citation" id="breusch1979" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Breusch1979<span class="fn-bracket">]</span></span>
<p>Breusch, T. S., and Pagan, A. R. (1979). A simple test for heteroscedasticity and random coefficient variation. <em>Econometrica</em>, 47(5), 1287–1294. The Breusch-Pagan test for detecting heteroskedasticity.</p>
</div>
<div class="citation" id="long2000" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Long2000<span class="fn-bracket">]</span></span>
<p>Long, J. S., and Ervin, L. H. (2000). Using heteroscedasticity consistent standard errors in the linear regression model. <em>The American Statistician</em>, 54(3), 217–224. Practical guidance on when and how to use robust standard errors.</p>
</div>
</div>
<p><strong>Modern Perspectives</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="seber2003" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Seber2003<span class="fn-bracket">]</span></span>
<p>Seber, G. A. F., and Lee, A. J. (2003). <em>Linear Regression Analysis</em> (2nd ed.). Wiley. Comprehensive modern treatment of linear regression including computational methods.</p>
</div>
<div class="citation" id="rencher2008" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Rencher2008<span class="fn-bracket">]</span></span>
<p>Rencher, A. C., and Schaalje, G. B. (2008). <em>Linear Models in Statistics</em> (2nd ed.). Wiley. Graduate textbook covering general linear models with extensive applications.</p>
</div>
<div class="citation" id="weisberg2014" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Weisberg2014<span class="fn-bracket">]</span></span>
<p>Weisberg, S. (2014). <em>Applied Linear Regression</em> (4th ed.). Wiley. Accessible applied treatment emphasizing diagnostics and practical considerations.</p>
</div>
</div>
<p><strong>Computational Aspects</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="golub1996" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Golub1996<span class="fn-bracket">]</span></span>
<p>Golub, G. H., and Van Loan, C. F. (1996). <em>Matrix Computations</em> (3rd ed.). Johns Hopkins University Press. Definitive reference on numerical linear algebra including QR decomposition for least squares.</p>
</div>
<div class="citation" id="bjorck1996" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Björck1996<span class="fn-bracket">]</span></span>
<p>Björck, Å. (1996). <em>Numerical Methods for Least Squares Problems</em>. SIAM. Comprehensive treatment of computational methods for solving least squares problems.</p>
</div>
</div>
<p><strong>Historical Perspective</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="stigler1986" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Stigler1986<span class="fn-bracket">]</span></span>
<p>Stigler, S. M. (1986). <em>The History of Statistics: The Measurement of Uncertainty before 1900</em>. Harvard University Press. Historical context for the development of regression and least squares methods.</p>
</div>
<div class="citation" id="magnus2019" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Magnus2019<span class="fn-bracket">]</span></span>
<p>Magnus, J. R. (2019). On the concept of matrix derivative. <em>Journal of Multivariate Analysis</em>, 169, 94–119. Modern perspective on matrix calculus notation used in deriving least squares estimators.</p>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="ch3_3-sampling-variability.html" class="btn btn-neutral float-left" title="Sampling Variability and Variance Estimation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ch3_5-generalized-linear-models.html" class="btn btn-neutral float-right" title="Generalized Linear Models" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>