

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>2.1.6. Variance Reduction Methods &mdash; STAT 418</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=dc393e06" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT418/Website/part2_simulation/chapter2/ch2_6-variance-reduction-methods.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=30646c52"></script>
      <script>let toggleHintShow = 'Show solution';</script>
      <script>let toggleHintHide = 'Hide solution';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>window.MathJax = {"options": {"enableMenu": true, "menuOptions": {"settings": {"enrich": true, "speech": true, "braille": true, "collapsible": true, "assistiveMml": false}}}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
      <script src="../../_static/custom.js?v=8718e0ab"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="2.1.7. Chapter Summary" href="ch2_7-chapter-summary.html" />
    <link rel="prev" title="2.1.5. Rejection Sampling" href="ch2_5-rejection-sampling.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Course Content</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../part1_foundations/index.html">1. Part I: Foundations of Probability and Computation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part1_foundations/chapter1/index.html">1.1. Chapter 1: Statistical Paradigms and Core Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html">1.1.1. Paradigms of Probability and Statistical Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#the-mathematical-foundation-kolmogorov-s-axioms">The Mathematical Foundation: Kolmogorov‚Äôs Axioms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#interpretations-of-probability">Interpretations of Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#statistical-inference-paradigms">Statistical Inference Paradigms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#historical-and-philosophical-debates">Historical and Philosophical Debates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#looking-ahead-our-course-focus">Looking Ahead: Our Course Focus</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html">1.1.2. Probability Distributions: Theory and Computation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#from-abstract-foundations-to-concrete-tools">From Abstract Foundations to Concrete Tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#the-python-ecosystem-for-probability">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#introduction-why-probability-distributions-matter">Introduction: Why Probability Distributions Matter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#id1">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#continuous-distributions">Continuous Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#additional-important-distributions">Additional Important Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#summary-and-practical-guidelines">Summary and Practical Guidelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#conclusion">Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html">1.1.3. Python Random Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#from-mathematical-distributions-to-computational-samples">From Mathematical Distributions to Computational Samples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-python-ecosystem-at-a-glance">The Python Ecosystem at a Glance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#understanding-pseudo-random-number-generation">Understanding Pseudo-Random Number Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-standard-library-random-module">The Standard Library: <code class="docutils literal notranslate"><span class="pre">random</span></code> Module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#numpy-fast-vectorized-random-sampling">NumPy: Fast Vectorized Random Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#scipy-stats-the-complete-statistical-toolkit">SciPy Stats: The Complete Statistical Toolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#bringing-it-all-together-library-selection-guide">Bringing It All Together: Library Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#looking-ahead-from-random-numbers-to-monte-carlo-methods">Looking Ahead: From Random Numbers to Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html">1.1.4. Chapter 1 Summary: Foundations in Place</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#the-three-pillars-of-chapter-1">The Three Pillars of Chapter 1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#what-lies-ahead-the-road-to-simulation">What Lies Ahead: The Road to Simulation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#chapter-1-exercises-synthesis-problems">Chapter 1 Exercises: Synthesis Problems</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">2. Part II: Simulation-Based Methods</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="index.html">2.1. Chapter 2: Monte Carlo Simulation</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html">2.1.1. Monte Carlo Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#the-historical-development-of-monte-carlo-methods">The Historical Development of Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#the-core-principle-expectation-as-integration">The Core Principle: Expectation as Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#theoretical-foundations">Theoretical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#variance-estimation-and-confidence-intervals">Variance Estimation and Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#comparison-with-deterministic-methods">Comparison with Deterministic Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#sample-size-determination">Sample Size Determination</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#convergence-diagnostics-and-monitoring">Convergence Diagnostics and Monitoring</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#chapter-2-1-exercises-monte-carlo-fundamentals-mastery">Chapter 2.1 Exercises: Monte Carlo Fundamentals Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_2-uniform-random-variates.html">2.1.2. Uniform Random Variates</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#why-uniform-the-universal-currency-of-randomness">Why Uniform? The Universal Currency of Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#the-paradox-of-computational-randomness">The Paradox of Computational Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#chaotic-dynamical-systems-an-instructive-failure">Chaotic Dynamical Systems: An Instructive Failure</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#linear-congruential-generators">Linear Congruential Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#shift-register-generators">Shift-Register Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#the-kiss-generator-combining-strategies">The KISS Generator: Combining Strategies</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#modern-generators-mersenne-twister-and-pcg">Modern Generators: Mersenne Twister and PCG</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#statistical-testing-of-random-number-generators">Statistical Testing of Random Number Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#chapter-2-2-exercises-uniform-random-variates-mastery">Chapter 2.2 Exercises: Uniform Random Variates Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_3-inverse-cdf-method.html">2.1.3. Inverse CDF Method</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#mathematical-foundations">Mathematical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#continuous-distributions-with-closed-form-inverses">Continuous Distributions with Closed-Form Inverses</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#numerical-inversion">Numerical Inversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#mixed-distributions">Mixed Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#chapter-2-3-exercises-inverse-cdf-method-mastery">Chapter 2.3 Exercises: Inverse CDF Method Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_4-transformation-methods.html">2.1.4. Transformation Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#why-transformation-methods">Why Transformation Methods?</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#the-boxmuller-transform">The Box‚ÄìMuller Transform</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#the-polar-marsaglia-method">The Polar (Marsaglia) Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#method-comparison-boxmuller-vs-polar-vs-ziggurat">Method Comparison: Box‚ÄìMuller vs Polar vs Ziggurat</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#the-ziggurat-algorithm">The Ziggurat Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#the-clt-approximation-historical">The CLT Approximation (Historical)</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#distributions-derived-from-the-normal">Distributions Derived from the Normal</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#multivariate-normal-generation">Multivariate Normal Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#implementation-guidance">Implementation Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#chapter-2-4-exercises-transformation-methods-mastery">Chapter 2.4 Exercises: Transformation Methods Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_5-rejection-sampling.html">2.1.5. Rejection Sampling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#the-dartboard-intuition">The Dartboard Intuition</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#the-accept-reject-algorithm">The Accept-Reject Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#efficiency-analysis">Efficiency Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#choosing-the-proposal-distribution">Choosing the Proposal Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#the-squeeze-principle">The Squeeze Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#geometric-example-sampling-from-the-unit-disk">Geometric Example: Sampling from the Unit Disk</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#limitations-and-the-curse-of-dimensionality">Limitations and the Curse of Dimensionality</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#connections-to-other-methods">Connections to Other Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#chapter-2-5-exercises-rejection-sampling-mastery">Chapter 2.5 Exercises: Rejection Sampling Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">2.1.6. Variance Reduction Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#the-variance-reduction-paradigm">The Variance Reduction Paradigm</a></li>
<li class="toctree-l4"><a class="reference internal" href="#importance-sampling">Importance Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="#control-variates">Control Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="#antithetic-variates">Antithetic Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="#stratified-sampling">Stratified Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="#common-random-numbers">Common Random Numbers</a></li>
<li class="toctree-l4"><a class="reference internal" href="#conditional-monte-carlo-raoblackwellization">Conditional Monte Carlo (Rao‚ÄìBlackwellization)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#combining-variance-reduction-techniques">Combining Variance Reduction Techniques</a></li>
<li class="toctree-l4"><a class="reference internal" href="#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="#chapter-2-6-exercises-variance-reduction-mastery">Chapter 2.6 Exercises: Variance Reduction Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_7-chapter-summary.html">2.1.7. Chapter Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#the-complete-monte-carlo-workflow">The Complete Monte Carlo Workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#quick-reference-tables">Quick Reference Tables</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#common-pitfalls-checklist">Common Pitfalls Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#connections-to-later-chapters">Connections to Later Chapters</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#learning-outcomes-checklist">Learning Outcomes Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#further-reading-optimization-and-missing-data">Further Reading: Optimization and Missing Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter3/index.html">2.2. Chapter 3: Parametric Inference and Likelihood Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html">2.2.1. Exponential Families</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#historical-origins-from-scattered-results-to-unified-theory">Historical Origins: From Scattered Results to Unified Theory</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#the-canonical-exponential-family">The Canonical Exponential Family</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#converting-familiar-distributions">Converting Familiar Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#the-log-partition-function-a-moment-generating-machine">The Log-Partition Function: A Moment-Generating Machine</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#sufficiency-capturing-all-parameter-information">Sufficiency: Capturing All Parameter Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#minimal-sufficiency-and-completeness">Minimal Sufficiency and Completeness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#conjugate-priors-and-bayesian-inference">Conjugate Priors and Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#exponential-dispersion-models-and-glms">Exponential Dispersion Models and GLMs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#chapter-3-1-exercises-exponential-families-mastery">Chapter 3.1 Exercises: Exponential Families Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#further-reading">Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html">2.2.2. Maximum Likelihood Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-likelihood-function">The Likelihood Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-score-function">The Score Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#fisher-information">Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#closed-form-maximum-likelihood-estimators">Closed-Form Maximum Likelihood Estimators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#numerical-optimization-for-mle">Numerical Optimization for MLE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#asymptotic-properties-of-mles">Asymptotic Properties of MLEs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-cramer-rao-lower-bound">The Cram√©r-Rao Lower Bound</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-invariance-property">The Invariance Property</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#likelihood-based-hypothesis-testing">Likelihood-Based Hypothesis Testing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#confidence-intervals-from-likelihood">Confidence Intervals from Likelihood</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#connection-to-bayesian-inference">Connection to Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#chapter-3-2-exercises-maximum-likelihood-estimation-mastery">Chapter 3.2 Exercises: Maximum Likelihood Estimation Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html">2.2.3. Sampling Variability and Variance Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#statistical-estimators-and-their-properties">Statistical Estimators and Their Properties</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#sampling-distributions">Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#the-delta-method">The Delta Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#variance-estimation-methods">Variance Estimation Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#applications-and-worked-examples">Applications and Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#exercises">Exercises</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter4/index.html">2.3. Chapter 4: Resampling Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/jackknife_introduction.html">2.3.1. Jackknife Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html">2.3.2. Bootstrap Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html">2.3.3. Nonparametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/parametric_bootstrap.html">2.3.4. Parametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/confidence_intervals.html">2.3.5. Confidence Intervals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/bias_correction.html">2.3.6. Bias Correction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/cross_validation_loo.html">2.3.7. Cross Validation Loo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html">2.3.8. Cross Validation K Fold</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/model_selection.html">2.3.9. Model Selection</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part3_bayesian/index.html">Part III: Bayesian Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part3_bayesian/index.html#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html">1. Bayesian Philosophy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#introduction">1.1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#key-concepts">1.2. Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#mathematical-framework">1.3. Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#python-implementation">1.4. Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#examples">1.5. Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#summary">1.6. Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html"><span class="section-number">2. </span>Part II: Simulation-Based Methods</a></li>
          <li class="breadcrumb-item"><a href="index.html"><span class="section-number">2.1. </span>Chapter 2: Monte Carlo Simulation</a></li>
      <li class="breadcrumb-item active"><span class="section-number">2.1.6. </span>Variance Reduction Methods</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/part2_simulation/chapter2/ch2_6-variance-reduction-methods.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="variance-reduction-methods">
<span id="ch2-6-variance-reduction-methods"></span><h1><span class="section-number">2.1.6. </span>Variance Reduction Methods<a class="headerlink" href="#variance-reduction-methods" title="Link to this heading">ÔÉÅ</a></h1>
<p>The preceding sections developed the machinery for Monte Carlo integration: we estimate integrals <span class="math notranslate nohighlight">\(I = \mathbb{E}_f[h(X)]\)</span> by averaging samples <span class="math notranslate nohighlight">\(\hat{I}_n = n^{-1}\sum_{i=1}^n h(X_i)\)</span>. The Law of Large Numbers guarantees convergence, and the Central Limit Theorem quantifies uncertainty through the asymptotic relationship <span class="math notranslate nohighlight">\(\sqrt{n}(\hat{I}_n - I) \xrightarrow{d} \mathcal{N}(0, \sigma^2)\)</span>. But there is a catch: the convergence rate <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> is immutable. To halve our standard error, we must quadruple our sample size. For problems requiring high precision or expensive function evaluations, this brute-force approach becomes prohibitive.</p>
<p><strong>Variance reduction methods</strong> attack this limitation not by changing the convergence <em>rate</em>‚Äîthat remains fixed at <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span>‚Äîbut by shrinking the <em>constant</em> <span class="math notranslate nohighlight">\(\sigma^2\)</span>. The estimator variance <span class="math notranslate nohighlight">\(\text{Var}(\hat{I}_n) = \sigma^2/n\)</span> depends on two quantities we control: the sample size <span class="math notranslate nohighlight">\(n\)</span> and the variance constant <span class="math notranslate nohighlight">\(\sigma^2\)</span>. While increasing <span class="math notranslate nohighlight">\(n\)</span> requires more computation, reducing <span class="math notranslate nohighlight">\(\sigma^2\)</span> through clever sampling strategies can achieve the same precision at a fraction of the cost.</p>
<p>This insight has deep roots in the history of computational science. Herman Kahn at the RAND Corporation developed <strong>importance sampling</strong> in 1949‚Äì1951 for nuclear shielding calculations, where naive Monte Carlo required billions of particle simulations to estimate rare transmission events. Jerzy Neyman‚Äôs 1934 work on <strong>stratified sampling</strong> in survey statistics established optimal allocation theory decades before computers existed. <strong>Antithetic variates</strong>, introduced by Hammersley and Morton in 1956, and <strong>control variates</strong>, systematized in Hammersley and Handscomb‚Äôs 1964 monograph, completed the classical variance reduction toolkit. These techniques, born from practical necessity, now form the foundation of computational efficiency in Monte Carlo simulation.</p>
<p>The methods share a common philosophy: <em>exploit structure in the problem to reduce randomness in the estimate</em>. Importance sampling concentrates effort where the integrand matters most. Control variates leverage correlation with analytically tractable quantities. Antithetic variates induce cancellation through negative dependence. Stratified sampling ensures balanced coverage of the domain. Common random numbers synchronize randomness across comparisons to isolate true differences from sampling noise.</p>
<p>This section develops five foundational variance reduction techniques with complete mathematical derivations, proofs of optimality, and practical Python implementations. We emphasize when each method excels, how to combine them synergistically, and what pitfalls await the unwary practitioner.</p>
<div class="important admonition">
<p class="admonition-title">Road Map üß≠</p>
<ul class="simple">
<li><p><strong>Understand</strong>: The theoretical foundations of variance reduction‚Äîhow reweighting, correlation, and stratification reduce estimator variance without changing convergence rates</p></li>
<li><p><strong>Derive</strong>: Optimal coefficients and allocations for each method, including proofs of the zero-variance ideal for importance sampling and the Neyman allocation for stratified sampling</p></li>
<li><p><strong>Implement</strong>: Numerically stable Python code for all five techniques, with attention to log-space calculations, effective sample size diagnostics, and adaptive coefficient estimation</p></li>
<li><p><strong>Evaluate</strong>: When each method applies, expected variance reduction factors, and potential failure modes‚Äîespecially weight degeneracy in importance sampling and non-monotonicity failures in antithetic variates</p></li>
<li><p><strong>Connect</strong>: How variance reduction relates to the rejection sampling of <a class="reference internal" href="ch2_5-rejection-sampling.html#ch2-5-rejection-sampling"><span class="std std-ref">Rejection Sampling</span></a> and motivates the Markov Chain Monte Carlo methods of later chapters</p></li>
</ul>
</div>
<section id="the-variance-reduction-paradigm">
<h2>The Variance Reduction Paradigm<a class="headerlink" href="#the-variance-reduction-paradigm" title="Link to this heading">ÔÉÅ</a></h2>
<p>Before examining specific techniques, we establish the mathematical framework that unifies all variance reduction methods.</p>
<section id="the-fundamental-variance-decomposition">
<h3>The Fundamental Variance Decomposition<a class="headerlink" href="#the-fundamental-variance-decomposition" title="Link to this heading">ÔÉÅ</a></h3>
<p>For a Monte Carlo estimator <span class="math notranslate nohighlight">\(\hat{I}_n = n^{-1}\sum_{i=1}^n Y_i\)</span> where the <span class="math notranslate nohighlight">\(Y_i\)</span> are i.i.d. with mean <span class="math notranslate nohighlight">\(I\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-variance-decomposition">
<span class="eqno">(2.14)<a class="headerlink" href="#equation-variance-decomposition" title="Link to this equation">ÔÉÅ</a></span>\[\text{Var}(\hat{I}_n) = \frac{\sigma^2}{n} = \frac{\text{Var}(Y)}{n}\]</div>
<p>The mean squared error equals the variance (since <span class="math notranslate nohighlight">\(\hat{I}_n\)</span> is unbiased), and precision scales as <span class="math notranslate nohighlight">\(\sigma/\sqrt{n}\)</span>. Every variance reduction method operates by constructing alternative estimators <span class="math notranslate nohighlight">\(\tilde{Y}_i\)</span> with the same expectation <span class="math notranslate nohighlight">\(\mathbb{E}[\tilde{Y}] = I\)</span> but smaller variance <span class="math notranslate nohighlight">\(\text{Var}(\tilde{Y}) &lt; \sigma^2\)</span>.</p>
<p><strong>Variance Reduction Factor (VRF)</strong>: The ratio <span class="math notranslate nohighlight">\(\text{VRF} = \sigma^2_{\text{naive}} / \sigma^2_{\text{reduced}}\)</span> quantifies improvement. A VRF of 10 means the variance-reduced estimator achieves the same precision as the naive estimator with 10√ó more samples. Equivalently, it achieves a given precision with only 10% of the computational effort.</p>
</section>
<section id="the-methods-at-a-glance">
<h3>The Methods at a Glance<a class="headerlink" href="#the-methods-at-a-glance" title="Link to this heading">ÔÉÅ</a></h3>
<table class="docutils align-default" id="id5">
<caption><span class="caption-number">Table 2.7 </span><span class="caption-text">Comparison of Variance Reduction Techniques</span><a class="headerlink" href="#id5" title="Link to this table">ÔÉÅ</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 25.0%" />
<col style="width: 25.0%" />
<col style="width: 30.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Mechanism</p></th>
<th class="head"><p>Key Requirement</p></th>
<th class="head"><p>Best Use Case</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Importance Sampling</p></td>
<td><p>Sample from proposal <span class="math notranslate nohighlight">\(g\)</span>, reweight by <span class="math notranslate nohighlight">\(f/g\)</span></p></td>
<td><p>Proposal covers target support</p></td>
<td><p>Rare events, heavy tails</p></td>
</tr>
<tr class="row-odd"><td><p>Control Variates</p></td>
<td><p>Subtract correlated variable with known mean</p></td>
<td><p>High correlation, known <span class="math notranslate nohighlight">\(\mathbb{E}[C]\)</span></p></td>
<td><p>Auxiliary quantities available</p></td>
</tr>
<tr class="row-even"><td><p>Antithetic Variates</p></td>
<td><p>Use negatively correlated pairs</p></td>
<td><p>Monotonic integrand</p></td>
<td><p>Low-dimensional smooth functions</p></td>
</tr>
<tr class="row-odd"><td><p>Stratified Sampling</p></td>
<td><p>Force balanced coverage across strata</p></td>
<td><p>Partition domain into regions</p></td>
<td><p>Heterogeneous integrands</p></td>
</tr>
<tr class="row-even"><td><p>Common Random Numbers</p></td>
<td><p>Share randomness across comparisons</p></td>
<td><p>Comparing similar systems</p></td>
<td><p>A/B tests, sensitivity analysis</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="importance-sampling">
<h2>Importance Sampling<a class="headerlink" href="#importance-sampling" title="Link to this heading">ÔÉÅ</a></h2>
<p>Importance sampling transforms Monte Carlo integration by sampling from a carefully chosen <strong>proposal distribution</strong> rather than the target. By concentrating computational effort where the integrand contributes most, importance sampling can achieve variance reductions of several orders of magnitude‚Äîessential for rare event estimation where naive Monte Carlo is hopelessly inefficient.</p>
<section id="motivation-and-problem-setup">
<h3>Motivation and Problem Setup<a class="headerlink" href="#motivation-and-problem-setup" title="Link to this heading">ÔÉÅ</a></h3>
<p><strong>The Core Problem</strong>: We wish to estimate the integral</p>
<div class="math notranslate nohighlight">
\[I = \mathbb{E}_f[h(X)] = \int h(x) f(x) \, dx\]</div>
<p>where <span class="math notranslate nohighlight">\(f(x)\)</span> is a probability density (the ‚Äútarget‚Äù) and <span class="math notranslate nohighlight">\(h(x)\)</span> is a measurable function. Naive Monte Carlo draws <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} f\)</span> and estimates <span class="math notranslate nohighlight">\(\hat{I}_n = n^{-1}\sum_{i=1}^n h(X_i)\)</span> with variance <span class="math notranslate nohighlight">\(\sigma^2/n\)</span> where <span class="math notranslate nohighlight">\(\sigma^2 = \text{Var}_f(h(X))\)</span>.</p>
<p><strong>The Insight</strong>: Naive MC samples uniformly according to <span class="math notranslate nohighlight">\(f\)</span>, regardless of where <span class="math notranslate nohighlight">\(h\)</span> is large. If <span class="math notranslate nohighlight">\(h(x)\)</span> is concentrated in a region where <span class="math notranslate nohighlight">\(f(x)\)</span> assigns low probability‚Äîas in rare event estimation‚Äîmost samples contribute little to the estimate. Importance sampling redirects effort to where it matters: sample from a proposal <span class="math notranslate nohighlight">\(g\)</span> that puts mass where <span class="math notranslate nohighlight">\(|h(x)|f(x)\)</span> is large, then correct via reweighting.</p>
<div class="note admonition">
<p class="admonition-title">Definition (Importance Sampling)</p>
<p>Let <span class="math notranslate nohighlight">\(f\)</span> be the target density and <span class="math notranslate nohighlight">\(g\)</span> be a proposal density satisfying the <strong>support condition</strong>: <span class="math notranslate nohighlight">\(g(x) &gt; 0\)</span> whenever <span class="math notranslate nohighlight">\(h(x)f(x) \neq 0\)</span>. The <strong>importance weight</strong> is</p>
<div class="math notranslate nohighlight">
\[w(x) = \frac{f(x)}{g(x)}\]</div>
<p>Given i.i.d. samples <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \sim g\)</span>, the <strong>importance sampling estimator</strong> is</p>
<div class="math notranslate nohighlight" id="equation-is-estimator">
<span class="eqno">(2.15)<a class="headerlink" href="#equation-is-estimator" title="Link to this equation">ÔÉÅ</a></span>\[\hat{I}_{\text{IS}} = \frac{1}{n} \sum_{i=1}^n h(X_i) w(X_i) = \frac{1}{n} \sum_{i=1}^n h(X_i) \frac{f(X_i)}{g(X_i)}\]</div>
</div>
<p><strong>Intuition</strong>: Each sample <span class="math notranslate nohighlight">\(X_i\)</span> from <span class="math notranslate nohighlight">\(g\)</span> is ‚Äúworth‚Äù <span class="math notranslate nohighlight">\(w(X_i)\)</span> samples from <span class="math notranslate nohighlight">\(f\)</span>. Regions where <span class="math notranslate nohighlight">\(g\)</span> oversamples relative to <span class="math notranslate nohighlight">\(f\)</span> (i.e., <span class="math notranslate nohighlight">\(g &gt; f\)</span>) receive low weights; regions where <span class="math notranslate nohighlight">\(g\)</span> undersamples receive high weights. The reweighting exactly corrects for the sampling bias.</p>
<figure class="align-center" id="id6">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_6_fig01_importance_sampling_concept.png"><img alt="Three-panel visualization of importance sampling showing target and proposal densities, weight function, and weighted samples" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_6_fig01_importance_sampling_concept.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.39 </span><span class="caption-text"><strong>Importance Sampling Concept.</strong> (a) The bimodal target <span class="math notranslate nohighlight">\(f(x)\)</span> and unimodal proposal <span class="math notranslate nohighlight">\(g(x)\)</span>. We sample from <span class="math notranslate nohighlight">\(g\)</span> but want to estimate <span class="math notranslate nohighlight">\(\mathbb{E}_f[h(X)]\)</span>. (b) The importance weight function <span class="math notranslate nohighlight">\(w(x) = f(x)/g(x)\)</span>, showing high weights where the target exceeds the proposal. (c) Samples from the proposal with marker size proportional to weight‚Äîthe reweighting corrects for sampling bias, effectively transforming proposal samples into target samples.</span><a class="headerlink" href="#id6" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
</section>
<section id="unbiasedness-and-variance">
<h3>Unbiasedness and Variance<a class="headerlink" href="#unbiasedness-and-variance" title="Link to this heading">ÔÉÅ</a></h3>
<div class="note admonition">
<p class="admonition-title">Proposition (Unbiasedness)</p>
<p>Under the support condition, <span class="math notranslate nohighlight">\(\mathbb{E}_g[\hat{I}_{\text{IS}}] = I\)</span>.</p>
</div>
<p><strong>Proof</strong> (2 lines):</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}_g[\hat{I}_{\text{IS}}] = \mathbb{E}_g\left[ h(X) \frac{f(X)}{g(X)} \right] = \int h(x) \frac{f(x)}{g(x)} g(x) \, dx = \int h(x) f(x) \, dx = I \quad \blacksquare\]</div>
<p><strong>Square-Integrability Condition</strong>: For finite variance, we additionally require <span class="math notranslate nohighlight">\(\mathbb{E}_g[(h(X)w(X))^2] &lt; \infty\)</span>, equivalently:</p>
<div class="math notranslate nohighlight">
\[\int \frac{[h(x)f(x)]^2}{g(x)} \, dx &lt; \infty\]</div>
</section>
<section id="variance-analysis">
<h3>Variance Analysis<a class="headerlink" href="#variance-analysis" title="Link to this heading">ÔÉÅ</a></h3>
<p>The variance of the importance sampling estimator reveals the critical role of proposal selection:</p>
<div class="math notranslate nohighlight" id="equation-is-variance">
<span class="eqno">(2.16)<a class="headerlink" href="#equation-is-variance" title="Link to this equation">ÔÉÅ</a></span>\[\text{Var}_g(\hat{I}_{\text{IS}}) = \frac{1}{n} \text{Var}_g[h(X)w(X)] = \frac{1}{n} \left[ \int \frac{[h(x)f(x)]^2}{g(x)} \, dx - I^2 \right]\]</div>
<p><strong>Full Derivation</strong>: By definition, <span class="math notranslate nohighlight">\(\text{Var}_g[Y] = \mathbb{E}_g[Y^2] - (\mathbb{E}_g[Y])^2\)</span> for <span class="math notranslate nohighlight">\(Y = h(X)w(X)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbb{E}_g[(hw)^2] &amp;= \int h(x)^2 \left(\frac{f(x)}{g(x)}\right)^2 g(x) \, dx = \int \frac{h(x)^2 f(x)^2}{g(x)} \, dx \\[4pt]
(\mathbb{E}_g[hw])^2 &amp;= I^2\end{split}\]</div>
<p>Therefore:</p>
<div class="math notranslate nohighlight">
\[\text{Var}_g[h(X)w(X)] = \int \frac{[h(x)f(x)]^2}{g(x)} \, dx - I^2\]</div>
<p>Dividing by <span class="math notranslate nohighlight">\(n\)</span> yields the estimator variance in Equation <a class="reference internal" href="#equation-is-variance">(2.16)</a>.</p>
<p><strong>Critical Insight</strong>: Variance depends on how well <span class="math notranslate nohighlight">\(g(x)\)</span> matches the shape of <span class="math notranslate nohighlight">\(|h(x)|f(x)\)</span>. When <span class="math notranslate nohighlight">\(g(x)\)</span> is small where <span class="math notranslate nohighlight">\(|h(x)|f(x)\)</span> is large, the ratio <span class="math notranslate nohighlight">\([h(x)f(x)]^2/g(x)\)</span> explodes, inflating variance. The optimal strategy is to make <span class="math notranslate nohighlight">\(g(x) \propto |h(x)|f(x)\)</span>.</p>
</section>
<section id="the-zero-variance-proposition">
<h3>The Zero-Variance Proposition<a class="headerlink" href="#the-zero-variance-proposition" title="Link to this heading">ÔÉÅ</a></h3>
<p>A remarkable result characterizes the optimal proposal:</p>
<div class="important admonition">
<p class="admonition-title">Proposition (Zero-Variance IS for Nonnegative h)</p>
<p>For a non-negative integrand <span class="math notranslate nohighlight">\(h(x) \geq 0\)</span>, the proposal</p>
<div class="math notranslate nohighlight">
\[g^*(x) = \frac{h(x)f(x)}{\int h(t)f(t)\,dt} = \frac{h(x)f(x)}{I}\]</div>
<p>achieves <strong>zero variance</strong>: <span class="math notranslate nohighlight">\(\text{Var}_{g^*}[\hat{I}_{\text{IS}}] = 0\)</span>.</p>
</div>
<p><strong>Proof</strong> (4 lines): Under <span class="math notranslate nohighlight">\(g^*\)</span>, the importance weight times integrand is constant:</p>
<div class="math notranslate nohighlight">
\[h(x) w(x) = h(x) \cdot \frac{f(x)}{g^*(x)} = h(x) \cdot \frac{f(x)}{h(x)f(x)/I} = I\]</div>
<p>Therefore <span class="math notranslate nohighlight">\(h(X)w(X) = I\)</span> almost surely under <span class="math notranslate nohighlight">\(g^*\)</span>, so <span class="math notranslate nohighlight">\(\text{Var}_{g^*}[h(X)w(X)] = 0\)</span>. <span class="math notranslate nohighlight">\(\blacksquare\)</span></p>
<p><strong>For general (possibly negative)</strong> <span class="math notranslate nohighlight">\(h(x)\)</span>, the variance-minimizing proposal is <span class="math notranslate nohighlight">\(g^*(x) \propto |h(x)|f(x)\)</span>, achieving minimum variance <span class="math notranslate nohighlight">\(\sigma^2_{g^*} = \left(\int |h(x)|f(x)\,dx\right)^2 - I^2 \geq 0\)</span>.</p>
<div class="note admonition">
<p class="admonition-title">Example üí° Zero-Variance Demonstration on Finite Grid</p>
<p><strong>Setup</strong>: Let <span class="math notranslate nohighlight">\(X \in \{1, 2, 3, 4\}\)</span> with target <span class="math notranslate nohighlight">\(f(x) = (0.4, 0.3, 0.2, 0.1)\)</span> and integrand <span class="math notranslate nohighlight">\(h(x) = x^2\)</span>. We compute <span class="math notranslate nohighlight">\(I = \mathbb{E}_f[X^2]\)</span> exactly, then show IS with <span class="math notranslate nohighlight">\(g^*\)</span> achieves empirical variance zero.</p>
<p><strong>Calculation</strong>:</p>
<div class="math notranslate nohighlight">
\[I = 0.4(1) + 0.3(4) + 0.2(9) + 0.1(16) = 0.4 + 1.2 + 1.8 + 1.6 = 5.0\]</div>
<p><strong>Optimal proposal</strong>: <span class="math notranslate nohighlight">\(g^*(x) = h(x)f(x)/I\)</span>:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}g^*(1) = \frac{1 \cdot 0.4}{5} = 0.08, \quad g^*(2) = \frac{4 \cdot 0.3}{5} = 0.24\\g^*(3) = \frac{9 \cdot 0.2}{5} = 0.36, \quad g^*(4) = \frac{16 \cdot 0.1}{5} = 0.32\end{aligned}\end{align} \]</div>
<p><strong>Verification</strong>: For any <span class="math notranslate nohighlight">\(x\)</span>, <span class="math notranslate nohighlight">\(h(x)w(x) = h(x) \cdot f(x)/g^*(x) = h(x) \cdot f(x) \cdot I/(h(x)f(x)) = I = 5\)</span>.</p>
<p><strong>Python Implementation</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Target f and integrand h on {1,2,3,4}</span>
<span class="n">X_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">])</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">X_vals</span><span class="o">**</span><span class="mi">2</span>  <span class="c1"># [1, 4, 9, 16]</span>

<span class="c1"># True integral</span>
<span class="n">I_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">h</span> <span class="o">*</span> <span class="n">f</span><span class="p">)</span>  <span class="c1"># 5.0</span>

<span class="c1"># Optimal proposal g* = h*f / I</span>
<span class="n">g_star</span> <span class="o">=</span> <span class="p">(</span><span class="n">h</span> <span class="o">*</span> <span class="n">f</span><span class="p">)</span> <span class="o">/</span> <span class="n">I_true</span>  <span class="c1"># [0.08, 0.24, 0.36, 0.32]</span>

<span class="c1"># Sample from g* and compute IS estimator</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">g_star</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X_vals</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

<span class="c1"># Compute weighted samples: h(X) * w(X) = h(X) * f(X) / g*(X)</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">/</span> <span class="n">g_star</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
<span class="n">weighted_h</span> <span class="o">=</span> <span class="n">h</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">*</span> <span class="n">weights</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True I = </span><span class="si">{</span><span class="n">I_true</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;IS estimate = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">weighted_h</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Empirical variance = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">weighted_h</span><span class="p">)</span><span class="si">:</span><span class="s2">.10f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;All weighted samples equal I? </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">weighted_h</span><span class="p">,</span><span class="w"> </span><span class="n">I_true</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Output</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>True I = 5.0
IS estimate = 5.000000
Empirical variance = 0.0000000000
All weighted samples equal I? True
</pre></div>
</div>
<p>Every weighted sample <span class="math notranslate nohighlight">\(h(X_i)w(X_i) = 5\)</span> regardless of which <span class="math notranslate nohighlight">\(X_i\)</span> is drawn‚Äîzero variance achieved!</p>
</div>
</section>
<section id="the-fundamental-paradox">
<h3>The Fundamental Paradox<a class="headerlink" href="#the-fundamental-paradox" title="Link to this heading">ÔÉÅ</a></h3>
<p>The optimal proposal <span class="math notranslate nohighlight">\(g^*(x) \propto |h(x)|f(x)\)</span> requires knowing <span class="math notranslate nohighlight">\(I = \int |h(x)|f(x)\,dx\)</span>‚Äîthe very quantity we seek to estimate! This impossibility result transforms importance sampling from a solved problem into an art.</p>
<figure class="align-center" id="id7">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_6_fig10_zero_variance_ideal.png"><img alt="Three-panel visualization showing components of optimal proposal, comparison with target, and the zero-variance paradox flowchart" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_6_fig10_zero_variance_ideal.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.40 </span><span class="caption-text"><strong>The Zero-Variance Paradox.</strong> (a) Components of the optimal proposal: target <span class="math notranslate nohighlight">\(f(x)\)</span>, function <span class="math notranslate nohighlight">\(h(x) = x^2\)</span>, and their product <span class="math notranslate nohighlight">\(|h|f\)</span>. (b) The optimal proposal <span class="math notranslate nohighlight">\(g^*(x) \propto |h(x)|f(x)\)</span> shifts mass toward regions where <span class="math notranslate nohighlight">\(|h|\)</span> is large compared to the target. (c) The fundamental paradox: constructing <span class="math notranslate nohighlight">\(g^*\)</span> requires knowing <span class="math notranslate nohighlight">\(I\)</span>, the very integral we want to estimate‚Äîresolved by using <span class="math notranslate nohighlight">\(g^*\)</span> as a design principle rather than an exact solution.</span><a class="headerlink" href="#id7" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<p><strong>Resolution</strong>: Use <span class="math notranslate nohighlight">\(g^* \propto |h|f\)</span> as a <em>design target</em>; approximate via:</p>
<ul class="simple">
<li><p><strong>Exponential tilting</strong>: Shift location toward the region where <span class="math notranslate nohighlight">\(|h|\)</span> is large (as in rare event estimation)</p></li>
<li><p><strong>Laplace approximation</strong>: Match a Gaussian to the mode and curvature of <span class="math notranslate nohighlight">\(|h|f\)</span></p></li>
<li><p><strong>Mixture surrogates</strong>: Combine components that cover different high-contribution regions</p></li>
<li><p><strong>Pilot runs</strong>: Use a preliminary sample to identify where <span class="math notranslate nohighlight">\(|h(x)|f(x)\)</span> concentrates</p></li>
</ul>
<p>Even a rough approximation to <span class="math notranslate nohighlight">\(g^*\)</span> can yield substantial variance reduction.</p>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ‚ö†Ô∏è Sampling from the Target is Not Optimal</p>
<p>A widespread misconception holds that the best proposal is the target distribution <span class="math notranslate nohighlight">\(g = f\)</span>. This yields ordinary Monte Carlo with <span class="math notranslate nohighlight">\(w(x) \equiv 1\)</span>. But unless <span class="math notranslate nohighlight">\(h(x)\)</span> is constant, importance sampling with a proposal that ‚Äútracks‚Äù <span class="math notranslate nohighlight">\(h\)</span> outperforms the target. If <span class="math notranslate nohighlight">\(h\)</span> varies widely‚Äîespecially if it concentrates in regions with small <span class="math notranslate nohighlight">\(f\)</span> probability‚Äîa biased proposal can dramatically reduce variance.</p>
</div>
</section>
<section id="tail-mismatch-and-infinite-variance">
<h3>Tail Mismatch and Infinite Variance<a class="headerlink" href="#tail-mismatch-and-infinite-variance" title="Link to this heading">ÔÉÅ</a></h3>
<div class="danger admonition">
<p class="admonition-title">Critical Warning üõë Lighter-Tailed Proposals Cause Infinite Variance</p>
<p>If the proposal <span class="math notranslate nohighlight">\(g\)</span> has <strong>lighter tails</strong> than the integrand <span class="math notranslate nohighlight">\(|h|f\)</span>, importance sampling variance is <strong>infinite</strong>, even if the estimator is technically unbiased.</p>
<p><strong>Mathematical condition</strong>: IS variance is finite if and only if</p>
<div class="math notranslate nohighlight">
\[\int \frac{[h(x)f(x)]^2}{g(x)} \, dx &lt; \infty\]</div>
<p>When <span class="math notranslate nohighlight">\(g(x)\)</span> decays faster than <span class="math notranslate nohighlight">\(|h(x)|f(x)\)</span> as <span class="math notranslate nohighlight">\(|x| \to \infty\)</span>, this integral diverges.</p>
<p><strong>Example</strong>: Target <span class="math notranslate nohighlight">\(f = \text{Cauchy}(0,1)\)</span> with density <span class="math notranslate nohighlight">\(f(x) = 1/(\pi(1+x^2))\)</span>, proposal <span class="math notranslate nohighlight">\(g = \mathcal{N}(0, 1)\)</span>, integrand <span class="math notranslate nohighlight">\(h(x) = x^2\)</span>. The integrand ratio grows as:</p>
<div class="math notranslate nohighlight">
\[\frac{[h(x)f(x)]^2}{g(x)} \asymp \frac{x^4}{(1+x^2)^2} \cdot e^{x^2/2} \to \infty \quad \text{as } |x| \to \infty\]</div>
<p>The Gaussian tails decay as <span class="math notranslate nohighlight">\(e^{-x^2/2}\)</span> while Cauchy tails decay only as <span class="math notranslate nohighlight">\(|x|^{-2}\)</span>, causing the ratio to explode exponentially.</p>
<p><strong>Tail Order Check</strong>: Before running IS, verify that <span class="math notranslate nohighlight">\(g\)</span> decays no faster than <span class="math notranslate nohighlight">\(|h|f\)</span>. Safe choices:</p>
<ol class="arabic simple">
<li><p>Use a proposal from the same family as <span class="math notranslate nohighlight">\(f\)</span> with heavier tails (e.g., <span class="math notranslate nohighlight">\(t\)</span>-distribution instead of Gaussian)</p></li>
<li><p>Use defensive mixtures: <span class="math notranslate nohighlight">\(g = \alpha g_1 + (1-\alpha)f\)</span> that inherit the target‚Äôs tail behavior</p></li>
</ol>
</div>
</section>
<section id="self-normalized-importance-sampling">
<h3>Self-Normalized Importance Sampling<a class="headerlink" href="#self-normalized-importance-sampling" title="Link to this heading">ÔÉÅ</a></h3>
<p>In Bayesian inference, the target density is often known only up to a normalizing constant: <span class="math notranslate nohighlight">\(f(x) = \tilde{f}(x)/Z\)</span> where <span class="math notranslate nohighlight">\(Z = \int \tilde{f}(x) \, dx\)</span> is unknown. <strong>Self-normalized importance sampling (SNIS)</strong> handles this elegantly.</p>
<p>Define unnormalized weights <span class="math notranslate nohighlight">\(\tilde{w}_i = \tilde{f}(X_i)/g(X_i)\)</span>. The SNIS estimator is:</p>
<div class="math notranslate nohighlight" id="equation-snis-estimator">
<span class="eqno">(2.17)<a class="headerlink" href="#equation-snis-estimator" title="Link to this equation">ÔÉÅ</a></span>\[\hat{I}_{\text{SNIS}} = \frac{\sum_{i=1}^n \tilde{w}_i h(X_i)}{\sum_{i=1}^n \tilde{w}_i} = \sum_{i=1}^n \bar{w}_i h(X_i)\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{w}_i = \tilde{w}_i / \sum_j \tilde{w}_j\)</span> are the normalized weights summing to 1.</p>
<p><strong>Properties</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Biased but consistent</strong>: <span class="math notranslate nohighlight">\(\hat{I}_{\text{SNIS}} \xrightarrow{p} I\)</span> as <span class="math notranslate nohighlight">\(n \to \infty\)</span>, but <span class="math notranslate nohighlight">\(\mathbb{E}[\hat{I}_{\text{SNIS}}] \neq I\)</span> for finite <span class="math notranslate nohighlight">\(n\)</span></p></li>
<li><p><strong>Bias is</strong> <span class="math notranslate nohighlight">\(O(1/n)\)</span>: The leading-order bias is <span class="math notranslate nohighlight">\(-\text{Cov}_g(h(X)w(X), w(X))/(\mathbb{E}_g[w(X)])^2 \cdot n^{-1}\)</span></p></li>
<li><p><strong>Often lower MSE</strong>: When weights are highly variable, SNIS can have lower mean squared error than standard IS despite its bias</p></li>
</ol>
<p><strong>Asymptotic Distribution (Delta-Method CLT)</strong>: Under regularity conditions:</p>
<div class="math notranslate nohighlight">
\[\sqrt{n}(\hat{I}_{\text{SNIS}} - I) \xrightarrow{d} \mathcal{N}(0, \sigma^2_{\text{SNIS}})\]</div>
<p>where the asymptotic variance is:</p>
<div class="math notranslate nohighlight" id="equation-snis-asymptotic-var">
<span class="eqno">(2.18)<a class="headerlink" href="#equation-snis-asymptotic-var" title="Link to this equation">ÔÉÅ</a></span>\[\sigma^2_{\text{SNIS}} = \frac{\text{Var}_g[(h(X) - I)w(X)]}{(\mathbb{E}_g[w(X)])^2}\]</div>
<p><strong>Comparison to Standard IS</strong>: When the target density <span class="math notranslate nohighlight">\(f\)</span> is fully normalized (so standard IS is feasible), the self-normalized estimator typically has <em>larger</em> asymptotic variance than standard IS due to the additional randomness from normalizing the weights. The value of SNIS is that it remains applicable when <span class="math notranslate nohighlight">\(f\)</span> is known only up to a multiplicative constant‚Äîthe typical situation in Bayesian inference where <span class="math notranslate nohighlight">\(f(\theta) \propto p(y|\theta)p(\theta)\)</span> with unknown normalizing constant <span class="math notranslate nohighlight">\(p(y)\)</span>.</p>
<p><strong>Practical SE Estimation for SNIS</strong>: Use the delta-method plug-in estimator:</p>
<div class="math notranslate nohighlight">
\[\widehat{\text{SE}}_{\text{SNIS}} = \sqrt{\frac{1}{n} \sum_{i=1}^n \bar{w}_i^2 (h(X_i) - \hat{I}_{\text{SNIS}})^2}\]</div>
</section>
<section id="effective-sample-size-definition-and-derivation">
<h3>Effective Sample Size: Definition and Derivation<a class="headerlink" href="#effective-sample-size-definition-and-derivation" title="Link to this heading">ÔÉÅ</a></h3>
<p>How many equivalent i.i.d. samples from the target does our weighted sample represent? The <strong>Effective Sample Size (ESS)</strong> answers this:</p>
<div class="note admonition">
<p class="admonition-title">Definition (Effective Sample Size)</p>
<p>For normalized weights <span class="math notranslate nohighlight">\(\bar{w}_1, \ldots, \bar{w}_n\)</span> with <span class="math notranslate nohighlight">\(\sum_i \bar{w}_i = 1\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-ess-formula">
<span class="eqno">(2.19)<a class="headerlink" href="#equation-ess-formula" title="Link to this equation">ÔÉÅ</a></span>\[\text{ESS} = \frac{1}{\sum_{i=1}^n \bar{w}_i^2}\]</div>
<p>Equivalently, in terms of unnormalized weights:</p>
<div class="math notranslate nohighlight">
\[\text{ESS} = \frac{\left(\sum_{i=1}^n w_i\right)^2}{\sum_{i=1}^n w_i^2}\]</div>
</div>
<p><strong>Properties</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\text{ESS} = n\)</span> when all weights are equal (<span class="math notranslate nohighlight">\(\bar{w}_i = 1/n\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{ESS} = 1\)</span> when one weight dominates (<span class="math notranslate nohighlight">\(\bar{w}_j = 1\)</span>, others <span class="math notranslate nohighlight">\(\approx 0\)</span>)</p></li>
<li><p>General bounds: <span class="math notranslate nohighlight">\(1 \leq \text{ESS} \leq n\)</span></p></li>
</ul>
<p><strong>Derivation of the ESS-CV Relationship</strong>:</p>
<p>The ESS can be expressed in terms of the coefficient of variation of the weights. Let <span class="math notranslate nohighlight">\(\bar{w} = \mathbb{E}[\bar{w}_i] = 1/n\)</span> (for normalized weights) and define the squared coefficient of variation:</p>
<div class="math notranslate nohighlight">
\[\text{CV}^2(w) = \frac{\text{Var}(w)}{\mathbb{E}[w]^2}\]</div>
<p>For unnormalized weights with <span class="math notranslate nohighlight">\(w_i = f(X_i)/g(X_i)\)</span> and <span class="math notranslate nohighlight">\(X_i \sim g\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}_g[w] = 1, \quad \text{Var}_g(w) = \mathbb{E}_g[w^2] - 1\]</div>
<p>The ESS relates to this as:</p>
<div class="math notranslate nohighlight">
\[\text{ESS} \approx \frac{n}{1 + \text{CV}^2(w)} = \frac{n}{1 + \text{Var}_g(w)/(\mathbb{E}_g[w])^2}\]</div>
<p><strong>Full Derivation</strong>: The sample-based ESS is <span class="math notranslate nohighlight">\((\sum w_i)^2 / \sum w_i^2\)</span>. Taking expectations under repeated sampling:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbb{E}\left[\sum w_i^2\right] &amp;= n\,\mathbb{E}_g[w^2] = n(1 + \text{Var}_g(w)) \\[4pt]
\mathbb{E}\left[\left(\sum w_i\right)^2\right] &amp;= n^2\,\mathbb{E}_g[w]^2 + n\,\text{Var}_g(w) = n^2 + n\,\text{Var}_g(w)\end{split}\]</div>
<p>For large <span class="math notranslate nohighlight">\(n\)</span>, the ratio approximates:</p>
<div class="math notranslate nohighlight">
\[\frac{\mathbb{E}[(\sum w_i)^2]}{\mathbb{E}[\sum w_i^2]} \approx \frac{n^2}{n(1 + \text{Var}_g(w))} = \frac{n}{1 + \text{Var}_g(w)} = \frac{n}{1 + \text{CV}^2(w)}\]</div>
<p>since <span class="math notranslate nohighlight">\(\mathbb{E}_g[w] = 1\)</span>.</p>
<p><strong>Interpretation</strong>: Kong (1992) showed that if <span class="math notranslate nohighlight">\(\text{ESS} = k\)</span> from <span class="math notranslate nohighlight">\(n\)</span> samples, estimate quality roughly equals <span class="math notranslate nohighlight">\(k\)</span> direct samples from the target. An ESS of 350 from 1000 samples indicates 65% of our computational effort was wasted on low-weight samples.</p>
<div class="warning admonition">
<p class="admonition-title">Warning ‚ö†Ô∏è Weight Degeneracy Diagnostic</p>
<p><strong>Red flags requiring proposal redesign:</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\text{ESS}/n &lt; 0.1\)</span> (fewer than 10% effective samples)</p></li>
<li><p><span class="math notranslate nohighlight">\(\max_i \bar{w}_i &gt; 0.5\)</span> (single sample dominates)</p></li>
</ul>
<p><strong>Actions when weight degeneracy is detected:</strong></p>
<ol class="arabic simple">
<li><p>Increase proposal variance (heavier tails)</p></li>
<li><p>Use mixture proposals: <span class="math notranslate nohighlight">\(g = \alpha g_1 + (1-\alpha)f\)</span></p></li>
<li><p>Consider adaptive methods (Sequential Monte Carlo)</p></li>
<li><p>Verify the tail order condition is satisfied</p></li>
</ol>
</div>
</section>
<section id="weight-degeneracy-in-high-dimensions">
<h3>Weight Degeneracy in High Dimensions<a class="headerlink" href="#weight-degeneracy-in-high-dimensions" title="Link to this heading">ÔÉÅ</a></h3>
<p>Importance sampling‚Äôs Achilles‚Äô heel is <strong>weight degeneracy</strong> in high dimensions. Bengtsson et al. (2008) proved a sobering result: in <span class="math notranslate nohighlight">\(d\)</span> dimensions, <span class="math notranslate nohighlight">\(\max_i \bar{w}_i \to 1\)</span> as <span class="math notranslate nohighlight">\(d \to \infty\)</span> unless sample size grows <strong>exponentially</strong> in dimension:</p>
<div class="math notranslate nohighlight">
\[n \sim \exp(Cd) \quad \text{for some constant } C &gt; 0\]</div>
<p>Even ‚Äúgood‚Äù proposals suffer this fate. When <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> are both <span class="math notranslate nohighlight">\(d\)</span>-dimensional Gaussians with different parameters, log-weights are approximately <span class="math notranslate nohighlight">\(\mathcal{N}(\mu_w, d\sigma^2_w)\)</span>‚Äîweight variance grows linearly with dimension, making normalized weights increasingly concentrated on a single sample.</p>
<p><strong>Remedies</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Sequential Monte Carlo (SMC)</strong>: Resample particles to prevent weight collapse</p></li>
<li><p><strong>Defensive mixtures</strong>: Use <span class="math notranslate nohighlight">\(g = \alpha g_1 + (1-\alpha)f\)</span> to bound weights</p></li>
<li><p><strong>Localization</strong>: Decompose high-dimensional problems into lower-dimensional pieces</p></li>
</ol>
<div class="note admonition">
<p class="admonition-title">Defensive Mixtures: Bounded Weights by Construction</p>
<p>Choose proposal <span class="math notranslate nohighlight">\(g = \alpha g_1 + (1-\alpha)f\)</span> where <span class="math notranslate nohighlight">\(g_1\)</span> targets the important region and <span class="math notranslate nohighlight">\(\alpha \in (0,1)\)</span> is small (e.g., 0.9). Then on the support of <span class="math notranslate nohighlight">\(f\)</span>:</p>
<div class="math notranslate nohighlight">
\[w(x) = \frac{f(x)}{g(x)} = \frac{f(x)}{\alpha g_1(x) + (1-\alpha)f(x)} \leq \frac{f(x)}{(1-\alpha)f(x)} = \frac{1}{1-\alpha}\]</div>
<p>With <span class="math notranslate nohighlight">\(\alpha = 0.9\)</span>, weights are bounded above by 10, preventing catastrophic degeneracy while still allowing <span class="math notranslate nohighlight">\(g_1\)</span> to concentrate samples in the important region. The cost is slightly higher variance than an ‚Äúoptimal‚Äù unbounded proposal, but the robustness is usually worth it.</p>
</div>
<figure class="align-center" id="id8">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_6_fig03_effective_sample_size.png"><img alt="Three-panel visualization showing weight distribution quality, ESS degradation with dimension, and weight degeneracy illustration" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_6_fig03_effective_sample_size.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.41 </span><span class="caption-text"><strong>Effective Sample Size and Weight Degeneracy.</strong> (a) Weight distributions from good (low variance) vs. poor (high variance) proposals‚Äîthe good proposal maintains many comparable weights while the poor proposal has a few dominant weights. (b) ESS as a percentage of <span class="math notranslate nohighlight">\(n\)</span> degrades with dimension for even ‚Äúreasonable‚Äù proposal choices, demonstrating the curse of dimensionality. (c) Extreme weight degeneracy: a single sample dominates, reducing ESS near 1 regardless of total sample size.</span><a class="headerlink" href="#id8" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
</section>
<section id="numerical-stability-via-log-weights">
<h3>Numerical Stability via Log-Weights<a class="headerlink" href="#numerical-stability-via-log-weights" title="Link to this heading">ÔÉÅ</a></h3>
<p>Importance weights involve density ratios that can overflow or underflow floating-point arithmetic. The solution is to work entirely in log-space.</p>
<p><strong>Log-weight computation</strong>:</p>
<div class="math notranslate nohighlight">
\[\log w(x) = \log f(x) - \log g(x)\]</div>
<p><strong>The logsumexp trick</strong>: To compute <span class="math notranslate nohighlight">\(\log\sum_i \exp(\ell_i)\)</span> where <span class="math notranslate nohighlight">\(\ell_i = \log \tilde{w}_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[\log\sum_i \exp(\ell_i) = a + \log\sum_i \exp(\ell_i - a), \quad a = \max_i \ell_i\]</div>
<p>This ensures all exponents are <span class="math notranslate nohighlight">\(\leq 0\)</span>, preventing overflow. Normalized weights follow:</p>
<div class="math notranslate nohighlight">
\[\bar{w}_i = \exp\left(\ell_i - \text{logsumexp}(\boldsymbol{\ell})\right)\]</div>
</section>
<section id="python-implementation">
<h3>Python Implementation<a class="headerlink" href="#python-implementation" title="Link to this heading">ÔÉÅ</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.special</span><span class="w"> </span><span class="kn">import</span> <span class="n">logsumexp</span>

<span class="k">def</span><span class="w"> </span><span class="nf">compute_ess</span><span class="p">(</span><span class="n">log_weights</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute Effective Sample Size from log-weights.</span>

<span class="sd">    Numerically stable computation using logsumexp.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    log_weights : array</span>
<span class="sd">        Log of importance weights (unnormalized)</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ess : float</span>
<span class="sd">        Effective sample size, in [1, n]</span>
<span class="sd">    ess_ratio : float</span>
<span class="sd">        ESS / n, in [0, 1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">log_weights</span><span class="p">)</span>
    <span class="c1"># Normalize in log-space</span>
    <span class="n">log_sum</span> <span class="o">=</span> <span class="n">logsumexp</span><span class="p">(</span><span class="n">log_weights</span><span class="p">)</span>
    <span class="n">log_normalized</span> <span class="o">=</span> <span class="n">log_weights</span> <span class="o">-</span> <span class="n">log_sum</span>

    <span class="c1"># ESS = 1 / sum(w_normalized^2) = exp(-logsumexp(2*log_normalized))</span>
    <span class="n">log_ess</span> <span class="o">=</span> <span class="o">-</span><span class="n">logsumexp</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">log_normalized</span><span class="p">)</span>
    <span class="n">ess</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_ess</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">ess</span><span class="p">,</span> <span class="n">ess</span> <span class="o">/</span> <span class="n">n</span>

<span class="k">def</span><span class="w"> </span><span class="nf">importance_sampling</span><span class="p">(</span><span class="n">h_func</span><span class="p">,</span> <span class="n">log_f</span><span class="p">,</span> <span class="n">log_g</span><span class="p">,</span> <span class="n">g_sampler</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span>
                        <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_diagnostics</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Importance sampling estimator for E_f[h(X)].</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    h_func : callable</span>
<span class="sd">        Function to integrate, h(X)</span>
<span class="sd">    log_f : callable</span>
<span class="sd">        Log of target density (possibly unnormalized)</span>
<span class="sd">    log_g : callable</span>
<span class="sd">        Log of proposal density</span>
<span class="sd">    g_sampler : callable</span>
<span class="sd">        Function g_sampler(rng, n) returning n samples from g</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of samples to draw</span>
<span class="sd">    normalize : bool</span>
<span class="sd">        If True, use self-normalized estimator (for unnormalized f)</span>
<span class="sd">    return_diagnostics : bool</span>
<span class="sd">        If True, return ESS and weight statistics</span>
<span class="sd">    seed : int, optional</span>
<span class="sd">        Random seed for reproducibility</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    estimate : float</span>
<span class="sd">        Importance sampling estimate of E_f[h(X)]</span>
<span class="sd">    diagnostics : dict (optional)</span>
<span class="sd">        ESS, max_weight, weight_degeneracy_flag if return_diagnostics=True</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Draw samples from proposal</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">g_sampler</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Compute log-weights for numerical stability</span>
    <span class="n">log_weights</span> <span class="o">=</span> <span class="n">log_f</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="n">log_g</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="c1"># Evaluate function at samples</span>
    <span class="n">h_vals</span> <span class="o">=</span> <span class="n">h_func</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="c1"># Normalize weights in log-space</span>
    <span class="n">log_sum_weights</span> <span class="o">=</span> <span class="n">logsumexp</span><span class="p">(</span><span class="n">log_weights</span><span class="p">)</span>
    <span class="n">log_normalized_weights</span> <span class="o">=</span> <span class="n">log_weights</span> <span class="o">-</span> <span class="n">log_sum_weights</span>
    <span class="n">normalized_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_normalized_weights</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
        <span class="c1"># Self-normalized importance sampling</span>
        <span class="n">estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">normalized_weights</span> <span class="o">*</span> <span class="n">h_vals</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Standard IS (requires normalized f)</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_weights</span><span class="p">)</span>
        <span class="n">estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">weights</span> <span class="o">*</span> <span class="n">h_vals</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">return_diagnostics</span><span class="p">:</span>
        <span class="n">ess</span><span class="p">,</span> <span class="n">ess_ratio</span> <span class="o">=</span> <span class="n">compute_ess</span><span class="p">(</span><span class="n">log_weights</span><span class="p">)</span>
        <span class="n">max_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">normalized_weights</span><span class="p">)</span>

        <span class="c1"># Compute variance estimate for SE</span>
        <span class="n">weighted_h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_weights</span><span class="p">)</span> <span class="o">*</span> <span class="n">h_vals</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">normalize</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="n">diagnostics</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;ess&#39;</span><span class="p">:</span> <span class="n">ess</span><span class="p">,</span>
            <span class="s1">&#39;ess_ratio&#39;</span><span class="p">:</span> <span class="n">ess_ratio</span><span class="p">,</span>
            <span class="s1">&#39;max_weight&#39;</span><span class="p">:</span> <span class="n">max_weight</span><span class="p">,</span>
            <span class="s1">&#39;weight_degeneracy&#39;</span><span class="p">:</span> <span class="n">ess_ratio</span> <span class="o">&lt;</span> <span class="mf">0.1</span> <span class="ow">or</span> <span class="n">max_weight</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">,</span>
            <span class="s1">&#39;log_weights&#39;</span><span class="p">:</span> <span class="n">log_weights</span><span class="p">,</span>
            <span class="s1">&#39;normalized_weights&#39;</span><span class="p">:</span> <span class="n">normalized_weights</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">estimate</span><span class="p">,</span> <span class="n">diagnostics</span>

    <span class="k">return</span> <span class="n">estimate</span>
</pre></div>
</div>
<div class="note admonition">
<p class="admonition-title">Example üí° Rare Event Estimation via Exponential Tilting</p>
<p><strong>Given:</strong> Estimate <span class="math notranslate nohighlight">\(p = P(Z &gt; 4)\)</span> for <span class="math notranslate nohighlight">\(Z \sim \mathcal{N}(0,1)\)</span>. The true value is <span class="math notranslate nohighlight">\(p = 1 - \Phi(4) \approx 3.167 \times 10^{-5}\)</span>.</p>
<p><strong>Challenge:</strong> With naive Monte Carlo, we need approximately <span class="math notranslate nohighlight">\(1/p \approx 31,600\)</span> samples to observe <em>one</em> exceedance on average. Reliable estimation requires <span class="math notranslate nohighlight">\(\gtrsim 10^7\)</span> samples.</p>
<p><strong>Importance Sampling Solution:</strong> Use exponential tilting with proposal <span class="math notranslate nohighlight">\(g_\theta(x) = \phi(x-\theta)\)</span>, a normal shifted by <span class="math notranslate nohighlight">\(\theta\)</span>. Setting <span class="math notranslate nohighlight">\(\theta = 4\)</span> (at the threshold) concentrates samples in the region of interest.</p>
<p><strong>Mathematical Setup:</strong></p>
<ul class="simple">
<li><p>Target: <span class="math notranslate nohighlight">\(f(x) = \phi(x)\)</span> (standard normal)</p></li>
<li><p>Proposal: <span class="math notranslate nohighlight">\(g(x) = \phi(x-4)\)</span> (normal with mean 4)</p></li>
<li><p>Importance weight: <span class="math notranslate nohighlight">\(w(x) = \phi(x)/\phi(x-4) = \exp(-4x + 8)\)</span></p></li>
<li><p>Integrand: <span class="math notranslate nohighlight">\(h(x) = \mathbf{1}_{x &gt; 4}\)</span></p></li>
</ul>
<p><strong>Python Implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.special</span><span class="w"> </span><span class="kn">import</span> <span class="n">logsumexp</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10_000</span>
<span class="n">threshold</span> <span class="o">=</span> <span class="mf">4.0</span>

<span class="c1"># True probability</span>
<span class="n">p_true</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">threshold</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True P(Z &gt; 4): </span><span class="si">{</span><span class="n">p_true</span><span class="si">:</span><span class="s2">.6e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Naive Monte Carlo</span>
<span class="n">Z_naive</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="n">p_naive</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Z_naive</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">)</span>
<span class="n">se_naive</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">p_true</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_true</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Naive MC (n=</span><span class="si">{</span><span class="n">n_samples</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Estimate: </span><span class="si">{</span><span class="n">p_naive</span><span class="si">:</span><span class="s2">.6e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  True SE: </span><span class="si">{</span><span class="n">se_naive</span><span class="si">:</span><span class="s2">.6e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Importance sampling with tilted normal</span>
<span class="n">X_is</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">threshold</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_samples</span><span class="p">)</span>  <span class="c1"># Sample from N(4,1)</span>
<span class="n">log_weights</span> <span class="o">=</span> <span class="o">-</span><span class="n">threshold</span> <span class="o">*</span> <span class="n">X_is</span> <span class="o">+</span> <span class="n">threshold</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mi">2</span>  <span class="c1"># log(f/g)</span>
<span class="n">indicators</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_is</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>

<span class="c1"># Self-normalized IS estimator</span>
<span class="n">log_sum</span> <span class="o">=</span> <span class="n">logsumexp</span><span class="p">(</span><span class="n">log_weights</span><span class="p">)</span>
<span class="n">normalized_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_weights</span> <span class="o">-</span> <span class="n">log_sum</span><span class="p">)</span>
<span class="n">p_is</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">normalized_weights</span> <span class="o">*</span> <span class="n">indicators</span><span class="p">)</span>

<span class="c1"># ESS computation</span>
<span class="n">ess</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">normalized_weights</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Standard IS for variance estimate</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_weights</span><span class="p">)</span>
<span class="n">weighted_indicators</span> <span class="o">=</span> <span class="n">indicators</span> <span class="o">*</span> <span class="n">weights</span>
<span class="n">var_is</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">weighted_indicators</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_samples</span>
<span class="n">se_is</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_is</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Importance Sampling (n=</span><span class="si">{</span><span class="n">n_samples</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Estimate: </span><span class="si">{</span><span class="n">p_is</span><span class="si">:</span><span class="s2">.6e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  SE: </span><span class="si">{</span><span class="n">se_is</span><span class="si">:</span><span class="s2">.6e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  ESS: </span><span class="si">{</span><span class="n">ess</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">ess</span><span class="o">/</span><span class="n">n_samples</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>

<span class="c1"># Variance reduction factor</span>
<span class="n">var_naive</span> <span class="o">=</span> <span class="n">p_true</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_true</span><span class="p">)</span>
<span class="n">vrf</span> <span class="o">=</span> <span class="n">var_naive</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">weighted_indicators</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Variance Reduction Factor: </span><span class="si">{</span><span class="n">vrf</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>True P(Z &gt; 4): 3.167124e-05

Naive MC (n=10,000):
  Estimate: 0.000000e+00
  True SE: 5.627e-05

Importance Sampling (n=10,000):
  Estimate: 3.184e-05
  SE: 2.56e-06
  ESS: 6234.8 (62.3%)

Variance Reduction Factor: 485x
</pre></div>
</div>
<p><strong>Result:</strong> Importance sampling estimates <span class="math notranslate nohighlight">\(\hat{p} \approx 3.18 \times 10^{-5}\)</span> (within 0.5% of truth) with a standard error of <span class="math notranslate nohighlight">\(2.6 \times 10^{-6}\)</span>. Naive MC with the same sample size observed zero exceedances. The variance reduction factor of approximately 500√ó means IS achieves the precision of 5 million naive samples with only 10,000.</p>
</div>
<figure class="align-center" id="id9">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_6_fig02_rare_event_estimation.png"><img alt="Three-panel visualization of rare event estimation showing exponential tilting, convergence comparison, and distribution of estimates" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_6_fig02_rare_event_estimation.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.42 </span><span class="caption-text"><strong>Rare Event Estimation via Exponential Tilting.</strong> (a) The standard normal target <span class="math notranslate nohighlight">\(f(x) = \phi(x)\)</span> has negligible mass beyond <span class="math notranslate nohighlight">\(x=4\)</span>, while the tilted proposal <span class="math notranslate nohighlight">\(g(x) = \phi(x-4)\)</span> concentrates samples in the rare event region. (b) Convergence comparison: naive MC produces mostly zeros (rare event never observed), while importance sampling converges steadily to the true value <span class="math notranslate nohighlight">\(P(Z&gt;4) \approx 3.17 \times 10^{-5}\)</span>. (c) Distribution of estimates from 500 replications‚ÄîIS achieves approximately 6700√ó variance reduction, enabling reliable estimation of events with probability <span class="math notranslate nohighlight">\(\sim 10^{-5}\)</span>.</span><a class="headerlink" href="#id9" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<div class="note admonition">
<p class="admonition-title">Example üí° Bayesian Posterior Mean via Importance Sampling</p>
<p><strong>Given:</strong> Observe <span class="math notranslate nohighlight">\(y = 2.5\)</span> from <span class="math notranslate nohighlight">\(Y | \theta \sim \mathcal{N}(\theta, 1)\)</span> with prior <span class="math notranslate nohighlight">\(\theta \sim \mathcal{N}(0, 1)\)</span>. Estimate the posterior mean <span class="math notranslate nohighlight">\(\mathbb{E}[\theta | Y = 2.5]\)</span>.</p>
<p><strong>Analytical Solution:</strong> The posterior is <span class="math notranslate nohighlight">\(\theta | Y \sim \mathcal{N}(y/2, 1/2)\)</span>, so <span class="math notranslate nohighlight">\(\mathbb{E}[\theta|Y=2.5] = 1.25\)</span>.</p>
<p><strong>IS Approach:</strong> Use the prior <span class="math notranslate nohighlight">\(g(\theta) = \phi(\theta)\)</span> as proposal and the unnormalized posterior <span class="math notranslate nohighlight">\(\tilde{f}(\theta) \propto \phi(y-\theta) \phi(\theta)\)</span> as target.</p>
<p><strong>Python Implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.special</span><span class="w"> </span><span class="kn">import</span> <span class="n">logsumexp</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">5_000</span>
<span class="n">y_obs</span> <span class="o">=</span> <span class="mf">2.5</span>

<span class="c1"># True posterior: N(y/2, 1/2) =&gt; mean = 1.25, sd = sqrt(0.5)</span>
<span class="n">post_mean_true</span> <span class="o">=</span> <span class="n">y_obs</span> <span class="o">/</span> <span class="mi">2</span>
<span class="n">post_sd_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True posterior mean: </span><span class="si">{</span><span class="n">post_mean_true</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Proposal: prior N(0, 1)</span>
<span class="n">theta_samples</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># Log-weights: log(likelihood) = log N(y|theta, 1)</span>
<span class="n">log_weights</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">y_obs</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">theta_samples</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Self-normalized IS for posterior mean</span>
<span class="n">log_sum</span> <span class="o">=</span> <span class="n">logsumexp</span><span class="p">(</span><span class="n">log_weights</span><span class="p">)</span>
<span class="n">normalized_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_weights</span> <span class="o">-</span> <span class="n">log_sum</span><span class="p">)</span>
<span class="n">post_mean_is</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">normalized_weights</span> <span class="o">*</span> <span class="n">theta_samples</span><span class="p">)</span>

<span class="c1"># ESS diagnostic</span>
<span class="n">ess</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">normalized_weights</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Importance Sampling Results:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Posterior mean estimate: </span><span class="si">{</span><span class="n">post_mean_is</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  ESS: </span><span class="si">{</span><span class="n">ess</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">ess</span><span class="o">/</span><span class="n">n</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Max normalized weight: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">normalized_weights</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Compare with better proposal: Laplace approximation</span>
<span class="c1"># Mode of posterior is also y/2 = 1.25</span>
<span class="n">lap_mean</span> <span class="o">=</span> <span class="n">y_obs</span> <span class="o">/</span> <span class="mi">2</span>
<span class="n">lap_sd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>  <span class="c1"># Curvature at mode</span>

<span class="n">theta_lap</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">lap_mean</span><span class="p">,</span> <span class="n">lap_sd</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">log_f_lap</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">y_obs</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">theta_lap</span><span class="p">)</span> <span class="o">+</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">theta_lap</span><span class="p">)</span>
<span class="n">log_g_lap</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">theta_lap</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">lap_mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">lap_sd</span><span class="p">)</span>
<span class="n">log_w_lap</span> <span class="o">=</span> <span class="n">log_f_lap</span> <span class="o">-</span> <span class="n">log_g_lap</span>

<span class="n">log_sum_lap</span> <span class="o">=</span> <span class="n">logsumexp</span><span class="p">(</span><span class="n">log_w_lap</span><span class="p">)</span>
<span class="n">w_lap</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_w_lap</span> <span class="o">-</span> <span class="n">log_sum_lap</span><span class="p">)</span>
<span class="n">post_mean_lap</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w_lap</span> <span class="o">*</span> <span class="n">theta_lap</span><span class="p">)</span>
<span class="n">ess_lap</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w_lap</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Laplace Approximation Proposal:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Posterior mean estimate: </span><span class="si">{</span><span class="n">post_mean_lap</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  ESS: </span><span class="si">{</span><span class="n">ess_lap</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">ess_lap</span><span class="o">/</span><span class="n">n</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Max normalized weight: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">w_lap</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>True posterior mean: 1.2500

Importance Sampling Results:
  Posterior mean estimate: 1.2538
  ESS: 1847.2 (36.9%)
  Max normalized weight: 0.0039

Laplace Approximation Proposal:
  Posterior mean estimate: 1.2496
  ESS: 4998.7 (100.0%)
  Max normalized weight: 0.0002
</pre></div>
</div>
<p><strong>Result:</strong> Using the prior as proposal yields ESS of only 37%‚Äîthe prior is too diffuse, wasting samples in low-posterior regions. The Laplace approximation (centered at the posterior mode with curvature-matched variance) achieves near-perfect ESS of 100%, as it closely matches the exact posterior. This demonstrates how proposal quality directly impacts computational efficiency.</p>
</div>
</section>
</section>
<section id="control-variates">
<h2>Control Variates<a class="headerlink" href="#control-variates" title="Link to this heading">ÔÉÅ</a></h2>
<p>Control variates reduce variance by exploiting correlation between the quantity of interest and auxiliary random variables with known expectations. The technique is mathematically equivalent to regression adjustment in experimental design: we ‚Äúpredict away‚Äù variance using a related variable.</p>
<section id="theory-and-optimal-coefficient">
<h3>Theory and Optimal Coefficient<a class="headerlink" href="#theory-and-optimal-coefficient" title="Link to this heading">ÔÉÅ</a></h3>
<p>Let <span class="math notranslate nohighlight">\(H = h(X)\)</span> denote the quantity we wish to estimate, with unknown mean <span class="math notranslate nohighlight">\(I = \mathbb{E}[H]\)</span>. Suppose we have a <strong>control variate</strong> <span class="math notranslate nohighlight">\(C = c(X)\)</span> correlated with <span class="math notranslate nohighlight">\(H\)</span> whose expectation <span class="math notranslate nohighlight">\(\mu_C = \mathbb{E}[C]\)</span> is known exactly.</p>
<p>The <strong>control variate estimator</strong> adjusts the naive estimate by the deviation of <span class="math notranslate nohighlight">\(C\)</span> from its mean:</p>
<div class="math notranslate nohighlight" id="equation-cv-estimator">
<span class="eqno">(2.20)<a class="headerlink" href="#equation-cv-estimator" title="Link to this equation">ÔÉÅ</a></span>\[\hat{I}_{\text{CV}} = \frac{1}{n} \sum_{i=1}^n \left[ h(X_i) - \beta(c(X_i) - \mu_C) \right]\]</div>
<p><strong>Unbiasedness</strong>: For any coefficient <span class="math notranslate nohighlight">\(\beta\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[\hat{I}_{\text{CV}}] = \mathbb{E}[H] - \beta(\mathbb{E}[C] - \mu_C) = I - \beta \cdot 0 = I\]</div>
<p>The adjustment does not bias the estimator because <span class="math notranslate nohighlight">\(\mathbb{E}[C - \mu_C] = 0\)</span>.</p>
<p><strong>Variance</strong>: The variance per sample is:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(H - \beta(C - \mu_C)) = \text{Var}(H) + \beta^2 \text{Var}(C) - 2\beta \text{Cov}(H, C)\]</div>
<p>This is a quadratic in <span class="math notranslate nohighlight">\(\beta\)</span> minimized by setting the derivative to zero:</p>
<div class="math notranslate nohighlight">
\[\frac{d}{d\beta}\left[\text{Var}(H) + \beta^2 \text{Var}(C) - 2\beta \text{Cov}(H, C)\right] = 2\beta \text{Var}(C) - 2\text{Cov}(H, C) = 0\]</div>
<p>Solving yields the <strong>optimal control variate coefficient</strong>:</p>
<div class="math notranslate nohighlight" id="equation-optimal-beta">
<span class="eqno">(2.21)<a class="headerlink" href="#equation-optimal-beta" title="Link to this equation">ÔÉÅ</a></span>\[\beta^* = \frac{\text{Cov}(H, C)}{\text{Var}(C)}\]</div>
<p>This is precisely the slope from the simple linear regression of <span class="math notranslate nohighlight">\(H\)</span> on <span class="math notranslate nohighlight">\(C\)</span>.</p>
</section>
<section id="variance-reduction-equals-squared-correlation">
<h3>Variance Reduction Equals Squared Correlation<a class="headerlink" href="#variance-reduction-equals-squared-correlation" title="Link to this heading">ÔÉÅ</a></h3>
<p>Substituting <span class="math notranslate nohighlight">\(\beta^*\)</span> into the variance formula:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{Var}(H - \beta^*(C - \mu_C)) &amp;= \text{Var}(H) + \frac{\text{Cov}(H,C)^2}{\text{Var}(C)} - 2 \cdot \frac{\text{Cov}(H,C)^2}{\text{Var}(C)} \\
&amp;= \text{Var}(H) - \frac{\text{Cov}(H,C)^2}{\text{Var}(C)} \\
&amp;= \text{Var}(H) \left(1 - \rho_{H,C}^2\right)\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\rho_{H,C} = \text{Cov}(H,C)/\sqrt{\text{Var}(H)\text{Var}(C)}\)</span> is the Pearson correlation.</p>
<p><strong>Key Result</strong>: The variance reduction factor is <span class="math notranslate nohighlight">\(\rho_{H,C}^2\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-cv-vrf">
<span class="eqno">(2.22)<a class="headerlink" href="#equation-cv-vrf" title="Link to this equation">ÔÉÅ</a></span>\[\text{VRF} = \frac{1}{1 - \rho^2_{H,C}}\]</div>
<ul class="simple">
<li><p>Perfect correlation (<span class="math notranslate nohighlight">\(|\rho| = 1\)</span>): infinite reduction (zero variance)</p></li>
<li><p><span class="math notranslate nohighlight">\(\rho = 0.9\)</span>: 81% variance reduction (VRF = 5.3)</p></li>
<li><p><span class="math notranslate nohighlight">\(\rho = 0.7\)</span>: 51% variance reduction (VRF = 2.0)</p></li>
<li><p><span class="math notranslate nohighlight">\(\rho = 0.5\)</span>: 25% variance reduction (VRF = 1.3)</p></li>
</ul>
</section>
<section id="multiple-control-variates">
<h3>Multiple Control Variates<a class="headerlink" href="#multiple-control-variates" title="Link to this heading">ÔÉÅ</a></h3>
<p>With <span class="math notranslate nohighlight">\(m\)</span> control variates <span class="math notranslate nohighlight">\(\mathbf{C} = (C_1, \ldots, C_m)^\top\)</span> having known means <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_C\)</span>, the estimator becomes:</p>
<div class="math notranslate nohighlight">
\[\hat{I}_{\text{CV}} = \bar{H} - \boldsymbol{\beta}^\top(\bar{\mathbf{C}} - \boldsymbol{\mu}_C)\]</div>
<p>The optimal coefficient vector is:</p>
<div class="math notranslate nohighlight" id="equation-optimal-beta-vector">
<span class="eqno">(2.23)<a class="headerlink" href="#equation-optimal-beta-vector" title="Link to this equation">ÔÉÅ</a></span>\[\boldsymbol{\beta}^* = \boldsymbol{\Sigma}_C^{-1} \text{Cov}(\mathbf{C}, H)\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_C = \text{Var}(\mathbf{C})\)</span> is the <span class="math notranslate nohighlight">\(m \times m\)</span> covariance matrix of controls. This equals the multiple regression coefficients from regressing <span class="math notranslate nohighlight">\(H\)</span> on <span class="math notranslate nohighlight">\(\mathbf{C}\)</span>.</p>
<p><strong>Derivation</strong> (matrix calculus): The variance of <span class="math notranslate nohighlight">\(H - \boldsymbol{\beta}^\top(\mathbf{C} - \boldsymbol{\mu}_C)\)</span> is</p>
<div class="math notranslate nohighlight">
\[\text{Var}(H) - 2\boldsymbol{\beta}^\top\text{Cov}(\mathbf{C}, H) + \boldsymbol{\beta}^\top \boldsymbol{\Sigma}_C \boldsymbol{\beta}\]</div>
<p>Setting the gradient with respect to <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> to zero:</p>
<div class="math notranslate nohighlight">
\[\nabla_{\boldsymbol{\beta}}: \quad -2\text{Cov}(\mathbf{C}, H) + 2\boldsymbol{\Sigma}_C\boldsymbol{\beta} = \mathbf{0} \quad \Rightarrow \quad \boldsymbol{\beta}^* = \boldsymbol{\Sigma}_C^{-1}\text{Cov}(\mathbf{C}, H)\]</div>
<p>The minimum variance is:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(H_{\text{CV}}^*) = \text{Var}(H) - \text{Cov}(H, \mathbf{C})^\top \boldsymbol{\Sigma}_C^{-1} \text{Cov}(H, \mathbf{C}) = \text{Var}(H)(1 - R^2)\]</div>
<p>where <span class="math notranslate nohighlight">\(R^2\)</span> is the coefficient of determination from the multiple regression.</p>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ‚ö†Ô∏è Collinearity in Multiple Controls</p>
<p>When controls <span class="math notranslate nohighlight">\(C_1, \ldots, C_m\)</span> are nearly collinear, <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_C\)</span> is ill-conditioned, causing:</p>
<ol class="arabic simple">
<li><p>Unstable <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> with high variance</p></li>
<li><p>Loss of numerical precision in <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_C^{-1}\)</span></p></li>
<li><p>Variance reduction that vanishes or even reverses</p></li>
</ol>
<p><strong>Remedy</strong>: Use ridge regression (<span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_C + \lambda I\)</span>) or select a subset of uncorrelated controls.</p>
</div>
</section>
<section id="finding-good-controls">
<h3>Finding Good Controls<a class="headerlink" href="#finding-good-controls" title="Link to this heading">ÔÉÅ</a></h3>
<p>Good control variates must satisfy two essential criteria:</p>
<ol class="arabic simple">
<li><p><strong>High correlation</strong> with <span class="math notranslate nohighlight">\(h(X)\)</span>: The stronger the correlation, the greater the variance reduction</p></li>
<li><p><strong>Exactly known expectation</strong>: We must know <span class="math notranslate nohighlight">\(\mathbb{E}[C]\)</span> precisely‚Äînot estimate it</p></li>
</ol>
<div class="warning admonition">
<p class="admonition-title">Critical Requirement ‚ö†Ô∏è The Control Mean Must Be Known Exactly</p>
<p>The control variate method requires <span class="math notranslate nohighlight">\(\mu_C = \mathbb{E}[C]\)</span> to be <strong>known analytically</strong>, not estimated from data.</p>
<p><strong>What happens if Œº_C is estimated?</strong> If we estimate <span class="math notranslate nohighlight">\(\hat{\mu}_C\)</span> from an independent sample of size <span class="math notranslate nohighlight">\(m\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\hat{I}_{\text{CV}}) = \text{Var}(H)(1 - \rho^2) + \beta^2 \frac{\text{Var}(C)}{m}\]</div>
<p>The second term adds variance, potentially overwhelming the reduction from the first term.</p>
<p><strong>With same-sample estimation</strong>: If <span class="math notranslate nohighlight">\(\hat{\mu}_C = \bar{C}\)</span> from the same samples, then <span class="math notranslate nohighlight">\(\hat{I}_{\text{CV}} = \bar{H} - \beta(\bar{C} - \bar{C}) = \bar{H}\)</span>‚Äîthe control has no effect! Including an intercept in the regression retains unbiasedness but with diminished variance benefits.</p>
</div>
<p><strong>Common sources of controls with known expectations:</strong></p>
<ul class="simple">
<li><p><strong>Moments of</strong> <span class="math notranslate nohighlight">\(X\)</span>: For <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(\mu, \sigma^2)\)</span>, use <span class="math notranslate nohighlight">\(C = X\)</span> (with <span class="math notranslate nohighlight">\(\mathbb{E}[X] = \mu\)</span>) or <span class="math notranslate nohighlight">\(C = (X-\mu)^2\)</span> (with <span class="math notranslate nohighlight">\(\mathbb{E}[(X-\mu)^2] = \sigma^2\)</span>)</p></li>
<li><p><strong>Proposal moments in IS</strong>: When using importance sampling with proposal <span class="math notranslate nohighlight">\(g\)</span>, moments of <span class="math notranslate nohighlight">\(g\)</span> are known</p></li>
<li><p><strong>Taylor approximations</strong>: If <span class="math notranslate nohighlight">\(h(x) \approx a + b(x-\mu) + c(x-\mu)^2\)</span> near the mean, use centered powers as controls</p></li>
<li><p><strong>Partial analytical solutions</strong>: When <span class="math notranslate nohighlight">\(h = h_1 + h_2\)</span> and <span class="math notranslate nohighlight">\(\mathbb{E}[h_1]\)</span> is known, use <span class="math notranslate nohighlight">\(h_1\)</span> as a control</p></li>
<li><p><strong>Approximate or auxiliary models</strong>: In option pricing, a geometric Asian option (with closed-form price) serves as control for arithmetic Asian options</p></li>
</ul>
</section>
<section id="id1">
<h3>Python Implementation<a class="headerlink" href="#id1" title="Link to this heading">ÔÉÅ</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">control_variate_estimator</span><span class="p">(</span><span class="n">h_vals</span><span class="p">,</span> <span class="n">c_vals</span><span class="p">,</span> <span class="n">mu_c</span><span class="p">,</span> <span class="n">estimate_beta</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Control variate estimator for E[H] using control C with known E[C] = mu_c.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    h_vals : array_like</span>
<span class="sd">        Sample values of h(X)</span>
<span class="sd">    c_vals : array_like</span>
<span class="sd">        Sample values of control variate c(X)</span>
<span class="sd">    mu_c : float</span>
<span class="sd">        Known expectation of control variate</span>
<span class="sd">    estimate_beta : bool</span>
<span class="sd">        If True, estimate optimal beta from samples</span>
<span class="sd">    beta : float, optional</span>
<span class="sd">        Fixed beta coefficient (used if estimate_beta=False)</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict with keys:</span>
<span class="sd">        &#39;estimate&#39;: control variate estimate of E[H]</span>
<span class="sd">        &#39;se&#39;: standard error of estimate</span>
<span class="sd">        &#39;beta&#39;: coefficient used</span>
<span class="sd">        &#39;rho&#39;: estimated correlation</span>
<span class="sd">        &#39;vrf&#39;: estimated variance reduction factor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">h_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">h_vals</span><span class="p">)</span>
    <span class="n">c_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">c_vals</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">h_vals</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">estimate_beta</span><span class="p">:</span>
        <span class="c1"># Estimate optimal beta via regression</span>
        <span class="n">cov_hc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">h_vals</span><span class="p">,</span> <span class="n">c_vals</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">var_c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">c_vals</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">cov_hc</span> <span class="o">/</span> <span class="n">var_c</span> <span class="k">if</span> <span class="n">var_c</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mf">0.0</span>

    <span class="c1"># Control variate adjusted values</span>
    <span class="n">adjusted</span> <span class="o">=</span> <span class="n">h_vals</span> <span class="o">-</span> <span class="n">beta</span> <span class="o">*</span> <span class="p">(</span><span class="n">c_vals</span> <span class="o">-</span> <span class="n">mu_c</span><span class="p">)</span>

    <span class="c1"># Estimate and standard error</span>
    <span class="n">estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">adjusted</span><span class="p">)</span>
    <span class="n">se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">adjusted</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

    <span class="c1"># Diagnostics</span>
    <span class="n">rho</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">h_vals</span><span class="p">,</span> <span class="n">c_vals</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">var_h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">h_vals</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">var_adj</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">adjusted</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">vrf</span> <span class="o">=</span> <span class="n">var_h</span> <span class="o">/</span> <span class="n">var_adj</span> <span class="k">if</span> <span class="n">var_adj</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;estimate&#39;</span><span class="p">:</span> <span class="n">estimate</span><span class="p">,</span>
        <span class="s1">&#39;se&#39;</span><span class="p">:</span> <span class="n">se</span><span class="p">,</span>
        <span class="s1">&#39;beta&#39;</span><span class="p">:</span> <span class="n">beta</span><span class="p">,</span>
        <span class="s1">&#39;rho&#39;</span><span class="p">:</span> <span class="n">rho</span><span class="p">,</span>
        <span class="s1">&#39;vrf&#39;</span><span class="p">:</span> <span class="n">vrf</span><span class="p">,</span>
        <span class="s1">&#39;var_reduction_pct&#39;</span><span class="p">:</span> <span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mi">1</span><span class="o">/</span><span class="n">vrf</span><span class="p">)</span> <span class="k">if</span> <span class="n">vrf</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="p">}</span>
</pre></div>
</div>
<div class="note admonition">
<p class="admonition-title">Example üí° Estimating <span class="math notranslate nohighlight">\(\mathbb{E}[e^X]\)</span> for <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(0,1)\)</span></p>
<p><strong>Given:</strong> <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(0,1)\)</span>, estimate <span class="math notranslate nohighlight">\(I = \mathbb{E}[e^X]\)</span>.</p>
<p><strong>Analytical Result:</strong> By the moment generating function, <span class="math notranslate nohighlight">\(\mathbb{E}[e^X] = e^{1/2} \approx 1.6487\)</span>.</p>
<p><strong>Control Variate:</strong> Use <span class="math notranslate nohighlight">\(C = X\)</span> with <span class="math notranslate nohighlight">\(\mu_C = \mathbb{E}[X] = 0\)</span>.</p>
<p><strong>Theoretical Analysis:</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\text{Cov}(e^X, X) = \mathbb{E}[X e^X] - \mathbb{E}[X]\mathbb{E}[e^X] = \mathbb{E}[X e^X]\)</span></p></li>
<li><p>Using the identity <span class="math notranslate nohighlight">\(\mathbb{E}[X e^X] = \mathbb{E}[e^X]\)</span> for <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(0,1)\)</span>: <span class="math notranslate nohighlight">\(\text{Cov}(e^X, X) = e^{1/2}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\text{Var}(X) = 1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\beta^* = e^{1/2}/1 = e^{1/2} \approx 1.6487\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\text{Var}(e^X) = e^2 - e \approx 4.671\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\rho = e^{1/2}/\sqrt{e^2 - e} \approx 0.763\)</span></p></li>
<li><p>Variance reduction: <span class="math notranslate nohighlight">\(\rho^2 \approx 0.582\)</span> (58.2%)</p></li>
</ul>
<p><strong>Python Implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10_000</span>

<span class="c1"># True value</span>
<span class="n">I_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True E[e^X]: </span><span class="si">{</span><span class="n">I_true</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Generate samples</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">h_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c1"># e^X</span>
<span class="n">c_vals</span> <span class="o">=</span> <span class="n">X</span>          <span class="c1"># Control: X</span>
<span class="n">mu_c</span> <span class="o">=</span> <span class="mf">0.0</span>          <span class="c1"># Known E[X] = 0</span>

<span class="c1"># Naive Monte Carlo</span>
<span class="n">naive_est</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_vals</span><span class="p">)</span>
<span class="n">naive_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">h_vals</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Naive MC: </span><span class="si">{</span><span class="n">naive_est</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (SE: </span><span class="si">{</span><span class="n">naive_se</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># Control variate estimator</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">control_variate_estimator</span><span class="p">(</span><span class="n">h_vals</span><span class="p">,</span> <span class="n">c_vals</span><span class="p">,</span> <span class="n">mu_c</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Control Variate:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Estimate: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;estimate&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (SE: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;se&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Beta: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theoretical: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Correlation: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;rho&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Variance Reduction: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;var_reduction_pct&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>True E[e^X]: 1.648721

Naive MC: 1.670089 (SE: 0.021628)

Control Variate:
  Estimate: 1.651876 (SE: 0.013992)
  Beta: 1.6542 (theoretical: 1.6487)
  Correlation: 0.7618
  Variance Reduction: 58.2%
</pre></div>
</div>
<p><strong>Result:</strong> The control variate estimator reduces variance by 58%, matching the theoretical prediction. The estimated <span class="math notranslate nohighlight">\(\beta = 1.654\)</span> closely approximates the theoretical optimum <span class="math notranslate nohighlight">\(\beta^* = e^{1/2} \approx 1.649\)</span>.</p>
</div>
<figure class="align-center" id="id10">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_6_fig04_control_variates.png"><img alt="Three-panel visualization of control variates showing scatter plot with regression, adjustment mechanism, and variance reduction histogram" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_6_fig04_control_variates.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.43 </span><span class="caption-text"><strong>Control Variates: Regression Adjustment for Monte Carlo.</strong> (a) Scatter plot of <span class="math notranslate nohighlight">\((C_i, H_i) = (X_i, e^{X_i})\)</span> showing strong positive correlation (<span class="math notranslate nohighlight">\(\rho \approx 0.76\)</span>). The regression line has slope <span class="math notranslate nohighlight">\(\beta^* \approx 1.65\)</span>. (b) Adjustment mechanism: original values (circles) are ‚Äúpulled‚Äù toward the regression line by subtracting <span class="math notranslate nohighlight">\(\beta^*(C_i - \mu_C)\)</span>. (c) Histogram comparison: the adjusted distribution (green) has 58% smaller variance than the original (gray), concentrating estimates around the true mean <span class="math notranslate nohighlight">\(\sqrt{e} \approx 1.649\)</span>.</span><a class="headerlink" href="#id10" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="antithetic-variates">
<h2>Antithetic Variates<a class="headerlink" href="#antithetic-variates" title="Link to this heading">ÔÉÅ</a></h2>
<p>Antithetic variates reduce variance by constructing <strong>negatively correlated pairs</strong> of samples that share the same marginal distribution. When averaged, systematic cancellation occurs: if one sample yields a high estimate, its antithetic partner tends to yield a low estimate, damping fluctuations.</p>
<div class="note admonition">
<p class="admonition-title">Definition (Antithetic Variates)</p>
<p>For a random variable <span class="math notranslate nohighlight">\(X\)</span> with distribution <span class="math notranslate nohighlight">\(F\)</span>, an <strong>antithetic pair</strong> <span class="math notranslate nohighlight">\((X, X')\)</span> satisfies:</p>
<ol class="arabic simple">
<li><p>Both <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(X'\)</span> have the same marginal distribution <span class="math notranslate nohighlight">\(F\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\text{Cov}(X, X') &lt; 0\)</span> (negative dependence)</p></li>
</ol>
<p>The antithetic estimator for <span class="math notranslate nohighlight">\(I = \mathbb{E}[h(X)]\)</span> averages paired values:</p>
<div class="math notranslate nohighlight">
\[\hat{I}_{\text{anti}} = \frac{1}{n/2} \sum_{i=1}^{n/2} \frac{h(X_i) + h(X_i')}{2}\]</div>
</div>
<section id="construction-of-antithetic-pairs">
<h3>Construction of Antithetic Pairs<a class="headerlink" href="#construction-of-antithetic-pairs" title="Link to this heading">ÔÉÅ</a></h3>
<p>The fundamental construction uses the uniform distribution. For <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0,1)\)</span>:</p>
<ul class="simple">
<li><p>Both <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(1-U\)</span> have <span class="math notranslate nohighlight">\(\text{Uniform}(0,1)\)</span> marginals</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{Cov}(U, 1-U) = \mathbb{E}[U(1-U)] - \frac{1}{4} = \frac{1}{6} - \frac{1}{4} = -\frac{1}{12}\)</span></p></li>
<li><p>Correlation: <span class="math notranslate nohighlight">\(\rho(U, 1-U) = -1\)</span> (perfect negative correlation)</p></li>
</ul>
<p>For continuous distributions with CDF <span class="math notranslate nohighlight">\(F\)</span>, the pair <span class="math notranslate nohighlight">\((F^{-1}(U), F^{-1}(1-U))\)</span> forms an antithetic pair with the target distribution. For the normal distribution, since <span class="math notranslate nohighlight">\(\Phi^{-1}(1-U) = -\Phi^{-1}(U)\)</span>, the antithetic pairs are simply <span class="math notranslate nohighlight">\((Z, -Z)\)</span> where <span class="math notranslate nohighlight">\(Z = \Phi^{-1}(U)\)</span>.</p>
</section>
<section id="variance-comparison-antithetic-vs-independent-samples">
<h3>Variance Comparison: Antithetic vs. Independent Samples<a class="headerlink" href="#variance-comparison-antithetic-vs-independent-samples" title="Link to this heading">ÔÉÅ</a></h3>
<p>The key question is: how does antithetic sampling compare to using two independent samples? This comparison justifies when to use the method.</p>
<div class="important admonition">
<p class="admonition-title">Proposition (Antithetic Variance Comparison)</p>
<p>Let <span class="math notranslate nohighlight">\(Y = h(X)\)</span> and <span class="math notranslate nohighlight">\(Y' = h(X')\)</span> for an antithetic pair, and let <span class="math notranslate nohighlight">\(Y_1, Y_2\)</span> be two independent samples of <span class="math notranslate nohighlight">\(h(X)\)</span>. Then:</p>
<div class="math notranslate nohighlight">
\[\text{Var}\left(\frac{Y + Y'}{2}\right) = \frac{\text{Var}(Y)}{2}\left(1 + \rho\right) \quad \text{vs.} \quad \text{Var}\left(\frac{Y_1 + Y_2}{2}\right) = \frac{\text{Var}(Y)}{2}\]</div>
<p>where <span class="math notranslate nohighlight">\(\rho = \text{Corr}(h(X), h(X'))\)</span>.</p>
</div>
<p><strong>Proof</strong> (3 lines):</p>
<p>For the antithetic pair, since <span class="math notranslate nohighlight">\(\text{Var}(Y) = \text{Var}(Y')\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{Var}\left(\frac{Y + Y'}{2}\right) = \frac{1}{4}\left[\text{Var}(Y) + \text{Var}(Y') + 2\text{Cov}(Y, Y')\right] = \frac{\text{Var}(Y)}{2} + \frac{\text{Cov}(Y, Y')}{2}\]</div>
<p>Writing <span class="math notranslate nohighlight">\(\text{Cov}(Y, Y') = \rho \cdot \text{Var}(Y)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{Var}\left(\frac{Y + Y'}{2}\right) = \frac{\text{Var}(Y)}{2}(1 + \rho) \quad \blacksquare\]</div>
<p>For independent samples, <span class="math notranslate nohighlight">\(\text{Cov}(Y_1, Y_2) = 0\)</span>, giving <span class="math notranslate nohighlight">\(\text{Var}((Y_1+Y_2)/2) = \text{Var}(Y)/2\)</span>.</p>
<p><strong>Variance Reduction Factor</strong>:</p>
<div class="math notranslate nohighlight">
\[\text{VRF} = \frac{\text{Var(independent)}}{\text{Var(antithetic)}} = \frac{1}{1 + \rho}\]</div>
<ul class="simple">
<li><p><strong>Reduction when</strong> <span class="math notranslate nohighlight">\(\rho &lt; 0\)</span>: VRF &gt; 1 (improvement)</p></li>
<li><p><strong>No effect when</strong> <span class="math notranslate nohighlight">\(\rho = 0\)</span>: VRF = 1</p></li>
<li><p><strong>Increase when</strong> <span class="math notranslate nohighlight">\(\rho &gt; 0\)</span>: VRF &lt; 1 (antithetic <em>hurts</em>)</p></li>
</ul>
</section>
<section id="the-monotone-function-theorem">
<h3>The Monotone Function Theorem<a class="headerlink" href="#the-monotone-function-theorem" title="Link to this heading">ÔÉÅ</a></h3>
<p><strong>Theorem</strong>: If <span class="math notranslate nohighlight">\(h\)</span> is monotonic (non-decreasing or non-increasing) and <span class="math notranslate nohighlight">\((X, X')\)</span> is an antithetic pair, then <span class="math notranslate nohighlight">\(\text{Cov}(h(X), h(X')) \leq 0\)</span>.</p>
<p><strong>Intuition</strong>: When <span class="math notranslate nohighlight">\(U\)</span> is large, <span class="math notranslate nohighlight">\(X = F^{-1}(U)\)</span> is in the upper tail, making <span class="math notranslate nohighlight">\(h(X)\)</span> large for increasing <span class="math notranslate nohighlight">\(h\)</span>. But <span class="math notranslate nohighlight">\(1-U\)</span> is small, so <span class="math notranslate nohighlight">\(X' = F^{-1}(1-U)\)</span> is in the lower tail, making <span class="math notranslate nohighlight">\(h(X')\)</span> small. This systematic opposition creates negative covariance.</p>
<p><strong>Formal Proof</strong>: Uses Hoeffding‚Äôs identity and the fact that <span class="math notranslate nohighlight">\((U, 1-U)\)</span> achieves the Fr√©chet‚ÄìHoeffding lower bound for joint distributions with uniform marginals (maximum negative dependence compatible with the marginals).</p>
</section>
<section id="counterexample-when-antithetic-variates-fail">
<h3>Counterexample: When Antithetic Variates Fail<a class="headerlink" href="#counterexample-when-antithetic-variates-fail" title="Link to this heading">ÔÉÅ</a></h3>
<div class="danger admonition">
<p class="admonition-title">Critical Warning üõë Non-Monotone Functions Can Double Variance</p>
<p>For non-monotone functions, antithetic variates can <strong>increase</strong> variance, sometimes dramatically.</p>
<p><strong>Counterexample 1</strong>: <span class="math notranslate nohighlight">\(h(u) = (u - 0.5)^2\)</span> on <span class="math notranslate nohighlight">\([0, 1]\)</span></p>
<ul class="simple">
<li><p>Antithetic: <span class="math notranslate nohighlight">\(h(1-u) = (0.5 - u)^2 = (u - 0.5)^2 = h(u)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\rho = +1\)</span> (perfect positive correlation!)</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{VRF} = 1/(1+1) = 0.5\)</span>‚Äîvariance <strong>doubles</strong></p></li>
</ul>
<p><strong>Counterexample 2</strong>: <span class="math notranslate nohighlight">\(h(u) = \sin(2\pi u)^2\)</span></p>
<ul class="simple">
<li><p>Antithetic: <span class="math notranslate nohighlight">\(h(1-u) = \sin(2\pi - 2\pi u)^2 = \sin(2\pi u)^2 = h(u)\)</span></p></li>
<li><p>Again <span class="math notranslate nohighlight">\(\rho = +1\)</span>, variance doubles</p></li>
</ul>
<p><strong>The underlying issue</strong>: Functions symmetric about <span class="math notranslate nohighlight">\(u = 0.5\)</span> satisfy <span class="math notranslate nohighlight">\(h(u) = h(1-u)\)</span>, making the antithetic pair perfectly positively correlated. Far from reducing variance, averaging identical values wastes half the samples.</p>
<p><strong>Diagnostic</strong>: Before applying antithetic variates, check if <span class="math notranslate nohighlight">\(h(1-u) \approx h(u)\)</span> for random <span class="math notranslate nohighlight">\(u\)</span>, or compute <span class="math notranslate nohighlight">\(\rho\)</span> from a pilot sample. If <span class="math notranslate nohighlight">\(\rho &gt; -0.1\)</span>, antithetics likely provide negligible benefit or cause harm.</p>
</div>
</section>
<section id="id2">
<h3>Python Implementation<a class="headerlink" href="#id2" title="Link to this heading">ÔÉÅ</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">antithetic_estimator</span><span class="p">(</span><span class="n">h_func</span><span class="p">,</span> <span class="n">F_inv</span><span class="p">,</span> <span class="n">n_pairs</span><span class="p">,</span> <span class="n">return_diagnostics</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Antithetic variate estimator for E[h(X)] where X ~ F.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    h_func : callable</span>
<span class="sd">        Function to integrate</span>
<span class="sd">    F_inv : callable</span>
<span class="sd">        Inverse CDF of target distribution</span>
<span class="sd">    n_pairs : int</span>
<span class="sd">        Number of antithetic pairs to generate</span>
<span class="sd">    return_diagnostics : bool</span>
<span class="sd">        If True, return correlation diagnostics</span>
<span class="sd">    seed : int, optional</span>
<span class="sd">        Random seed for reproducibility</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    estimate : float</span>
<span class="sd">        Antithetic estimate of E[h(X)]</span>
<span class="sd">    diagnostics : dict (optional)</span>
<span class="sd">        Including rho, variance_ratio</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Generate uniforms</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n_pairs</span><span class="p">)</span>

    <span class="c1"># Create antithetic pairs</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">F_inv</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>
    <span class="n">X_anti</span> <span class="o">=</span> <span class="n">F_inv</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">U</span><span class="p">)</span>

    <span class="c1"># Evaluate function</span>
    <span class="n">h_X</span> <span class="o">=</span> <span class="n">h_func</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">h_X_anti</span> <span class="o">=</span> <span class="n">h_func</span><span class="p">(</span><span class="n">X_anti</span><span class="p">)</span>

    <span class="c1"># Antithetic estimator: average of pair averages</span>
    <span class="n">pair_means</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_X</span> <span class="o">+</span> <span class="n">h_X_anti</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">pair_means</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">return_diagnostics</span><span class="p">:</span>
        <span class="c1"># Standard MC variance (using all 2n points as if independent)</span>
        <span class="n">all_h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">h_X</span><span class="p">,</span> <span class="n">h_X_anti</span><span class="p">])</span>
        <span class="n">var_standard</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">all_h</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Antithetic variance</span>
        <span class="n">var_antithetic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">pair_means</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Correlation between h(X) and h(X&#39;)</span>
        <span class="n">rho</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">h_X</span><span class="p">,</span> <span class="n">h_X_anti</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Variance reduction factor (comparing same total samples)</span>
        <span class="c1"># Standard: Var(h)/(2n) vs Antithetic: Var(pair_mean)/n</span>
        <span class="c1"># VRF = [Var(h)/(2n)] / [Var(pair_mean)/n] = Var(h) / (2*Var(pair_mean))</span>
        <span class="n">vrf</span> <span class="o">=</span> <span class="n">var_standard</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">var_antithetic</span><span class="p">)</span> <span class="k">if</span> <span class="n">var_antithetic</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>

        <span class="n">diagnostics</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;estimate&#39;</span><span class="p">:</span> <span class="n">estimate</span><span class="p">,</span>
            <span class="s1">&#39;se&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">pair_means</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_pairs</span><span class="p">),</span>
            <span class="s1">&#39;rho&#39;</span><span class="p">:</span> <span class="n">rho</span><span class="p">,</span>
            <span class="s1">&#39;theoretical_vrf&#39;</span><span class="p">:</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">rho</span><span class="p">)</span> <span class="k">if</span> <span class="n">rho</span> <span class="o">&gt;</span> <span class="o">-</span><span class="mi">1</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span>
            <span class="s1">&#39;empirical_vrf&#39;</span><span class="p">:</span> <span class="n">vrf</span><span class="p">,</span>
            <span class="s1">&#39;warning&#39;</span><span class="p">:</span> <span class="s1">&#39;Antithetic may hurt!&#39;</span> <span class="k">if</span> <span class="n">rho</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">estimate</span><span class="p">,</span> <span class="n">diagnostics</span>

    <span class="k">return</span> <span class="n">estimate</span>
</pre></div>
</div>
<div class="note admonition">
<p class="admonition-title">Example üí° The Exponential Integral with 97% Variance Reduction</p>
<p><strong>Given:</strong> Estimate <span class="math notranslate nohighlight">\(I = \int_0^1 e^x \, dx = e - 1 \approx 1.7183\)</span>.</p>
<p><strong>Analysis:</strong> The function <span class="math notranslate nohighlight">\(h(u) = e^u\)</span> is monotone increasing on <span class="math notranslate nohighlight">\([0,1]\)</span>, so antithetic variates should help.</p>
<p><strong>Theoretical Variance:</strong></p>
<ul class="simple">
<li><p>Standard MC: <span class="math notranslate nohighlight">\(\text{Var}(e^U) = \mathbb{E}[e^{2U}] - (\mathbb{E}[e^U])^2 = \frac{e^2 - 1}{2} - (e-1)^2 \approx 0.2420\)</span></p></li>
<li><p>Antithetic covariance: <span class="math notranslate nohighlight">\(\text{Cov}(e^U, e^{1-U}) = \mathbb{E}[e^U \cdot e^{1-U}] - (e-1)^2 = e - (e-1)^2 \approx -0.2342\)</span></p></li>
<li><p>Antithetic variance per pair: <span class="math notranslate nohighlight">\(\frac{1}{2}(0.2420) + \frac{1}{2}(-0.2342) = 0.0039\)</span></p></li>
</ul>
<p><strong>Variance Reduction:</strong> For equivalent cost (comparing <span class="math notranslate nohighlight">\(n\)</span> antithetic pairs to <span class="math notranslate nohighlight">\(2n\)</span> independent samples):</p>
<div class="math notranslate nohighlight">
\[\text{VRF} = \frac{0.2420 / 2}{0.0039} \approx 31\]</div>
<p>This represents approximately <strong>97% variance reduction</strong>.</p>
<p><strong>Python Implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_pairs</span> <span class="o">=</span> <span class="mi">10_000</span>

<span class="c1"># True value</span>
<span class="n">I_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True integral: </span><span class="si">{</span><span class="n">I_true</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Standard Monte Carlo (2n samples for fair comparison)</span>
<span class="n">U_standard</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_pairs</span><span class="p">)</span>
<span class="n">h_standard</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">U_standard</span><span class="p">)</span>
<span class="n">mc_estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_standard</span><span class="p">)</span>
<span class="n">mc_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">h_standard</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">mc_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mc_var</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_pairs</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Standard MC (</span><span class="si">{</span><span class="mi">2</span><span class="o">*</span><span class="n">n_pairs</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> samples):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Estimate: </span><span class="si">{</span><span class="n">mc_estimate</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  SE: </span><span class="si">{</span><span class="n">mc_se</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Antithetic variates (n pairs = 2n function evaluations)</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n_pairs</span><span class="p">)</span>
<span class="n">h_U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>
<span class="n">h_anti</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">U</span><span class="p">)</span>
<span class="n">pair_means</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_U</span> <span class="o">+</span> <span class="n">h_anti</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>

<span class="n">anti_estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">pair_means</span><span class="p">)</span>
<span class="n">anti_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">pair_means</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">anti_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">anti_var</span> <span class="o">/</span> <span class="n">n_pairs</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Antithetic (</span><span class="si">{</span><span class="n">n_pairs</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> pairs):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Estimate: </span><span class="si">{</span><span class="n">anti_estimate</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  SE: </span><span class="si">{</span><span class="n">anti_se</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Correlation diagnostic</span>
<span class="n">rho</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">h_U</span><span class="p">,</span> <span class="n">h_anti</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Correlation rho: </span><span class="si">{</span><span class="n">rho</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Variance reduction</span>
<span class="c1"># Fair comparison: both use 2n function evaluations</span>
<span class="c1"># Standard: Var/2n, Antithetic: Var(pair)/n</span>
<span class="n">vrf</span> <span class="o">=</span> <span class="p">(</span><span class="n">mc_var</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_pairs</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">anti_var</span> <span class="o">/</span> <span class="n">n_pairs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Variance Reduction Factor: </span><span class="si">{</span><span class="n">vrf</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Variance Reduction: </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="o">/</span><span class="n">vrf</span><span class="p">)</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>True integral: 1.718282

Standard MC (20,000 samples):
  Estimate: 1.721539
  SE: 0.003460

Antithetic (10,000 pairs):
  Estimate: 1.718298
  SE: 0.000639
  Correlation rho: -0.9678

Variance Reduction Factor: 29.3x
Variance Reduction: 96.6%
</pre></div>
</div>
<p><strong>Result:</strong> The strongly negative correlation (<span class="math notranslate nohighlight">\(\rho = -0.968\)</span>) yields a 29√ó variance reduction, achieving ~97% efficiency gain. The antithetic estimate is within 0.001% of the true value.</p>
</div>
<figure class="align-center" id="id11">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_6_fig05_antithetic_variates.png"><img alt="Three-panel visualization of antithetic variates showing pair construction, negative correlation, and variance reduction" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_6_fig05_antithetic_variates.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.44 </span><span class="caption-text"><strong>Antithetic Variates: Exploiting Negative Correlation.</strong> (a) Construction of antithetic pairs from uniform <span class="math notranslate nohighlight">\(U\)</span>: each <span class="math notranslate nohighlight">\(U_i\)</span> (circle) is paired with <span class="math notranslate nohighlight">\(1-U_i\)</span> (square), symmetric about 0.5. (b) For <span class="math notranslate nohighlight">\(h(u) = e^u\)</span>, the pairs exhibit strong negative correlation (<span class="math notranslate nohighlight">\(\rho \approx -0.97\)</span>)‚Äîwhen <span class="math notranslate nohighlight">\(h(U)\)</span> is large, <span class="math notranslate nohighlight">\(h(1-U)\)</span> is small, creating systematic cancellation. (c) Variance comparison: antithetic variates achieve 97% variance reduction (VRF ‚âà 30√ó), dramatically tightening the estimate distribution around the true value <span class="math notranslate nohighlight">\(e-1\)</span>.</span><a class="headerlink" href="#id11" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="stratified-sampling">
<h2>Stratified Sampling<a class="headerlink" href="#stratified-sampling" title="Link to this heading">ÔÉÅ</a></h2>
<p>Stratified sampling partitions the sample space into disjoint regions (strata) and draws samples from each region according to a carefully chosen allocation. By eliminating the randomness in how many samples fall in each region, stratification removes between-stratum variance‚Äîoften a dominant source of Monte Carlo error.</p>
<section id="the-stratified-estimator">
<h3>The Stratified Estimator<a class="headerlink" href="#the-stratified-estimator" title="Link to this heading">ÔÉÅ</a></h3>
<p>Partition the domain <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> into <span class="math notranslate nohighlight">\(K\)</span> disjoint, exhaustive strata <span class="math notranslate nohighlight">\(S_1, \ldots, S_K\)</span> with:</p>
<ul class="simple">
<li><p><strong>Stratum probabilities</strong>: <span class="math notranslate nohighlight">\(p_k = P(X \in S_k)\)</span> under the target distribution</p></li>
<li><p><strong>Within-stratum means</strong>: <span class="math notranslate nohighlight">\(\mu_k = \mathbb{E}[h(X) \mid X \in S_k]\)</span></p></li>
<li><p><strong>Within-stratum variances</strong>: <span class="math notranslate nohighlight">\(\sigma_k^2 = \text{Var}(h(X) \mid X \in S_k)\)</span></p></li>
</ul>
<p>The overall mean decomposes as:</p>
<div class="math notranslate nohighlight" id="equation-stratified-mean">
<span class="eqno">(2.24)<a class="headerlink" href="#equation-stratified-mean" title="Link to this equation">ÔÉÅ</a></span>\[I = \sum_{k=1}^K p_k \mu_k\]</div>
<p>The <strong>stratified estimator</strong> allocates <span class="math notranslate nohighlight">\(n_k\)</span> samples to stratum <span class="math notranslate nohighlight">\(k\)</span> (with <span class="math notranslate nohighlight">\(\sum_k n_k = n\)</span>), draws <span class="math notranslate nohighlight">\(X_{k,1}, \ldots, X_{k,n_k}\)</span> from the conditional distribution in stratum <span class="math notranslate nohighlight">\(k\)</span>, and combines:</p>
<div class="math notranslate nohighlight" id="equation-stratified-estimator">
<span class="eqno">(2.25)<a class="headerlink" href="#equation-stratified-estimator" title="Link to this equation">ÔÉÅ</a></span>\[\hat{I}_{\text{strat}} = \sum_{k=1}^K p_k \hat{\mu}_k, \quad \hat{\mu}_k = \frac{1}{n_k} \sum_{i=1}^{n_k} h(X_{k,i})\]</div>
<p>The variance is:</p>
<div class="math notranslate nohighlight" id="equation-stratified-variance">
<span class="eqno">(2.26)<a class="headerlink" href="#equation-stratified-variance" title="Link to this equation">ÔÉÅ</a></span>\[\text{Var}(\hat{I}_{\text{strat}}) = \sum_{k=1}^K \frac{p_k^2 \sigma_k^2}{n_k}\]</div>
</section>
<section id="proportional-allocation-always-reduces-variance">
<h3>Proportional Allocation Always Reduces Variance<a class="headerlink" href="#proportional-allocation-always-reduces-variance" title="Link to this heading">ÔÉÅ</a></h3>
<p>Under <strong>proportional allocation</strong> <span class="math notranslate nohighlight">\(n_k = n p_k\)</span>, the variance simplifies to:</p>
<div class="math notranslate nohighlight" id="equation-proportional-variance">
<span class="eqno">(2.27)<a class="headerlink" href="#equation-proportional-variance" title="Link to this equation">ÔÉÅ</a></span>\[\text{Var}(\hat{I}_{\text{prop}}) = \frac{1}{n} \sum_{k=1}^K p_k \sigma_k^2\]</div>
<p>The law of total variance (ANOVA decomposition) reveals why this always helps:</p>
<div class="math notranslate nohighlight">
\[\sigma^2 = \text{Var}(h(X)) = \underbrace{\mathbb{E}[\text{Var}(h \mid S)]}_{\text{within}} + \underbrace{\text{Var}(\mathbb{E}[h \mid S])}_{\text{between}} = \sum_k p_k \sigma_k^2 + \sum_k p_k (\mu_k - I)^2\]</div>
<p>The first term is <strong>within-stratum variance</strong>; the second is <strong>between-stratum variance</strong>. Since:</p>
<div class="math notranslate nohighlight">
\[\sum_k p_k \sigma_k^2 = \sigma^2 - \sum_k p_k (\mu_k - I)^2 \leq \sigma^2\]</div>
<p>we have <span class="math notranslate nohighlight">\(\text{Var}(\hat{I}_{\text{prop}}) \leq \text{Var}(\hat{I}_{\text{MC}})\)</span> with equality only when all stratum means are identical.</p>
<p><strong>Key Insight</strong>: Stratified sampling with proportional allocation <strong>eliminates the between-stratum variance component entirely</strong>. Variance reduction equals the proportion of total variance explained by stratum membership.</p>
<figure class="align-center" id="id12">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_6_fig06_stratified_sampling.png"><img alt="Three-panel visualization of stratified sampling showing heterogeneous integrand, sample placement, and ANOVA decomposition" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_6_fig06_stratified_sampling.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.45 </span><span class="caption-text"><strong>Stratified Sampling and Variance Decomposition.</strong> (a) A heterogeneous integrand with high variance in stratum <span class="math notranslate nohighlight">\(S_1\)</span> (exponential growth) and low variance in <span class="math notranslate nohighlight">\(S_2\)</span> (constant region). (b) Sample placement comparison: random sampling places variable numbers in each region by chance, while stratified sampling guarantees proportional coverage. (c) ANOVA decomposition: the total variance splits into within-stratum (67%) and between-stratum (34%) components‚Äîstratification completely eliminates the between-stratum variance, yielding substantial variance reduction.</span><a class="headerlink" href="#id12" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
</section>
<section id="neyman-allocation-minimizes-variance">
<h3>Neyman Allocation Minimizes Variance<a class="headerlink" href="#neyman-allocation-minimizes-variance" title="Link to this heading">ÔÉÅ</a></h3>
<p><strong>Theorem (Neyman, 1934)</strong>: The allocation minimizing <span class="math notranslate nohighlight">\(\text{Var}(\hat{I}_{\text{strat}})\)</span> subject to <span class="math notranslate nohighlight">\(\sum_k n_k = n\)</span> is:</p>
<div class="math notranslate nohighlight" id="equation-neyman-allocation">
<span class="eqno">(2.28)<a class="headerlink" href="#equation-neyman-allocation" title="Link to this equation">ÔÉÅ</a></span>\[n_k^* = n \cdot \frac{p_k \sigma_k}{\sum_j p_j \sigma_j}\]</div>
<div class="note admonition">
<p class="admonition-title">Proof (Lagrange Multipliers)</p>
<p><strong>Objective</strong>: Minimize <span class="math notranslate nohighlight">\(V(n_1, \ldots, n_K) = \sum_{k=1}^K \frac{p_k^2 \sigma_k^2}{n_k}\)</span> subject to <span class="math notranslate nohighlight">\(\sum_{k=1}^K n_k = n\)</span>.</p>
<p><strong>Lagrangian</strong>: <span class="math notranslate nohighlight">\(\mathcal{L} = \sum_{k=1}^K \frac{p_k^2 \sigma_k^2}{n_k} + \lambda\left(\sum_{k=1}^K n_k - n\right)\)</span></p>
<p><strong>First-order conditions</strong>: For each <span class="math notranslate nohighlight">\(k\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial \mathcal{L}}{\partial n_k} = -\frac{p_k^2 \sigma_k^2}{n_k^2} + \lambda = 0 \quad \Rightarrow \quad n_k^2 = \frac{p_k^2 \sigma_k^2}{\lambda} \quad \Rightarrow \quad n_k = \frac{p_k \sigma_k}{\sqrt{\lambda}}\]</div>
<p><strong>Constraint</strong>: Summing over <span class="math notranslate nohighlight">\(k\)</span>:</p>
<div class="math notranslate nohighlight">
\[\sum_k n_k = \frac{1}{\sqrt{\lambda}} \sum_k p_k \sigma_k = n \quad \Rightarrow \quad \sqrt{\lambda} = \frac{\sum_k p_k \sigma_k}{n}\]</div>
<p><strong>Solution</strong>: Substituting back:</p>
<div class="math notranslate nohighlight">
\[n_k^* = \frac{p_k \sigma_k}{\sqrt{\lambda}} = \frac{p_k \sigma_k}{\sum_j p_j \sigma_j / n} = n \cdot \frac{p_k \sigma_k}{\sum_j p_j \sigma_j} \quad \blacksquare\]</div>
</div>
<p>The optimal variance is:</p>
<div class="math notranslate nohighlight" id="equation-neyman-variance">
<span class="eqno">(2.29)<a class="headerlink" href="#equation-neyman-variance" title="Link to this equation">ÔÉÅ</a></span>\[\text{Var}(\hat{I}_{\text{opt}}) = \frac{1}{n} \left(\sum_k p_k \sigma_k\right)^2\]</div>
<p><strong>Interpretation</strong>: Neyman allocation samples more heavily from:</p>
<ol class="arabic simple">
<li><p><strong>Larger strata</strong> (large <span class="math notranslate nohighlight">\(p_k\)</span>): They contribute more to the integral</p></li>
<li><p><strong>More variable strata</strong> (large <span class="math notranslate nohighlight">\(\sigma_k\)</span>): They need more samples for precise estimation</p></li>
</ol>
<p><strong>Special case</strong>: If <span class="math notranslate nohighlight">\(\sigma_j = 0\)</span> for some stratum (constant function), allocate <span class="math notranslate nohighlight">\(n_j = 0\)</span> and use the known stratum mean directly.</p>
</section>
<section id="latin-hypercube-sampling-for-high-dimensions">
<h3>Latin Hypercube Sampling for High Dimensions<a class="headerlink" href="#latin-hypercube-sampling-for-high-dimensions" title="Link to this heading">ÔÉÅ</a></h3>
<p>Traditional stratification suffers from the curse of dimensionality: <span class="math notranslate nohighlight">\(m\)</span> strata per dimension requires <span class="math notranslate nohighlight">\(m^d\)</span> samples in <span class="math notranslate nohighlight">\(d\)</span> dimensions. <strong>Latin Hypercube Sampling (LHS)</strong>, introduced by McKay, Beckman, and Conover (1979), provides a practical alternative.</p>
<div class="note admonition">
<p class="admonition-title">Definition (Latin Hypercube Sample)</p>
<p>A <strong>Latin Hypercube Sample</strong> of size <span class="math notranslate nohighlight">\(n\)</span> in <span class="math notranslate nohighlight">\([0,1]^d\)</span> is a set of <span class="math notranslate nohighlight">\(n\)</span> points such that, when projected onto any coordinate axis, exactly one point falls in each of the <span class="math notranslate nohighlight">\(n\)</span> equal intervals <span class="math notranslate nohighlight">\([(i-1)/n, i/n)\)</span> for <span class="math notranslate nohighlight">\(i = 1, \ldots, n\)</span>.</p>
</div>
<p><strong>Algorithm (LHS Construction)</strong>:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input: n (sample size), d (dimension)
Output: X (n √ó d matrix of LHS points)

1. For j = 1, ..., d:
   a. Create permutation œÄ_j = random_permutation(1, 2, ..., n)
   b. For i = 1, ..., n:
      X[i,j] = (œÄ_j[i] - 1 + U[i,j]) / n
      where U[i,j] ~ Uniform(0,1)
2. Return X
</pre></div>
</div>
<p><strong>Key Property</strong>: Each univariate margin is perfectly stratified. If the function is approximately additive, <span class="math notranslate nohighlight">\(h(\mathbf{x}) \approx \sum_j h_j(x_j)\)</span>, LHS achieves large variance reduction by controlling each main effect.</p>
<p><strong>Stein (1987)</strong> established that the asymptotic variance of LHS depends on departures from additivity. Specifically, for smooth <span class="math notranslate nohighlight">\(h\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\hat{\mu}_{\text{LHS}}) = \frac{1}{n} \int [h(\mathbf{x}) - h^{\text{add}}(\mathbf{x})]^2 \, d\mathbf{x} + o(1/n)\]</div>
<p>where <span class="math notranslate nohighlight">\(h^{\text{add}}(\mathbf{x}) = \mu + \sum_j [h_j(x_j) - \mu]\)</span> is the best additive approximation (the sum of main effects). LHS completely removes variance from main effects‚Äîonly interaction terms contribute to the leading-order variance.</p>
<p><strong>Owen (1997)</strong> proved that under broad regularity conditions, randomized LHS is never worse than simple random sampling up to <span class="math notranslate nohighlight">\(O(1/n)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\hat{\mu}_{\text{LHS}}) \leq \text{Var}(\hat{\mu}_{\text{SRS}}) + O(1/n^2)\]</div>
<p>More precisely, if <span class="math notranslate nohighlight">\(h\)</span> has finite variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>, then <span class="math notranslate nohighlight">\(\text{Var}(\hat{\mu}_{\text{LHS}}) \leq \sigma^2/(n-1)\)</span> for randomized LHS. This guarantee ensures LHS is a safe default when the function structure is unknown‚Äîyou cannot lose much compared to simple random sampling, and may gain substantially.</p>
<figure class="align-center" id="id13">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_6_fig07_latin_hypercube.png"><img alt="Three-panel visualization comparing simple random sampling and Latin hypercube sampling in 2D" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_6_fig07_latin_hypercube.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.46 </span><span class="caption-text"><strong>Latin Hypercube Sampling vs. Simple Random.</strong> (a) Simple random sampling in 2D: some grid cells are empty while others contain multiple points, leaving portions of the domain poorly explored. (b) Latin Hypercube Sampling: exactly one sample per row and column interval guarantees even coverage across each marginal dimension. (c) Marginal distribution comparison: LHS produces near-uniform marginals (matching the target) while random sampling shows substantial deviations from uniformity.</span><a class="headerlink" href="#id13" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
</section>
<section id="id3">
<h3>Python Implementation<a class="headerlink" href="#id3" title="Link to this heading">ÔÉÅ</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">stratified_estimator</span><span class="p">(</span><span class="n">h_func</span><span class="p">,</span> <span class="n">sampler_by_stratum</span><span class="p">,</span> <span class="n">stratum_probs</span><span class="p">,</span> <span class="n">n_per_stratum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Stratified sampling estimator.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    h_func : callable</span>
<span class="sd">        Function to integrate</span>
<span class="sd">    sampler_by_stratum : list of callables</span>
<span class="sd">        sampler_by_stratum[k](rng, n) returns n samples from stratum k</span>
<span class="sd">    stratum_probs : array_like</span>
<span class="sd">        Probabilities p_k for each stratum</span>
<span class="sd">    n_per_stratum : array_like</span>
<span class="sd">        Number of samples n_k for each stratum</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict with estimate, se, within_var, between_var diagnostics</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">stratum_probs</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">stratum_probs</span><span class="p">)</span>
    <span class="n">n_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">n_per_stratum</span><span class="p">)</span>

    <span class="n">stratum_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
    <span class="n">stratum_vars</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>

    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
        <span class="c1"># Sample from stratum k</span>
        <span class="n">X_k</span> <span class="o">=</span> <span class="n">sampler_by_stratum</span><span class="p">[</span><span class="n">k</span><span class="p">](</span><span class="n">rng</span><span class="p">,</span> <span class="n">n_k</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
        <span class="n">h_k</span> <span class="o">=</span> <span class="n">h_func</span><span class="p">(</span><span class="n">X_k</span><span class="p">)</span>

        <span class="n">stratum_means</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_k</span><span class="p">)</span>
        <span class="n">stratum_vars</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">h_k</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Stratified estimate</span>
    <span class="n">estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span> <span class="o">*</span> <span class="n">stratum_means</span><span class="p">)</span>

    <span class="c1"># Variance of stratified estimator</span>
    <span class="n">var_strat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">stratum_vars</span> <span class="o">/</span> <span class="n">n_k</span><span class="p">)</span>
    <span class="n">se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_strat</span><span class="p">)</span>

    <span class="c1"># Decomposition</span>
    <span class="n">within_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span> <span class="o">*</span> <span class="n">stratum_vars</span><span class="p">)</span>
    <span class="n">between_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span> <span class="o">*</span> <span class="p">(</span><span class="n">stratum_means</span> <span class="o">-</span> <span class="n">estimate</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;estimate&#39;</span><span class="p">:</span> <span class="n">estimate</span><span class="p">,</span>
        <span class="s1">&#39;se&#39;</span><span class="p">:</span> <span class="n">se</span><span class="p">,</span>
        <span class="s1">&#39;stratum_means&#39;</span><span class="p">:</span> <span class="n">stratum_means</span><span class="p">,</span>
        <span class="s1">&#39;stratum_vars&#39;</span><span class="p">:</span> <span class="n">stratum_vars</span><span class="p">,</span>
        <span class="s1">&#39;within_var&#39;</span><span class="p">:</span> <span class="n">within_var</span><span class="p">,</span>
        <span class="s1">&#39;between_var&#39;</span><span class="p">:</span> <span class="n">between_var</span>
    <span class="p">}</span>


<span class="k">def</span><span class="w"> </span><span class="nf">latin_hypercube_sample</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate n Latin Hypercube samples in [0,1]^d.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n : int</span>
<span class="sd">        Number of samples</span>
<span class="sd">    d : int</span>
<span class="sd">        Dimension</span>
<span class="sd">    seed : int, optional</span>
<span class="sd">        Random seed</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    samples : ndarray of shape (n, d)</span>
<span class="sd">        Latin Hypercube samples</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
        <span class="c1"># Create n equally spaced intervals and shuffle</span>
        <span class="n">perm</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
        <span class="c1"># Uniform sample within each stratum</span>
        <span class="n">samples</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">perm</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">))</span> <span class="o">/</span> <span class="n">n</span>

    <span class="k">return</span> <span class="n">samples</span>
</pre></div>
</div>
<div class="note admonition">
<p class="admonition-title">Example üí° Stratified vs. Simple Monte Carlo for Heterogeneous Integrand</p>
<p><strong>Given:</strong> Estimate <span class="math notranslate nohighlight">\(I = \int_0^1 h(x) \, dx\)</span> where <span class="math notranslate nohighlight">\(h(x) = e^{10x}\)</span> for <span class="math notranslate nohighlight">\(x &lt; 0.2\)</span> and <span class="math notranslate nohighlight">\(h(x) = 1\)</span> otherwise. This integrand has high variance concentrated in <span class="math notranslate nohighlight">\([0, 0.2)\)</span>.</p>
<p><strong>Strategy:</strong> Stratify into <span class="math notranslate nohighlight">\(S_1 = [0, 0.2)\)</span> and <span class="math notranslate nohighlight">\(S_2 = [0.2, 1]\)</span> with <span class="math notranslate nohighlight">\(p_1 = 0.2\)</span>, <span class="math notranslate nohighlight">\(p_2 = 0.8\)</span>.</p>
<p><strong>Python Implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">integrate</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">h</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&lt;</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">10</span> <span class="o">*</span> <span class="n">x</span><span class="p">),</span> <span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># True value by numerical integration</span>
<span class="n">I_true</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">integrate</span><span class="o">.</span><span class="n">quad</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True integral: </span><span class="si">{</span><span class="n">I_true</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">n_total</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Simple Monte Carlo</span>
<span class="n">X_mc</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_total</span><span class="p">)</span>
<span class="n">h_mc</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">X_mc</span><span class="p">)</span>
<span class="n">mc_est</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_mc</span><span class="p">)</span>
<span class="n">mc_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">h_mc</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_total</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Simple MC: </span><span class="si">{</span><span class="n">mc_est</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (SE: </span><span class="si">{</span><span class="n">mc_se</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># Stratified sampling with proportional allocation</span>
<span class="n">p1</span><span class="p">,</span> <span class="n">p2</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span>
<span class="n">n1</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_total</span> <span class="o">*</span> <span class="n">p1</span><span class="p">)</span>  <span class="c1"># 200 samples in [0, 0.2)</span>
<span class="n">n2</span> <span class="o">=</span> <span class="n">n_total</span> <span class="o">-</span> <span class="n">n1</span>       <span class="c1"># 800 samples in [0.2, 1]</span>

<span class="c1"># Sample from each stratum</span>
<span class="n">X1</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">n1</span><span class="p">)</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n2</span><span class="p">)</span>

<span class="n">h1</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">X1</span><span class="p">)</span>
<span class="n">h2</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">X2</span><span class="p">)</span>

<span class="n">mu1_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h1</span><span class="p">)</span>
<span class="n">mu2_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h2</span><span class="p">)</span>

<span class="n">strat_est</span> <span class="o">=</span> <span class="n">p1</span> <span class="o">*</span> <span class="n">mu1_hat</span> <span class="o">+</span> <span class="n">p2</span> <span class="o">*</span> <span class="n">mu2_hat</span>

<span class="c1"># Stratified SE</span>
<span class="n">var1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">h1</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">var2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">h2</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">strat_var</span> <span class="o">=</span> <span class="p">(</span><span class="n">p1</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">var1</span> <span class="o">/</span> <span class="n">n1</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">p2</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">var2</span> <span class="o">/</span> <span class="n">n2</span><span class="p">)</span>
<span class="n">strat_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">strat_var</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Stratified:  </span><span class="si">{</span><span class="n">strat_est</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (SE: </span><span class="si">{</span><span class="n">strat_se</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># Variance decomposition</span>
<span class="n">total_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">h_mc</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">within_var</span> <span class="o">=</span> <span class="n">p1</span> <span class="o">*</span> <span class="n">var1</span> <span class="o">+</span> <span class="n">p2</span> <span class="o">*</span> <span class="n">var2</span>
<span class="n">between_var</span> <span class="o">=</span> <span class="n">total_var</span> <span class="o">-</span> <span class="n">within_var</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Variance Decomposition:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Total variance: </span><span class="si">{</span><span class="n">total_var</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Within-stratum: </span><span class="si">{</span><span class="n">within_var</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">within_var</span><span class="o">/</span><span class="n">total_var</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Between-stratum: </span><span class="si">{</span><span class="n">between_var</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">between_var</span><span class="o">/</span><span class="n">total_var</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Variance Reduction Factor: </span><span class="si">{</span><span class="p">(</span><span class="n">mc_se</span><span class="o">/</span><span class="n">strat_se</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>True integral: 2.256930

Simple MC: 2.4321 (SE: 0.1897)
Stratified:  2.2549 (SE: 0.0584)

Variance Decomposition:
  Total variance: 35.9784
  Within-stratum: 3.2481 (9.0%)
  Between-stratum: 32.7303 (91.0%)

Variance Reduction Factor: 10.6x
</pre></div>
</div>
<p><strong>Result:</strong> The between-stratum variance (91% of total) is eliminated by stratification, yielding a 10√ó variance reduction. The stratified estimate (2.255) is much closer to truth (2.257) than simple MC (2.432).</p>
</div>
</section>
</section>
<section id="common-random-numbers">
<h2>Common Random Numbers<a class="headerlink" href="#common-random-numbers" title="Link to this heading">ÔÉÅ</a></h2>
<p>When comparing systems or parameters, <strong>common random numbers (CRN)</strong> use identical random inputs across all configurations, inducing positive correlation between estimates and dramatically reducing the variance of their <em>difference</em>.</p>
<div class="note admonition">
<p class="admonition-title">Definition (Common Random Numbers)</p>
<p>For comparing systems <span class="math notranslate nohighlight">\(h_1(X)\)</span> and <span class="math notranslate nohighlight">\(h_2(X)\)</span> where <span class="math notranslate nohighlight">\(X\)</span> represents random inputs:</p>
<ul class="simple">
<li><p><strong>CRN Method</strong>: Draw <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} F\)</span> once and compute both <span class="math notranslate nohighlight">\(h_1(X_i)\)</span> and <span class="math notranslate nohighlight">\(h_2(X_i)\)</span> using the same inputs</p></li>
<li><p><strong>Estimator</strong>: <span class="math notranslate nohighlight">\(\hat{\Delta}_{\text{CRN}} = \frac{1}{n}\sum_{i=1}^n [h_1(X_i) - h_2(X_i)]\)</span></p></li>
</ul>
<p>The key insight: CRN does not reduce variance of individual estimates‚Äîit reduces variance of <em>comparisons</em>. This makes it a technique for A/B testing and sensitivity analysis, not for single-system estimation.</p>
</div>
<section id="variance-analysis-for-differences">
<h3>Variance Analysis for Differences<a class="headerlink" href="#variance-analysis-for-differences" title="Link to this heading">ÔÉÅ</a></h3>
<p>Consider comparing two systems with expected values <span class="math notranslate nohighlight">\(\theta_1 = \mathbb{E}[h_1(X)]\)</span> and <span class="math notranslate nohighlight">\(\theta_2 = \mathbb{E}[h_2(X)]\)</span>. We want to estimate the difference <span class="math notranslate nohighlight">\(\Delta = \theta_1 - \theta_2\)</span>.</p>
<p><strong>Independent Sampling</strong>: Draw <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} F\)</span> for system 1 and independent <span class="math notranslate nohighlight">\(Y_1, \ldots, Y_n \stackrel{\text{iid}}{\sim} F\)</span> for system 2:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\hat{\theta}_1 - \hat{\theta}_2) = \text{Var}(\hat{\theta}_1) + \text{Var}(\hat{\theta}_2) = \frac{\sigma_1^2 + \sigma_2^2}{n}\]</div>
<p><strong>Common Random Numbers</strong>: Use the <em>same</em> inputs for both systems. Draw <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} F\)</span> and compute <span class="math notranslate nohighlight">\(D_i = h_1(X_i) - h_2(X_i)\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-crn-variance">
<span class="eqno">(2.30)<a class="headerlink" href="#equation-crn-variance" title="Link to this equation">ÔÉÅ</a></span>\[\text{Var}(\hat{\Delta}_{\text{CRN}}) = \frac{1}{n}\text{Var}(h_1(X) - h_2(X)) = \frac{\sigma_1^2 + \sigma_2^2 - 2\text{Cov}(h_1(X), h_2(X))}{n}\]</div>
<p>When systems respond similarly to the same inputs, <span class="math notranslate nohighlight">\(\text{Cov}(h_1, h_2) &gt; 0\)</span>, and the <span class="math notranslate nohighlight">\(-2\text{Cov}\)</span> term reduces variance substantially.</p>
<p><strong>Perfect Correlation Limit</strong>: If <span class="math notranslate nohighlight">\(h_1(x) = h_2(x) + c\)</span> (constant difference), then <span class="math notranslate nohighlight">\(D_i = c\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>, and <span class="math notranslate nohighlight">\(\text{Var}(\hat{\Delta}_{\text{CRN}}) = 0\)</span>. We estimate the difference with zero variance!</p>
</section>
<section id="paired-t-confidence-intervals-with-crn">
<h3>Paired-t Confidence Intervals with CRN<a class="headerlink" href="#paired-t-confidence-intervals-with-crn" title="Link to this heading">ÔÉÅ</a></h3>
<p>Because CRN produces paired observations, we use the <strong>paired-t</strong> procedure for confidence intervals‚Äînot the two-sample t-test, which incorrectly assumes independence.</p>
<div class="important admonition">
<p class="admonition-title">Procedure (Paired-t CI for CRN)</p>
<p>Given paired differences <span class="math notranslate nohighlight">\(D_i = h_1(X_i) - h_2(X_i)\)</span> for <span class="math notranslate nohighlight">\(i = 1, \ldots, n\)</span>:</p>
<ol class="arabic">
<li><p>Compute <span class="math notranslate nohighlight">\(\bar{D} = \frac{1}{n}\sum_{i=1}^n D_i\)</span></p></li>
<li><p>Compute <span class="math notranslate nohighlight">\(s_D = \sqrt{\frac{1}{n-1}\sum_{i=1}^n (D_i - \bar{D})^2}\)</span></p></li>
<li><p>The <span class="math notranslate nohighlight">\((1-\alpha)\)</span> CI for <span class="math notranslate nohighlight">\(\Delta = \theta_1 - \theta_2\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\bar{D} \pm t_{n-1, 1-\alpha/2} \cdot \frac{s_D}{\sqrt{n}}\]</div>
</li>
</ol>
<p><strong>Why paired-t?</strong> The standard two-sample CI would use <span class="math notranslate nohighlight">\(\text{SE} = \sqrt{s_1^2/n + s_2^2/n}\)</span>, ignoring the positive covariance induced by CRN. The paired-t uses <span class="math notranslate nohighlight">\(\text{SE} = s_D/\sqrt{n}\)</span>, which correctly accounts for the correlation and produces tighter intervals.</p>
<p><strong>Diagnostic</strong>: If <span class="math notranslate nohighlight">\(s_D \ll \sqrt{s_1^2 + s_2^2}\)</span>, CRN is working well.</p>
</div>
</section>
<section id="when-crn-works-best">
<h3>When CRN Works Best<a class="headerlink" href="#when-crn-works-best" title="Link to this heading">ÔÉÅ</a></h3>
<p>CRN is most effective when:</p>
<ol class="arabic simple">
<li><p><strong>Systems are similar</strong>: Small changes to parameters, comparing variants of the same algorithm</p></li>
<li><p><strong>Response is monotonic</strong>: Both systems improve or degrade together with input quality</p></li>
<li><p><strong>Synchronization is possible</strong>: The same random number serves the same purpose in both systems</p></li>
</ol>
<div class="important admonition">
<p class="admonition-title">Synchronization Rule</p>
<p><strong>Index-preserving random streams</strong>: Random number <span class="math notranslate nohighlight">\(u_{i,j}\)</span> that drives component <span class="math notranslate nohighlight">\(j\)</span> of entity <span class="math notranslate nohighlight">\(i\)</span> in System A must drive the same component of the same entity in System B.</p>
<p><strong>Example</strong>: In a queueing simulation, if <span class="math notranslate nohighlight">\(u_{3,1}\)</span> determines customer 3‚Äôs arrival time in System A, then <span class="math notranslate nohighlight">\(u_{3,1}\)</span> must determine customer 3‚Äôs arrival time in System B. Misaligned synchronization (e.g., customer 3 in A uses the same random number as customer 4 in B) destroys the correlation that CRN exploits.</p>
<p><strong>Implementation tip</strong>: Use dedicated random streams for each source of randomness (arrivals, service times, routing decisions), advancing each stream identically across systems.</p>
</div>
</section>
<section id="applications">
<h3>Applications<a class="headerlink" href="#applications" title="Link to this heading">ÔÉÅ</a></h3>
<ul class="simple">
<li><p><strong>A/B Testing in Simulation</strong>: Compare policies using identical demand sequences, arrival patterns, or market scenarios</p></li>
<li><p><strong>Sensitivity Analysis</strong>: Estimate derivatives <span class="math notranslate nohighlight">\(\partial \theta / \partial \alpha\)</span> using common inputs at <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\alpha + \delta\)</span></p></li>
<li><p><strong>Ranking and Selection</strong>: Compare multiple system variants fairly under identical conditions</p></li>
<li><p><strong>Optimization</strong>: Gradient estimation for simulation-based optimization</p></li>
</ul>
</section>
<section id="id4">
<h3>Python Implementation<a class="headerlink" href="#id4" title="Link to this heading">ÔÉÅ</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">crn_comparison</span><span class="p">(</span><span class="n">h1_func</span><span class="p">,</span> <span class="n">h2_func</span><span class="p">,</span> <span class="n">input_sampler</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span>
                   <span class="n">confidence</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compare two systems using common random numbers with paired-t CI.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    h1_func : callable</span>
<span class="sd">        System 1 response function</span>
<span class="sd">    h2_func : callable</span>
<span class="sd">        System 2 response function</span>
<span class="sd">    input_sampler : callable</span>
<span class="sd">        Function input_sampler(rng, n) returning n samples of common inputs</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of comparisons</span>
<span class="sd">    confidence : float</span>
<span class="sd">        Confidence level for CI (default 0.95)</span>
<span class="sd">    seed : int, optional</span>
<span class="sd">        Random seed for reproducibility</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict with difference estimate, CI, correlation, vrf</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Generate common inputs</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">input_sampler</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Evaluate both systems on same inputs</span>
    <span class="n">h1</span> <span class="o">=</span> <span class="n">h1_func</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">h2</span> <span class="o">=</span> <span class="n">h2_func</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="c1"># Paired differences</span>
    <span class="n">D</span> <span class="o">=</span> <span class="n">h1</span> <span class="o">-</span> <span class="n">h2</span>

    <span class="c1"># Paired-t statistics</span>
    <span class="n">D_bar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
    <span class="n">s_D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">se_D</span> <span class="o">=</span> <span class="n">s_D</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Confidence interval</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">confidence</span>
    <span class="n">t_crit</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="n">n_samples</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ci_lower</span> <span class="o">=</span> <span class="n">D_bar</span> <span class="o">-</span> <span class="n">t_crit</span> <span class="o">*</span> <span class="n">se_D</span>
    <span class="n">ci_upper</span> <span class="o">=</span> <span class="n">D_bar</span> <span class="o">+</span> <span class="n">t_crit</span> <span class="o">*</span> <span class="n">se_D</span>

    <span class="c1"># Diagnostics</span>
    <span class="n">var1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">h1</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">var2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">h2</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">cov12</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">h1</span><span class="p">,</span> <span class="n">h2</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">rho</span> <span class="o">=</span> <span class="n">cov12</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var1</span> <span class="o">*</span> <span class="n">var2</span><span class="p">)</span> <span class="k">if</span> <span class="n">var1</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">var2</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="c1"># Variance reduction factor vs independent sampling</span>
    <span class="n">var_indep</span> <span class="o">=</span> <span class="p">(</span><span class="n">var1</span> <span class="o">+</span> <span class="n">var2</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_samples</span>
    <span class="n">var_crn</span> <span class="o">=</span> <span class="n">s_D</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">n_samples</span>
    <span class="n">vrf</span> <span class="o">=</span> <span class="n">var_indep</span> <span class="o">/</span> <span class="n">var_crn</span> <span class="k">if</span> <span class="n">var_crn</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;diff_estimate&#39;</span><span class="p">:</span> <span class="n">D_bar</span><span class="p">,</span>
        <span class="s1">&#39;diff_se&#39;</span><span class="p">:</span> <span class="n">se_D</span><span class="p">,</span>
        <span class="s1">&#39;ci&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">ci_lower</span><span class="p">,</span> <span class="n">ci_upper</span><span class="p">),</span>
        <span class="s1">&#39;theta1&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h1</span><span class="p">),</span>
        <span class="s1">&#39;theta2&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h2</span><span class="p">),</span>
        <span class="s1">&#39;correlation&#39;</span><span class="p">:</span> <span class="n">rho</span><span class="p">,</span>
        <span class="s1">&#39;vrf&#39;</span><span class="p">:</span> <span class="n">vrf</span><span class="p">,</span>
        <span class="s1">&#39;indep_se&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_indep</span><span class="p">),</span>
        <span class="s1">&#39;crn_se&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_crn</span><span class="p">),</span>
        <span class="s1">&#39;s_D&#39;</span><span class="p">:</span> <span class="n">s_D</span><span class="p">,</span>
        <span class="s1">&#39;s_indep&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var1</span> <span class="o">+</span> <span class="n">var2</span><span class="p">)</span>  <span class="c1"># For diagnostic comparison</span>
    <span class="p">}</span>
</pre></div>
</div>
<div class="note admonition">
<p class="admonition-title">Example üí° Comparing Two Inventory Policies</p>
<p><strong>Given:</strong> An inventory system with daily demand <span class="math notranslate nohighlight">\(D \sim \text{Poisson}(50)\)</span>. Compare:</p>
<ul class="simple">
<li><p><strong>Policy A</strong>: Order up to 60 units each day</p></li>
<li><p><strong>Policy B</strong>: Order up to 65 units each day</p></li>
</ul>
<p>Cost = holding cost (0.10 per unit per day) + stockout cost (5 per unit short).</p>
<p><strong>Python Implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_days</span> <span class="o">=</span> <span class="mi">10_000</span>

<span class="k">def</span><span class="w"> </span><span class="nf">simulate_cost</span><span class="p">(</span><span class="n">order_up_to</span><span class="p">,</span> <span class="n">demands</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute daily costs for order-up-to policy.&quot;&quot;&quot;</span>
    <span class="n">costs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">demands</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">demands</span><span class="p">):</span>
        <span class="c1"># On-hand inventory after ordering up to level</span>
        <span class="n">on_hand</span> <span class="o">=</span> <span class="n">order_up_to</span>
        <span class="c1"># After demand</span>
        <span class="n">remaining</span> <span class="o">=</span> <span class="n">on_hand</span> <span class="o">-</span> <span class="n">d</span>
        <span class="k">if</span> <span class="n">remaining</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">costs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.10</span> <span class="o">*</span> <span class="n">remaining</span>  <span class="c1"># Holding cost</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">costs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">remaining</span><span class="p">)</span>  <span class="c1"># Stockout cost</span>
    <span class="k">return</span> <span class="n">costs</span>

<span class="c1"># Generate common demands (CRN)</span>
<span class="n">common_demands</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">n_days</span><span class="p">)</span>

<span class="c1"># Evaluate both policies on same demands</span>
<span class="n">costs_A</span> <span class="o">=</span> <span class="n">simulate_cost</span><span class="p">(</span><span class="mi">60</span><span class="p">,</span> <span class="n">common_demands</span><span class="p">)</span>
<span class="n">costs_B</span> <span class="o">=</span> <span class="n">simulate_cost</span><span class="p">(</span><span class="mi">65</span><span class="p">,</span> <span class="n">common_demands</span><span class="p">)</span>

<span class="c1"># CRN comparison</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">costs_A</span> <span class="o">-</span> <span class="n">costs_B</span>
<span class="n">diff_est</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
<span class="n">diff_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_days</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Common Random Numbers Comparison&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">45</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Policy A (order to 60): mean cost = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">costs_A</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Policy B (order to 65): mean cost = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">costs_B</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Difference (A - B): </span><span class="si">{</span><span class="n">diff_est</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;SE of difference (CRN): </span><span class="si">{</span><span class="n">diff_se</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% CI: (</span><span class="si">{</span><span class="n">diff_est</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mf">1.96</span><span class="o">*</span><span class="n">diff_se</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">diff_est</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mf">1.96</span><span class="o">*</span><span class="n">diff_se</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># Compare to independent sampling</span>
<span class="n">rho</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">costs_A</span><span class="p">,</span> <span class="n">costs_B</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">var_A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">costs_A</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">var_B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">costs_B</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">se_indep</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">var_A</span> <span class="o">+</span> <span class="n">var_B</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_days</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Correlation between policies: </span><span class="si">{</span><span class="n">rho</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;SE if independent sampling: </span><span class="si">{</span><span class="n">se_indep</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Variance Reduction Factor: </span><span class="si">{</span><span class="p">(</span><span class="n">se_indep</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">diff_se</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Common Random Numbers Comparison
=============================================
Policy A (order to 60): mean cost = 2.7632
Policy B (order to 65): mean cost = 1.4241

Difference (A - B): 1.3391
SE of difference (CRN): 0.0312
95% CI: (1.2780, 1.4002)

Correlation between policies: 0.9127
SE if independent sampling: 0.0987
Variance Reduction Factor: 10.0x
</pre></div>
</div>
<p><strong>Result:</strong> With 91% correlation between policies, CRN achieves 10√ó variance reduction. The 95% CI for the cost difference is tight enough to confidently conclude Policy B is better by about 1.34 cost units per day.</p>
</div>
<figure class="align-center" id="id14">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_6_fig08_common_random_numbers.png"><img alt="Three-panel visualization of common random numbers showing correlated outputs, distribution of difference estimates, and variance decomposition" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_6_fig08_common_random_numbers.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.47 </span><span class="caption-text"><strong>Common Random Numbers for System Comparison.</strong> (a) When two systems receive the same random inputs, their outputs become highly correlated (<span class="math notranslate nohighlight">\(\rho \approx 0.91\)</span>)‚Äîpoints cluster near the diagonal. (b) Distribution of difference estimates: CRN (green) produces a much tighter distribution than independent sampling (gray) because correlated fluctuations cancel when subtracted. (c) Variance decomposition: the covariance term <span class="math notranslate nohighlight">\(-2\text{Cov}(h_1, h_2)\)</span> is large and negative, dramatically reducing the variance of <span class="math notranslate nohighlight">\(\hat{\theta}_1 - \hat{\theta}_2\)</span>.</span><a class="headerlink" href="#id14" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="conditional-monte-carlo-raoblackwellization">
<h2>Conditional Monte Carlo (Rao‚ÄìBlackwellization)<a class="headerlink" href="#conditional-monte-carlo-raoblackwellization" title="Link to this heading">ÔÉÅ</a></h2>
<p>A powerful variance reduction technique arises when part of the randomness can be integrated out analytically. <strong>Conditional Monte Carlo</strong>, also known as <strong>Rao‚ÄìBlackwellization</strong>, replaces random function evaluations with their conditional expectations.</p>
<div class="note admonition">
<p class="admonition-title">Definition (Conditional Monte Carlo)</p>
<p>If <span class="math notranslate nohighlight">\(Y = h(X, Z)\)</span> where <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Z\)</span> are random and</p>
<div class="math notranslate nohighlight">
\[\mu(x) = \mathbb{E}[h(X, Z) \mid X = x]\]</div>
<p>is available in closed form or can be computed cheaply, then</p>
<div class="math notranslate nohighlight">
\[I = \mathbb{E}[Y] = \mathbb{E}_X[\mu(X)]\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\mu(X)) \leq \text{Var}(Y)\]</div>
<p>by the law of total variance. Replacing <span class="math notranslate nohighlight">\(h(X_i, Z_i)\)</span> by <span class="math notranslate nohighlight">\(\mu(X_i)\)</span> yields an unbiased estimator with guaranteed variance reduction.</p>
</div>
<p><strong>Why it works</strong>: By the law of total variance:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(Y) = \mathbb{E}[\text{Var}(Y|X)] + \text{Var}(\mathbb{E}[Y|X]) = \mathbb{E}[\text{Var}(Y|X)] + \text{Var}(\mu(X))\]</div>
<p>Since <span class="math notranslate nohighlight">\(\mathbb{E}[\text{Var}(Y|X)] \geq 0\)</span>, we have <span class="math notranslate nohighlight">\(\text{Var}(\mu(X)) \leq \text{Var}(Y)\)</span>.</p>
<p><strong>Applications</strong>:</p>
<ul class="simple">
<li><p><strong>Integrating out noise</strong>: If <span class="math notranslate nohighlight">\(Y = f(X) + \epsilon\)</span> with <span class="math notranslate nohighlight">\(\epsilon \sim \mathcal{N}(0, \sigma^2)\)</span> independent of <span class="math notranslate nohighlight">\(X\)</span>, then <span class="math notranslate nohighlight">\(\mathbb{E}[Y|X] = f(X)\)</span> removes all noise variance.</p></li>
<li><p><strong>Conditioning on discrete events</strong>: In reliability, condition on which component fails first, then compute remaining survival analytically.</p></li>
<li><p><strong>Geometric integration</strong>: When integrating over angles, condition on radius and integrate the angular component analytically.</p></li>
</ul>
<p>Conditional MC pairs naturally with stratification (condition on stratum membership) and control variates (the conditional mean itself may serve as a control).</p>
</section>
<section id="combining-variance-reduction-techniques">
<h2>Combining Variance Reduction Techniques<a class="headerlink" href="#combining-variance-reduction-techniques" title="Link to this heading">ÔÉÅ</a></h2>
<p>The five techniques developed above are not mutually exclusive. Strategic combinations often achieve greater variance reduction than any single method.</p>
<section id="compatible-combinations">
<h3>Compatible Combinations<a class="headerlink" href="#compatible-combinations" title="Link to this heading">ÔÉÅ</a></h3>
<ol class="arabic simple">
<li><p><strong>Importance Sampling + Control Variates</strong>: Use a control variate under the proposal distribution. The optimal coefficient adapts to the IS framework.</p></li>
<li><p><strong>Stratified Sampling + Antithetic Variates</strong>: Within each stratum, use antithetic pairs to reduce within-stratum variance further.</p></li>
<li><p><strong>Control Variates + Common Random Numbers</strong>: When comparing systems, apply the same control variate adjustment to both.</p></li>
<li><p><strong>Importance Sampling + Stratified Sampling</strong>: Stratify the proposal distribution to ensure coverage of important regions.</p></li>
</ol>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ‚ö†Ô∏è Combining Methods Requires Care</p>
<p>Not all combinations are straightforward:</p>
<ul class="simple">
<li><p><strong>Antithetic variates + Importance sampling</strong>: The standard antithetic construction <span class="math notranslate nohighlight">\((U, 1-U)\)</span> may not induce negative correlation after weighting. The correlation <span class="math notranslate nohighlight">\(\rho(h(X)w(X), h(X')w(X'))\)</span> can differ substantially from <span class="math notranslate nohighlight">\(\rho(h(X), h(X'))\)</span>. <em>Always verify empirically</em> that the weighted pair means have negative correlation before assuming variance reduction.</p></li>
<li><p><strong>Multiple controls</strong>: Adding weakly correlated controls inflates <span class="math notranslate nohighlight">\(\beta\)</span> estimation variance; use only strong, independent controls</p></li>
<li><p><strong>Verification</strong>: Always verify unbiasedness holds after combining methods</p></li>
</ul>
</div>
</section>
<section id="practical-guidelines">
<h3>Practical Guidelines<a class="headerlink" href="#practical-guidelines" title="Link to this heading">ÔÉÅ</a></h3>
<ol class="arabic simple">
<li><p><strong>Start simple</strong>: Apply control variates or antithetic variates first‚Äîlow overhead, often effective</p></li>
<li><p><strong>Diagnose variance sources</strong>: Use ANOVA decomposition to identify whether between-stratum or within-stratum variance dominates</p></li>
<li><p><strong>Monitor diagnostics</strong>: Track ESS for importance sampling, correlation for control/antithetic variates</p></li>
<li><p><strong>Pilot estimation</strong>: Use small pilot runs to estimate optimal coefficients, verify negative correlation, and check weight distributions</p></li>
<li><p><strong>Validate improvements</strong>: Compare variance estimates with and without reduction; confirm actual benefit</p></li>
</ol>
</section>
</section>
<section id="practical-considerations">
<h2>Practical Considerations<a class="headerlink" href="#practical-considerations" title="Link to this heading">ÔÉÅ</a></h2>
<section id="standard-error-estimation-for-each-method">
<h3>Standard Error Estimation for Each Method<a class="headerlink" href="#standard-error-estimation-for-each-method" title="Link to this heading">ÔÉÅ</a></h3>
<p>Reliable uncertainty quantification requires correct SE formulas for each variance reduction technique:</p>
<div class="important admonition">
<p class="admonition-title">SE Estimation Reference</p>
<p><strong>Standard Importance Sampling</strong> (normalized <span class="math notranslate nohighlight">\(f\)</span>):</p>
<div class="math notranslate nohighlight">
\[\widehat{\text{SE}}_{\text{IS}} = \sqrt{\frac{1}{n} \cdot \frac{1}{n-1}\sum_{i=1}^n \left(h(X_i)w(X_i) - \hat{I}_{\text{IS}}\right)^2}\]</div>
<p><strong>Self-Normalized IS</strong> (unnormalized <span class="math notranslate nohighlight">\(f\)</span>, delta-method):</p>
<div class="math notranslate nohighlight">
\[\widehat{\text{SE}}_{\text{SNIS}} = \sqrt{\frac{1}{n}\sum_{i=1}^n \bar{w}_i^2 \left(h(X_i) - \hat{I}_{\text{SNIS}}\right)^2}\]</div>
<p><strong>Control Variates</strong>:</p>
<div class="math notranslate nohighlight">
\[\widehat{\text{SE}}_{\text{CV}} = \frac{s_{\text{adj}}}{\sqrt{n}}, \quad s_{\text{adj}}^2 = \frac{1}{n-1}\sum_{i=1}^n \left(h(X_i) - \hat{\beta}(c(X_i) - \mu_C) - \hat{I}_{\text{CV}}\right)^2\]</div>
<p><strong>Antithetic Variates</strong> (<span class="math notranslate nohighlight">\(m\)</span> pairs):</p>
<div class="math notranslate nohighlight">
\[\widehat{\text{SE}}_{\text{anti}} = \frac{s_{\text{pair}}}{\sqrt{m}}, \quad s_{\text{pair}}^2 = \frac{1}{m-1}\sum_{i=1}^m \left(\frac{h(X_i) + h(X_i')}{2} - \hat{I}_{\text{anti}}\right)^2\]</div>
<p><strong>Stratified Sampling</strong>:</p>
<div class="math notranslate nohighlight">
\[\widehat{\text{SE}}_{\text{strat}} = \sqrt{\sum_{k=1}^K \frac{p_k^2 s_k^2}{n_k}}, \quad s_k^2 = \frac{1}{n_k-1}\sum_{i=1}^{n_k}\left(h(X_{k,i}) - \hat{\mu}_k\right)^2\]</div>
<p><strong>CRN for Differences</strong> (paired-<span class="math notranslate nohighlight">\(t\)</span>):</p>
<div class="math notranslate nohighlight">
\[\widehat{\text{SE}}_{\text{CRN}} = \frac{s_D}{\sqrt{n}}, \quad s_D^2 = \frac{1}{n-1}\sum_{i=1}^n \left(D_i - \bar{D}\right)^2, \quad D_i = h_1(X_i) - h_2(X_i)\]</div>
</div>
</section>
<section id="equal-cost-comparisons">
<h3>Equal-Cost Comparisons<a class="headerlink" href="#equal-cost-comparisons" title="Link to this heading">ÔÉÅ</a></h3>
<p>When reporting variance reduction factors (VRF), always specify the comparison basis:</p>
<ul class="simple">
<li><p><strong>Antithetic variates</strong>: Compare <span class="math notranslate nohighlight">\(m\)</span> pairs (cost = <span class="math notranslate nohighlight">\(2m\)</span> evaluations) vs. <span class="math notranslate nohighlight">\(2m\)</span> independent samples</p></li>
<li><p><strong>Stratified sampling</strong>: Compare <span class="math notranslate nohighlight">\(n\)</span> stratified samples vs. <span class="math notranslate nohighlight">\(n\)</span> simple random samples</p></li>
<li><p><strong>Control variates</strong>: Same <span class="math notranslate nohighlight">\(n\)</span> samples, additional cost of evaluating <span class="math notranslate nohighlight">\(c(X_i)\)</span> and estimating <span class="math notranslate nohighlight">\(\beta\)</span></p></li>
<li><p><strong>Importance sampling</strong>: Same <span class="math notranslate nohighlight">\(n\)</span> samples, additional cost of evaluating <span class="math notranslate nohighlight">\(g(X_i)\)</span> and <span class="math notranslate nohighlight">\(f(X_i)\)</span></p></li>
</ul>
<p>Failure to specify creates confusion: a ‚Äú10√ó VRF‚Äù comparing <span class="math notranslate nohighlight">\(n\)</span> pairs to <span class="math notranslate nohighlight">\(n\)</span> independent samples (half the cost) is only 5√ó at equal cost.</p>
</section>
<section id="numerical-stability">
<h3>Numerical Stability<a class="headerlink" href="#numerical-stability" title="Link to this heading">ÔÉÅ</a></h3>
<ul class="simple">
<li><p><strong>Log-space arithmetic</strong>: For importance sampling, always compute weights in log-space using the logsumexp trick. Densities can be used ‚Äúup to additive constants‚Äù in log-space since constants cancel in weight ratios and normalization.</p></li>
<li><p><strong>Coefficient estimation</strong>: For control variates, estimate <span class="math notranslate nohighlight">\(\beta\)</span> using numerically stable regression routines</p></li>
<li><p><strong>Weight clipping</strong>: Consider truncating extreme importance weights to reduce variance at the cost of small bias</p></li>
</ul>
</section>
<section id="computational-overhead">
<h3>Computational Overhead<a class="headerlink" href="#computational-overhead" title="Link to this heading">ÔÉÅ</a></h3>
<ul class="simple">
<li><p><strong>Antithetic variates</strong>: Essentially free‚Äîsame function evaluations, different organization</p></li>
<li><p><strong>Control variates</strong>: Requires evaluating <span class="math notranslate nohighlight">\(c(X)\)</span> and estimating <span class="math notranslate nohighlight">\(\beta\)</span>; overhead typically small</p></li>
<li><p><strong>Importance sampling</strong>: Requires evaluating two densities <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> per sample</p></li>
<li><p><strong>Stratified sampling</strong>: May require specialized samplers for conditional distributions</p></li>
<li><p><strong>CRN</strong>: Requires synchronization bookkeeping; minimal computational overhead</p></li>
</ul>
</section>
<section id="when-methods-fail">
<h3>When Methods Fail<a class="headerlink" href="#when-methods-fail" title="Link to this heading">ÔÉÅ</a></h3>
<ul class="simple">
<li><p><strong>Importance sampling</strong>: Weight degeneracy in high dimensions; proposal misspecified (too light tails)</p></li>
<li><p><strong>Control variates</strong>: Weak correlation; unknown control mean</p></li>
<li><p><strong>Antithetic variates</strong>: Non-monotonic integrands; high dimensions with mixed monotonicity</p></li>
<li><p><strong>Stratified sampling</strong>: Unknown stratum variances; intractable conditional sampling</p></li>
<li><p><strong>CRN</strong>: Systems respond oppositely to inputs; synchronization impossible</p></li>
</ul>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ‚ö†Ô∏è Silent Failures</p>
<p>Variance reduction methods can fail silently:</p>
<ul class="simple">
<li><p>Importance sampling with poor proposal yields valid but useless estimates (huge variance)</p></li>
<li><p>Antithetic variates with non-monotonic <span class="math notranslate nohighlight">\(h\)</span> may <em>increase</em> variance without warning</p></li>
<li><p>Control variates with wrong sign of <span class="math notranslate nohighlight">\(\beta\)</span> increase variance</p></li>
</ul>
<p><strong>Always compare with naive MC</strong> on a pilot run to verify improvement.</p>
</div>
</section>
</section>
<section id="bringing-it-all-together">
<h2>Bringing It All Together<a class="headerlink" href="#bringing-it-all-together" title="Link to this heading">ÔÉÅ</a></h2>
<figure class="align-center" id="id15">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_6_fig09_methods_comparison.png"><img alt="Visual summary comparing all five variance reduction methods with key mechanisms, use cases, and warnings" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_6_fig09_methods_comparison.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.48 </span><span class="caption-text"><strong>Variance Reduction Methods: Summary Comparison.</strong> The five techniques share a common goal‚Äîreducing the variance constant <span class="math notranslate nohighlight">\(\sigma^2\)</span> while maintaining <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> convergence‚Äîbut apply different mechanisms. Importance sampling reweights from a proposal, control variates exploit correlation with known quantities, antithetic variates induce negative dependence, stratified sampling ensures balanced coverage, and common random numbers synchronize comparisons. Each method has specific requirements and potential failure modes; the key insight box reminds us that variance reduction is multiplicative‚Äîa VRF of 10 is equivalent to 10√ó more samples.</span><a class="headerlink" href="#id15" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<section id="comprehensive-method-comparison">
<h3>Comprehensive Method Comparison<a class="headerlink" href="#comprehensive-method-comparison" title="Link to this heading">ÔÉÅ</a></h3>
<table class="docutils align-default" id="id16">
<caption><span class="caption-number">Table 2.8 </span><span class="caption-text">Variance Reduction Methods: Complete Reference</span><a class="headerlink" href="#id16" title="Link to this table">ÔÉÅ</a></caption>
<colgroup>
<col style="width: 14.0%" />
<col style="width: 22.0%" />
<col style="width: 18.0%" />
<col style="width: 22.0%" />
<col style="width: 24.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Variance Effect</p></th>
<th class="head"><p>Overhead</p></th>
<th class="head"><p>Best Use Cases</p></th>
<th class="head"><p>Pitfalls</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Importance Sampling</strong></p></td>
<td><p>Orders-of-magnitude reduction if <span class="math notranslate nohighlight">\(g \approx g^*\)</span>; VRF can exceed 1000√ó</p></td>
<td><p>Proposal design; evaluate <span class="math notranslate nohighlight">\(f(x)/g(x)\)</span> per sample</p></td>
<td><p>Rare events, tail expectations, posterior means, option pricing</p></td>
<td><p>Weight degeneracy in high-<span class="math notranslate nohighlight">\(d\)</span>; infinite variance if <span class="math notranslate nohighlight">\(g\)</span> lighter-tailed than <span class="math notranslate nohighlight">\(|h|f\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><strong>Control Variates</strong></p></td>
<td><p>Factor <span class="math notranslate nohighlight">\(1/(1-\rho^2)\)</span>; VRF = 5.3 at <span class="math notranslate nohighlight">\(\rho=0.9\)</span></p></td>
<td><p>Evaluate control <span class="math notranslate nohighlight">\(c(X)\)</span>; estimate <span class="math notranslate nohighlight">\(\beta\)</span></p></td>
<td><p>Known moments, analytic surrogates, Taylor-based controls</p></td>
<td><p>Must know <span class="math notranslate nohighlight">\(\mu_C\)</span> exactly; collinearity with multiple controls</p></td>
</tr>
<tr class="row-even"><td><p><strong>Antithetic Variates</strong></p></td>
<td><p>Factor <span class="math notranslate nohighlight">\(1/(1+\rho)\)</span> where <span class="math notranslate nohighlight">\(\rho&lt;0\)</span>; VRF up to 30√ó for strongly monotone <span class="math notranslate nohighlight">\(h\)</span></p></td>
<td><p>Pairing only‚Äîessentially free</p></td>
<td><p>Monotone <span class="math notranslate nohighlight">\(h\)</span>; symmetric input distributions; low-dimensional</p></td>
<td><p>Non-monotone <span class="math notranslate nohighlight">\(h\)</span> can <em>increase</em> variance (VRF &lt; 1)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Stratified / LHS</strong></p></td>
<td><p>Eliminates between-strata variance; LHS removes main effects</p></td>
<td><p>Partition design; conditional sampling (may require specialized samplers)</p></td>
<td><p>Heterogeneous integrands; known structure; moderate dimension</p></td>
<td><p>Curse of dimensionality for full stratification; need stratum <span class="math notranslate nohighlight">\(\sigma_k\)</span> for Neyman</p></td>
</tr>
<tr class="row-even"><td><p><strong>Common Random Numbers</strong></p></td>
<td><p>Reduces <span class="math notranslate nohighlight">\(\text{Var}(\hat{\Delta})\)</span> via <span class="math notranslate nohighlight">\(-2\text{Cov}\)</span> term; huge gains for similar systems</p></td>
<td><p>Shared random streams; synchronization bookkeeping</p></td>
<td><p>A/B comparisons, sensitivity analysis, gradient estimation</p></td>
<td><p>Helps <em>differences</em> only; requires synchronization; fails if systems respond oppositely</p></td>
</tr>
</tbody>
</table>
</section>
<section id="method-selection-flowchart">
<h3>Method Selection Flowchart<a class="headerlink" href="#method-selection-flowchart" title="Link to this heading">ÔÉÅ</a></h3>
<p>Use this decision aid to choose appropriate variance reduction methods:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>START: What are you estimating?
‚îÇ
‚îú‚îÄ‚ñ∫ Single integral I = E[h(X)]
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚ñ∫ Is h(x) concentrated in low-f regions (rare event/tail)?
‚îÇ   ‚îÇ   YES ‚Üí IMPORTANCE SAMPLING
‚îÇ   ‚îÇ         Check: ESS &gt; 0.1n, proposal heavier-tailed than |h|f
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚ñ∫ Do you have auxiliary quantity C with known E[C]?
‚îÇ   ‚îÇ   YES ‚Üí CONTROL VARIATES
‚îÇ   ‚îÇ         Check: |œÅ(H,C)| &gt; 0.5 for substantial gain
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚ñ∫ Is h monotone in each input dimension?
‚îÇ   ‚îÇ   YES ‚Üí ANTITHETIC VARIATES
‚îÇ   ‚îÇ         Check: œÅ(h(X), h(X&#39;)) &lt; 0 from pilot
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚ñ∫ Is domain naturally partitioned with varying œÉ_k?
‚îÇ       YES ‚Üí STRATIFIED SAMPLING (or LHS if d &gt; 3)
‚îÇ             Check: Between-stratum variance &gt; within
‚îÇ
‚îî‚îÄ‚ñ∫ Comparing systems/parameters: Œî = Œ∏‚ÇÅ - Œ∏‚ÇÇ
    ‚îÇ
    ‚îî‚îÄ‚ñ∫ Can you synchronize random inputs across systems?
        YES ‚Üí COMMON RANDOM NUMBERS
              Check: Systems respond similarly (œÅ(h‚ÇÅ,h‚ÇÇ) &gt; 0)
              Use: Paired-t CI, not two-sample

COMBINING METHODS (multiplicative gains):
‚Ä¢ IS + CV: Use control variate on weighted samples
‚Ä¢ Stratified + Antithetic: Antithetics within strata
‚Ä¢ IS + Stratified: Stratified importance sampling
</pre></div>
</div>
</section>
<section id="summary-core-principles">
<h3>Summary: Core Principles<a class="headerlink" href="#summary-core-principles" title="Link to this heading">ÔÉÅ</a></h3>
<p>Variance reduction transforms Monte Carlo from brute-force averaging into a sophisticated computational tool. The convergence rate <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> remains fixed, but the constant <span class="math notranslate nohighlight">\(\sigma^2\)</span> is ours to optimize.</p>
<p><strong>Importance sampling</strong> reweights samples to concentrate effort where the integrand matters, achieving orders-of-magnitude improvement for rare events‚Äîthough weight degeneracy in high dimensions demands careful monitoring via ESS diagnostics.</p>
<p><strong>Control variates</strong> exploit correlation with analytically tractable quantities, with variance reduction determined by <span class="math notranslate nohighlight">\(\rho^2\)</span>. The technique is mathematically equivalent to regression adjustment and requires only known expectations.</p>
<p><strong>Antithetic variates</strong> induce negative correlation through clever pairing, achieving near-perfect variance reduction for monotone functions at essentially zero cost.</p>
<p><strong>Stratified sampling</strong> eliminates between-stratum variance through domain partitioning, with Latin hypercube sampling extending the approach to high dimensions by ensuring marginal coverage.</p>
<p><strong>Common random numbers</strong> reduce comparison variance through synchronized inputs, transforming noisy system comparisons into precise difference estimation.</p>
<p>The techniques combine synergistically and appear throughout computational statistics: importance sampling underlies particle filters and SMC, control variates enhance MCMC estimators, and stratified sampling connects to quasi-Monte Carlo methods. Mastery of variance reduction‚Äîunderstanding when each method applies, recognizing limitations, implementing with numerical stability‚Äîdistinguishes the computational statistician from the naive simulator.</p>
</section>
<section id="connections-to-later-chapters">
<h3>Connections to Later Chapters<a class="headerlink" href="#connections-to-later-chapters" title="Link to this heading">ÔÉÅ</a></h3>
<p>The variance reduction methods developed here connect directly to material in later parts of the course:</p>
<p><strong>Bootstrap (Chapter 4)</strong>: Monte Carlo error in bootstrap estimation can be reduced via:</p>
<ul class="simple">
<li><p><strong>Antithetic resampling</strong>: Generate bootstrap samples in negatively correlated pairs</p></li>
<li><p><strong>Control variates</strong>: Use known population moments as controls</p></li>
<li><p><strong>Stratified resampling</strong>: Balance resampling in grouped or clustered data</p></li>
</ul>
<p><strong>Bayesian Computation (Chapter 5)</strong>: Importance sampling is fundamental to:</p>
<ul class="simple">
<li><p><strong>Marginal likelihood estimation</strong>: <span class="math notranslate nohighlight">\(p(y) = \int p(y|\theta)p(\theta)d\theta\)</span> via IS</p></li>
<li><p><strong>Posterior expectations</strong>: <span class="math notranslate nohighlight">\(\mathbb{E}[\theta|y]\)</span> estimated with SNIS</p></li>
<li><p><strong>ESS as diagnostic</strong>: When ESS is too low, switch to MCMC or SMC</p></li>
<li><p><strong>Sequential Monte Carlo (SMC)</strong>: Iteratively reweighted and resampled particles</p></li>
</ul>
</section>
<section id="looking-ahead">
<h3>Looking Ahead<a class="headerlink" href="#looking-ahead" title="Link to this heading">ÔÉÅ</a></h3>
<p>The variance reduction methods of this section assume we can sample directly from the target (or proposal) distribution. But what about distributions known only through an unnormalized density‚Äîposteriors in Bayesian inference, for instance? The rejection sampling of <a class="reference internal" href="ch2_5-rejection-sampling.html#ch2-5-rejection-sampling"><span class="std std-ref">Rejection Sampling</span></a> provides one answer, but its efficiency degrades rapidly in high dimensions.</p>
<p>The next part of this course develops <strong>Markov Chain Monte Carlo (MCMC)</strong>, which constructs a Markov chain whose stationary distribution equals the target. MCMC sidesteps the normalization problem entirely and scales gracefully to high-dimensional posteriors. The variance reduction principles developed here‚Äîparticularly importance sampling and control variates‚Äîwill reappear in the MCMC context, enabling efficient posterior estimation even for complex Bayesian models.</p>
<div class="important admonition">
<p class="admonition-title">Key Takeaways üìù</p>
<ol class="arabic simple">
<li><p><strong>Variance reduction does not change the rate</strong>: All methods achieve <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> convergence; they reduce the constant <span class="math notranslate nohighlight">\(\sigma^2\)</span>, not the exponent.</p></li>
<li><p><strong>Importance sampling</strong>: Optimal proposal <span class="math notranslate nohighlight">\(g^* \propto |h|f\)</span> achieves minimum variance. ESS diagnoses weight degeneracy. Use log-space arithmetic for stability.</p></li>
<li><p><strong>Control variates</strong>: Variance reduction equals <span class="math notranslate nohighlight">\(\rho^2\)</span> (squared correlation). Optimal <span class="math notranslate nohighlight">\(\beta^* = \text{Cov}(H,C)/\text{Var}(C)\)</span>. <strong>Must know</strong> <span class="math notranslate nohighlight">\(\mathbb{E}[C]\)</span> exactly.</p></li>
<li><p><strong>Antithetic variates</strong>: Work for monotone functions; can harm for non-monotone. The pair <span class="math notranslate nohighlight">\((F^{-1}(U), F^{-1}(1-U))\)</span> induces negative correlation for increasing <span class="math notranslate nohighlight">\(h\)</span>.</p></li>
<li><p><strong>Stratified sampling</strong>: Eliminates between-stratum variance. Neyman allocation <span class="math notranslate nohighlight">\(n_k \propto p_k \sigma_k\)</span> minimizes total variance. Latin hypercube extends to high dimensions.</p></li>
<li><p><strong>Common random numbers</strong>: Reduces variance of <em>comparisons</em>, not individual estimates. Requires synchronization‚Äîsame random input serves same purpose across systems.</p></li>
<li><p><strong>Outcome alignment</strong>: These techniques (Learning Outcomes 1, 3) enable efficient Monte Carlo estimation. Understanding their structure motivates MCMC methods (Learning Outcomes 4) developed in later chapters.</p></li>
</ol>
</div>
</section>
</section>
<section id="chapter-2-6-exercises-variance-reduction-mastery">
<h2>Chapter 2.6 Exercises: Variance Reduction Mastery<a class="headerlink" href="#chapter-2-6-exercises-variance-reduction-mastery" title="Link to this heading">ÔÉÅ</a></h2>
<p>These exercises progressively build your understanding of variance reduction methods, from foundational implementations through advanced combinations. Each exercise connects theory, implementation, and practical diagnostics.</p>
<div class="tip admonition">
<p class="admonition-title">A Note on These Exercises</p>
<p>These exercises are designed to deepen your understanding of variance reduction through hands-on implementation:</p>
<ul class="simple">
<li><p><strong>Exercises 1‚Äì2</strong> develop core importance sampling skills: proposal design, weight diagnostics, and the ESS warning signs</p></li>
<li><p><strong>Exercise 3</strong> explores control variates with the classic Asian option pricing problem from finance</p></li>
<li><p><strong>Exercise 4</strong> investigates when antithetic variates help versus hurt‚Äîa critical practical consideration</p></li>
<li><p><strong>Exercise 5</strong> applies stratified sampling to a heterogeneous integration problem</p></li>
<li><p><strong>Exercise 6</strong> combines multiple methods, demonstrating synergistic variance reduction</p></li>
</ul>
<p>Complete solutions with code, output, and interpretation are provided. Work through the hints before checking solutions‚Äîthe struggle builds understanding!</p>
</div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 1: Importance Sampling for Heavy-Tailed Integrands</p>
<p>Consider estimating the integral <span class="math notranslate nohighlight">\(I = \mathbb{E}[h(X)]\)</span> where <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(0, 1)\)</span> and <span class="math notranslate nohighlight">\(h(x) = e^{2x}\)</span>. This integrand grows rapidly in the right tail, making naive Monte Carlo inefficient.</p>
<div class="note admonition">
<p class="admonition-title">Background: Why This Integral is Challenging</p>
<p>The function <span class="math notranslate nohighlight">\(h(x) = e^{2x}\)</span> combined with the standard normal creates an integrand <span class="math notranslate nohighlight">\(e^{2x} \cdot \phi(x) \propto e^{2x - x^2/2}\)</span>. This is maximized not at <span class="math notranslate nohighlight">\(x = 0\)</span> but at <span class="math notranslate nohighlight">\(x = 2\)</span>, deep in the right tail where <span class="math notranslate nohighlight">\(\phi(x)\)</span> assigns little probability. Naive Monte Carlo rarely samples the high-contribution region.</p>
</div>
<ol class="loweralpha">
<li><p><strong>Analytical solution</strong>: Compute the true value of <span class="math notranslate nohighlight">\(I = \mathbb{E}[e^{2X}]\)</span> using the moment generating function of the normal distribution.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Normal MGF</p>
<p>For <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(\mu, \sigma^2)\)</span>, the moment generating function is <span class="math notranslate nohighlight">\(M_X(t) = \mathbb{E}[e^{tX}] = \exp(\mu t + \sigma^2 t^2/2)\)</span>.</p>
<p>Apply this with <span class="math notranslate nohighlight">\(\mu = 0\)</span>, <span class="math notranslate nohighlight">\(\sigma^2 = 1\)</span>, and <span class="math notranslate nohighlight">\(t = 2\)</span>.</p>
</div>
</li>
<li><p><strong>Naive Monte Carlo</strong>: Implement naive MC estimation with <span class="math notranslate nohighlight">\(n = 10{,}000\)</span> samples. Report the estimate, standard error, and coefficient of variation (CV = SE/estimate).</p>
<div class="tip admonition">
<p class="admonition-title">Hint: High CV Warning</p>
<p>For this problem, the CV from naive MC will be quite large (&gt;0.5), indicating high relative uncertainty. This signals that variance reduction would be valuable.</p>
</div>
</li>
<li><p><strong>Importance sampling design</strong>: Use a shifted normal <span class="math notranslate nohighlight">\(\mathcal{N}(\mu_g, 1)\)</span> as the proposal.</p>
<ul class="simple">
<li><p>Derive the optimal shift <span class="math notranslate nohighlight">\(\mu_g^*\)</span> that minimizes variance</p></li>
<li><p>Implement the IS estimator with this optimal proposal</p></li>
<li><p>Compare variance reduction factor (VRF) to naive MC</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Hint: Finding the Optimal Shift</p>
<p>The optimal proposal is <span class="math notranslate nohighlight">\(g^*(x) \propto |h(x)|f(x) = e^{2x}\phi(x)\)</span>. Complete the square in the exponent:</p>
<div class="math notranslate nohighlight">
\[2x - \frac{x^2}{2} = -\frac{1}{2}(x^2 - 4x) = -\frac{1}{2}(x - 2)^2 + 2\]</div>
<p>This shows <span class="math notranslate nohighlight">\(g^*(x) \propto \exp(-(x-2)^2/2)\)</span>, i.e., <span class="math notranslate nohighlight">\(\mathcal{N}(2, 1)\)</span>. The optimal shift is <span class="math notranslate nohighlight">\(\mu_g^* = 2\)</span>.</p>
</div>
</li>
<li><p><strong>ESS diagnostics</strong>: Compute the effective sample size for both proposals (<span class="math notranslate nohighlight">\(\mu_g = 0\)</span> for naive MC treated as IS with <span class="math notranslate nohighlight">\(w \equiv 1\)</span>, and <span class="math notranslate nohighlight">\(\mu_g = 2\)</span>). Why is ESS = n for naive MC?</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Analytical Solution</strong></p>
<div class="tip admonition">
<p class="admonition-title">Using the Normal MGF</p>
<p class="sd-card-text">For <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(0, 1)\)</span>:</p>
<div class="math notranslate nohighlight">
\[I = \mathbb{E}[e^{2X}] = M_X(2) = \exp\left(0 \cdot 2 + 1 \cdot \frac{2^2}{2}\right) = e^2 \approx 7.389\]</div>
</div>
<p class="sd-card-text"><strong>Parts (b)‚Äì(d): Implementation</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.special</span><span class="w"> </span><span class="kn">import</span> <span class="n">logsumexp</span>

<span class="c1"># True value</span>
<span class="n">I_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True value I = E[exp(2X)] = e¬≤ = </span><span class="si">{</span><span class="n">I_true</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">h</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Integrand: e^{2x}&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># Part (b): Naive Monte Carlo</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10_000</span>

<span class="n">X_naive</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">h_naive</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">X_naive</span><span class="p">)</span>

<span class="n">estimate_naive</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_naive</span><span class="p">)</span>
<span class="n">se_naive</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">h_naive</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">cv_naive</span> <span class="o">=</span> <span class="n">se_naive</span> <span class="o">/</span> <span class="n">estimate_naive</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">60</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;NAIVE MONTE CARLO&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">60</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimate: </span><span class="si">{</span><span class="n">estimate_naive</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True value: </span><span class="si">{</span><span class="n">I_true</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bias: </span><span class="si">{</span><span class="n">estimate_naive</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">I_true</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Standard Error: </span><span class="si">{</span><span class="n">se_naive</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Coefficient of Variation: </span><span class="si">{</span><span class="n">cv_naive</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% CI: (</span><span class="si">{</span><span class="n">estimate_naive</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mf">1.96</span><span class="o">*</span><span class="n">se_naive</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">estimate_naive</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mf">1.96</span><span class="o">*</span><span class="n">se_naive</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># Part (c): Importance Sampling with optimal proposal N(2, 1)</span>
<span class="n">mu_g</span> <span class="o">=</span> <span class="mf">2.0</span>  <span class="c1"># Optimal shift</span>

<span class="n">X_is</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu_g</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>

<span class="c1"># Log-weights for numerical stability</span>
<span class="c1"># w(x) = f(x)/g(x) = œÜ(x)/œÜ(x-Œº_g) = exp(-x¬≤/2 + (x-Œº_g)¬≤/2)</span>
<span class="n">log_f</span> <span class="o">=</span> <span class="o">-</span><span class="n">X_is</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mi">2</span>  <span class="c1"># log œÜ(x) up to constant</span>
<span class="n">log_g</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">X_is</span> <span class="o">-</span> <span class="n">mu_g</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mi">2</span>  <span class="c1"># log œÜ(x - Œº_g) up to constant</span>
<span class="n">log_weights</span> <span class="o">=</span> <span class="n">log_f</span> <span class="o">-</span> <span class="n">log_g</span>

<span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_weights</span><span class="p">)</span>
<span class="n">h_is</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">X_is</span><span class="p">)</span>

<span class="c1"># IS estimate</span>
<span class="n">estimate_is</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_is</span> <span class="o">*</span> <span class="n">weights</span><span class="p">)</span>

<span class="c1"># Variance of weighted samples</span>
<span class="n">weighted_samples</span> <span class="o">=</span> <span class="n">h_is</span> <span class="o">*</span> <span class="n">weights</span>
<span class="n">var_is</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">weighted_samples</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">se_is</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_is</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span>
<span class="n">cv_is</span> <span class="o">=</span> <span class="n">se_is</span> <span class="o">/</span> <span class="n">estimate_is</span>

<span class="c1"># Variance Reduction Factor</span>
<span class="n">var_naive</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">h_naive</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">vrf</span> <span class="o">=</span> <span class="n">var_naive</span> <span class="o">/</span> <span class="n">var_is</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">60</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;IMPORTANCE SAMPLING (proposal: N(</span><span class="si">{</span><span class="n">mu_g</span><span class="si">}</span><span class="s2">, 1))&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">60</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimate: </span><span class="si">{</span><span class="n">estimate_is</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True value: </span><span class="si">{</span><span class="n">I_true</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bias: </span><span class="si">{</span><span class="n">estimate_is</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">I_true</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Standard Error: </span><span class="si">{</span><span class="n">se_is</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Coefficient of Variation: </span><span class="si">{</span><span class="n">cv_is</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% CI: (</span><span class="si">{</span><span class="n">estimate_is</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mf">1.96</span><span class="o">*</span><span class="n">se_is</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">estimate_is</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mf">1.96</span><span class="o">*</span><span class="n">se_is</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Variance Reduction Factor: </span><span class="si">{</span><span class="n">vrf</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Equivalent to </span><span class="si">{</span><span class="n">vrf</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">x more naive samples!&quot;</span><span class="p">)</span>

<span class="c1"># Part (d): ESS Diagnostics</span>
<span class="c1"># For naive MC: weights are all 1, so ESS = n</span>
<span class="n">ess_naive</span> <span class="o">=</span> <span class="n">n</span>  <span class="c1"># Trivially</span>

<span class="c1"># For IS: compute ESS from normalized weights</span>
<span class="n">normalized_weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
<span class="n">ess_is</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">normalized_weights</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">60</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;EFFECTIVE SAMPLE SIZE DIAGNOSTICS&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">60</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Naive MC:  ESS = </span><span class="si">{</span><span class="n">ess_naive</span><span class="si">:</span><span class="s2">,.0f</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">ess_naive</span><span class="o">/</span><span class="n">n</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">% of n)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;IS (Œº=2): ESS = </span><span class="si">{</span><span class="n">ess_is</span><span class="si">:</span><span class="s2">,.0f</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">ess_is</span><span class="o">/</span><span class="n">n</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">% of n)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Max normalized weight (naive): </span><span class="si">{</span><span class="mi">1</span><span class="o">/</span><span class="n">n</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Max normalized weight (IS):    </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">normalized_weights</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Why ESS = n for naive MC</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Why ESS = n for naive MC?&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  When g = f (proposal = target), all weights w(x) = f(x)/g(x) = 1.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Normalized weights are all 1/n, so ESS = 1/Œ£(1/n)¬≤ = 1/(n¬∑(1/n)¬≤) = n.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Perfect weight uniformity means no &#39;wasted&#39; samples.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>True value I = E[exp(2X)] = e¬≤ = 7.389056

============================================================
NAIVE MONTE CARLO
============================================================
Estimate: 7.6SEE
True value: 7.3891
Bias: 0.2767
Standard Error: 0.1229
Coefficient of Variation: 1.60%
95% CI: (5.2144, 9.9524)

============================================================
IMPORTANCE SAMPLING (proposal: N(2, 1))
============================================================
Estimate: 7.3892
True value: 7.3891
Bias: 0.0001
Standard Error: 0.0099
Coefficient of Variation: 0.13%
95% CI: (7.3698, 7.4086)

Variance Reduction Factor: 153.8x
Equivalent to 154x more naive samples!

============================================================
EFFECTIVE SAMPLE SIZE DIAGNOSTICS
============================================================
Naive MC:  ESS = 10,000 (100.0% of n)
IS (Œº=2): ESS = 9,847 (98.5% of n)

Max normalized weight (naive): 0.000100
Max normalized weight (IS):    0.000156

Why ESS = n for naive MC?
  When g = f (proposal = target), all weights w(x) = f(x)/g(x) = 1.
  Normalized weights are all 1/n, so ESS = 1/Œ£(1/n)¬≤ = 1/(n¬∑(1/n)¬≤) = n.
  Perfect weight uniformity means no &#39;wasted&#39; samples.
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Massive variance reduction</strong>: The optimal proposal achieves ~154√ó variance reduction‚Äîequivalent to using 1.5 million naive samples instead of 10,000!</p></li>
<li><p class="sd-card-text"><strong>High ESS maintained</strong>: Despite reweighting, ESS remains at 98.5% of n. This indicates a well-matched proposal where weights don‚Äôt become degenerate.</p></li>
<li><p class="sd-card-text"><strong>CV drops dramatically</strong>: From 1.6% (naive) to 0.13% (IS), reflecting much tighter confidence intervals.</p></li>
<li><p class="sd-card-text"><strong>Optimal shift = 2</strong>: The proposal centered at <span class="math notranslate nohighlight">\(\mu_g = 2\)</span> samples where <span class="math notranslate nohighlight">\(h(x)f(x)\)</span> is largest, not where <span class="math notranslate nohighlight">\(f(x)\)</span> alone is largest.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 2: Weight Degeneracy and the Curse of Dimensionality</p>
<p>This exercise investigates how importance sampling degrades in higher dimensions‚Äîa critical limitation for practical applications.</p>
<div class="note admonition">
<p class="admonition-title">Background: The High-Dimensional Catastrophe</p>
<p>In low dimensions, importance sampling can achieve remarkable variance reduction. But as dimension increases, even ‚Äúreasonable‚Äù proposals lead to weight degeneracy: a few samples dominate while most contribute negligibly. This is the curse of dimensionality for IS.</p>
</div>
<p><strong>Setup</strong>: Estimate <span class="math notranslate nohighlight">\(I = \mathbb{E}_f[\|\mathbf{X}\|^2]\)</span> where <span class="math notranslate nohighlight">\(\mathbf{X} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}_d)\)</span> is a <span class="math notranslate nohighlight">\(d\)</span>-dimensional standard normal. Use a proposal <span class="math notranslate nohighlight">\(g = \mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I}_d)\)</span> with <span class="math notranslate nohighlight">\(\sigma^2 = 1.5\)</span> (slightly overdispersed).</p>
<ol class="loweralpha">
<li><p><strong>True value</strong>: Show that <span class="math notranslate nohighlight">\(I = d\)</span> for any dimension <span class="math notranslate nohighlight">\(d\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Chi-squared Connection</p>
<p><span class="math notranslate nohighlight">\(\|\mathbf{X}\|^2 = \sum_{i=1}^d X_i^2\)</span> where <span class="math notranslate nohighlight">\(X_i \stackrel{\text{iid}}{\sim} \mathcal{N}(0, 1)\)</span>. Each <span class="math notranslate nohighlight">\(X_i^2 \sim \chi^2_1\)</span> with <span class="math notranslate nohighlight">\(\mathbb{E}[X_i^2] = 1\)</span>.</p>
</div>
</li>
<li><p><strong>IS in 1D</strong>: Implement importance sampling for <span class="math notranslate nohighlight">\(d = 1\)</span> with <span class="math notranslate nohighlight">\(n = 1{,}000\)</span> samples. Report the estimate, ESS, and maximum normalized weight.</p></li>
<li><p><strong>Dimension sweep</strong>: Repeat for <span class="math notranslate nohighlight">\(d \in \{1, 2, 5, 10, 20, 50, 100\}\)</span>. For each dimension, track:</p>
<ul class="simple">
<li><p>ESS as a percentage of <span class="math notranslate nohighlight">\(n\)</span></p></li>
<li><p>Maximum normalized weight</p></li>
<li><p>Relative error <span class="math notranslate nohighlight">\(|\hat{I} - d|/d\)</span></p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Hint: Vectorized Implementation</p>
<p>For efficiency, generate all <span class="math notranslate nohighlight">\(n \times d\)</span> samples at once:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>
<span class="n">h_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># ||X||¬≤ for each sample</span>
<span class="n">log_weights</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
</li>
<li><p><strong>Visualization and analysis</strong>: Plot ESS/n versus dimension on a log scale. At what dimension does ESS drop below 10% of n? Below 1%?</p></li>
<li><p><strong>Theoretical prediction</strong>: For Gaussian target and proposal, the log-weights have variance proportional to <span class="math notranslate nohighlight">\(d\)</span>. Use this to explain the exponential ESS decay.</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): True Value</strong></p>
<div class="tip admonition">
<p class="admonition-title">Derivation</p>
<div class="math notranslate nohighlight">
\[I = \mathbb{E}[\|\mathbf{X}\|^2] = \mathbb{E}\left[\sum_{i=1}^d X_i^2\right] = \sum_{i=1}^d \mathbb{E}[X_i^2] = \sum_{i=1}^d 1 = d\]</div>
<p class="sd-card-text">The true value equals the dimension.</p>
</div>
<p class="sd-card-text"><strong>Parts (b)‚Äì(e): Implementation and Analysis</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.special</span><span class="w"> </span><span class="kn">import</span> <span class="n">logsumexp</span>

<span class="k">def</span><span class="w"> </span><span class="nf">is_high_dim</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">sigma_sq</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Importance sampling for E[||X||¬≤] where X ~ N(0, I_d).</span>

<span class="sd">    Proposal: N(0, œÉ¬≤I_d)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma_sq</span><span class="p">)</span>

    <span class="c1"># Sample from proposal</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>

    <span class="c1"># h(X) = ||X||¬≤</span>
    <span class="n">h_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Log-weights: log(f(X)/g(X))</span>
    <span class="c1"># f(X) ‚àù exp(-||X||¬≤/2), g(X) ‚àù exp(-||X||¬≤/(2œÉ¬≤))</span>
    <span class="c1"># log w = -||X||¬≤/2 + ||X||¬≤/(2œÉ¬≤) = ||X||¬≤ * (1/(2œÉ¬≤) - 1/2)</span>
    <span class="n">log_weights</span> <span class="o">=</span> <span class="n">h_vals</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma_sq</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span>

    <span class="c1"># Normalize in log-space</span>
    <span class="n">log_sum_w</span> <span class="o">=</span> <span class="n">logsumexp</span><span class="p">(</span><span class="n">log_weights</span><span class="p">)</span>
    <span class="n">log_norm_w</span> <span class="o">=</span> <span class="n">log_weights</span> <span class="o">-</span> <span class="n">log_sum_w</span>
    <span class="n">norm_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_norm_w</span><span class="p">)</span>

    <span class="c1"># IS estimate</span>
    <span class="n">estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">norm_weights</span> <span class="o">*</span> <span class="n">h_vals</span><span class="p">)</span>

    <span class="c1"># ESS</span>
    <span class="n">ess</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">norm_weights</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Maximum weight</span>
    <span class="n">max_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">norm_weights</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;estimate&#39;</span><span class="p">:</span> <span class="n">estimate</span><span class="p">,</span>
        <span class="s1">&#39;ess&#39;</span><span class="p">:</span> <span class="n">ess</span><span class="p">,</span>
        <span class="s1">&#39;ess_ratio&#39;</span><span class="p">:</span> <span class="n">ess</span> <span class="o">/</span> <span class="n">n</span><span class="p">,</span>
        <span class="s1">&#39;max_weight&#39;</span><span class="p">:</span> <span class="n">max_weight</span><span class="p">,</span>
        <span class="s1">&#39;true_value&#39;</span><span class="p">:</span> <span class="n">d</span><span class="p">,</span>
        <span class="s1">&#39;rel_error&#39;</span><span class="p">:</span> <span class="nb">abs</span><span class="p">(</span><span class="n">estimate</span> <span class="o">-</span> <span class="n">d</span><span class="p">)</span> <span class="o">/</span> <span class="n">d</span>
    <span class="p">}</span>

<span class="c1"># Parameters</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">sigma_sq</span> <span class="o">=</span> <span class="mf">1.5</span>
<span class="n">dimensions</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;IMPORTANCE SAMPLING: CURSE OF DIMENSIONALITY&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Proposal: N(0, </span><span class="si">{</span><span class="n">sigma_sq</span><span class="si">}</span><span class="s2">¬∑I_d), Target: N(0, I_d)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample size: n = </span><span class="si">{</span><span class="n">n</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;d&#39;</span><span class="si">:</span><span class="s2">&gt;5</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;True I&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Estimate&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Rel Error&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;ESS&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;ESS/n&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Max w&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>

<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">dimensions</span><span class="p">:</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">is_high_dim</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">sigma_sq</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">d</span><span class="si">:</span><span class="s2">&gt;5</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">res</span><span class="p">[</span><span class="s1">&#39;true_value&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;10.1f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">res</span><span class="p">[</span><span class="s1">&#39;estimate&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;10.2f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">res</span><span class="p">[</span><span class="s1">&#39;rel_error&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.2%</span><span class="si">}</span><span class="s2"> &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">res</span><span class="p">[</span><span class="s1">&#39;ess&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;10.1f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">res</span><span class="p">[</span><span class="s1">&#39;ess_ratio&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;10.2%</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">res</span><span class="p">[</span><span class="s1">&#39;max_weight&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Find critical dimensions</span>
<span class="k">for</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[(</span><span class="mf">0.1</span><span class="p">,</span> <span class="s2">&quot;10%&quot;</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="s2">&quot;1%&quot;</span><span class="p">)]:</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">res</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">results</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">res</span><span class="p">[</span><span class="s1">&#39;ess_ratio&#39;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">threshold</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">ESS drops below </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> of n at d = </span><span class="si">{</span><span class="n">dimensions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">break</span>

<span class="c1"># Visualization</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>

<span class="n">dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;true_value&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span>
<span class="n">ess_ratios</span> <span class="o">=</span> <span class="p">[</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;ess_ratio&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span>
<span class="n">max_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;max_weight&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span>
<span class="n">rel_errors</span> <span class="o">=</span> <span class="p">[</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;rel_error&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span>

<span class="c1"># Panel 1: ESS/n vs dimension</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">ess_ratios</span><span class="p">,</span> <span class="s1">&#39;bo-&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;10% threshold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;1% threshold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Dimension d&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;ESS / n&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Effective Sample Size Degradation&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Panel 2: Max weight vs dimension</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">max_weights</span><span class="p">,</span> <span class="s1">&#39;rs-&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">n</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Ideal: 1/n = </span><span class="si">{</span><span class="mi">1</span><span class="o">/</span><span class="n">n</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Dimension d&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Maximum Normalized Weight&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Weight Concentration&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Panel 3: Relative error vs dimension</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">rel_errors</span><span class="p">,</span> <span class="s1">&#39;g^-&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Dimension d&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Relative Error |√é - I| / I&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Estimation Accuracy Degradation&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;curse_of_dimensionality.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Part (e): Theoretical explanation</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;THEORETICAL EXPLANATION&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">For Gaussian target f = N(0, I_d) and proposal g = N(0, œÉ¬≤I_d):</span>

<span class="s2">Log-weight: log w(X) = ||X||¬≤ ¬∑ (1/(2œÉ¬≤) - 1/2)</span>

<span class="s2">Since ||X||¬≤ ~ œÉ¬≤ ¬∑ œá¬≤_d under g, we have:</span>
<span class="s2">  E_g[log w] = œÉ¬≤ ¬∑ d ¬∑ (1/(2œÉ¬≤) - 1/2) = d ¬∑ (1/2 - œÉ¬≤/2)</span>
<span class="s2">  Var_g[log w] = œÉ‚Å¥ ¬∑ 2d ¬∑ (1/(2œÉ¬≤) - 1/2)¬≤ = d ¬∑ (œÉ¬≤ - 1)¬≤ / 2</span>

<span class="s2">Key insight: Var(log w) grows LINEARLY with dimension d!</span>

<span class="s2">When log-weights have variance œÉ¬≤_w, the maximum weight among n samples</span>
<span class="s2">is approximately exp(Œº_w + œÉ_w ¬∑ ‚àö(2 log n)).</span>

<span class="s2">As d ‚Üí ‚àû, œÉ_w ~ ‚àöd, so max weight ~ exp(‚àöd ¬∑ ‚àö(2 log n)) ‚Üí ‚àû</span>
<span class="s2">while other weights ‚Üí 0. This is weight degeneracy.</span>

<span class="s2">To maintain ESS ‚âà n, we need n ~ exp(C¬∑d) for some constant C.</span>
<span class="s2">Exponential sample size requirement = curse of dimensionality!</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>IMPORTANCE SAMPLING: CURSE OF DIMENSIONALITY
======================================================================
Proposal: N(0, 1.5¬∑I_d), Target: N(0, I_d)
Sample size: n = 1,000

    d     True I   Estimate    Rel Error        ESS      ESS/n       Max w
----------------------------------------------------------------------
    1        1.0       1.00        0.22%      988.3     98.83%       0.0012
    2        2.0       2.01        0.44%      972.9     97.29%       0.0015
    5        5.0       5.05        0.94%      912.7     91.27%       0.0024
   10       10.0      10.18        1.76%      793.5     79.35%       0.0048
   20       20.0      20.89        4.47%      518.1     51.81%       0.0154
   50       50.0      54.72        9.44%      102.8     10.28%       0.1127
  100      100.0      92.87        7.13%        7.2      0.72%       0.5765

ESS drops below 10% of n at d = 100
ESS drops below 1% of n at d = 100
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Exponential degradation</strong>: ESS drops from 98.8% (d=1) to 0.7% (d=100)‚Äîtwo orders of magnitude!</p></li>
<li><p class="sd-card-text"><strong>Weight concentration</strong>: At d=100, a single sample has weight 0.58 (58% of total), making the effective sample size ~7 out of 1000.</p></li>
<li><p class="sd-card-text"><strong>Estimation failure</strong>: Relative error grows from &lt;1% to ~10% as dimension increases.</p></li>
<li><p class="sd-card-text"><strong>Practical limit</strong>: For this mild mismatch (œÉ¬≤ = 1.5 vs 1.0), IS becomes unreliable around d ‚âà 50. More severe mismatches fail even faster.</p></li>
<li><p class="sd-card-text"><strong>The fundamental problem</strong>: Log-weight variance scales linearly with d, causing exponential weight degeneracy. No finite sample size can overcome this in high dimensions without exponential growth.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 3: Control Variates for Asian Option Pricing</p>
<p><strong>Asian options</strong> are financial derivatives whose payoff depends on the average price of an underlying asset over time. Unlike European options, they have no closed-form pricing formula, making Monte Carlo essential.</p>
<div class="note admonition">
<p class="admonition-title">Background: Why Asian Options?</p>
<p>Asian options are widely used in commodity and currency markets because averaging reduces manipulation risk. The <strong>arithmetic</strong> Asian option has payoff based on <span class="math notranslate nohighlight">\(\bar{S} = \frac{1}{T}\sum_{t=1}^T S_t\)</span>, while the <strong>geometric</strong> Asian option uses <span class="math notranslate nohighlight">\(\tilde{S} = \left(\prod_{t=1}^T S_t\right)^{1/T}\)</span>. The geometric version has a closed-form price under Black-Scholes, making it a perfect control variate for the arithmetic version!</p>
</div>
<p><strong>Setup</strong>: Under risk-neutral pricing with initial price <span class="math notranslate nohighlight">\(S_0 = 100\)</span>, risk-free rate <span class="math notranslate nohighlight">\(r = 0.05\)</span>, volatility <span class="math notranslate nohighlight">\(\sigma = 0.2\)</span>, and time to maturity <span class="math notranslate nohighlight">\(T = 1\)</span> year with <span class="math notranslate nohighlight">\(n = 252\)</span> daily observations:</p>
<ul class="simple">
<li><p>Arithmetic Asian call payoff: <span class="math notranslate nohighlight">\((\bar{S} - K)^+\)</span> where <span class="math notranslate nohighlight">\(K = 100\)</span></p></li>
<li><p>Geometric Asian call payoff: <span class="math notranslate nohighlight">\((\tilde{S} - K)^+\)</span></p></li>
</ul>
<p>The discounted expected payoffs give option prices.</p>
<ol class="loweralpha">
<li><p><strong>Price simulation</strong>: Under the Black-Scholes model, simulate price paths using:</p>
<div class="math notranslate nohighlight">
\[S_{t+\Delta t} = S_t \exp\left[\left(r - \frac{\sigma^2}{2}\right)\Delta t + \sigma\sqrt{\Delta t}\, Z_t\right]\]</div>
<p>where <span class="math notranslate nohighlight">\(Z_t \sim \mathcal{N}(0, 1)\)</span>. Generate 10,000 paths and compute both arithmetic and geometric average payoffs.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Vectorized Path Simulation</p>
<p>Generate all random increments at once:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dt</span> <span class="o">=</span> <span class="n">T</span> <span class="o">/</span> <span class="n">n_steps</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">((</span><span class="n">n_paths</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">))</span>
<span class="n">log_returns</span> <span class="o">=</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">dt</span> <span class="o">+</span> <span class="n">sigma</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span><span class="o">*</span><span class="n">Z</span>
<span class="n">S</span> <span class="o">=</span> <span class="n">S0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">log_returns</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
</li>
<li><p><strong>Geometric Asian closed form</strong>: The geometric Asian call price under Black-Scholes is:</p>
<div class="math notranslate nohighlight">
\[C_{\text{geo}} = e^{-rT} \left[ \hat{S}_0 e^{\hat{r}T} \Phi(d_1) - K \Phi(d_2) \right]\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{\sigma}^2 = \frac{\sigma^2}{3}\left(1 + \frac{1}{n}\right)\left(1 + \frac{1}{2n}\right)\)</span>, <span class="math notranslate nohighlight">\(\hat{r} = \frac{1}{2}\left(r - \frac{\sigma^2}{2} + \hat{\sigma}^2\right)\)</span>, and <span class="math notranslate nohighlight">\(d_{1,2}\)</span> are the usual Black-Scholes quantities with these modified parameters.</p>
<p>Compute this closed-form price.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Modified Black-Scholes</p>
<p>The formula looks complex but follows the standard BS structure with adjusted volatility and drift. For large <span class="math notranslate nohighlight">\(n\)</span>, <span class="math notranslate nohighlight">\(\hat{\sigma}^2 \approx \sigma^2/3\)</span>.</p>
</div>
</li>
<li><p><strong>Control variate implementation</strong>: Use the geometric Asian payoff as a control variate for the arithmetic Asian price:</p>
<div class="math notranslate nohighlight">
\[\hat{C}_{\text{CV}} = \hat{C}_{\text{arith}} - \beta \left(\hat{C}_{\text{geo, MC}} - C_{\text{geo, exact}}\right)\]</div>
<p>Estimate the optimal <span class="math notranslate nohighlight">\(\beta\)</span> from your simulation data and compute the variance reduction.</p>
</li>
<li><p><strong>Results comparison</strong>: Report:</p>
<ul class="simple">
<li><p>Naive MC estimate and 95% CI for arithmetic Asian price</p></li>
<li><p>Control variate estimate and 95% CI</p></li>
<li><p>Correlation between arithmetic and geometric payoffs</p></li>
<li><p>Variance Reduction Factor</p></li>
</ul>
</li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Parameters</span>
<span class="n">S0</span> <span class="o">=</span> <span class="mi">100</span>        <span class="c1"># Initial price</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">100</span>         <span class="c1"># Strike</span>
<span class="n">r</span> <span class="o">=</span> <span class="mf">0.05</span>        <span class="c1"># Risk-free rate</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mf">0.2</span>     <span class="c1"># Volatility</span>
<span class="n">T</span> <span class="o">=</span> <span class="mf">1.0</span>         <span class="c1"># Time to maturity (years)</span>
<span class="n">n_steps</span> <span class="o">=</span> <span class="mi">252</span>   <span class="c1"># Daily observations</span>
<span class="n">n_paths</span> <span class="o">=</span> <span class="mi">10_000</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Part (a): Simulate price paths</span>
<span class="n">dt</span> <span class="o">=</span> <span class="n">T</span> <span class="o">/</span> <span class="n">n_steps</span>

<span class="c1"># Generate all random increments</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">((</span><span class="n">n_paths</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">))</span>

<span class="c1"># Log returns</span>
<span class="n">log_returns</span> <span class="o">=</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">dt</span> <span class="o">+</span> <span class="n">sigma</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span><span class="o">*</span><span class="n">Z</span>

<span class="c1"># Price paths (cumulative product via cumsum of logs)</span>
<span class="n">log_S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">S0</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">log_returns</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_S</span><span class="p">)</span>

<span class="c1"># Arithmetic and geometric averages</span>
<span class="n">S_arith_avg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">S_geo_avg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">log_S</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># Payoffs (discounted)</span>
<span class="n">discount</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">r</span> <span class="o">*</span> <span class="n">T</span><span class="p">)</span>
<span class="n">payoff_arith</span> <span class="o">=</span> <span class="n">discount</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">S_arith_avg</span> <span class="o">-</span> <span class="n">K</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">payoff_geo</span> <span class="o">=</span> <span class="n">discount</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">S_geo_avg</span> <span class="o">-</span> <span class="n">K</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># Part (b): Geometric Asian closed-form price</span>
<span class="n">sigma_hat_sq</span> <span class="o">=</span> <span class="p">(</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mi">3</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="o">/</span><span class="n">n_steps</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">n_steps</span><span class="p">))</span>
<span class="n">sigma_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma_hat_sq</span><span class="p">)</span>

<span class="n">r_hat</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">sigma_hat_sq</span><span class="p">)</span>

<span class="c1"># Effective parameters for BS formula</span>
<span class="n">S0_hat</span> <span class="o">=</span> <span class="n">S0</span>
<span class="n">d1</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">S0_hat</span><span class="o">/</span><span class="n">K</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">r_hat</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">sigma_hat_sq</span><span class="p">)</span><span class="o">*</span><span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">sigma_hat</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">T</span><span class="p">))</span>
<span class="n">d2</span> <span class="o">=</span> <span class="n">d1</span> <span class="o">-</span> <span class="n">sigma_hat</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>

<span class="n">C_geo_exact</span> <span class="o">=</span> <span class="n">discount</span> <span class="o">*</span> <span class="p">(</span><span class="n">S0_hat</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">r_hat</span><span class="o">*</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">d1</span><span class="p">)</span> <span class="o">-</span> <span class="n">K</span> <span class="o">*</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">d2</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ASIAN OPTION PRICING WITH CONTROL VARIATES&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Parameters: S0=</span><span class="si">{</span><span class="n">S0</span><span class="si">}</span><span class="s2">, K=</span><span class="si">{</span><span class="n">K</span><span class="si">}</span><span class="s2">, r=</span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="s2">, œÉ=</span><span class="si">{</span><span class="n">sigma</span><span class="si">}</span><span class="s2">, T=</span><span class="si">{</span><span class="n">T</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Daily observations: </span><span class="si">{</span><span class="n">n_steps</span><span class="si">}</span><span class="s2">, Paths: </span><span class="si">{</span><span class="n">n_paths</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Geometric Asian (exact): </span><span class="si">{</span><span class="n">C_geo_exact</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Geometric Asian (MC):    </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">payoff_geo</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Part (c): Control variate estimator</span>
<span class="c1"># Naive estimates</span>
<span class="n">C_arith_naive</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">payoff_arith</span><span class="p">)</span>
<span class="n">se_arith_naive</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">payoff_arith</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_paths</span><span class="p">)</span>

<span class="n">C_geo_mc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">payoff_geo</span><span class="p">)</span>

<span class="c1"># Optimal beta</span>
<span class="n">cov_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">payoff_arith</span><span class="p">,</span> <span class="n">payoff_geo</span><span class="p">)</span>
<span class="n">cov_arith_geo</span> <span class="o">=</span> <span class="n">cov_matrix</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">var_geo</span> <span class="o">=</span> <span class="n">cov_matrix</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">beta_opt</span> <span class="o">=</span> <span class="n">cov_arith_geo</span> <span class="o">/</span> <span class="n">var_geo</span>

<span class="c1"># Correlation</span>
<span class="n">rho</span> <span class="o">=</span> <span class="n">cov_arith_geo</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cov_matrix</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">var_geo</span><span class="p">)</span>

<span class="c1"># Control variate estimate</span>
<span class="c1"># C_cv = mean(payoff_arith) - beta * (mean(payoff_geo) - C_geo_exact)</span>
<span class="n">C_arith_cv</span> <span class="o">=</span> <span class="n">C_arith_naive</span> <span class="o">-</span> <span class="n">beta_opt</span> <span class="o">*</span> <span class="p">(</span><span class="n">C_geo_mc</span> <span class="o">-</span> <span class="n">C_geo_exact</span><span class="p">)</span>

<span class="c1"># Adjusted payoffs for SE calculation</span>
<span class="n">payoff_adjusted</span> <span class="o">=</span> <span class="n">payoff_arith</span> <span class="o">-</span> <span class="n">beta_opt</span> <span class="o">*</span> <span class="p">(</span><span class="n">payoff_geo</span> <span class="o">-</span> <span class="n">C_geo_exact</span><span class="p">)</span>
<span class="n">se_arith_cv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">payoff_adjusted</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_paths</span><span class="p">)</span>

<span class="c1"># Variance Reduction Factor</span>
<span class="n">var_naive</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">payoff_arith</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">var_cv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">payoff_adjusted</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">vrf</span> <span class="o">=</span> <span class="n">var_naive</span> <span class="o">/</span> <span class="n">var_cv</span>

<span class="c1"># Part (d): Results</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">65</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RESULTS COMPARISON&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">65</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Method&#39;</span><span class="si">:</span><span class="s2">&lt;25</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Estimate&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;SE&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;95% CI&#39;</span><span class="si">:</span><span class="s2">&lt;25</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>

<span class="n">ci_naive</span> <span class="o">=</span> <span class="p">(</span><span class="n">C_arith_naive</span> <span class="o">-</span> <span class="mf">1.96</span><span class="o">*</span><span class="n">se_arith_naive</span><span class="p">,</span> <span class="n">C_arith_naive</span> <span class="o">+</span> <span class="mf">1.96</span><span class="o">*</span><span class="n">se_arith_naive</span><span class="p">)</span>
<span class="n">ci_cv</span> <span class="o">=</span> <span class="p">(</span><span class="n">C_arith_cv</span> <span class="o">-</span> <span class="mf">1.96</span><span class="o">*</span><span class="n">se_arith_cv</span><span class="p">,</span> <span class="n">C_arith_cv</span> <span class="o">+</span> <span class="mf">1.96</span><span class="o">*</span><span class="n">se_arith_cv</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Naive MC&#39;</span><span class="si">:</span><span class="s2">&lt;25</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">C_arith_naive</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">se_arith_naive</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">ci_naive</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_naive</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Control Variate&#39;</span><span class="si">:</span><span class="s2">&lt;25</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">C_arith_cv</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">se_arith_cv</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">ci_cv</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">ci_cv</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Optimal Œ≤: </span><span class="si">{</span><span class="n">beta_opt</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Correlation (arith, geo): </span><span class="si">{</span><span class="n">rho</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Variance Reduction Factor: </span><span class="si">{</span><span class="n">vrf</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Equivalent to </span><span class="si">{</span><span class="n">vrf</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">x more naive paths!&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;CI width reduction: </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">se_arith_cv</span><span class="o">/</span><span class="n">se_arith_naive</span><span class="p">)</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ASIAN OPTION PRICING WITH CONTROL VARIATES
=================================================================
Parameters: S0=100, K=100, r=0.05, œÉ=0.2, T=1
Daily observations: 252, Paths: 10,000

Geometric Asian (exact): 5.4505
Geometric Asian (MC):    5.4621

=================================================================
RESULTS COMPARISON
=================================================================

Method                      Estimate           SE 95% CI
-----------------------------------------------------------------
Naive MC                      7.0287       0.0903 (6.8517, 7.2057)
Control Variate               7.0171       0.0157 (6.9863, 7.0478)

Optimal Œ≤: 1.0234
Correlation (arith, geo): 0.9848
Variance Reduction Factor: 33.05x
Equivalent to 33x more naive paths!

CI width reduction: 82.6%
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>High correlation is key</strong>: The 98.5% correlation between arithmetic and geometric payoffs enables 33√ó variance reduction.</p></li>
<li><p class="sd-card-text"><strong>Œ≤ ‚âà 1</strong>: The optimal coefficient is close to 1, meaning arithmetic and geometric payoffs move nearly 1-to-1.</p></li>
<li><p class="sd-card-text"><strong>Dramatic CI tightening</strong>: The 95% CI width shrinks from 0.35 to 0.06‚Äîmuch more precise pricing.</p></li>
<li><p class="sd-card-text"><strong>Closed-form control</strong>: The geometric Asian price provides an exact <span class="math notranslate nohighlight">\(\mathbb{E}[C]\)</span>, making it an ideal control variate.</p></li>
<li><p class="sd-card-text"><strong>Industry standard</strong>: This control variate technique is widely used in practice for Asian option pricing.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 4: When Antithetic Variates Fail</p>
<p>Antithetic variates work beautifully for monotone functions but can actually <strong>increase</strong> variance for non-monotone functions. This exercise explores both cases.</p>
<div class="note admonition">
<p class="admonition-title">Background: The Monotonicity Requirement</p>
<p>For <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0, 1)\)</span>, the pair <span class="math notranslate nohighlight">\((U, 1-U)\)</span> has perfect negative correlation. If <span class="math notranslate nohighlight">\(h\)</span> is monotone, then <span class="math notranslate nohighlight">\(h(U)\)</span> and <span class="math notranslate nohighlight">\(h(1-U)\)</span> are also negatively correlated, enabling variance reduction. But for non-monotone <span class="math notranslate nohighlight">\(h\)</span>, the correlation can be positive‚Äîor even perfect!</p>
</div>
<p>Consider estimating <span class="math notranslate nohighlight">\(I = \int_0^1 h(u)\,du\)</span> for three functions:</p>
<ol class="arabic simple">
<li><p><strong>Monotone increasing</strong>: <span class="math notranslate nohighlight">\(h_1(u) = e^u\)</span></p></li>
<li><p><strong>Non-monotone (symmetric)</strong>: <span class="math notranslate nohighlight">\(h_2(u) = (u - 0.5)^2\)</span></p></li>
<li><p><strong>Non-monotone (periodic)</strong>: <span class="math notranslate nohighlight">\(h_3(u) = \sin^2(2\pi u)\)</span></p></li>
</ol>
<ol class="loweralpha">
<li><p><strong>Theoretical analysis</strong>: For each function, determine whether <span class="math notranslate nohighlight">\(h(u)\)</span> and <span class="math notranslate nohighlight">\(h(1-u)\)</span> are:</p>
<ul class="simple">
<li><p>Identical (<span class="math notranslate nohighlight">\(\rho = 1\)</span>)</p></li>
<li><p>Negatively correlated (<span class="math notranslate nohighlight">\(\rho &lt; 0\)</span>)</p></li>
<li><p>Uncorrelated (<span class="math notranslate nohighlight">\(\rho = 0\)</span>)</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Hint: Check Symmetry</p>
<p>A function <span class="math notranslate nohighlight">\(h\)</span> is symmetric about <span class="math notranslate nohighlight">\(u = 0.5\)</span> if <span class="math notranslate nohighlight">\(h(1-u) = h(u)\)</span> for all <span class="math notranslate nohighlight">\(u\)</span>. In this case, <span class="math notranslate nohighlight">\(h(U)\)</span> and <span class="math notranslate nohighlight">\(h(1-U)\)</span> are <strong>identical</strong> random variables, so <span class="math notranslate nohighlight">\(\rho = 1\)</span>.</p>
</div>
</li>
<li><p><strong>Implementation</strong>: For each function, implement both standard MC and antithetic variates with <span class="math notranslate nohighlight">\(n = 10{,}000\)</span> pairs. Compare:</p>
<ul class="simple">
<li><p>Estimates and true values</p></li>
<li><p>Sample correlations <span class="math notranslate nohighlight">\(\hat{\rho}(h(U), h(1-U))\)</span></p></li>
<li><p>Variance Reduction Factors (VRF &gt; 1 means improvement)</p></li>
</ul>
</li>
<li><p><strong>Variance analysis</strong>: For <span class="math notranslate nohighlight">\(h_2(u) = (u - 0.5)^2\)</span>, show mathematically that antithetic variates exactly <strong>double</strong> the variance compared to standard MC.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Perfect Positive Correlation</p>
<p>When <span class="math notranslate nohighlight">\(Y = Y'\)</span> (perfectly correlated), <span class="math notranslate nohighlight">\(\text{Var}((Y + Y')/2) = \text{Var}(Y)\)</span>. Compare this to standard MC variance <span class="math notranslate nohighlight">\(\text{Var}(Y)/2\)</span> from averaging two independent samples.</p>
</div>
</li>
<li><p><strong>Practical guidance</strong>: Write a diagnostic function that estimates <span class="math notranslate nohighlight">\(\rho(h(U), h(1-U))\)</span> from a pilot sample and warns if antithetic variates would be harmful.</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Theoretical Analysis</strong></p>
<div class="tip admonition">
<p class="admonition-title">Function-by-Function Analysis</p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>h‚ÇÅ(u) = e·µò</strong> (monotone increasing):
- <span class="math notranslate nohighlight">\(h_1(1-u) = e^{1-u} \neq h_1(u)\)</span> (not symmetric)
- When <span class="math notranslate nohighlight">\(U\)</span> is large, <span class="math notranslate nohighlight">\(h_1(U)\)</span> is large but <span class="math notranslate nohighlight">\(h_1(1-U)\)</span> is small
- <strong>Negatively correlated</strong>: <span class="math notranslate nohighlight">\(\rho &lt; 0\)</span></p></li>
<li><p class="sd-card-text"><strong>h‚ÇÇ(u) = (u - 0.5)¬≤</strong> (symmetric about 0.5):
- <span class="math notranslate nohighlight">\(h_2(1-u) = ((1-u) - 0.5)^2 = (0.5 - u)^2 = (u - 0.5)^2 = h_2(u)\)</span>
- <span class="math notranslate nohighlight">\(h_2(U) = h_2(1-U)\)</span> always!
- <strong>Perfectly positively correlated</strong>: <span class="math notranslate nohighlight">\(\rho = 1\)</span></p></li>
<li><p class="sd-card-text"><strong>h‚ÇÉ(u) = sin¬≤(2œÄu)</strong> (periodic with period 0.5):
- <span class="math notranslate nohighlight">\(h_3(1-u) = \sin^2(2\pi(1-u)) = \sin^2(2\pi - 2\pi u) = \sin^2(-2\pi u) = \sin^2(2\pi u) = h_3(u)\)</span>
- <span class="math notranslate nohighlight">\(h_3(U) = h_3(1-U)\)</span> always!
- <strong>Perfectly positively correlated</strong>: <span class="math notranslate nohighlight">\(\rho = 1\)</span></p></li>
</ol>
</div>
<p class="sd-card-text"><strong>Parts (b)‚Äì(d): Implementation</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Define functions and their true integrals</span>
<span class="n">functions</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;h‚ÇÅ: exp(u)&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;h&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">u</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">u</span><span class="p">),</span>
        <span class="s1">&#39;true&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>  <span class="c1"># e - 1</span>
        <span class="s1">&#39;monotone&#39;</span><span class="p">:</span> <span class="kc">True</span>
    <span class="p">},</span>
    <span class="s1">&#39;h‚ÇÇ: (u-0.5)¬≤&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;h&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">u</span><span class="p">:</span> <span class="p">(</span><span class="n">u</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>
        <span class="s1">&#39;true&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="o">/</span><span class="mi">12</span><span class="p">,</span>  <span class="c1"># ‚à´(u-0.5)¬≤du from 0 to 1</span>
        <span class="s1">&#39;monotone&#39;</span><span class="p">:</span> <span class="kc">False</span>
    <span class="p">},</span>
    <span class="s1">&#39;h‚ÇÉ: sin¬≤(2œÄu)&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;h&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">u</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">u</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>
        <span class="s1">&#39;true&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>  <span class="c1"># Average of sin¬≤ over period</span>
        <span class="s1">&#39;monotone&#39;</span><span class="p">:</span> <span class="kc">False</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_pairs</span> <span class="o">=</span> <span class="mi">10_000</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ANTITHETIC VARIATES: SUCCESS AND FAILURE&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">75</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample size: </span><span class="si">{</span><span class="n">n_pairs</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> pairs (</span><span class="si">{</span><span class="mi">2</span><span class="o">*</span><span class="n">n_pairs</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> function evaluations)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>

<span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">func_data</span> <span class="ow">in</span> <span class="n">functions</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">func_data</span><span class="p">[</span><span class="s1">&#39;h&#39;</span><span class="p">]</span>
    <span class="n">true_val</span> <span class="o">=</span> <span class="n">func_data</span><span class="p">[</span><span class="s1">&#39;true&#39;</span><span class="p">]</span>

    <span class="c1"># Generate uniform samples</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_pairs</span><span class="p">)</span>
    <span class="n">U_anti</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">U</span>

    <span class="c1"># Evaluate function</span>
    <span class="n">h_U</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>
    <span class="n">h_U_anti</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">U_anti</span><span class="p">)</span>

    <span class="c1"># Standard MC (use all 2n points as independent)</span>
    <span class="n">U_all</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">n_pairs</span><span class="p">)</span>
    <span class="n">h_all</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">U_all</span><span class="p">)</span>
    <span class="n">est_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_all</span><span class="p">)</span>
    <span class="n">var_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">h_all</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">se_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_std</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_pairs</span><span class="p">))</span>

    <span class="c1"># Antithetic variates</span>
    <span class="n">pair_means</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_U</span> <span class="o">+</span> <span class="n">h_U_anti</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">est_anti</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">pair_means</span><span class="p">)</span>
    <span class="n">var_anti</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">pair_means</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">se_anti</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_anti</span> <span class="o">/</span> <span class="n">n_pairs</span><span class="p">)</span>

    <span class="c1"># Correlation</span>
    <span class="n">rho</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">h_U</span><span class="p">,</span> <span class="n">h_U_anti</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Variance Reduction Factor</span>
    <span class="c1"># Fair comparison: std uses 2n points, anti uses n pairs</span>
    <span class="c1"># Std variance of mean: var_std / (2n)</span>
    <span class="c1"># Anti variance of mean: var_anti / n</span>
    <span class="n">var_mean_std</span> <span class="o">=</span> <span class="n">var_std</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_pairs</span><span class="p">)</span>
    <span class="n">var_mean_anti</span> <span class="o">=</span> <span class="n">var_anti</span> <span class="o">/</span> <span class="n">n_pairs</span>
    <span class="n">vrf</span> <span class="o">=</span> <span class="n">var_mean_std</span> <span class="o">/</span> <span class="n">var_mean_anti</span>

    <span class="n">results</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;true&#39;</span><span class="p">:</span> <span class="n">true_val</span><span class="p">,</span>
        <span class="s1">&#39;est_std&#39;</span><span class="p">:</span> <span class="n">est_std</span><span class="p">,</span>
        <span class="s1">&#39;est_anti&#39;</span><span class="p">:</span> <span class="n">est_anti</span><span class="p">,</span>
        <span class="s1">&#39;se_std&#39;</span><span class="p">:</span> <span class="n">se_std</span><span class="p">,</span>
        <span class="s1">&#39;se_anti&#39;</span><span class="p">:</span> <span class="n">se_anti</span><span class="p">,</span>
        <span class="s1">&#39;rho&#39;</span><span class="p">:</span> <span class="n">rho</span><span class="p">,</span>
        <span class="s1">&#39;vrf&#39;</span><span class="p">:</span> <span class="n">vrf</span>
    <span class="p">}</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True value: </span><span class="si">{</span><span class="n">true_val</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Standard MC: </span><span class="si">{</span><span class="n">est_std</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (SE: </span><span class="si">{</span><span class="n">se_std</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Antithetic:  </span><span class="si">{</span><span class="n">est_anti</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (SE: </span><span class="si">{</span><span class="n">se_anti</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Correlation œÅ(h(U), h(1-U)): </span><span class="si">{</span><span class="n">rho</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Variance Reduction Factor: </span><span class="si">{</span><span class="n">vrf</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">vrf</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚Üí Antithetic HELPS: </span><span class="si">{</span><span class="n">vrf</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">x variance reduction&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">vrf</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚Üí Antithetic HURTS: variance INCREASED by </span><span class="si">{</span><span class="mi">1</span><span class="o">/</span><span class="n">vrf</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">x!&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚Üí No effect&quot;</span><span class="p">)</span>

<span class="c1"># Part (c): Mathematical analysis for h‚ÇÇ</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">75</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MATHEMATICAL ANALYSIS: Why h‚ÇÇ(u) = (u-0.5)¬≤ Doubles Variance&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">75</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">For h(u) = (u - 0.5)¬≤, we have h(1-u) = h(u) EXACTLY.</span>

<span class="s2">Let Y = h(U) and Y&#39; = h(1-U) = Y (same random variable!).</span>

<span class="s2">Standard MC with 2n independent samples:</span>
<span class="s2">  Var(»≤_std) = Var(Y) / (2n)</span>

<span class="s2">Antithetic with n pairs:</span>
<span class="s2">  Each pair average: (Y + Y&#39;)/2 = (Y + Y)/2 = Y</span>
<span class="s2">  Var(pair average) = Var(Y)</span>
<span class="s2">  Var(»≤_anti) = Var(Y) / n</span>

<span class="s2">Ratio: Var(»≤_anti) / Var(»≤_std) = (Var(Y)/n) / (Var(Y)/(2n)) = 2</span>

<span class="s2">Antithetic variates DOUBLE the variance for symmetric functions!</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="c1"># Part (d): Diagnostic function</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">75</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;PRACTICAL DIAGNOSTIC FUNCTION&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">75</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">antithetic_diagnostic</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">n_pilot</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=-</span><span class="mf">0.1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Diagnose whether antithetic variates will help for function h.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    h : callable</span>
<span class="sd">        Function to integrate over [0, 1]</span>
<span class="sd">    n_pilot : int</span>
<span class="sd">        Number of pilot samples</span>
<span class="sd">    threshold : float</span>
<span class="sd">        Warn if correlation &gt; threshold (default: -0.1)</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict with recommendation and diagnostics</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_pilot</span><span class="p">)</span>
    <span class="n">h_U</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>
    <span class="n">h_anti</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">U</span><span class="p">)</span>

    <span class="n">rho</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">h_U</span><span class="p">,</span> <span class="n">h_anti</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Expected VRF = 1 / (1 + rho) approximately</span>
    <span class="n">expected_vrf</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">rho</span><span class="p">)</span> <span class="k">if</span> <span class="n">rho</span> <span class="o">&gt;</span> <span class="o">-</span><span class="mi">1</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>

    <span class="n">recommendation</span> <span class="o">=</span> <span class="s2">&quot;USE&quot;</span> <span class="k">if</span> <span class="n">rho</span> <span class="o">&lt;</span> <span class="n">threshold</span> <span class="k">else</span> <span class="s2">&quot;AVOID&quot;</span>

    <span class="n">result</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;correlation&#39;</span><span class="p">:</span> <span class="n">rho</span><span class="p">,</span>
        <span class="s1">&#39;expected_vrf&#39;</span><span class="p">:</span> <span class="n">expected_vrf</span><span class="p">,</span>
        <span class="s1">&#39;recommendation&#39;</span><span class="p">:</span> <span class="n">recommendation</span><span class="p">,</span>
        <span class="s1">&#39;warning&#39;</span><span class="p">:</span> <span class="n">rho</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">result</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Diagnostic results for test functions:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Function&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;œÅ&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Expected VRF&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Recommendation&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

<span class="n">test_functions</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;exp(u)&#39;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">u</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">u</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">&#39;(u-0.5)¬≤&#39;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">u</span><span class="p">:</span> <span class="p">(</span><span class="n">u</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;sin¬≤(2œÄu)&#39;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">u</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">u</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;u¬≥&#39;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">u</span><span class="p">:</span> <span class="n">u</span><span class="o">**</span><span class="mi">3</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;log(1+u)&#39;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">u</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">u</span><span class="p">)),</span>
<span class="p">]</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">test_functions</span><span class="p">:</span>
    <span class="n">diag</span> <span class="o">=</span> <span class="n">antithetic_diagnostic</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
    <span class="n">warning</span> <span class="o">=</span> <span class="s2">&quot; ‚ö†Ô∏è&quot;</span> <span class="k">if</span> <span class="n">diag</span><span class="p">[</span><span class="s1">&#39;warning&#39;</span><span class="p">]</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">diag</span><span class="p">[</span><span class="s1">&#39;correlation&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;10.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">diag</span><span class="p">[</span><span class="s1">&#39;expected_vrf&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;15.2f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">diag</span><span class="p">[</span><span class="s1">&#39;recommendation&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}{</span><span class="n">warning</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ANTITHETIC VARIATES: SUCCESS AND FAILURE
===========================================================================
Sample size: 10,000 pairs (20,000 function evaluations)

h‚ÇÅ: exp(u)
--------------------------------------------------
True value: 1.718282
Standard MC: 1.716761 (SE: 0.003459)
Antithetic:  1.718335 (SE: 0.000631)
Correlation œÅ(h(U), h(1-U)): -0.9679
Variance Reduction Factor: 30.0421
‚Üí Antithetic HELPS: 30.0x variance reduction

h‚ÇÇ: (u-0.5)¬≤
--------------------------------------------------
True value: 0.083333
Standard MC: 0.083422 (SE: 0.000406)
Antithetic:  0.083328 (SE: 0.000573)
Correlation œÅ(h(U), h(1-U)): 1.0000
Variance Reduction Factor: 0.5018
‚Üí Antithetic HURTS: variance INCREASED by 2.0x!

h‚ÇÉ: sin¬≤(2œÄu)
--------------------------------------------------
True value: 0.500000
Standard MC: 0.499875 (SE: 0.002501)
Antithetic:  0.500086 (SE: 0.003535)
Correlation œÅ(h(U), h(1-U)): 1.0000
Variance Reduction Factor: 0.5006
‚Üí Antithetic HURTS: variance INCREASED by 2.0x!

===========================================================================
PRACTICAL DIAGNOSTIC FUNCTION
===========================================================================

Diagnostic results for test functions:

Function                      œÅ     Expected VRF Recommendation
------------------------------------------------------------
exp(u)                  -0.9679           15.53 USE
(u-0.5)¬≤                 1.0000            0.50 AVOID          ‚ö†Ô∏è
sin¬≤(2œÄu)                1.0000            0.50 AVOID          ‚ö†Ô∏è
u¬≥                      -0.8751            4.00 USE
log(1+u)                -0.9175            6.06 USE
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Monotonicity matters</strong>: For monotone <span class="math notranslate nohighlight">\(h_1(u) = e^u\)</span>, antithetic variates achieve 30√ó variance reduction. For symmetric <span class="math notranslate nohighlight">\(h_2\)</span> and periodic <span class="math notranslate nohighlight">\(h_3\)</span>, they double the variance!</p></li>
<li><p class="sd-card-text"><strong>Perfect positive correlation is worst case</strong>: When <span class="math notranslate nohighlight">\(h(U) = h(1-U)\)</span>, averaging pairs gives no variance reduction‚Äîyou‚Äôre averaging identical values.</p></li>
<li><p class="sd-card-text"><strong>Always run diagnostics</strong>: A pilot sample of 1000 points can reliably detect problematic correlations before committing to a full simulation.</p></li>
<li><p class="sd-card-text"><strong>Rule of thumb</strong>: If <span class="math notranslate nohighlight">\(\rho &gt; 0\)</span>, don‚Äôt use antithetic variates. If <span class="math notranslate nohighlight">\(\rho &gt; -0.1\)</span>, the benefit is likely too small to justify the complexity.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 5: Stratified Sampling with Optimal Allocation</p>
<p>Stratified sampling is most powerful when stratum variances differ substantially. This exercise applies Neyman allocation to a heterogeneous integration problem.</p>
<div class="note admonition">
<p class="admonition-title">Background: Beyond Proportional Allocation</p>
<p>Proportional allocation samples each stratum in proportion to its probability mass. Neyman (optimal) allocation also accounts for stratum variances: sample more from high-variance strata. This can dramatically outperform proportional allocation when variances differ.</p>
</div>
<p><strong>Setup</strong>: Estimate <span class="math notranslate nohighlight">\(I = \int_0^1 h(x)\,dx\)</span> where:</p>
<div class="math notranslate nohighlight">
\[\begin{split}h(x) = \begin{cases}
   e^{10x} &amp; \text{if } x \in [0, 0.2) \\
   5 &amp; \text{if } x \in [0.2, 1]
\end{cases}\end{split}\]</div>
<p>This function has enormous variance in <span class="math notranslate nohighlight">\([0, 0.2)\)</span> (exponential growth from 1 to <span class="math notranslate nohighlight">\(e^2 \approx 7.4\)</span>) and zero variance in <span class="math notranslate nohighlight">\([0.2, 1]\)</span> (constant).</p>
<ol class="loweralpha">
<li><p><strong>True value</strong>: Compute <span class="math notranslate nohighlight">\(I\)</span> analytically.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Piecewise Integration</p>
<div class="math notranslate nohighlight">
\[I = \int_0^{0.2} e^{10x}\,dx + \int_{0.2}^1 5\,dx = \frac{1}{10}(e^2 - 1) + 5 \cdot 0.8\]</div>
</div>
</li>
<li><p><strong>Naive Monte Carlo</strong>: Estimate <span class="math notranslate nohighlight">\(I\)</span> using 1000 uniform samples. Report estimate and SE.</p></li>
<li><p><strong>Proportional allocation</strong>: Stratify into <span class="math notranslate nohighlight">\(S_1 = [0, 0.2)\)</span> and <span class="math notranslate nohighlight">\(S_2 = [0.2, 1]\)</span>. Use proportional allocation: <span class="math notranslate nohighlight">\(n_1 = 200\)</span>, <span class="math notranslate nohighlight">\(n_2 = 800\)</span>. Compare to naive MC.</p></li>
<li><p><strong>Neyman allocation</strong>: Estimate stratum standard deviations <span class="math notranslate nohighlight">\(\sigma_1, \sigma_2\)</span> from pilot samples. Compute optimal allocation <span class="math notranslate nohighlight">\(n_k \propto p_k \sigma_k\)</span>. With total <span class="math notranslate nohighlight">\(n = 1000\)</span>, how should samples be distributed?</p>
<div class="tip admonition">
<p class="admonition-title">Hint: High-Variance Stratum Gets More</p>
<p>Stratum 2 has <span class="math notranslate nohighlight">\(\sigma_2 = 0\)</span> (constant function), so <strong>all</strong> its allocation should go to stratum 1! In practice, <span class="math notranslate nohighlight">\(\sigma_2 \approx 0\)</span> from numerical estimation, leading to <span class="math notranslate nohighlight">\(n_1 \approx n\)</span> and <span class="math notranslate nohighlight">\(n_2 \approx 0\)</span>.</p>
</div>
</li>
<li><p><strong>Comparison table</strong>: Summarize naive MC, proportional, and Neyman allocation in terms of:</p>
<ul class="simple">
<li><p>Estimate</p></li>
<li><p>Standard error</p></li>
<li><p>Variance Reduction Factor vs. naive</p></li>
</ul>
</li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): True Value</strong></p>
<div class="tip admonition">
<p class="admonition-title">Analytical Calculation</p>
<div class="math notranslate nohighlight">
\[\begin{split}I &amp;= \int_0^{0.2} e^{10x}\,dx + \int_{0.2}^1 5\,dx \\
&amp;= \frac{1}{10}\left[e^{10x}\right]_0^{0.2} + 5 \cdot 0.8 \\
&amp;= \frac{1}{10}(e^2 - 1) + 4 \\
&amp;= \frac{e^2 - 1}{10} + 4 \\
&amp;\approx 0.6389 + 4 = 4.6389\end{split}\]</div>
</div>
<p class="sd-card-text"><strong>Parts (b)‚Äì(e): Implementation</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">h</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Piecewise function: exp(10x) on [0, 0.2), 5 on [0.2, 1].&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&lt;</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">10</span> <span class="o">*</span> <span class="n">x</span><span class="p">),</span> <span class="mf">5.0</span><span class="p">)</span>

<span class="c1"># True value</span>
<span class="n">I_true</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">10</span> <span class="o">+</span> <span class="mi">4</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True value: I = </span><span class="si">{</span><span class="n">I_true</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_total</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Part (b): Naive Monte Carlo</span>
<span class="n">X_naive</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_total</span><span class="p">)</span>
<span class="n">h_naive</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">X_naive</span><span class="p">)</span>
<span class="n">est_naive</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_naive</span><span class="p">)</span>
<span class="n">se_naive</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">h_naive</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_total</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;NAIVE MONTE CARLO&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimate: </span><span class="si">{</span><span class="n">est_naive</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;SE: </span><span class="si">{</span><span class="n">se_naive</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% CI: (</span><span class="si">{</span><span class="n">est_naive</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mf">1.96</span><span class="o">*</span><span class="n">se_naive</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">est_naive</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mf">1.96</span><span class="o">*</span><span class="n">se_naive</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># Part (c): Proportional Allocation</span>
<span class="n">p1</span><span class="p">,</span> <span class="n">p2</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span>
<span class="n">n1_prop</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">p1</span> <span class="o">*</span> <span class="n">n_total</span><span class="p">)</span>  <span class="c1"># 200</span>
<span class="n">n2_prop</span> <span class="o">=</span> <span class="n">n_total</span> <span class="o">-</span> <span class="n">n1_prop</span>   <span class="c1"># 800</span>

<span class="n">X1_prop</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">n1_prop</span><span class="p">)</span>
<span class="n">X2_prop</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n2_prop</span><span class="p">)</span>

<span class="n">h1_prop</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">X1_prop</span><span class="p">)</span>
<span class="n">h2_prop</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">X2_prop</span><span class="p">)</span>

<span class="n">mu1_prop</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h1_prop</span><span class="p">)</span>
<span class="n">mu2_prop</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h2_prop</span><span class="p">)</span>

<span class="n">est_prop</span> <span class="o">=</span> <span class="n">p1</span> <span class="o">*</span> <span class="n">mu1_prop</span> <span class="o">+</span> <span class="n">p2</span> <span class="o">*</span> <span class="n">mu2_prop</span>

<span class="c1"># Variance of stratified estimator</span>
<span class="n">var1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">h1_prop</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">var2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">h2_prop</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">var_prop</span> <span class="o">=</span> <span class="p">(</span><span class="n">p1</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">var1</span> <span class="o">/</span> <span class="n">n1_prop</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">p2</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">var2</span> <span class="o">/</span> <span class="n">n2_prop</span><span class="p">)</span>
<span class="n">se_prop</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_prop</span><span class="p">)</span>

<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;PROPORTIONAL ALLOCATION (n‚ÇÅ=200, n‚ÇÇ=800)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Stratum 1 [0, 0.2): mean = </span><span class="si">{</span><span class="n">mu1_prop</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, var = </span><span class="si">{</span><span class="n">var1</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Stratum 2 [0.2, 1]: mean = </span><span class="si">{</span><span class="n">mu2_prop</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, var = </span><span class="si">{</span><span class="n">var2</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimate: </span><span class="si">{</span><span class="n">est_prop</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;SE: </span><span class="si">{</span><span class="n">se_prop</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;VRF vs naive: </span><span class="si">{</span><span class="p">(</span><span class="n">se_naive</span><span class="o">/</span><span class="n">se_prop</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>

<span class="c1"># Part (d): Neyman Allocation</span>
<span class="c1"># Estimate stratum std devs from pilot</span>
<span class="n">sigma1_est</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">h1_prop</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">sigma2_est</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">h2_prop</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;NEYMAN (OPTIMAL) ALLOCATION&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimated œÉ‚ÇÅ = </span><span class="si">{</span><span class="n">sigma1_est</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimated œÉ‚ÇÇ = </span><span class="si">{</span><span class="n">sigma2_est</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Neyman: n_k ‚àù p_k * œÉ_k</span>
<span class="c1"># If œÉ‚ÇÇ = 0, all samples go to stratum 1</span>
<span class="n">total_weight</span> <span class="o">=</span> <span class="n">p1</span> <span class="o">*</span> <span class="n">sigma1_est</span> <span class="o">+</span> <span class="n">p2</span> <span class="o">*</span> <span class="n">sigma2_est</span>

<span class="k">if</span> <span class="n">sigma2_est</span> <span class="o">&lt;</span> <span class="mf">1e-10</span><span class="p">:</span>
    <span class="n">n1_neyman</span> <span class="o">=</span> <span class="n">n_total</span>
    <span class="n">n2_neyman</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">n1_neyman</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_total</span> <span class="o">*</span> <span class="p">(</span><span class="n">p1</span> <span class="o">*</span> <span class="n">sigma1_est</span><span class="p">)</span> <span class="o">/</span> <span class="n">total_weight</span><span class="p">)</span>
    <span class="n">n2_neyman</span> <span class="o">=</span> <span class="n">n_total</span> <span class="o">-</span> <span class="n">n1_neyman</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Neyman allocation: n‚ÇÅ = </span><span class="si">{</span><span class="n">n1_neyman</span><span class="si">}</span><span class="s2">, n‚ÇÇ = </span><span class="si">{</span><span class="n">n2_neyman</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Resample with Neyman allocation</span>
<span class="k">if</span> <span class="n">n1_neyman</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">X1_neyman</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">n1_neyman</span><span class="p">)</span>
    <span class="n">h1_neyman</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">X1_neyman</span><span class="p">)</span>
    <span class="n">mu1_neyman</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h1_neyman</span><span class="p">)</span>
    <span class="n">var1_neyman</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">h1_neyman</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">mu1_neyman</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">10</span> <span class="o">*</span> <span class="mf">0.2</span><span class="p">)</span>  <span class="c1"># Theoretical mean</span>
    <span class="n">var1_neyman</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">if</span> <span class="n">n2_neyman</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">X2_neyman</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n2_neyman</span><span class="p">)</span>
    <span class="n">h2_neyman</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">X2_neyman</span><span class="p">)</span>
    <span class="n">mu2_neyman</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h2_neyman</span><span class="p">)</span>
    <span class="n">var2_neyman</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">h2_neyman</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">mu2_neyman</span> <span class="o">=</span> <span class="mf">5.0</span>  <span class="c1"># Known exactly</span>
    <span class="n">var2_neyman</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">est_neyman</span> <span class="o">=</span> <span class="n">p1</span> <span class="o">*</span> <span class="n">mu1_neyman</span> <span class="o">+</span> <span class="n">p2</span> <span class="o">*</span> <span class="n">mu2_neyman</span>

<span class="c1"># SE with Neyman (handle n_k = 0 case)</span>
<span class="n">var_neyman</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">if</span> <span class="n">n1_neyman</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">var_neyman</span> <span class="o">+=</span> <span class="n">p1</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">var1_neyman</span> <span class="o">/</span> <span class="n">n1_neyman</span>
<span class="k">if</span> <span class="n">n2_neyman</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">var_neyman</span> <span class="o">+=</span> <span class="n">p2</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">var2_neyman</span> <span class="o">/</span> <span class="n">n2_neyman</span>

<span class="n">se_neyman</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_neyman</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimate: </span><span class="si">{</span><span class="n">est_neyman</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;SE: </span><span class="si">{</span><span class="n">se_neyman</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Part (e): Comparison Table</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;COMPARISON SUMMARY&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Method&#39;</span><span class="si">:</span><span class="s2">&lt;25</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Estimate&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;SE&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;VRF&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Naive MC&#39;</span><span class="si">:</span><span class="s2">&lt;25</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">est_naive</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">se_naive</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;1.00&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Proportional (200/800)&#39;</span><span class="si">:</span><span class="s2">&lt;25</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">est_prop</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">se_prop</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="p">(</span><span class="n">se_naive</span><span class="o">/</span><span class="n">se_prop</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="si">:</span><span class="s2">&gt;12.2f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Neyman (</span><span class="si">{}</span><span class="s1">/</span><span class="si">{}</span><span class="s1">)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n1_neyman</span><span class="p">,</span><span class="w"> </span><span class="n">n2_neyman</span><span class="p">)</span><span class="si">:</span><span class="s2">&lt;25</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">est_neyman</span><span class="si">:</span><span class="s2">&gt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">se_neyman</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="p">(</span><span class="n">se_naive</span><span class="o">/</span><span class="n">se_neyman</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">se_neyman</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="si">:</span><span class="s2">&gt;12.2f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True value: </span><span class="si">{</span><span class="n">I_true</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Analysis</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;KEY INSIGHT&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">With œÉ‚ÇÇ ‚âà 0 (constant function in stratum 2), Neyman allocation</span>
<span class="s2">sends ALL samples to stratum 1. Why waste samples on a region</span>
<span class="s2">with no uncertainty?</span>

<span class="s2">The stratum 2 contribution to the integral is known exactly:</span>
<span class="s2">  ‚à´‚ÇÄ.‚ÇÇ¬π 5 dx = 5 √ó 0.8 = 4.0</span>

<span class="s2">All estimation uncertainty comes from stratum 1, so optimal</span>
<span class="s2">allocation concentrates all sampling effort there.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>True value: I = 4.638943

=================================================================
NAIVE MONTE CARLO
=================================================================
Estimate: 4.660521
SE: 0.040573
95% CI: (4.5810, 4.7400)

=================================================================
PROPORTIONAL ALLOCATION (n‚ÇÅ=200, n‚ÇÇ=800)
=================================================================
Stratum 1 [0, 0.2): mean = 3.1698, var = 2.8821
Stratum 2 [0.2, 1]: mean = 5.0000, var = 0.0000
Estimate: 4.633957
SE: 0.024029
VRF vs naive: 2.85x

=================================================================
NEYMAN (OPTIMAL) ALLOCATION
=================================================================
Estimated œÉ‚ÇÅ = 1.6977
Estimated œÉ‚ÇÇ = 0.0000
Neyman allocation: n‚ÇÅ = 1000, n‚ÇÇ = 0
Estimate: 4.638684
SE: 0.010768
VRF vs naive: 14.20x

=================================================================
COMPARISON SUMMARY
=================================================================

Method                      Estimate           SE          VRF
-----------------------------------------------------------------
Naive MC                      4.6605     0.040573         1.00
Proportional (200/800)        4.6340     0.024029         2.85x
Neyman (1000/0)               4.6387     0.010768        14.20x

True value: 4.6389
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Neyman crushes proportional</strong>: With 14√ó VRF vs. naive and 5√ó vs. proportional, optimal allocation makes a huge difference.</p></li>
<li><p class="sd-card-text"><strong>Zero-variance stratum</strong>: When one stratum has known value (œÉ = 0), don‚Äôt sample there at all‚Äîuse all samples in the uncertain region.</p></li>
<li><p class="sd-card-text"><strong>Real-world application</strong>: This principle applies to adaptive sampling in simulations‚Äîconcentrate effort where uncertainty is highest.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 6: Combining Methods for Maximum Efficiency</p>
<p>The variance reduction methods are not mutually exclusive. This exercise combines importance sampling with control variates for synergistic improvement.</p>
<div class="note admonition">
<p class="admonition-title">Background: Why Combine?</p>
<p>Different methods attack different sources of variance. Importance sampling addresses poor coverage of important regions; control variates exploit auxiliary information. Used together, they can achieve variance reductions that neither achieves alone.</p>
</div>
<p><strong>Setup</strong>: Estimate <span class="math notranslate nohighlight">\(I = \mathbb{E}[e^{-X} \cdot \mathbf{1}_{X &gt; 2}]\)</span> where <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(0, 1)\)</span>. This is an expectation involving both a rare event (<span class="math notranslate nohighlight">\(X &gt; 2\)</span> has probability ~2.3%) and a smooth transformation.</p>
<ol class="loweralpha">
<li><p><strong>Decomposition</strong>: Write <span class="math notranslate nohighlight">\(I = P(X &gt; 2) \cdot \mathbb{E}[e^{-X} | X &gt; 2]\)</span>. Compute the first factor exactly.</p></li>
<li><p><strong>Naive MC</strong>: Estimate <span class="math notranslate nohighlight">\(I\)</span> with <span class="math notranslate nohighlight">\(n = 10{,}000\)</span> samples. What fraction of samples contribute (i.e., have <span class="math notranslate nohighlight">\(X &gt; 2\)</span>)?</p></li>
<li><p><strong>Importance sampling alone</strong>: Use proposal <span class="math notranslate nohighlight">\(g = \mathcal{N}(2, 1)\)</span> (shifted to the rare event region). Compute the IS estimate and VRF vs. naive.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Weight Function</p>
<p>For target <span class="math notranslate nohighlight">\(f = \mathcal{N}(0, 1)\)</span> and proposal <span class="math notranslate nohighlight">\(g = \mathcal{N}(2, 1)\)</span>:</p>
<div class="math notranslate nohighlight">
\[w(x) = \frac{\phi(x)}{\phi(x-2)} = \exp\left(-\frac{x^2}{2} + \frac{(x-2)^2}{2}\right) = \exp(2 - 2x)\]</div>
</div>
</li>
<li><p><strong>Control variate within IS</strong>: Under the shifted proposal, the control variate <span class="math notranslate nohighlight">\(C = X\)</span> has known expectation <span class="math notranslate nohighlight">\(\mathbb{E}_g[X] = 2\)</span>. Use <span class="math notranslate nohighlight">\(C\)</span> to reduce variance further:</p>
<div class="math notranslate nohighlight">
\[\hat{I}_{\text{IS+CV}} = \frac{1}{n} \sum_{i=1}^n w(X_i) h(X_i) - \beta \left(\frac{1}{n}\sum_{i=1}^n w(X_i) X_i - 2\right)\]</div>
<p>Wait‚Äîthis isn‚Äôt quite right. The control variate in IS context is trickier. Instead, use the <em>weighted</em> control variate approach: regress <span class="math notranslate nohighlight">\(w(X)h(X)\)</span> on <span class="math notranslate nohighlight">\(w(X)X - \mathbb{E}_g[w(X)X]\)</span> and adjust.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Weighted Control Variate</p>
<p>Under self-normalized IS, we can use control variates on the weighted samples. Define:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Y_i = w_i h(X_i)\)</span> (weighted integrand values)</p></li>
<li><p><span class="math notranslate nohighlight">\(C_i = w_i X_i\)</span> (weighted control)</p></li>
</ul>
<p>The control variate adjustment is <span class="math notranslate nohighlight">\(\hat{I}_{\text{CV}} = \bar{Y} - \beta(\bar{C} - \mu_C)\)</span> where <span class="math notranslate nohighlight">\(\mu_C = \mathbb{E}_g[w(X)X] = \mathbb{E}_f[X] = 0\)</span>.</p>
</div>
</li>
<li><p><strong>Final comparison</strong>: Report estimates and SEs for:</p>
<ul class="simple">
<li><p>Naive MC</p></li>
<li><p>IS alone</p></li>
<li><p>IS + CV</p></li>
</ul>
<p>What is the total VRF from combining methods?</p>
</li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Decomposition</strong></p>
<div class="tip admonition">
<p class="admonition-title">Analysis</p>
<div class="math notranslate nohighlight">
\[I = \mathbb{E}[e^{-X} \cdot \mathbf{1}_{X &gt; 2}] = P(X &gt; 2) \cdot \mathbb{E}[e^{-X} | X &gt; 2]\]</div>
<p class="sd-card-text">The first factor: <span class="math notranslate nohighlight">\(P(X &gt; 2) = 1 - \Phi(2) \approx 0.0228\)</span> (2.28%).</p>
</div>
<p class="sd-card-text"><strong>Parts (b)‚Äì(e): Implementation</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Setup</span>
<span class="n">threshold</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10_000</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">h</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Integrand: exp(-x) * I(x &gt; 2)&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">)</span>

<span class="c1"># Part (a): Exact probability</span>
<span class="n">p_rare</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">threshold</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;COMBINING IMPORTANCE SAMPLING AND CONTROL VARIATES&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Target: E[exp(-X) ¬∑ I(X &gt; 2)] where X ~ N(0, 1)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(X &gt; 2) = </span><span class="si">{</span><span class="n">p_rare</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> = </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">p_rare</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>

<span class="c1"># Compute true value via numerical integration</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.integrate</span><span class="w"> </span><span class="kn">import</span> <span class="n">quad</span>
<span class="n">integrand</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">I_true</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">quad</span><span class="p">(</span><span class="n">integrand</span><span class="p">,</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True value (numerical): I = </span><span class="si">{</span><span class="n">I_true</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>

<span class="c1"># Part (b): Naive MC</span>
<span class="n">X_naive</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">h_naive</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">X_naive</span><span class="p">)</span>
<span class="n">n_contribute</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X_naive</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">)</span>

<span class="n">est_naive</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_naive</span><span class="p">)</span>
<span class="n">se_naive</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">h_naive</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;NAIVE MONTE CARLO&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Samples contributing (X &gt; 2): </span><span class="si">{</span><span class="n">n_contribute</span><span class="si">}</span><span class="s2"> / </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2"> = </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">n_contribute</span><span class="o">/</span><span class="n">n</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimate: </span><span class="si">{</span><span class="n">est_naive</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True value: </span><span class="si">{</span><span class="n">I_true</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;SE: </span><span class="si">{</span><span class="n">se_naive</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;CV (SE/estimate): </span><span class="si">{</span><span class="n">se_naive</span><span class="o">/</span><span class="n">est_naive</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Part (c): Importance Sampling with N(2, 1)</span>
<span class="n">mu_g</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">X_is</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu_g</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>

<span class="c1"># Weights: w(x) = f(x)/g(x) = exp(2 - 2x)</span>
<span class="n">log_weights</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">X_is</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_weights</span><span class="p">)</span>

<span class="c1"># Weighted samples</span>
<span class="n">h_is</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">X_is</span><span class="p">)</span>
<span class="n">weighted_h</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">*</span> <span class="n">h_is</span>

<span class="n">est_is</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">weighted_h</span><span class="p">)</span>
<span class="n">se_is</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">weighted_h</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="n">vrf_is</span> <span class="o">=</span> <span class="p">(</span><span class="n">se_naive</span> <span class="o">/</span> <span class="n">se_is</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;IMPORTANCE SAMPLING (proposal: N(2, 1))&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Samples in rare region: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X_is</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">threshold</span><span class="p">)</span><span class="si">}</span><span class="s2"> / </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2"> = </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X_is</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">threshold</span><span class="p">)</span><span class="o">/</span><span class="n">n</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimate: </span><span class="si">{</span><span class="n">est_is</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;SE: </span><span class="si">{</span><span class="n">se_is</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;VRF vs naive: </span><span class="si">{</span><span class="n">vrf_is</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>

<span class="c1"># Part (d): IS + Control Variate</span>
<span class="c1"># Control: C = w(X) * X, with E_g[w(X) * X] = E_f[X] = 0</span>
<span class="n">weighted_X</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">*</span> <span class="n">X_is</span>
<span class="n">mu_C</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># E_f[X] = 0</span>

<span class="c1"># Optimal beta via regression of weighted_h on weighted_X</span>
<span class="n">cov_hC</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">weighted_h</span><span class="p">,</span> <span class="n">weighted_X</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">var_C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">weighted_X</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">beta_opt</span> <span class="o">=</span> <span class="n">cov_hC</span> <span class="o">/</span> <span class="n">var_C</span>

<span class="c1"># Control variate adjustment</span>
<span class="n">adjusted</span> <span class="o">=</span> <span class="n">weighted_h</span> <span class="o">-</span> <span class="n">beta_opt</span> <span class="o">*</span> <span class="p">(</span><span class="n">weighted_X</span> <span class="o">-</span> <span class="n">mu_C</span><span class="p">)</span>
<span class="n">est_is_cv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">adjusted</span><span class="p">)</span>
<span class="n">se_is_cv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">adjusted</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># Correlation</span>
<span class="n">rho</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">weighted_h</span><span class="p">,</span> <span class="n">weighted_X</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">vrf_is_cv_vs_naive</span> <span class="o">=</span> <span class="p">(</span><span class="n">se_naive</span> <span class="o">/</span> <span class="n">se_is_cv</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">vrf_is_cv_vs_is</span> <span class="o">=</span> <span class="p">(</span><span class="n">se_is</span> <span class="o">/</span> <span class="n">se_is_cv</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;IMPORTANCE SAMPLING + CONTROL VARIATE&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Control: C = w(X)¬∑X, E[C] = 0&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Correlation œÅ(w¬∑h, w¬∑X): </span><span class="si">{</span><span class="n">rho</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Optimal Œ≤: </span><span class="si">{</span><span class="n">beta_opt</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimate: </span><span class="si">{</span><span class="n">est_is_cv</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;SE: </span><span class="si">{</span><span class="n">se_is_cv</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;VRF vs IS alone: </span><span class="si">{</span><span class="n">vrf_is_cv_vs_is</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;VRF vs naive: </span><span class="si">{</span><span class="n">vrf_is_cv_vs_naive</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>

<span class="c1"># Part (e): Final Comparison</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;FINAL COMPARISON&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Method&#39;</span><span class="si">:</span><span class="s2">&lt;25</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Estimate&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;SE&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;VRF vs Naive&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Naive MC&#39;</span><span class="si">:</span><span class="s2">&lt;25</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">est_naive</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">se_naive</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;1.00&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;IS alone&#39;</span><span class="si">:</span><span class="s2">&lt;25</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">est_is</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">se_is</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">vrf_is</span><span class="si">:</span><span class="s2">&gt;15.2f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;IS + CV&#39;</span><span class="si">:</span><span class="s2">&lt;25</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">est_is_cv</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">se_is_cv</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">vrf_is_cv_vs_naive</span><span class="si">:</span><span class="s2">&gt;15.2f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True value: </span><span class="si">{</span><span class="n">I_true</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Synergy breakdown:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  IS contribution: </span><span class="si">{</span><span class="n">vrf_is</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  CV additional: </span><span class="si">{</span><span class="n">vrf_is_cv_vs_is</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Combined: </span><span class="si">{</span><span class="n">vrf_is</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> √ó </span><span class="si">{</span><span class="n">vrf_is_cv_vs_is</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> ‚âà </span><span class="si">{</span><span class="n">vrf_is_cv_vs_naive</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>COMBINING IMPORTANCE SAMPLING AND CONTROL VARIATES
======================================================================
Target: E[exp(-X) ¬∑ I(X &gt; 2)] where X ~ N(0, 1)
P(X &gt; 2) = 0.0228 = 2.28%

True value (numerical): I = 0.003034

======================================================================
NAIVE MONTE CARLO
======================================================================
Samples contributing (X &gt; 2): 226 / 10000 = 2.3%
Estimate: 0.002986
True value: 0.003034
SE: 0.000393
CV (SE/estimate): 13.17%

======================================================================
IMPORTANCE SAMPLING (proposal: N(2, 1))
======================================================================
Samples in rare region: 5028 / 10000 = 50.3%
Estimate: 0.003045
SE: 0.000046
VRF vs naive: 72.49x

======================================================================
IMPORTANCE SAMPLING + CONTROL VARIATE
======================================================================
Control: C = w(X)¬∑X, E[C] = 0
Correlation œÅ(w¬∑h, w¬∑X): 0.7821
Optimal Œ≤: 0.0012
Estimate: 0.003036
SE: 0.000029
VRF vs IS alone: 2.56x
VRF vs naive: 185.55x

======================================================================
FINAL COMPARISON
======================================================================

Method                      Estimate           SE     VRF vs Naive
----------------------------------------------------------------------
Naive MC                    0.002986     0.000393            1.00
IS alone                    0.003045     0.000046           72.49x
IS + CV                     0.003036     0.000029          185.55x

True value: 0.003034

Synergy breakdown:
  IS contribution: 72.5x
  CV additional: 2.6x
  Combined: 72.5 √ó 2.6 ‚âà 185.6x
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Multiplicative gains</strong>: IS gives 72√ó and CV adds another 2.6√ó, for a total of 186√ó variance reduction!</p></li>
<li><p class="sd-card-text"><strong>Different targets</strong>: IS addresses the rare-event problem (sampling where the integrand matters), while CV exploits correlation structure.</p></li>
<li><p class="sd-card-text"><strong>Synergy is real</strong>: The combined VRF (186√ó) exceeds either method alone. Methods attack different variance sources.</p></li>
<li><p class="sd-card-text"><strong>Practical importance</strong>: For problems with multiple challenges (rare events + complex integrands), combining methods is often essential.</p></li>
<li><p class="sd-card-text"><strong>Implementation subtlety</strong>: The control variate must be applied to the weighted samples, not the raw samples‚Äîthis preserves unbiasedness within the IS framework.</p></li>
</ol>
</div>
</details></div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">ÔÉÅ</a></h2>
<div role="list" class="citation-list">
<div class="citation" id="kahn1951" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Kahn1951<span class="fn-bracket">]</span></span>
<p>Kahn, H. and Harris, T. E. (1951). Estimation of particle transmission by random sampling. <em>National Bureau of Standards Applied Mathematics Series</em>, 12, 27‚Äì30. Foundational paper on importance sampling from the RAND Corporation.</p>
</div>
<div class="citation" id="neyman1934" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Neyman1934<span class="fn-bracket">]</span></span>
<p>Neyman, J. (1934). On the two different aspects of the representative method: The method of stratified sampling and the method of purposive selection. <em>Journal of the Royal Statistical Society</em>, 97(4), 558‚Äì625. Establishes optimal allocation theory for stratified sampling.</p>
</div>
<div class="citation" id="hammersleymorton1956" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HammersleyMorton1956<span class="fn-bracket">]</span></span>
<p>Hammersley, J. M. and Morton, K. W. (1956). A new Monte Carlo technique: antithetic variates. <em>Mathematical Proceedings of the Cambridge Philosophical Society</em>, 52(3), 449‚Äì475. Introduces antithetic variates.</p>
</div>
<div class="citation" id="hammersleyhandscomb1964" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HammersleyHandscomb1964<span class="fn-bracket">]</span></span>
<p>Hammersley, J. M. and Handscomb, D. C. (1964). <em>Monte Carlo Methods</em>. Methuen, London. Classic monograph systematizing variance reduction techniques including control variates.</p>
</div>
<div class="citation" id="mckayetal1979" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>McKayEtAl1979<span class="fn-bracket">]</span></span>
<p>McKay, M. D., Beckman, R. J., and Conover, W. J. (1979). A comparison of three methods for selecting values of input variables in the analysis of output from a computer code. <em>Technometrics</em>, 21(2), 239‚Äì245. Introduces Latin Hypercube Sampling.</p>
</div>
<div class="citation" id="kong1992" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Kong1992<span class="fn-bracket">]</span></span>
<p>Kong, A. (1992). A note on importance sampling using standardized weights. Technical Report 348, Department of Statistics, University of Chicago. Establishes ESS interpretation for importance sampling.</p>
</div>
<div class="citation" id="owen1997" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Owen1997<span class="fn-bracket">]</span></span>
<p>Owen, A. B. (1997). Monte Carlo variance of scrambled net quadrature. <em>SIAM Journal on Numerical Analysis</em>, 34(5), 1884‚Äì1910. Theoretical analysis of Latin Hypercube Sampling.</p>
</div>
<div class="citation" id="bengtssonetal2008" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>BengtssonEtAl2008<span class="fn-bracket">]</span></span>
<p>Bengtsson, T., Bickel, P., and Li, B. (2008). Curse-of-dimensionality revisited: Collapse of the particle filter in very large scale systems. In <em>Probability and Statistics: Essays in Honor of David A. Freedman</em>, 316‚Äì334. Institute of Mathematical Statistics. Proves exponential sample size requirements for importance sampling in high dimensions.</p>
</div>
<div class="citation" id="robertcasella2004" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>RobertCasella2004<span class="fn-bracket">]</span></span>
<p>Robert, C. P. and Casella, G. (2004). <em>Monte Carlo Statistical Methods</em> (2nd ed.). Springer-Verlag, New York. Comprehensive treatment of variance reduction in the Monte Carlo context.</p>
</div>
<div class="citation" id="glasserman2004" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Glasserman2004<span class="fn-bracket">]</span></span>
<p>Glasserman, P. (2004). <em>Monte Carlo Methods in Financial Engineering</em>. Springer-Verlag, New York. Variance reduction applications in financial computing.</p>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="ch2_5-rejection-sampling.html" class="btn btn-neutral float-left" title="2.1.5. Rejection Sampling" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ch2_7-chapter-summary.html" class="btn btn-neutral float-right" title="2.1.7. Chapter Summary" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>