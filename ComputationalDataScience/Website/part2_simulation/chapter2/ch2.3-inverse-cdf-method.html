

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>2.1.1. Inverse CDF Method &mdash; STAT 418</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=6826d573" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT418/Website/part2_simulation/chapter2/ch2.3-inverse-cdf-method.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=30646c52"></script>
      <script>window.MathJax = {"options": {"enableMenu": true, "menuOptions": {"settings": {"enrich": true, "speech": true, "braille": true, "collapsible": true, "assistiveMml": false}}}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
      <script src="../../_static/custom.js?v=d2113767"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="2.1.2. Transformation Methods" href="ch2.4-transformation-methods.html" />
    <link rel="prev" title="2.1. Chapter 2: Monte Carlo Simulation" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Course Content</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../part1_foundations/index.html">1. Part I: Foundations of Probability and Computation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part1_foundations/chapter1/index.html">1.1. Chapter 1: Statistical Paradigms and Core Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html">1.1.1. Paradigms of Probability and Statistical Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#the-mathematical-foundation-kolmogorov-s-axioms">The Mathematical Foundation: Kolmogorov‚Äôs Axioms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#interpretations-of-probability">Interpretations of Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#statistical-inference-paradigms">Statistical Inference Paradigms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#historical-and-philosophical-debates">Historical and Philosophical Debates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#looking-ahead-our-course-focus">Looking Ahead: Our Course Focus</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html">1.1.2. Probability Distributions: Theory and Computation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#from-abstract-foundations-to-concrete-tools">From Abstract Foundations to Concrete Tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#the-python-ecosystem-for-probability">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#introduction-why-probability-distributions-matter">Introduction: Why Probability Distributions Matter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#id1">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#continuous-distributions">Continuous Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#additional-important-distributions">Additional Important Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#summary-and-practical-guidelines">Summary and Practical Guidelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html">1.1.3. Python Random Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#from-mathematical-distributions-to-computational-samples">From Mathematical Distributions to Computational Samples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-python-ecosystem-at-a-glance">The Python Ecosystem at a Glance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#understanding-pseudo-random-number-generation">Understanding Pseudo-Random Number Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-standard-library-random-module">The Standard Library: <code class="docutils literal notranslate"><span class="pre">random</span></code> Module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#numpy-fast-vectorized-random-sampling">NumPy: Fast Vectorized Random Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#scipy-stats-the-complete-statistical-toolkit">SciPy Stats: The Complete Statistical Toolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#bringing-it-all-together-library-selection-guide">Bringing It All Together: Library Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#looking-ahead-from-random-numbers-to-monte-carlo-methods">Looking Ahead: From Random Numbers to Monte Carlo Methods</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">2. Part II: Simulation-Based Methods</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="index.html">2.1. Chapter 2: Monte Carlo Simulation</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">2.1.1. Inverse CDF Method</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#mathematical-foundations">Mathematical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#continuous-distributions-with-closed-form-inverses">Continuous Distributions with Closed-Form Inverses</a></li>
<li class="toctree-l4"><a class="reference internal" href="#numerical-inversion">Numerical Inversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#mixed-distributions">Mixed Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="#transition-to-what-follows">Transition to What Follows</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2.4-transformation-methods.html">2.1.2. Transformation Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2.4-transformation-methods.html#why-transformation-methods">Why Transformation Methods?</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.4-transformation-methods.html#the-boxmuller-transform">The Box‚ÄìMuller Transform</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.4-transformation-methods.html#the-polar-marsaglia-method">The Polar (Marsaglia) Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.4-transformation-methods.html#the-ziggurat-algorithm">The Ziggurat Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.4-transformation-methods.html#the-clt-approximation-historical">The CLT Approximation (Historical)</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.4-transformation-methods.html#distributions-derived-from-the-normal">Distributions Derived from the Normal</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.4-transformation-methods.html#infinite-discrete-distributions">Infinite Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.4-transformation-methods.html#multivariate-normal-generation">Multivariate Normal Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.4-transformation-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.4-transformation-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter3/index.html">2.2. Chapter 3: Frequentist Statistical Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/sampling_variability.html">2.2.1. Sampling Variability</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/statistical_estimators.html">2.2.2. Statistical Estimators</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/plugin_methods.html">2.2.3. Plugin Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/parametric_inference.html">2.2.4. Parametric Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/exponential_families.html">2.2.5. Exponential Families</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/maximum_likelihood.html">2.2.6. Maximum Likelihood</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/linear_models.html">2.2.7. Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/generalized_linear_models.html">2.2.8. Generalized Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter4/index.html">2.3. Chapter 4: Resampling Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/jackknife_introduction.html">2.3.1. Jackknife Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html">2.3.2. Bootstrap Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html">2.3.3. Nonparametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/parametric_bootstrap.html">2.3.4. Parametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/confidence_intervals.html">2.3.5. Confidence Intervals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/bias_correction.html">2.3.6. Bias Correction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/cross_validation_loo.html">2.3.7. Cross Validation Loo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html">2.3.8. Cross Validation K Fold</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/model_selection.html">2.3.9. Model Selection</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part3_bayesian/index.html">Part III: Bayesian Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part3_bayesian/index.html#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html">1. Bayesian Philosophy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#introduction">1.1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#key-concepts">1.2. Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#mathematical-framework">1.3. Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#python-implementation">1.4. Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#examples">1.5. Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#summary">1.6. Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html"><span class="section-number">2. </span>Part II: Simulation-Based Methods</a></li>
          <li class="breadcrumb-item"><a href="index.html"><span class="section-number">2.1. </span>Chapter 2: Monte Carlo Simulation</a></li>
      <li class="breadcrumb-item active"><span class="section-number">2.1.1. </span>Inverse CDF Method</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/part2_simulation/chapter2/ch2.3-inverse-cdf-method.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="inverse-cdf-method">
<span id="ch2-3-inverse-cdf-method"></span><h1><span class="section-number">2.1.1. </span>Inverse CDF Method<a class="headerlink" href="#inverse-cdf-method" title="Link to this heading">ÔÉÅ</a></h1>
<p>With a reliable supply of uniform random variates in hand‚Äîthe fruit of <a class="reference internal" href="ch2_2-uniform-random-variates.html#ch2-2-uniform-random-variates"><span class="std std-ref">Uniform Random Variates</span></a>‚Äôs exploration of pseudo-random number generators‚Äîwe now face the central question of random variable generation: <em>how do we transform these uniform numbers into samples from other distributions?</em></p>
<p>The answer lies in one of the most elegant results in probability theory: the Probability Integral Transform. This theorem, which we previewed in our discussion of why uniform variates are ‚Äúuniversal currency,‚Äù now receives its full mathematical treatment. It tells us that any distribution can be sampled by computing the inverse of its cumulative distribution function applied to a uniform random number. The method is universal in principle: it works for continuous, discrete, and mixed distributions alike. When the inverse CDF has a closed-form expression‚Äîas it does for exponential, Weibull, Pareto, and Cauchy distributions‚Äîimplementation is immediate and efficient.</p>
<p>But the inverse CDF method is more than a theoretical curiosity. It is the workhorse of random number generation, the first algorithm any practitioner should consider when faced with a new distribution. Understanding when it applies, how to implement it efficiently, and when to seek alternatives is essential knowledge for computational statistics.</p>
<p>This section develops the inverse CDF method in full generality. We begin with the mathematical foundations‚Äîproving why the method works and what the ‚Äúgeneralized inverse‚Äù means for distributions that lack smooth inverses. We then work through continuous distributions with closed-form inverses, deriving and implementing samplers for several important cases. For discrete distributions, we develop increasingly sophisticated algorithms: linear search, binary search, and the remarkable alias method that achieves constant-time sampling. We address numerical issues that arise in practice and identify the method‚Äôs limitations‚Äîsetting the stage for rejection sampling and specialized transformations in subsequent sections.</p>
<div class="important admonition">
<p class="admonition-title">Road Map üß≠</p>
<ul class="simple">
<li><p><strong>Understand</strong>: Why the inverse CDF method produces correctly distributed samples, with complete proofs for both continuous and discrete cases</p></li>
<li><p><strong>Master</strong>: Closed-form inverse CDFs for exponential, Weibull, Pareto, Cauchy, and logistic distributions</p></li>
<li><p><strong>Develop</strong>: Efficient algorithms for discrete distributions‚Äîfrom <span class="math notranslate nohighlight">\(O(K)\)</span> linear search through <span class="math notranslate nohighlight">\(O(\log K)\)</span> binary search to <span class="math notranslate nohighlight">\(O(1)\)</span> alias method</p></li>
<li><p><strong>Implement</strong>: Complete Python code with attention to numerical precision and edge cases</p></li>
<li><p><strong>Evaluate</strong>: When the inverse CDF method excels and when alternatives (rejection sampling, specialized transforms) are preferable</p></li>
</ul>
</div>
<section id="mathematical-foundations">
<h2>Mathematical Foundations<a class="headerlink" href="#mathematical-foundations" title="Link to this heading">ÔÉÅ</a></h2>
<p>The inverse CDF method rests on a deep connection between the uniform distribution and all other distributions. We now develop this connection rigorously.</p>
<section id="the-cumulative-distribution-function">
<h3>The Cumulative Distribution Function<a class="headerlink" href="#the-cumulative-distribution-function" title="Link to this heading">ÔÉÅ</a></h3>
<p>For any random variable <span class="math notranslate nohighlight">\(X\)</span>, the <strong>cumulative distribution function</strong> (CDF) <span class="math notranslate nohighlight">\(F_X: \mathbb{R} \to [0, 1]\)</span> is defined by:</p>
<div class="math notranslate nohighlight">
\[F_X(x) = P(X \leq x)\]</div>
<p>The CDF has several fundamental properties:</p>
<ol class="arabic simple">
<li><p><strong>Monotonicity</strong>: <span class="math notranslate nohighlight">\(F_X\)</span> is non-decreasing. If <span class="math notranslate nohighlight">\(x_1 &lt; x_2\)</span>, then <span class="math notranslate nohighlight">\(F_X(x_1) \leq F_X(x_2)\)</span>.</p></li>
<li><p><strong>Right-continuity</strong>: <span class="math notranslate nohighlight">\(\lim_{h \to 0^+} F_X(x + h) = F_X(x)\)</span> for all <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
<li><p><strong>Limits at infinity</strong>: <span class="math notranslate nohighlight">\(\lim_{x \to -\infty} F_X(x) = 0\)</span> and <span class="math notranslate nohighlight">\(\lim_{x \to \infty} F_X(x) = 1\)</span>.</p></li>
</ol>
<p>For <strong>continuous</strong> random variables with density <span class="math notranslate nohighlight">\(f_X\)</span>, the CDF is obtained by integration:</p>
<div class="math notranslate nohighlight">
\[F_X(x) = \int_{-\infty}^{x} f_X(t) \, dt\]</div>
<p>and the CDF is continuous everywhere.</p>
<p>For <strong>discrete</strong> random variables taking values <span class="math notranslate nohighlight">\(x_1 &lt; x_2 &lt; \cdots\)</span> with probabilities <span class="math notranslate nohighlight">\(p_1, p_2, \ldots\)</span>, the CDF is a step function:</p>
<div class="math notranslate nohighlight">
\[F_X(x) = \sum_{x_i \leq x} p_i\]</div>
<p>The CDF jumps by <span class="math notranslate nohighlight">\(p_i\)</span> at each value <span class="math notranslate nohighlight">\(x_i\)</span>.</p>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide009_img003_8228ef1b.png"><img alt="CDF for discrete distribution showing step function behavior" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide009_img003_8228ef1b.png" style="width: 55%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.1 </span><span class="caption-text"><strong>CDF for Discrete Distributions.</strong> The cumulative distribution function of a discrete random variable is a step function with jumps at each possible value. For the killdeer clutch size example shown, the CDF maps any <span class="math notranslate nohighlight">\(u \in (0, 1)\)</span> to the smallest outcome <span class="math notranslate nohighlight">\(x\)</span> where <span class="math notranslate nohighlight">\(F(x) \geq u\)</span>‚Äîthis is precisely the generalized inverse.</span><a class="headerlink" href="#id1" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
</section>
<section id="the-generalized-inverse">
<h3>The Generalized Inverse<a class="headerlink" href="#the-generalized-inverse" title="Link to this heading">ÔÉÅ</a></h3>
<p>When <span class="math notranslate nohighlight">\(F_X\)</span> is continuous and strictly increasing, it has a unique inverse: <span class="math notranslate nohighlight">\(F_X^{-1}(u)\)</span> is the unique <span class="math notranslate nohighlight">\(x\)</span> such that <span class="math notranslate nohighlight">\(F_X(x) = u\)</span>. But for discrete distributions (where <span class="math notranslate nohighlight">\(F_X\)</span> is a step function) or mixed distributions, the ordinary inverse does not exist. We need a more general definition.</p>
<div class="note admonition">
<p class="admonition-title">Definition: Generalized Inverse (Quantile Function)</p>
<p>For any distribution function <span class="math notranslate nohighlight">\(F: \mathbb{R} \to [0, 1]\)</span>, the <strong>generalized inverse</strong> (or <strong>quantile function</strong>) is:</p>
<div class="math notranslate nohighlight" id="equation-gen-inverse-def">
<span class="eqno">(2.1)<a class="headerlink" href="#equation-gen-inverse-def" title="Link to this equation">ÔÉÅ</a></span>\[F^{-1}(u) = \inf\{x \in \mathbb{R} : F(x) \geq u\} \quad \text{for } u \in (0, 1)\]</div>
<p>This is the smallest <span class="math notranslate nohighlight">\(x\)</span> for which the CDF reaches or exceeds <span class="math notranslate nohighlight">\(u\)</span>.</p>
</div>
<p>The generalized inverse has several important properties:</p>
<p><strong>Property 1</strong>: For continuous, strictly increasing <span class="math notranslate nohighlight">\(F\)</span>, the generalized inverse coincides with the ordinary inverse.</p>
<p><strong>Property 2</strong>: For any <span class="math notranslate nohighlight">\(F\)</span>, <span class="math notranslate nohighlight">\(F^{-1}(u) \leq x\)</span> if and only if <span class="math notranslate nohighlight">\(u \leq F(x)\)</span>. This equivalence is the key to proving the inverse CDF method works.</p>
<p><strong>Property 3</strong>: <span class="math notranslate nohighlight">\(F^{-1}\)</span> is non-decreasing and left-continuous.</p>
<p><strong>Proof of Property 2</strong>:</p>
<p>(<span class="math notranslate nohighlight">\(\Leftarrow\)</span>) Suppose <span class="math notranslate nohighlight">\(u \leq F(x)\)</span>. Then <span class="math notranslate nohighlight">\(x \in \{t : F(t) \geq u\}\)</span>, so <span class="math notranslate nohighlight">\(F^{-1}(u) = \inf\{t : F(t) \geq u\} \leq x\)</span>.</p>
<p>(<span class="math notranslate nohighlight">\(\Rightarrow\)</span>) Suppose <span class="math notranslate nohighlight">\(F^{-1}(u) \leq x\)</span>. By definition, <span class="math notranslate nohighlight">\(F^{-1}(u) = \inf\{t : F(t) \geq u\}\)</span>. By right-continuity of <span class="math notranslate nohighlight">\(F\)</span>, <span class="math notranslate nohighlight">\(F(F^{-1}(u)) \geq u\)</span>. Since <span class="math notranslate nohighlight">\(F\)</span> is non-decreasing and <span class="math notranslate nohighlight">\(F^{-1}(u) \leq x\)</span>, we have <span class="math notranslate nohighlight">\(F(x) \geq F(F^{-1}(u)) \geq u\)</span>. ‚àé</p>
</section>
<section id="the-probability-integral-transform">
<h3>The Probability Integral Transform<a class="headerlink" href="#the-probability-integral-transform" title="Link to this heading">ÔÉÅ</a></h3>
<p>With the generalized inverse defined, we can state and prove the fundamental result.</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Probability Integral Transform (Inverse Direction)</p>
<p>If <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0, 1)\)</span> and <span class="math notranslate nohighlight">\(F\)</span> is any distribution function, then <span class="math notranslate nohighlight">\(X = F^{-1}(U)\)</span> has distribution function <span class="math notranslate nohighlight">\(F\)</span>.</p>
</div>
<p><strong>Proof</strong>: We must show that <span class="math notranslate nohighlight">\(P(X \leq x) = F(x)\)</span> for all <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}P(X \leq x) &amp;= P(F^{-1}(U) \leq x) \\
            &amp;= P(U \leq F(x)) \quad \text{(by Property 2)} \\
            &amp;= F(x) \quad \text{(since } U \sim \text{Uniform}(0,1) \text{)}\end{split}\]</div>
<p>The final equality uses the fact that for <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0,1)\)</span>, <span class="math notranslate nohighlight">\(P(U \leq u) = u\)</span> for any <span class="math notranslate nohighlight">\(u \in [0, 1]\)</span>. ‚àé</p>
<p>This proof is remarkably general. It applies to:</p>
<ul class="simple">
<li><p>Continuous distributions with smooth inverses</p></li>
<li><p>Discrete distributions with step-function CDFs</p></li>
<li><p>Mixed distributions combining point masses and continuous components</p></li>
<li><p>Any distribution whatsoever, provided we use the generalized inverse</p></li>
</ul>
<p>The converse also holds under mild conditions:</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Probability Integral Transform (Forward Direction)</p>
<p>If <span class="math notranslate nohighlight">\(X\)</span> has a continuous CDF <span class="math notranslate nohighlight">\(F\)</span>, then <span class="math notranslate nohighlight">\(U = F(X) \sim \text{Uniform}(0, 1)\)</span>.</p>
</div>
<p><strong>Proof</strong>: For <span class="math notranslate nohighlight">\(u \in (0, 1)\)</span>:</p>
<div class="math notranslate nohighlight">
\[P(F(X) \leq u) = P(X \leq F^{-1}(u)) = F(F^{-1}(u)) = u\]</div>
<p>where the last equality holds because <span class="math notranslate nohighlight">\(F\)</span> is continuous and strictly increasing on its support. ‚àé</p>
</section>
<section id="geometric-interpretation">
<h3>Geometric Interpretation<a class="headerlink" href="#geometric-interpretation" title="Link to this heading">ÔÉÅ</a></h3>
<p>The inverse CDF method has an intuitive geometric interpretation. Imagine the graph of the CDF <span class="math notranslate nohighlight">\(F(x)\)</span>:</p>
<ol class="arabic simple">
<li><p>Generate a uniform random number <span class="math notranslate nohighlight">\(u \in (0, 1)\)</span> ‚Äî a random height on the <span class="math notranslate nohighlight">\(y\)</span>-axis.</p></li>
<li><p>Draw a horizontal line at height <span class="math notranslate nohighlight">\(u\)</span>.</p></li>
<li><p>Find where this line first intersects the CDF curve (or, for step functions, where it first reaches the CDF from below).</p></li>
<li><p>Project down to the <span class="math notranslate nohighlight">\(x\)</span>-axis. This is your sample <span class="math notranslate nohighlight">\(F^{-1}(u)\)</span>.</p></li>
</ol>
<p>For continuous distributions with strictly increasing CDFs, the intersection is unique. For discrete distributions, the horizontal line may intersect a ‚Äúriser‚Äù of the step function; the generalized inverse picks the <span class="math notranslate nohighlight">\(x\)</span>-value at the top of that riser.</p>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_3_fig01_inverse_cdf_method.png"><img alt="Six-panel visualization of the inverse CDF method showing the geometric transformation" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_3_fig01_inverse_cdf_method.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.2 </span><span class="caption-text"><strong>The Inverse CDF Method: Geometric View.</strong> Top row: For an exponential distribution, we start with a uniform sample <span class="math notranslate nohighlight">\(U\)</span> (left), apply the inverse CDF transformation <span class="math notranslate nohighlight">\(X = F^{-1}(U)\)</span> (center), and obtain an exponentially distributed sample (right). The transformation ‚Äústretches‚Äù uniform samples according to where the CDF is shallow (rare values) and ‚Äúcompresses‚Äù where the CDF is steep (common values). Bottom row: The same process applied to three different distributions‚ÄîCauchy (heavy tails), Normal (symmetric), and Beta (flexible shape)‚Äîdemonstrates the universality of the method.</span><a class="headerlink" href="#id2" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<div class="tip admonition">
<p class="admonition-title">Try It Yourself üñ•Ô∏è Interactive Inverse CDF Visualizations</p>
<p>Two interactive simulations help you build intuition for the inverse CDF method. We strongly recommend experimenting with both before proceeding.</p>
<p><strong>Simulation 1: Generalized Inverse CDF Explorer</strong></p>
<p><a class="reference external" href="https://treese41528.github.io/ComputationalDataScience/Simulations/MonteCarloSimulation/inverse_cdf_transform_general.html">https://treese41528.github.io/ComputationalDataScience/Simulations/MonteCarloSimulation/inverse_cdf_transform_general.html</a></p>
<p>This simulation shows the geometric relationship between a CDF and its inverse side-by-side. Use it to understand <em>why</em> the method works:</p>
<ul class="simple">
<li><p><strong>Left panel (CDF)</strong>: Shows <span class="math notranslate nohighlight">\(F(x)\)</span> with a horizontal line at the current <span class="math notranslate nohighlight">\(u\)</span> value. Watch how the vertical projection finds where <span class="math notranslate nohighlight">\(F(x) \geq u\)</span>.</p></li>
<li><p><strong>Right panel (Inverse CDF)</strong>: Shows <span class="math notranslate nohighlight">\(F^{-1}(u)\)</span> directly as a function of <span class="math notranslate nohighlight">\(u\)</span>. The red dot traces along this curve as you change <span class="math notranslate nohighlight">\(u\)</span>.</p></li>
<li><p><strong>Distribution selector</strong>: Switch between <em>Continuous (Normal)</em>, <em>Discrete (Poisson)</em>, and <em>Mixed (Zero-inflated)</em> to see how the generalized inverse handles each case.</p></li>
<li><p><strong>Animation mode</strong>: Check ‚ÄúAnimate u from 0 to 1‚Äù to watch how the entire range of uniform values maps to the target distribution.</p></li>
</ul>
<p><strong>Key observations to make</strong>:</p>
<ol class="arabic simple">
<li><p>For the <em>continuous normal</em>, both the CDF and inverse CDF are smooth curves. Moving <span class="math notranslate nohighlight">\(u\)</span> produces a smooth change in <span class="math notranslate nohighlight">\(F^{-1}(u)\)</span>.</p></li>
<li><p>For the <em>discrete Poisson</em>, the CDF is a step function. Notice how the inverse CDF has <em>flat regions</em> (plateaus)‚Äîmany different <span class="math notranslate nohighlight">\(u\)</span> values map to the same integer outcome. This is the generalized inverse in action: <span class="math notranslate nohighlight">\(F^{-1}(u) = \inf\{x : F(x) \geq u\}\)</span> finds the smallest <span class="math notranslate nohighlight">\(x\)</span> that ‚Äúclears‚Äù the <span class="math notranslate nohighlight">\(u\)</span> threshold.</p></li>
<li><p>For the <em>mixed zero-inflated</em>, the CDF has a jump at zero (the point mass) followed by a continuous portion. The inverse CDF shows a flat region at <span class="math notranslate nohighlight">\(x = 0\)</span> for <span class="math notranslate nohighlight">\(u \in [0, p_0]\)</span>, then becomes continuous.</p></li>
<li><p><strong>Hover over either canvas</strong> to interactively set the <span class="math notranslate nohighlight">\(u\)</span> value‚Äîthis helps you see the duality between the two representations.</p></li>
</ol>
<p><strong>Simulation 2: Inverse Transform Sampling in Action</strong></p>
<p><a class="reference external" href="https://treese41528.github.io/ComputationalDataScience/Simulations/MonteCarloSimulation/inverse_cdf_transform_discrete_example.html">https://treese41528.github.io/ComputationalDataScience/Simulations/MonteCarloSimulation/inverse_cdf_transform_discrete_example.html</a></p>
<p>This simulation demonstrates the <em>sampling process</em> itself. Watch uniform random numbers transform into samples from your chosen distribution:</p>
<ul class="simple">
<li><p><strong>Left histogram</strong>: Shows the uniform <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0,1)\)</span> samples accumulating. This histogram should converge to a flat (uniform) density.</p></li>
<li><p><strong>Right histogram</strong>: Shows the transformed <span class="math notranslate nohighlight">\(X = F^{-1}(U)\)</span> samples. The gold curve overlaid is the true PDF‚Äîwatch as the histogram converges to match it.</p></li>
<li><p><strong>Computation display</strong>: Shows the exact arithmetic for each sample: the uniform input <span class="math notranslate nohighlight">\(U\)</span>, the formula applied, and the output <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
<li><p><strong>Distribution selector</strong>: Try <em>Exponential</em>, <em>Normal</em>, <em>Logistic</em>, <em>Pareto</em>, <em>Cauchy</em>, and <em>Gamma</em> to see different inverse CDF formulas in action.</p></li>
<li><p><strong>Speed control</strong>: Slow down the animation to follow individual transformations, or speed up to see convergence faster.</p></li>
</ul>
<p><strong>Key observations to make</strong>:</p>
<ol class="arabic simple">
<li><p><strong>The magic of transformation</strong>: Uniform samples (flat histogram on the left) become correctly distributed samples (matching the gold PDF on the right) through nothing more than arithmetic.</p></li>
<li><p><strong>Concentration vs. spread</strong>: For the exponential, notice how uniform values near 0 produce small <span class="math notranslate nohighlight">\(X\)</span>, while uniform values near 1 produce large <span class="math notranslate nohighlight">\(X\)</span>. The inverse CDF ‚Äústretches‚Äù the uniform distribution according to where the target PDF has more or less mass.</p></li>
<li><p><strong>Heavy tails</strong>: Try the Cauchy distribution and watch for occasional extreme values. These correspond to <span class="math notranslate nohighlight">\(U\)</span> values very close to 0 or 1, where <span class="math notranslate nohighlight">\(\tan(\pi(U - 0.5)) \to \pm\infty\)</span>.</p></li>
<li><p><strong>Sample mean convergence</strong>: Watch the ‚ÄúMean‚Äù statistic. For the exponential (theoretical mean = 1), it converges nicely. For the Cauchy (no theoretical mean), it fluctuates wildly even with many samples‚Äîa vivid demonstration of heavy tails.</p></li>
<li><p><strong>Formula verification</strong>: Pause the animation and verify the computation manually. For exponential with <span class="math notranslate nohighlight">\(U = 0.3\)</span>: <span class="math notranslate nohighlight">\(X = -\ln(1 - 0.3) = -\ln(0.7) \approx 0.357\)</span>.</p></li>
</ol>
</div>
</section>
</section>
<section id="continuous-distributions-with-closed-form-inverses">
<h2>Continuous Distributions with Closed-Form Inverses<a class="headerlink" href="#continuous-distributions-with-closed-form-inverses" title="Link to this heading">ÔÉÅ</a></h2>
<p>The inverse CDF method is most elegant when <span class="math notranslate nohighlight">\(F^{-1}\)</span> has a closed-form expression. We can then generate samples with just a few arithmetic operations‚Äîtypically faster than any alternative method.</p>
<section id="the-exponential-distribution">
<h3>The Exponential Distribution<a class="headerlink" href="#the-exponential-distribution" title="Link to this heading">ÔÉÅ</a></h3>
<p>The exponential distribution is the canonical example for the inverse CDF method. It models waiting times in Poisson processes and appears throughout reliability theory, queuing theory, and survival analysis.</p>
<p><strong>Setup</strong>: <span class="math notranslate nohighlight">\(X \sim \text{Exponential}(\lambda)\)</span> with rate parameter <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span>.</p>
<p><strong>CDF</strong>:</p>
<div class="math notranslate nohighlight">
\[F(x) = 1 - e^{-\lambda x}, \quad x \geq 0\]</div>
<p><strong>Deriving the Inverse</strong>: Solve <span class="math notranslate nohighlight">\(F(x) = u\)</span> for <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}1 - e^{-\lambda x} &amp;= u \\
e^{-\lambda x} &amp;= 1 - u \\
-\lambda x &amp;= \ln(1 - u) \\
x &amp;= -\frac{\ln(1 - u)}{\lambda}\end{split}\]</div>
<p><strong>Simplification</strong>: Since <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(1 - U\)</span> have the same distribution when <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0, 1)\)</span>, we can use either:</p>
<div class="math notranslate nohighlight">
\[F^{-1}(u) = -\frac{\ln(1 - u)}{\lambda} \quad \text{or} \quad F^{-1}(u) = -\frac{\ln(u)}{\lambda}\]</div>
<p>The second form saves one subtraction but requires care when <span class="math notranslate nohighlight">\(u = 0\)</span> (which occurs with probability zero but may arise from floating-point edge cases).</p>
<div class="note admonition">
<p class="admonition-title">Example üí° Generating Exponential Random Variables</p>
<p><strong>Given</strong>: Generate 10,000 samples from <span class="math notranslate nohighlight">\(\text{Exponential}(\lambda = 2)\)</span>.</p>
<p><strong>Mathematical approach</strong>:</p>
<p>For <span class="math notranslate nohighlight">\(\lambda = 2\)</span>, <span class="math notranslate nohighlight">\(F^{-1}(u) = -\ln(1-u)/2\)</span>.</p>
<p><strong>Theoretical properties</strong>:</p>
<ul class="simple">
<li><p>Mean: <span class="math notranslate nohighlight">\(\mathbb{E}[X] = 1/\lambda = 0.5\)</span></p></li>
<li><p>Variance: <span class="math notranslate nohighlight">\(\text{Var}(X) = 1/\lambda^2 = 0.25\)</span></p></li>
<li><p>Median: <span class="math notranslate nohighlight">\(F^{-1}(0.5) = \ln(2)/\lambda \approx 0.347\)</span></p></li>
</ul>
<p><strong>Python implementation</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">exponential_inverse_cdf</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">rate</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate Exponential(rate) samples via inverse CDF.</span>

<span class="sd">    Uses the tail-stable form X = -log(U)/rate, avoiding (1-U).</span>
<span class="sd">    Since U and 1-U have the same distribution, this is equivalent</span>
<span class="sd">    but provides better resolution for large X values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="c1"># Guard against log(0) if the RNG can return exactly 0</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">u</span><span class="p">)</span> <span class="o">/</span> <span class="n">rate</span>

<span class="c1"># Generate samples</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10_000</span>
<span class="n">rate</span> <span class="o">=</span> <span class="mf">2.0</span>

<span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">exponential_inverse_cdf</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">rate</span><span class="p">)</span>

<span class="c1"># Verify</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="mi">1</span><span class="o">/</span><span class="n">rate</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="mi">1</span><span class="o">/</span><span class="n">rate</span><span class="o">**</span><span class="mi">2</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample median: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">rate</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Output</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Sample</span> <span class="n">mean</span><span class="p">:</span> <span class="mf">0.4976</span> <span class="p">(</span><span class="n">theory</span><span class="p">:</span> <span class="mf">0.5000</span><span class="p">)</span>
<span class="n">Sample</span> <span class="n">variance</span><span class="p">:</span> <span class="mf">0.2467</span> <span class="p">(</span><span class="n">theory</span><span class="p">:</span> <span class="mf">0.2500</span><span class="p">)</span>
<span class="n">Sample</span> <span class="n">median</span><span class="p">:</span> <span class="mf">0.3441</span> <span class="p">(</span><span class="n">theory</span><span class="p">:</span> <span class="mf">0.3466</span><span class="p">)</span>
</pre></div>
</div>
<p>The sample statistics closely match theoretical values, confirming the correctness of our sampler.</p>
</div>
</section>
<section id="the-weibull-distribution">
<h3>The Weibull Distribution<a class="headerlink" href="#the-weibull-distribution" title="Link to this heading">ÔÉÅ</a></h3>
<p>The Weibull distribution generalizes the exponential to allow for increasing or decreasing hazard rates, making it fundamental in reliability analysis.</p>
<p><strong>Setup</strong>: <span class="math notranslate nohighlight">\(X \sim \text{Weibull}(k, \lambda)\)</span> with shape <span class="math notranslate nohighlight">\(k &gt; 0\)</span> and scale <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span>.</p>
<p><strong>CDF</strong>:</p>
<div class="math notranslate nohighlight">
\[F(x) = 1 - \exp\left( -\left(\frac{x}{\lambda}\right)^k \right), \quad x \geq 0\]</div>
<p><strong>Deriving the Inverse</strong>: Solve <span class="math notranslate nohighlight">\(F(x) = u\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}1 - e^{-(x/\lambda)^k} &amp;= u \\
e^{-(x/\lambda)^k} &amp;= 1 - u \\
-\left(\frac{x}{\lambda}\right)^k &amp;= \ln(1 - u) \\
\left(\frac{x}{\lambda}\right)^k &amp;= -\ln(1 - u) \\
x &amp;= \lambda \left( -\ln(1 - u) \right)^{1/k}\end{split}\]</div>
<p><strong>Result</strong>:</p>
<div class="math notranslate nohighlight" id="equation-weibull-inverse">
<span class="eqno">(2.2)<a class="headerlink" href="#equation-weibull-inverse" title="Link to this equation">ÔÉÅ</a></span>\[F^{-1}(u) = \lambda \left( -\ln(1 - u) \right)^{1/k}\]</div>
<p>Note that when <span class="math notranslate nohighlight">\(k = 1\)</span>, the Weibull reduces to <span class="math notranslate nohighlight">\(\text{Exponential}(1/\lambda)\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">weibull_inverse_cdf</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate Weibull(shape, scale) samples via inverse CDF.</span>

<span class="sd">    Uses X = scale * (-log(U))^(1/shape), the tail-stable form</span>
<span class="sd">    that avoids computing (1-U).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">)</span>  <span class="c1"># Guard against log(0)</span>
    <span class="k">return</span> <span class="n">scale</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">u</span><span class="p">))</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">shape</span><span class="p">)</span>

<span class="c1"># Example: Weibull(k=2, Œª=1) - the Rayleigh distribution</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="mi">10_000</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">weibull_inverse_cdf</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample std: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="the-pareto-distribution">
<h3>The Pareto Distribution<a class="headerlink" href="#the-pareto-distribution" title="Link to this heading">ÔÉÅ</a></h3>
<p>The Pareto distribution models phenomena with ‚Äúheavy tails‚Äù‚Äîsituations where extreme values are more likely than a normal or exponential distribution would suggest. It appears in economics (income distribution), insurance (claim sizes), and network science (degree distributions).</p>
<p><strong>Setup</strong>: <span class="math notranslate nohighlight">\(X \sim \text{Pareto}(\alpha, x_m)\)</span> with shape <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span> and scale (minimum) <span class="math notranslate nohighlight">\(x_m &gt; 0\)</span>.</p>
<p><strong>CDF</strong>:</p>
<div class="math notranslate nohighlight">
\[F(x) = 1 - \left( \frac{x_m}{x} \right)^\alpha, \quad x \geq x_m\]</div>
<p><strong>Deriving the Inverse</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}1 - \left( \frac{x_m}{x} \right)^\alpha &amp;= u \\
\left( \frac{x_m}{x} \right)^\alpha &amp;= 1 - u \\
\frac{x_m}{x} &amp;= (1 - u)^{1/\alpha} \\
x &amp;= \frac{x_m}{(1 - u)^{1/\alpha}}\end{split}\]</div>
<p><strong>Result</strong>:</p>
<div class="math notranslate nohighlight" id="equation-pareto-inverse">
<span class="eqno">(2.3)<a class="headerlink" href="#equation-pareto-inverse" title="Link to this equation">ÔÉÅ</a></span>\[F^{-1}(u) = \frac{x_m}{(1 - u)^{1/\alpha}} = x_m (1 - u)^{-1/\alpha}\]</div>
<p>Since <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(1-U\)</span> have the same distribution, the tail-stable implementation uses:</p>
<div class="math notranslate nohighlight">
\[X = x_m \cdot U^{-1/\alpha}\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">pareto_inverse_cdf</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">x_min</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate Pareto(alpha, x_min) samples via inverse CDF.</span>

<span class="sd">    Uses the tail-stable form X = x_min * U^(-1/alpha),</span>
<span class="sd">    which avoids forming (1-U) and provides better resolution</span>
<span class="sd">    for large X values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="c1"># Guard against u=0 which would give infinity</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_min</span> <span class="o">*</span> <span class="n">u</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">/</span> <span class="n">alpha</span><span class="p">)</span>

<span class="c1"># Example: Pareto(Œ±=2.5, x_m=1)</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="mi">10_000</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pareto_inverse_cdf</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">x_min</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># For Œ± &gt; 1, mean = Œ± * x_m / (Œ± - 1)</span>
<span class="n">theoretical_mean</span> <span class="o">=</span> <span class="mf">2.5</span> <span class="o">*</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">2.5</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="n">theoretical_mean</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="the-cauchy-distribution">
<h3>The Cauchy Distribution<a class="headerlink" href="#the-cauchy-distribution" title="Link to this heading">ÔÉÅ</a></h3>
<p>The Cauchy distribution is infamous for having no mean or variance‚Äîits tails are so heavy that these moments do not exist. It arises as the ratio of two independent standard normals and appears in physics as the Lorentzian distribution.</p>
<p><strong>Setup</strong>: <span class="math notranslate nohighlight">\(X \sim \text{Cauchy}(\mu, \sigma)\)</span> with location <span class="math notranslate nohighlight">\(\mu\)</span> and scale <span class="math notranslate nohighlight">\(\sigma &gt; 0\)</span>.</p>
<p><strong>CDF</strong>:</p>
<div class="math notranslate nohighlight">
\[F(x) = \frac{1}{\pi} \arctan\left( \frac{x - \mu}{\sigma} \right) + \frac{1}{2}\]</div>
<p><strong>Deriving the Inverse</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{1}{\pi} \arctan\left( \frac{x - \mu}{\sigma} \right) + \frac{1}{2} &amp;= u \\
\arctan\left( \frac{x - \mu}{\sigma} \right) &amp;= \pi \left( u - \frac{1}{2} \right) \\
\frac{x - \mu}{\sigma} &amp;= \tan\left( \pi \left( u - \frac{1}{2} \right) \right) \\
x &amp;= \mu + \sigma \tan\left( \pi \left( u - \frac{1}{2} \right) \right)\end{split}\]</div>
<p><strong>Result</strong>:</p>
<div class="math notranslate nohighlight" id="equation-cauchy-inverse">
<span class="eqno">(2.4)<a class="headerlink" href="#equation-cauchy-inverse" title="Link to this equation">ÔÉÅ</a></span>\[F^{-1}(u) = \mu + \sigma \tan\left( \pi (u - 1/2) \right)\]</div>
<p><strong>Note</strong>: At <span class="math notranslate nohighlight">\(u = 0\)</span> and <span class="math notranslate nohighlight">\(u = 1\)</span>, the tangent function produces <span class="math notranslate nohighlight">\(\pm\infty\)</span>, which is mathematically correct since the Cauchy distribution has infinite support.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">cauchy_inverse_cdf</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate Cauchy(loc, scale) samples via inverse CDF.</span>

<span class="sd">    Note: Returns ¬±inf at u=0 and u=1, which is mathematically correct</span>
<span class="sd">    since the Cauchy has infinite support.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">loc</span> <span class="o">+</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">tan</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="n">u</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">))</span>

<span class="c1"># Standard Cauchy</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="mi">10_000</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">cauchy_inverse_cdf</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>

<span class="c1"># The median is the location parameter</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample median: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: 0.0)&quot;</span><span class="p">)</span>
<span class="c1"># Mean doesn&#39;t exist, but sample mean will be highly variable</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (undefined theoretically)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="the-logistic-distribution">
<h3>The Logistic Distribution<a class="headerlink" href="#the-logistic-distribution" title="Link to this heading">ÔÉÅ</a></h3>
<p>The logistic distribution resembles the normal but has heavier tails. It appears in logistic regression and as a smooth approximation to the step function.</p>
<p><strong>Setup</strong>: <span class="math notranslate nohighlight">\(X \sim \text{Logistic}(\mu, s)\)</span> with location <span class="math notranslate nohighlight">\(\mu\)</span> and scale <span class="math notranslate nohighlight">\(s &gt; 0\)</span>.</p>
<p><strong>CDF</strong>:</p>
<div class="math notranslate nohighlight">
\[F(x) = \frac{1}{1 + e^{-(x-\mu)/s}}\]</div>
<p><strong>Deriving the Inverse</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{1}{1 + e^{-(x-\mu)/s}} &amp;= u \\
1 + e^{-(x-\mu)/s} &amp;= \frac{1}{u} \\
e^{-(x-\mu)/s} &amp;= \frac{1-u}{u} \\
-(x-\mu)/s &amp;= \ln\left( \frac{1-u}{u} \right) \\
x &amp;= \mu - s \ln\left( \frac{1-u}{u} \right) = \mu + s \ln\left( \frac{u}{1-u} \right)\end{split}\]</div>
<p><strong>Result</strong>:</p>
<div class="math notranslate nohighlight" id="equation-logistic-inverse">
<span class="eqno">(2.5)<a class="headerlink" href="#equation-logistic-inverse" title="Link to this equation">ÔÉÅ</a></span>\[F^{-1}(u) = \mu + s \ln\left( \frac{u}{1-u} \right) = \mu + s \cdot \text{logit}(u)\]</div>
<p>The function <span class="math notranslate nohighlight">\(\text{logit}(u) = \ln(u/(1-u))\)</span> is the inverse of the logistic (sigmoid) function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">logistic_inverse_cdf</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate Logistic(loc, scale) samples via inverse CDF.</span>

<span class="sd">    Uses log(u) - log1p(-u) for numerical stability at both tails.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="c1"># Clamp to avoid exact 0 or 1</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
    <span class="c1"># Stable logit: log(u) - log(1-u) via log1p</span>
    <span class="k">return</span> <span class="n">loc</span> <span class="o">+</span> <span class="n">scale</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">u</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="o">-</span><span class="n">u</span><span class="p">))</span>

<span class="c1"># Standard logistic</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="mi">10_000</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">logistic_inverse_cdf</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>

<span class="c1"># Mean = loc, Variance = (œÄ * scale)¬≤ / 3</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: 0.0)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">**</span><span class="mi">2</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">3</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="summary-continuous-distributions">
<h3>Summary: Continuous Distributions<a class="headerlink" href="#summary-continuous-distributions" title="Link to this heading">ÔÉÅ</a></h3>
<p>The following table summarizes the inverse CDF formulas for common distributions:</p>
<table class="docutils align-default" id="id3">
<caption><span class="caption-number">Table 2.1 </span><span class="caption-text">Inverse CDF Formulas for Continuous Distributions</span><a class="headerlink" href="#id3" title="Link to this table">ÔÉÅ</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 35.0%" />
<col style="width: 45.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Distribution</p></th>
<th class="head"><p>CDF <span class="math notranslate nohighlight">\(F(x)\)</span></p></th>
<th class="head"><p>Inverse <span class="math notranslate nohighlight">\(F^{-1}(u)\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Exponential(<span class="math notranslate nohighlight">\(\lambda\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(1 - e^{-\lambda x}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\ln(1-u)/\lambda\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Weibull(<span class="math notranslate nohighlight">\(k, \lambda\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(1 - e^{-(x/\lambda)^k}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\lambda(-\ln(1-u))^{1/k}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Pareto(<span class="math notranslate nohighlight">\(\alpha, x_m\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(1 - (x_m/x)^\alpha\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_m (1-u)^{-1/\alpha}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Cauchy(<span class="math notranslate nohighlight">\(\mu, \sigma\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{1}{\pi}\arctan(\frac{x-\mu}{\sigma}) + \frac{1}{2}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu + \sigma\tan(\pi(u-1/2))\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Logistic(<span class="math notranslate nohighlight">\(\mu, s\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(1/(1 + e^{-(x-\mu)/s})\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu + s\ln(u/(1-u))\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Uniform(<span class="math notranslate nohighlight">\(a, b\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\((x-a)/(b-a)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(a + (b-a)u\)</span></p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="numerical-inversion">
<h2>Numerical Inversion<a class="headerlink" href="#numerical-inversion" title="Link to this heading">ÔÉÅ</a></h2>
<p>What about distributions whose inverse CDFs have no closed form? The normal distribution is the most important example: <span class="math notranslate nohighlight">\(\Phi^{-1}(u)\)</span> cannot be expressed in terms of elementary functions. For such distributions, we have two options:</p>
<ol class="arabic simple">
<li><p><strong>Numerical root-finding</strong>: Solve <span class="math notranslate nohighlight">\(F(x) = u\)</span> numerically for each sample.</p></li>
<li><p><strong>Use specialized algorithms</strong>: Box-Muller for normals, rejection sampling, etc.</p></li>
</ol>
<p>Numerical inversion is always possible but often slow. Let us examine when it is practical.</p>
<section id="root-finding-approach">
<h3>Root-Finding Approach<a class="headerlink" href="#root-finding-approach" title="Link to this heading">ÔÉÅ</a></h3>
<p>To compute <span class="math notranslate nohighlight">\(F^{-1}(u)\)</span>, we solve the equation <span class="math notranslate nohighlight">\(F(x) - u = 0\)</span>. Since <span class="math notranslate nohighlight">\(F\)</span> is monotonically increasing, standard root-finding methods are guaranteed to converge.</p>
<p><strong>Brent‚Äôs method</strong> is particularly suitable: it combines bisection (guaranteed convergence) with faster interpolation methods (speed). SciPy provides this as <code class="docutils literal notranslate"><span class="pre">scipy.optimize.brentq</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">optimize</span><span class="p">,</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">numerical_inverse_cdf</span><span class="p">(</span><span class="n">cdf_func</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">bracket</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute F‚Åª¬π(u) numerically using Brent&#39;s method.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    cdf_func : callable</span>
<span class="sd">        CDF function F(x).</span>
<span class="sd">    u : float or array</span>
<span class="sd">        Probability value(s) in (0, 1).</span>
<span class="sd">    bracket : tuple</span>
<span class="sd">        Interval [a, b] containing the root.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    float or array</span>
<span class="sd">        Quantile value(s).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">u_val</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">u</span><span class="p">):</span>
        <span class="c1"># Solve F(x) = u, i.e., F(x) - u = 0</span>
        <span class="n">result</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">optimize</span><span class="o">.</span><span class="n">brentq</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">cdf_func</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">u_val</span><span class="p">,</span>
            <span class="n">bracket</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">bracket</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">result</span>

<span class="c1"># Example: Normal distribution (for comparison with scipy.stats.norm.ppf)</span>
<span class="n">normal_cdf</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span>

<span class="n">u_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.975</span><span class="p">])</span>
<span class="n">numerical_quantiles</span> <span class="o">=</span> <span class="n">numerical_inverse_cdf</span><span class="p">(</span><span class="n">normal_cdf</span><span class="p">,</span> <span class="n">u_test</span><span class="p">,</span> <span class="n">bracket</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">exact_quantiles</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">u_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Comparison of numerical vs. exact normal quantiles:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">u</span><span class="p">,</span> <span class="n">num</span><span class="p">,</span> <span class="n">exact</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">u_test</span><span class="p">,</span> <span class="n">numerical_quantiles</span><span class="p">,</span> <span class="n">exact_quantiles</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  F‚Åª¬π(</span><span class="si">{</span><span class="n">u</span><span class="si">}</span><span class="s2">) = </span><span class="si">{</span><span class="n">num</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (numerical) vs </span><span class="si">{</span><span class="n">exact</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (exact)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Computational cost</strong>: Brent‚Äôs method typically requires <span class="math notranslate nohighlight">\(O(\log(1/\epsilon))\)</span> iterations to achieve tolerance <span class="math notranslate nohighlight">\(\epsilon\)</span>, with each iteration evaluating the CDF once. For normal samples, this is about 10-20 CDF evaluations per sample‚Äîmuch slower than the specialized methods we‚Äôll see later.</p>
</section>
<section id="when-numerical-inversion-makes-sense">
<h3>When Numerical Inversion Makes Sense<a class="headerlink" href="#when-numerical-inversion-makes-sense" title="Link to this heading">ÔÉÅ</a></h3>
<p>Numerical inversion is practical when:</p>
<ol class="arabic simple">
<li><p><strong>You need only a few samples</strong>: Setup costs of specialized methods may dominate.</p></li>
<li><p><strong>The distribution is unusual</strong>: Custom distributions without known fast samplers.</p></li>
<li><p><strong>Accuracy is paramount</strong>: Numerical inversion can achieve arbitrary precision.</p></li>
</ol>
<p>Numerical inversion is impractical when:</p>
<ol class="arabic simple">
<li><p><strong>You need many samples</strong>: The per-sample cost adds up.</p></li>
<li><p><strong>A fast specialized method exists</strong>: Box-Muller for normals, ratio-of-uniforms for gammas.</p></li>
<li><p><strong>The CDF is expensive to evaluate</strong>: Each sample requires multiple CDF calls.</p></li>
</ol>
<p><strong>Rule of thumb</strong>: For standard distributions, use library implementations (<code class="docutils literal notranslate"><span class="pre">scipy.stats</span></code>, NumPy). These use the most efficient method known for each distribution. Reserve numerical inversion for custom or unusual distributions when other methods fail.</p>
</section>
</section>
<section id="discrete-distributions">
<h2>Discrete Distributions<a class="headerlink" href="#discrete-distributions" title="Link to this heading">ÔÉÅ</a></h2>
<p>For discrete distributions taking values <span class="math notranslate nohighlight">\(x_1 &lt; x_2 &lt; \cdots &lt; x_K\)</span> with probabilities <span class="math notranslate nohighlight">\(p_1, p_2, \ldots, p_K\)</span>, the inverse CDF method still applies. The CDF is now a step function:</p>
<div class="math notranslate nohighlight">
\[F(x) = \sum_{x_i \leq x} p_i\]</div>
<p>and the generalized inverse sets <span class="math notranslate nohighlight">\(X = x_k\)</span> when <span class="math notranslate nohighlight">\(F(x_{k-1}) &lt; U \leq F(x_k)\)</span>, where <span class="math notranslate nohighlight">\(F(x_0) = 0\)</span>.</p>
<p>The challenge is computational: <strong>how do we efficiently find which ‚Äústep‚Äù of the CDF our uniform value :math:`U` lands on?</strong></p>
<p>We present five algorithms with different complexity trade-offs, beginning with the simplest and progressing to constant-time methods:</p>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_3_fig02_discrete_sampling.png"><img alt="Comparison of linear search, binary search, and alias method for discrete sampling" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_3_fig02_discrete_sampling.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.3 </span><span class="caption-text"><strong>Discrete Sampling Algorithms: Overview.</strong> Top row: Visual illustration of the three fundamental approaches. Linear search scans sequentially until cumulative probability exceeds <span class="math notranslate nohighlight">\(U\)</span>. Binary search bisects the CDF in <span class="math notranslate nohighlight">\(O(\log K)\)</span> steps. The alias method constructs equal-height columns allowing <span class="math notranslate nohighlight">\(O(1)\)</span> lookup. Bottom row: Practical performance analysis for <span class="math notranslate nohighlight">\(K=1000\)</span> categories, showing when each method dominates. Additional algorithms (interpolation search, exponential doubling) are discussed below.</span><a class="headerlink" href="#id4" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<section id="linear-search-the-baseline">
<h3>Linear Search: The Baseline<a class="headerlink" href="#linear-search-the-baseline" title="Link to this heading">ÔÉÅ</a></h3>
<p>The simplest approach is <strong>linear search</strong>: scan through outcomes in order until the cumulative probability exceeds <span class="math notranslate nohighlight">\(U\)</span>.</p>
<p><strong>Algorithm: Linear Search</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input: Probabilities (p‚ÇÅ, ..., p‚Çñ), uniform value U
Output: Sample X

1. Set cumsum = 0
2. For k = 1, 2, ..., K:
   a. cumsum = cumsum + p‚Çñ
   b. If U ‚â§ cumsum, return x‚Çñ
3. Return x‚Çñ (fallback for numerical edge cases)
</pre></div>
</div>
<p><strong>Complexity</strong>: <span class="math notranslate nohighlight">\(O(K)\)</span> per sample in the worst case (uniform distribution), <span class="math notranslate nohighlight">\(O(1)\)</span> in the best case (all mass on <span class="math notranslate nohighlight">\(x_1\)</span>).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">linear_search_sample</span><span class="p">(</span><span class="n">pmf</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">u</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sample from discrete distribution via linear search.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    pmf : array</span>
<span class="sd">        Probability mass function (must sum to 1).</span>
<span class="sd">    values : array</span>
<span class="sd">        Possible outcomes.</span>
<span class="sd">    u : float</span>
<span class="sd">        Uniform random value.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    Sampled value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cumsum</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">pmf</span><span class="p">):</span>
        <span class="n">cumsum</span> <span class="o">+=</span> <span class="n">p</span>
        <span class="k">if</span> <span class="n">u</span> <span class="o">&lt;=</span> <span class="n">cumsum</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">values</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">values</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Fallback</span>

<span class="c1"># Example: biased die</span>
<span class="n">pmf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span>  <span class="c1"># Heavy on 5, 6</span>
<span class="n">values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="p">[</span><span class="n">linear_search_sample</span><span class="p">(</span><span class="n">pmf</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">())</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sample frequencies:&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">minlength</span><span class="o">=</span><span class="mi">7</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">/</span> <span class="mi">10000</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True probabilities:&quot;</span><span class="p">,</span> <span class="n">pmf</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="binary-search-logarithmic-time">
<h3>Binary Search: Logarithmic Time<a class="headerlink" href="#binary-search-logarithmic-time" title="Link to this heading">ÔÉÅ</a></h3>
<p>When <span class="math notranslate nohighlight">\(K\)</span> is large, linear search becomes slow. We can exploit the monotonicity of the CDF to use <strong>binary search</strong>, reducing complexity to <span class="math notranslate nohighlight">\(O(\log K)\)</span>.</p>
<p>The idea: precompute the cumulative probabilities <span class="math notranslate nohighlight">\(F_k = \sum_{i=1}^k p_i\)</span>, then use binary search to find the smallest <span class="math notranslate nohighlight">\(k\)</span> such that <span class="math notranslate nohighlight">\(F_k \geq U\)</span>.</p>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide013_img001_50c6995d.png"><img alt="Binary search tree for discrete distribution sampling" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide013_img001_50c6995d.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.4 </span><span class="caption-text"><strong>Binary Search for Discrete Sampling.</strong> For a U-shaped distribution over <span class="math notranslate nohighlight">\(K = 16\)</span> categories, the binary search tree (center) organizes cumulative probabilities at each node. Left panel: The CDF as a step function, with horizontal lines at key thresholds. Right panel: A complete search trace for <span class="math notranslate nohighlight">\(U = 0.885\)</span>. Starting at the root (<span class="math notranslate nohighlight">\(k=8, F[8]=0.500\)</span>), we go right (since <span class="math notranslate nohighlight">\(U &gt; 0.500\)</span>), then left, then right, then left, arriving at <span class="math notranslate nohighlight">\(k=11\)</span> in 4 steps. The algorithm pseudocode (left box) shows the standard binary search pattern.</span><a class="headerlink" href="#id5" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<p><strong>Algorithm: Binary Search</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input: Cumulative probabilities (F‚ÇÅ, ..., F‚Çñ), uniform value U
Output: Sample index k

1. Set lo = 0, hi = K
2. While lo &lt; hi:
   a. mid = (lo + hi) / 2
   b. If U ‚â§ F[mid], set hi = mid
   c. Else set lo = mid + 1
3. Return lo
</pre></div>
</div>
<p><strong>Complexity</strong>: <span class="math notranslate nohighlight">\(O(\log K)\)</span> per sample, <span class="math notranslate nohighlight">\(O(K)\)</span> preprocessing to compute cumulative sums.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">DiscreteDistributionBinarySearch</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Discrete distribution sampler using binary search.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pmf</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize with probability mass function.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        pmf : array</span>
<span class="sd">            Probabilities (will be normalized if needed).</span>
<span class="sd">        values : array, optional</span>
<span class="sd">            Outcome values. Defaults to 0, 1, ..., K-1.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pmf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">pmf</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pmf</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cdf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pmf</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">values</span> <span class="o">=</span> <span class="n">values</span> <span class="k">if</span> <span class="n">values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pmf</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pmf</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate samples using binary search.&quot;&quot;&quot;</span>
        <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

        <span class="c1"># np.searchsorted finds insertion point - exactly what we need</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">searchsorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cdf</span><span class="p">,</span> <span class="n">U</span><span class="p">,</span> <span class="n">side</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">)</span>

        <span class="c1"># Clip to valid range (handles floating-point edge cases where U &gt;= cdf[-1])</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>

<span class="c1"># Example: Zipf distribution (heavy-tailed)</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">s</span> <span class="o">=</span> <span class="mf">2.0</span>  <span class="c1"># Zipf exponent</span>
<span class="n">pmf</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">K</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="n">s</span>
<span class="n">pmf</span> <span class="o">/=</span> <span class="n">pmf</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="n">dist</span> <span class="o">=</span> <span class="n">DiscreteDistributionBinarySearch</span><span class="p">(</span><span class="n">pmf</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(X = 1) = </span><span class="si">{</span><span class="n">pmf</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, sample frequency = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samples</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(X = 2) = </span><span class="si">{</span><span class="n">pmf</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, sample frequency = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samples</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>NumPy‚Äôs <code class="docutils literal notranslate"><span class="pre">np.searchsorted</span></code> implements binary search efficiently, making this approach practical for distributions with thousands or millions of outcomes.</p>
</section>
<section id="interpolation-search-exploiting-structure">
<h3>Interpolation Search: Exploiting Structure<a class="headerlink" href="#interpolation-search-exploiting-structure" title="Link to this heading">ÔÉÅ</a></h3>
<p>Binary search treats the CDF as a black box, ignoring its shape. But if we know the CDF is approximately linear (as for near-uniform distributions), we can do better. <strong>Interpolation search</strong> guesses where in the array the target value might be, based on linear interpolation.</p>
<figure class="align-center" id="id6">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide016_img002_4e0de5c4.png"><img alt="Interpolation search for uniform distribution" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide016_img002_4e0de5c4.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.5 </span><span class="caption-text"><strong>Interpolation Search: Near-Uniform Distribution.</strong> For distributions where probability mass is spread relatively evenly, interpolation search uses linear interpolation to guess the target index. Here with <span class="math notranslate nohighlight">\(K = 16\)</span> categories and <span class="math notranslate nohighlight">\(u = 0.420\)</span>, the algorithm finds the answer in just 2 steps by exploiting the near-linear CDF structure.</span><a class="headerlink" href="#id6" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<p><strong>The Key Insight</strong></p>
<p>Rather than always probing the middle of the search interval, interpolation search estimates <em>where</em> the target should be based on the CDF values at the interval endpoints:</p>
<div class="math notranslate nohighlight">
\[\text{guess} = \text{low} + \left\lfloor \frac{u - F[\text{low}]}{F[\text{high}] - F[\text{low}]} \cdot (\text{high} - \text{low}) \right\rfloor\]</div>
<p>This formula computes what fraction of the way through the probability range our target <span class="math notranslate nohighlight">\(u\)</span> lies, then maps that fraction to the index range.</p>
<p><strong>Algorithm: Interpolation Search</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input: Cumulative probabilities (F‚ÇÅ, ..., F‚Çñ), uniform value U
Output: Sample index k

1. Set lo = 1, hi = K, F[0] = 0
2. While lo &lt; hi:
   a. ratio = (U - F[lo-1]) / (F[hi] - F[lo-1])
   b. guess = lo + floor(ratio * (hi - lo))
   c. If U ‚â§ F[guess], set hi = guess
   d. Else set lo = guess + 1
3. Return lo
</pre></div>
</div>
<p><strong>Complexity</strong>:</p>
<ul class="simple">
<li><p><strong>Best case</strong>: <span class="math notranslate nohighlight">\(O(\log \log K)\)</span> for uniform or near-uniform distributions</p></li>
<li><p><strong>Worst case</strong>: <span class="math notranslate nohighlight">\(O(K)\)</span> for highly skewed distributions (where the linear assumption fails badly)</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">DiscreteDistributionInterpolationSearch</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Discrete distribution sampler using interpolation search.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pmf</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize with probability mass function.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        pmf : array</span>
<span class="sd">            Probabilities (will be normalized if needed).</span>
<span class="sd">        values : array, optional</span>
<span class="sd">            Outcome values. Defaults to 0, 1, ..., K-1.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pmf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">pmf</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pmf</span><span class="p">)</span>
        <span class="c1"># Prepend 0 for F[0] = 0 convention</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cdf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pmf</span><span class="p">)])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">values</span> <span class="o">=</span> <span class="n">values</span> <span class="k">if</span> <span class="n">values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pmf</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pmf</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_interpolation_search</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">u</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Find smallest k such that F[k] &gt;= u using interpolation search.&quot;&quot;&quot;</span>
        <span class="n">lo</span><span class="p">,</span> <span class="n">hi</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span>

        <span class="k">while</span> <span class="n">lo</span> <span class="o">&lt;</span> <span class="n">hi</span><span class="p">:</span>
            <span class="c1"># Avoid division by zero</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cdf</span><span class="p">[</span><span class="n">hi</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cdf</span><span class="p">[</span><span class="n">lo</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]:</span>
                <span class="k">return</span> <span class="n">lo</span>

            <span class="c1"># Linear interpolation</span>
            <span class="n">ratio</span> <span class="o">=</span> <span class="p">(</span><span class="n">u</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">cdf</span><span class="p">[</span><span class="n">lo</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cdf</span><span class="p">[</span><span class="n">hi</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">cdf</span><span class="p">[</span><span class="n">lo</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span>
            <span class="n">guess</span> <span class="o">=</span> <span class="n">lo</span> <span class="o">+</span> <span class="nb">int</span><span class="p">(</span><span class="n">ratio</span> <span class="o">*</span> <span class="p">(</span><span class="n">hi</span> <span class="o">-</span> <span class="n">lo</span><span class="p">))</span>

            <span class="c1"># Clamp guess to valid range</span>
            <span class="n">guess</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">lo</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">guess</span><span class="p">,</span> <span class="n">hi</span><span class="p">))</span>

            <span class="k">if</span> <span class="n">u</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cdf</span><span class="p">[</span><span class="n">guess</span><span class="p">]:</span>
                <span class="n">hi</span> <span class="o">=</span> <span class="n">guess</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">lo</span> <span class="o">=</span> <span class="n">guess</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="k">return</span> <span class="n">lo</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate samples using interpolation search.&quot;&quot;&quot;</span>
        <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

        <span class="c1"># Interpolation search for each U value</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_interpolation_search</span><span class="p">(</span><span class="n">u</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="n">U</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>

<span class="c1"># Example: Near-uniform distribution (interpolation search excels)</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">pmf_uniform</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">K</span><span class="p">)</span> <span class="o">/</span> <span class="n">K</span>

<span class="n">dist_interp</span> <span class="o">=</span> <span class="n">DiscreteDistributionInterpolationSearch</span><span class="p">(</span><span class="n">pmf_uniform</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">dist_interp</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">10_000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="p">(</span><span class="n">K</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center" id="id7">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide017_img002_d5767450.png"><img alt="Interpolation search for monotonic distribution" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide017_img002_d5767450.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.6 </span><span class="caption-text"><strong>Interpolation Search: Skewed Distribution.</strong> For a monotonically increasing PMF (more mass on higher categories), interpolation search requires 3 steps to find <span class="math notranslate nohighlight">\(k = 15\)</span> for <span class="math notranslate nohighlight">\(u = 0.780\)</span>. The linear interpolation estimates are less accurate when the CDF curves away from a straight line, but the algorithm still converges correctly.</span><a class="headerlink" href="#id7" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<p><strong>When Interpolation Search Helps</strong></p>
<p>Interpolation search shines when:</p>
<ul class="simple">
<li><p>The distribution is close to uniform: <span class="math notranslate nohighlight">\(O(\log \log K)\)</span> expected comparisons</p></li>
<li><p>The CDF is approximately linear over most of its range</p></li>
<li><p>You need many samples from a fixed distribution (no setup cost beyond computing the CDF)</p></li>
</ul>
<p>Interpolation search struggles when:</p>
<ul class="simple">
<li><p>The distribution is highly skewed (Zipf, geometric): can degrade to <span class="math notranslate nohighlight">\(O(K)\)</span></p></li>
<li><p>The probability mass is concentrated in a few categories</p></li>
<li><p>The CDF has sharp jumps</p></li>
</ul>
<p><strong>Practical recommendation</strong>: For general-purpose discrete sampling, binary search is safer. Use interpolation search only when you know the distribution is nearly uniform and need the extra speed.</p>
</section>
<section id="exponential-doubling-search">
<h3>Exponential Doubling Search<a class="headerlink" href="#exponential-doubling-search" title="Link to this heading">ÔÉÅ</a></h3>
<p><strong>Exponential doubling</strong> (also called galloping or exponential search) combines the best of linear and binary search. It‚Äôs particularly effective when samples tend to fall in the ‚Äúhead‚Äù of the distribution (early categories).</p>
<p><strong>The Idea</strong>: Instead of immediately binary searching the entire array, first find a rough range by doubling the search position until we overshoot, then binary search within that range.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input: Cumulative probabilities (F‚ÇÅ, ..., F‚Çñ), uniform value U
Output: Sample index k

1. If U ‚â§ F[1], return 1
2. Set bound = 1
3. While bound &lt; K and U &gt; F[bound]:
   bound = min(2 * bound, K)
4. Binary search in range [bound/2, bound]
5. Return result
</pre></div>
</div>
<p><strong>Complexity</strong>: <span class="math notranslate nohighlight">\(O(\log k)\)</span> where <span class="math notranslate nohighlight">\(k\)</span> is the result index‚Äîmuch better than <span class="math notranslate nohighlight">\(O(\log K)\)</span> when <span class="math notranslate nohighlight">\(k \ll K\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">exponential_doubling_search</span><span class="p">(</span><span class="n">cdf</span><span class="p">,</span> <span class="n">u</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Find smallest k such that cdf[k] &gt;= u using exponential doubling.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    cdf : array</span>
<span class="sd">        Cumulative probabilities, cdf[0] = 0.</span>
<span class="sd">    u : float</span>
<span class="sd">        Target uniform value.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    int</span>
<span class="sd">        Index k (1-indexed in the probability sense).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cdf</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="n">u</span> <span class="o">&lt;=</span> <span class="n">cdf</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="k">return</span> <span class="mi">1</span>

    <span class="c1"># Exponential doubling to find upper bound</span>
    <span class="n">bound</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">while</span> <span class="n">bound</span> <span class="o">&lt;</span> <span class="n">K</span> <span class="ow">and</span> <span class="n">u</span> <span class="o">&gt;</span> <span class="n">cdf</span><span class="p">[</span><span class="n">bound</span><span class="p">]:</span>
        <span class="n">bound</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">bound</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>

    <span class="c1"># Binary search in [bound//2, bound]</span>
    <span class="n">lo</span> <span class="o">=</span> <span class="n">bound</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="n">hi</span> <span class="o">=</span> <span class="n">bound</span>

    <span class="k">while</span> <span class="n">lo</span> <span class="o">&lt;</span> <span class="n">hi</span><span class="p">:</span>
        <span class="n">mid</span> <span class="o">=</span> <span class="p">(</span><span class="n">lo</span> <span class="o">+</span> <span class="n">hi</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="k">if</span> <span class="n">u</span> <span class="o">&lt;=</span> <span class="n">cdf</span><span class="p">[</span><span class="n">mid</span><span class="p">]:</span>
            <span class="n">hi</span> <span class="o">=</span> <span class="n">mid</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lo</span> <span class="o">=</span> <span class="n">mid</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">lo</span>

<span class="c1"># Example: Head-heavy distribution (exponential doubling excels)</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">2.0</span>
<span class="n">pmf_zipf</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">K</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="n">alpha</span>
<span class="n">pmf_zipf</span> <span class="o">/=</span> <span class="n">pmf_zipf</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">cdf_zipf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pmf_zipf</span><span class="p">)])</span>

<span class="c1"># Most samples will be in early categories</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="n">exponential_doubling_search</span><span class="p">(</span><span class="n">cdf_zipf</span><span class="p">,</span> <span class="n">u</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="n">U</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Median result index: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2"> out of </span><span class="si">{</span><span class="n">K</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;90th percentile: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">results</span><span class="p">,</span><span class="w"> </span><span class="mi">90</span><span class="p">)</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>When to Use Exponential Doubling</strong></p>
<ul class="simple">
<li><p>Head-heavy distributions (Zipf, geometric, Poisson with small Œª)</p></li>
<li><p>When most samples fall in early categories</p></li>
<li><p>Adaptive scenarios where the distribution may change</p></li>
</ul>
<p>The alias method still wins for truly large <span class="math notranslate nohighlight">\(K\)</span> with many samples, but exponential doubling provides a good middle ground with no setup cost.</p>
</section>
<section id="the-alias-method-constant-time">
<h3>The Alias Method: Constant Time<a class="headerlink" href="#the-alias-method-constant-time" title="Link to this heading">ÔÉÅ</a></h3>
<p>Binary search achieves <span class="math notranslate nohighlight">\(O(\log K)\)</span> per sample‚Äîexcellent, but can we do better? The remarkable <strong>alias method</strong>, developed by Walker (1977), achieves <span class="math notranslate nohighlight">\(O(1)\)</span> sampling time after <span class="math notranslate nohighlight">\(O(K)\)</span> preprocessing.</p>
<p>The key insight is to restructure the probability distribution into <span class="math notranslate nohighlight">\(K\)</span> ‚Äúcolumns‚Äù of equal total probability <span class="math notranslate nohighlight">\(1/K\)</span>, where each column contains probability mass from at most two original outcomes. Sampling then requires only:</p>
<ol class="arabic simple">
<li><p>Choose a column uniformly at random: <span class="math notranslate nohighlight">\(O(1)\)</span></p></li>
<li><p>Decide between the two outcomes in that column: <span class="math notranslate nohighlight">\(O(1)\)</span></p></li>
</ol>
<p><strong>The Setup Algorithm</strong></p>
<p>Imagine <span class="math notranslate nohighlight">\(K\)</span> cups, each of capacity <span class="math notranslate nohighlight">\(1/K\)</span>. We pour the probability mass of each outcome into cups, filling some and leaving others partially empty. The alias method pairs each underfilled cup with an overfilled one, ‚Äútopping up‚Äù the underfilled cup with probability mass from the overfilled outcome.</p>
<figure class="align-center" id="id8">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide018_img002_ac0643ba.png"><img alt="Alias method cups visualization" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide018_img002_ac0643ba.png" style="width: 90%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.7 </span><span class="caption-text"><strong>The Alias Method: Balancing Cups.</strong> Top: Original PMF with probabilities 0.1, 0.2, 0.3, 0.4 for categories 1-4. Bottom left: After scaling by <span class="math notranslate nohighlight">\(K=4\)</span>, the heights become 0.4, 0.8, 1.2, 1.6‚Äîsome below the average of 1, others above. Bottom right: The final balanced cups, each with total height 1. Each cup contains its ‚Äúnative‚Äù probability (solid color) plus ‚Äúborrowed‚Äù probability from an overfilled cup (lighter shading with alias label). To sample: pick a cup uniformly, then flip a coin weighted by the cup‚Äôs native vs. borrowed portions.</span><a class="headerlink" href="#id8" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input: Probabilities (p‚ÇÅ, ..., p‚Çñ)
Output: Arrays prob[k] and alias[k]

1. Scale probabilities: q[k] = K * p[k]  (so Œ£q[k] = K)
2. Initialize:
   - small = {k : q[k] &lt; 1}  (underfilled)
   - large = {k : q[k] ‚â• 1}  (overfilled)
   - alias[k] = k for all k
3. While small and large are both non-empty:
   a. Remove j from small (underfilled)
   b. Remove ‚Ñì from large (overfilled)
   c. Set prob[j] = q[j], alias[j] = ‚Ñì  (record j&#39;s native prob and alias)
   d. Set q[‚Ñì] = q[‚Ñì] - (1 - q[j])  (reduce ‚Ñì&#39;s excess)
   e. If q[‚Ñì] &lt; 1, move ‚Ñì to small; else keep in large
4. For any remaining k in small or large, set prob[k] = 1
</pre></div>
</div>
<p><strong>The Sampling Algorithm</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input: Arrays prob[k] and alias[k], K
Output: Sample index

1. Generate U ~ Uniform(0, K)
2. Let i = floor(U), V = U - i  (so V ~ Uniform(0, 1))
3. If V &lt; prob[i], return i
4. Else return alias[i]
</pre></div>
</div>
<p>Both steps are <span class="math notranslate nohighlight">\(O(1)\)</span>, making sampling constant-time regardless of <span class="math notranslate nohighlight">\(K\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">AliasMethod</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Alias method for O(1) discrete distribution sampling.</span>

<span class="sd">    After O(K) setup, each sample takes O(1) time.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pmf</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize alias tables.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        pmf : array-like</span>
<span class="sd">            Probability mass function (will be normalized).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">pmf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">pmf</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pmf</span><span class="p">)</span>
        <span class="n">pmf</span> <span class="o">=</span> <span class="n">pmf</span> <span class="o">/</span> <span class="n">pmf</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

        <span class="c1"># Scale probabilities</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">K</span> <span class="o">*</span> <span class="n">pmf</span>

        <span class="c1"># Initialize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

        <span class="c1"># Separate into small and large (with small epsilon for numerical stability)</span>
        <span class="n">small</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">large</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">q</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="mf">1e-10</span><span class="p">:</span>
                <span class="n">small</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">large</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>

        <span class="c1"># Build tables</span>
        <span class="k">while</span> <span class="n">small</span> <span class="ow">and</span> <span class="n">large</span><span class="p">:</span>
            <span class="n">j</span> <span class="o">=</span> <span class="n">small</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>  <span class="c1"># Underfilled</span>
            <span class="n">ell</span> <span class="o">=</span> <span class="n">large</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>  <span class="c1"># Overfilled</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">prob</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">q</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">alias</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">ell</span>

            <span class="c1"># Transfer mass from ‚Ñì to j</span>
            <span class="n">q</span><span class="p">[</span><span class="n">ell</span><span class="p">]</span> <span class="o">=</span> <span class="n">q</span><span class="p">[</span><span class="n">ell</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">q</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>

            <span class="k">if</span> <span class="n">q</span><span class="p">[</span><span class="n">ell</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="mf">1e-10</span><span class="p">:</span>
                <span class="n">small</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ell</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">large</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ell</span><span class="p">)</span>

        <span class="c1"># Remaining entries get probability 1</span>
        <span class="k">while</span> <span class="n">large</span><span class="p">:</span>
            <span class="n">ell</span> <span class="o">=</span> <span class="n">large</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prob</span><span class="p">[</span><span class="n">ell</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="k">while</span> <span class="n">small</span><span class="p">:</span>
            <span class="n">j</span> <span class="o">=</span> <span class="n">small</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prob</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">=</span> <span class="n">K</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate samples in O(1) per sample.&quot;&quot;&quot;</span>
        <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

        <span class="c1"># Generate uniform values in [0, K)</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span>

        <span class="c1"># Extract integer and fractional parts</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">U</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">U</span> <span class="o">-</span> <span class="n">i</span>  <span class="c1"># V ~ Uniform(0, 1)</span>

        <span class="c1"># Handle edge case where i = K (due to floating point)</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Choose between original and alias</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">V</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">prob</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">i</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alias</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

<span class="c1"># Compare performance</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="n">K</span> <span class="o">=</span> <span class="mi">10_000</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">pmf</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">dirichlet</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">K</span><span class="p">))</span>  <span class="c1"># Random distribution</span>

<span class="c1"># Setup</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
<span class="n">alias_dist</span> <span class="o">=</span> <span class="n">AliasMethod</span><span class="p">(</span><span class="n">pmf</span><span class="p">)</span>
<span class="n">alias_setup</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
<span class="n">binary_dist</span> <span class="o">=</span> <span class="n">DiscreteDistributionBinarySearch</span><span class="p">(</span><span class="n">pmf</span><span class="p">)</span>
<span class="n">binary_setup</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Setup time - Alias: </span><span class="si">{</span><span class="n">alias_setup</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> ms, Binary: </span><span class="si">{</span><span class="n">binary_setup</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> ms&quot;</span><span class="p">)</span>

<span class="c1"># Sampling</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1_000_000</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">alias_dist</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">alias_sample</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">binary_dist</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">binary_sample</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample time (</span><span class="si">{</span><span class="n">n_samples</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">) - Alias: </span><span class="si">{</span><span class="n">alias_sample</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> ms, Binary: </span><span class="si">{</span><span class="n">binary_sample</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> ms&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="performance-comparison">
<h3>Performance Comparison<a class="headerlink" href="#performance-comparison" title="Link to this heading">ÔÉÅ</a></h3>
<p>With five discrete sampling algorithms now in our toolkit‚Äîlinear search, binary search, interpolation search, exponential doubling, and the alias method‚Äîthe natural question is: <strong>which should I use?</strong> The answer depends on distribution shape, size <span class="math notranslate nohighlight">\(K\)</span>, and number of samples needed.</p>
<p>The following benchmarks compare all five methods across four distribution types, measuring both wall-clock time and iteration counts as <span class="math notranslate nohighlight">\(K\)</span> grows from 10 to 100,000.</p>
<p><strong>Tail-Heavy Distribution</strong> (probability increases with category index):</p>
<figure class="align-center" id="id9">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide019_img003_7e5a8c9f.png"><img alt="Performance comparison for tail distribution" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide019_img003_7e5a8c9f.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.8 </span><span class="caption-text"><strong>Tail Distribution Benchmark.</strong> Left: Time per sample (seconds, log scale). Right: Iterations per sample (log scale). For tail-heavy distributions, most samples require searching toward the end of the array. Linear scan (purple) grows as <span class="math notranslate nohighlight">\(O(K)\)</span>. Binary search (orange) and exponential doubling (green) both achieve <span class="math notranslate nohighlight">\(O(\log K)\)</span> but with different constants. Interpolation search (red) also grows logarithmically. The alias method (blue) maintains <span class="math notranslate nohighlight">\(O(1)\)</span> regardless of <span class="math notranslate nohighlight">\(K\)</span>.</span><a class="headerlink" href="#id9" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<p><strong>Head-Heavy Distribution</strong> (Zipf-like, probability decreases with category index):</p>
<figure class="align-center" id="id10">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide019_img004_b8994dbd.png"><img alt="Performance comparison for head-heavy distribution" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide019_img004_b8994dbd.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.9 </span><span class="caption-text"><strong>Head-Heavy Distribution Benchmark.</strong> When probability mass concentrates in early categories, linear scan often terminates quickly (nearly constant iterations in the right panel). Exponential doubling excels here‚Äîits <span class="math notranslate nohighlight">\(O(\log k)\)</span> complexity (where <span class="math notranslate nohighlight">\(k\)</span> is the <em>result</em> index) means it rarely explores deep into the array. Interpolation search struggles because the CDF is highly nonlinear. The alias method remains constant.</span><a class="headerlink" href="#id10" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<p><strong>Symmetric Distribution</strong> (U-shaped, mass at both ends):</p>
<figure class="align-center" id="id11">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide020_img004_f6113b1d.png"><img alt="Performance comparison for symmetric distribution" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide020_img004_f6113b1d.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.10 </span><span class="caption-text"><strong>Symmetric Distribution Benchmark.</strong> A U-shaped distribution with mass at both head and tail represents a middle ground. Linear scan performs poorly (must traverse to the tail half the time). Binary search and exponential doubling both achieve <span class="math notranslate nohighlight">\(O(\log K)\)</span>. The alias method‚Äôs constant time becomes increasingly advantageous as <span class="math notranslate nohighlight">\(K\)</span> grows.</span><a class="headerlink" href="#id11" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<p><strong>Uniform Distribution</strong> (all categories equally likely):</p>
<figure class="align-center" id="id12">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide020_img005_f28deb1f.png"><img alt="Performance comparison for uniform distribution" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide020_img005_f28deb1f.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.11 </span><span class="caption-text"><strong>Uniform Distribution Benchmark.</strong> This is interpolation search‚Äôs best case‚Äîthe nearly linear CDF allows it to converge in <span class="math notranslate nohighlight">\(O(\log \log K)\)</span> iterations (notice the nearly flat red line in the right panel). For very large <span class="math notranslate nohighlight">\(K\)</span>, interpolation search can outperform even binary search in iteration count, though per-iteration overhead may offset this advantage in wall-clock time.</span><a class="headerlink" href="#id12" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<p><strong>Summary: Choosing the Right Algorithm</strong></p>
<table class="docutils align-default" id="id13">
<caption><span class="caption-number">Table 2.2 </span><span class="caption-text">Discrete Sampling Algorithm Selection Guide</span><a class="headerlink" href="#id13" title="Link to this table">ÔÉÅ</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 15.0%" />
<col style="width: 15.0%" />
<col style="width: 15.0%" />
<col style="width: 35.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Algorithm</p></th>
<th class="head"><p>Setup</p></th>
<th class="head"><p>Per-Sample</p></th>
<th class="head"><p>Best For</p></th>
<th class="head"><p>Avoid When</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Linear scan</p></td>
<td><p><span class="math notranslate nohighlight">\(O(1)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(K)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(K &lt; 20\)</span>, extreme head-heavy</p></td>
<td><p><span class="math notranslate nohighlight">\(K &gt; 100\)</span>, tail-heavy</p></td>
</tr>
<tr class="row-odd"><td><p>Binary search</p></td>
<td><p><span class="math notranslate nohighlight">\(O(K)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(\log K)\)</span></p></td>
<td><p>General purpose, changing distributions</p></td>
<td><p>Fixed distribution with <span class="math notranslate nohighlight">\(n \gg K\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Interpolation</p></td>
<td><p><span class="math notranslate nohighlight">\(O(K)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(\log\log K)\)</span> to <span class="math notranslate nohighlight">\(O(K)\)</span></p></td>
<td><p>Near-uniform distributions</p></td>
<td><p>Skewed distributions</p></td>
</tr>
<tr class="row-odd"><td><p>Exp. doubling</p></td>
<td><p><span class="math notranslate nohighlight">\(O(K)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(\log k)\)</span></p></td>
<td><p>Head-heavy, unknown structure</p></td>
<td><p>Known uniform structure</p></td>
</tr>
<tr class="row-even"><td><p>Alias method</p></td>
<td><p><span class="math notranslate nohighlight">\(O(K)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(1)\)</span></p></td>
<td><p>Many samples from fixed distribution</p></td>
<td><p>Changing distributions, small <span class="math notranslate nohighlight">\(n\)</span></p></td>
</tr>
</tbody>
</table>
<p><strong>Practical recommendations</strong>:</p>
<ul class="simple">
<li><p><strong>Default choice</strong>: Binary search via <code class="docutils literal notranslate"><span class="pre">np.searchsorted</span></code>‚Äîreliable <span class="math notranslate nohighlight">\(O(\log K)\)</span> with minimal code</p></li>
<li><p><strong>Head-heavy distributions</strong> (Zipf, geometric): Exponential doubling or linear scan</p></li>
<li><p><strong>Near-uniform distributions</strong>: Consider interpolation search if <span class="math notranslate nohighlight">\(K\)</span> is very large</p></li>
<li><p><strong>Fixed distribution, many samples</strong>: Alias method pays back its setup cost when <span class="math notranslate nohighlight">\(n &gt; O(K)\)</span></p></li>
<li><p><strong>Very small</strong> <span class="math notranslate nohighlight">\(K &lt; 20\)</span>: Linear scan‚Äôs simplicity often wins</p></li>
</ul>
</section>
</section>
<section id="mixed-distributions">
<h2>Mixed Distributions<a class="headerlink" href="#mixed-distributions" title="Link to this heading">ÔÉÅ</a></h2>
<p>Real-world data often follow <strong>mixed distributions</strong> combining discrete point masses and continuous components. The inverse CDF method handles these naturally.</p>
<section id="zero-inflated-distributions">
<h3>Zero-Inflated Distributions<a class="headerlink" href="#zero-inflated-distributions" title="Link to this heading">ÔÉÅ</a></h3>
<p>A common example is the <strong>zero-inflated distribution</strong>: with probability <span class="math notranslate nohighlight">\(p_0\)</span>, the outcome is exactly zero; otherwise, it follows a continuous distribution <span class="math notranslate nohighlight">\(F_{\text{cont}}\)</span>.</p>
<figure class="align-center" id="id14">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide025_img002_fb46d81c.png"><img alt="Zero-inflated exponential PDF and CDF" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide025_img002_fb46d81c.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.12 </span><span class="caption-text"><strong>Zero-Inflated Exponential: Structure.</strong> Left: The PDF/PMF combines a point mass at zero (red arrow, probability <span class="math notranslate nohighlight">\(p_0 = 0.3\)</span>) with a scaled exponential density <span class="math notranslate nohighlight">\((1-p_0)\lambda e^{-\lambda x}\)</span> for <span class="math notranslate nohighlight">\(x &gt; 0\)</span>. Right: The CDF has a jump discontinuity at zero‚Äîit jumps from <span class="math notranslate nohighlight">\(F(0^-) = 0\)</span> to <span class="math notranslate nohighlight">\(F(0) = p_0\)</span>, then grows continuously following the exponential CDF scaled to the interval <span class="math notranslate nohighlight">\([p_0, 1]\)</span>.</span><a class="headerlink" href="#id14" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<p>The CDF of a zero-inflated distribution is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}F(x) = \begin{cases}
0 &amp; x &lt; 0 \\
p_0 + (1 - p_0) F_{\text{cont}}(x) &amp; x \geq 0
\end{cases}\end{split}\]</div>
<figure class="align-center" id="id15">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_3_fig03_mixed_distribution.png"><img alt="Zero-inflated exponential distribution showing PDF, CDF, and sampling" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_3_fig03_mixed_distribution.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.13 </span><span class="caption-text"><strong>Mixed Distributions: Sampling Verification.</strong> Left: The zero-inflated exponential has a point mass at zero (probability <span class="math notranslate nohighlight">\(p_0 = 0.3\)</span>) plus a continuous exponential component. Center: The CDF has a jump discontinuity of size <span class="math notranslate nohighlight">\(p_0\)</span> at zero, then grows continuously. Right: Sampling demonstration with the continuous part shown as a histogram matching the theoretical density. The inset bar chart confirms that the proportion of exact zeros (30.4%) closely matches the theoretical 30%.</span><a class="headerlink" href="#id15" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<p><strong>Inverse CDF for Zero-Inflated Exponential</strong>:</p>
<p>For <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0, 1)\)</span>:</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(U \leq p_0\)</span>: return <span class="math notranslate nohighlight">\(X = 0\)</span></p></li>
<li><p>Else: transform <span class="math notranslate nohighlight">\(U\)</span> to the continuous part and apply the exponential inverse</p></li>
</ul>
<p>The transformation for the continuous part rescales <span class="math notranslate nohighlight">\(U\)</span> from <span class="math notranslate nohighlight">\([p_0, 1]\)</span> to <span class="math notranslate nohighlight">\([0, 1]\)</span>:</p>
<div class="math notranslate nohighlight">
\[U_{\text{cont}} = \frac{U - p_0}{1 - p_0}\]</div>
<p>Then <span class="math notranslate nohighlight">\(X = F_{\text{cont}}^{-1}(U_{\text{cont}})\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">zero_inflated_exponential</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">p_zero</span><span class="p">,</span> <span class="n">rate</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sample from zero-inflated exponential via inverse CDF.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    u : float or array</span>
<span class="sd">        Uniform random value(s).</span>
<span class="sd">    p_zero : float</span>
<span class="sd">        Probability of structural zero.</span>
<span class="sd">    rate : float</span>
<span class="sd">        Rate of exponential component.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    float or array</span>
<span class="sd">        Sample(s) from the zero-inflated distribution.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>

    <span class="c1"># Points above p_zero come from the exponential</span>
    <span class="n">continuous_mask</span> <span class="o">=</span> <span class="n">u</span> <span class="o">&gt;</span> <span class="n">p_zero</span>

    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">continuous_mask</span><span class="p">):</span>
        <span class="c1"># Rescale U from [p_zero, 1] to [0, 1]</span>
        <span class="n">u_scaled</span> <span class="o">=</span> <span class="p">(</span><span class="n">u</span><span class="p">[</span><span class="n">continuous_mask</span><span class="p">]</span> <span class="o">-</span> <span class="n">p_zero</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_zero</span><span class="p">)</span>
        <span class="c1"># Guard against log(0)</span>
        <span class="n">u_scaled</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">u_scaled</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">)</span>
        <span class="c1"># Apply exponential inverse CDF using tail-stable form</span>
        <span class="n">result</span><span class="p">[</span><span class="n">continuous_mask</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">u_scaled</span><span class="p">)</span> <span class="o">/</span> <span class="n">rate</span>

    <span class="k">return</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">result</span>

<span class="c1"># Verify</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="mi">100_000</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">zero_inflated_exponential</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">p_zero</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">prop_zeros</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samples</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">mean_nonzero</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="n">samples</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Proportion of zeros: </span><span class="si">{</span><span class="n">prop_zeros</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: 0.3000)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean of non-zeros: </span><span class="si">{</span><span class="n">mean_nonzero</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: 2.0000)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="general-mixed-distributions">
<h3>General Mixed Distributions<a class="headerlink" href="#general-mixed-distributions" title="Link to this heading">ÔÉÅ</a></h3>
<p>More generally, a mixed distribution might have <span class="math notranslate nohighlight">\(m\)</span> point masses at <span class="math notranslate nohighlight">\(a_1, \ldots, a_m\)</span> with probabilities <span class="math notranslate nohighlight">\(\pi_1, \ldots, \pi_m\)</span>, plus a continuous component <span class="math notranslate nohighlight">\(F_{\text{cont}}\)</span> with weight <span class="math notranslate nohighlight">\(\pi_{\text{cont}} = 1 - \sum_i \pi_i\)</span>.</p>
<p>The inverse CDF method partitions <span class="math notranslate nohighlight">\([0, 1]\)</span> accordingly:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">mixed_distribution_sample</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">point_masses</span><span class="p">,</span> <span class="n">point_probs</span><span class="p">,</span>
                               <span class="n">continuous_inverse_cdf</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sample from a mixed discrete-continuous distribution.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    u : float</span>
<span class="sd">        Uniform random value.</span>
<span class="sd">    point_masses : array</span>
<span class="sd">        Values with point mass.</span>
<span class="sd">    point_probs : array</span>
<span class="sd">        Probabilities of point masses.</span>
<span class="sd">    continuous_inverse_cdf : callable</span>
<span class="sd">        Inverse CDF of the continuous component.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    float</span>
<span class="sd">        Sample from the mixed distribution.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cumprob</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="c1"># Check point masses</span>
    <span class="k">for</span> <span class="n">mass</span><span class="p">,</span> <span class="n">prob</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">point_masses</span><span class="p">,</span> <span class="n">point_probs</span><span class="p">):</span>
        <span class="n">cumprob</span> <span class="o">+=</span> <span class="n">prob</span>
        <span class="k">if</span> <span class="n">u</span> <span class="o">&lt;=</span> <span class="n">cumprob</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">mass</span>

    <span class="c1"># Continuous component</span>
    <span class="c1"># Rescale U to [0, 1] for the continuous part</span>
    <span class="n">u_cont</span> <span class="o">=</span> <span class="p">(</span><span class="n">u</span> <span class="o">-</span> <span class="n">cumprob</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">cumprob</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">continuous_inverse_cdf</span><span class="p">(</span><span class="n">u_cont</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="practical-considerations">
<h2>Practical Considerations<a class="headerlink" href="#practical-considerations" title="Link to this heading">ÔÉÅ</a></h2>
<section id="numerical-precision">
<h3>Numerical Precision<a class="headerlink" href="#numerical-precision" title="Link to this heading">ÔÉÅ</a></h3>
<p>Several numerical issues can arise with the inverse CDF method. Understanding and mitigating them is essential for reliable implementations.</p>
<figure class="align-center" id="id16">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_3_fig04_numerical_precision.png"><img alt="Numerical precision issues in inverse CDF implementation" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_3_fig04_numerical_precision.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.14 </span><span class="caption-text"><strong>Numerical Precision Issues.</strong> Left: Two equivalent formulas for exponential sampling‚Äî<span class="math notranslate nohighlight">\(-\ln(U)\)</span> and <span class="math notranslate nohighlight">\(-\ln(1-U)\)</span>‚Äîbehave differently at the boundaries; <span class="math notranslate nohighlight">\(-\ln(U)\)</span> produces large samples for small <span class="math notranslate nohighlight">\(U\)</span> without precision issues. Center: Catastrophic cancellation when computing <span class="math notranslate nohighlight">\(1-U\)</span> for <span class="math notranslate nohighlight">\(U\)</span> near 1; each decimal digit of precision needed consumes one of float64‚Äôs ~16 available digits. Right: Explanation of why the <span class="math notranslate nohighlight">\(-\ln(U)\)</span> form is preferred‚Äîit places large samples where floating point has finer resolution.</span><a class="headerlink" href="#id16" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<p><strong>1. Boundary Handling</strong></p>
<p>When <span class="math notranslate nohighlight">\(U\)</span> is very close to 0 or 1, <span class="math notranslate nohighlight">\(F^{-1}(U)\)</span> may produce extreme values or overflow. For instance, <span class="math notranslate nohighlight">\(-\ln(U) \to \infty\)</span> as <span class="math notranslate nohighlight">\(U \to 0\)</span>.</p>
<p><strong>Mitigation</strong>: Clamp <span class="math notranslate nohighlight">\(U\)</span> away from exact 0. For tail-stable forms like <span class="math notranslate nohighlight">\(-\log(U)\)</span>, use the smallest positive float to maximize tail reach:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">safe_uniform</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate uniform values safely bounded away from 0.&quot;&quot;&quot;</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
    <span class="c1"># Use smallest positive float to maximize tail reach</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>2. Logarithms, (1-u), and What log1p Actually Fixes</strong></p>
<p>Many inverse CDF formulas naturally appear in the form <span class="math notranslate nohighlight">\(\ln(1-u)\)</span> because they derive from the survival function. For example, for <span class="math notranslate nohighlight">\(X \sim \text{Exponential}(\lambda)\)</span>:</p>
<div class="math notranslate nohighlight">
\[X = -\frac{\ln(1-U)}{\lambda}\]</div>
<p>In exact arithmetic this is perfectly fine. In floating-point arithmetic there are <em>two</em> separate concerns:</p>
<p><strong>(a) Cancellation in forming</strong> <span class="math notranslate nohighlight">\(1-U\)</span>. When <span class="math notranslate nohighlight">\(U\)</span> is close to 1, the quantity <span class="math notranslate nohighlight">\(1-U\)</span> is small, and the subtraction can lose significant digits. The function <code class="docutils literal notranslate"><span class="pre">log1p(x)</span></code> computes <span class="math notranslate nohighlight">\(\ln(1+x)\)</span> in a way that avoids the naive, accuracy-losing evaluation of <code class="docutils literal notranslate"><span class="pre">log(1+x)</span></code> when <code class="docutils literal notranslate"><span class="pre">1+x</span></code> is close to 1. Therefore <code class="docutils literal notranslate"><span class="pre">np.log1p(-u)</span></code> <em>does</em> improve accuracy for <span class="math notranslate nohighlight">\(\ln(1-u)\)</span> when <span class="math notranslate nohighlight">\(u\)</span> is close to 1, because it avoids catastrophic cancellation in forming <span class="math notranslate nohighlight">\(1-u\)</span>.</p>
<p><strong>(b) Tail resolution near 1 (the bigger practical problem)</strong>. Even if you compute <span class="math notranslate nohighlight">\(\ln(1-U)\)</span> accurately via <code class="docutils literal notranslate"><span class="pre">log1p</span></code>, sampling large exponential values requires <span class="math notranslate nohighlight">\(U\)</span> extremely close to 1. But IEEE-754 doubles are <em>coarsely spaced</em> near 1. The smallest positive gap near 1 in float64 is on the order of <span class="math notranslate nohighlight">\(10^{-16}\)</span>, so:</p>
<div class="math notranslate nohighlight">
\[-\ln(1-U) \text{ is effectively capped around } 36\text{‚Äì}37\]</div>
<p>This effectively truncates the far right tail and produces too few extreme values. This is a <em>representational/granularity</em> limitation that <code class="docutils literal notranslate"><span class="pre">log1p</span></code> cannot fix‚Äîit‚Äôs not a cancellation issue, it‚Äôs that the RNG cannot produce <span class="math notranslate nohighlight">\(U\)</span> values close enough to 1.</p>
<p><strong>Best Practice for Exponential-Type Inverses</strong></p>
<p>Because <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(1-U\)</span> have the same Uniform(0,1) distribution, rewrite the inverse in the equivalent form:</p>
<div class="math notranslate nohighlight">
\[X = -\frac{\ln(U)}{\lambda}\]</div>
<p>Now large <span class="math notranslate nohighlight">\(X\)</span> corresponds to <span class="math notranslate nohighlight">\(U\)</span> near 0, where floating point has much finer resolution (and can represent much smaller positive numbers). This preserves tail behavior far better in finite-precision implementations.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">exponential_inverse_cdf</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">rate</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential(rate) via inverse CDF, tail-stable form.</span>

<span class="sd">    Uses X = -log(U)/rate and avoids forming (1-U).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="c1"># Guard against log(0) - use smallest positive float for max tail reach</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">u</span><span class="p">)</span> <span class="o">/</span> <span class="n">rate</span>
</pre></div>
</div>
<p><strong>When log1p Is the Right Tool</strong></p>
<p>Use <code class="docutils literal notranslate"><span class="pre">log1p</span></code> when:</p>
<ul class="simple">
<li><p>You genuinely need <span class="math notranslate nohighlight">\(\ln(1-u)\)</span> and cannot use the symmetry trick (e.g., certain non-exponential-type distributions)</p></li>
<li><p>You‚Äôre computing logit: <code class="docutils literal notranslate"><span class="pre">np.log(u)</span> <span class="pre">-</span> <span class="pre">np.log1p(-u)</span></code> is more stable than <code class="docutils literal notranslate"><span class="pre">np.log(u</span> <span class="pre">/</span> <span class="pre">(1</span> <span class="pre">-</span> <span class="pre">u))</span></code></p></li>
<li><p>You‚Äôre evaluating log-survival terms where the survival function <span class="math notranslate nohighlight">\(S(x) = 1 - F(x)\)</span> is near 1</p></li>
</ul>
<p>But <code class="docutils literal notranslate"><span class="pre">log1p(-u)</span></code> does <em>not</em> solve the tail granularity problem‚Äîit only addresses subtraction accuracy. For distributions where you can switch from <code class="docutils literal notranslate"><span class="pre">log(1-u)</span></code> to <code class="docutils literal notranslate"><span class="pre">log(u)</span></code> by symmetry (exponential, Weibull, Pareto), the symmetry-based rewrite is the most robust solution.</p>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ‚ö†Ô∏è</p>
<p><strong>Two distinct issues, two distinct fixes</strong>: (1) <em>Cancellation</em> when forming <span class="math notranslate nohighlight">\(1-U\)</span> for <span class="math notranslate nohighlight">\(U \approx 1\)</span>‚Äî<code class="docutils literal notranslate"><span class="pre">log1p(-u)</span></code> helps here. (2) <em>Tail granularity</em> because the RNG cannot produce <span class="math notranslate nohighlight">\(U\)</span> close enough to 1‚Äî<code class="docutils literal notranslate"><span class="pre">log1p</span></code> does <strong>not</strong> help here; the fix is to use <span class="math notranslate nohighlight">\(-\ln(U)\)</span> so large samples correspond to small <span class="math notranslate nohighlight">\(U\)</span>, where floats have finer spacing.</p>
</div>
<p><strong>3. CDF Evaluation Accuracy</strong></p>
<p>For numerical inversion, the CDF must be evaluated accurately throughout its domain. In the tails, naive implementations may suffer from cancellation or underflow.</p>
<p><strong>Example</strong>: The normal CDF <span class="math notranslate nohighlight">\(\Phi(x)\)</span> for <span class="math notranslate nohighlight">\(x = 8\)</span> is about <span class="math notranslate nohighlight">\(1 - 6 \times 10^{-16}\)</span>‚Äîindistinguishable from 1 in double precision. Libraries like SciPy use special functions and asymptotic expansions for accuracy in the tails.</p>
</section>
<section id="when-not-to-use-inverse-cdf">
<h3>When Not to Use Inverse CDF<a class="headerlink" href="#when-not-to-use-inverse-cdf" title="Link to this heading">ÔÉÅ</a></h3>
<p>The inverse CDF method is not always the best choice:</p>
<p><strong>1. No Closed-Form Inverse</strong></p>
<p>The normal distribution has no closed-form inverse CDF. While numerical inversion works, specialized methods like <strong>Box-Muller</strong> (Section 2.4) are faster:</p>
<div class="math notranslate nohighlight">
\[Z_1 = \sqrt{-2 \ln U_1} \cos(2\pi U_2), \quad
Z_2 = \sqrt{-2 \ln U_1} \sin(2\pi U_2)\]</div>
<p>produces two independent <span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span> samples from two uniform samples‚Äîwith only logarithms and trigonometric functions, no root-finding.</p>
<p><strong>2. Multivariate Distributions</strong></p>
<p>The inverse CDF method extends awkwardly to multiple dimensions. For a random vector <span class="math notranslate nohighlight">\(\mathbf{X} = (X_1, \ldots, X_d)\)</span>, we would need to:</p>
<ol class="arabic simple">
<li><p>Sample <span class="math notranslate nohighlight">\(X_1\)</span> from its marginal</p></li>
<li><p>Sample <span class="math notranslate nohighlight">\(X_2\)</span> from its conditional given <span class="math notranslate nohighlight">\(X_1\)</span></p></li>
<li><p>Continue sequentially‚Ä¶</p></li>
</ol>
<p>This requires knowing all conditional distributions, which is often impractical. <strong>Rejection sampling</strong> and <strong>MCMC</strong> (Part 3) are better suited to multivariate problems.</p>
<p><strong>3. Complex Dependencies</strong></p>
<p>For distributions defined implicitly‚Äîsuch as the stationary distribution of a Markov chain or the posterior in Bayesian inference‚Äîwe typically cannot even write down the CDF. <strong>Markov chain Monte Carlo</strong> methods are necessary.</p>
<p><strong>Rule of Thumb</strong>: Use inverse CDF when:</p>
<ul class="simple">
<li><p>The inverse has a closed-form expression, OR</p></li>
<li><p>You need samples from a custom discrete distribution, OR</p></li>
<li><p>You need only a few samples from an unusual distribution</p></li>
</ul>
<p>Use other methods when:</p>
<ul class="simple">
<li><p>Specialized algorithms exist (Box-Muller for normals, ratio-of-uniforms for gammas)</p></li>
<li><p>The distribution is multivariate</p></li>
<li><p>The distribution is defined implicitly</p></li>
</ul>
</section>
</section>
<section id="bringing-it-all-together">
<h2>Bringing It All Together<a class="headerlink" href="#bringing-it-all-together" title="Link to this heading">ÔÉÅ</a></h2>
<p>The inverse CDF method is the theoretical foundation for random variable generation. Its power lies in universality: the same principle‚Äîtransform uniform samples through the quantile function‚Äîapplies to continuous, discrete, and mixed distributions alike.</p>
<p>For <strong>continuous distributions with tractable inverses</strong>, the method is both elegant and efficient:</p>
<ul class="simple">
<li><p>Exponential, Weibull, Pareto: one logarithm, a few arithmetic operations</p></li>
<li><p>Cauchy, logistic: trigonometric or logarithmic functions</p></li>
<li><p>All achieve <span class="math notranslate nohighlight">\(O(1)\)</span> sampling with minimal code</p></li>
</ul>
<p>For <strong>discrete distributions</strong>, we developed a hierarchy of five algorithms:</p>
<ul class="simple">
<li><p><strong>Linear search</strong>: Simple, <span class="math notranslate nohighlight">\(O(K)\)</span> worst case, good for small <span class="math notranslate nohighlight">\(K\)</span> or head-heavy distributions</p></li>
<li><p><strong>Binary search</strong>: <span class="math notranslate nohighlight">\(O(\log K)\)</span>, the general-purpose workhorse</p></li>
<li><p><strong>Interpolation search</strong>: <span class="math notranslate nohighlight">\(O(\log \log K)\)</span> for near-uniform distributions, but degrades to <span class="math notranslate nohighlight">\(O(K)\)</span> for skewed ones</p></li>
<li><p><strong>Exponential doubling</strong>: <span class="math notranslate nohighlight">\(O(\log k)\)</span> where <span class="math notranslate nohighlight">\(k\)</span> is the result index, excellent for head-heavy distributions</p></li>
<li><p><strong>Alias method</strong>: <span class="math notranslate nohighlight">\(O(1)\)</span> sampling after <span class="math notranslate nohighlight">\(O(K)\)</span> setup, optimal for large <span class="math notranslate nohighlight">\(K\)</span> with many samples from a fixed distribution</p></li>
</ul>
<p>For <strong>mixed distributions</strong>, the inverse CDF handles point masses and continuous components naturally by partitioning the uniform range.</p>
<p><strong>Numerical precision</strong> demands attention: for exponential-type inverses, use <span class="math notranslate nohighlight">\(-\ln(U)\)</span> rather than <span class="math notranslate nohighlight">\(-\ln(1-U)\)</span> to preserve tail resolution; clamp uniforms away from boundaries; and respect the limitations of floating-point arithmetic.</p>
<p>Most importantly, we identified <strong>when the method fails</strong>: distributions without closed-form inverses (normals), multivariate distributions (need conditional decomposition), and implicitly defined distributions (need MCMC).</p>
</section>
<section id="transition-to-what-follows">
<h2>Transition to What Follows<a class="headerlink" href="#transition-to-what-follows" title="Link to this heading">ÔÉÅ</a></h2>
<p>The inverse CDF method cannot efficiently handle all distributions. Two important classes require different approaches:</p>
<p><strong>Box-Muller and Related Transformations</strong> (next section): When the inverse CDF lacks a closed form but clever algebraic transformations exist, we can often find efficient alternatives. The Box-Muller transform generates normal samples from uniforms using polar coordinates‚Äîfaster than numerical inversion, requiring only logarithms and trigonometric functions.</p>
<p><strong>Rejection Sampling</strong> (Section 2.5): When no transformation works, we can generate samples from a simpler ‚Äúproposal‚Äù distribution and accept them with probability proportional to the target density. This powerful technique requires only that we can evaluate the target density up to a normalizing constant‚Äîprecisely the situation in Bayesian computation.</p>
<p>Together with the inverse CDF method, these techniques form a complete toolkit for random variable generation. Each has its domain of applicability; the skilled practitioner knows when to use which.</p>
<div class="important admonition">
<p class="admonition-title">Key Takeaways üìù</p>
<ol class="arabic simple">
<li><p><strong>Core concept</strong>: The inverse CDF method generates <span class="math notranslate nohighlight">\(X \sim F\)</span> by computing <span class="math notranslate nohighlight">\(X = F^{-1}(U)\)</span> where <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0, 1)\)</span>. The generalized inverse <span class="math notranslate nohighlight">\(F^{-1}(u) = \inf\{x : F(x) \geq u\}\)</span> handles all distributions‚Äîcontinuous, discrete, or mixed.</p></li>
<li><p><strong>Continuous distributions</strong>: When <span class="math notranslate nohighlight">\(F^{-1}\)</span> has closed form (exponential, Weibull, Pareto, Cauchy, logistic), sampling is <span class="math notranslate nohighlight">\(O(1)\)</span> with a few arithmetic operations. Derive the inverse by solving <span class="math notranslate nohighlight">\(F(x) = u\)</span> for <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
<li><p><strong>Discrete distributions</strong>: Five algorithms with different trade-offs: linear search <span class="math notranslate nohighlight">\(O(K)\)</span>, binary search <span class="math notranslate nohighlight">\(O(\log K)\)</span>, interpolation search <span class="math notranslate nohighlight">\(O(\log \log K)\)</span> for uniform distributions, exponential doubling <span class="math notranslate nohighlight">\(O(\log k)\)</span> for head-heavy distributions, and the alias method <span class="math notranslate nohighlight">\(O(1)\)</span> after <span class="math notranslate nohighlight">\(O(K)\)</span> setup. Choose based on distribution shape, size <span class="math notranslate nohighlight">\(K\)</span>, and number of samples.</p></li>
<li><p><strong>Numerical precision</strong>: For exponential-type inverses, use <span class="math notranslate nohighlight">\(-\ln(U)\)</span> rather than <span class="math notranslate nohighlight">\(-\ln(1-U)\)</span> so large samples correspond to <span class="math notranslate nohighlight">\(U \approx 0\)</span> where floating point has finer resolution. Guard against <code class="docutils literal notranslate"><span class="pre">log(0)</span></code> with <code class="docutils literal notranslate"><span class="pre">np.maximum(u,</span> <span class="pre">np.finfo(float).tiny)</span></code>.</p></li>
<li><p><strong>Method selection</strong>: Use inverse CDF for tractable inverses and custom discrete distributions. Use Box-Muller for normals, rejection sampling for complex densities, MCMC for implicit distributions.</p></li>
<li><p><strong>Outcome alignment</strong>: This section directly addresses Learning Outcome 1 (apply simulation techniques including inverse CDF transformation) and provides the foundational random variable generation methods used throughout Monte Carlo integration, resampling, and Bayesian computation.</p></li>
</ol>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="2.1. Chapter 2: Monte Carlo Simulation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ch2.4-transformation-methods.html" class="btn btn-neutral float-right" title="2.1.2. Transformation Methods" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>