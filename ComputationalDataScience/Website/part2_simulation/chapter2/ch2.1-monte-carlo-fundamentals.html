

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>2.1.1. Monte Carlo Fundamentals &mdash; STAT 418</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=6826d573" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT418/Website/part2_simulation/chapter2/ch2.1-monte-carlo-fundamentals.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=30646c52"></script>
      <script>window.MathJax = {"options": {"enableMenu": true, "menuOptions": {"settings": {"enrich": true, "speech": true, "braille": true, "collapsible": true, "assistiveMml": false}}}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
      <script src="../../_static/custom.js?v=d2113767"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="2.1.2. Uniform Random Variates" href="ch2.2-uniform-random-variates.html" />
    <link rel="prev" title="2.1. Chapter 2: Monte Carlo Simulation" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Course Content</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../part1_foundations/index.html">1. Part I: Foundations of Probability and Computation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part1_foundations/chapter1/index.html">1.1. Chapter 1: Statistical Paradigms and Core Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html">1.1.1. Paradigms of Probability and Statistical Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#the-mathematical-foundation-kolmogorov-s-axioms">The Mathematical Foundation: Kolmogorov‚Äôs Axioms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#interpretations-of-probability">Interpretations of Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#statistical-inference-paradigms">Statistical Inference Paradigms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#historical-and-philosophical-debates">Historical and Philosophical Debates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#looking-ahead-our-course-focus">Looking Ahead: Our Course Focus</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html">1.1.2. Probability Distributions: Theory and Computation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#from-abstract-foundations-to-concrete-tools">From Abstract Foundations to Concrete Tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#the-python-ecosystem-for-probability">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#introduction-why-probability-distributions-matter">Introduction: Why Probability Distributions Matter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#id1">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#continuous-distributions">Continuous Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#additional-important-distributions">Additional Important Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#summary-and-practical-guidelines">Summary and Practical Guidelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html">1.1.3. Python Random Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#from-mathematical-distributions-to-computational-samples">From Mathematical Distributions to Computational Samples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-python-ecosystem-at-a-glance">The Python Ecosystem at a Glance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#understanding-pseudo-random-number-generation">Understanding Pseudo-Random Number Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-standard-library-random-module">The Standard Library: <code class="docutils literal notranslate"><span class="pre">random</span></code> Module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#numpy-fast-vectorized-random-sampling">NumPy: Fast Vectorized Random Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#scipy-stats-the-complete-statistical-toolkit">SciPy Stats: The Complete Statistical Toolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#bringing-it-all-together-library-selection-guide">Bringing It All Together: Library Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#looking-ahead-from-random-numbers-to-monte-carlo-methods">Looking Ahead: From Random Numbers to Monte Carlo Methods</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">2. Part II: Simulation-Based Methods</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="index.html">2.1. Chapter 2: Monte Carlo Simulation</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">2.1.1. Monte Carlo Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#the-historical-development-of-monte-carlo-methods">The Historical Development of Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-core-principle-expectation-as-integration">The Core Principle: Expectation as Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="#theoretical-foundations">Theoretical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#variance-estimation-and-confidence-intervals">Variance Estimation and Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="#comparison-with-deterministic-methods">Comparison with Deterministic Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sample-size-determination">Sample Size Determination</a></li>
<li class="toctree-l4"><a class="reference internal" href="#convergence-diagnostics-and-monitoring">Convergence Diagnostics and Monitoring</a></li>
<li class="toctree-l4"><a class="reference internal" href="#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="#transition-to-what-follows">Transition to What Follows</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2.2-uniform-random-variates.html">2.1.2. Uniform Random Variates</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2.2-uniform-random-variates.html#why-uniform-the-universal-currency-of-randomness">Why Uniform? The Universal Currency of Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.2-uniform-random-variates.html#the-paradox-of-computational-randomness">The Paradox of Computational Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.2-uniform-random-variates.html#chaotic-dynamical-systems-an-instructive-failure">Chaotic Dynamical Systems: An Instructive Failure</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.2-uniform-random-variates.html#linear-congruential-generators">Linear Congruential Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.2-uniform-random-variates.html#shift-register-generators">Shift-Register Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.2-uniform-random-variates.html#the-kiss-generator-combining-strategies">The KISS Generator: Combining Strategies</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.2-uniform-random-variates.html#modern-generators-mersenne-twister-and-pcg">Modern Generators: Mersenne Twister and PCG</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.2-uniform-random-variates.html#statistical-testing-of-random-number-generators">Statistical Testing of Random Number Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.2-uniform-random-variates.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.2-uniform-random-variates.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.2-uniform-random-variates.html#transition-to-what-follows">Transition to What Follows</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2.3-inverse-cdf-method.html">2.1.3. Inverse CDF Method</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2.3-inverse-cdf-method.html#mathematical-foundations">Mathematical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.3-inverse-cdf-method.html#continuous-distributions-with-closed-form-inverses">Continuous Distributions with Closed-Form Inverses</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.3-inverse-cdf-method.html#numerical-inversion">Numerical Inversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.3-inverse-cdf-method.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.3-inverse-cdf-method.html#mixed-distributions">Mixed Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.3-inverse-cdf-method.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.3-inverse-cdf-method.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2.3-inverse-cdf-method.html#transition-to-what-follows">Transition to What Follows</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter3/index.html">2.2. Chapter 3: Frequentist Statistical Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/sampling_variability.html">2.2.1. Sampling Variability</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/statistical_estimators.html">2.2.2. Statistical Estimators</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/plugin_methods.html">2.2.3. Plugin Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/parametric_inference.html">2.2.4. Parametric Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/exponential_families.html">2.2.5. Exponential Families</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/maximum_likelihood.html">2.2.6. Maximum Likelihood</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/linear_models.html">2.2.7. Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/generalized_linear_models.html">2.2.8. Generalized Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter4/index.html">2.3. Chapter 4: Resampling Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/jackknife_introduction.html">2.3.1. Jackknife Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html">2.3.2. Bootstrap Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html">2.3.3. Nonparametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/parametric_bootstrap.html">2.3.4. Parametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/confidence_intervals.html">2.3.5. Confidence Intervals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/bias_correction.html">2.3.6. Bias Correction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/cross_validation_loo.html">2.3.7. Cross Validation Loo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html">2.3.8. Cross Validation K Fold</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/model_selection.html">2.3.9. Model Selection</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part3_bayesian/index.html">2. Part III: Bayesian Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part3_bayesian/index.html#overview">2.1. Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html">1. Bayesian Philosophy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#introduction">1.1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#key-concepts">1.2. Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#mathematical-framework">1.3. Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#python-implementation">1.4. Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#examples">1.5. Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#summary">1.6. Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html"><span class="section-number">2. </span>Part II: Simulation-Based Methods</a></li>
          <li class="breadcrumb-item"><a href="index.html"><span class="section-number">2.1. </span>Chapter 2: Monte Carlo Simulation</a></li>
      <li class="breadcrumb-item active"><span class="section-number">2.1.1. </span>Monte Carlo Fundamentals</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/part2_simulation/chapter2/ch2.1-monte-carlo-fundamentals.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="monte-carlo-fundamentals">
<span id="ch2-1-monte-carlo-fundamentals"></span><h1><span class="section-number">2.1.1. </span>Monte Carlo Fundamentals<a class="headerlink" href="#monte-carlo-fundamentals" title="Link to this heading">ÔÉÅ</a></h1>
<p>In the spring of 1946, the mathematician Stanislaw Ulam was recovering from a near-fatal case of viral encephalitis at his home in Los Angeles. To pass the time during his convalescence, he played countless games of solitaire‚Äîand found himself wondering: what is the probability of winning a game of Canfield solitaire? The combinatorics were hopelessly complex. There were too many possible configurations, too many branching paths through a game, to enumerate them all. But Ulam realized something profound: he didn‚Äôt need to enumerate every possibility. He could simply <em>play</em> a hundred games and count how many he won.</p>
<p>This insight‚Äîthat we can estimate probabilities by running experiments rather than computing them analytically‚Äîwas not new. The Comte de Buffon had used a similar approach in 1777 to estimate œÄ by dropping needles onto a lined floor. But Ulam saw something that Buffon could not have imagined: the recently completed ENIAC computer could ‚Äúplay‚Äù millions of such games, transforming a parlor trick into a serious computational method. Within weeks, Ulam had discussed the idea with his colleague John von Neumann, and the two began developing what would become one of the most powerful computational frameworks in all of science.</p>
<p>They needed a code name for this method, which they were applying to classified problems in nuclear weapons design at Los Alamos. Nicholas Metropolis suggested ‚ÄúMonte Carlo,‚Äù after the famous casino in Monaco where Ulam‚Äôs uncle had a gambling habit. The name stuck, and with it, a new era in computational science began.</p>
<p>This chapter introduces Monte Carlo methods‚Äîa family of algorithms that use random sampling to solve problems that would otherwise be intractable. We will see how randomness, properly harnessed, becomes a precision instrument for computing integrals, estimating probabilities, and approximating quantities that resist analytical attack. The ideas are simple, but their power is immense: Monte Carlo methods now pervade physics, finance, machine learning, and statistics, anywhere that high-dimensional integration or complex probability calculations arise.</p>
<div class="important admonition">
<p class="admonition-title">Road Map üß≠</p>
<ul class="simple">
<li><p><strong>Understand</strong>: The fundamental principle that Monte Carlo integration estimates integrals as expectations of random samples, and why this works via the Law of Large Numbers and Central Limit Theorem</p></li>
<li><p><strong>Develop</strong>: Deep intuition for the <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> convergence rate‚Äîwhat it means, why it arises, and its remarkable dimension-independence</p></li>
<li><p><strong>Implement</strong>: Complete Python code for Monte Carlo estimation with variance quantification, confidence intervals, and convergence diagnostics</p></li>
<li><p><strong>Evaluate</strong>: When Monte Carlo methods outperform deterministic alternatives, and how to assess estimation quality in practice</p></li>
<li><p><strong>Connect</strong>: How Monte Carlo integration motivates the random variable generation techniques of subsequent sections</p></li>
</ul>
</div>
<section id="the-historical-development-of-monte-carlo-methods">
<h2>The Historical Development of Monte Carlo Methods<a class="headerlink" href="#the-historical-development-of-monte-carlo-methods" title="Link to this heading">ÔÉÅ</a></h2>
<p>Before diving into the mathematics, it is worth understanding how Monte Carlo methods emerged and evolved. This history illuminates why the methods work, what problems motivated their development, and why they remain central to computational science today.</p>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide003_img002_8c3660db.png"><img alt="Timeline showing evolution of Monte Carlo methods from Buffon's Needle (1777) to modern neural-enhanced methods (2020s)" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/Monte%20Carlo%20Methods_slide003_img002_8c3660db.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.1 </span><span class="caption-text"><strong>Historical Evolution of Monte Carlo Methods.</strong> This timeline traces 250 years of algorithmic innovation, from Buffon‚Äôs needle experiment in 1777 through the founding contributions of Ulam and von Neumann in the 1940s, the development of MCMC methods, resampling techniques, and modern neural-enhanced approaches. Each innovation opened new classes of problems to computational attack.</span><a class="headerlink" href="#id1" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<section id="buffon-s-needle-the-first-monte-carlo-experiment">
<h3>Buffon‚Äôs Needle: The First Monte Carlo Experiment<a class="headerlink" href="#buffon-s-needle-the-first-monte-carlo-experiment" title="Link to this heading">ÔÉÅ</a></h3>
<p>In 1777, Georges-Louis Leclerc, Comte de Buffon, posed a deceptively simple question: suppose we have a floor made of parallel wooden planks, each of width <span class="math notranslate nohighlight">\(d\)</span>, and we drop a needle of length <span class="math notranslate nohighlight">\(\ell \leq d\)</span> onto this floor. What is the probability that the needle crosses one of the cracks between planks?</p>
<p>To answer this, Buffon introduced what we would now recognize as a probabilistic model. Let <span class="math notranslate nohighlight">\(\theta\)</span> denote the angle between the needle and the direction of the planks, uniformly distributed on <span class="math notranslate nohighlight">\([0, \pi)\)</span>. Let <span class="math notranslate nohighlight">\(y\)</span> denote the distance from the needle‚Äôs center to the nearest crack, uniformly distributed on <span class="math notranslate nohighlight">\([0, d/2]\)</span>. The needle crosses a crack if and only if the vertical projection of half the needle exceeds the distance to the crack‚Äîthat is, if <span class="math notranslate nohighlight">\(y \leq \frac{\ell}{2} \sin\theta\)</span>.</p>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_1_fig01_buffon_geometry.png"><img alt="Three-panel visualization of Buffon's needle problem showing physical setup, crossing geometry, and probability space" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_1_fig01_buffon_geometry.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.2 </span><span class="caption-text"><strong>Buffon‚Äôs Needle Geometry.</strong> Left: The physical setup with needles scattered across parallel planks (red = crossing, blue = not crossing). Middle: The key geometry‚Äîa needle crosses if its center‚Äôs distance <span class="math notranslate nohighlight">\(y\)</span> to the nearest crack is less than the vertical projection <span class="math notranslate nohighlight">\((\ell/2)\sin\theta\)</span>. Right: The probability space showing the crossing region; the probability equals the ratio of areas: <span class="math notranslate nohighlight">\(2\ell/(\pi d)\)</span>.</span><a class="headerlink" href="#id2" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<p>The probability of crossing is therefore:</p>
<div class="math notranslate nohighlight">
\[P(\text{crossing}) = \frac{1}{\pi} \cdot \frac{2}{d} \int_0^{\pi} \int_0^{(\ell/2)\sin\theta} dy \, d\theta\]</div>
<p>Evaluating the inner integral yields <span class="math notranslate nohighlight">\(\frac{\ell}{2}\sin\theta\)</span>, and the outer integral gives <span class="math notranslate nohighlight">\(\int_0^{\pi} \sin\theta \, d\theta = 2\)</span>. Thus:</p>
<div class="math notranslate nohighlight">
\[P(\text{crossing}) = \frac{1}{\pi} \cdot \frac{2}{d} \cdot \frac{\ell}{2} \cdot 2 = \frac{2\ell}{\pi d}\]</div>
<p>This elegant result has a remarkable consequence. If we drop <span class="math notranslate nohighlight">\(n\)</span> needles and observe that <span class="math notranslate nohighlight">\(k\)</span> of them cross a crack, then our estimate of the crossing probability is <span class="math notranslate nohighlight">\(\hat{p} = k/n\)</span>. Rearranging Buffon‚Äôs formula:</p>
<div class="math notranslate nohighlight">
\[\pi = \frac{2\ell}{d \cdot P(\text{crossing})} \approx \frac{2\ell n}{d k}\]</div>
<p>We can estimate <span class="math notranslate nohighlight">\(\pi\)</span> by throwing needles!</p>
<p>This is a Monte Carlo method avant la lettre: we use random experiments to estimate a deterministic quantity. Of course, Buffon lacked computers, and actually throwing thousands of needles by hand is tedious. In 1901, the Italian mathematician Mario Lazzarini claimed to have obtained <span class="math notranslate nohighlight">\(\pi \approx 3.1415929\)</span> by throwing a needle 3,408 times‚Äîsuspiciously close to the correct value of <span class="math notranslate nohighlight">\(355/113\)</span>. Most historians believe Lazzarini fudged his data, but the underlying principle was sound.</p>
<div class="tip admonition">
<p class="admonition-title">Try It Yourself üñ•Ô∏è Buffon‚Äôs Needle Simulation</p>
<p>Experience Buffon‚Äôs experiment interactively:</p>
<p><strong>Interactive Simulation</strong>: <a class="reference external" href="https://treese41528.github.io/ComputationalDataScience/Simulations/MonteCarloSimulation/buffons_needle_simulation.html">https://treese41528.github.io/ComputationalDataScience/Simulations/MonteCarloSimulation/buffons_needle_simulation.html</a></p>
<p>Watch how the <span class="math notranslate nohighlight">\(\pi\)</span> estimate fluctuates wildly with few needles, then gradually stabilizes as you accumulate thousands of throws. This is the Law of Large Numbers in action‚Äîa theme we will return to throughout this chapter.</p>
</div>
</section>
<section id="fermi-s-envelope-calculations">
<h3>Fermi‚Äôs Envelope Calculations<a class="headerlink" href="#fermi-s-envelope-calculations" title="Link to this heading">ÔÉÅ</a></h3>
<p>The physicist Enrico Fermi was famous for his ability to estimate quantities that seemed impossibly difficult to calculate. How many piano tuners are there in Chicago? How much energy is released in a nuclear explosion? Fermi would break these problems into pieces, estimate each piece roughly, and multiply‚Äîoften achieving answers accurate to within an order of magnitude.</p>
<p>Less well known is that Fermi also pioneered proto-Monte Carlo methods. In the 1930s, working on neutron diffusion problems in Rome, Fermi developed a mechanical device‚Äîessentially a specialized slide rule‚Äîthat could generate random numbers to simulate neutron paths through matter. He used these simulations to estimate quantities like neutron absorption cross-sections, which were too complex to compute analytically.</p>
<p>This work remained largely unpublished, but it anticipated the key insight of Monte Carlo: when a deterministic calculation is intractable, a stochastic simulation may succeed. Fermi‚Äôs physical random number generator was crude, but the principle was the same one that Ulam and von Neumann would later implement on electronic computers.</p>
</section>
<section id="the-manhattan-project-and-eniac">
<h3>The Manhattan Project and ENIAC<a class="headerlink" href="#the-manhattan-project-and-eniac" title="Link to this heading">ÔÉÅ</a></h3>
<p>The development of nuclear weapons during World War II created an urgent need for computational methods. The behavior of neutrons in a nuclear reaction‚Äîhow they scatter, slow down, and trigger fission‚Äîdepends on complex integrals over energy and angle that resist analytical solution. The physicists at Los Alamos needed numbers, not theorems.</p>
<p>It was in this context that Ulam‚Äôs solitaire insight proved transformative. Ulam and von Neumann realized that the same principle‚Äîestimate a complicated quantity by averaging over random samples‚Äîcould be applied to neutron transport. Instead of integrating over all possible neutron paths analytically (impossible), they could simulate thousands of individual neutrons, tracking each one as it scattered and absorbed through the weapon‚Äôs core.</p>
<p>Von Neumann took the lead in implementing these ideas on ENIAC, one of the first general-purpose electronic computers. ENIAC could perform about 5,000 operations per second‚Äîglacially slow by modern standards, but revolutionary in 1946. Von Neumann and his team programmed ENIAC to simulate neutron histories, and the results helped validate the design of thermonuclear weapons.</p>
<p>The ‚ÄúMonte Carlo method‚Äù was formally introduced to the broader scientific community in a 1949 paper by Metropolis and Ulam, though much of the early work remained classified for decades. The name, coined by Metropolis, captured both the element of chance central to the method and the slightly disreputable excitement of gambling‚Äîa fitting tribute to Ulam‚Äôs card-playing origins.</p>
</section>
<section id="why-monte-carlo-changed-everything">
<h3>Why ‚ÄúMonte Carlo‚Äù Changed Everything<a class="headerlink" href="#why-monte-carlo-changed-everything" title="Link to this heading">ÔÉÅ</a></h3>
<p>The Monte Carlo revolution was not merely about having faster computers. It represented a conceptual breakthrough: <em>randomness is a computational resource</em>. By embracing uncertainty rather than fighting it, Monte Carlo methods could attack problems that deterministic methods could not touch.</p>
<p>Consider the challenge of computing a 100-dimensional integral. Deterministic quadrature methods‚Äîthe trapezoidal rule, Simpson‚Äôs rule, Gaussian quadrature‚Äîall suffer from the ‚Äúcurse of dimensionality.‚Äù If we use <span class="math notranslate nohighlight">\(n\)</span> points per dimension, we need <span class="math notranslate nohighlight">\(n^{100}\)</span> total evaluations. Even with <span class="math notranslate nohighlight">\(n = 2\)</span>, this exceeds <span class="math notranslate nohighlight">\(10^{30}\)</span>‚Äîmore function evaluations than atoms in a human body.</p>
<p>Monte Carlo methods sidestep this curse entirely. As we will see, the error in a Monte Carlo estimate depends only on the number of samples and the variance of the integrand, not on the dimension of the space. A 100-dimensional integral is no harder than a one-dimensional integral, at least in terms of convergence rate. This dimension-independence is the source of Monte Carlo‚Äôs power.</p>
</section>
</section>
<section id="the-core-principle-expectation-as-integration">
<h2>The Core Principle: Expectation as Integration<a class="headerlink" href="#the-core-principle-expectation-as-integration" title="Link to this heading">ÔÉÅ</a></h2>
<p>We now turn to the mathematical foundations of Monte Carlo integration. The key insight is simple but profound: any integral can be rewritten as an expected value, and expected values can be estimated by averaging samples.</p>
<section id="from-integrals-to-expectations">
<h3>From Integrals to Expectations<a class="headerlink" href="#from-integrals-to-expectations" title="Link to this heading">ÔÉÅ</a></h3>
<p>Consider a general integral of the form:</p>
<div class="math notranslate nohighlight" id="equation-general-integral">
<span class="eqno">(2.1)<a class="headerlink" href="#equation-general-integral" title="Link to this equation">ÔÉÅ</a></span>\[I = \int_{\mathcal{X}} h(x) \, dx\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{X} \subseteq \mathbb{R}^d\)</span> is the domain of integration and <span class="math notranslate nohighlight">\(h: \mathcal{X} \to \mathbb{R}\)</span> is the function we wish to integrate. At first glance, this seems like a problem for calculus, not probability. But watch what happens when we introduce a probability density.</p>
<p>Let <span class="math notranslate nohighlight">\(f(x)\)</span> be any probability density function on <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>‚Äîthat is, <span class="math notranslate nohighlight">\(f(x) \geq 0\)</span> everywhere and <span class="math notranslate nohighlight">\(\int_{\mathcal{X}} f(x) \, dx = 1\)</span>. We can rewrite our integral as:</p>
<div class="math notranslate nohighlight" id="equation-importance-rewrite">
<span class="eqno">(2.2)<a class="headerlink" href="#equation-importance-rewrite" title="Link to this equation">ÔÉÅ</a></span>\[I = \int_{\mathcal{X}} h(x) \, dx = \int_{\mathcal{X}} \frac{h(x)}{f(x)} f(x) \, dx = \mathbb{E}_f\left[ \frac{h(X)}{f(X)} \right]\]</div>
<p>where the expectation is taken over a random variable <span class="math notranslate nohighlight">\(X\)</span> with density <span class="math notranslate nohighlight">\(f\)</span>. We have transformed an integral into an expected value!</p>
<p>The simplest choice is the uniform density on <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. If <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> has finite volume <span class="math notranslate nohighlight">\(V = \int_{\mathcal{X}} dx\)</span>, then <span class="math notranslate nohighlight">\(f(x) = 1/V\)</span> is a valid density, and:</p>
<div class="math notranslate nohighlight">
\[I = V \cdot \mathbb{E}_{\text{Uniform}(\mathcal{X})}[h(X)]\]</div>
<p>For example, to compute <span class="math notranslate nohighlight">\(\int_0^1 e^{-x^2} dx\)</span>, we write:</p>
<div class="math notranslate nohighlight">
\[\int_0^1 e^{-x^2} dx = \mathbb{E}[e^{-U^2}] \quad \text{where } U \sim \text{Uniform}(0, 1)\]</div>
<p>This rewriting is always possible. But why is it useful?</p>
</section>
<section id="the-monte-carlo-estimator">
<h3>The Monte Carlo Estimator<a class="headerlink" href="#the-monte-carlo-estimator" title="Link to this heading">ÔÉÅ</a></h3>
<p>The power of the expectation formulation becomes clear when we recall the Law of Large Numbers. If <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> are independent and identically distributed (iid) with <span class="math notranslate nohighlight">\(\mathbb{E}[X_i] = \mu\)</span>, then the sample mean converges to the true mean:</p>
<div class="math notranslate nohighlight">
\[\bar{X}_n = \frac{1}{n} \sum_{i=1}^{n} X_i \xrightarrow{\text{a.s.}} \mu \quad \text{as } n \to \infty\]</div>
<p>Applied to our integral:</p>
<div class="note admonition">
<p class="admonition-title">Definition: Monte Carlo Estimator</p>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> be iid samples from a density <span class="math notranslate nohighlight">\(f\)</span> on <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. The <strong>Monte Carlo estimator</strong> of the integral <span class="math notranslate nohighlight">\(I = \int_{\mathcal{X}} h(x) f(x) \, dx = \mathbb{E}_f[h(X)]\)</span> is:</p>
<div class="math notranslate nohighlight" id="equation-mc-estimator-def">
<span class="eqno">(2.3)<a class="headerlink" href="#equation-mc-estimator-def" title="Link to this equation">ÔÉÅ</a></span>\[\hat{I}_n = \frac{1}{n} \sum_{i=1}^{n} h(X_i)\]</div>
<p>More generally, for <span class="math notranslate nohighlight">\(I = \int_{\mathcal{X}} g(x) \, dx\)</span> where we sample from density <span class="math notranslate nohighlight">\(f\)</span>:</p>
<div class="math notranslate nohighlight">
\[\hat{I}_n = \frac{1}{n} \sum_{i=1}^{n} \frac{g(X_i)}{f(X_i)}\]</div>
</div>
<p>The Monte Carlo method is disarmingly simple: draw random samples, evaluate the function at each sample, and average the results. No derivatives, no quadrature weights, no mesh generation‚Äîjust sampling and averaging.</p>
<p>But this simplicity conceals depth. The choice of sampling density <span class="math notranslate nohighlight">\(f\)</span> is entirely up to us, and different choices lead to dramatically different performance. We will explore this in the section on importance sampling; for now, we focus on the ‚Äúnaive‚Äù case where <span class="math notranslate nohighlight">\(f\)</span> matches the density of the integrand or is uniform on the domain.</p>
</section>
<section id="a-first-example-estimating-pi">
<h3>A First Example: Estimating <span class="math notranslate nohighlight">\(\pi\)</span><a class="headerlink" href="#a-first-example-estimating-pi" title="Link to this heading">ÔÉÅ</a></h3>
<p>Let us return to the problem of estimating <span class="math notranslate nohighlight">\(\pi\)</span>, now with Monte Carlo machinery. Consider the integral:</p>
<div class="math notranslate nohighlight">
\[\pi = \int_{-1}^{1} \int_{-1}^{1} \mathbf{1}_{x^2 + y^2 \leq 1} \, dx \, dy\]</div>
<p>This is the area of the unit disk. Rewriting as an expectation:</p>
<div class="math notranslate nohighlight">
\[\pi = 4 \cdot \mathbb{E}[\mathbf{1}_{X^2 + Y^2 \leq 1}] \quad \text{where } (X, Y) \sim \text{Uniform}([-1,1]^2)\]</div>
<p>The factor of 4 accounts for the area of the square <span class="math notranslate nohighlight">\([-1,1]^2\)</span>. The Monte Carlo estimator is:</p>
<div class="math notranslate nohighlight">
\[\hat{\pi}_n = \frac{4}{n} \sum_{i=1}^{n} \mathbf{1}_{X_i^2 + Y_i^2 \leq 1}\]</div>
<p>That is: generate <span class="math notranslate nohighlight">\(n\)</span> uniform points in the square, count how many fall inside the unit circle, and multiply by 4.</p>
<p>The geometric intuition is immediate: the ratio of points landing inside the circle to total points approximates the ratio of areas, <span class="math notranslate nohighlight">\(\pi/4\)</span>.</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_1_fig02_pi_estimation.png"><img alt="Monte Carlo estimation of œÄ showing scatter plot of points inside and outside unit circle, plus convergence plot" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_1_fig02_pi_estimation.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.3 </span><span class="caption-text"><strong>Monte Carlo œÄ Estimation.</strong> Left: Blue points fall inside the unit circle; coral points fall outside. The ratio of blue to total points estimates <span class="math notranslate nohighlight">\(\pi/4\)</span>. Right: The running estimate stabilizes as samples accumulate‚Äîthe characteristic ‚Äúnoisy convergence‚Äù of Monte Carlo.</span><a class="headerlink" href="#id3" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">estimate_pi_monte_carlo</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Estimate œÄ using Monte Carlo integration.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of random points to generate.</span>
<span class="sd">    seed : int, optional</span>
<span class="sd">        Random seed for reproducibility.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        Contains estimate, standard_error, and confidence interval.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Generate uniform points in [-1, 1]¬≤</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Count points inside unit circle</span>
    <span class="n">inside</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span>

    <span class="c1"># Monte Carlo estimate</span>
    <span class="n">p_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">inside</span><span class="p">)</span>
    <span class="n">pi_hat</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">p_hat</span>

    <span class="c1"># Standard error (indicator has Bernoulli variance p(1-p))</span>
    <span class="n">se_p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">p_hat</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_hat</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="n">se_pi</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">se_p</span>

    <span class="c1"># 95% confidence interval</span>
    <span class="n">ci</span> <span class="o">=</span> <span class="p">(</span><span class="n">pi_hat</span> <span class="o">-</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">se_pi</span><span class="p">,</span> <span class="n">pi_hat</span> <span class="o">+</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">se_pi</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;estimate&#39;</span><span class="p">:</span> <span class="n">pi_hat</span><span class="p">,</span>
        <span class="s1">&#39;standard_error&#39;</span><span class="p">:</span> <span class="n">se_pi</span><span class="p">,</span>
        <span class="s1">&#39;ci_95&#39;</span><span class="p">:</span> <span class="n">ci</span><span class="p">,</span>
        <span class="s1">&#39;n_samples&#39;</span><span class="p">:</span> <span class="n">n_samples</span>
    <span class="p">}</span>

<span class="c1"># Run the estimation</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">estimate_pi_monte_carlo</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;œÄ estimate: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;estimate&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True œÄ:     </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error:      </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;estimate&#39;</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Std Error:  </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;standard_error&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% CI:     (</span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci_95&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci_95&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>œÄ estimate: 3.143080
True œÄ:     3.141593
Error:      0.001487
Std Error:  0.005190
95% CI:     (3.132908, 3.153252)
</pre></div>
</div>
<p>The true value <span class="math notranslate nohighlight">\(\pi\)</span> lies comfortably within the 95% confidence interval. With a million samples, the error shrinks by a factor of <span class="math notranslate nohighlight">\(\sqrt{10} \approx 3.16\)</span>, and with ten million, by another factor of <span class="math notranslate nohighlight">\(\sqrt{10}\)</span>.</p>
<p><strong>Why Bernoulli variance?</strong> The comment in the code mentions ‚ÄúBernoulli variance‚Äù‚Äîlet‚Äôs unpack this. Each random point either lands inside the circle (we record <span class="math notranslate nohighlight">\(I_i = 1\)</span>) or outside (we record <span class="math notranslate nohighlight">\(I_i = 0\)</span>). This is a <strong>Bernoulli trial</strong> with success probability <span class="math notranslate nohighlight">\(p = \pi/4\)</span>, which is the ratio of the circle‚Äôs area to the square‚Äôs area. For any Bernoulli random variable:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[I_i] = p, \qquad \text{Var}(I_i) = p(1 - p)\]</div>
<p>Our estimator <span class="math notranslate nohighlight">\(\hat{p} = \frac{1}{n}\sum_{i=1}^n I_i\)</span> is the sample proportion of points inside the circle. Since the <span class="math notranslate nohighlight">\(I_i\)</span> are independent:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\hat{p}) = \text{Var}\left(\frac{1}{n}\sum_{i=1}^n I_i\right) = \frac{1}{n^2} \cdot n \cdot p(1-p) = \frac{p(1-p)}{n}\]</div>
<p>The standard error is the square root: <span class="math notranslate nohighlight">\(\text{SE}(\hat{p}) = \sqrt{p(1-p)/n}\)</span>. Since <span class="math notranslate nohighlight">\(\hat{\pi} = 4\hat{p}\)</span>, the standard error of <span class="math notranslate nohighlight">\(\hat{\pi}\)</span> scales by 4:</p>
<div class="math notranslate nohighlight">
\[\text{SE}(\hat{\pi}) = 4 \cdot \sqrt{\frac{p(1-p)}{n}}\]</div>
<p>We don‚Äôt know the true <span class="math notranslate nohighlight">\(p\)</span>, so we substitute our estimate <span class="math notranslate nohighlight">\(\hat{p}\)</span> to get a usable formula. This Bernoulli structure is a special case of the general Monte Carlo variance formula‚Äîwhenever the function <span class="math notranslate nohighlight">\(h(x)\)</span> is an indicator (0 or 1), the variance simplifies to the familiar <span class="math notranslate nohighlight">\(p(1-p)\)</span> form.</p>
<p>This is Monte Carlo at its most basic: evaluate a simple function at random points and average. But even this toy example illustrates the key features of the method‚Äîease of implementation, probabilistic error bounds, and graceful scaling with sample size.</p>
</section>
</section>
<section id="theoretical-foundations">
<h2>Theoretical Foundations<a class="headerlink" href="#theoretical-foundations" title="Link to this heading">ÔÉÅ</a></h2>
<p>Why does the Monte Carlo method work? What determines the rate of convergence? These questions have precise mathematical answers rooted in classical probability theory.</p>
<section id="the-law-of-large-numbers">
<h3>The Law of Large Numbers<a class="headerlink" href="#the-law-of-large-numbers" title="Link to this heading">ÔÉÅ</a></h3>
<p>The Law of Large Numbers (LLN) is the foundational result guaranteeing that Monte Carlo estimators converge to the true value. There are several versions; we state the strongest form.</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Strong Law of Large Numbers</p>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots\)</span> be independent and identically distributed random variables with <span class="math notranslate nohighlight">\(\mathbb{E}[|X_1|] &lt; \infty\)</span>. Then:</p>
<div class="math notranslate nohighlight">
\[\bar{X}_n = \frac{1}{n} \sum_{i=1}^{n} X_i \xrightarrow{\text{a.s.}} \mathbb{E}[X_1] \quad \text{as } n \to \infty\]</div>
<p>The notation <span class="math notranslate nohighlight">\(\xrightarrow{\text{a.s.}}\)</span> denotes <em>almost sure convergence</em>: the probability that the sequence converges is exactly 1.</p>
</div>
<p>For Monte Carlo integration, we apply this theorem with <span class="math notranslate nohighlight">\(X_i = h(X_i)\)</span> where the <span class="math notranslate nohighlight">\(X_i\)</span> are iid from density <span class="math notranslate nohighlight">\(f\)</span>. The condition <span class="math notranslate nohighlight">\(\mathbb{E}[|h(X)|] &lt; \infty\)</span> ensures that the integral we are estimating actually exists.</p>
<p>The LLN tells us that <span class="math notranslate nohighlight">\(\hat{I}_n \to I\)</span> with probability 1. No matter how complex the integrand, no matter how high the dimension, the Monte Carlo estimator will eventually get arbitrarily close to the true value. This is an extraordinarily powerful guarantee.</p>
<p>But the LLN is silent on <em>how fast</em> convergence occurs. For that, we need the Central Limit Theorem.</p>
</section>
<section id="the-central-limit-theorem-and-the-o-n-1-2-rate">
<h3>The Central Limit Theorem and the <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> Rate<a class="headerlink" href="#the-central-limit-theorem-and-the-o-n-1-2-rate" title="Link to this heading">ÔÉÅ</a></h3>
<p>The Central Limit Theorem (CLT) is the workhorse result for quantifying Monte Carlo error.</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Central Limit Theorem</p>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots\)</span> be iid with mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2 &lt; \infty\)</span>. Then:</p>
<div class="math notranslate nohighlight">
\[\sqrt{n} \left( \bar{X}_n - \mu \right) \xrightarrow{d} \mathcal{N}(0, \sigma^2)\]</div>
<p>where <span class="math notranslate nohighlight">\(\xrightarrow{d}\)</span> denotes convergence in distribution. Equivalently, for large <span class="math notranslate nohighlight">\(n\)</span>:</p>
<div class="math notranslate nohighlight">
\[\bar{X}_n \stackrel{\cdot}{\sim} \mathcal{N}\left( \mu, \frac{\sigma^2}{n} \right)\]</div>
</div>
<p>Applied to Monte Carlo integration:</p>
<div class="math notranslate nohighlight">
\[\hat{I}_n \stackrel{\cdot}{\sim} \mathcal{N}\left( I, \frac{\sigma^2}{n} \right)\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma^2 = \text{Var}_f[h(X)] = \mathbb{E}_f[(h(X) - I)^2]\)</span> is the variance of the integrand under the sampling distribution.</p>
<p>The <strong>standard error</strong> of the Monte Carlo estimator is:</p>
<div class="math notranslate nohighlight" id="equation-mc-standard-error">
<span class="eqno">(2.4)<a class="headerlink" href="#equation-mc-standard-error" title="Link to this equation">ÔÉÅ</a></span>\[\text{SE}(\hat{I}_n) = \frac{\sigma}{\sqrt{n}}\]</div>
<p>This is the celebrated <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> convergence rate. To reduce the standard error by a factor of 10, we need 100 times as many samples. To gain one decimal place of accuracy, we need 100 times the computational effort.</p>
<p>This rate may seem slow‚Äîand compared to some deterministic methods, it is. But the rate is <em>independent of dimension</em>. Whether we are integrating over <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> or <span class="math notranslate nohighlight">\(\mathbb{R}^{1000}\)</span>, the error decreases as <span class="math notranslate nohighlight">\(1/\sqrt{n}\)</span>. This dimension-independence is the source of Monte Carlo‚Äôs power in high-dimensional problems.</p>
<div class="note admonition">
<p class="admonition-title">Example üí° Understanding the Square Root Law</p>
<p><strong>Scenario</strong>: You estimate an integral with 1,000 samples and get a standard error of 0.1. Your boss needs the error reduced to 0.01.</p>
<p><strong>Analysis</strong>: The standard error scales as <span class="math notranslate nohighlight">\(1/\sqrt{n}\)</span>. To reduce the standard error by a factor of 10, you need <span class="math notranslate nohighlight">\(n\)</span> to increase by a factor of <span class="math notranslate nohighlight">\(10^2 = 100\)</span>.</p>
<p><strong>Conclusion</strong>: You need <span class="math notranslate nohighlight">\(1000 \times 100 = 100,000\)</span> samples.</p>
<p>This quadratic penalty is the price of Monte Carlo‚Äôs simplicity. In low dimensions, deterministic methods often achieve polynomial convergence rates like <span class="math notranslate nohighlight">\(O(n^{-2})\)</span> or better, making them far more efficient. But in high dimensions, Monte Carlo‚Äôs dimension-independent <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> beats any polynomial rate that degrades with dimension.</p>
</div>
</section>
<section id="why-the-square-root">
<h3>Why the Square Root?<a class="headerlink" href="#why-the-square-root" title="Link to this heading">ÔÉÅ</a></h3>
<p>The <span class="math notranslate nohighlight">\(1/\sqrt{n}\)</span> rate may seem mysterious, but it has a simple explanation rooted in the behavior of sums of random variables.</p>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_1_fig03_sqrt_n_convergence.png"><img alt="Three-panel visualization explaining the square root convergence law" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_1_fig03_sqrt_n_convergence.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.4 </span><span class="caption-text"><strong>The Square Root Convergence Law.</strong> Left: The central limit theorem in action‚Äîsample means cluster more tightly around the true mean as <span class="math notranslate nohighlight">\(n\)</span> grows. Middle: Why variance shrinks‚Äîthe variance of a sum equals <span class="math notranslate nohighlight">\(n\sigma^2\)</span>, but dividing by <span class="math notranslate nohighlight">\(n\)</span> to get the mean gives variance <span class="math notranslate nohighlight">\(\sigma^2/n\)</span>. Right: The practical consequence‚Äîthe <span class="math notranslate nohighlight">\(1/\sqrt{n}\)</span> law means diminishing returns, with each additional decimal place of accuracy costing 100√ó more samples.</span><a class="headerlink" href="#id4" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<p>Consider <span class="math notranslate nohighlight">\(n\)</span> iid random variables <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span>, each with variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>. Their sum has variance:</p>
<div class="math notranslate nohighlight">
\[\text{Var}\left( \sum_{i=1}^n X_i \right) = \sum_{i=1}^n \text{Var}(X_i) = n\sigma^2\]</div>
<p>The variance of the sum grows linearly with <span class="math notranslate nohighlight">\(n\)</span>. But when we take the mean, we divide by <span class="math notranslate nohighlight">\(n\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{Var}\left( \frac{1}{n} \sum_{i=1}^n X_i \right) = \frac{1}{n^2} \cdot n\sigma^2 = \frac{\sigma^2}{n}\]</div>
<p>The standard deviation is the square root of variance, giving <span class="math notranslate nohighlight">\(\sigma/\sqrt{n}\)</span>.</p>
<p>This behavior is fundamental to averages of random quantities. Each additional sample adds information, but with diminishing returns: the first sample reduces uncertainty enormously; the millionth sample contributes almost nothing. This is why the square root appears.</p>
</section>
</section>
<section id="variance-estimation-and-confidence-intervals">
<h2>Variance Estimation and Confidence Intervals<a class="headerlink" href="#variance-estimation-and-confidence-intervals" title="Link to this heading">ÔÉÅ</a></h2>
<p>The CLT tells us that <span class="math notranslate nohighlight">\(\hat{I}_n\)</span> is approximately normal with known variance <span class="math notranslate nohighlight">\(\sigma^2/n\)</span>. But we rarely know <span class="math notranslate nohighlight">\(\sigma^2\)</span>‚Äîit depends on the integrand and the sampling distribution. We must estimate it from the same samples we use to estimate <span class="math notranslate nohighlight">\(I\)</span>.</p>
<section id="the-sample-variance">
<h3>The Sample Variance<a class="headerlink" href="#the-sample-variance" title="Link to this heading">ÔÉÅ</a></h3>
<p>The natural estimator of <span class="math notranslate nohighlight">\(\sigma^2 = \text{Var}[h(X)]\)</span> is the sample variance:</p>
<div class="math notranslate nohighlight" id="equation-sample-var">
<span class="eqno">(2.5)<a class="headerlink" href="#equation-sample-var" title="Link to this equation">ÔÉÅ</a></span>\[\hat{\sigma}^2_n = \frac{1}{n-1} \sum_{i=1}^{n} \left( h(X_i) - \hat{I}_n \right)^2\]</div>
<p>The divisor <span class="math notranslate nohighlight">\(n-1\)</span> (rather than <span class="math notranslate nohighlight">\(n\)</span>) makes this estimator unbiased: <span class="math notranslate nohighlight">\(\mathbb{E}[\hat{\sigma}^2_n] = \sigma^2\)</span>. This is known as Bessel‚Äôs correction.</p>
<p>By the Law of Large Numbers, <span class="math notranslate nohighlight">\(\hat{\sigma}^2_n \to \sigma^2\)</span> almost surely as <span class="math notranslate nohighlight">\(n \to \infty\)</span>. Combined with the CLT, this gives us a practical way to construct confidence intervals.</p>
</section>
<section id="constructing-confidence-intervals">
<h3>Constructing Confidence Intervals<a class="headerlink" href="#constructing-confidence-intervals" title="Link to this heading">ÔÉÅ</a></h3>
<p>An asymptotic <span class="math notranslate nohighlight">\((1-\alpha)\)</span> confidence interval for <span class="math notranslate nohighlight">\(I\)</span> is:</p>
<div class="math notranslate nohighlight" id="equation-mc-ci-full">
<span class="eqno">(2.6)<a class="headerlink" href="#equation-mc-ci-full" title="Link to this equation">ÔÉÅ</a></span>\[\left[ \hat{I}_n - z_{\alpha/2} \frac{\hat{\sigma}_n}{\sqrt{n}}, \quad \hat{I}_n + z_{\alpha/2} \frac{\hat{\sigma}_n}{\sqrt{n}} \right]\]</div>
<p>where <span class="math notranslate nohighlight">\(z_{\alpha/2} = \Phi^{-1}(1 - \alpha/2)\)</span> is the standard normal quantile. For common confidence levels:</p>
<ul class="simple">
<li><p>90% CI: <span class="math notranslate nohighlight">\(z_{0.05} \approx 1.645\)</span></p></li>
<li><p>95% CI: <span class="math notranslate nohighlight">\(z_{0.025} \approx 1.960\)</span></p></li>
<li><p>99% CI: <span class="math notranslate nohighlight">\(z_{0.005} \approx 2.576\)</span></p></li>
</ul>
<p>The interval has the interpretation: in repeated sampling, approximately <span class="math notranslate nohighlight">\((1-\alpha) \times 100\%\)</span> of such intervals will contain the true value <span class="math notranslate nohighlight">\(I\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">monte_carlo_integrate</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">sampler</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">confidence</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Monte Carlo integration with uncertainty quantification.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    h : callable</span>
<span class="sd">        Function to integrate. Must accept array input.</span>
<span class="sd">    sampler : callable</span>
<span class="sd">        Function that takes (n, rng) and returns n samples from the target distribution.</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of Monte Carlo samples.</span>
<span class="sd">    confidence : float</span>
<span class="sd">        Confidence level for interval (default 0.95).</span>
<span class="sd">    seed : int, optional</span>
<span class="sd">        Random seed for reproducibility.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        Contains estimate, std_error, confidence interval, and diagnostics.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Generate samples and evaluate function</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">sampler</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
    <span class="n">h_values</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

    <span class="c1"># Point estimate</span>
    <span class="n">estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_values</span><span class="p">)</span>

    <span class="c1"># Variance estimation (Bessel&#39;s correction)</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">h_values</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">std_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">variance</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Confidence interval</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">confidence</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">ci_lower</span> <span class="o">=</span> <span class="n">estimate</span> <span class="o">-</span> <span class="n">z</span> <span class="o">*</span> <span class="n">std_error</span>
    <span class="n">ci_upper</span> <span class="o">=</span> <span class="n">estimate</span> <span class="o">+</span> <span class="n">z</span> <span class="o">*</span> <span class="n">std_error</span>

    <span class="c1"># Effective sample size (for future variance reduction comparisons)</span>
    <span class="n">ess</span> <span class="o">=</span> <span class="n">n_samples</span>  <span class="c1"># For standard MC, ESS = n</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;estimate&#39;</span><span class="p">:</span> <span class="n">estimate</span><span class="p">,</span>
        <span class="s1">&#39;std_error&#39;</span><span class="p">:</span> <span class="n">std_error</span><span class="p">,</span>
        <span class="s1">&#39;variance&#39;</span><span class="p">:</span> <span class="n">variance</span><span class="p">,</span>
        <span class="s1">&#39;ci&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">ci_lower</span><span class="p">,</span> <span class="n">ci_upper</span><span class="p">),</span>
        <span class="s1">&#39;confidence&#39;</span><span class="p">:</span> <span class="n">confidence</span><span class="p">,</span>
        <span class="s1">&#39;n_samples&#39;</span><span class="p">:</span> <span class="n">n_samples</span><span class="p">,</span>
        <span class="s1">&#39;ess&#39;</span><span class="p">:</span> <span class="n">ess</span><span class="p">,</span>
        <span class="s1">&#39;h_values&#39;</span><span class="p">:</span> <span class="n">h_values</span>
    <span class="p">}</span>
</pre></div>
</div>
<div class="note admonition">
<p class="admonition-title">Example üí° Using the Monte Carlo Integration Function</p>
<p><strong>Problem:</strong> Estimate <span class="math notranslate nohighlight">\(\int_0^2 e^{-x^2} dx\)</span> using our <code class="docutils literal notranslate"><span class="pre">monte_carlo_integrate</span></code> function.</p>
<p><strong>Setup:</strong> We need to define: (1) the integrand <span class="math notranslate nohighlight">\(h(x) = 2 e^{-x^2}\)</span> (the factor of 2 accounts for the interval length), and (2) a sampler that generates uniform samples on <span class="math notranslate nohighlight">\([0, 2]\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.special</span><span class="w"> </span><span class="kn">import</span> <span class="n">erf</span>

<span class="c1"># Define the integrand (scaled by interval length)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">h</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Define the sampler: Uniform(0, 2)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">uniform_sampler</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">rng</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>

<span class="c1"># Run Monte Carlo integration</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">monte_carlo_integrate</span><span class="p">(</span>
    <span class="n">h</span><span class="o">=</span><span class="n">h</span><span class="p">,</span>
    <span class="n">sampler</span><span class="o">=</span><span class="n">uniform_sampler</span><span class="p">,</span>
    <span class="n">n_samples</span><span class="o">=</span><span class="mi">100_000</span><span class="p">,</span>
    <span class="n">confidence</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="c1"># True value for comparison</span>
<span class="n">true_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">erf</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimate:    </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;estimate&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True value:  </span><span class="si">{</span><span class="n">true_value</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Std Error:   </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;std_error&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% CI:      (</span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;CI contains true value: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">true_value</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Estimate:    0.880204
True value:  0.882081
Std Error:   0.002179
95% CI:      (0.875934, 0.884474)
CI contains true value: True
</pre></div>
</div>
<p>The function returns all the diagnostics we need: the point estimate, standard error for assessing precision, and a confidence interval that correctly captures the true value. The <code class="docutils literal notranslate"><span class="pre">h_values</span></code> array can be passed to convergence diagnostics for further analysis.</p>
</div>
</section>
<section id="numerical-stability-welford-s-algorithm">
<h3>Numerical Stability: Welford‚Äôs Algorithm<a class="headerlink" href="#numerical-stability-welford-s-algorithm" title="Link to this heading">ÔÉÅ</a></h3>
<p>Computing the sample variance naively using the one-pass formula <span class="math notranslate nohighlight">\(\frac{1}{n-1}\left(\sum h_i^2 - \frac{(\sum h_i)^2}{n}\right)\)</span> can suffer catastrophic cancellation when the mean is large compared to the standard deviation. The two terms <span class="math notranslate nohighlight">\(\sum h_i^2\)</span> and <span class="math notranslate nohighlight">\(\frac{(\sum h_i)^2}{n}\)</span> may be nearly equal, and their difference may lose many significant digits.</p>
<p><strong>The Problem Illustrated</strong>: Suppose we have data with mean <span class="math notranslate nohighlight">\(\mu = 10^9\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma = 1\)</span>. Then <span class="math notranslate nohighlight">\(\sum x_i^2 \approx n \cdot 10^{18}\)</span> and <span class="math notranslate nohighlight">\((\sum x_i)^2/n \approx n \cdot 10^{18}\)</span> as well. Their difference should be approximately <span class="math notranslate nohighlight">\(n \cdot \sigma^2 = n\)</span>, but when subtracting two numbers of size <span class="math notranslate nohighlight">\(10^{18}\)</span> that agree in their first 16-17 digits, we lose almost all precision in 64-bit floating point arithmetic (which has about 15-16 significant decimal digits).</p>
<p><strong>Welford‚Äôs Insight</strong>: Instead of computing variance from <span class="math notranslate nohighlight">\(\sum x_i^2\)</span> and <span class="math notranslate nohighlight">\(\sum x_i\)</span>, we can maintain the sum of squared deviations <em>from the current running mean</em>. As each new observation arrives, we update both the mean and the sum of squared deviations using a clever algebraic identity.</p>
<p>Let <span class="math notranslate nohighlight">\(\bar{x}_n\)</span> denote the mean of the first <span class="math notranslate nohighlight">\(n\)</span> observations, and let <span class="math notranslate nohighlight">\(M_{2,n} = \sum_{i=1}^n (x_i - \bar{x}_n)^2\)</span> denote the sum of squared deviations from this mean. Welford showed that these can be updated incrementally:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\bar{x}_n &amp;= \bar{x}_{n-1} + \frac{x_n - \bar{x}_{n-1}}{n} \\
M_{2,n} &amp;= M_{2,n-1} + (x_n - \bar{x}_{n-1})(x_n - \bar{x}_n)\end{split}\]</div>
<p>The key insight is that <span class="math notranslate nohighlight">\((x_n - \bar{x}_{n-1})\)</span> and <span class="math notranslate nohighlight">\((x_n - \bar{x}_n)\)</span> are both small numbers (deviations from means), so their product is numerically stable. We never subtract two large, nearly-equal quantities.</p>
<p>The sample variance is then simply <span class="math notranslate nohighlight">\(s^2 = M_{2,n} / (n-1)\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">WelfordAccumulator</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Online algorithm for computing mean and variance in a single pass.</span>

<span class="sd">    This is a true streaming algorithm: we never store the data,</span>
<span class="sd">    only the running statistics. Memory usage is O(1) regardless</span>
<span class="sd">    of how many values we process.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">M2</span> <span class="o">=</span> <span class="mf">0.0</span>  <span class="c1"># Sum of squared deviations from current mean</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Process a single new observation.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">+=</span> <span class="n">delta</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span>
        <span class="n">delta2</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>  <span class="c1"># Note: uses UPDATED mean</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">M2</span> <span class="o">+=</span> <span class="n">delta</span> <span class="o">*</span> <span class="n">delta2</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">variance</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sample variance (with Bessel&#39;s correction).&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;nan&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">M2</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">std</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sample standard deviation.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">variance</span><span class="p">)</span>

<span class="c1"># Generate data once (in practice, this would arrive as a stream)</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">5678</span><span class="p">)</span>
<span class="n">large_mean_data</span> <span class="o">=</span> <span class="mf">1e9</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span>  <span class="c1"># Mean ‚âà 10‚Åπ, SD ‚âà 1</span>

<span class="c1"># Demonstrate the ONLINE nature: process data one value at a time</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">WelfordAccumulator</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Processing data as a stream (mean ‚âà 10‚Åπ, SD ‚âà 1)...&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;n&#39;</span><span class="si">:</span><span class="s2">&gt;8</span><span class="si">}</span><span class="s2">  </span><span class="si">{</span><span class="s1">&#39;Running Mean&#39;</span><span class="si">:</span><span class="s2">&gt;20</span><span class="si">}</span><span class="s2">  </span><span class="si">{</span><span class="s1">&#39;Running Variance&#39;</span><span class="si">:</span><span class="s2">&gt;18</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">52</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">large_mean_data</span><span class="p">):</span>
    <span class="c1"># In a real streaming application, x would arrive from a sensor,</span>
    <span class="c1"># network connection, etc. We process it and discard it.</span>
    <span class="n">acc</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Periodically report running statistics</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">]:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">acc</span><span class="o">.</span><span class="n">n</span><span class="si">:</span><span class="s2">&gt;8</span><span class="si">}</span><span class="s2">  </span><span class="si">{</span><span class="n">acc</span><span class="o">.</span><span class="n">mean</span><span class="si">:</span><span class="s2">&gt;20.6f</span><span class="si">}</span><span class="s2">  </span><span class="si">{</span><span class="n">acc</span><span class="o">.</span><span class="n">variance</span><span class="si">:</span><span class="s2">&gt;18.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Final Welford estimates: mean = </span><span class="si">{</span><span class="n">acc</span><span class="o">.</span><span class="n">mean</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, var = </span><span class="si">{</span><span class="n">acc</span><span class="o">.</span><span class="n">variance</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The output shows the algorithm refining its estimates as more data streams in:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Processing data as a stream (mean ‚âà 10‚Åπ, SD ‚âà 1)...
       n          Running Mean    Running Variance
----------------------------------------------------
     10      999999999.769556          0.723562
    100     1000000000.024273          0.934600
   1000      999999999.989465          0.974390
  10000      999999999.991273          0.962625

Final Welford estimates: mean = 999999999.99, var = 0.962625
</pre></div>
</div>
<p><strong>Why this matters</strong>: In a true streaming scenario, we would process values as they arrive without ever storing them. Memory usage is O(1)‚Äîjust three numbers (n, mean, M2)‚Äîregardless of how much data we process. The naive formula would require either storing all values (O(n) memory) or suffer catastrophic cancellation.</p>
<p>Now let‚Äôs see what happens with the naive one-pass formula on the same data:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compare stable vs unstable on the SAME data</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">large_mean_data</span><span class="p">)</span>

<span class="c1"># One-pass naive formula (UNSTABLE): Var = (Œ£x¬≤ - (Œ£x)¬≤/n) / (n-1)</span>
<span class="n">sum_sq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">large_mean_data</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">sum_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">large_mean_data</span><span class="p">)</span>
<span class="n">naive_var_onepass</span> <span class="o">=</span> <span class="p">(</span><span class="n">sum_sq</span> <span class="o">-</span> <span class="n">sum_x</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># NumPy&#39;s implementation (also stable)</span>
<span class="n">numpy_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">large_mean_data</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True variance (approx): 1.0&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;One-pass naive:         </span><span class="si">{</span><span class="n">naive_var_onepass</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">  &lt;- CATASTROPHIC FAILURE!&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Welford (online):       </span><span class="si">{</span><span class="n">acc</span><span class="o">.</span><span class="n">variance</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;NumPy variance:         </span><span class="si">{</span><span class="n">numpy_var</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>True variance (approx): 1.0
One-pass naive:         209.74  &lt;- CATASTROPHIC FAILURE!
Welford (online):       0.962625
NumPy variance:         0.962625
</pre></div>
</div>
<p>The naive formula gives 209.74 instead of ‚âà1.0‚Äîa 200√ó error! Both Welford and NumPy (which uses a stable two-pass algorithm) give the correct answer.</p>
<p>In practice, NumPy‚Äôs <code class="docutils literal notranslate"><span class="pre">np.var</span></code> uses numerically stable algorithms, so you rarely need to implement Welford‚Äôs algorithm yourself. But understanding why stability matters is important for several reasons:</p>
<ol class="arabic simple">
<li><p><strong>Debugging unexpected results</strong>: If you‚Äôre computing variances in a custom loop or in a language without stable built-ins, you may encounter this issue.</p></li>
<li><p><strong>Streaming data</strong>: Welford‚Äôs algorithm processes data in a single pass, making it ideal for streaming applications where you can‚Äôt store all values in memory.</p></li>
<li><p><strong>Parallel computation</strong>: The algorithm can be extended to combine statistics from separate batches (useful for distributed computing).</p></li>
<li><p><strong>Understanding Monte Carlo diagnostics</strong>: Running variance calculations in convergence diagnostics use similar techniques.</p></li>
</ol>
<p>The key lesson: when the mean is large relative to the standard deviation, naive variance formulas fail catastrophically. Always use stable algorithms.</p>
</section>
</section>
<section id="worked-examples">
<h2>Worked Examples<a class="headerlink" href="#worked-examples" title="Link to this heading">ÔÉÅ</a></h2>
<p>We now work through several examples of increasing complexity, illustrating the breadth of Monte Carlo applications.</p>
<section id="example-1-the-gaussian-integral">
<h3>Example 1: The Gaussian Integral<a class="headerlink" href="#example-1-the-gaussian-integral" title="Link to this heading">ÔÉÅ</a></h3>
<p>The integral <span class="math notranslate nohighlight">\(\int_{-\infty}^{\infty} e^{-x^2} dx = \sqrt{\pi}\)</span> is famous for being impossible to evaluate in closed form using elementary antiderivatives, yet having a beautiful exact answer. Let us estimate it via Monte Carlo.</p>
<p><strong>Challenge</strong>: The domain is infinite, so we cannot sample uniformly.</p>
<p><strong>Solution</strong>: Recognize the integrand as proportional to a Gaussian density. If <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(0, 1/\sqrt{2})\)</span>, then:</p>
<div class="math notranslate nohighlight">
\[f(x) = \sqrt{\frac{1}{\pi}} e^{-x^2}\]</div>
<p>This is a valid probability density (it integrates to 1). We can write:</p>
<div class="math notranslate nohighlight">
\[\int_{-\infty}^{\infty} e^{-x^2} dx = \int_{-\infty}^{\infty} \frac{e^{-x^2}}{f(x)} f(x) dx = \sqrt{\pi} \int_{-\infty}^{\infty} f(x) dx = \sqrt{\pi}\]</div>
<p>This derivation shows the integral equals <span class="math notranslate nohighlight">\(\sqrt{\pi}\)</span> exactly, but let‚Äôs also verify by Monte Carlo.</p>
<p><strong>Alternative approach</strong>: Sample from a distribution that covers the domain and reweight:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">gaussian_integral_mc</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Estimate ‚à´ exp(-x¬≤) dx via Monte Carlo.</span>

<span class="sd">    We sample from N(0, 1) and use importance sampling:</span>
<span class="sd">    ‚à´ exp(-x¬≤) dx = ‚à´ [exp(-x¬≤) / œÜ(x)] œÜ(x) dx</span>
<span class="sd">    where œÜ(x) is the standard normal density.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Sample from N(0, 1)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Integrand: exp(-x¬≤)</span>
    <span class="c1"># Sampling density: œÜ(x) = exp(-x¬≤/2) / sqrt(2œÄ)</span>
    <span class="c1"># Ratio: exp(-x¬≤) / œÜ(x) = sqrt(2œÄ) * exp(-x¬≤/2)</span>

    <span class="n">h_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">X</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>

    <span class="n">estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_values</span><span class="p">)</span>
    <span class="n">std_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">h_values</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">estimate</span><span class="p">,</span> <span class="n">std_error</span>

<span class="n">est</span><span class="p">,</span> <span class="n">se</span> <span class="o">=</span> <span class="n">gaussian_integral_mc</span><span class="p">(</span><span class="mi">100_000</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimate: </span><span class="si">{</span><span class="n">est</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> ¬± </span><span class="si">{</span><span class="n">se</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True value: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Estimate: 1.770751 ¬± 0.002214
True value: 1.772454
</pre></div>
</div>
<p>The estimate is very close to <span class="math notranslate nohighlight">\(\sqrt{\pi} \approx 1.7725\)</span>.</p>
</section>
<section id="example-2-probability-of-a-rare-event">
<h3>Example 2: Probability of a Rare Event<a class="headerlink" href="#example-2-probability-of-a-rare-event" title="Link to this heading">ÔÉÅ</a></h3>
<p>Suppose <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(0, 1)\)</span>. What is <span class="math notranslate nohighlight">\(P(X &gt; 4)\)</span>?</p>
<p>From standard normal tables, <span class="math notranslate nohighlight">\(P(X &gt; 4) = 1 - \Phi(4) \approx 3.167 \times 10^{-5}\)</span>. This is a rare event‚Äîonly about 3 in 100,000 standard normal draws exceed 4.</p>
<p><strong>Naive Monte Carlo</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100_000</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">p_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span> <span class="o">&gt;</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">p_hat</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_hat</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimate: </span><span class="si">{</span><span class="n">p_hat</span><span class="si">:</span><span class="s2">.6e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Std Error: </span><span class="si">{</span><span class="n">se</span><span class="si">:</span><span class="s2">.6e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True value: </span><span class="si">{</span><span class="mi">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="si">:</span><span class="s2">.6e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Estimate: 9.000000e-05
Std Error: 2.999865e-05
True value: 3.167124e-05
</pre></div>
</div>
<p>With 100,000 samples, we observed 9 exceedances (instead of the expected ~3), giving an estimate nearly 3√ó too large! This illustrates the high variability inherent in estimating rare events. The relative error (standard error divided by estimate) is enormous.</p>
<p><strong>Problem</strong>: To estimate a probability <span class="math notranslate nohighlight">\(p\)</span>, the standard error is <span class="math notranslate nohighlight">\(\sqrt{p(1-p)/n} \approx \sqrt{p/n}\)</span> for small <span class="math notranslate nohighlight">\(p\)</span>. The relative error is <span class="math notranslate nohighlight">\(\sqrt{(1-p)/(np)} \approx 1/\sqrt{np}\)</span>. To achieve 10% relative error for <span class="math notranslate nohighlight">\(p = 10^{-5}\)</span>, we need <span class="math notranslate nohighlight">\(n \approx 100/p = 10^7\)</span> samples.</p>
<p>This motivates <strong>importance sampling</strong> (covered in a later section), which generates samples preferentially in the region of interest. For now, the lesson is that naive Monte Carlo struggles with rare events.</p>
</section>
<section id="example-3-a-high-dimensional-integral">
<h3>Example 3: A High-Dimensional Integral<a class="headerlink" href="#example-3-a-high-dimensional-integral" title="Link to this heading">ÔÉÅ</a></h3>
<p>Consider the integral:</p>
<div class="math notranslate nohighlight">
\[I = \int_{[0,1]^d} \prod_{j=1}^{d} \left( 1 + \frac{x_j}{d} \right) dx_1 \cdots dx_d\]</div>
<p>For large <span class="math notranslate nohighlight">\(d\)</span>, this integral is analytically tractable. The key observation is that the integrand <em>factors</em> into a product of functions, each depending on only one variable:</p>
<div class="math notranslate nohighlight">
\[\prod_{j=1}^{d} \left( 1 + \frac{x_j}{d} \right) = \underbrace{\left(1 + \frac{x_1}{d}\right)}_{f_1(x_1)} \cdot \underbrace{\left(1 + \frac{x_2}{d}\right)}_{f_2(x_2)} \cdots \underbrace{\left(1 + \frac{x_d}{d}\right)}_{f_d(x_d)}\]</div>
<p>When an integrand factors this way, <strong>Fubini‚Äôs theorem</strong> tells us the multiple integral equals the product of single integrals:</p>
<div class="math notranslate nohighlight">
\[\int_{[0,1]^d} f_1(x_1) \cdot f_2(x_2) \cdots f_d(x_d) \, dx_1 \cdots dx_d = \left(\int_0^1 f_1(x_1) \, dx_1\right) \cdot \left(\int_0^1 f_2(x_2) \, dx_2\right) \cdots \left(\int_0^1 f_d(x_d) \, dx_d\right)\]</div>
<p>This is analogous to how <span class="math notranslate nohighlight">\(\sum_{i,j} a_i b_j = (\sum_i a_i)(\sum_j b_j)\)</span> for sums. The factorization works because each <span class="math notranslate nohighlight">\(x_j\)</span> integrates independently over its own copy of <span class="math notranslate nohighlight">\([0,1]\)</span>.</p>
<p>Applying this to our integrand, each single integral evaluates to:</p>
<div class="math notranslate nohighlight">
\[\int_0^1 \left(1 + \frac{x_j}{d}\right) dx_j = \left[x_j + \frac{x_j^2}{2d}\right]_0^1 = 1 + \frac{1}{2d}\]</div>
<p>Since all <span class="math notranslate nohighlight">\(d\)</span> integrals are identical, we get:</p>
<div class="math notranslate nohighlight">
\[I = \prod_{j=1}^{d} \int_0^1 \left( 1 + \frac{x_j}{d} \right) dx_j = \left( 1 + \frac{1}{2d} \right)^d \xrightarrow{d \to \infty} \sqrt{e}\]</div>
<p>The limit <span class="math notranslate nohighlight">\(\sqrt{e}\)</span> follows from the definition of <span class="math notranslate nohighlight">\(e\)</span> as <span class="math notranslate nohighlight">\(\lim_{n \to \infty} (1 + 1/n)^n\)</span>. To see this, write:</p>
<div class="math notranslate nohighlight">
\[\left(1 + \frac{1}{2d}\right)^d = \left[\left(1 + \frac{1}{2d}\right)^{2d}\right]^{1/2} \xrightarrow{d \to \infty} e^{1/2} = \sqrt{e}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We chose this particular integrand <em>because</em> it factors nicely, giving us a known true value to verify our Monte Carlo estimates. Most high-dimensional integrands do not factor this way‚Äîthe variables interact in complex ways‚Äîwhich is precisely why Monte Carlo methods are essential. When there‚Äôs no analytical solution, Monte Carlo may be the only practical approach.</p>
</div>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_1_fig06_high_dim_integral.png"><img alt="Three-panel visualization of high-dimensional integral showing true value convergence, error distributions, and parallel convergence rates" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_1_fig06_high_dim_integral.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.5 </span><span class="caption-text"><strong>High-Dimensional Integration.</strong> Left: The true integral value approaches <span class="math notranslate nohighlight">\(\sqrt{e}\)</span> as dimension increases. Middle: At fixed sample size <span class="math notranslate nohighlight">\(n = 10{,}000\)</span>, error spread actually <em>decreases</em> with dimension for this integrand‚Äîeach factor <span class="math notranslate nohighlight">\((1 + x_j/d)\)</span> becomes closer to 1 as <span class="math notranslate nohighlight">\(d\)</span> grows, reducing variance. This is integrand-specific, not a general property. Right: The key insight is that the <strong>convergence rate</strong> <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> is identical across all dimensions‚Äîthe curves are parallel on the log-log plot. The constant (vertical offset) may vary, but the slope does not. This dimension-independent rate is Monte Carlo‚Äôs superpower.</span><a class="headerlink" href="#id5" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<p>Let us estimate this integral in <span class="math notranslate nohighlight">\(d = 100\)</span> dimensions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">high_dim_integrand</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute product integrand: ‚àè(1 + x_j/d).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : ndarray of shape (n_samples, d)</span>
<span class="sd">        Sample points in [0,1]^d.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ndarray of shape (n_samples,)</span>
<span class="sd">        Function values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">X</span> <span class="o">/</span> <span class="n">d</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">mc_high_dim</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Monte Carlo integration in d dimensions.&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Uniform samples in [0,1]^d</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>

    <span class="c1"># Evaluate integrand</span>
    <span class="n">h_values</span> <span class="o">=</span> <span class="n">high_dim_integrand</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="n">estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_values</span><span class="p">)</span>
    <span class="n">std_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">h_values</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># True value (for comparison)</span>
    <span class="n">true_value</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">/</span><span class="n">d</span><span class="p">)</span><span class="o">**</span><span class="n">d</span>

    <span class="k">return</span> <span class="n">estimate</span><span class="p">,</span> <span class="n">std_error</span><span class="p">,</span> <span class="n">true_value</span>

<span class="c1"># Estimate in 100 dimensions</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">est</span><span class="p">,</span> <span class="n">se</span><span class="p">,</span> <span class="n">truth</span> <span class="o">=</span> <span class="n">mc_high_dim</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="mi">100_000</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;d = </span><span class="si">{</span><span class="n">d</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimate: </span><span class="si">{</span><span class="n">est</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> ¬± </span><span class="si">{</span><span class="n">se</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True value: </span><span class="si">{</span><span class="n">truth</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error: </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">est</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">truth</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>d = 100
Estimate: 1.646530 ¬± 0.000149
True value: 1.646668
Error: 0.000138
</pre></div>
</div>
<p>The Monte Carlo estimate converges as <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> regardless of <span class="math notranslate nohighlight">\(d\)</span>. Try increasing <span class="math notranslate nohighlight">\(d\)</span> to 1000 or 10,000‚Äîthe convergence rate remains unchanged.</p>
</section>
<section id="example-4-bayesian-posterior-mean">
<h3>Example 4: Bayesian Posterior Mean<a class="headerlink" href="#example-4-bayesian-posterior-mean" title="Link to this heading">ÔÉÅ</a></h3>
<p>Bayesian inference often requires computing posterior expectations:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[\theta | \text{data}] = \int \theta \cdot \pi(\theta | \text{data}) \, d\theta\]</div>
<p>When the posterior <span class="math notranslate nohighlight">\(\pi(\theta | \text{data})\)</span> is available (perhaps up to a normalizing constant), Monte Carlo integration applies directly‚Äîif we can sample from the posterior. This is the motivation for Markov chain Monte Carlo methods in Part 3.</p>
<p>As a simple example, suppose we observe <span class="math notranslate nohighlight">\(x = 7\)</span> successes in <span class="math notranslate nohighlight">\(n = 10\)</span> Bernoulli trials with unknown success probability <span class="math notranslate nohighlight">\(\theta\)</span>. With a <span class="math notranslate nohighlight">\(\text{Beta}(1, 1)\)</span> (uniform) prior, the posterior is <span class="math notranslate nohighlight">\(\text{Beta}(8, 4)\)</span>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Posterior is Beta(8, 4)</span>
<span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span>
<span class="n">posterior</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>

<span class="c1"># True posterior mean</span>
<span class="n">true_mean</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">/</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="p">)</span>

<span class="c1"># Monte Carlo estimate</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10_000</span>
<span class="n">theta_samples</span> <span class="o">=</span> <span class="n">posterior</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">mc_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">theta_samples</span><span class="p">)</span>
<span class="n">mc_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">theta_samples</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True posterior mean: </span><span class="si">{</span><span class="n">true_mean</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MC estimate: </span><span class="si">{</span><span class="n">mc_mean</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> ¬± </span><span class="si">{</span><span class="n">mc_se</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># We can also estimate posterior quantiles, variance, etc.</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Posterior median (MC): </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">theta_samples</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Posterior median (exact): </span><span class="si">{</span><span class="n">posterior</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>True posterior mean: 0.666667
MC estimate: 0.666200 ¬± 0.001320
Posterior median (MC): 0.675577
Posterior median (exact): 0.676196
</pre></div>
</div>
<p>The Monte Carlo estimates closely match the exact values. In more complex Bayesian models where the posterior has no closed form, Monte Carlo (via MCMC) becomes essential.</p>
</section>
<section id="example-5-the-normal-cdf">
<h3>Example 5: The Normal CDF<a class="headerlink" href="#example-5-the-normal-cdf" title="Link to this heading">ÔÉÅ</a></h3>
<p>The cumulative distribution function of the standard normal, <span class="math notranslate nohighlight">\(\Phi(t) = P(Z \leq t)\)</span> for <span class="math notranslate nohighlight">\(Z \sim \mathcal{N}(0, 1)\)</span>, has no closed-form expression. Yet it is one of the most important functions in statistics. Let us estimate <span class="math notranslate nohighlight">\(\Phi(1.96)\)</span>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">estimate_normal_cdf</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Estimate Œ¶(t) = P(Z ‚â§ t) for Z ~ N(0,1).&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Indicator function: 1 if Z ‚â§ t, else 0</span>
    <span class="n">indicators</span> <span class="o">=</span> <span class="n">Z</span> <span class="o">&lt;=</span> <span class="n">t</span>

    <span class="n">p_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">indicators</span><span class="p">)</span>
    <span class="n">se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">p_hat</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_hat</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">p_hat</span><span class="p">,</span> <span class="n">se</span>

<span class="n">t</span> <span class="o">=</span> <span class="mf">1.96</span>
<span class="n">est</span><span class="p">,</span> <span class="n">se</span> <span class="o">=</span> <span class="n">estimate_normal_cdf</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="mi">1_000_000</span><span class="p">)</span>
<span class="n">true_val</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Œ¶(</span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s2">) estimate: </span><span class="si">{</span><span class="n">est</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> ¬± </span><span class="si">{</span><span class="n">se</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Œ¶(</span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s2">) true:     </span><span class="si">{</span><span class="n">true_val</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Œ¶(1.96) estimate: 0.974928 ¬± 0.000156
Œ¶(1.96) true:     0.975002
</pre></div>
</div>
<p>With one million samples, the estimate is accurate to about four decimal places. For extreme quantiles (e.g., <span class="math notranslate nohighlight">\(t = 5\)</span>), the probability is so small that accurate estimation requires importance sampling.</p>
</section>
</section>
<section id="comparison-with-deterministic-methods">
<h2>Comparison with Deterministic Methods<a class="headerlink" href="#comparison-with-deterministic-methods" title="Link to this heading">ÔÉÅ</a></h2>
<p>Monte Carlo integration is not the only way to compute integrals numerically. Deterministic quadrature methods‚Äîthe trapezoidal rule, Simpson‚Äôs rule, Gaussian quadrature‚Äîhave been studied for centuries and, in low dimensions, often outperform Monte Carlo. Understanding when to use which approach is essential for the computational practitioner.</p>
<section id="one-dimensional-quadrature">
<h3>One-Dimensional Quadrature<a class="headerlink" href="#one-dimensional-quadrature" title="Link to this heading">ÔÉÅ</a></h3>
<p>For a one-dimensional integral <span class="math notranslate nohighlight">\(\int_a^b f(x) dx\)</span>, deterministic methods exploit the smoothness of <span class="math notranslate nohighlight">\(f\)</span> to achieve rapid convergence. The core idea is to approximate the integral as a weighted sum of function values at selected points:</p>
<div class="math notranslate nohighlight">
\[\int_a^b f(x) dx \approx \sum_{i=1}^{n} w_i f(x_i)\]</div>
<p>The methods differ in how they choose the points <span class="math notranslate nohighlight">\(x_i\)</span> and weights <span class="math notranslate nohighlight">\(w_i\)</span>.</p>
<p><strong>Trapezoidal Rule</strong>: Approximate the integrand by piecewise linear functions connecting adjacent points. For <span class="math notranslate nohighlight">\(n\)</span> equally spaced points <span class="math notranslate nohighlight">\(x_0, x_1, \ldots, x_{n-1}\)</span> with spacing <span class="math notranslate nohighlight">\(h = (b-a)/(n-1)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\int_a^b f(x) dx \approx h \left[ \frac{f(x_0)}{2} + f(x_1) + f(x_2) + \cdots + f(x_{n-2}) + \frac{f(x_{n-1})}{2} \right]\]</div>
<p>Geometrically, this approximates the area under the curve by a series of trapezoids. The error is <span class="math notranslate nohighlight">\(O(h^2) = O(n^{-2})\)</span> for twice-differentiable <span class="math notranslate nohighlight">\(f\)</span>‚Äîdoubling the number of points reduces the error by a factor of 4.</p>
<p><strong>Simpson‚Äôs Rule</strong>: Approximate by piecewise quadratic (parabolic) curves through consecutive triples of points. For an odd number of equally spaced points:</p>
<div class="math notranslate nohighlight">
\[\int_a^b f(x) dx \approx \frac{h}{3} \left[ f(x_0) + 4f(x_1) + 2f(x_2) + 4f(x_3) + \cdots + 4f(x_{n-2}) + f(x_{n-1}) \right]\]</div>
<p>The alternating pattern of coefficients (1, 4, 2, 4, 2, ‚Ä¶, 4, 1) arises from fitting parabolas through each group of three points. The error is <span class="math notranslate nohighlight">\(O(h^4) = O(n^{-4})\)</span> for sufficiently smooth <span class="math notranslate nohighlight">\(f\)</span>‚Äîdoubling the points reduces error by a factor of 16.</p>
<p><strong>Gaussian Quadrature</strong>: Rather than using equally spaced points, choose both points <span class="math notranslate nohighlight">\(x_i\)</span> and weights <span class="math notranslate nohighlight">\(w_i\)</span> to maximize accuracy. With <span class="math notranslate nohighlight">\(n\)</span> optimally chosen points, Gaussian quadrature integrates polynomials of degree up to <span class="math notranslate nohighlight">\(2n-1\)</span> <em>exactly</em>. For analytic functions, convergence can be exponentially fast‚Äîfar better than any fixed polynomial rate.</p>
<p>The optimal points turn out to be roots of orthogonal polynomials (Legendre polynomials for integration on <span class="math notranslate nohighlight">\([-1, 1]\)</span>). SciPy‚Äôs <code class="docutils literal notranslate"><span class="pre">scipy.integrate.quad</span></code> uses adaptive Gaussian quadrature internally.</p>
<p>The geometric difference between these approaches is illuminating:</p>
<figure class="align-center" id="id6">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_1_fig05_sampling_strategies.png"><img alt="Six-panel comparison of grid-based quadrature versus Monte Carlo sampling in 1D and 2D" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_1_fig05_sampling_strategies.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.6 </span><span class="caption-text"><strong>Grid vs. Monte Carlo Sampling.</strong> Top row: In 1D, grid sampling (left) places evaluation points at regular intervals, while Monte Carlo (middle) uses random points. Quadrature wins decisively in 1D (right)‚Äîthis is expected and correct. Bottom row: In 2D, a 10√ó10 grid uses 100 points in a regular lattice (left), while Monte Carlo distributes 100 points without structure (middle). The crucial insight (right): grid cost grows as <span class="math notranslate nohighlight">\(m^d\)</span> while Monte Carlo is dimension-independent; the crossover occurs around <span class="math notranslate nohighlight">\(d \approx 3\)</span>, after which Monte Carlo wins.</span><a class="headerlink" href="#id6" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<p>Compare these to Monte Carlo‚Äôs <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span>. In one dimension, Monte Carlo loses badly:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">integrate</span>

<span class="c1"># Integrand: exp(-x¬≤) on [0, 2]</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># True value (via error function)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.special</span><span class="w"> </span><span class="kn">import</span> <span class="n">erf</span>
<span class="n">true_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">erf</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Monte Carlo</span>
<span class="k">def</span><span class="w"> </span><span class="nf">mc_estimate</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># Multiply by interval length</span>

<span class="c1"># Trapezoidal rule</span>
<span class="k">def</span><span class="w"> </span><span class="nf">trap_estimate</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">trapz</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># Simpson&#39;s rule</span>
<span class="k">def</span><span class="w"> </span><span class="nf">simp_estimate</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">integrate</span><span class="o">.</span><span class="n">simpson</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True value: </span><span class="si">{</span><span class="n">true_value</span><span class="si">:</span><span class="s2">.10f</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]:</span>
    <span class="n">mc</span> <span class="o">=</span> <span class="n">mc_estimate</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">tr</span> <span class="o">=</span> <span class="n">trap_estimate</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">si</span> <span class="o">=</span> <span class="n">simp_estimate</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Monte Carlo: </span><span class="si">{</span><span class="n">mc</span><span class="si">:</span><span class="s2">.10f</span><span class="si">}</span><span class="s2">  (error </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">mc</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">true_value</span><span class="p">)</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Trapezoidal: </span><span class="si">{</span><span class="n">tr</span><span class="si">:</span><span class="s2">.10f</span><span class="si">}</span><span class="s2">  (error </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">tr</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">true_value</span><span class="p">)</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Simpson:     </span><span class="si">{</span><span class="n">si</span><span class="si">:</span><span class="s2">.10f</span><span class="si">}</span><span class="s2">  (error </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">si</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">true_value</span><span class="p">)</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>True value: 0.8820813908

n = 10
  Monte Carlo: 0.6600529708  (error 2.22e-01)
  Trapezoidal: 0.8817823811  (error 2.99e-04)
  Simpson:     0.8819697523  (error 1.12e-04)

n = 100
  Monte Carlo: 0.8973426558  (error 1.53e-02)
  Trapezoidal: 0.8820788993  (error 2.49e-06)
  Simpson:     0.8820813848  (error 5.96e-09)

n = 1000
  Monte Carlo: 0.8906166325  (error 8.54e-03)
  Trapezoidal: 0.8820813663  (error 2.45e-08)
  Simpson:     0.8820813908  (error 5.58e-13)
</pre></div>
</div>
<p>With <span class="math notranslate nohighlight">\(n = 1000\)</span> points, Simpson‚Äôs rule achieves machine precision (<span class="math notranslate nohighlight">\(10^{-13}\)</span>) while Monte Carlo‚Äôs error is still around <span class="math notranslate nohighlight">\(10^{-2}\)</span>. In one dimension, there is no contest.</p>
</section>
<section id="the-curse-of-dimensionality">
<h3>The Curse of Dimensionality<a class="headerlink" href="#the-curse-of-dimensionality" title="Link to this heading">ÔÉÅ</a></h3>
<p>The situation reverses dramatically in high dimensions. Consider integrating over <span class="math notranslate nohighlight">\([0, 1]^d\)</span>. A deterministic method using a grid with <span class="math notranslate nohighlight">\(m\)</span> points per dimension requires <span class="math notranslate nohighlight">\(m^d\)</span> total evaluations.</p>
<p>To build geometric intuition for why high dimensions are so challenging, consider a simple question: what fraction of a hypercube‚Äôs volume lies within the inscribed hypersphere?</p>
<p><strong>Volume formulas.</strong> Consider the hypercube <span class="math notranslate nohighlight">\([-1, 1]^d\)</span> with side length 2 and the unit hypersphere <span class="math notranslate nohighlight">\(\{x : \|x\| \leq 1\}\)</span> inscribed within it.</p>
<p>The <strong>hypercube volume</strong> is simply:</p>
<div class="math notranslate nohighlight">
\[V_{\text{cube}}(d) = 2^d\]</div>
<p>The <strong>hypersphere volume</strong> in <span class="math notranslate nohighlight">\(d\)</span> dimensions is:</p>
<div class="math notranslate nohighlight">
\[V_{\text{sphere}}(d) = \frac{\pi^{d/2}}{\Gamma(d/2 + 1)}\]</div>
<p>where <span class="math notranslate nohighlight">\(\Gamma\)</span> is the gamma function. For positive integers, <span class="math notranslate nohighlight">\(\Gamma(n+1) = n!\)</span>. For half-integers, <span class="math notranslate nohighlight">\(\Gamma(1/2) = \sqrt{\pi}\)</span> and <span class="math notranslate nohighlight">\(\Gamma(n + 1/2) = \frac{(2n-1)!!}{2^n}\sqrt{\pi}\)</span> where <span class="math notranslate nohighlight">\(!!\)</span> denotes the double factorial. For practical computation, use <code class="docutils literal notranslate"><span class="pre">scipy.special.gamma</span></code>.</p>
<p>This formula gives familiar results: <span class="math notranslate nohighlight">\(V_2 = \pi\)</span> (area of unit circle), <span class="math notranslate nohighlight">\(V_3 = \frac{4\pi}{3}\)</span> (volume of unit sphere). For higher dimensions: <span class="math notranslate nohighlight">\(V_4 = \frac{\pi^2}{2}\)</span>, <span class="math notranslate nohighlight">\(V_5 = \frac{8\pi^2}{15}\)</span>, and so on.</p>
<p>The <strong>ratio</strong> of sphere volume to cube volume is:</p>
<div class="math notranslate nohighlight">
\[\frac{V_{\text{sphere}}(d)}{V_{\text{cube}}(d)} = \frac{\pi^{d/2}}{2^d \, \Gamma(d/2 + 1)}\]</div>
<p>Let‚Äôs compute this ratio for several dimensions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.special</span><span class="w"> </span><span class="kn">import</span> <span class="n">gamma</span>

<span class="k">def</span><span class="w"> </span><span class="nf">hypersphere_volume</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Volume of d-dimensional hypersphere with radius r.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">**</span> <span class="p">(</span><span class="n">d</span><span class="o">/</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">gamma</span><span class="p">(</span><span class="n">d</span><span class="o">/</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">r</span><span class="o">**</span><span class="n">d</span>

<span class="k">def</span><span class="w"> </span><span class="nf">hypercube_volume</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">side</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Volume of d-dimensional hypercube with given side length.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">side</span> <span class="o">**</span> <span class="n">d</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;d&#39;</span><span class="si">:</span><span class="s2">&gt;3</span><span class="si">}</span><span class="s2">  </span><span class="si">{</span><span class="s1">&#39;Cube&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">  </span><span class="si">{</span><span class="s1">&#39;Sphere&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">  </span><span class="si">{</span><span class="s1">&#39;Ratio&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">  </span><span class="si">{</span><span class="s1">&#39;</span><span class="si">% i</span><span class="s1">n sphere&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">58</span><span class="p">)</span>

<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">]:</span>
    <span class="n">cube_vol</span> <span class="o">=</span> <span class="n">hypercube_volume</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
    <span class="n">sphere_vol</span> <span class="o">=</span> <span class="n">hypersphere_volume</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
    <span class="n">ratio</span> <span class="o">=</span> <span class="n">sphere_vol</span> <span class="o">/</span> <span class="n">cube_vol</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">d</span><span class="si">:</span><span class="s2">&gt;3</span><span class="si">}</span><span class="s2">  </span><span class="si">{</span><span class="n">cube_vol</span><span class="si">:</span><span class="s2">&gt;12.2e</span><span class="si">}</span><span class="s2">  </span><span class="si">{</span><span class="n">sphere_vol</span><span class="si">:</span><span class="s2">&gt;12.4e</span><span class="si">}</span><span class="s2">  </span><span class="si">{</span><span class="n">ratio</span><span class="si">:</span><span class="s2">&gt;12.4e</span><span class="si">}</span><span class="s2">  </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">ratio</span><span class="si">:</span><span class="s2">&gt;11.2e</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>  d          Cube        Sphere         Ratio   % in sphere
----------------------------------------------------------
  1      2.00e+00    2.0000e+00    1.0000e+00     1.00e+02%
  2      4.00e+00    3.1416e+00    7.8540e-01     7.85e+01%
  3      8.00e+00    4.1888e+00    5.2360e-01     5.24e+01%
  5      3.20e+01    5.2638e+00    1.6449e-01     1.64e+01%
 10      1.02e+03    2.5502e+00    2.4904e-03     2.49e-01%
 20      1.05e+06    2.5807e-02    2.4611e-08     2.46e-06%
 50      1.13e+15    1.7302e-13    1.5367e-28     1.54e-26%
100      1.27e+30    2.3682e-40    1.8682e-70     1.87e-68%
</pre></div>
</div>
<p>The numbers are striking: in 10 dimensions, only 0.25% of the cube lies in the sphere. By 20 dimensions, it‚Äôs about <span class="math notranslate nohighlight">\(2.5 \times 10^{-6}`%. By 100 dimensions, the ratio is :math:`10^{-70}\)</span>‚Äîessentially zero.</p>
<p><strong>What does this mean for integration?</strong> If you‚Äôre trying to integrate a function that‚Äôs concentrated near the origin (like a multivariate Gaussian), random uniform samples over the hypercube will almost <em>never</em> land where the function is large. This is why importance sampling becomes essential in high dimensions.</p>
<figure class="align-center" id="id7">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_1_fig04_curse_of_dimensionality.png"><img alt="Three-panel visualization of the curse of dimensionality showing volume ratio decay, shell concentration, and grid point explosion" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_1_fig04_curse_of_dimensionality.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.7 </span><span class="caption-text"><strong>The Curse of Dimensionality.</strong> Left: The inscribed hypersphere occupies a vanishing fraction of the hypercube as dimension increases‚Äîby <span class="math notranslate nohighlight">\(d = 20\)</span>, less than <span class="math notranslate nohighlight">\(10^{-7}\)</span> of the cube‚Äôs volume lies within the sphere. Middle: In high dimensions, nearly all the volume of a ball concentrates in a thin shell near its surface. Right: Grid points required for deterministic methods explode exponentially‚Äîwith just 10 points per dimension, a 20-dimensional integral requires <span class="math notranslate nohighlight">\(10^{20}\)</span> evaluations.</span><a class="headerlink" href="#id7" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<p>For Simpson‚Äôs rule with error <span class="math notranslate nohighlight">\(O(h^4) = O(m^{-4})\)</span>, the total error is <span class="math notranslate nohighlight">\(O(m^{-4})\)</span> but the cost is <span class="math notranslate nohighlight">\(m^d\)</span>. If we want error <span class="math notranslate nohighlight">\(\epsilon\)</span>, we need <span class="math notranslate nohighlight">\(m \sim \epsilon^{-1/4}\)</span>, giving cost <span class="math notranslate nohighlight">\(\sim \epsilon^{-d/4}\)</span>.</p>
<p>For Monte Carlo with error <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span>, achieving error <span class="math notranslate nohighlight">\(\epsilon\)</span> requires <span class="math notranslate nohighlight">\(n \sim \epsilon^{-2}\)</span> samples, independent of <span class="math notranslate nohighlight">\(d\)</span>.</p>
<p>The crossover occurs when <span class="math notranslate nohighlight">\(\epsilon^{-d/4} = \epsilon^{-2}\)</span>, i.e., <span class="math notranslate nohighlight">\(d = 8\)</span>. For <span class="math notranslate nohighlight">\(d &gt; 8\)</span>, Monte Carlo requires fewer function evaluations than Simpson‚Äôs rule to achieve the same accuracy‚Äîand the advantage grows exponentially with dimension.</p>
<p>This analysis ignores constants, which can favor either method in specific cases. But the fundamental message is robust: <strong>in high dimensions, Monte Carlo wins</strong>.</p>
<table class="docutils align-default" id="id8">
<caption><span class="caption-number">Table 2.1 </span><span class="caption-text">Comparison of Integration Methods</span><a class="headerlink" href="#id8" title="Link to this table">ÔÉÅ</a></caption>
<colgroup>
<col style="width: 25.0%" />
<col style="width: 25.0%" />
<col style="width: 25.0%" />
<col style="width: 25.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>1D Convergence</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(d\)</span>-D Convergence</p></th>
<th class="head"><p>Best For</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Monte Carlo</p></td>
<td><p><span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span></p></td>
<td><p>High dimensions, complex domains</p></td>
</tr>
<tr class="row-odd"><td><p>Trapezoidal</p></td>
<td><p><span class="math notranslate nohighlight">\(O(n^{-2})\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(n^{-2/d})\)</span></p></td>
<td><p>Low-dim smooth functions</p></td>
</tr>
<tr class="row-even"><td><p>Simpson</p></td>
<td><p><span class="math notranslate nohighlight">\(O(n^{-4})\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(n^{-4/d})\)</span></p></td>
<td><p>Low-dim very smooth functions</p></td>
</tr>
<tr class="row-odd"><td><p>Gaussian</p></td>
<td><p><span class="math notranslate nohighlight">\(O(e^{-cn})\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(e^{-cn^{1/d}})\)</span></p></td>
<td><p>Low-dim analytic functions</p></td>
</tr>
<tr class="row-even"><td><p>Quasi-Monte Carlo</p></td>
<td><p><span class="math notranslate nohighlight">\(O(n^{-1}(\log n)^d)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(n^{-1}(\log n)^d)\)</span></p></td>
<td><p>Moderate dimensions, smooth functions</p></td>
</tr>
</tbody>
</table>
</section>
<section id="quasi-monte-carlo-methods">
<h3>Quasi-Monte Carlo Methods<a class="headerlink" href="#quasi-monte-carlo-methods" title="Link to this heading">ÔÉÅ</a></h3>
<p>A middle ground between deterministic quadrature and Monte Carlo is provided by <strong>quasi-Monte Carlo</strong> (QMC) methods. Instead of random samples, QMC uses carefully constructed <strong>low-discrepancy sequences</strong>‚Äîdeterministic sequences that fill space more uniformly than random points.</p>
<p>Famous examples include Halton sequences, Sobol sequences, and lattice rules. Under smoothness conditions on the integrand, QMC achieves convergence rates of <span class="math notranslate nohighlight">\(O(n^{-1} (\log n)^d)\)</span>, faster than Monte Carlo‚Äôs <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> but with a dependence on dimension.</p>
<p>QMC is increasingly popular in computational finance and computer graphics. However, it requires more care: the sequences must match the problem structure, variance estimation is trickier, and the smoothness assumptions may fail. For general-purpose integration, especially with non-smooth or high-variance integrands, standard Monte Carlo remains the most robust choice.</p>
</section>
</section>
<section id="sample-size-determination">
<h2>Sample Size Determination<a class="headerlink" href="#sample-size-determination" title="Link to this heading">ÔÉÅ</a></h2>
<p>A critical practical question is: <strong>how many samples do I need?</strong> The answer depends on the desired precision, the variance of the integrand, and the acceptable probability of error.</p>
<section id="the-sample-size-formula">
<h3>The Sample Size Formula<a class="headerlink" href="#the-sample-size-formula" title="Link to this heading">ÔÉÅ</a></h3>
<p>From the CLT, the Monte Carlo estimator is approximately:</p>
<div class="math notranslate nohighlight">
\[\hat{I}_n \sim \mathcal{N}\left( I, \frac{\sigma^2}{n} \right)\]</div>
<p>To achieve a standard error of <span class="math notranslate nohighlight">\(\epsilon\)</span>, we need:</p>
<div class="math notranslate nohighlight">
\[\frac{\sigma}{\sqrt{n}} \leq \epsilon \implies n \geq \frac{\sigma^2}{\epsilon^2}\]</div>
<p>For a 95% confidence interval of half-width <span class="math notranslate nohighlight">\(\epsilon\)</span>, we need:</p>
<div class="math notranslate nohighlight">
\[1.96 \cdot \frac{\sigma}{\sqrt{n}} \leq \epsilon \implies n \geq \left( \frac{1.96 \cdot \sigma}{\epsilon} \right)^2 \approx \frac{4\sigma^2}{\epsilon^2}\]</div>
<p>The factor of 4 (approximately <span class="math notranslate nohighlight">\(1.96^2\)</span>) accounts for the confidence level.</p>
</section>
<section id="practical-sample-size-determination">
<h3>Practical Sample Size Determination<a class="headerlink" href="#practical-sample-size-determination" title="Link to this heading">ÔÉÅ</a></h3>
<p>In practice, <span class="math notranslate nohighlight">\(\sigma\)</span> is unknown. A common approach is:</p>
<ol class="arabic simple">
<li><p><strong>Pilot study</strong>: Run a small simulation (e.g., 1,000 samples) to estimate <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span>.</p></li>
<li><p><strong>Compute required sample size</strong>: Use the formula <span class="math notranslate nohighlight">\(n = 4\hat{\sigma}^2 / \epsilon^2\)</span> for 95% confidence.</p></li>
<li><p><strong>Run full simulation</strong>: Generate <span class="math notranslate nohighlight">\(n\)</span> samples (possibly in addition to the pilot).</p></li>
<li><p><strong>Verify</strong>: Check that the final standard error meets requirements.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">determine_sample_size</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">sampler</span><span class="p">,</span> <span class="n">target_se</span><span class="p">,</span> <span class="n">pilot_n</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                           <span class="n">confidence</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determine required sample size for target standard error.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    h : callable</span>
<span class="sd">        Function to integrate.</span>
<span class="sd">    sampler : callable</span>
<span class="sd">        Function that takes (n, rng) and returns samples from target distribution.</span>
<span class="sd">    target_se : float</span>
<span class="sd">        Desired standard error.</span>
<span class="sd">    pilot_n : int</span>
<span class="sd">        Pilot sample size for variance estimation.</span>
<span class="sd">    confidence : float</span>
<span class="sd">        Confidence level for interval.</span>
<span class="sd">    seed : int</span>
<span class="sd">        Random seed.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        Contains required_n, estimated_variance, pilot results.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Pilot study</span>
    <span class="n">pilot_samples</span> <span class="o">=</span> <span class="n">sampler</span><span class="p">(</span><span class="n">pilot_n</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
    <span class="n">pilot_h</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">pilot_samples</span><span class="p">)</span>
    <span class="n">sigma_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">pilot_h</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Required sample size</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">confidence</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">required_n</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">z</span> <span class="o">*</span> <span class="n">sigma_hat</span> <span class="o">/</span> <span class="n">target_se</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;required_n&#39;</span><span class="p">:</span> <span class="n">required_n</span><span class="p">,</span>
        <span class="s1">&#39;estimated_sigma&#39;</span><span class="p">:</span> <span class="n">sigma_hat</span><span class="p">,</span>
        <span class="s1">&#39;pilot_n&#39;</span><span class="p">:</span> <span class="n">pilot_n</span><span class="p">,</span>
        <span class="s1">&#39;pilot_estimate&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">pilot_h</span><span class="p">),</span>
        <span class="s1">&#39;pilot_se&#39;</span><span class="p">:</span> <span class="n">sigma_hat</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">pilot_n</span><span class="p">)</span>
    <span class="p">}</span>

<span class="c1"># Example: Estimate E[X¬≤] for X ~ N(0,1) with SE ‚â§ 0.01</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">determine_sample_size</span><span class="p">(</span>
    <span class="n">h</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">sampler</span><span class="o">=</span><span class="k">lambda</span> <span class="n">n</span><span class="p">,</span> <span class="n">rng</span><span class="p">:</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n</span><span class="p">),</span>
    <span class="n">target_se</span><span class="o">=</span><span class="mf">0.01</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimated œÉ: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;estimated_sigma&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Required n for SE ‚â§ 0.01: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;required_n&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Estimated œÉ: 1.4153
Required n for SE ‚â§ 0.01: 76,948
</pre></div>
</div>
<p>For estimating <span class="math notranslate nohighlight">\(\mathbb{E}[X^2]\)</span> where <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(0,1)\)</span>, the true variance is <span class="math notranslate nohighlight">\(\text{Var}(X^2) = \mathbb{E}[X^4] - (\mathbb{E}[X^2])^2 = 3 - 1 = 2\)</span>, so <span class="math notranslate nohighlight">\(\sigma = \sqrt{2} \approx 1.414\)</span>. To achieve standard error 0.01, we need <span class="math notranslate nohighlight">\(n \geq (1.96)^2 \times 2 / (0.01)^2 \approx 77{,}000\)</span> samples‚Äîconsistent with our pilot estimate.</p>
</section>
</section>
<section id="convergence-diagnostics-and-monitoring">
<h2>Convergence Diagnostics and Monitoring<a class="headerlink" href="#convergence-diagnostics-and-monitoring" title="Link to this heading">ÔÉÅ</a></h2>
<p>Beyond computing point estimates and confidence intervals, it is important to monitor the convergence of Monte Carlo simulations. Visual diagnostics can reveal problems‚Äîheavy tails, multimodality, slow mixing‚Äîthat summary statistics might miss.</p>
<section id="running-mean-plots">
<h3>Running Mean Plots<a class="headerlink" href="#running-mean-plots" title="Link to this heading">ÔÉÅ</a></h3>
<p>The most basic diagnostic is a plot of the running mean:</p>
<div class="math notranslate nohighlight">
\[\hat{I}_k = \frac{1}{k} \sum_{i=1}^{k} h(X_i) \quad \text{for } k = 1, 2, \ldots, n\]</div>
<p>If the simulation is converging properly, this plot should:</p>
<ol class="arabic simple">
<li><p>Fluctuate widely for small <span class="math notranslate nohighlight">\(k\)</span></p></li>
<li><p>Stabilize and approach a horizontal asymptote as <span class="math notranslate nohighlight">\(k\)</span> grows</p></li>
<li><p>Have diminishing fluctuations proportional to <span class="math notranslate nohighlight">\(1/\sqrt{k}\)</span></p></li>
</ol>
</section>
<section id="standard-error-decay">
<h3>Standard Error Decay<a class="headerlink" href="#standard-error-decay" title="Link to this heading">ÔÉÅ</a></h3>
<p>The standard error should decrease as <span class="math notranslate nohighlight">\(1/\sqrt{n}\)</span>. A log-log plot of standard error versus sample size should have slope <span class="math notranslate nohighlight">\(-1/2\)</span>. Deviations suggest:</p>
<ul class="simple">
<li><p><strong>Steeper slope</strong>: Variance is decreasing (possibly a problem with the estimator)</p></li>
<li><p><strong>Shallower slope</strong>: Correlation in samples, infinite variance, or other issues</p></li>
</ul>
<p>The following figure demonstrates these diagnostics for a simple example: estimating <span class="math notranslate nohighlight">\(\mathbb{E}[X^2] = 1\)</span> where <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(0, 1)\)</span>.</p>
<figure class="align-center" id="id9">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_1_fig07_convergence_diagnostics.png"><img alt="Four-panel convergence diagnostics showing running mean, standard error decay, h-value distribution, and autocorrelation" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_1_fig07_convergence_diagnostics.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.8 </span><span class="caption-text"><strong>Convergence Diagnostics for Monte Carlo.</strong> Top-left: Running mean with 95% confidence band‚Äînote how the estimate stabilizes and the band shrinks as samples accumulate. Top-right: Standard error decay on log-log axes‚Äîthe observed SE (blue) closely tracks the theoretical <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> rate (red dashed). Bottom-left: Distribution of <span class="math notranslate nohighlight">\(h(X) = X^2\)</span> values, which follows a <span class="math notranslate nohighlight">\(\chi^2(1)\)</span> distribution (orange curve). Bottom-right: Autocorrelation at various lags‚Äîall values fall within the significance bounds (red dashed), confirming that samples are independent.</span><a class="headerlink" href="#id9" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<p><strong>What to look for:</strong></p>
<ul class="simple">
<li><p><strong>Running mean</strong>: Should stabilize (not drift or oscillate)</p></li>
<li><p><strong>SE decay</strong>: Should follow <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> line; shallower slope indicates problems</p></li>
<li><p><strong>Distribution</strong>: Check for heavy tails or multimodality that might inflate variance</p></li>
<li><p><strong>Autocorrelation</strong>: For iid samples, all bars should be near zero; significant autocorrelation indicates dependence (common in MCMC)</p></li>
</ul>
<p>The code below generates these diagnostic plots. Study it to understand how each panel is constructed:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">convergence_diagnostics</span><span class="p">(</span><span class="n">h_values</span><span class="p">,</span> <span class="n">true_value</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create comprehensive convergence diagnostics for Monte Carlo.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    h_values : array</span>
<span class="sd">        Sequence of h(X_i) values from Monte Carlo simulation.</span>
<span class="sd">    true_value : float, optional</span>
<span class="sd">        True integral value (if known) for reference line.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    fig : matplotlib Figure</span>
<span class="sd">        Four-panel diagnostic figure.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">h_values</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Warning: Too few samples for meaningful diagnostics&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Running mean: cumulative sum divided by count</span>
    <span class="n">cumsum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">h_values</span><span class="p">)</span>
    <span class="n">running_mean</span> <span class="o">=</span> <span class="n">cumsum</span> <span class="o">/</span> <span class="n">indices</span>

    <span class="c1"># Running variance using Welford&#39;s algorithm (numerically stable)</span>
    <span class="n">running_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">M2</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">h_values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">h_values</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-</span> <span class="n">mean</span>
        <span class="n">mean</span> <span class="o">+=</span> <span class="n">delta</span> <span class="o">/</span> <span class="p">(</span><span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">delta2</span> <span class="o">=</span> <span class="n">h_values</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-</span> <span class="n">mean</span>
        <span class="n">M2</span> <span class="o">+=</span> <span class="n">delta</span> <span class="o">*</span> <span class="n">delta2</span>
        <span class="n">running_var</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">M2</span> <span class="o">/</span> <span class="n">k</span> <span class="k">if</span> <span class="n">k</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="n">running_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">running_var</span> <span class="o">/</span> <span class="n">indices</span><span class="p">)</span>

    <span class="c1"># Create 2x2 figure</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

    <span class="c1"># Panel 1: Running mean with confidence band</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">running_mean</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
    <span class="n">start_idx</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">n</span> <span class="o">//</span> <span class="mi">10</span><span class="p">)</span>  <span class="c1"># Skip early noisy estimates</span>
    <span class="k">if</span> <span class="n">start_idx</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">indices</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:],</span>
                        <span class="n">running_mean</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:]</span> <span class="o">-</span> <span class="mf">1.96</span><span class="o">*</span><span class="n">running_se</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:],</span>
                        <span class="n">running_mean</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:]</span> <span class="o">+</span> <span class="mf">1.96</span><span class="o">*</span><span class="n">running_se</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:],</span>
                        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;95% CI&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">true_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">true_value</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span>
                   <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;True value = </span><span class="si">{</span><span class="n">true_value</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Sample size&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Running mean&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Running Mean with 95% Confidence Band&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>

    <span class="c1"># Panel 2: Standard error decay (log-log plot)</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">se_start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">99</span><span class="p">,</span> <span class="n">n</span> <span class="o">//</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">se_plot_idx</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">se_start</span><span class="p">:]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">se_plot_idx</span><span class="p">,</span> <span class="n">running_se</span><span class="p">[</span><span class="n">se_start</span><span class="p">:],</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
              <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Observed SE&#39;</span><span class="p">)</span>
    <span class="c1"># Reference line: SE should decay as 1/sqrt(n)</span>
    <span class="n">ref_se</span> <span class="o">=</span> <span class="n">running_se</span><span class="p">[</span><span class="n">se_start</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">se_start</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">se_plot_idx</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">se_plot_idx</span><span class="p">,</span> <span class="n">ref_se</span><span class="p">,</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
              <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$O(n^{-1/2})$ reference&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Sample size&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Standard Error&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Standard Error Decay&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="c1"># Panel 3: Distribution of h-values</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">h_values</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_values</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
               <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Mean = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_values</span><span class="p">)</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;h(X)&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Density&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Distribution of h(X) Values&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="c1"># Panel 4: Autocorrelation (detects dependence in samples)</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">max_lag</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">n</span> <span class="o">//</span> <span class="mi">20</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">max_lag</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">autocorr</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">h_values</span><span class="p">[:</span><span class="o">-</span><span class="n">k</span><span class="p">],</span> <span class="n">h_values</span><span class="p">[</span><span class="n">k</span><span class="p">:])[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
                    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_lag</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_lag</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">autocorr</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">)</span>
        <span class="c1"># 95% significance bounds for white noise</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mf">1.96</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span>
                   <span class="n">label</span><span class="o">=</span><span class="s1">&#39;95% bounds&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="o">-</span><span class="mf">1.96</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Lag&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Autocorrelation&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Autocorrelation (should be ~0 for iid samples)&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">fig</span>

<span class="c1"># Example: Estimate E[X¬≤] where X ~ N(0,1)</span>
<span class="c1"># True value is 1, variance of X¬≤ is 2</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="mi">50000</span><span class="p">)</span>
<span class="n">h_values</span> <span class="o">=</span> <span class="n">X</span><span class="o">**</span><span class="mi">2</span>  <span class="c1"># h(X) = X¬≤</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">convergence_diagnostics</span><span class="p">(</span><span class="n">h_values</span><span class="p">,</span> <span class="n">true_value</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;convergence_diagnostics.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="note admonition">
<p class="admonition-title">Key Implementation Details</p>
<ol class="arabic simple">
<li><p><strong>Running mean</strong>: Use <code class="docutils literal notranslate"><span class="pre">np.cumsum</span></code> for efficiency rather than recomputing the sum at each step.</p></li>
<li><p><strong>Running variance</strong>: Welford‚Äôs algorithm (lines 24-31) avoids numerical instability that plagues the naive one-pass formula.</p></li>
<li><p><strong>Log scale</strong>: The x-axis uses log scale so early fluctuations don‚Äôt dominate the plot.</p></li>
<li><p><strong>Autocorrelation bounds</strong>: For iid samples, autocorrelation at lag <span class="math notranslate nohighlight">\(k\)</span> is approximately <span class="math notranslate nohighlight">\(\mathcal{N}(0, 1/n)\)</span>, so <span class="math notranslate nohighlight">\(\pm 1.96/\sqrt{n}\)</span> gives 95% bounds.</p></li>
</ol>
</div>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ‚ö†Ô∏è Pathological Cases</p>
<p>Several situations can cause Monte Carlo to behave unexpectedly. Recognizing these pathologies‚Äîand knowing how to address them‚Äîis essential for reliable simulation.</p>
<p><strong>Heavy tails</strong> (infinite variance)</p>
<p><em>Problem</em>: If <span class="math notranslate nohighlight">\(\text{Var}[h(X)] = \infty\)</span>, the CLT does not apply. The estimator still converges by LLN (if <span class="math notranslate nohighlight">\(\mathbb{E}[|h(X)|] &lt; \infty\)</span>), but standard error estimates are meaningless and confidence intervals are invalid.</p>
<p><em>Example</em>: Estimating <span class="math notranslate nohighlight">\(\mathbb{E}[1/U]\)</span> where <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0,1)\)</span>. The expectation is infinite, but even for <span class="math notranslate nohighlight">\(\mathbb{E}[1/\sqrt{U}] = 2\)</span> (finite mean), the variance is infinite.</p>
<p><em>Remedies</em>:</p>
<ul class="simple">
<li><p><strong>Truncation</strong>: Replace <span class="math notranslate nohighlight">\(h(x)\)</span> with <span class="math notranslate nohighlight">\(\min(h(x), M)\)</span> for some threshold <span class="math notranslate nohighlight">\(M\)</span>; introduces bias but restores finite variance</p></li>
<li><p><strong>Transformation</strong>: If <span class="math notranslate nohighlight">\(h(x)\)</span> blows up near a boundary, change variables to smooth the integrand</p></li>
<li><p><strong>Importance sampling</strong>: Sample more heavily where <span class="math notranslate nohighlight">\(h(x)\)</span> is large, reducing variance</p></li>
<li><p><strong>Diagnostics</strong>: Plot the histogram of <span class="math notranslate nohighlight">\(h(X_i)\)</span> values; extreme outliers suggest heavy tails</p></li>
</ul>
<p><strong>Multimodality</strong> (isolated peaks)</p>
<p><em>Problem</em>: If the integrand has isolated peaks that the sampling distribution rarely visits, estimates may be severely biased. You might run millions of samples without ever hitting a significant mode.</p>
<p><em>Example</em>: A mixture of two narrow Gaussians separated by 100 standard deviations; uniform sampling over the domain almost never hits either peak.</p>
<p><em>Remedies</em>:</p>
<ul class="simple">
<li><p><strong>Importance sampling</strong>: Design a proposal distribution that covers all modes</p></li>
<li><p><strong>Stratified sampling</strong>: Divide the domain into regions and sample each region separately</p></li>
<li><p><strong>Adaptive methods</strong>: Use preliminary runs to identify modes, then concentrate sampling there</p></li>
<li><p><strong>MCMC with tempering</strong>: Parallel tempering or simulated tempering can help samplers jump between modes (covered in Part 3)</p></li>
</ul>
<p><strong>Rare events</strong> (small probabilities)</p>
<p><em>Problem</em>: Estimating <span class="math notranslate nohighlight">\(P(A)\)</span> for rare events requires approximately <span class="math notranslate nohighlight">\(100/P(A)\)</span> samples for 10% relative error. For <span class="math notranslate nohighlight">\(P(A) = 10^{-6}\)</span>, that‚Äôs <span class="math notranslate nohighlight">\(10^8\)</span> samples.</p>
<p><em>Example</em>: Probability that a complex system fails; probability of extreme portfolio losses.</p>
<p><em>Remedies</em>:</p>
<ul class="simple">
<li><p><strong>Importance sampling</strong>: Sample preferentially from the rare event region; this is the standard solution</p></li>
<li><p><strong>Splitting/cloning</strong>: In sequential simulations, replicate trajectories that approach the rare event</p></li>
<li><p><strong>Cross-entropy method</strong>: Iteratively adapt the sampling distribution to concentrate on rare events</p></li>
<li><p><strong>Large deviations theory</strong>: Use asymptotic approximations when sampling is infeasible</p></li>
</ul>
<p><strong>Dependent samples</strong> (autocorrelation)</p>
<p><em>Problem</em>: If samples are correlated (as in MCMC), the effective sample size (ESS) is smaller than the nominal sample size <span class="math notranslate nohighlight">\(n\)</span>. Standard error formulas that assume independence underestimate uncertainty.</p>
<p><em>Example</em>: Metropolis-Hastings chains with high autocorrelation; ESS might be <span class="math notranslate nohighlight">\(n/100\)</span> or worse.</p>
<p><em>Remedies</em>:</p>
<ul class="simple">
<li><p><strong>Thinning</strong>: Keep every <span class="math notranslate nohighlight">\(k\)</span>-th sample to reduce autocorrelation (simple but wasteful)</p></li>
<li><p><strong>Effective sample size</strong>: Compute ESS and report uncertainty based on ESS, not <span class="math notranslate nohighlight">\(n\)</span></p></li>
<li><p><strong>Batch means</strong>: Divide the chain into batches and estimate variance from batch means</p></li>
<li><p><strong>Better samplers</strong>: Hamiltonian Monte Carlo, NUTS, or other advanced MCMC methods reduce autocorrelation</p></li>
<li><p><strong>Diagnostics</strong>: Always check autocorrelation plots; values should decay quickly to zero</p></li>
</ul>
</div>
</section>
</section>
<section id="practical-considerations">
<h2>Practical Considerations<a class="headerlink" href="#practical-considerations" title="Link to this heading">ÔÉÅ</a></h2>
<p>Before concluding, we collect several practical points for implementing Monte Carlo methods effectively.</p>
<section id="when-to-use-monte-carlo">
<h3>When to Use Monte Carlo<a class="headerlink" href="#when-to-use-monte-carlo" title="Link to this heading">ÔÉÅ</a></h3>
<p>Monte Carlo integration is the method of choice when:</p>
<ol class="arabic simple">
<li><p><strong>Dimension is high</strong> (<span class="math notranslate nohighlight">\(d \gtrsim 5\)</span>): The curse of dimensionality kills deterministic methods.</p></li>
<li><p><strong>Domain is complex</strong>: Irregular regions, constraints, and complex boundaries are natural for Monte Carlo but problematic for quadrature.</p></li>
<li><p><strong>Integrand is non-smooth</strong>: Monte Carlo doesn‚Äôt require derivatives or smoothness.</p></li>
<li><p><strong>Sampling is easy</strong>: If we can easily generate samples from the target distribution, Monte Carlo is straightforward to implement.</p></li>
</ol>
<p>Monte Carlo is a poor choice when:</p>
<ol class="arabic simple">
<li><p><strong>Dimension is very low</strong> (<span class="math notranslate nohighlight">\(d \leq 3\)</span>) and the integrand is smooth: Use Gaussian quadrature.</p></li>
<li><p><strong>High precision is required</strong> with smooth integrands: Deterministic methods converge faster.</p></li>
<li><p><strong>Sampling is expensive</strong>: Each Monte Carlo sample requires a new function evaluation; quadrature methods can achieve more with fewer evaluations for smooth functions.</p></li>
</ol>
</section>
<section id="reproducibility">
<h3>Reproducibility<a class="headerlink" href="#reproducibility" title="Link to this heading">ÔÉÅ</a></h3>
<p>Always set random seeds and document them. Monte Carlo results should be reproducible.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">my_monte_carlo_analysis</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Document the seed in the function signature.&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="c1"># ... analysis using rng ...</span>
    <span class="k">return</span> <span class="n">results</span>

<span class="c1"># Call with explicit seed</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">my_monte_carlo_analysis</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="computational-efficiency">
<h3>Computational Efficiency<a class="headerlink" href="#computational-efficiency" title="Link to this heading">ÔÉÅ</a></h3>
<ul class="simple">
<li><p><strong>Vectorize</strong>: Use NumPy operations on arrays, not Python loops.</p></li>
<li><p><strong>Generate samples in batches</strong>: <code class="docutils literal notranslate"><span class="pre">rng.random(100_000)</span></code> is faster than 100,000 calls to <code class="docutils literal notranslate"><span class="pre">rng.random()</span></code>.</p></li>
<li><p><strong>Parallelize when possible</strong>: For embarrassingly parallel problems, distribute samples across cores.</p></li>
</ul>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ‚ö†Ô∏è</p>
<p><strong>Reporting estimates without uncertainty</strong>: A Monte Carlo estimate without a standard error or confidence interval is nearly meaningless. Always report <span class="math notranslate nohighlight">\(\hat{I}_n \pm z_{\alpha/2} \cdot \hat{\sigma}_n / \sqrt{n}\)</span>.</p>
<p><strong>Bad</strong>: ‚ÄúThe integral is 3.14159.‚Äù</p>
<p><strong>Good</strong>: ‚ÄúThe integral is 3.142 ¬± 0.005 (95% CI: [3.132, 3.152]) based on 100,000 samples.‚Äù</p>
</div>
</section>
</section>
<section id="bringing-it-all-together">
<h2>Bringing It All Together<a class="headerlink" href="#bringing-it-all-together" title="Link to this heading">ÔÉÅ</a></h2>
<p>Monte Carlo integration transforms the ancient problem of computing integrals into the modern practice of sampling and averaging. The method‚Äôs power derives from three pillars:</p>
<ol class="arabic simple">
<li><p><strong>Universality</strong>: Any integral can be written as an expected value, and expected values can be estimated by sample means.</p></li>
<li><p><strong>The Law of Large Numbers</strong>: Sample means converge to population means, guaranteeing that Monte Carlo estimators approach the true value.</p></li>
<li><p><strong>The Central Limit Theorem</strong>: The error decreases as <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span>, providing both a convergence rate and a framework for uncertainty quantification via confidence intervals.</p></li>
</ol>
<p>The <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> rate is simultaneously the method‚Äôs weakness and its strength. In low dimensions, deterministic quadrature methods converge faster. But in high dimensions, where deterministic methods suffer the curse of dimensionality, Monte Carlo‚Äôs dimension-independent rate makes it the only viable option.</p>
<p>The examples in this section‚Äîestimating <span class="math notranslate nohighlight">\(\pi\)</span>, computing Gaussian integrals, evaluating posterior means, tackling high-dimensional problems‚Äîillustrate the breadth of Monte Carlo applications. The diagnostic tools‚Äîrunning mean plots, standard error decay, autocorrelation checks‚Äîequip you to assess whether your simulations are converging properly.</p>
</section>
<section id="transition-to-what-follows">
<h2>Transition to What Follows<a class="headerlink" href="#transition-to-what-follows" title="Link to this heading">ÔÉÅ</a></h2>
<p>With the foundations of Monte Carlo integration in place, we face a critical question: <em>where do the random samples come from?</em></p>
<p>Throughout this section, we have assumed that generating samples from the target distribution‚Äîuniform on <span class="math notranslate nohighlight">\([0, 1]^d\)</span>, standard normal, a posterior distribution‚Äîis straightforward. But this assumption hides a mountain of computational machinery.</p>
<p>The <strong>next section</strong> (<a class="reference internal" href="ch2.2-uniform-random-variates.html#ch2-2-uniform-random-variates"><span class="std std-ref">Uniform Random Variates</span></a>) addresses the most fundamental case: generating uniform random numbers. We will see that computers, being deterministic machines, cannot produce ‚Äútrue‚Äù randomness‚Äîonly pseudo-random sequences that pass stringent statistical tests. Understanding how these sequences are generated, what can go wrong, and how to ensure reproducibility is essential for any serious practitioner.</p>
<p>Following that, the <strong>inverse CDF method</strong> (<a class="reference internal" href="ch2.3-inverse-cdf-method.html#ch2-3-inverse-cdf-method"><span class="std std-ref">Inverse CDF Method</span></a>) shows how to transform uniform random numbers into samples from other distributions. If we can compute the inverse cumulative distribution function <span class="math notranslate nohighlight">\(F^{-1}(u)\)</span>, we can generate samples from <span class="math notranslate nohighlight">\(F\)</span> by applying <span class="math notranslate nohighlight">\(F^{-1}\)</span> to uniform random numbers. This elegant technique works for many important distributions‚Äîexponential, Weibull, Cauchy‚Äîbut fails when <span class="math notranslate nohighlight">\(F^{-1}\)</span> has no closed form.</p>
<p>For distributions like the normal, where the inverse CDF is not available analytically, <strong>specialized transformations</strong> like Box-Muller offer efficient alternatives. And for truly complex distributions‚Äîposteriors in Bayesian inference, for instance‚Äî<strong>rejection sampling</strong> and eventually <strong>Markov chain Monte Carlo</strong> (Part 3) provide the tools to generate the samples that Monte Carlo integration requires.</p>
<p>The story of Monte Carlo methods is thus a story of two interlocking challenges: using random samples to estimate integrals (this section), and generating the random samples in the first place (the sections to come). Master both, and you hold a computational toolkit of extraordinary power.</p>
<div class="important admonition">
<p class="admonition-title">Key Takeaways üìù</p>
<ol class="arabic simple">
<li><p><strong>Core concept</strong>: Monte Carlo integration estimates integrals by rewriting them as expectations and averaging random samples. The Law of Large Numbers guarantees convergence; the Central Limit Theorem provides the <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> rate.</p></li>
<li><p><strong>Historical insight</strong>: Monte Carlo emerged from the Manhattan Project, where Ulam and von Neumann needed to compute neutron transport integrals that resisted analytical attack. The method turned randomness from a nuisance into a computational tool.</p></li>
<li><p><strong>Dimension independence</strong>: The <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> rate does not depend on the dimension of the integration domain. This is why Monte Carlo dominates in high-dimensional problems where deterministic methods fail.</p></li>
<li><p><strong>Practical application</strong>: Always report standard errors and confidence intervals. Use pilot studies to estimate variance for sample size planning. Monitor convergence visually with running mean plots and standard error decay.</p></li>
<li><p><strong>Method selection</strong>: In 1‚Äì3 dimensions with smooth integrands, use deterministic quadrature. In high dimensions or with complex domains, use Monte Carlo. Consider quasi-Monte Carlo for moderate dimensions with smooth functions.</p></li>
<li><p><strong>Outcome alignment</strong>: This section directly addresses Learning Outcome 1 (apply simulation techniques for Monte Carlo integration) and provides the conceptual foundation for all subsequent simulation methods, including variance reduction, importance sampling, and MCMC.</p></li>
</ol>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="2.1. Chapter 2: Monte Carlo Simulation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ch2.2-uniform-random-variates.html" class="btn btn-neutral float-right" title="2.1.2. Uniform Random Variates" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>