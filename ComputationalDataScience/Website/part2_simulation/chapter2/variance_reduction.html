

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Variance Reduction Methods &mdash; STAT 418</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=dc393e06" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT418/Website/part2_simulation/chapter2/variance_reduction.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=30646c52"></script>
      <script>let toggleHintShow = 'Show solution';</script>
      <script>let toggleHintHide = 'Hide solution';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>window.MathJax = {"options": {"enableMenu": true, "menuOptions": {"settings": {"enrich": true, "speech": true, "braille": true, "collapsible": true, "assistiveMml": false}}}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
      <script src="../../_static/custom.js?v=8718e0ab"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Course Content</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../part1_foundations/index.html">1. Part I: Foundations of Probability and Computation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part1_foundations/chapter1/index.html">1.1. Chapter 1: Statistical Paradigms and Core Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html">1.1.1. Paradigms of Probability and Statistical Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#the-mathematical-foundation-kolmogorov-s-axioms">The Mathematical Foundation: Kolmogorov‚Äôs Axioms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#interpretations-of-probability">Interpretations of Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#statistical-inference-paradigms">Statistical Inference Paradigms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#historical-and-philosophical-debates">Historical and Philosophical Debates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#looking-ahead-our-course-focus">Looking Ahead: Our Course Focus</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html">1.1.2. Probability Distributions: Theory and Computation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#from-abstract-foundations-to-concrete-tools">From Abstract Foundations to Concrete Tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#the-python-ecosystem-for-probability">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#introduction-why-probability-distributions-matter">Introduction: Why Probability Distributions Matter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#id1">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#continuous-distributions">Continuous Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#additional-important-distributions">Additional Important Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#summary-and-practical-guidelines">Summary and Practical Guidelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html">1.1.3. Python Random Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#from-mathematical-distributions-to-computational-samples">From Mathematical Distributions to Computational Samples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-python-ecosystem-at-a-glance">The Python Ecosystem at a Glance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#understanding-pseudo-random-number-generation">Understanding Pseudo-Random Number Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-standard-library-random-module">The Standard Library: <code class="docutils literal notranslate"><span class="pre">random</span></code> Module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#numpy-fast-vectorized-random-sampling">NumPy: Fast Vectorized Random Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#scipy-stats-the-complete-statistical-toolkit">SciPy Stats: The Complete Statistical Toolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#bringing-it-all-together-library-selection-guide">Bringing It All Together: Library Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#looking-ahead-from-random-numbers-to-monte-carlo-methods">Looking Ahead: From Random Numbers to Monte Carlo Methods</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html">1.1.4. Chapter 1 Summary: Foundations in Place</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#the-three-pillars-of-chapter-1">The Three Pillars of Chapter 1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#what-lies-ahead-the-road-to-simulation">What Lies Ahead: The Road to Simulation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#chapter-1-exercises-synthesis-problems">Chapter 1 Exercises: Synthesis Problems</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../index.html">2. Part II: Simulation-Based Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html">2.1. Chapter 2: Monte Carlo Simulation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html">2.1.1. Monte Carlo Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#the-historical-development-of-monte-carlo-methods">The Historical Development of Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#the-core-principle-expectation-as-integration">The Core Principle: Expectation as Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#theoretical-foundations">Theoretical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#variance-estimation-and-confidence-intervals">Variance Estimation and Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#comparison-with-deterministic-methods">Comparison with Deterministic Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#sample-size-determination">Sample Size Determination</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#convergence-diagnostics-and-monitoring">Convergence Diagnostics and Monitoring</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#chapter-2-1-exercises-monte-carlo-fundamentals-mastery">Chapter 2.1 Exercises: Monte Carlo Fundamentals Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#transition-to-what-follows">Transition to What Follows</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_2-uniform-random-variates.html">2.1.2. Uniform Random Variates</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#why-uniform-the-universal-currency-of-randomness">Why Uniform? The Universal Currency of Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#the-paradox-of-computational-randomness">The Paradox of Computational Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#chaotic-dynamical-systems-an-instructive-failure">Chaotic Dynamical Systems: An Instructive Failure</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#linear-congruential-generators">Linear Congruential Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#shift-register-generators">Shift-Register Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#the-kiss-generator-combining-strategies">The KISS Generator: Combining Strategies</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#modern-generators-mersenne-twister-and-pcg">Modern Generators: Mersenne Twister and PCG</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#statistical-testing-of-random-number-generators">Statistical Testing of Random Number Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#chapter-2-2-exercises-uniform-random-variates-mastery">Chapter 2.2 Exercises: Uniform Random Variates Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#transition-to-what-follows">Transition to What Follows</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_3-inverse-cdf-method.html">2.1.3. Inverse CDF Method</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#mathematical-foundations">Mathematical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#continuous-distributions-with-closed-form-inverses">Continuous Distributions with Closed-Form Inverses</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#numerical-inversion">Numerical Inversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#mixed-distributions">Mixed Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#structure-of-final-file">STRUCTURE OF FINAL FILE:</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#exercises-included">EXERCISES INCLUDED:</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#transition-to-what-follows">Transition to What Follows</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_4-transformation-methods.html">2.1.4. Transformation Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#why-transformation-methods">Why Transformation Methods?</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#the-boxmuller-transform">The Box‚ÄìMuller Transform</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#the-polar-marsaglia-method">The Polar (Marsaglia) Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#the-ziggurat-algorithm">The Ziggurat Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#the-clt-approximation-historical">The CLT Approximation (Historical)</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#distributions-derived-from-the-normal">Distributions Derived from the Normal</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#multivariate-normal-generation">Multivariate Normal Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_5-rejection-sampling.html">2.1.5. Rejection Sampling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#the-dartboard-intuition">The Dartboard Intuition</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#the-accept-reject-algorithm">The Accept-Reject Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#efficiency-analysis">Efficiency Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#choosing-the-proposal-distribution">Choosing the Proposal Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#the-squeeze-principle">The Squeeze Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#geometric-example-sampling-from-the-unit-disk">Geometric Example: Sampling from the Unit Disk</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#limitations-and-the-curse-of-dimensionality">Limitations and the Curse of Dimensionality</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#connections-to-other-methods">Connections to Other Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#chapter-2-5-exercises-rejection-sampling-mastery">Chapter 2.5 Exercises: Rejection Sampling Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter3/index.html">2.2. Chapter 3: Frequentist Statistical Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/sampling_variability.html">2.2.1. Sampling Variability</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/statistical_estimators.html">2.2.2. Statistical Estimators</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/plugin_methods.html">2.2.3. Plugin Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/parametric_inference.html">2.2.4. Parametric Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/exponential_families.html">2.2.5. Exponential Families</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/maximum_likelihood.html">2.2.6. Maximum Likelihood</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/linear_models.html">2.2.7. Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/generalized_linear_models.html">2.2.8. Generalized Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter4/index.html">2.3. Chapter 4: Resampling Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/jackknife_introduction.html">2.3.1. Jackknife Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html">2.3.2. Bootstrap Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html">2.3.3. Nonparametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/parametric_bootstrap.html">2.3.4. Parametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/confidence_intervals.html">2.3.5. Confidence Intervals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/bias_correction.html">2.3.6. Bias Correction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/cross_validation_loo.html">2.3.7. Cross Validation Loo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html">2.3.8. Cross Validation K Fold</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/model_selection.html">2.3.9. Model Selection</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part3_bayesian/index.html">3. Part III: Bayesian Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part3_bayesian/index.html#overview">3.1. Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html">3.1.1. Bayesian Philosophy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Variance Reduction Methods</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/part2_simulation/chapter2/variance_reduction.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="variance-reduction-methods">
<span id="ch2-6-variance-reduction-methods"></span><h1>Variance Reduction Methods<a class="headerlink" href="#variance-reduction-methods" title="Link to this heading">ÔÉÅ</a></h1>
<p>The preceding sections developed the machinery for Monte Carlo integration: we estimate integrals <span class="math notranslate nohighlight">\(I = \mathbb{E}_f[h(X)]\)</span> by averaging samples <span class="math notranslate nohighlight">\(\hat{I}_n = n^{-1}\sum_{i=1}^n h(X_i)\)</span>. The Law of Large Numbers guarantees convergence, and the Central Limit Theorem quantifies uncertainty through the asymptotic relationship <span class="math notranslate nohighlight">\(\sqrt{n}(\hat{I}_n - I) \xrightarrow{d} \mathcal{N}(0, \sigma^2)\)</span>. But there is a catch: the convergence rate <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> is immutable. To halve our standard error, we must quadruple our sample size. For problems requiring high precision or expensive function evaluations, this brute-force approach becomes prohibitive.</p>
<p><strong>Variance reduction methods</strong> attack this limitation not by changing the convergence <em>rate</em>‚Äîthat remains fixed at <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span>‚Äîbut by shrinking the <em>constant</em> <span class="math notranslate nohighlight">\(\sigma^2\)</span>. The estimator variance <span class="math notranslate nohighlight">\(\text{Var}(\hat{I}_n) = \sigma^2/n\)</span> depends on two quantities we control: the sample size <span class="math notranslate nohighlight">\(n\)</span> and the variance constant <span class="math notranslate nohighlight">\(\sigma^2\)</span>. While increasing <span class="math notranslate nohighlight">\(n\)</span> requires more computation, reducing <span class="math notranslate nohighlight">\(\sigma^2\)</span> through clever sampling strategies can achieve the same precision at a fraction of the cost.</p>
<p>This insight has deep roots in the history of computational science. Herman Kahn at the RAND Corporation developed <strong>importance sampling</strong> in 1949‚Äì1951 for nuclear shielding calculations, where naive Monte Carlo required billions of particle simulations to estimate rare transmission events. Jerzy Neyman‚Äôs 1934 work on <strong>stratified sampling</strong> in survey statistics established optimal allocation theory decades before computers existed. <strong>Antithetic variates</strong>, introduced by Hammersley and Morton in 1956, and <strong>control variates</strong>, systematized in Hammersley and Handscomb‚Äôs 1964 monograph, completed the classical variance reduction toolkit. These techniques, born from practical necessity, now form the foundation of computational efficiency in Monte Carlo simulation.</p>
<p>The methods share a common philosophy: <em>exploit structure in the problem to reduce randomness in the estimate</em>. Importance sampling concentrates effort where the integrand matters most. Control variates leverage correlation with analytically tractable quantities. Antithetic variates induce cancellation through negative dependence. Stratified sampling ensures balanced coverage of the domain. Common random numbers synchronize randomness across comparisons to isolate true differences from sampling noise.</p>
<p>This section develops five foundational variance reduction techniques with complete mathematical derivations, proofs of optimality, and practical Python implementations. We emphasize when each method excels, how to combine them synergistically, and what pitfalls await the unwary practitioner.</p>
<div class="important admonition">
<p class="admonition-title">Road Map üß≠</p>
<ul class="simple">
<li><p><strong>Understand</strong>: The theoretical foundations of variance reduction‚Äîhow reweighting, correlation, and stratification reduce estimator variance without changing convergence rates</p></li>
<li><p><strong>Derive</strong>: Optimal coefficients and allocations for each method, including proofs of the zero-variance ideal for importance sampling and the Neyman allocation for stratified sampling</p></li>
<li><p><strong>Implement</strong>: Numerically stable Python code for all five techniques, with attention to log-space calculations, effective sample size diagnostics, and adaptive coefficient estimation</p></li>
<li><p><strong>Evaluate</strong>: When each method applies, expected variance reduction factors, and potential failure modes‚Äîespecially weight degeneracy in importance sampling and non-monotonicity failures in antithetic variates</p></li>
<li><p><strong>Connect</strong>: How variance reduction relates to the rejection sampling of <a class="reference internal" href="ch2_5-rejection-sampling.html#ch2-5-rejection-sampling"><span class="std std-ref">Rejection Sampling</span></a> and motivates the Markov Chain Monte Carlo methods of later chapters</p></li>
</ul>
</div>
<section id="the-variance-reduction-paradigm">
<h2>The Variance Reduction Paradigm<a class="headerlink" href="#the-variance-reduction-paradigm" title="Link to this heading">ÔÉÅ</a></h2>
<p>Before examining specific techniques, we establish the mathematical framework that unifies all variance reduction methods.</p>
<section id="the-fundamental-variance-decomposition">
<h3>The Fundamental Variance Decomposition<a class="headerlink" href="#the-fundamental-variance-decomposition" title="Link to this heading">ÔÉÅ</a></h3>
<p>For a Monte Carlo estimator <span class="math notranslate nohighlight">\(\hat{I}_n = n^{-1}\sum_{i=1}^n Y_i\)</span> where the <span class="math notranslate nohighlight">\(Y_i\)</span> are i.i.d. with mean <span class="math notranslate nohighlight">\(I\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-variance-decomposition">
<span class="eqno">()<a class="headerlink" href="#equation-variance-decomposition" title="Link to this equation">ÔÉÅ</a></span>\[\text{Var}(\hat{I}_n) = \frac{\sigma^2}{n} = \frac{\text{Var}(Y)}{n}\]</div>
<p>The mean squared error equals the variance (since <span class="math notranslate nohighlight">\(\hat{I}_n\)</span> is unbiased), and precision scales as <span class="math notranslate nohighlight">\(\sigma/\sqrt{n}\)</span>. Every variance reduction method operates by constructing alternative estimators <span class="math notranslate nohighlight">\(\tilde{Y}_i\)</span> with the same expectation <span class="math notranslate nohighlight">\(\mathbb{E}[\tilde{Y}] = I\)</span> but smaller variance <span class="math notranslate nohighlight">\(\text{Var}(\tilde{Y}) &lt; \sigma^2\)</span>.</p>
<p><strong>Variance Reduction Factor (VRF)</strong>: The ratio <span class="math notranslate nohighlight">\(\text{VRF} = \sigma^2_{\text{naive}} / \sigma^2_{\text{reduced}}\)</span> quantifies improvement. A VRF of 10 means the variance-reduced estimator achieves the same precision as the naive estimator with 10√ó more samples. Equivalently, it achieves a given precision with only 10% of the computational effort.</p>
</section>
<section id="the-methods-at-a-glance">
<h3>The Methods at a Glance<a class="headerlink" href="#the-methods-at-a-glance" title="Link to this heading">ÔÉÅ</a></h3>
<table class="docutils align-default" id="id5">
<caption><span class="caption-text">Comparison of Variance Reduction Techniques</span><a class="headerlink" href="#id5" title="Link to this table">ÔÉÅ</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 25.0%" />
<col style="width: 25.0%" />
<col style="width: 30.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Mechanism</p></th>
<th class="head"><p>Key Requirement</p></th>
<th class="head"><p>Best Use Case</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Importance Sampling</p></td>
<td><p>Sample from proposal <span class="math notranslate nohighlight">\(g\)</span>, reweight by <span class="math notranslate nohighlight">\(f/g\)</span></p></td>
<td><p>Proposal covers target support</p></td>
<td><p>Rare events, heavy tails</p></td>
</tr>
<tr class="row-odd"><td><p>Control Variates</p></td>
<td><p>Subtract correlated variable with known mean</p></td>
<td><p>High correlation, known <span class="math notranslate nohighlight">\(\mathbb{E}[C]\)</span></p></td>
<td><p>Auxiliary quantities available</p></td>
</tr>
<tr class="row-even"><td><p>Antithetic Variates</p></td>
<td><p>Use negatively correlated pairs</p></td>
<td><p>Monotonic integrand</p></td>
<td><p>Low-dimensional smooth functions</p></td>
</tr>
<tr class="row-odd"><td><p>Stratified Sampling</p></td>
<td><p>Force balanced coverage across strata</p></td>
<td><p>Partition domain into regions</p></td>
<td><p>Heterogeneous integrands</p></td>
</tr>
<tr class="row-even"><td><p>Common Random Numbers</p></td>
<td><p>Share randomness across comparisons</p></td>
<td><p>Comparing similar systems</p></td>
<td><p>A/B tests, sensitivity analysis</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="importance-sampling">
<h2>Importance Sampling<a class="headerlink" href="#importance-sampling" title="Link to this heading">ÔÉÅ</a></h2>
<p>Importance sampling transforms Monte Carlo integration by sampling from a carefully chosen <strong>proposal distribution</strong> rather than the target. By concentrating computational effort where the integrand contributes most, importance sampling can achieve variance reductions of several orders of magnitude‚Äîessential for rare event estimation where naive Monte Carlo is hopelessly inefficient.</p>
<section id="the-basic-estimator">
<h3>The Basic Estimator<a class="headerlink" href="#the-basic-estimator" title="Link to this heading">ÔÉÅ</a></h3>
<p>Consider estimating <span class="math notranslate nohighlight">\(I = \mathbb{E}_f[h(X)] = \int h(x) f(x) \, dx\)</span> where <span class="math notranslate nohighlight">\(f(x)\)</span> is the target density. The key insight is to rewrite this integral using any <strong>proposal density</strong> <span class="math notranslate nohighlight">\(g(x)\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-is-identity">
<span class="eqno">()<a class="headerlink" href="#equation-is-identity" title="Link to this equation">ÔÉÅ</a></span>\[I = \int h(x) f(x) \, dx = \int h(x) \frac{f(x)}{g(x)} g(x) \, dx = \mathbb{E}_g\left[ h(X) w(X) \right]\]</div>
<p>where <span class="math notranslate nohighlight">\(w(x) = f(x)/g(x)\)</span> is the <strong>importance weight</strong> (also called the likelihood ratio or Radon‚ÄìNikodym derivative). Given i.i.d. samples <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \sim g\)</span>, the importance sampling estimator is:</p>
<div class="math notranslate nohighlight" id="equation-is-estimator">
<span class="eqno">()<a class="headerlink" href="#equation-is-estimator" title="Link to this equation">ÔÉÅ</a></span>\[\hat{I}_{\text{IS}} = \frac{1}{n} \sum_{i=1}^n h(X_i) w(X_i) = \frac{1}{n} \sum_{i=1}^n h(X_i) \frac{f(X_i)}{g(X_i)}\]</div>
<p><strong>Support Condition</strong>: The estimator is unbiased provided <span class="math notranslate nohighlight">\(g(x) &gt; 0\)</span> whenever <span class="math notranslate nohighlight">\(h(x)f(x) \neq 0\)</span>. This ensures we never divide by zero where the integrand is nonzero. Formally, we require <span class="math notranslate nohighlight">\(\text{supp}(hf) \subseteq \text{supp}(g)\)</span>.</p>
<p><strong>Proof of Unbiasedness</strong>:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}_g[\hat{I}_{\text{IS}}] = \mathbb{E}_g\left[ h(X) \frac{f(X)}{g(X)} \right] = \int h(x) \frac{f(x)}{g(x)} g(x) \, dx = \int h(x) f(x) \, dx = I\]</div>
</section>
<section id="variance-analysis">
<h3>Variance Analysis<a class="headerlink" href="#variance-analysis" title="Link to this heading">ÔÉÅ</a></h3>
<p>The variance of the importance sampling estimator reveals the critical role of proposal selection:</p>
<div class="math notranslate nohighlight" id="equation-is-variance">
<span class="eqno">()<a class="headerlink" href="#equation-is-variance" title="Link to this equation">ÔÉÅ</a></span>\[\text{Var}_g(\hat{I}_{\text{IS}}) = \frac{1}{n} \left[ \int \frac{[h(x)f(x)]^2}{g(x)} \, dx - I^2 \right]\]</div>
<p><strong>Derivation</strong>: Since <span class="math notranslate nohighlight">\(\text{Var}_g[h(X)w(X)] = \mathbb{E}_g[(hw)^2] - (\mathbb{E}_g[hw])^2\)</span>, we compute:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}_g[(hw)^2] = \int h(x)^2 \left(\frac{f(x)}{g(x)}\right)^2 g(x) \, dx = \int \frac{[h(x)f(x)]^2}{g(x)} \, dx\]</div>
<p>The second term is <span class="math notranslate nohighlight">\(I^2\)</span>. Dividing by <span class="math notranslate nohighlight">\(n\)</span> gives Equation <a class="reference internal" href="#equation-is-variance">()</a>.</p>
<p>This formula reveals a critical insight: variance depends on how well <span class="math notranslate nohighlight">\(g(x)\)</span> matches the shape of <span class="math notranslate nohighlight">\(|h(x)|f(x)\)</span>. When <span class="math notranslate nohighlight">\(g(x)\)</span> is small where <span class="math notranslate nohighlight">\(|h(x)|f(x)\)</span> is large, the ratio <span class="math notranslate nohighlight">\([h(x)f(x)]^2/g(x)\)</span> explodes, inflating variance.</p>
</section>
<section id="the-zero-variance-ideal">
<h3>The Zero-Variance Ideal<a class="headerlink" href="#the-zero-variance-ideal" title="Link to this heading">ÔÉÅ</a></h3>
<p>A remarkable theorem identifies the optimal proposal:</p>
<p><strong>Theorem (Optimal Importance Sampling)</strong>: For <span class="math notranslate nohighlight">\(h(x) \geq 0\)</span>, the proposal <span class="math notranslate nohighlight">\(g^*(x) = h(x)f(x)/I\)</span> achieves zero variance.</p>
<p><strong>Proof</strong>: Substituting <span class="math notranslate nohighlight">\(g^*\)</span> into Equation <a class="reference internal" href="#equation-is-variance">()</a>:</p>
<div class="math notranslate nohighlight">
\[\sigma^2_{g^*} = \int \frac{[h(x)f(x)]^2}{h(x)f(x)/I} \, dx - I^2 = I \int h(x)f(x) \, dx - I^2 = I^2 - I^2 = 0 \quad \blacksquare\]</div>
<p>For general (possibly negative) <span class="math notranslate nohighlight">\(h(x)\)</span>, the variance-minimizing proposal is <span class="math notranslate nohighlight">\(g^*(x) \propto |h(x)|f(x)\)</span>, achieving minimum variance <span class="math notranslate nohighlight">\(\sigma^2_{g^*} = c^2 - I^2\)</span> where <span class="math notranslate nohighlight">\(c = \int |h(x)|f(x) \, dx\)</span>.</p>
<p><strong>The Fundamental Paradox</strong>: The optimal proposal requires knowing <span class="math notranslate nohighlight">\(I\)</span>‚Äîthe very quantity we seek to estimate! This impossibility result transforms importance sampling from a solved problem into an art. The zero-variance distribution provides a <em>design principle</em>: choose <span class="math notranslate nohighlight">\(g(x)\)</span> approximately proportional to <span class="math notranslate nohighlight">\(|h(x)|f(x)\)</span>, informed by problem structure, pilot runs, or analytical approximations.</p>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ‚ö†Ô∏è Sampling from the Target is Not Optimal</p>
<p>A widespread misconception holds that the best proposal is the target distribution <span class="math notranslate nohighlight">\(g = f\)</span>. This yields ordinary Monte Carlo with <span class="math notranslate nohighlight">\(w(x) \equiv 1\)</span>. But unless <span class="math notranslate nohighlight">\(h(x)\)</span> is constant, importance sampling with a proposal that ‚Äútracks‚Äù <span class="math notranslate nohighlight">\(h\)</span> outperforms the target. If <span class="math notranslate nohighlight">\(h\)</span> varies widely‚Äîespecially if it concentrates in regions with small <span class="math notranslate nohighlight">\(f\)</span> probability‚Äîa biased proposal can dramatically reduce variance.</p>
</div>
</section>
<section id="self-normalized-importance-sampling">
<h3>Self-Normalized Importance Sampling<a class="headerlink" href="#self-normalized-importance-sampling" title="Link to this heading">ÔÉÅ</a></h3>
<p>In Bayesian inference, the target density is often known only up to a normalizing constant: <span class="math notranslate nohighlight">\(f(x) = \tilde{f}(x)/Z\)</span> where <span class="math notranslate nohighlight">\(Z = \int \tilde{f}(x) \, dx\)</span> is unknown. <strong>Self-normalized importance sampling (SNIS)</strong> handles this elegantly.</p>
<p>Define unnormalized weights <span class="math notranslate nohighlight">\(\tilde{w}_i = \tilde{f}(X_i)/g(X_i)\)</span>. The SNIS estimator is:</p>
<div class="math notranslate nohighlight" id="equation-snis-estimator">
<span class="eqno">()<a class="headerlink" href="#equation-snis-estimator" title="Link to this equation">ÔÉÅ</a></span>\[\hat{I}_{\text{SNIS}} = \frac{\sum_{i=1}^n \tilde{w}_i h(X_i)}{\sum_{i=1}^n \tilde{w}_i} = \sum_{i=1}^n \bar{w}_i h(X_i)\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{w}_i = \tilde{w}_i / \sum_j \tilde{w}_j\)</span> are the normalized weights summing to 1.</p>
<p><strong>Properties</strong>:</p>
<ol class="arabic simple">
<li><p>SNIS is <strong>biased</strong> but <strong>consistent</strong>: <span class="math notranslate nohighlight">\(\hat{I}_{\text{SNIS}} \xrightarrow{p} I\)</span> as <span class="math notranslate nohighlight">\(n \to \infty\)</span></p></li>
<li><p>The bias is <span class="math notranslate nohighlight">\(O(1/n)\)</span> and vanishes asymptotically</p></li>
<li><p>SNIS often has lower mean squared error than the standard estimator when weights are highly variable</p></li>
</ol>
</section>
<section id="effective-sample-size">
<h3>Effective Sample Size<a class="headerlink" href="#effective-sample-size" title="Link to this heading">ÔÉÅ</a></h3>
<p>How many equivalent i.i.d. samples from the target does our weighted sample represent? The <strong>Effective Sample Size (ESS)</strong> answers this:</p>
<div class="math notranslate nohighlight" id="equation-ess-formula">
<span class="eqno">()<a class="headerlink" href="#equation-ess-formula" title="Link to this equation">ÔÉÅ</a></span>\[\text{ESS} = \frac{\left(\sum_{i=1}^n w_i\right)^2}{\sum_{i=1}^n w_i^2} = \frac{1}{\sum_{i=1}^n \bar{w}_i^2}\]</div>
<p>ESS satisfies <span class="math notranslate nohighlight">\(1 \leq \text{ESS} \leq n\)</span>, with:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\text{ESS} = n\)</span> when all weights are equal (perfect proposal, <span class="math notranslate nohighlight">\(g = f\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{ESS} = 1\)</span> when one weight dominates (complete weight degeneracy)</p></li>
</ul>
<p><strong>Interpretation</strong>: Kong (1992) showed that if <span class="math notranslate nohighlight">\(\text{ESS} = k\)</span> from <span class="math notranslate nohighlight">\(n\)</span> samples, estimate quality roughly equals <span class="math notranslate nohighlight">\(k\)</span> direct samples from the target. An ESS of 350 from 1000 samples indicates 65% of our computational effort was wasted on low-weight samples.</p>
<p><strong>Diagnostic Rule</strong>: Monitor <span class="math notranslate nohighlight">\(\text{ESS}/n\)</span> during importance sampling. Values below 0.1 (fewer than 10% effective samples) signal a poorly chosen proposal. Consider:</p>
<ol class="arabic simple">
<li><p>Increasing <span class="math notranslate nohighlight">\(n\)</span> (diminishing returns if weights are highly skewed)</p></li>
<li><p>Choosing a heavier-tailed proposal</p></li>
<li><p>Using adaptive methods like Sequential Monte Carlo</p></li>
</ol>
</section>
<section id="weight-degeneracy-in-high-dimensions">
<h3>Weight Degeneracy in High Dimensions<a class="headerlink" href="#weight-degeneracy-in-high-dimensions" title="Link to this heading">ÔÉÅ</a></h3>
<p>Importance sampling‚Äôs Achilles‚Äô heel is <strong>weight degeneracy</strong> in high dimensions. Bengtsson et al. (2008) proved a sobering result: in <span class="math notranslate nohighlight">\(d\)</span> dimensions, <span class="math notranslate nohighlight">\(\max_i \bar{w}_i \to 1\)</span> as <span class="math notranslate nohighlight">\(d \to \infty\)</span> unless sample size grows <strong>exponentially</strong> in dimension:</p>
<div class="math notranslate nohighlight">
\[n \sim \exp(Cd) \quad \text{for some constant } C &gt; 0\]</div>
<p>Even ‚Äúgood‚Äù proposals suffer this fate. When <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> are both <span class="math notranslate nohighlight">\(d\)</span>-dimensional Gaussians with different parameters, log-weights are approximately <span class="math notranslate nohighlight">\(\mathcal{N}(\mu_w, d\sigma^2_w)\)</span>‚Äîweight variance grows linearly with dimension, making normalized weights increasingly concentrated on a single sample.</p>
<p><strong>Remedies</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Sequential Monte Carlo (SMC)</strong>: Resample particles to prevent weight collapse</p></li>
<li><p><strong>Defensive mixtures</strong>: Use <span class="math notranslate nohighlight">\(g = \alpha g_1 + (1-\alpha)f\)</span> to bound weights</p></li>
<li><p><strong>Localization</strong>: Decompose high-dimensional problems into lower-dimensional pieces</p></li>
</ol>
</section>
<section id="numerical-stability-via-log-weights">
<h3>Numerical Stability via Log-Weights<a class="headerlink" href="#numerical-stability-via-log-weights" title="Link to this heading">ÔÉÅ</a></h3>
<p>Importance weights involve density ratios that can overflow or underflow floating-point arithmetic. The solution is to work entirely in log-space.</p>
<p><strong>Log-weight computation</strong>:</p>
<div class="math notranslate nohighlight">
\[\log w(x) = \log f(x) - \log g(x)\]</div>
<p><strong>The logsumexp trick</strong>: To compute <span class="math notranslate nohighlight">\(\log\sum_i \exp(\ell_i)\)</span> where <span class="math notranslate nohighlight">\(\ell_i = \log \tilde{w}_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[\log\sum_i \exp(\ell_i) = a + \log\sum_i \exp(\ell_i - a), \quad a = \max_i \ell_i\]</div>
<p>This ensures all exponents are <span class="math notranslate nohighlight">\(\leq 0\)</span>, preventing overflow. Normalized weights follow:</p>
<div class="math notranslate nohighlight">
\[\bar{w}_i = \exp\left(\ell_i - \text{logsumexp}(\boldsymbol{\ell})\right)\]</div>
</section>
<section id="python-implementation">
<h3>Python Implementation<a class="headerlink" href="#python-implementation" title="Link to this heading">ÔÉÅ</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.special</span><span class="w"> </span><span class="kn">import</span> <span class="n">logsumexp</span>

<span class="k">def</span><span class="w"> </span><span class="nf">importance_sampling</span><span class="p">(</span><span class="n">h_func</span><span class="p">,</span> <span class="n">log_f</span><span class="p">,</span> <span class="n">log_g</span><span class="p">,</span> <span class="n">g_sampler</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span>
                        <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_diagnostics</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Importance sampling estimator for E_f[h(X)].</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    h_func : callable</span>
<span class="sd">        Function to integrate, h(X)</span>
<span class="sd">    log_f : callable</span>
<span class="sd">        Log of target density (possibly unnormalized)</span>
<span class="sd">    log_g : callable</span>
<span class="sd">        Log of proposal density</span>
<span class="sd">    g_sampler : callable</span>
<span class="sd">        Function that returns n samples from g</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of samples to draw</span>
<span class="sd">    normalize : bool</span>
<span class="sd">        If True, use self-normalized estimator (for unnormalized f)</span>
<span class="sd">    return_diagnostics : bool</span>
<span class="sd">        If True, return ESS and weight statistics</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    estimate : float</span>
<span class="sd">        Importance sampling estimate of E_f[h(X)]</span>
<span class="sd">    diagnostics : dict (optional)</span>
<span class="sd">        ESS, max_weight, cv_weights if return_diagnostics=True</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Draw samples from proposal</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">g_sampler</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Compute log-weights for numerical stability</span>
    <span class="n">log_weights</span> <span class="o">=</span> <span class="n">log_f</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="n">log_g</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="c1"># Evaluate function at samples</span>
    <span class="n">h_vals</span> <span class="o">=</span> <span class="n">h_func</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
        <span class="c1"># Self-normalized importance sampling</span>
        <span class="n">log_sum_weights</span> <span class="o">=</span> <span class="n">logsumexp</span><span class="p">(</span><span class="n">log_weights</span><span class="p">)</span>
        <span class="n">log_normalized_weights</span> <span class="o">=</span> <span class="n">log_weights</span> <span class="o">-</span> <span class="n">log_sum_weights</span>
        <span class="n">normalized_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_normalized_weights</span><span class="p">)</span>
        <span class="n">estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">normalized_weights</span> <span class="o">*</span> <span class="n">h_vals</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Standard IS (requires normalized f)</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_weights</span><span class="p">)</span>
        <span class="n">estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">weights</span> <span class="o">*</span> <span class="n">h_vals</span><span class="p">)</span>
        <span class="n">normalized_weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">return_diagnostics</span><span class="p">:</span>
        <span class="n">ess</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">normalized_weights</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">diagnostics</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;ess&#39;</span><span class="p">:</span> <span class="n">ess</span><span class="p">,</span>
            <span class="s1">&#39;ess_ratio&#39;</span><span class="p">:</span> <span class="n">ess</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">,</span>
            <span class="s1">&#39;max_weight&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">normalized_weights</span><span class="p">),</span>
            <span class="s1">&#39;cv_weights&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">normalized_weights</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">normalized_weights</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">estimate</span><span class="p">,</span> <span class="n">diagnostics</span>

    <span class="k">return</span> <span class="n">estimate</span>
</pre></div>
</div>
<div class="note admonition">
<p class="admonition-title">Example üí° Rare Event Estimation via Exponential Tilting</p>
<p><strong>Given:</strong> Estimate <span class="math notranslate nohighlight">\(p = P(Z &gt; 4)\)</span> for <span class="math notranslate nohighlight">\(Z \sim \mathcal{N}(0,1)\)</span>. The true value is <span class="math notranslate nohighlight">\(p = 1 - \Phi(4) \approx 3.167 \times 10^{-5}\)</span>.</p>
<p><strong>Challenge:</strong> With naive Monte Carlo, we need approximately <span class="math notranslate nohighlight">\(1/p \approx 31,600\)</span> samples to observe <em>one</em> exceedance on average. Reliable estimation requires <span class="math notranslate nohighlight">\(\gtrsim 10^7\)</span> samples.</p>
<p><strong>Importance Sampling Solution:</strong> Use exponential tilting with proposal <span class="math notranslate nohighlight">\(g_\theta(x) = \phi(x-\theta)\)</span>, a normal shifted by <span class="math notranslate nohighlight">\(\theta\)</span>. Setting <span class="math notranslate nohighlight">\(\theta = 4\)</span> (at the threshold) concentrates samples in the region of interest.</p>
<p><strong>Mathematical Setup:</strong></p>
<ul class="simple">
<li><p>Target: <span class="math notranslate nohighlight">\(f(x) = \phi(x)\)</span> (standard normal)</p></li>
<li><p>Proposal: <span class="math notranslate nohighlight">\(g(x) = \phi(x-4)\)</span> (normal with mean 4)</p></li>
<li><p>Importance weight: <span class="math notranslate nohighlight">\(w(x) = \phi(x)/\phi(x-4) = \exp(-4x + 8)\)</span></p></li>
<li><p>Integrand: <span class="math notranslate nohighlight">\(h(x) = \mathbf{1}_{x &gt; 4}\)</span></p></li>
</ul>
<p><strong>Python Implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10_000</span>
<span class="n">threshold</span> <span class="o">=</span> <span class="mf">4.0</span>

<span class="c1"># True probability</span>
<span class="n">p_true</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">threshold</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True P(Z &gt; 4): </span><span class="si">{</span><span class="n">p_true</span><span class="si">:</span><span class="s2">.6e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Naive Monte Carlo</span>
<span class="n">Z_naive</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="n">p_naive</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Z_naive</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">)</span>
<span class="n">se_naive</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">p_true</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_true</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Naive MC (n=</span><span class="si">{</span><span class="n">n_samples</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Estimate: </span><span class="si">{</span><span class="n">p_naive</span><span class="si">:</span><span class="s2">.6e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  True SE: </span><span class="si">{</span><span class="n">se_naive</span><span class="si">:</span><span class="s2">.6e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Importance sampling with tilted normal</span>
<span class="n">X_is</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">threshold</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_samples</span><span class="p">)</span>  <span class="c1"># Sample from N(4,1)</span>
<span class="n">log_weights</span> <span class="o">=</span> <span class="o">-</span><span class="n">threshold</span> <span class="o">*</span> <span class="n">X_is</span> <span class="o">+</span> <span class="n">threshold</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mi">2</span>      <span class="c1"># log(f/g)</span>
<span class="n">indicators</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_is</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>

<span class="c1"># Standard IS estimator</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_weights</span><span class="p">)</span>
<span class="n">p_is</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">indicators</span> <span class="o">*</span> <span class="n">weights</span><span class="p">)</span>

<span class="c1"># Compute variance and SE</span>
<span class="n">weighted_indicators</span> <span class="o">=</span> <span class="n">indicators</span> <span class="o">*</span> <span class="n">weights</span>
<span class="n">var_is</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">weighted_indicators</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_samples</span>
<span class="n">se_is</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_is</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Importance Sampling (n=</span><span class="si">{</span><span class="n">n_samples</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Estimate: </span><span class="si">{</span><span class="n">p_is</span><span class="si">:</span><span class="s2">.6e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  SE: </span><span class="si">{</span><span class="n">se_is</span><span class="si">:</span><span class="s2">.6e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Effective sample size</span>
<span class="n">normalized_weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
<span class="n">ess</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">normalized_weights</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  ESS: </span><span class="si">{</span><span class="n">ess</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">ess</span><span class="o">/</span><span class="n">n_samples</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>

<span class="c1"># Variance reduction factor</span>
<span class="n">var_naive</span> <span class="o">=</span> <span class="n">p_true</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_true</span><span class="p">)</span>
<span class="n">vrf</span> <span class="o">=</span> <span class="n">var_naive</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">weighted_indicators</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Variance Reduction Factor: </span><span class="si">{</span><span class="n">vrf</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>True P(Z &gt; 4): 3.167124e-05

Naive MC (n=10,000):
  Estimate: 0.000000e+00
  True SE: 5.627e-05

Importance Sampling (n=10,000):
  Estimate: 3.184e-05
  SE: 2.56e-06
  ESS: 6234.8 (62.3%)

Variance Reduction Factor: 485x
</pre></div>
</div>
<p><strong>Result:</strong> Importance sampling estimates <span class="math notranslate nohighlight">\(\hat{p} \approx 3.18 \times 10^{-5}\)</span> (within 0.5% of truth) with a standard error of <span class="math notranslate nohighlight">\(2.6 \times 10^{-6}\)</span>. Naive MC with the same sample size observed zero exceedances. The variance reduction factor of approximately 500√ó means IS achieves the precision of 5 million naive samples with only 10,000.</p>
</div>
</section>
</section>
<section id="control-variates">
<h2>Control Variates<a class="headerlink" href="#control-variates" title="Link to this heading">ÔÉÅ</a></h2>
<p>Control variates reduce variance by exploiting correlation between the quantity of interest and auxiliary random variables with known expectations. The technique is mathematically equivalent to regression adjustment in experimental design: we ‚Äúpredict away‚Äù variance using a related variable.</p>
<section id="theory-and-optimal-coefficient">
<h3>Theory and Optimal Coefficient<a class="headerlink" href="#theory-and-optimal-coefficient" title="Link to this heading">ÔÉÅ</a></h3>
<p>Let <span class="math notranslate nohighlight">\(H = h(X)\)</span> denote the quantity we wish to estimate, with unknown mean <span class="math notranslate nohighlight">\(I = \mathbb{E}[H]\)</span>. Suppose we have a <strong>control variate</strong> <span class="math notranslate nohighlight">\(C = c(X)\)</span> correlated with <span class="math notranslate nohighlight">\(H\)</span> whose expectation <span class="math notranslate nohighlight">\(\mu_C = \mathbb{E}[C]\)</span> is known exactly.</p>
<p>The <strong>control variate estimator</strong> adjusts the naive estimate by the deviation of <span class="math notranslate nohighlight">\(C\)</span> from its mean:</p>
<div class="math notranslate nohighlight" id="equation-cv-estimator">
<span class="eqno">()<a class="headerlink" href="#equation-cv-estimator" title="Link to this equation">ÔÉÅ</a></span>\[\hat{I}_{\text{CV}} = \frac{1}{n} \sum_{i=1}^n \left[ h(X_i) - \beta(c(X_i) - \mu_C) \right]\]</div>
<p><strong>Unbiasedness</strong>: For any coefficient <span class="math notranslate nohighlight">\(\beta\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[\hat{I}_{\text{CV}}] = \mathbb{E}[H] - \beta(\mathbb{E}[C] - \mu_C) = I - \beta \cdot 0 = I\]</div>
<p>The adjustment does not bias the estimator because <span class="math notranslate nohighlight">\(\mathbb{E}[C - \mu_C] = 0\)</span>.</p>
<p><strong>Variance</strong>: The variance per sample is:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(H - \beta(C - \mu_C)) = \text{Var}(H) + \beta^2 \text{Var}(C) - 2\beta \text{Cov}(H, C)\]</div>
<p>This is a quadratic in <span class="math notranslate nohighlight">\(\beta\)</span> minimized by setting the derivative to zero:</p>
<div class="math notranslate nohighlight">
\[\frac{d}{d\beta}\left[\text{Var}(H) + \beta^2 \text{Var}(C) - 2\beta \text{Cov}(H, C)\right] = 2\beta \text{Var}(C) - 2\text{Cov}(H, C) = 0\]</div>
<p>Solving yields the <strong>optimal control variate coefficient</strong>:</p>
<div class="math notranslate nohighlight" id="equation-optimal-beta">
<span class="eqno">()<a class="headerlink" href="#equation-optimal-beta" title="Link to this equation">ÔÉÅ</a></span>\[\beta^* = \frac{\text{Cov}(H, C)}{\text{Var}(C)}\]</div>
<p>This is precisely the slope from the simple linear regression of <span class="math notranslate nohighlight">\(H\)</span> on <span class="math notranslate nohighlight">\(C\)</span>.</p>
</section>
<section id="variance-reduction-equals-squared-correlation">
<h3>Variance Reduction Equals Squared Correlation<a class="headerlink" href="#variance-reduction-equals-squared-correlation" title="Link to this heading">ÔÉÅ</a></h3>
<p>Substituting <span class="math notranslate nohighlight">\(\beta^*\)</span> into the variance formula:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{Var}(H - \beta^*(C - \mu_C)) &amp;= \text{Var}(H) + \frac{\text{Cov}(H,C)^2}{\text{Var}(C)} - 2 \cdot \frac{\text{Cov}(H,C)^2}{\text{Var}(C)} \\
&amp;= \text{Var}(H) - \frac{\text{Cov}(H,C)^2}{\text{Var}(C)} \\
&amp;= \text{Var}(H) \left(1 - \rho_{H,C}^2\right)\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\rho_{H,C} = \text{Cov}(H,C)/\sqrt{\text{Var}(H)\text{Var}(C)}\)</span> is the Pearson correlation.</p>
<p><strong>Key Result</strong>: The variance reduction factor is <span class="math notranslate nohighlight">\(\rho_{H,C}^2\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-cv-vrf">
<span class="eqno">()<a class="headerlink" href="#equation-cv-vrf" title="Link to this equation">ÔÉÅ</a></span>\[\text{VRF} = \frac{1}{1 - \rho^2_{H,C}}\]</div>
<ul class="simple">
<li><p>Perfect correlation (<span class="math notranslate nohighlight">\(|\rho| = 1\)</span>): infinite reduction (zero variance)</p></li>
<li><p><span class="math notranslate nohighlight">\(\rho = 0.9\)</span>: 81% variance reduction (VRF = 5.3)</p></li>
<li><p><span class="math notranslate nohighlight">\(\rho = 0.7\)</span>: 51% variance reduction (VRF = 2.0)</p></li>
<li><p><span class="math notranslate nohighlight">\(\rho = 0.5\)</span>: 25% variance reduction (VRF = 1.3)</p></li>
</ul>
</section>
<section id="multiple-control-variates">
<h3>Multiple Control Variates<a class="headerlink" href="#multiple-control-variates" title="Link to this heading">ÔÉÅ</a></h3>
<p>With <span class="math notranslate nohighlight">\(m\)</span> control variates <span class="math notranslate nohighlight">\(\mathbf{C} = (C_1, \ldots, C_m)^\top\)</span> having known means <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_C\)</span>, the estimator becomes:</p>
<div class="math notranslate nohighlight">
\[\hat{I}_{\text{CV}} = \bar{H} - \boldsymbol{\beta}^\top(\bar{\mathbf{C}} - \boldsymbol{\mu}_C)\]</div>
<p>The optimal coefficient vector is:</p>
<div class="math notranslate nohighlight" id="equation-optimal-beta-vector">
<span class="eqno">()<a class="headerlink" href="#equation-optimal-beta-vector" title="Link to this equation">ÔÉÅ</a></span>\[\boldsymbol{\beta}^* = \boldsymbol{\Sigma}_C^{-1} \text{Cov}(\mathbf{C}, H)\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_C = \text{Var}(\mathbf{C})\)</span> is the <span class="math notranslate nohighlight">\(m \times m\)</span> covariance matrix of controls. This equals the multiple regression coefficients from regressing <span class="math notranslate nohighlight">\(H\)</span> on <span class="math notranslate nohighlight">\(\mathbf{C}\)</span>.</p>
<p>The minimum variance is:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(H_{\text{CV}}^*) = \text{Var}(H) - \text{Cov}(H, \mathbf{C})^\top \boldsymbol{\Sigma}_C^{-1} \text{Cov}(H, \mathbf{C})\]</div>
<p>which equals <span class="math notranslate nohighlight">\(\text{Var}(H)(1 - R^2)\)</span> where <span class="math notranslate nohighlight">\(R^2\)</span> is the coefficient of determination from the multiple regression.</p>
</section>
<section id="finding-good-controls">
<h3>Finding Good Controls<a class="headerlink" href="#finding-good-controls" title="Link to this heading">ÔÉÅ</a></h3>
<p>Good control variates satisfy two criteria:</p>
<ol class="arabic simple">
<li><p><strong>High correlation</strong> with <span class="math notranslate nohighlight">\(h(X)\)</span>: The stronger the correlation, the greater the variance reduction</p></li>
<li><p><strong>Known expectation</strong>: We must know <span class="math notranslate nohighlight">\(\mathbb{E}[C]\)</span> exactly, not estimate it</p></li>
</ol>
<p><strong>Common sources of controls:</strong></p>
<ul class="simple">
<li><p><strong>Simple functions of</strong> <span class="math notranslate nohighlight">\(X\)</span>: For <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(\mu, \sigma^2)\)</span>, use <span class="math notranslate nohighlight">\(C = X\)</span> (with <span class="math notranslate nohighlight">\(\mathbb{E}[X] = \mu\)</span>) or <span class="math notranslate nohighlight">\(C = (X-\mu)^2\)</span> (with <span class="math notranslate nohighlight">\(\mathbb{E}[(X-\mu)^2] = \sigma^2\)</span>)</p></li>
<li><p><strong>Taylor approximations</strong>: If <span class="math notranslate nohighlight">\(h(x) \approx a + b(x-\mu) + c(x-\mu)^2\)</span> near the mean, use centered powers as controls</p></li>
<li><p><strong>Partial analytical solutions</strong>: When <span class="math notranslate nohighlight">\(h = h_1 + h_2\)</span> and <span class="math notranslate nohighlight">\(\mathbb{E}[h_1]\)</span> is known, use <span class="math notranslate nohighlight">\(h_1\)</span> as a control</p></li>
<li><p><strong>Approximate or auxiliary models</strong>: In option pricing, a geometric Asian option (with closed-form price) serves as control for arithmetic Asian options</p></li>
</ul>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ‚ö†Ô∏è Estimating vs. Knowing the Control Mean</p>
<p>If <span class="math notranslate nohighlight">\(\mu_C\)</span> is estimated rather than known exactly, the uncertainty in <span class="math notranslate nohighlight">\(\hat{\mu}_C\)</span> adds variance to the control variate estimator, potentially nullifying benefits. Control variates require <em>exact</em> knowledge of <span class="math notranslate nohighlight">\(\mathbb{E}[C]\)</span>. Using an estimated mean converts variance reduction into variance shuffling.</p>
</div>
</section>
<section id="id1">
<h3>Python Implementation<a class="headerlink" href="#id1" title="Link to this heading">ÔÉÅ</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">control_variate_estimator</span><span class="p">(</span><span class="n">h_vals</span><span class="p">,</span> <span class="n">c_vals</span><span class="p">,</span> <span class="n">mu_c</span><span class="p">,</span> <span class="n">estimate_beta</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Control variate estimator for E[H] using control C with known E[C] = mu_c.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    h_vals : array_like</span>
<span class="sd">        Sample values of h(X)</span>
<span class="sd">    c_vals : array_like</span>
<span class="sd">        Sample values of control variate c(X)</span>
<span class="sd">    mu_c : float</span>
<span class="sd">        Known expectation of control variate</span>
<span class="sd">    estimate_beta : bool</span>
<span class="sd">        If True, estimate optimal beta from samples</span>
<span class="sd">    beta : float, optional</span>
<span class="sd">        Fixed beta coefficient (used if estimate_beta=False)</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict with keys:</span>
<span class="sd">        &#39;estimate&#39;: control variate estimate of E[H]</span>
<span class="sd">        &#39;se&#39;: standard error of estimate</span>
<span class="sd">        &#39;beta&#39;: coefficient used</span>
<span class="sd">        &#39;rho&#39;: estimated correlation</span>
<span class="sd">        &#39;vrf&#39;: estimated variance reduction factor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">h_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">h_vals</span><span class="p">)</span>
    <span class="n">c_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">c_vals</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">h_vals</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">estimate_beta</span><span class="p">:</span>
        <span class="c1"># Estimate optimal beta via regression</span>
        <span class="n">cov_hc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">h_vals</span><span class="p">,</span> <span class="n">c_vals</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">var_c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">c_vals</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">cov_hc</span> <span class="o">/</span> <span class="n">var_c</span> <span class="k">if</span> <span class="n">var_c</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mf">0.0</span>

    <span class="c1"># Control variate adjusted values</span>
    <span class="n">adjusted</span> <span class="o">=</span> <span class="n">h_vals</span> <span class="o">-</span> <span class="n">beta</span> <span class="o">*</span> <span class="p">(</span><span class="n">c_vals</span> <span class="o">-</span> <span class="n">mu_c</span><span class="p">)</span>

    <span class="c1"># Estimate and standard error</span>
    <span class="n">estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">adjusted</span><span class="p">)</span>
    <span class="n">se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">adjusted</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

    <span class="c1"># Diagnostics</span>
    <span class="n">rho</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">h_vals</span><span class="p">,</span> <span class="n">c_vals</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">var_h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">h_vals</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">var_adj</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">adjusted</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">vrf</span> <span class="o">=</span> <span class="n">var_h</span> <span class="o">/</span> <span class="n">var_adj</span> <span class="k">if</span> <span class="n">var_adj</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;estimate&#39;</span><span class="p">:</span> <span class="n">estimate</span><span class="p">,</span>
        <span class="s1">&#39;se&#39;</span><span class="p">:</span> <span class="n">se</span><span class="p">,</span>
        <span class="s1">&#39;beta&#39;</span><span class="p">:</span> <span class="n">beta</span><span class="p">,</span>
        <span class="s1">&#39;rho&#39;</span><span class="p">:</span> <span class="n">rho</span><span class="p">,</span>
        <span class="s1">&#39;vrf&#39;</span><span class="p">:</span> <span class="n">vrf</span><span class="p">,</span>
        <span class="s1">&#39;var_reduction_pct&#39;</span><span class="p">:</span> <span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mi">1</span><span class="o">/</span><span class="n">vrf</span><span class="p">)</span> <span class="k">if</span> <span class="n">vrf</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="p">}</span>
</pre></div>
</div>
<div class="note admonition">
<p class="admonition-title">Example üí° Estimating <span class="math notranslate nohighlight">\(\mathbb{E}[e^X]\)</span> for <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(0,1)\)</span></p>
<p><strong>Given:</strong> <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(0,1)\)</span>, estimate <span class="math notranslate nohighlight">\(I = \mathbb{E}[e^X]\)</span>.</p>
<p><strong>Analytical Result:</strong> By the moment generating function, <span class="math notranslate nohighlight">\(\mathbb{E}[e^X] = e^{1/2} \approx 1.6487\)</span>.</p>
<p><strong>Control Variate:</strong> Use <span class="math notranslate nohighlight">\(C = X\)</span> with <span class="math notranslate nohighlight">\(\mu_C = \mathbb{E}[X] = 0\)</span>.</p>
<p><strong>Theoretical Analysis:</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\text{Cov}(e^X, X) = \mathbb{E}[X e^X] - \mathbb{E}[X]\mathbb{E}[e^X] = \mathbb{E}[X e^X]\)</span></p></li>
<li><p>Using the identity <span class="math notranslate nohighlight">\(\mathbb{E}[X e^X] = \mathbb{E}[e^X]\)</span> for <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(0,1)\)</span>: <span class="math notranslate nohighlight">\(\text{Cov}(e^X, X) = e^{1/2}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\text{Var}(X) = 1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\beta^* = e^{1/2}/1 = e^{1/2} \approx 1.6487\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\text{Var}(e^X) = e^2 - e \approx 4.671\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\rho = e^{1/2}/\sqrt{e^2 - e} \approx 0.763\)</span></p></li>
<li><p>Variance reduction: <span class="math notranslate nohighlight">\(\rho^2 \approx 0.582\)</span> (58.2%)</p></li>
</ul>
<p><strong>Python Implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10_000</span>

<span class="c1"># True value</span>
<span class="n">I_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True E[e^X]: </span><span class="si">{</span><span class="n">I_true</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Generate samples</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">h_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c1"># e^X</span>
<span class="n">c_vals</span> <span class="o">=</span> <span class="n">X</span>          <span class="c1"># Control: X</span>
<span class="n">mu_c</span> <span class="o">=</span> <span class="mf">0.0</span>          <span class="c1"># Known E[X] = 0</span>

<span class="c1"># Naive Monte Carlo</span>
<span class="n">naive_est</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_vals</span><span class="p">)</span>
<span class="n">naive_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">h_vals</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Naive MC: </span><span class="si">{</span><span class="n">naive_est</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (SE: </span><span class="si">{</span><span class="n">naive_se</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># Control variate estimator</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">control_variate_estimator</span><span class="p">(</span><span class="n">h_vals</span><span class="p">,</span> <span class="n">c_vals</span><span class="p">,</span> <span class="n">mu_c</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Control Variate:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Estimate: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;estimate&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (SE: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;se&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Beta: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theoretical: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Correlation: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;rho&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Variance Reduction: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;var_reduction_pct&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>True E[e^X]: 1.648721

Naive MC: 1.670089 (SE: 0.021628)

Control Variate:
  Estimate: 1.651876 (SE: 0.013992)
  Beta: 1.6542 (theoretical: 1.6487)
  Correlation: 0.7618
  Variance Reduction: 58.2%
</pre></div>
</div>
<p><strong>Result:</strong> The control variate estimator reduces variance by 58%, matching the theoretical prediction. The estimated <span class="math notranslate nohighlight">\(\beta = 1.654\)</span> closely approximates the theoretical optimum <span class="math notranslate nohighlight">\(\beta^* = e^{1/2} \approx 1.649\)</span>.</p>
</div>
</section>
</section>
<section id="antithetic-variates">
<h2>Antithetic Variates<a class="headerlink" href="#antithetic-variates" title="Link to this heading">ÔÉÅ</a></h2>
<p>Antithetic variates reduce variance by constructing <strong>negatively correlated pairs</strong> of samples that share the same marginal distribution. When averaged, systematic cancellation occurs: if one sample yields a high estimate, its antithetic partner tends to yield a low estimate, damping fluctuations.</p>
<section id="construction-of-antithetic-pairs">
<h3>Construction of Antithetic Pairs<a class="headerlink" href="#construction-of-antithetic-pairs" title="Link to this heading">ÔÉÅ</a></h3>
<p>The fundamental construction uses the uniform distribution. For <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0,1)\)</span>:</p>
<ul class="simple">
<li><p>Both <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(1-U\)</span> have <span class="math notranslate nohighlight">\(\text{Uniform}(0,1)\)</span> marginals</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{Cov}(U, 1-U) = \mathbb{E}[U(1-U)] - \frac{1}{4} = \frac{1}{2} - \frac{1}{3} - \frac{1}{4} = -\frac{1}{12}\)</span></p></li>
<li><p>Correlation: <span class="math notranslate nohighlight">\(\rho(U, 1-U) = -1\)</span> (perfect negative correlation)</p></li>
</ul>
<p>For continuous distributions with CDF <span class="math notranslate nohighlight">\(F\)</span>, the pair <span class="math notranslate nohighlight">\((F^{-1}(U), F^{-1}(1-U))\)</span> forms an antithetic pair with the target distribution. For the normal distribution, since <span class="math notranslate nohighlight">\(\Phi^{-1}(1-U) = -\Phi^{-1}(U)\)</span>, the antithetic pairs are simply <span class="math notranslate nohighlight">\((Z, -Z)\)</span> where <span class="math notranslate nohighlight">\(Z = \Phi^{-1}(U)\)</span>.</p>
</section>
<section id="variance-formula">
<h3>Variance Formula<a class="headerlink" href="#variance-formula" title="Link to this heading">ÔÉÅ</a></h3>
<p>Consider estimating <span class="math notranslate nohighlight">\(I = \mathbb{E}[h(X)]\)</span> using antithetic pairs. Generate <span class="math notranslate nohighlight">\(n/2\)</span> independent uniforms <span class="math notranslate nohighlight">\(U_1, \ldots, U_{n/2}\)</span>, and for each <span class="math notranslate nohighlight">\(U_i\)</span>, form the pair <span class="math notranslate nohighlight">\((X_i, X_i')\)</span> where <span class="math notranslate nohighlight">\(X_i = F^{-1}(U_i)\)</span> and <span class="math notranslate nohighlight">\(X_i' = F^{-1}(1-U_i)\)</span>.</p>
<p>The antithetic estimator averages paired values:</p>
<div class="math notranslate nohighlight" id="equation-antithetic-estimator">
<span class="eqno">()<a class="headerlink" href="#equation-antithetic-estimator" title="Link to this equation">ÔÉÅ</a></span>\[\hat{I}_{\text{anti}} = \frac{1}{n/2} \sum_{i=1}^{n/2} \frac{h(X_i) + h(X_i')}{2}\]</div>
<p><strong>Variance Analysis</strong>: Let <span class="math notranslate nohighlight">\(Y = h(X)\)</span> and <span class="math notranslate nohighlight">\(Y' = h(X')\)</span> for an antithetic pair. The variance of the pair average is:</p>
<div class="math notranslate nohighlight">
\[\text{Var}\left(\frac{Y + Y'}{2}\right) = \frac{1}{4}\left[\text{Var}(Y) + \text{Var}(Y') + 2\text{Cov}(Y, Y')\right]\]</div>
<p>Since <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(X'\)</span> have the same marginal distribution, <span class="math notranslate nohighlight">\(\text{Var}(Y) = \text{Var}(Y')\)</span>. Thus:</p>
<div class="math notranslate nohighlight" id="equation-antithetic-variance">
<span class="eqno">()<a class="headerlink" href="#equation-antithetic-variance" title="Link to this equation">ÔÉÅ</a></span>\[\text{Var}\left(\frac{Y + Y'}{2}\right) = \frac{\text{Var}(h(X))}{2} + \frac{\text{Cov}(h(X), h(X'))}{2}\]</div>
<p><strong>Variance Reduction Condition</strong>: The estimator variance is <span class="math notranslate nohighlight">\(\frac{1}{2}\text{Var}(h(X))(1 + \rho)\)</span> where <span class="math notranslate nohighlight">\(\rho = \text{Corr}(h(X), h(X'))\)</span>.</p>
<ul class="simple">
<li><p><strong>Reduction occurs when</strong> <span class="math notranslate nohighlight">\(\rho &lt; 0\)</span> (negative correlation between <span class="math notranslate nohighlight">\(h(X)\)</span> and <span class="math notranslate nohighlight">\(h(X')\)</span>)</p></li>
<li><p><strong>No effect when</strong> <span class="math notranslate nohighlight">\(\rho = 0\)</span></p></li>
<li><p><strong>Variance increases when</strong> <span class="math notranslate nohighlight">\(\rho &gt; 0\)</span></p></li>
</ul>
</section>
<section id="the-monotone-function-theorem">
<h3>The Monotone Function Theorem<a class="headerlink" href="#the-monotone-function-theorem" title="Link to this heading">ÔÉÅ</a></h3>
<p><strong>Theorem</strong>: If <span class="math notranslate nohighlight">\(h\)</span> is monotonic (non-decreasing or non-increasing) and <span class="math notranslate nohighlight">\((X, X')\)</span> is an antithetic pair, then <span class="math notranslate nohighlight">\(\text{Cov}(h(X), h(X')) \leq 0\)</span>.</p>
<p><strong>Intuition</strong>: When <span class="math notranslate nohighlight">\(U\)</span> is large, <span class="math notranslate nohighlight">\(X = F^{-1}(U)\)</span> is in the upper tail, making <span class="math notranslate nohighlight">\(h(X)\)</span> large for increasing <span class="math notranslate nohighlight">\(h\)</span>. But <span class="math notranslate nohighlight">\(1-U\)</span> is small, so <span class="math notranslate nohighlight">\(X' = F^{-1}(1-U)\)</span> is in the lower tail, making <span class="math notranslate nohighlight">\(h(X')\)</span> small. This systematic opposition creates negative covariance.</p>
<p><strong>Formal Proof</strong>: Uses Hoeffding‚Äôs identity and the fact that <span class="math notranslate nohighlight">\((U, 1-U)\)</span> achieves the Fr√©chet‚ÄìHoeffding lower bound for joint distributions with uniform marginals (maximum negative dependence compatible with the marginals).</p>
</section>
<section id="limitations-for-non-monotone-functions">
<h3>Limitations for Non-Monotone Functions<a class="headerlink" href="#limitations-for-non-monotone-functions" title="Link to this heading">ÔÉÅ</a></h3>
<p>Antithetic variates can <strong>increase variance</strong> for non-monotone functions:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(h(u) = \cos(4\pi u)\)</span>: The antithetic <span class="math notranslate nohighlight">\(h(1-u) = \cos(4\pi - 4\pi u) = \cos(4\pi u) = h(u)\)</span>. Perfect positive correlation (<span class="math notranslate nohighlight">\(\rho = 1\)</span>) doubles variance!</p></li>
<li><p><span class="math notranslate nohighlight">\(h(u) = (u - 0.5)^2\)</span>: The antithetic <span class="math notranslate nohighlight">\(h(1-u) = (0.5 - u)^2 = h(u)\)</span>. Again, <span class="math notranslate nohighlight">\(\rho = 1\)</span>.</p></li>
</ul>
<p><strong>Rule</strong>: Before applying antithetic variates, verify that <span class="math notranslate nohighlight">\(h\)</span> is monotonic or estimate <span class="math notranslate nohighlight">\(\rho\)</span> from a pilot sample. For non-monotonic functions, antithetic variates may harm rather than help.</p>
</section>
<section id="id2">
<h3>Python Implementation<a class="headerlink" href="#id2" title="Link to this heading">ÔÉÅ</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">antithetic_estimator</span><span class="p">(</span><span class="n">h_func</span><span class="p">,</span> <span class="n">F_inv</span><span class="p">,</span> <span class="n">n_pairs</span><span class="p">,</span> <span class="n">return_diagnostics</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Antithetic variate estimator for E[h(X)] where X ~ F.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    h_func : callable</span>
<span class="sd">        Function to integrate</span>
<span class="sd">    F_inv : callable</span>
<span class="sd">        Inverse CDF of target distribution</span>
<span class="sd">    n_pairs : int</span>
<span class="sd">        Number of antithetic pairs to generate</span>
<span class="sd">    return_diagnostics : bool</span>
<span class="sd">        If True, return correlation diagnostics</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    estimate : float</span>
<span class="sd">        Antithetic estimate of E[h(X)]</span>
<span class="sd">    diagnostics : dict (optional)</span>
<span class="sd">        Including rho, variance_ratio</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Generate uniforms</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n_pairs</span><span class="p">)</span>

    <span class="c1"># Create antithetic pairs</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">F_inv</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>
    <span class="n">X_anti</span> <span class="o">=</span> <span class="n">F_inv</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">U</span><span class="p">)</span>

    <span class="c1"># Evaluate function</span>
    <span class="n">h_X</span> <span class="o">=</span> <span class="n">h_func</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">h_X_anti</span> <span class="o">=</span> <span class="n">h_func</span><span class="p">(</span><span class="n">X_anti</span><span class="p">)</span>

    <span class="c1"># Antithetic estimator: average of pair averages</span>
    <span class="n">pair_means</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_X</span> <span class="o">+</span> <span class="n">h_X_anti</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">pair_means</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">return_diagnostics</span><span class="p">:</span>
        <span class="c1"># Standard MC variance (using all 2n points)</span>
        <span class="n">all_h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">h_X</span><span class="p">,</span> <span class="n">h_X_anti</span><span class="p">])</span>
        <span class="n">var_standard</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">all_h</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Antithetic variance</span>
        <span class="n">var_antithetic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">pair_means</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Correlation between h(X) and h(X&#39;)</span>
        <span class="n">rho</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">h_X</span><span class="p">,</span> <span class="n">h_X_anti</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Variance reduction factor (comparing same total samples)</span>
        <span class="c1"># Standard: Var(h)/n vs Antithetic: Var(pair_mean)/(n/2)</span>
        <span class="n">vrf</span> <span class="o">=</span> <span class="p">(</span><span class="n">var_standard</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_pairs</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">var_antithetic</span> <span class="o">/</span> <span class="n">n_pairs</span><span class="p">)</span>

        <span class="n">diagnostics</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;estimate&#39;</span><span class="p">:</span> <span class="n">estimate</span><span class="p">,</span>
            <span class="s1">&#39;se&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">pair_means</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_pairs</span><span class="p">),</span>
            <span class="s1">&#39;rho&#39;</span><span class="p">:</span> <span class="n">rho</span><span class="p">,</span>
            <span class="s1">&#39;var_standard&#39;</span><span class="p">:</span> <span class="n">var_standard</span><span class="p">,</span>
            <span class="s1">&#39;var_antithetic&#39;</span><span class="p">:</span> <span class="n">var_antithetic</span><span class="p">,</span>
            <span class="s1">&#39;vrf&#39;</span><span class="p">:</span> <span class="n">vrf</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">estimate</span><span class="p">,</span> <span class="n">diagnostics</span>

    <span class="k">return</span> <span class="n">estimate</span>
</pre></div>
</div>
<div class="note admonition">
<p class="admonition-title">Example üí° The Exponential Integral with 97% Variance Reduction</p>
<p><strong>Given:</strong> Estimate <span class="math notranslate nohighlight">\(I = \int_0^1 e^x \, dx = e - 1 \approx 1.7183\)</span>.</p>
<p><strong>Analysis:</strong> The function <span class="math notranslate nohighlight">\(h(u) = e^u\)</span> is monotone increasing on <span class="math notranslate nohighlight">\([0,1]\)</span>, so antithetic variates should help.</p>
<p><strong>Theoretical Variance:</strong></p>
<ul class="simple">
<li><p>Standard MC: <span class="math notranslate nohighlight">\(\text{Var}(e^U) = \mathbb{E}[e^{2U}] - (\mathbb{E}[e^U])^2 = \frac{e^2 - 1}{2} - (e-1)^2 \approx 0.2420\)</span></p></li>
<li><p>Antithetic covariance: <span class="math notranslate nohighlight">\(\text{Cov}(e^U, e^{1-U}) = \mathbb{E}[e^U \cdot e^{1-U}] - (e-1)^2 = e - (e-1)^2 \approx -0.2342\)</span></p></li>
<li><p>Antithetic variance per pair: <span class="math notranslate nohighlight">\(\frac{1}{2}(0.2420) + \frac{1}{2}(-0.2342) = 0.0039\)</span></p></li>
</ul>
<p><strong>Variance Reduction:</strong> For equivalent cost (comparing <span class="math notranslate nohighlight">\(n\)</span> antithetic pairs to <span class="math notranslate nohighlight">\(2n\)</span> independent samples):</p>
<div class="math notranslate nohighlight">
\[\text{VRF} = \frac{0.2420 / 2}{0.0039} \approx 31\]</div>
<p>This represents approximately <strong>97% variance reduction</strong>.</p>
<p><strong>Python Implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_pairs</span> <span class="o">=</span> <span class="mi">10_000</span>

<span class="c1"># True value</span>
<span class="n">I_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True integral: </span><span class="si">{</span><span class="n">I_true</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Standard Monte Carlo (2n samples for fair comparison)</span>
<span class="n">U_standard</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_pairs</span><span class="p">)</span>
<span class="n">h_standard</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">U_standard</span><span class="p">)</span>
<span class="n">mc_estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_standard</span><span class="p">)</span>
<span class="n">mc_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">h_standard</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">mc_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mc_var</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_pairs</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Standard MC (</span><span class="si">{</span><span class="mi">2</span><span class="o">*</span><span class="n">n_pairs</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> samples):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Estimate: </span><span class="si">{</span><span class="n">mc_estimate</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  SE: </span><span class="si">{</span><span class="n">mc_se</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Antithetic variates (n pairs = 2n function evaluations)</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n_pairs</span><span class="p">)</span>
<span class="n">h_U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>
<span class="n">h_anti</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">U</span><span class="p">)</span>
<span class="n">pair_means</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_U</span> <span class="o">+</span> <span class="n">h_anti</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>

<span class="n">anti_estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">pair_means</span><span class="p">)</span>
<span class="n">anti_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">pair_means</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">anti_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">anti_var</span> <span class="o">/</span> <span class="n">n_pairs</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Antithetic (</span><span class="si">{</span><span class="n">n_pairs</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> pairs):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Estimate: </span><span class="si">{</span><span class="n">anti_estimate</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  SE: </span><span class="si">{</span><span class="n">anti_se</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Correlation diagnostic</span>
<span class="n">rho</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">h_U</span><span class="p">,</span> <span class="n">h_anti</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Correlation rho: </span><span class="si">{</span><span class="n">rho</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Variance reduction</span>
<span class="c1"># Fair comparison: both use 2n function evaluations</span>
<span class="c1"># Standard: Var/2n, Antithetic: Var(pair)/n</span>
<span class="n">vrf</span> <span class="o">=</span> <span class="p">(</span><span class="n">mc_var</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_pairs</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">anti_var</span> <span class="o">/</span> <span class="n">n_pairs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Variance Reduction Factor: </span><span class="si">{</span><span class="n">vrf</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Variance Reduction: </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="o">/</span><span class="n">vrf</span><span class="p">)</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>True integral: 1.718282

Standard MC (20,000 samples):
  Estimate: 1.721539
  SE: 0.003460

Antithetic (10,000 pairs):
  Estimate: 1.718298
  SE: 0.000639
  Correlation rho: -0.9678

Variance Reduction Factor: 29.3x
Variance Reduction: 96.6%
</pre></div>
</div>
<p><strong>Result:</strong> The strongly negative correlation (<span class="math notranslate nohighlight">\(\rho = -0.968\)</span>) yields a 29√ó variance reduction, achieving ~97% efficiency gain. The antithetic estimate is within 0.001% of the true value.</p>
</div>
</section>
</section>
<section id="stratified-sampling">
<h2>Stratified Sampling<a class="headerlink" href="#stratified-sampling" title="Link to this heading">ÔÉÅ</a></h2>
<p>Stratified sampling partitions the sample space into disjoint regions (strata) and draws samples from each region according to a carefully chosen allocation. By eliminating the randomness in how many samples fall in each region, stratification removes between-stratum variance‚Äîoften a dominant source of Monte Carlo error.</p>
<section id="the-stratified-estimator">
<h3>The Stratified Estimator<a class="headerlink" href="#the-stratified-estimator" title="Link to this heading">ÔÉÅ</a></h3>
<p>Partition the domain <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> into <span class="math notranslate nohighlight">\(K\)</span> disjoint, exhaustive strata <span class="math notranslate nohighlight">\(S_1, \ldots, S_K\)</span> with:</p>
<ul class="simple">
<li><p><strong>Stratum probabilities</strong>: <span class="math notranslate nohighlight">\(p_k = P(X \in S_k)\)</span> under the target distribution</p></li>
<li><p><strong>Within-stratum means</strong>: <span class="math notranslate nohighlight">\(\mu_k = \mathbb{E}[h(X) \mid X \in S_k]\)</span></p></li>
<li><p><strong>Within-stratum variances</strong>: <span class="math notranslate nohighlight">\(\sigma_k^2 = \text{Var}(h(X) \mid X \in S_k)\)</span></p></li>
</ul>
<p>The overall mean decomposes as:</p>
<div class="math notranslate nohighlight" id="equation-stratified-mean">
<span class="eqno">()<a class="headerlink" href="#equation-stratified-mean" title="Link to this equation">ÔÉÅ</a></span>\[I = \sum_{k=1}^K p_k \mu_k\]</div>
<p>The <strong>stratified estimator</strong> allocates <span class="math notranslate nohighlight">\(n_k\)</span> samples to stratum <span class="math notranslate nohighlight">\(k\)</span> (with <span class="math notranslate nohighlight">\(\sum_k n_k = n\)</span>), draws <span class="math notranslate nohighlight">\(X_{k,1}, \ldots, X_{k,n_k}\)</span> from the conditional distribution in stratum <span class="math notranslate nohighlight">\(k\)</span>, and combines:</p>
<div class="math notranslate nohighlight" id="equation-stratified-estimator">
<span class="eqno">()<a class="headerlink" href="#equation-stratified-estimator" title="Link to this equation">ÔÉÅ</a></span>\[\hat{I}_{\text{strat}} = \sum_{k=1}^K p_k \hat{\mu}_k, \quad \hat{\mu}_k = \frac{1}{n_k} \sum_{i=1}^{n_k} h(X_{k,i})\]</div>
<p>The variance is:</p>
<div class="math notranslate nohighlight" id="equation-stratified-variance">
<span class="eqno">()<a class="headerlink" href="#equation-stratified-variance" title="Link to this equation">ÔÉÅ</a></span>\[\text{Var}(\hat{I}_{\text{strat}}) = \sum_{k=1}^K \frac{p_k^2 \sigma_k^2}{n_k}\]</div>
</section>
<section id="proportional-allocation-always-reduces-variance">
<h3>Proportional Allocation Always Reduces Variance<a class="headerlink" href="#proportional-allocation-always-reduces-variance" title="Link to this heading">ÔÉÅ</a></h3>
<p>Under <strong>proportional allocation</strong> <span class="math notranslate nohighlight">\(n_k = n p_k\)</span>, the variance simplifies to:</p>
<div class="math notranslate nohighlight" id="equation-proportional-variance">
<span class="eqno">()<a class="headerlink" href="#equation-proportional-variance" title="Link to this equation">ÔÉÅ</a></span>\[\text{Var}(\hat{I}_{\text{prop}}) = \frac{1}{n} \sum_{k=1}^K p_k \sigma_k^2\]</div>
<p>The law of total variance (ANOVA decomposition) reveals why this always helps:</p>
<div class="math notranslate nohighlight">
\[\sigma^2 = \text{Var}(h(X)) = \underbrace{\mathbb{E}[\text{Var}(h \mid S)]}_{\text{within}} + \underbrace{\text{Var}(\mathbb{E}[h \mid S])}_{\text{between}} = \sum_k p_k \sigma_k^2 + \sum_k p_k (\mu_k - I)^2\]</div>
<p>The first term is <strong>within-stratum variance</strong>; the second is <strong>between-stratum variance</strong>. Since:</p>
<div class="math notranslate nohighlight">
\[\sum_k p_k \sigma_k^2 = \sigma^2 - \sum_k p_k (\mu_k - I)^2 \leq \sigma^2\]</div>
<p>we have <span class="math notranslate nohighlight">\(\text{Var}(\hat{I}_{\text{prop}}) \leq \text{Var}(\hat{I}_{\text{MC}})\)</span> with equality only when all stratum means are identical.</p>
<p><strong>Key Insight</strong>: Stratified sampling with proportional allocation <strong>eliminates the between-stratum variance component entirely</strong>. Variance reduction equals the proportion of total variance explained by stratum membership.</p>
</section>
<section id="neyman-allocation-minimizes-variance">
<h3>Neyman Allocation Minimizes Variance<a class="headerlink" href="#neyman-allocation-minimizes-variance" title="Link to this heading">ÔÉÅ</a></h3>
<p><strong>Theorem (Neyman, 1934)</strong>: The allocation minimizing <span class="math notranslate nohighlight">\(\text{Var}(\hat{I}_{\text{strat}})\)</span> subject to <span class="math notranslate nohighlight">\(\sum_k n_k = n\)</span> is:</p>
<div class="math notranslate nohighlight" id="equation-neyman-allocation">
<span class="eqno">()<a class="headerlink" href="#equation-neyman-allocation" title="Link to this equation">ÔÉÅ</a></span>\[n_k^* = n \cdot \frac{p_k \sigma_k}{\sum_j p_j \sigma_j}\]</div>
<p><strong>Proof</strong>: Using Lagrange multipliers on <span class="math notranslate nohighlight">\(\mathcal{L} = \sum_k p_k^2 \sigma_k^2 / n_k + \lambda(\sum_k n_k - n)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial \mathcal{L}}{\partial n_k} = -\frac{p_k^2 \sigma_k^2}{n_k^2} + \lambda = 0 \quad \Rightarrow \quad n_k = \frac{p_k \sigma_k}{\sqrt{\lambda}}\]</div>
<p>The constraint yields <span class="math notranslate nohighlight">\(\sqrt{\lambda} = (\sum_j p_j \sigma_j)/n\)</span>, giving the result. <span class="math notranslate nohighlight">\(\blacksquare\)</span></p>
<p>The optimal variance is:</p>
<div class="math notranslate nohighlight" id="equation-neyman-variance">
<span class="eqno">()<a class="headerlink" href="#equation-neyman-variance" title="Link to this equation">ÔÉÅ</a></span>\[\text{Var}(\hat{I}_{\text{opt}}) = \frac{1}{n} \left(\sum_k p_k \sigma_k\right)^2\]</div>
<p><strong>Interpretation</strong>: Neyman allocation samples more heavily from:</p>
<ol class="arabic simple">
<li><p><strong>Larger strata</strong> (large <span class="math notranslate nohighlight">\(p_k\)</span>): They contribute more to the integral</p></li>
<li><p><strong>More variable strata</strong> (large <span class="math notranslate nohighlight">\(\sigma_k\)</span>): They need more samples for precise estimation</p></li>
</ol>
</section>
<section id="latin-hypercube-sampling-for-high-dimensions">
<h3>Latin Hypercube Sampling for High Dimensions<a class="headerlink" href="#latin-hypercube-sampling-for-high-dimensions" title="Link to this heading">ÔÉÅ</a></h3>
<p>Traditional stratification suffers from the curse of dimensionality: <span class="math notranslate nohighlight">\(m\)</span> strata per dimension requires <span class="math notranslate nohighlight">\(m^d\)</span> samples in <span class="math notranslate nohighlight">\(d\)</span> dimensions. <strong>Latin Hypercube Sampling (LHS)</strong>, introduced by McKay, Beckman, and Conover (1979), provides a practical alternative.</p>
<p>For <span class="math notranslate nohighlight">\(n\)</span> samples in <span class="math notranslate nohighlight">\([0,1]^d\)</span>, LHS:</p>
<ol class="arabic simple">
<li><p>Divides each dimension into <span class="math notranslate nohighlight">\(n\)</span> equal intervals</p></li>
<li><p>Places exactly one sample point in each interval for each dimension</p></li>
<li><p>Pairs intervals across dimensions randomly</p></li>
</ol>
<p><strong>Key Property</strong>: Each univariate margin is perfectly stratified. If the function is approximately additive, <span class="math notranslate nohighlight">\(h(\mathbf{x}) \approx \sum_j h_j(x_j)\)</span>, LHS achieves large variance reduction by controlling each main effect.</p>
<p>Stein (1987) proved that LHS variance is asymptotically:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\hat{\mu}_{\text{LHS}}) = \frac{1}{n} \int [h(\mathbf{x}) - h^{\text{add}}(\mathbf{x})]^2 \, dx + o(1/n)\]</div>
<p>where <span class="math notranslate nohighlight">\(h^{\text{add}}(\mathbf{x}) = \sum_j h_j(x_j)\)</span> is the additive approximation. LHS filters out all main effects‚Äîvariance reduction depends on the degree of non-additivity (interactions) in <span class="math notranslate nohighlight">\(h\)</span>.</p>
<p>Owen (1997) proved: <span class="math notranslate nohighlight">\(\text{Var}(\hat{\mu}_{\text{LHS}}) \leq \sigma^2/(n-1)\)</span>. LHS with <span class="math notranslate nohighlight">\(n\)</span> points is never worse than simple MC with <span class="math notranslate nohighlight">\(n-1\)</span> points.</p>
</section>
<section id="id3">
<h3>Python Implementation<a class="headerlink" href="#id3" title="Link to this heading">ÔÉÅ</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">stratified_estimator</span><span class="p">(</span><span class="n">h_func</span><span class="p">,</span> <span class="n">sampler_by_stratum</span><span class="p">,</span> <span class="n">stratum_probs</span><span class="p">,</span> <span class="n">n_per_stratum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Stratified sampling estimator.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    h_func : callable</span>
<span class="sd">        Function to integrate</span>
<span class="sd">    sampler_by_stratum : list of callables</span>
<span class="sd">        sampler_by_stratum[k](n) returns n samples from stratum k</span>
<span class="sd">    stratum_probs : array_like</span>
<span class="sd">        Probabilities p_k for each stratum</span>
<span class="sd">    n_per_stratum : array_like</span>
<span class="sd">        Number of samples n_k for each stratum</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict with estimate, se, within_var, between_var diagnostics</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">stratum_probs</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">stratum_probs</span><span class="p">)</span>
    <span class="n">n_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">n_per_stratum</span><span class="p">)</span>

    <span class="n">stratum_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
    <span class="n">stratum_vars</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
        <span class="c1"># Sample from stratum k</span>
        <span class="n">X_k</span> <span class="o">=</span> <span class="n">sampler_by_stratum</span><span class="p">[</span><span class="n">k</span><span class="p">](</span><span class="n">n_k</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
        <span class="n">h_k</span> <span class="o">=</span> <span class="n">h_func</span><span class="p">(</span><span class="n">X_k</span><span class="p">)</span>

        <span class="n">stratum_means</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_k</span><span class="p">)</span>
        <span class="n">stratum_vars</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">h_k</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Stratified estimate</span>
    <span class="n">estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span> <span class="o">*</span> <span class="n">stratum_means</span><span class="p">)</span>

    <span class="c1"># Variance of stratified estimator</span>
    <span class="n">var_strat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">stratum_vars</span> <span class="o">/</span> <span class="n">n_k</span><span class="p">)</span>
    <span class="n">se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_strat</span><span class="p">)</span>

    <span class="c1"># Decomposition</span>
    <span class="n">within_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span> <span class="o">*</span> <span class="n">stratum_vars</span><span class="p">)</span>
    <span class="n">between_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span> <span class="o">*</span> <span class="p">(</span><span class="n">stratum_means</span> <span class="o">-</span> <span class="n">estimate</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;estimate&#39;</span><span class="p">:</span> <span class="n">estimate</span><span class="p">,</span>
        <span class="s1">&#39;se&#39;</span><span class="p">:</span> <span class="n">se</span><span class="p">,</span>
        <span class="s1">&#39;stratum_means&#39;</span><span class="p">:</span> <span class="n">stratum_means</span><span class="p">,</span>
        <span class="s1">&#39;stratum_vars&#39;</span><span class="p">:</span> <span class="n">stratum_vars</span><span class="p">,</span>
        <span class="s1">&#39;within_var&#39;</span><span class="p">:</span> <span class="n">within_var</span><span class="p">,</span>
        <span class="s1">&#39;between_var&#39;</span><span class="p">:</span> <span class="n">between_var</span>
    <span class="p">}</span>


<span class="k">def</span><span class="w"> </span><span class="nf">latin_hypercube_sample</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate n Latin Hypercube samples in [0,1]^d.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n : int</span>
<span class="sd">        Number of samples</span>
<span class="sd">    d : int</span>
<span class="sd">        Dimension</span>
<span class="sd">    seed : int, optional</span>
<span class="sd">        Random seed</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    samples : ndarray of shape (n, d)</span>
<span class="sd">        Latin Hypercube samples</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
        <span class="c1"># Create n equally spaced intervals and shuffle</span>
        <span class="n">perm</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
        <span class="c1"># Uniform sample within each stratum</span>
        <span class="n">samples</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">perm</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">))</span> <span class="o">/</span> <span class="n">n</span>

    <span class="k">return</span> <span class="n">samples</span>
</pre></div>
</div>
<div class="note admonition">
<p class="admonition-title">Example üí° Stratified vs. Simple Monte Carlo for Heterogeneous Integrand</p>
<p><strong>Given:</strong> Estimate <span class="math notranslate nohighlight">\(I = \int_0^1 h(x) \, dx\)</span> where <span class="math notranslate nohighlight">\(h(x) = e^{10x}\)</span> for <span class="math notranslate nohighlight">\(x &lt; 0.2\)</span> and <span class="math notranslate nohighlight">\(h(x) = 1\)</span> otherwise. This integrand has high variance concentrated in <span class="math notranslate nohighlight">\([0, 0.2)\)</span>.</p>
<p><strong>Strategy:</strong> Stratify into <span class="math notranslate nohighlight">\(S_1 = [0, 0.2)\)</span> and <span class="math notranslate nohighlight">\(S_2 = [0.2, 1]\)</span> with <span class="math notranslate nohighlight">\(p_1 = 0.2\)</span>, <span class="math notranslate nohighlight">\(p_2 = 0.8\)</span>.</p>
<p><strong>Python Implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">integrate</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">h</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&lt;</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">10</span> <span class="o">*</span> <span class="n">x</span><span class="p">),</span> <span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># True value by numerical integration</span>
<span class="n">I_true</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">integrate</span><span class="o">.</span><span class="n">quad</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True integral: </span><span class="si">{</span><span class="n">I_true</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">n_total</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Simple Monte Carlo</span>
<span class="n">X_mc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_total</span><span class="p">)</span>
<span class="n">h_mc</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">X_mc</span><span class="p">)</span>
<span class="n">mc_est</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_mc</span><span class="p">)</span>
<span class="n">mc_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">h_mc</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_total</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Simple MC: </span><span class="si">{</span><span class="n">mc_est</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (SE: </span><span class="si">{</span><span class="n">mc_se</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># Stratified sampling with proportional allocation</span>
<span class="n">p1</span><span class="p">,</span> <span class="n">p2</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span>
<span class="n">n1</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_total</span> <span class="o">*</span> <span class="n">p1</span><span class="p">)</span>  <span class="c1"># 200 samples in [0, 0.2)</span>
<span class="n">n2</span> <span class="o">=</span> <span class="n">n_total</span> <span class="o">-</span> <span class="n">n1</span>       <span class="c1"># 800 samples in [0.2, 1]</span>

<span class="c1"># Sample from each stratum</span>
<span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">n1</span><span class="p">)</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n2</span><span class="p">)</span>

<span class="n">h1</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">X1</span><span class="p">)</span>
<span class="n">h2</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">X2</span><span class="p">)</span>

<span class="n">mu1_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h1</span><span class="p">)</span>
<span class="n">mu2_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h2</span><span class="p">)</span>

<span class="n">strat_est</span> <span class="o">=</span> <span class="n">p1</span> <span class="o">*</span> <span class="n">mu1_hat</span> <span class="o">+</span> <span class="n">p2</span> <span class="o">*</span> <span class="n">mu2_hat</span>

<span class="c1"># Stratified SE</span>
<span class="n">var1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">h1</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">var2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">h2</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">strat_var</span> <span class="o">=</span> <span class="p">(</span><span class="n">p1</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">var1</span> <span class="o">/</span> <span class="n">n1</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">p2</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">var2</span> <span class="o">/</span> <span class="n">n2</span><span class="p">)</span>
<span class="n">strat_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">strat_var</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Stratified:  </span><span class="si">{</span><span class="n">strat_est</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (SE: </span><span class="si">{</span><span class="n">strat_se</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># Variance decomposition</span>
<span class="n">total_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">h_mc</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">within_var</span> <span class="o">=</span> <span class="n">p1</span> <span class="o">*</span> <span class="n">var1</span> <span class="o">+</span> <span class="n">p2</span> <span class="o">*</span> <span class="n">var2</span>
<span class="n">between_var</span> <span class="o">=</span> <span class="n">total_var</span> <span class="o">-</span> <span class="n">within_var</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Variance Decomposition:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Total variance: </span><span class="si">{</span><span class="n">total_var</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Within-stratum: </span><span class="si">{</span><span class="n">within_var</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">within_var</span><span class="o">/</span><span class="n">total_var</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Between-stratum: </span><span class="si">{</span><span class="n">between_var</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">between_var</span><span class="o">/</span><span class="n">total_var</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Variance Reduction Factor: </span><span class="si">{</span><span class="p">(</span><span class="n">mc_se</span><span class="o">/</span><span class="n">strat_se</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>True integral: 2.256930

Simple MC: 2.4321 (SE: 0.1897)
Stratified:  2.2549 (SE: 0.0584)

Variance Decomposition:
  Total variance: 35.9784
  Within-stratum: 3.2481 (9.0%)
  Between-stratum: 32.7303 (91.0%)

Variance Reduction Factor: 10.6x
</pre></div>
</div>
<p><strong>Result:</strong> The between-stratum variance (91% of total) is eliminated by stratification, yielding a 10√ó variance reduction. The stratified estimate (2.255) is much closer to truth (2.257) than simple MC (2.432).</p>
</div>
</section>
</section>
<section id="common-random-numbers">
<h2>Common Random Numbers<a class="headerlink" href="#common-random-numbers" title="Link to this heading">ÔÉÅ</a></h2>
<p>When comparing systems or parameters, <strong>common random numbers (CRN)</strong> use identical random inputs across all configurations, inducing positive correlation between estimates and dramatically reducing the variance of their <em>difference</em>.</p>
<p>CRN does not reduce the variance of individual estimates‚Äîit reduces the variance of comparisons. This distinction is crucial: CRN is a technique for A/B testing, sensitivity analysis, and system comparison, not for single-system estimation.</p>
<section id="variance-analysis-for-differences">
<h3>Variance Analysis for Differences<a class="headerlink" href="#variance-analysis-for-differences" title="Link to this heading">ÔÉÅ</a></h3>
<p>Consider comparing two systems with expected values <span class="math notranslate nohighlight">\(\theta_1 = \mathbb{E}[h_1(X)]\)</span> and <span class="math notranslate nohighlight">\(\theta_2 = \mathbb{E}[h_2(X)]\)</span>. We want to estimate the difference <span class="math notranslate nohighlight">\(\Delta = \theta_1 - \theta_2\)</span>.</p>
<p><strong>Independent Sampling</strong>: Draw <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} F\)</span> for system 1 and independent <span class="math notranslate nohighlight">\(Y_1, \ldots, Y_n \stackrel{\text{iid}}{\sim} F\)</span> for system 2:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\hat{\theta}_1 - \hat{\theta}_2) = \text{Var}(\hat{\theta}_1) + \text{Var}(\hat{\theta}_2) = \frac{\sigma_1^2 + \sigma_2^2}{n}\]</div>
<p><strong>Common Random Numbers</strong>: Use the <em>same</em> inputs for both systems. Draw <span class="math notranslate nohighlight">\(X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} F\)</span> and compute <span class="math notranslate nohighlight">\(D_i = h_1(X_i) - h_2(X_i)\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-crn-variance">
<span class="eqno">()<a class="headerlink" href="#equation-crn-variance" title="Link to this equation">ÔÉÅ</a></span>\[\text{Var}(\hat{\Delta}_{\text{CRN}}) = \frac{1}{n}\text{Var}(h_1(X) - h_2(X)) = \frac{\sigma_1^2 + \sigma_2^2 - 2\text{Cov}(h_1(X), h_2(X))}{n}\]</div>
<p>When systems respond similarly to the same inputs, <span class="math notranslate nohighlight">\(\text{Cov}(h_1, h_2) &gt; 0\)</span>, and the <span class="math notranslate nohighlight">\(-2\text{Cov}\)</span> term reduces variance substantially.</p>
<p><strong>Perfect Correlation Limit</strong>: If <span class="math notranslate nohighlight">\(h_1(x) = h_2(x) + c\)</span> (constant difference), then <span class="math notranslate nohighlight">\(D_i = c\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>, and <span class="math notranslate nohighlight">\(\text{Var}(\hat{\Delta}_{\text{CRN}}) = 0\)</span>. We estimate the difference with zero variance!</p>
</section>
<section id="when-crn-works-best">
<h3>When CRN Works Best<a class="headerlink" href="#when-crn-works-best" title="Link to this heading">ÔÉÅ</a></h3>
<p>CRN is most effective when:</p>
<ol class="arabic simple">
<li><p><strong>Systems are similar</strong>: Small changes to parameters, comparing variants of the same algorithm</p></li>
<li><p><strong>Response is monotonic</strong>: Both systems improve or degrade together with input quality</p></li>
<li><p><strong>Synchronization is possible</strong>: The same random number serves the same purpose in both systems</p></li>
</ol>
<p><strong>Synchronization is critical</strong>: In a queueing simulation, the random number generating customer <span class="math notranslate nohighlight">\(i\)</span>‚Äôs arrival time must generate customer <span class="math notranslate nohighlight">\(i\)</span>‚Äôs arrival time in <em>all</em> configurations. Misaligned synchronization destroys the correlation that CRN exploits.</p>
</section>
<section id="applications">
<h3>Applications<a class="headerlink" href="#applications" title="Link to this heading">ÔÉÅ</a></h3>
<ul class="simple">
<li><p><strong>A/B Testing in Simulation</strong>: Compare policies using identical demand sequences, arrival patterns, or market scenarios</p></li>
<li><p><strong>Sensitivity Analysis</strong>: Estimate derivatives <span class="math notranslate nohighlight">\(\partial \theta / \partial \alpha\)</span> using common inputs at <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\alpha + \delta\)</span></p></li>
<li><p><strong>Ranking and Selection</strong>: Compare multiple system variants fairly under identical conditions</p></li>
<li><p><strong>Optimization</strong>: Gradient estimation for simulation-based optimization</p></li>
</ul>
</section>
<section id="id4">
<h3>Python Implementation<a class="headerlink" href="#id4" title="Link to this heading">ÔÉÅ</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">crn_comparison</span><span class="p">(</span><span class="n">h1_func</span><span class="p">,</span> <span class="n">h2_func</span><span class="p">,</span> <span class="n">input_sampler</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compare two systems using common random numbers.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    h1_func : callable</span>
<span class="sd">        System 1 response function</span>
<span class="sd">    h2_func : callable</span>
<span class="sd">        System 2 response function</span>
<span class="sd">    input_sampler : callable</span>
<span class="sd">        Function that returns n samples of common inputs</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of comparisons</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict with difference estimate, se, correlation, vrf</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Generate common inputs</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">input_sampler</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Evaluate both systems on same inputs</span>
    <span class="n">h1</span> <span class="o">=</span> <span class="n">h1_func</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">h2</span> <span class="o">=</span> <span class="n">h2_func</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="c1"># Paired differences</span>
    <span class="n">D</span> <span class="o">=</span> <span class="n">h1</span> <span class="o">-</span> <span class="n">h2</span>

    <span class="c1"># CRN estimate and SE</span>
    <span class="n">diff_est</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
    <span class="n">diff_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Diagnostics</span>
    <span class="n">var1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">h1</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">var2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">h2</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">cov12</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">h1</span><span class="p">,</span> <span class="n">h2</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">rho</span> <span class="o">=</span> <span class="n">cov12</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var1</span> <span class="o">*</span> <span class="n">var2</span><span class="p">)</span>

    <span class="c1"># Variance reduction factor vs independent sampling</span>
    <span class="n">var_indep</span> <span class="o">=</span> <span class="p">(</span><span class="n">var1</span> <span class="o">+</span> <span class="n">var2</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_samples</span>
    <span class="n">var_crn</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_samples</span>
    <span class="n">vrf</span> <span class="o">=</span> <span class="n">var_indep</span> <span class="o">/</span> <span class="n">var_crn</span> <span class="k">if</span> <span class="n">var_crn</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;diff_estimate&#39;</span><span class="p">:</span> <span class="n">diff_est</span><span class="p">,</span>
        <span class="s1">&#39;diff_se&#39;</span><span class="p">:</span> <span class="n">diff_se</span><span class="p">,</span>
        <span class="s1">&#39;theta1&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h1</span><span class="p">),</span>
        <span class="s1">&#39;theta2&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h2</span><span class="p">),</span>
        <span class="s1">&#39;correlation&#39;</span><span class="p">:</span> <span class="n">rho</span><span class="p">,</span>
        <span class="s1">&#39;vrf&#39;</span><span class="p">:</span> <span class="n">vrf</span><span class="p">,</span>
        <span class="s1">&#39;indep_se&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_indep</span><span class="p">),</span>
        <span class="s1">&#39;crn_se&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_crn</span><span class="p">)</span>
    <span class="p">}</span>
</pre></div>
</div>
<div class="note admonition">
<p class="admonition-title">Example üí° Comparing Two Inventory Policies</p>
<p><strong>Given:</strong> An inventory system with daily demand <span class="math notranslate nohighlight">\(D \sim \text{Poisson}(50)\)</span>. Compare:</p>
<ul class="simple">
<li><p><strong>Policy A</strong>: Order up to 60 units each day</p></li>
<li><p><strong>Policy B</strong>: Order up to 65 units each day</p></li>
</ul>
<p>Cost = holding cost (0.10 per unit per day) + stockout cost (5 per unit short).</p>
<p><strong>Python Implementation:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_days</span> <span class="o">=</span> <span class="mi">10_000</span>

<span class="k">def</span><span class="w"> </span><span class="nf">simulate_cost</span><span class="p">(</span><span class="n">order_up_to</span><span class="p">,</span> <span class="n">demands</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute daily costs for order-up-to policy.&quot;&quot;&quot;</span>
    <span class="n">costs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">demands</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">demands</span><span class="p">):</span>
        <span class="c1"># On-hand inventory after ordering up to level</span>
        <span class="n">on_hand</span> <span class="o">=</span> <span class="n">order_up_to</span>
        <span class="c1"># After demand</span>
        <span class="n">remaining</span> <span class="o">=</span> <span class="n">on_hand</span> <span class="o">-</span> <span class="n">d</span>
        <span class="k">if</span> <span class="n">remaining</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">costs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.10</span> <span class="o">*</span> <span class="n">remaining</span>  <span class="c1"># Holding cost</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">costs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mf">5.0</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">remaining</span><span class="p">)</span>  <span class="c1"># Stockout cost</span>
    <span class="k">return</span> <span class="n">costs</span>

<span class="c1"># Generate common demands (CRN)</span>
<span class="n">common_demands</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">n_days</span><span class="p">)</span>

<span class="c1"># Evaluate both policies on same demands</span>
<span class="n">costs_A</span> <span class="o">=</span> <span class="n">simulate_cost</span><span class="p">(</span><span class="mi">60</span><span class="p">,</span> <span class="n">common_demands</span><span class="p">)</span>
<span class="n">costs_B</span> <span class="o">=</span> <span class="n">simulate_cost</span><span class="p">(</span><span class="mi">65</span><span class="p">,</span> <span class="n">common_demands</span><span class="p">)</span>

<span class="c1"># CRN comparison</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">costs_A</span> <span class="o">-</span> <span class="n">costs_B</span>
<span class="n">diff_est</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
<span class="n">diff_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_days</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Common Random Numbers Comparison&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">45</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Policy A (order to 60): mean cost = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">costs_A</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Policy B (order to 65): mean cost = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">costs_B</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Difference (A - B): </span><span class="si">{</span><span class="n">diff_est</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;SE of difference (CRN): </span><span class="si">{</span><span class="n">diff_se</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% CI: (</span><span class="si">{</span><span class="n">diff_est</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mf">1.96</span><span class="o">*</span><span class="n">diff_se</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">diff_est</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mf">1.96</span><span class="o">*</span><span class="n">diff_se</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># Compare to independent sampling</span>
<span class="n">rho</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">costs_A</span><span class="p">,</span> <span class="n">costs_B</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">var_A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">costs_A</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">var_B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">costs_B</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">se_indep</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">var_A</span> <span class="o">+</span> <span class="n">var_B</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_days</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Correlation between policies: </span><span class="si">{</span><span class="n">rho</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;SE if independent sampling: </span><span class="si">{</span><span class="n">se_indep</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Variance Reduction Factor: </span><span class="si">{</span><span class="p">(</span><span class="n">se_indep</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">diff_se</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Common Random Numbers Comparison
=============================================
Policy A (order to 60): mean cost = 2.7632
Policy B (order to 65): mean cost = 1.4241

Difference (A - B): 1.3391
SE of difference (CRN): 0.0312
95% CI: (1.2780, 1.4002)

Correlation between policies: 0.9127
SE if independent sampling: 0.0987
Variance Reduction Factor: 10.0x
</pre></div>
</div>
<p><strong>Result:</strong> With 91% correlation between policies, CRN achieves 10√ó variance reduction. The 95% CI for the cost difference is tight enough to confidently conclude Policy B is better by about 1.34 cost units per day.</p>
</div>
</section>
</section>
<section id="combining-variance-reduction-techniques">
<h2>Combining Variance Reduction Techniques<a class="headerlink" href="#combining-variance-reduction-techniques" title="Link to this heading">ÔÉÅ</a></h2>
<p>The five techniques developed above are not mutually exclusive. Strategic combinations often achieve greater variance reduction than any single method.</p>
<section id="compatible-combinations">
<h3>Compatible Combinations<a class="headerlink" href="#compatible-combinations" title="Link to this heading">ÔÉÅ</a></h3>
<ol class="arabic simple">
<li><p><strong>Importance Sampling + Control Variates</strong>: Use a control variate under the proposal distribution. The optimal coefficient adapts to the IS framework.</p></li>
<li><p><strong>Stratified Sampling + Antithetic Variates</strong>: Within each stratum, use antithetic pairs to reduce within-stratum variance further.</p></li>
<li><p><strong>Control Variates + Common Random Numbers</strong>: When comparing systems, apply the same control variate adjustment to both.</p></li>
<li><p><strong>Importance Sampling + Stratified Sampling</strong>: Stratify the proposal distribution to ensure coverage of important regions.</p></li>
</ol>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ‚ö†Ô∏è Combining Methods Requires Care</p>
<p>Not all combinations are straightforward:</p>
<ul class="simple">
<li><p><strong>Antithetic variates + Importance sampling</strong>: The standard antithetic construction <span class="math notranslate nohighlight">\((U, 1-U)\)</span> may not induce negative correlation after weighting</p></li>
<li><p><strong>Multiple controls</strong>: Adding weakly correlated controls inflates <span class="math notranslate nohighlight">\(\beta\)</span> estimation variance; use only strong, independent controls</p></li>
<li><p><strong>Verification</strong>: Always verify unbiasedness holds after combining methods</p></li>
</ul>
</div>
</section>
<section id="practical-guidelines">
<h3>Practical Guidelines<a class="headerlink" href="#practical-guidelines" title="Link to this heading">ÔÉÅ</a></h3>
<ol class="arabic simple">
<li><p><strong>Start simple</strong>: Apply control variates or antithetic variates first‚Äîlow overhead, often effective</p></li>
<li><p><strong>Diagnose variance sources</strong>: Use ANOVA decomposition to identify whether between-stratum or within-stratum variance dominates</p></li>
<li><p><strong>Monitor diagnostics</strong>: Track ESS for importance sampling, correlation for control/antithetic variates</p></li>
<li><p><strong>Pilot estimation</strong>: Use small pilot runs to estimate optimal coefficients, verify negative correlation, and check weight distributions</p></li>
<li><p><strong>Validate improvements</strong>: Compare variance estimates with and without reduction; confirm actual benefit</p></li>
</ol>
</section>
</section>
<section id="practical-considerations">
<h2>Practical Considerations<a class="headerlink" href="#practical-considerations" title="Link to this heading">ÔÉÅ</a></h2>
<section id="numerical-stability">
<h3>Numerical Stability<a class="headerlink" href="#numerical-stability" title="Link to this heading">ÔÉÅ</a></h3>
<ul class="simple">
<li><p><strong>Log-space arithmetic</strong>: For importance sampling, always compute weights in log-space using the logsumexp trick</p></li>
<li><p><strong>Coefficient estimation</strong>: For control variates, estimate <span class="math notranslate nohighlight">\(\beta\)</span> using numerically stable regression routines</p></li>
<li><p><strong>Weight clipping</strong>: Consider truncating extreme importance weights to reduce variance at the cost of small bias</p></li>
</ul>
</section>
<section id="computational-overhead">
<h3>Computational Overhead<a class="headerlink" href="#computational-overhead" title="Link to this heading">ÔÉÅ</a></h3>
<ul class="simple">
<li><p><strong>Antithetic variates</strong>: Essentially free‚Äîsame function evaluations, different organization</p></li>
<li><p><strong>Control variates</strong>: Requires evaluating <span class="math notranslate nohighlight">\(c(X)\)</span> and estimating <span class="math notranslate nohighlight">\(\beta\)</span>; overhead typically small</p></li>
<li><p><strong>Importance sampling</strong>: Requires evaluating two densities <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> per sample</p></li>
<li><p><strong>Stratified sampling</strong>: May require specialized samplers for conditional distributions</p></li>
<li><p><strong>CRN</strong>: Requires synchronization bookkeeping; minimal computational overhead</p></li>
</ul>
</section>
<section id="when-methods-fail">
<h3>When Methods Fail<a class="headerlink" href="#when-methods-fail" title="Link to this heading">ÔÉÅ</a></h3>
<ul class="simple">
<li><p><strong>Importance sampling</strong>: Weight degeneracy in high dimensions; proposal misspecified (too light tails)</p></li>
<li><p><strong>Control variates</strong>: Weak correlation; unknown control mean</p></li>
<li><p><strong>Antithetic variates</strong>: Non-monotonic integrands; high dimensions with mixed monotonicity</p></li>
<li><p><strong>Stratified sampling</strong>: Unknown stratum variances; intractable conditional sampling</p></li>
<li><p><strong>CRN</strong>: Systems respond oppositely to inputs; synchronization impossible</p></li>
</ul>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ‚ö†Ô∏è Silent Failures</p>
<p>Variance reduction methods can fail silently:</p>
<ul class="simple">
<li><p>Importance sampling with poor proposal yields valid but useless estimates (huge variance)</p></li>
<li><p>Antithetic variates with non-monotonic <span class="math notranslate nohighlight">\(h\)</span> may <em>increase</em> variance without warning</p></li>
<li><p>Control variates with wrong sign of <span class="math notranslate nohighlight">\(\beta\)</span> increase variance</p></li>
</ul>
<p><strong>Always compare with naive MC</strong> on a pilot run to verify improvement.</p>
</div>
</section>
</section>
<section id="bringing-it-all-together">
<h2>Bringing It All Together<a class="headerlink" href="#bringing-it-all-together" title="Link to this heading">ÔÉÅ</a></h2>
<p>Variance reduction transforms Monte Carlo from brute-force averaging into a sophisticated computational tool. The convergence rate <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> remains fixed, but the constant <span class="math notranslate nohighlight">\(\sigma^2\)</span> is ours to optimize.</p>
<p><strong>Importance sampling</strong> reweights samples to concentrate effort where the integrand matters, achieving orders-of-magnitude improvement for rare events‚Äîthough weight degeneracy in high dimensions demands careful monitoring via ESS diagnostics.</p>
<p><strong>Control variates</strong> exploit correlation with analytically tractable quantities, with variance reduction determined by <span class="math notranslate nohighlight">\(\rho^2\)</span>. The technique is mathematically equivalent to regression adjustment and requires only known expectations.</p>
<p><strong>Antithetic variates</strong> induce negative correlation through clever pairing, achieving near-perfect variance reduction for monotone functions at essentially zero cost.</p>
<p><strong>Stratified sampling</strong> eliminates between-stratum variance through domain partitioning, with Latin hypercube sampling extending the approach to high dimensions by ensuring marginal coverage.</p>
<p><strong>Common random numbers</strong> reduce comparison variance through synchronized inputs, transforming noisy system comparisons into precise difference estimation.</p>
<p>The techniques combine synergistically and appear throughout computational statistics: importance sampling underlies particle filters and SMC, control variates enhance MCMC estimators, and stratified sampling connects to quasi-Monte Carlo methods. Mastery of variance reduction‚Äîunderstanding when each method applies, recognizing limitations, implementing with numerical stability‚Äîdistinguishes the computational statistician from the naive simulator.</p>
<section id="looking-ahead">
<h3>Looking Ahead<a class="headerlink" href="#looking-ahead" title="Link to this heading">ÔÉÅ</a></h3>
<p>The variance reduction methods of this section assume we can sample directly from the target (or proposal) distribution. But what about distributions known only through an unnormalized density‚Äîposteriors in Bayesian inference, for instance? The rejection sampling of <a class="reference internal" href="ch2_5-rejection-sampling.html#ch2-5-rejection-sampling"><span class="std std-ref">Rejection Sampling</span></a> provides one answer, but its efficiency degrades rapidly in high dimensions.</p>
<p>The next part of this course develops <strong>Markov Chain Monte Carlo (MCMC)</strong>, which constructs a Markov chain whose stationary distribution equals the target. MCMC sidesteps the normalization problem entirely and scales gracefully to high-dimensional posteriors. The variance reduction principles developed here‚Äîparticularly importance sampling and control variates‚Äîwill reappear in the MCMC context, enabling efficient posterior estimation even for complex Bayesian models.</p>
<div class="important admonition">
<p class="admonition-title">Key Takeaways üìù</p>
<ol class="arabic simple">
<li><p><strong>Variance reduction does not change the rate</strong>: All methods achieve <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> convergence; they reduce the constant <span class="math notranslate nohighlight">\(\sigma^2\)</span>, not the exponent.</p></li>
<li><p><strong>Importance sampling</strong>: Optimal proposal <span class="math notranslate nohighlight">\(g^* \propto |h|f\)</span> achieves minimum variance. ESS diagnoses weight degeneracy. Use log-space arithmetic for stability.</p></li>
<li><p><strong>Control variates</strong>: Variance reduction equals <span class="math notranslate nohighlight">\(\rho^2\)</span> (squared correlation). Optimal <span class="math notranslate nohighlight">\(\beta^* = \text{Cov}(H,C)/\text{Var}(C)\)</span>. <strong>Must know</strong> <span class="math notranslate nohighlight">\(\mathbb{E}[C]\)</span> exactly.</p></li>
<li><p><strong>Antithetic variates</strong>: Work for monotone functions; can harm for non-monotone. The pair <span class="math notranslate nohighlight">\((F^{-1}(U), F^{-1}(1-U))\)</span> induces negative correlation for increasing <span class="math notranslate nohighlight">\(h\)</span>.</p></li>
<li><p><strong>Stratified sampling</strong>: Eliminates between-stratum variance. Neyman allocation <span class="math notranslate nohighlight">\(n_k \propto p_k \sigma_k\)</span> minimizes total variance. Latin hypercube extends to high dimensions.</p></li>
<li><p><strong>Common random numbers</strong>: Reduces variance of <em>comparisons</em>, not individual estimates. Requires synchronization‚Äîsame random input serves same purpose across systems.</p></li>
<li><p><strong>Outcome alignment</strong>: These techniques (Learning Outcomes 1, 3) enable efficient Monte Carlo estimation. Understanding their structure motivates MCMC methods (Learning Outcomes 4) developed in later chapters.</p></li>
</ol>
</div>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">ÔÉÅ</a></h2>
<div role="list" class="citation-list">
<div class="citation" id="kahn1951" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Kahn1951<span class="fn-bracket">]</span></span>
<p>Kahn, H. and Harris, T. E. (1951). Estimation of particle transmission by random sampling. <em>National Bureau of Standards Applied Mathematics Series</em>, 12, 27‚Äì30. Foundational paper on importance sampling from the RAND Corporation.</p>
</div>
<div class="citation" id="neyman1934" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Neyman1934<span class="fn-bracket">]</span></span>
<p>Neyman, J. (1934). On the two different aspects of the representative method: The method of stratified sampling and the method of purposive selection. <em>Journal of the Royal Statistical Society</em>, 97(4), 558‚Äì625. Establishes optimal allocation theory for stratified sampling.</p>
</div>
<div class="citation" id="hammersleymorton1956" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HammersleyMorton1956<span class="fn-bracket">]</span></span>
<p>Hammersley, J. M. and Morton, K. W. (1956). A new Monte Carlo technique: antithetic variates. <em>Mathematical Proceedings of the Cambridge Philosophical Society</em>, 52(3), 449‚Äì475. Introduces antithetic variates.</p>
</div>
<div class="citation" id="hammersleyhandscomb1964" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HammersleyHandscomb1964<span class="fn-bracket">]</span></span>
<p>Hammersley, J. M. and Handscomb, D. C. (1964). <em>Monte Carlo Methods</em>. Methuen, London. Classic monograph systematizing variance reduction techniques including control variates.</p>
</div>
<div class="citation" id="mckayetal1979" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>McKayEtAl1979<span class="fn-bracket">]</span></span>
<p>McKay, M. D., Beckman, R. J., and Conover, W. J. (1979). A comparison of three methods for selecting values of input variables in the analysis of output from a computer code. <em>Technometrics</em>, 21(2), 239‚Äì245. Introduces Latin Hypercube Sampling.</p>
</div>
<div class="citation" id="kong1992" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Kong1992<span class="fn-bracket">]</span></span>
<p>Kong, A. (1992). A note on importance sampling using standardized weights. Technical Report 348, Department of Statistics, University of Chicago. Establishes ESS interpretation for importance sampling.</p>
</div>
<div class="citation" id="owen1997" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Owen1997<span class="fn-bracket">]</span></span>
<p>Owen, A. B. (1997). Monte Carlo variance of scrambled net quadrature. <em>SIAM Journal on Numerical Analysis</em>, 34(5), 1884‚Äì1910. Theoretical analysis of Latin Hypercube Sampling.</p>
</div>
<div class="citation" id="bengtssonetal2008" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>BengtssonEtAl2008<span class="fn-bracket">]</span></span>
<p>Bengtsson, T., Bickel, P., and Li, B. (2008). Curse-of-dimensionality revisited: Collapse of the particle filter in very large scale systems. In <em>Probability and Statistics: Essays in Honor of David A. Freedman</em>, 316‚Äì334. Institute of Mathematical Statistics. Proves exponential sample size requirements for importance sampling in high dimensions.</p>
</div>
<div class="citation" id="robertcasella2004" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>RobertCasella2004<span class="fn-bracket">]</span></span>
<p>Robert, C. P. and Casella, G. (2004). <em>Monte Carlo Statistical Methods</em> (2nd ed.). Springer-Verlag, New York. Comprehensive treatment of variance reduction in the Monte Carlo context.</p>
</div>
<div class="citation" id="glasserman2004" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Glasserman2004<span class="fn-bracket">]</span></span>
<p>Glasserman, P. (2004). <em>Monte Carlo Methods in Financial Engineering</em>. Springer-Verlag, New York. Variance reduction applications in financial computing.</p>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>