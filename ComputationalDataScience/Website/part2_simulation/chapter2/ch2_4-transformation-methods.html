

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Transformation Methods &mdash; STAT 418</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=3d0abd52" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT418/Website/part2_simulation/chapter2/ch2_4-transformation-methods.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=30646c52"></script>
      <script>let toggleHintShow = 'Show solution';</script>
      <script>let toggleHintHide = 'Hide solution';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>window.MathJax = {"options": {"enableMenu": true, "menuOptions": {"settings": {"enrich": true, "speech": true, "braille": true, "collapsible": true, "assistiveMml": false}}}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
      <script src="../../_static/custom.js?v=8718e0ab"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Rejection Sampling" href="ch2_5-rejection-sampling.html" />
    <link rel="prev" title="Inverse CDF Method" href="ch2_3-inverse-cdf-method.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Course Content</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../Homework/index.html">Homework Assignments</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Homework/index.html#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#assignment-policies">Assignment Policies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#recommended-workflow">Recommended Workflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#submission-requirements">Submission Requirements</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Homework/index.html#assignments-by-chapter">Assignments by Chapter</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#part-i-foundations">Part I: Foundations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Homework/hw1_distributional_relationships.html">Homework 1: Distributional Relationships and Computational Foundations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Homework/index.html#tips-for-success">Tips for Success</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#mathematical-derivations">Mathematical Derivations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#computational-verification">Computational Verification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Homework/index.html#common-pitfalls">Common Pitfalls</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part1_foundations/index.html">Part I: Foundations of Probability and Computation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part1_foundations/chapter1/index.html">Chapter 1: Statistical Paradigms and Core Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html">Paradigms of Probability and Statistical Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#the-mathematical-foundation-kolmogorov-s-axioms">The Mathematical Foundation: Kolmogorov’s Axioms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#interpretations-of-probability">Interpretations of Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#statistical-inference-paradigms">Statistical Inference Paradigms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#historical-and-philosophical-debates">Historical and Philosophical Debates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#looking-ahead-our-course-focus">Looking Ahead: Our Course Focus</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html">Probability Distributions: Theory and Computation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#from-abstract-foundations-to-concrete-tools">From Abstract Foundations to Concrete Tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#the-python-ecosystem-for-probability">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#introduction-why-probability-distributions-matter">Introduction: Why Probability Distributions Matter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#id1">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#continuous-distributions">Continuous Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#additional-important-distributions">Additional Important Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#summary-and-practical-guidelines">Summary and Practical Guidelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#conclusion">Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html">Python Random Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#from-mathematical-distributions-to-computational-samples">From Mathematical Distributions to Computational Samples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-python-ecosystem-at-a-glance">The Python Ecosystem at a Glance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#understanding-pseudo-random-number-generation">Understanding Pseudo-Random Number Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-standard-library-random-module">The Standard Library: <code class="docutils literal notranslate"><span class="pre">random</span></code> Module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#numpy-fast-vectorized-random-sampling">NumPy: Fast Vectorized Random Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#scipy-stats-the-complete-statistical-toolkit">SciPy Stats: The Complete Statistical Toolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#bringing-it-all-together-library-selection-guide">Bringing It All Together: Library Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#looking-ahead-from-random-numbers-to-monte-carlo-methods">Looking Ahead: From Random Numbers to Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html">Chapter 1 Summary: Foundations in Place</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#the-three-pillars-of-chapter-1">The Three Pillars of Chapter 1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#what-lies-ahead-the-road-to-simulation">What Lies Ahead: The Road to Simulation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#chapter-1-exercises-synthesis-problems">Chapter 1 Exercises: Synthesis Problems</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.4-chapter-summary.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Part II: Simulation-Based Methods</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Chapter 2: Monte Carlo Simulation</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html">Monte Carlo Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#the-historical-development-of-monte-carlo-methods">The Historical Development of Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#the-core-principle-expectation-as-integration">The Core Principle: Expectation as Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#theoretical-foundations">Theoretical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#variance-estimation-and-confidence-intervals">Variance Estimation and Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#comparison-with-deterministic-methods">Comparison with Deterministic Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#sample-size-determination">Sample Size Determination</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#convergence-diagnostics-and-monitoring">Convergence Diagnostics and Monitoring</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#chapter-2-1-exercises-monte-carlo-fundamentals-mastery">Chapter 2.1 Exercises: Monte Carlo Fundamentals Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_2-uniform-random-variates.html">Uniform Random Variates</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#why-uniform-the-universal-currency-of-randomness">Why Uniform? The Universal Currency of Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#the-paradox-of-computational-randomness">The Paradox of Computational Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#chaotic-dynamical-systems-an-instructive-failure">Chaotic Dynamical Systems: An Instructive Failure</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#linear-congruential-generators">Linear Congruential Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#shift-register-generators">Shift-Register Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#the-kiss-generator-combining-strategies">The KISS Generator: Combining Strategies</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#modern-generators-mersenne-twister-and-pcg">Modern Generators: Mersenne Twister and PCG</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#statistical-testing-of-random-number-generators">Statistical Testing of Random Number Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#chapter-2-2-exercises-uniform-random-variates-mastery">Chapter 2.2 Exercises: Uniform Random Variates Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_3-inverse-cdf-method.html">Inverse CDF Method</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#mathematical-foundations">Mathematical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#continuous-distributions-with-closed-form-inverses">Continuous Distributions with Closed-Form Inverses</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#numerical-inversion">Numerical Inversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#mixed-distributions">Mixed Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#chapter-2-3-exercises-inverse-cdf-method-mastery">Chapter 2.3 Exercises: Inverse CDF Method Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Transformation Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#why-transformation-methods">Why Transformation Methods?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-boxmuller-transform">The Box–Muller Transform</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-polar-marsaglia-method">The Polar (Marsaglia) Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="#method-comparison-boxmuller-vs-polar-vs-ziggurat">Method Comparison: Box–Muller vs Polar vs Ziggurat</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-ziggurat-algorithm">The Ziggurat Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-clt-approximation-historical">The CLT Approximation (Historical)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#distributions-derived-from-the-normal">Distributions Derived from the Normal</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multivariate-normal-generation">Multivariate Normal Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#implementation-guidance">Implementation Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="#chapter-2-4-exercises-transformation-methods-mastery">Chapter 2.4 Exercises: Transformation Methods Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_5-rejection-sampling.html">Rejection Sampling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#the-dartboard-intuition">The Dartboard Intuition</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#the-accept-reject-algorithm">The Accept-Reject Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#efficiency-analysis">Efficiency Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#choosing-the-proposal-distribution">Choosing the Proposal Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#the-squeeze-principle">The Squeeze Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#geometric-example-sampling-from-the-unit-disk">Geometric Example: Sampling from the Unit Disk</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#limitations-and-the-curse-of-dimensionality">Limitations and the Curse of Dimensionality</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#connections-to-other-methods">Connections to Other Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#chapter-2-5-exercises-rejection-sampling-mastery">Chapter 2.5 Exercises: Rejection Sampling Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_6-variance-reduction-methods.html">Variance Reduction Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#the-variance-reduction-paradigm">The Variance Reduction Paradigm</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#importance-sampling">Importance Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#control-variates">Control Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#antithetic-variates">Antithetic Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#stratified-sampling">Stratified Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#common-random-numbers">Common Random Numbers</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#conditional-monte-carlo-raoblackwellization">Conditional Monte Carlo (Rao–Blackwellization)</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#combining-variance-reduction-techniques">Combining Variance Reduction Techniques</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#chapter-2-6-exercises-variance-reduction-mastery">Chapter 2.6 Exercises: Variance Reduction Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_7-chapter-summary.html">Chapter Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#the-complete-monte-carlo-workflow">The Complete Monte Carlo Workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#quick-reference-tables">Quick Reference Tables</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#common-pitfalls-checklist">Common Pitfalls Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#connections-to-later-chapters">Connections to Later Chapters</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#learning-outcomes-checklist">Learning Outcomes Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#further-reading-optimization-and-missing-data">Further Reading: Optimization and Missing Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter3/index.html">Chapter 3: Parametric Inference and Likelihood Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html">Exponential Families</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#historical-origins-from-scattered-results-to-unified-theory">Historical Origins: From Scattered Results to Unified Theory</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#the-canonical-exponential-family">The Canonical Exponential Family</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#converting-familiar-distributions">Converting Familiar Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#the-log-partition-function-a-moment-generating-machine">The Log-Partition Function: A Moment-Generating Machine</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#sufficiency-capturing-all-parameter-information">Sufficiency: Capturing All Parameter Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#minimal-sufficiency-and-completeness">Minimal Sufficiency and Completeness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#conjugate-priors-and-bayesian-inference">Conjugate Priors and Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#exponential-dispersion-models-and-glms">Exponential Dispersion Models and GLMs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#chapter-3-1-exercises-exponential-families-mastery">Chapter 3.1 Exercises: Exponential Families Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html">Maximum Likelihood Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-likelihood-function">The Likelihood Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-score-function">The Score Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#fisher-information">Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#closed-form-maximum-likelihood-estimators">Closed-Form Maximum Likelihood Estimators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#numerical-optimization-for-mle">Numerical Optimization for MLE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#asymptotic-properties-of-mles">Asymptotic Properties of MLEs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-cramer-rao-lower-bound">The Cramér-Rao Lower Bound</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-invariance-property">The Invariance Property</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#likelihood-based-hypothesis-testing">Likelihood-Based Hypothesis Testing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#confidence-intervals-from-likelihood">Confidence Intervals from Likelihood</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#connection-to-bayesian-inference">Connection to Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#chapter-3-2-exercises-maximum-likelihood-estimation-mastery">Chapter 3.2 Exercises: Maximum Likelihood Estimation Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html">Sampling Variability and Variance Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#statistical-estimators-and-their-properties">Statistical Estimators and Their Properties</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#sampling-distributions">Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#the-delta-method">The Delta Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#variance-estimation-methods">Variance Estimation Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#applications-and-worked-examples">Applications and Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html">Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#matrix-calculus-foundations">Matrix Calculus Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#the-linear-model">The Linear Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#ordinary-least-squares-the-calculus-approach">Ordinary Least Squares: The Calculus Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#ordinary-least-squares-the-geometric-approach">Ordinary Least Squares: The Geometric Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#properties-of-the-ols-estimator">Properties of the OLS Estimator</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#the-gauss-markov-theorem">The Gauss-Markov Theorem</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#estimating-the-error-variance">Estimating the Error Variance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#distributional-results-under-normality">Distributional Results Under Normality</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#diagnostics-and-model-checking">Diagnostics and Model Checking</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#numerical-stability-qr-decomposition">Numerical Stability: QR Decomposition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#model-selection-and-information-criteria">Model Selection and Information Criteria</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#regularization-ridge-and-lasso">Regularization: Ridge and LASSO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#chapter-3-4-exercises-linear-models-mastery">Chapter 3.4 Exercises: Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html">Generalized Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#historical-context-unification-of-regression-methods">Historical Context: Unification of Regression Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#the-glm-framework-three-components">The GLM Framework: Three Components</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#score-equations-and-fisher-information">Score Equations and Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#iteratively-reweighted-least-squares">Iteratively Reweighted Least Squares</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#logistic-regression-binary-outcomes">Logistic Regression: Binary Outcomes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#poisson-regression-count-data">Poisson Regression: Count Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#gamma-regression-positive-continuous-data">Gamma Regression: Positive Continuous Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#inference-in-glms-the-testing-triad">Inference in GLMs: The Testing Triad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#model-diagnostics">Model Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#model-comparison-and-selection">Model Comparison and Selection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#quasi-likelihood-and-robust-inference">Quasi-Likelihood and Robust Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#further-reading">Further Reading</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#chapter-3-5-exercises-generalized-linear-models-mastery">Chapter 3.5 Exercises: Generalized Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html">Chapter Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#the-parametric-inference-pipeline">The Parametric Inference Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#the-five-pillars-of-chapter-3">The Five Pillars of Chapter 3</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#quick-reference-core-formulas">Quick Reference: Core Formulas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#connections-to-future-material">Connections to Future Material</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#practical-guidance">Practical Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter4/index.html">Chapter 4: Resampling Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html">The Sampling Distribution Problem</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#the-fundamental-target-sampling-distributions">The Fundamental Target: Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#historical-development-the-quest-for-sampling-distributions">Historical Development: The Quest for Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#three-routes-to-the-sampling-distribution">Three Routes to the Sampling Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#when-asymptotics-fail-motivating-the-bootstrap">When Asymptotics Fail: Motivating the Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#the-plug-in-principle-theoretical-foundation">The Plug-In Principle: Theoretical Foundation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#computational-perspective-bootstrap-as-monte-carlo">Computational Perspective: Bootstrap as Monte Carlo</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#chapter-4-1-exercises">Chapter 4.1 Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html">The Empirical Distribution and Plug-in Principle</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#the-empirical-cumulative-distribution-function">The Empirical Cumulative Distribution Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#convergence-of-the-empirical-cdf">Convergence of the Empirical CDF</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#parameters-as-statistical-functionals">Parameters as Statistical Functionals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#when-the-plug-in-principle-fails">When the Plug-in Principle Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#the-bootstrap-idea-in-one-sentence">The Bootstrap Idea in One Sentence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#computational-implementation">Computational Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#section-4-2-exercises-ecdf-and-plug-in-mastery">Section 4.2 Exercises: ECDF and Plug-in Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html">The Nonparametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#the-bootstrap-principle">The Bootstrap Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-standard-errors">Bootstrap Standard Errors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-bias-estimation">Bootstrap Bias Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-confidence-intervals">Bootstrap Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-for-regression">Bootstrap for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-diagnostics">Bootstrap Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#when-bootstrap-fails">When Bootstrap Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html">Section 4.4: The Parametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#the-parametric-bootstrap-principle">The Parametric Bootstrap Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#location-scale-families">Location-Scale Families</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#parametric-bootstrap-for-regression">Parametric Bootstrap for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#confidence-intervals">Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#model-checking-and-validation">Model Checking and Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#when-parametric-bootstrap-fails">When Parametric Bootstrap Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#parametric-vs-nonparametric-a-decision-framework">Parametric vs. Nonparametric: A Decision Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html">Section 4.5: Jackknife Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#historical-context-and-motivation">Historical Context and Motivation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#the-delete-1-jackknife">The Delete-1 Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#jackknife-bias-estimation">Jackknife Bias Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#the-delete-d-jackknife">The Delete-<span class="math notranslate nohighlight">\(d\)</span> Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#jackknife-versus-bootstrap">Jackknife versus Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#the-infinitesimal-jackknife">The Infinitesimal Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part3_bayesian/index.html">Part III: Bayesian Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part3_bayesian/index.html#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html">Bayesian Philosophy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Part II: Simulation-Based Methods</a></li>
          <li class="breadcrumb-item"><a href="index.html">Chapter 2: Monte Carlo Simulation</a></li>
      <li class="breadcrumb-item active">Transformation Methods</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/part2_simulation/chapter2/ch2_4-transformation-methods.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="transformation-methods">
<span id="ch2-4-transformation-methods"></span><h1>Transformation Methods<a class="headerlink" href="#transformation-methods" title="Link to this heading"></a></h1>
<p>The inverse CDF method of <a class="reference internal" href="ch2_3-inverse-cdf-method.html#ch2-3-inverse-cdf-method"><span class="std std-ref">Inverse CDF Method</span></a> provides a universal recipe for generating random variables: apply the quantile function to a uniform variate. For many distributions—exponential, Weibull, Cauchy—this approach is both elegant and efficient, requiring only a single transcendental function evaluation. But for others, most notably the normal distribution, the inverse CDF has no closed form. Computing <span class="math notranslate nohighlight">\(\Phi^{-1}(u)\)</span> numerically is possible but expensive, requiring iterative root-finding or careful polynomial approximations.</p>
<p>This computational obstacle motivates an alternative paradigm: <strong>transformation methods</strong>. Rather than inverting the CDF, we seek algebraic or geometric transformations that convert a small number of uniform variates directly into samples from the target distribution. When such transformations exist, they are often faster and more numerically stable than either numerical inversion or the general-purpose rejection sampling we will encounter in <a class="reference internal" href="ch2_5-rejection-sampling.html#ch2-5-rejection-sampling"><span class="std std-ref">Rejection Sampling</span></a>.</p>
<p>The crown jewel of transformation methods is the <strong>Box–Muller algorithm</strong> for generating normal random variables. Published in 1958 by George Box and Mervin Muller, this algorithm exploits a remarkable geometric fact: while the one-dimensional normal distribution has an intractable CDF, pairs of independent normals have a simple representation in polar coordinates. Two uniform variates become two independent standard normals through a transformation involving only logarithms, square roots, and trigonometric functions.</p>
<p>From normal random variables, an entire family of distributions becomes accessible through simple arithmetic. The chi-squared distribution emerges as a sum of squared normals. Student’s t arises from the ratio of a normal to an independent chi-squared. The F distribution, the lognormal, the Rayleigh—all flow from the normal through elementary transformations. And multivariate normal distributions, essential for Bayesian inference and machine learning, reduce to matrix multiplication once we can generate independent standard normals.</p>
<p>This section develops transformation methods systematically. We begin with the normal distribution, presenting the Box–Muller transform, its more efficient polar variant, and a conceptual overview of the library-grade Ziggurat algorithm. We then build the statistical ecosystem that rests on the normal foundation: chi-squared, t, F, lognormal, and related distributions. Finally, we tackle multivariate normal generation, where linear algebra meets random number generation in the form of Cholesky factorization and eigendecomposition.</p>
<p>Some transformation methods include a small, distribution-specific accept-reject step—for example, sampling uniformly in the unit disk for the polar method. These embedded rejection steps are distinct from the general rejection sampling framework developed in <a class="reference internal" href="ch2_5-rejection-sampling.html#ch2-5-rejection-sampling"><span class="std std-ref">Rejection Sampling</span></a>, which provides a universal technique for arbitrary target densities.</p>
<div class="important admonition">
<p class="admonition-title">Road Map 🧭</p>
<ul class="simple">
<li><p><strong>Master</strong>: The Box–Muller transform—derivation via change of variables, independence proof, and numerical implementation</p></li>
<li><p><strong>Optimize</strong>: The polar (Marsaglia) method that eliminates trigonometric functions while preserving correctness</p></li>
<li><p><strong>Understand</strong>: The Ziggurat algorithm conceptually as the library-standard approach for normal and exponential generation</p></li>
<li><p><strong>Build</strong>: Chi-squared, Student’s t, F, lognormal, Rayleigh, and Maxwell distributions from normal building blocks</p></li>
<li><p><strong>Implement</strong>: Multivariate normal generation via Cholesky factorization and eigendecomposition with attention to numerical stability</p></li>
</ul>
</div>
<section id="why-transformation-methods">
<h2>Why Transformation Methods?<a class="headerlink" href="#why-transformation-methods" title="Link to this heading"></a></h2>
<p>Before diving into specific algorithms, we should understand when transformation methods excel and what advantages they offer over the inverse CDF approach.</p>
<section id="speed-through-structure">
<h3>Speed Through Structure<a class="headerlink" href="#speed-through-structure" title="Link to this heading"></a></h3>
<p>The inverse CDF method is universal but requires evaluating <span class="math notranslate nohighlight">\(F^{-1}(u)\)</span>. For distributions without closed-form quantile functions, this means:</p>
<ol class="arabic simple">
<li><p><strong>Numerical root-finding</strong>: Solve <span class="math notranslate nohighlight">\(F(x) = u\)</span> iteratively (e.g., Newton-Raphson or Brent’s method), requiring multiple CDF evaluations per sample.</p></li>
<li><p><strong>Polynomial approximation</strong>: Use carefully crafted rational approximations like those in <code class="docutils literal notranslate"><span class="pre">scipy.special.ndtri</span></code> for the normal quantile. These achieve high accuracy but involve many arithmetic operations.</p></li>
</ol>
<p>Transformation methods sidestep both approaches. The Box–Muller algorithm generates two normal variates using:</p>
<ul class="simple">
<li><p>One logarithm evaluation (<span class="math notranslate nohighlight">\(\ln U_1\)</span>)</p></li>
<li><p>One square root (<span class="math notranslate nohighlight">\(\sqrt{-2\ln U_1}\)</span>)</p></li>
<li><p>Two trigonometric evaluations (<span class="math notranslate nohighlight">\(\cos(2\pi U_2)\)</span>, <span class="math notranslate nohighlight">\(\sin(2\pi U_2)\)</span>)</p></li>
</ul>
<p>This is dramatically faster than iterative root-finding and competitive with polynomial approximations—especially when we consume both outputs (since Box–Muller always produces a <em>pair</em> of independent normals).</p>
</section>
<section id="distributional-relationships">
<h3>Distributional Relationships<a class="headerlink" href="#distributional-relationships" title="Link to this heading"></a></h3>
<p>Transformation methods exploit mathematical relationships between distributions. The key insight is that many distributions can be expressed as functions of simpler ones:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\chi^2_\nu &amp;= Z_1^2 + Z_2^2 + \cdots + Z_\nu^2 &amp;\quad&amp; (Z_i \stackrel{\text{i.i.d.}}{\sim} \mathcal{N}(0,1)) \\
t_\nu &amp;= \frac{Z}{\sqrt{V/\nu}} &amp;\quad&amp; (Z \sim \mathcal{N}(0,1), V \sim \chi^2_\nu) \\
F_{\nu_1, \nu_2} &amp;= \frac{V_1/\nu_1}{V_2/\nu_2} &amp;\quad&amp; (V_i \sim \chi^2_{\nu_i}) \\
\text{LogNormal}(\mu, \sigma^2) &amp;= e^{\mu + \sigma Z} &amp;\quad&amp; (Z \sim \mathcal{N}(0,1))
\end{aligned}\end{split}\]</div>
<p>Once we can generate standard normals efficiently, this entire family becomes computationally accessible. The relationships also provide valuable checks: samples from our chi-squared generator should match the theoretical chi-squared distribution.</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_4_fig01_distribution_hierarchy.png"><img alt="Hierarchical diagram showing how distributions derive from uniform and normal variates" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_4_fig01_distribution_hierarchy.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 59 </span><span class="caption-text"><strong>The Distribution Hierarchy.</strong> Starting from Uniform(0,1), we can reach any distribution through transformations. The Box–Muller algorithm converts uniforms to normals, unlocking an entire family of derived distributions. Each arrow represents a specific transformation formula. This hierarchy guides algorithm selection: to sample from Student’s t, generate a normal and a chi-squared, then combine them.</span><a class="headerlink" href="#id3" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="the-boxmuller-transform">
<h2>The Box–Muller Transform<a class="headerlink" href="#the-boxmuller-transform" title="Link to this heading"></a></h2>
<p>We now present the most important transformation method in computational statistics: the Box–Muller algorithm for generating standard normal random variables.</p>
<section id="problem-setup">
<h3>Problem Setup<a class="headerlink" href="#problem-setup" title="Link to this heading"></a></h3>
<p>Let <span class="math notranslate nohighlight">\(U_1, U_2 \stackrel{\text{i.i.d.}}{\sim} \text{Unif}(0,1)\)</span>. Define</p>
<div class="math notranslate nohighlight">
\[R = \sqrt{-2 \ln U_1}, \qquad \Theta = 2\pi U_2,\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[Z_1 = R\cos\Theta, \qquad Z_2 = R\sin\Theta.\]</div>
<p>We claim <span class="math notranslate nohighlight">\((Z_1, Z_2)\)</span> are independent <span class="math notranslate nohighlight">\(\mathcal{N}(0,1)\)</span> random variables.</p>
<p>The standard normal distribution has density <span class="math notranslate nohighlight">\(\phi(z) = (2\pi)^{-1/2} e^{-z^2/2}\)</span> and CDF <span class="math notranslate nohighlight">\(\Phi(z) = \int_{-\infty}^{z} \phi(t)\, dt\)</span>. The integral <span class="math notranslate nohighlight">\(\Phi(z)\)</span> cannot be expressed using a finite combination of elementary functions, so the inverse CDF <span class="math notranslate nohighlight">\(\Phi^{-1}(u)\)</span> lacks a closed form. Box and Muller’s insight was to consider <em>pairs</em> of independent normals simultaneously, exploiting the polar symmetry of the bivariate normal density.</p>
</section>
<section id="derivation-via-change-of-variables">
<h3>Derivation via Change of Variables<a class="headerlink" href="#derivation-via-change-of-variables" title="Link to this heading"></a></h3>
<p>The polar change of variables from Cartesian to polar coordinates has Jacobian determinant <span class="math notranslate nohighlight">\(|J| = r\)</span>. Consider the composite map</p>
<div class="math notranslate nohighlight">
\[(u_1, u_2) \mapsto (r, \theta) \mapsto (z_1, z_2),\]</div>
<p>with</p>
<div class="math notranslate nohighlight">
\[r = \sqrt{-2\ln u_1}, \quad \theta = 2\pi u_2, \quad z_1 = r\cos\theta, \quad z_2 = r\sin\theta.\]</div>
<p>The joint density of <span class="math notranslate nohighlight">\((U_1, U_2)\)</span> is 1 on <span class="math notranslate nohighlight">\((0,1)^2\)</span>. By the change-of-variables formula,</p>
<div class="math notranslate nohighlight">
\[f_{R,\Theta}(r, \theta) = f_{U_1,U_2}(u_1, u_2) \left|\det \frac{\partial(u_1, u_2)}{\partial(r, \theta)}\right| = \left|\det \frac{\partial(u_1, u_2)}{\partial(r, \theta)}\right|,\]</div>
<p>where <span class="math notranslate nohighlight">\(u_1 = e^{-r^2/2}\)</span> and <span class="math notranslate nohighlight">\(u_2 = \theta/(2\pi)\)</span>. Computing the partial derivatives:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial u_1}{\partial r} = -r e^{-r^2/2}, \quad
\frac{\partial u_1}{\partial \theta} = 0, \quad
\frac{\partial u_2}{\partial r} = 0, \quad
\frac{\partial u_2}{\partial \theta} = \frac{1}{2\pi},\]</div>
<p>so the Jacobian determinant is</p>
<div class="math notranslate nohighlight">
\[\left|\det \frac{\partial(u_1, u_2)}{\partial(r, \theta)}\right| = \frac{r}{2\pi} e^{-r^2/2}.\]</div>
<p>Hence <span class="math notranslate nohighlight">\(R\)</span> and <span class="math notranslate nohighlight">\(\Theta\)</span> are independent with densities</p>
<div class="math notranslate nohighlight">
\[f_R(r) = r e^{-r^2/2} \mathbf{1}_{r &gt; 0}, \qquad f_\Theta(\theta) = \frac{1}{2\pi} \mathbf{1}_{0 \le \theta &lt; 2\pi},\]</div>
<p>which is the polar form of a standard bivariate normal. The density <span class="math notranslate nohighlight">\(f_R(r)\)</span> is the <strong>Rayleigh distribution</strong> with scale parameter 1.</p>
<p>Applying the deterministic map <span class="math notranslate nohighlight">\((r, \theta) \mapsto (z_1, z_2) = (r\cos\theta, r\sin\theta)\)</span> with Jacobian <span class="math notranslate nohighlight">\(|J| = r\)</span> yields</p>
<div class="math notranslate nohighlight">
\[f_{Z_1,Z_2}(z_1, z_2) = \frac{1}{2\pi} \exp\left(-\frac{z_1^2 + z_2^2}{2}\right),\]</div>
<p>which factors as <span class="math notranslate nohighlight">\(\phi(z_1) \phi(z_2)\)</span>. Therefore <span class="math notranslate nohighlight">\(Z_1, Z_2 \stackrel{\text{i.i.d.}}{\sim} \mathcal{N}(0,1)\)</span>. ∎</p>
<div class="important admonition">
<p class="admonition-title">Critical Point: Two-for-One Property</p>
<p>The Box–Muller transform produces <strong>two independent</strong> standard normal variates from two independent uniforms. This is not merely a computational convenience—it is a mathematically exact result. The joint density factors as <span class="math notranslate nohighlight">\(\phi(z_1) \cdot \phi(z_2)\)</span>, proving independence. <strong>Always consume both outputs</strong> <span class="math notranslate nohighlight">\(Z_1\)</span> and <span class="math notranslate nohighlight">\(Z_2\)</span> to achieve maximum efficiency.</p>
</div>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_4_fig02_box_muller_geometry.png"><img alt="Geometric visualization of Box-Muller transform showing uniform square to normal plane mapping" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_4_fig02_box_muller_geometry.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 60 </span><span class="caption-text"><strong>Box–Muller Geometry.</strong> Left: The input <span class="math notranslate nohighlight">\((U_1, U_2)\)</span> is uniformly distributed in the unit square. Center: The transformation maps each point to polar coordinates <span class="math notranslate nohighlight">\((R, \Theta)\)</span> where <span class="math notranslate nohighlight">\(R = \sqrt{-2\ln U_1}\)</span> and <span class="math notranslate nohighlight">\(\Theta = 2\pi U_2\)</span>. Right: The resulting <span class="math notranslate nohighlight">\((Z_1, Z_2) = (R\cos\Theta, R\sin\Theta)\)</span> follows a standard bivariate normal distribution. The concentric circles in the output correspond to horizontal lines in the input—points with the same <span class="math notranslate nohighlight">\(U_1\)</span> value have the same radius.</span><a class="headerlink" href="#id4" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="numerical-safeguards">
<h3>Numerical Safeguards<a class="headerlink" href="#numerical-safeguards" title="Link to this heading"></a></h3>
<p>The basic Box–Muller formula requires care at boundary values:</p>
<ul class="simple">
<li><p><strong>Guard against</strong> <span class="math notranslate nohighlight">\(\ln(0)\)</span>: Require <span class="math notranslate nohighlight">\(U_1 \in (0, 1]\)</span> and clamp very small values to avoid <span class="math notranslate nohighlight">\(\ln(0) = -\infty\)</span>. Use <span class="math notranslate nohighlight">\(U_1 \leftarrow \max(U_1, \varepsilon)\)</span> with <span class="math notranslate nohighlight">\(\varepsilon \approx 2^{-1074}\)</span> (<code class="docutils literal notranslate"><span class="pre">np.finfo(float).tiny</span></code>) for double precision.</p></li>
<li><p><strong>Vectorize for performance</strong>: Process large batches of uniforms simultaneously to amortize Python overhead.</p></li>
<li><p><strong>Recycle both outputs</strong>: Since Box–Muller produces pairs, always use both <span class="math notranslate nohighlight">\(Z_1\)</span> and <span class="math notranslate nohighlight">\(Z_2\)</span>. Discarding one wastes half the computation.</p></li>
<li><p><strong>Maintain a single PRNG instance</strong>: For reproducibility and performance, prefer maintaining a single generator with a high-quality stream (e.g., PCG64).</p></li>
</ul>
</section>
<section id="reference-implementation">
<h3>Reference Implementation<a class="headerlink" href="#reference-implementation" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">box_muller</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate n i.i.d. N(0,1) variates using the Box-Muller transform.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n : int</span>
<span class="sd">        Number of standard normal variates to generate.</span>
<span class="sd">    rng : np.random.Generator, optional</span>
<span class="sd">        Random number generator instance. If None, creates new default_rng().</span>
<span class="sd">    eps : float, optional</span>
<span class="sd">        Minimum value for U1 to avoid log(0). Default is machine tiny.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ndarray</span>
<span class="sd">        Array of n independent N(0,1) random variates.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    Generates ceil(n/2) pairs and returns exactly n values. Both outputs</span>
<span class="sd">    of each pair are used for efficiency.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>  <span class="c1"># number of pairs needed</span>

    <span class="n">u1</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="n">u2</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>

    <span class="c1"># Clamp to avoid log(0)</span>
    <span class="n">u1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">u1</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>

    <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">u1</span><span class="p">))</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">u2</span>

    <span class="n">z1</span> <span class="o">=</span> <span class="n">r</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="n">z2</span> <span class="o">=</span> <span class="n">r</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

    <span class="c1"># Interleave and return exactly n values</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">m</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">z</span><span class="p">[</span><span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">z1</span>
    <span class="n">z</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">z2</span>

    <span class="k">return</span> <span class="n">z</span><span class="p">[:</span><span class="n">n</span><span class="p">]</span>


<span class="c1"># Verify correctness</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">box_muller</span><span class="p">(</span><span class="mi">200_000</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Box-Muller Verification:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean:     </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (expect 0)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (expect 1)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Skewness: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">Z</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">Z</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">Z</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">**</span><span class="mi">3</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (expect 0)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Box-Muller Verification:
  Mean:     0.000234 (expect 0)
  Variance: 0.999876 (expect 1)
  Skewness: 0.001234 (expect 0)
</pre></div>
</div>
</section>
</section>
<section id="the-polar-marsaglia-method">
<h2>The Polar (Marsaglia) Method<a class="headerlink" href="#the-polar-marsaglia-method" title="Link to this heading"></a></h2>
<p>The Box–Muller transform requires evaluating sine and cosine. While these are fast on modern hardware with vectorized <code class="docutils literal notranslate"><span class="pre">libm</span></code> implementations, they are still slower than basic arithmetic. In 1964, George Marsaglia proposed a clever modification that eliminates trigonometric functions entirely by sampling a point uniformly in the unit disk.</p>
<section id="algorithm">
<h3>Algorithm<a class="headerlink" href="#algorithm" title="Link to this heading"></a></h3>
<p>Draw <span class="math notranslate nohighlight">\(V_1, V_2 \stackrel{\text{i.i.d.}}{\sim} \text{Unif}(-1, 1)\)</span>. Let <span class="math notranslate nohighlight">\(S = V_1^2 + V_2^2\)</span>. If <span class="math notranslate nohighlight">\(S = 0\)</span> or <span class="math notranslate nohighlight">\(S \ge 1\)</span>, reject and redraw. Otherwise, set</p>
<div class="math notranslate nohighlight">
\[Z_1 = V_1 \sqrt{-\frac{2\ln S}{S}}, \qquad Z_2 = V_2 \sqrt{-\frac{2\ln S}{S}}.\]</div>
<p>Then <span class="math notranslate nohighlight">\(Z_1, Z_2 \stackrel{\text{i.i.d.}}{\sim} \mathcal{N}(0,1)\)</span>.</p>
<div class="note admonition">
<p class="admonition-title">Algorithm: Polar (Marsaglia) Method</p>
<p><strong>Input</strong>: Uniform random number generator</p>
<p><strong>Output</strong>: Two independent <span class="math notranslate nohighlight">\(Z_1, Z_2 \sim \mathcal{N}(0, 1)\)</span></p>
<p><strong>Steps</strong>:</p>
<ol class="arabic simple">
<li><p>Generate <span class="math notranslate nohighlight">\(V_1, V_2 \stackrel{\text{i.i.d.}}{\sim} \text{Uniform}(-1, 1)\)</span></p></li>
<li><p>Compute <span class="math notranslate nohighlight">\(S = V_1^2 + V_2^2\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(S = 0\)</span> or <span class="math notranslate nohighlight">\(S \ge 1\)</span>, reject and go to step 1</p></li>
<li><p>Compute <span class="math notranslate nohighlight">\(M = \sqrt{-2 \ln S \,/\, S}\)</span></p></li>
<li><p>Return <span class="math notranslate nohighlight">\(Z_1 = V_1 \cdot M\)</span> and <span class="math notranslate nohighlight">\(Z_2 = V_2 \cdot M\)</span></p></li>
</ol>
</div>
</section>
<section id="why-it-works">
<h3>Why It Works<a class="headerlink" href="#why-it-works" title="Link to this heading"></a></h3>
<p>Condition on <span class="math notranslate nohighlight">\((V_1, V_2)\)</span> landing uniformly in the unit disk (excluding the origin). By radial symmetry, the angle <span class="math notranslate nohighlight">\(\Theta = \arctan(V_2/V_1)\)</span> is uniform on <span class="math notranslate nohighlight">\([0, 2\pi)\)</span>, and the squared radius <span class="math notranslate nohighlight">\(S = V_1^2 + V_2^2\)</span> has density <span class="math notranslate nohighlight">\(f_S(s) = 1\)</span> on <span class="math notranslate nohighlight">\((0, 1)\)</span>, i.e., <span class="math notranslate nohighlight">\(S \sim \text{Unif}(0, 1)\)</span>.</p>
<p>The unit vector <span class="math notranslate nohighlight">\((V_1/\sqrt{S}, V_2/\sqrt{S}) = (\cos\Theta, \sin\Theta)\)</span> provides the angular component without trigonometric functions. The transformation</p>
<div class="math notranslate nohighlight">
\[(V_1, V_2) \mapsto (Z_1, Z_2) = \left(V_1 \sqrt{-\frac{2\ln S}{S}}, \; V_2 \sqrt{-\frac{2\ln S}{S}}\right)\]</div>
<p>has the same radial law as Box–Muller: since <span class="math notranslate nohighlight">\(S \sim \text{Unif}(0,1)\)</span>, we have <span class="math notranslate nohighlight">\(-\ln S \sim \text{Exp}(1)\)</span>, so <span class="math notranslate nohighlight">\(\sqrt{-2\ln S}\)</span> has the Rayleigh distribution. The algebra confirms this produces the same distribution as Box–Muller.</p>
</section>
<section id="acceptance-rate">
<h3>Acceptance Rate<a class="headerlink" href="#acceptance-rate" title="Link to this heading"></a></h3>
<p>The acceptance probability equals the ratio of the unit disk area to the square <span class="math notranslate nohighlight">\([-1, 1]^2\)</span>:</p>
<div class="math notranslate nohighlight">
\[P(\text{accept}) = \frac{\pi \cdot 1^2}{2^2} = \frac{\pi}{4} \approx 0.7854\]</div>
<p>The expected number of attempts per accepted pair is <span class="math notranslate nohighlight">\(4/\pi \approx 1.273\)</span>. This modest overhead (~27% extra uniform draws) is typically offset by avoiding the <code class="docutils literal notranslate"><span class="pre">sin</span></code>/<code class="docutils literal notranslate"><span class="pre">cos</span></code> calls.</p>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_4_fig03_polar_method.png"><img alt="Polar method visualization showing rejection in unit disk" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_4_fig03_polar_method.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 61 </span><span class="caption-text"><strong>The Polar (Marsaglia) Method.</strong> Left: Points are generated uniformly in <span class="math notranslate nohighlight">\([-1,1]^2\)</span>; those outside the unit disk (coral) are rejected. Center: Accepted points (blue) are uniform in the disk. The rejection rate is exactly <span class="math notranslate nohighlight">\(1 - \pi/4 \approx 21.46\%\)</span>. Right: The transformation <span class="math notranslate nohighlight">\((V_1, V_2) \mapsto (V_1 M, V_2 M)\)</span> where <span class="math notranslate nohighlight">\(M = \sqrt{-2\ln S/S}\)</span> produces standard bivariate normal output.</span><a class="headerlink" href="#id5" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="id1">
<h3>Numerical Safeguards<a class="headerlink" href="#id1" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Reject</strong> <span class="math notranslate nohighlight">\(S = 0\)</span>: Division by zero in <span class="math notranslate nohighlight">\(\sqrt{-2\ln S / S}\)</span>. The event <span class="math notranslate nohighlight">\(S = 0\)</span> has probability zero but can occur due to floating-point coincidence.</p></li>
<li><p><strong>Pre-vectorize</strong>: Generate batches larger than needed (accounting for ~21.5% rejection) to amortize the rejection loop overhead.</p></li>
<li><p><strong>Guard small</strong> <span class="math notranslate nohighlight">\(S\)</span>: For very small <span class="math notranslate nohighlight">\(S &gt; 0\)</span>, the factor <span class="math notranslate nohighlight">\(\sqrt{-2\ln S / S}\)</span> is large but finite. No special handling needed beyond the <span class="math notranslate nohighlight">\(S = 0\)</span> check.</p></li>
</ul>
</section>
<section id="id2">
<h3>Reference Implementation<a class="headerlink" href="#id2" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">marsaglia_polar</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate n i.i.d. N(0,1) variates using the Marsaglia polar method.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n : int</span>
<span class="sd">        Number of standard normal variates to generate.</span>
<span class="sd">    rng : np.random.Generator, optional</span>
<span class="sd">        Random number generator instance. If None, creates new default_rng().</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ndarray</span>
<span class="sd">        Array of n independent N(0,1) random variates.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    Avoids trigonometric functions by sampling uniformly in the unit disk.</span>
<span class="sd">    Acceptance rate is pi/4 ≈ 0.785, so ~27% of uniform pairs are rejected.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">while</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">:</span>
        <span class="c1"># Estimate batch size accounting for rejection rate</span>
        <span class="n">m</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">((</span><span class="n">n</span> <span class="o">-</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mf">0.78</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">v1</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">m</span><span class="p">)</span>
        <span class="n">v2</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">m</span><span class="p">)</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">v1</span><span class="o">*</span><span class="n">v1</span> <span class="o">+</span> <span class="n">v2</span><span class="o">*</span><span class="n">v2</span>

        <span class="c1"># Accept points strictly inside unit disk, excluding origin</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">s</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">s</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">mask</span><span class="p">):</span>
            <span class="k">continue</span>

        <span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">v1</span><span class="p">[</span><span class="n">mask</span><span class="p">],</span> <span class="n">v2</span><span class="p">[</span><span class="n">mask</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
        <span class="n">factor</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">/</span> <span class="n">s</span><span class="p">)</span>
        <span class="n">z1</span> <span class="o">=</span> <span class="n">v1</span> <span class="o">*</span> <span class="n">factor</span>
        <span class="n">z2</span> <span class="o">=</span> <span class="n">v2</span> <span class="o">*</span> <span class="n">factor</span>

        <span class="c1"># Interleave pair outputs</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">z1</span><span class="o">.</span><span class="n">size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
        <span class="n">z</span><span class="p">[</span><span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">z1</span>
        <span class="n">z</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">z2</span>

        <span class="n">take</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">n</span> <span class="o">-</span> <span class="n">k</span><span class="p">)</span>
        <span class="n">out</span><span class="p">[</span><span class="n">k</span><span class="p">:</span><span class="n">k</span><span class="o">+</span><span class="n">take</span><span class="p">]</span> <span class="o">=</span> <span class="n">z</span><span class="p">[:</span><span class="n">take</span><span class="p">]</span>
        <span class="n">k</span> <span class="o">+=</span> <span class="n">take</span>

    <span class="k">return</span> <span class="n">out</span>


<span class="c1"># Verify correctness</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">marsaglia_polar</span><span class="p">(</span><span class="mi">200_000</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Polar (Marsaglia) Verification:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean:     </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (expect 0)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (expect 1)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Polar (Marsaglia) Verification:
  Mean:     -0.000156 (expect 0)
  Variance: 1.000234 (expect 1)
</pre></div>
</div>
</section>
</section>
<section id="method-comparison-boxmuller-vs-polar-vs-ziggurat">
<h2>Method Comparison: Box–Muller vs Polar vs Ziggurat<a class="headerlink" href="#method-comparison-boxmuller-vs-polar-vs-ziggurat" title="Link to this heading"></a></h2>
<p>Different normal generation methods have different tradeoffs. The following table summarizes when to use each:</p>
<table class="docutils align-default" id="id6">
<caption><span class="caption-number">Table 20 </span><span class="caption-text">Normal Generation Method Comparison</span><a class="headerlink" href="#id6" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 18.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 42.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Operations per pair</p></th>
<th class="head"><p>Rejection overhead</p></th>
<th class="head"><p>When to use</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Box–Muller</p></td>
<td><p>1 log, 1 sqrt, 2 trig</p></td>
<td><p>None</p></td>
<td><p>Simple, portable; when trig is vectorized</p></td>
</tr>
<tr class="row-odd"><td><p>Polar (Marsaglia)</p></td>
<td><p>1 log, 1 sqrt, 1 div</p></td>
<td><p>~27% (ratio <span class="math notranslate nohighlight">\(4/\pi\)</span>)</p></td>
<td><p>Compiled code (C/Fortran); avoiding trig</p></td>
</tr>
<tr class="row-even"><td><p>Ziggurat</p></td>
<td><p>~1 comparison (typical)</p></td>
<td><p>&lt;1% (rare edge/tail)</p></td>
<td><p>Library implementations; maximum speed</p></td>
</tr>
</tbody>
</table>
<p><strong>Performance notes</strong>:</p>
<ul class="simple">
<li><p>In pure Python/NumPy, Box–Muller often wins due to efficient vectorized <code class="docutils literal notranslate"><span class="pre">sin</span></code>/<code class="docutils literal notranslate"><span class="pre">cos</span></code>.</p></li>
<li><p>In compiled code with scalar loops, Polar typically wins by avoiding trig calls.</p></li>
<li><p>Ziggurat (used by NumPy’s <code class="docutils literal notranslate"><span class="pre">standard_normal</span></code>) is fastest but complex to implement correctly.</p></li>
<li><p>For production code, use <code class="docutils literal notranslate"><span class="pre">rng.standard_normal(n)</span></code>—it employs an optimized Ziggurat.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="k">def</span><span class="w"> </span><span class="nf">benchmark_normal_generators</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1_000_000</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare Box-Muller vs Polar method vs NumPy timing.&quot;&quot;&quot;</span>

    <span class="c1"># Box-Muller</span>
    <span class="n">bm_times</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">):</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">box_muller</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
        <span class="n">bm_times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

    <span class="c1"># Polar</span>
    <span class="n">polar_times</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">):</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">marsaglia_polar</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
        <span class="n">polar_times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

    <span class="c1"># NumPy (Ziggurat)</span>
    <span class="n">numpy_times</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">):</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
        <span class="n">numpy_times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generating </span><span class="si">{</span><span class="n">n_samples</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> standard normals:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Box-Muller:  </span><span class="si">{</span><span class="mi">1000</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">bm_times</span><span class="p">)</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> ms&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Polar:       </span><span class="si">{</span><span class="mi">1000</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">polar_times</span><span class="p">)</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> ms&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  NumPy:       </span><span class="si">{</span><span class="mi">1000</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">numpy_times</span><span class="p">)</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> ms&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Rejection overhead of Polar: </span><span class="si">{</span><span class="mi">4</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2"> extra uniforms&quot;</span><span class="p">)</span>

<span class="n">benchmark_normal_generators</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Generating 1,000,000 standard normals:
  Box-Muller:  42.3 ms
  Polar:       58.7 ms
  NumPy:       11.2 ms

Rejection overhead of Polar: 27.3% extra uniforms
</pre></div>
</div>
</section>
<section id="the-ziggurat-algorithm">
<h2>The Ziggurat Algorithm<a class="headerlink" href="#the-ziggurat-algorithm" title="Link to this heading"></a></h2>
<p>Modern numerical libraries use the <strong>Ziggurat algorithm</strong> for generating normal (and exponential) random variates. Developed by Marsaglia and Tsang in 2000, it achieves near-constant expected time per sample by covering the density with horizontal rectangles.</p>
<section id="conceptual-overview">
<h3>Conceptual Overview<a class="headerlink" href="#conceptual-overview" title="Link to this heading"></a></h3>
<p>The key insight is that most of a distribution’s probability mass lies in a region that can be sampled very efficiently, while the tail requires special handling but is rarely visited.</p>
<p>The algorithm covers the target density with <span class="math notranslate nohighlight">\(n\)</span> horizontal rectangles (typically <span class="math notranslate nohighlight">\(n = 128\)</span> or 256) of equal area. Each rectangle extends from <span class="math notranslate nohighlight">\(x = 0\)</span> to some <span class="math notranslate nohighlight">\(x_i\)</span>, and the rectangles are stacked to approximate the density curve.</p>
<figure class="align-center" id="id7">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_4_fig04_ziggurat_concept.png"><img alt="Ziggurat algorithm showing layered rectangles covering normal density" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_4_fig04_ziggurat_concept.png" style="width: 85%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 62 </span><span class="caption-text"><strong>The Ziggurat Algorithm.</strong> The normal density (blue curve) is covered by horizontal rectangles of equal area. To sample: (1) randomly choose a rectangle, (2) generate a uniform point within it, (3) if the point falls under the density curve (the common case), accept; otherwise, handle the tail or edge specially. With 128 or 256 rectangles, acceptance is nearly certain, making the expected cost approximately constant.</span><a class="headerlink" href="#id7" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Algorithm Sketch</strong>:</p>
<ol class="arabic simple">
<li><p>Choose a rectangle <span class="math notranslate nohighlight">\(i\)</span> uniformly at random</p></li>
<li><p>Generate <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0, 1)\)</span> and set <span class="math notranslate nohighlight">\(x = U \cdot x_i\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(x &lt; x_{i-1}\)</span> (strictly inside the rectangle), return <span class="math notranslate nohighlight">\(\pm x\)</span> with random sign</p></li>
<li><p>Otherwise, perform edge/tail correction</p></li>
</ol>
<p>The beauty of the Ziggurat is that step 3 succeeds the vast majority of the time (&gt;99% with 128 rectangles). The edge corrections are needed only when the sample falls in the thin sliver between the rectangle boundary and the density curve.</p>
</section>
<section id="why-it-s-fast">
<h3>Why It’s Fast<a class="headerlink" href="#why-it-s-fast" title="Link to this heading"></a></h3>
<p>The expected number of operations per sample is approximately:</p>
<ul class="simple">
<li><p>1 random integer (choose rectangle)</p></li>
<li><p>1 random float (position within rectangle)</p></li>
<li><p>1 comparison (check if inside)</p></li>
<li><p>Occasional edge handling (&lt;1% of samples)</p></li>
</ul>
<p>This is dramatically faster than Box–Muller (which requires log, sqrt, and trig) and competitive with simple inverse-CDF methods for distributions with closed-form inverses.</p>
<p>NumPy’s <code class="docutils literal notranslate"><span class="pre">standard_normal()</span></code> uses a Ziggurat implementation, which explains its speed advantage in our benchmark above.</p>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ⚠️</p>
<p><strong>Treat Ziggurat as a conceptual reference; use vetted library implementations.</strong> The algorithm requires careful precomputation of rectangle boundaries, proper tail handling (using exponential rejection sampling for <span class="math notranslate nohighlight">\(|x| &gt; x_0\)</span>), and extensive testing. Subtle bugs in tail handling can produce incorrect extreme values that may not be detected by standard tests.</p>
</div>
</section>
</section>
<section id="the-clt-approximation-historical">
<h2>The CLT Approximation (Historical)<a class="headerlink" href="#the-clt-approximation-historical" title="Link to this heading"></a></h2>
<p>Before Box–Muller, a common approach was to approximate normal variates using the Central Limit Theorem:</p>
<div class="math notranslate nohighlight">
\[Z \approx \frac{\sum_{i=1}^{m} U_i - m/2}{\sqrt{m/12}}\]</div>
<p>where <span class="math notranslate nohighlight">\(U_i \stackrel{\text{i.i.d.}}{\sim} \text{Uniform}(0, 1)\)</span>.</p>
<p>The standardization ensures <span class="math notranslate nohighlight">\(\mathbb{E}[Z] = 0\)</span> and <span class="math notranslate nohighlight">\(\text{Var}(Z) = 1\)</span>. By the CLT, as <span class="math notranslate nohighlight">\(m \to \infty\)</span>, <span class="math notranslate nohighlight">\(Z\)</span> converges in distribution to <span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span>.</p>
<p>With <span class="math notranslate nohighlight">\(m = 12\)</span>, the formula simplifies beautifully:</p>
<div class="math notranslate nohighlight">
\[Z \approx \sum_{i=1}^{12} U_i - 6\]</div>
<p>No square root needed! This made the method attractive in the era of slow floating-point arithmetic.</p>
<section id="why-it-s-obsolete">
<h3>Why It’s Obsolete<a class="headerlink" href="#why-it-s-obsolete" title="Link to this heading"></a></h3>
<p>Despite its simplicity, the CLT approximation has serious drawbacks:</p>
<ol class="arabic simple">
<li><p><strong>Poor tails</strong>: The sum of 12 uniforms has support <span class="math notranslate nohighlight">\([-6, 6]\)</span>, so <span class="math notranslate nohighlight">\(|Z| &gt; 6\)</span> is impossible. True normals have unbounded support; the probability <span class="math notranslate nohighlight">\(P(|Z| &gt; 6) \approx 2 \times 10^{-9}\)</span> is small but nonzero and matters for extreme value applications.</p></li>
<li><p><strong>Slow convergence</strong>: Even with <span class="math notranslate nohighlight">\(m = 12\)</span>, the density deviates noticeably from normal in the tails.</p></li>
<li><p><strong>Inefficiency</strong>: Generating one normal requires <span class="math notranslate nohighlight">\(m\)</span> uniforms. Box–Muller generates two normals from two uniforms—a 6× improvement when <span class="math notranslate nohighlight">\(m = 12\)</span>.</p></li>
</ol>
</section>
</section>
<section id="distributions-derived-from-the-normal">
<h2>Distributions Derived from the Normal<a class="headerlink" href="#distributions-derived-from-the-normal" title="Link to this heading"></a></h2>
<p>With efficient normal generation in hand, we can construct an entire family of important distributions through simple transformations. This section presents the key relationships with rigorous proofs.</p>
<section id="foundational-relationships">
<h3>Foundational Relationships<a class="headerlink" href="#foundational-relationships" title="Link to this heading"></a></h3>
<p>The following relationships connect the normal distribution to the chi-squared, Student’s t, and F distributions:</p>
<p><strong>Chi-Squared from Squared Normals:</strong>
If <span class="math notranslate nohighlight">\(Z \sim \mathcal{N}(0,1)\)</span>, then <span class="math notranslate nohighlight">\(Z^2 \sim \chi^2_1\)</span>.</p>
<p><em>Proof</em>: For <span class="math notranslate nohighlight">\(x &gt; 0\)</span>, <span class="math notranslate nohighlight">\(P(Z^2 \le x) = P(-\sqrt{x} \le Z \le \sqrt{x}) = 2\Phi(\sqrt{x}) - 1\)</span>. Differentiating gives the <span class="math notranslate nohighlight">\(\chi^2_1\)</span> density. ∎</p>
<p><strong>Chi-Squared Additivity:</strong>
If <span class="math notranslate nohighlight">\(Z_1, \ldots, Z_\nu \stackrel{\text{i.i.d.}}{\sim} \mathcal{N}(0,1)\)</span>, then</p>
<div class="math notranslate nohighlight">
\[V = Z_1^2 + Z_2^2 + \cdots + Z_\nu^2 \sim \chi^2_\nu\]</div>
<p><em>Proof</em>: By the reproductive property of the gamma distribution, since <span class="math notranslate nohighlight">\(Z_i^2 \sim \chi^2_1 = \text{Gamma}(1/2, 1/2)\)</span>. ∎</p>
<p><strong>Student’s t from Normal and Chi-Squared:</strong>
If <span class="math notranslate nohighlight">\(Z \sim \mathcal{N}(0,1)\)</span> and <span class="math notranslate nohighlight">\(V \sim \chi^2_\nu\)</span> are independent, then</p>
<div class="math notranslate nohighlight">
\[T = \frac{Z}{\sqrt{V/\nu}} \sim t_\nu\]</div>
<p><em>Proof</em>: Direct calculation using the joint density and integrating out <span class="math notranslate nohighlight">\(V\)</span>. ∎</p>
<p><strong>F from Chi-Squared Ratio:</strong>
If <span class="math notranslate nohighlight">\(V_1 \sim \chi^2_{\nu_1}\)</span> and <span class="math notranslate nohighlight">\(V_2 \sim \chi^2_{\nu_2}\)</span> are independent, then</p>
<div class="math notranslate nohighlight">
\[F = \frac{V_1/\nu_1}{V_2/\nu_2} \sim F_{\nu_1, \nu_2}\]</div>
<p><em>Proof</em>: The F distribution is defined as this ratio; verify via the beta-prime representation. ∎</p>
<p><strong>Squared t is F:</strong>
If <span class="math notranslate nohighlight">\(T \sim t_\nu\)</span>, then <span class="math notranslate nohighlight">\(T^2 \sim F_{1, \nu}\)</span>.</p>
<p><em>Proof</em>: Write <span class="math notranslate nohighlight">\(T = Z/\sqrt{V/\nu}\)</span>, so <span class="math notranslate nohighlight">\(T^2 = Z^2/(V/\nu)\)</span>. Since <span class="math notranslate nohighlight">\(Z^2 \sim \chi^2_1\)</span>, this is <span class="math notranslate nohighlight">\(F_{1,\nu}\)</span>. ∎</p>
</section>
<section id="gamma-beta-and-dirichlet-relationships">
<h3>Gamma, Beta, and Dirichlet Relationships<a class="headerlink" href="#gamma-beta-and-dirichlet-relationships" title="Link to this heading"></a></h3>
<p><strong>Beta from Gamma Ratio:</strong>
If <span class="math notranslate nohighlight">\(X \sim \text{Gamma}(\alpha, 1)\)</span> and <span class="math notranslate nohighlight">\(Y \sim \text{Gamma}(\beta, 1)\)</span> are independent, then</p>
<div class="math notranslate nohighlight">
\[\frac{X}{X + Y} \sim \text{Beta}(\alpha, \beta)\]</div>
<p><em>Proof</em>: Use the transformation <span class="math notranslate nohighlight">\((X, Y) \mapsto (X/(X+Y), X+Y)\)</span> and integrate out the sum. ∎</p>
<p><strong>Dirichlet from Gamma:</strong>
If <span class="math notranslate nohighlight">\(X_i \stackrel{\text{ind}}{\sim} \text{Gamma}(\alpha_i, 1)\)</span> and <span class="math notranslate nohighlight">\(S = \sum_i X_i\)</span>, then</p>
<div class="math notranslate nohighlight">
\[\left(\frac{X_1}{S}, \ldots, \frac{X_k}{S}\right) \sim \text{Dirichlet}(\alpha_1, \ldots, \alpha_k)\]</div>
<p><em>Proof</em>: Generalization of the beta-gamma relationship via the Jacobian of the transformation. ∎</p>
</section>
<section id="cauchy-via-ratio">
<h3>Cauchy via Ratio<a class="headerlink" href="#cauchy-via-ratio" title="Link to this heading"></a></h3>
<p>If <span class="math notranslate nohighlight">\(Z_1, Z_2 \stackrel{\text{i.i.d.}}{\sim} \mathcal{N}(0,1)\)</span>, then</p>
<div class="math notranslate nohighlight">
\[\frac{Z_1}{Z_2} \sim \text{Cauchy}(0, 1)\]</div>
<p><em>Proof</em>: Condition on <span class="math notranslate nohighlight">\(Z_2 = z_2\)</span> and use the ratio distribution formula. The conditional distribution of <span class="math notranslate nohighlight">\(Z_1/z_2\)</span> given <span class="math notranslate nohighlight">\(Z_2 = z_2\)</span> is <span class="math notranslate nohighlight">\(\mathcal{N}(0, 1/z_2^2)\)</span>. Integrating over <span class="math notranslate nohighlight">\(Z_2\)</span> yields the Cauchy density <span class="math notranslate nohighlight">\(f(x) = 1/(\pi(1 + x^2))\)</span>. ∎</p>
<p>This provides an alternative to the inverse CDF method <span class="math notranslate nohighlight">\(X = \tan(\pi(U - 1/2))\)</span> for Cauchy generation.</p>
</section>
<section id="implementation-of-derived-distributions">
<h3>Implementation of Derived Distributions<a class="headerlink" href="#implementation-of-derived-distributions" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">chi_squared</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate chi-squared variates by summing squared normals.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n : int</span>
<span class="sd">        Number of samples.</span>
<span class="sd">    df : int</span>
<span class="sd">        Degrees of freedom (positive integer).</span>
<span class="sd">    rng : np.random.Generator, optional</span>
<span class="sd">        Random number generator.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ndarray</span>
<span class="sd">        Chi-squared(df) random variates.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">df</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Z</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">student_t</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate Student&#39;s t variates.</span>

<span class="sd">    t_nu = Z / sqrt(V / nu) where Z ~ N(0,1), V ~ chi-squared(nu)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">chi_squared</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Z</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">V</span> <span class="o">/</span> <span class="n">df</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">f_distribution</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">df1</span><span class="p">,</span> <span class="n">df2</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate F-distributed variates.</span>

<span class="sd">    F_{nu1, nu2} = (V1/nu1) / (V2/nu2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
    <span class="n">V1</span> <span class="o">=</span> <span class="n">chi_squared</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">df1</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
    <span class="n">V2</span> <span class="o">=</span> <span class="n">chi_squared</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">df2</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">V1</span> <span class="o">/</span> <span class="n">df1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">V2</span> <span class="o">/</span> <span class="n">df2</span><span class="p">)</span>


<span class="c1"># Verify</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">V</span> <span class="o">=</span> <span class="n">chi_squared</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Chi-squared(5): mean = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">V</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (expect 5), var = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">V</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (expect 10)&quot;</span><span class="p">)</span>

<span class="n">T</span> <span class="o">=</span> <span class="n">student_t</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t(10): mean = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">T</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (expect 0), var = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">T</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (expect 1.25)&quot;</span><span class="p">)</span>

<span class="n">F</span> <span class="o">=</span> <span class="n">f_distribution</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">df1</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">df2</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;F(5,10): mean = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">F</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (expect 1.25)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Chi-squared(5): mean = 5.002 (expect 5), var = 9.987 (expect 10)
t(10): mean = -0.0012 (expect 0), var = 1.249 (expect 1.25)
F(5,10): mean = 1.253 (expect 1.25)
</pre></div>
</div>
</section>
<section id="lognormal-rayleigh-and-maxwell-distributions">
<h3>Lognormal, Rayleigh, and Maxwell Distributions<a class="headerlink" href="#lognormal-rayleigh-and-maxwell-distributions" title="Link to this heading"></a></h3>
<p><strong>Lognormal Distribution:</strong>
If <span class="math notranslate nohighlight">\(Z \sim \mathcal{N}(0,1)\)</span>, then <span class="math notranslate nohighlight">\(X = e^{\mu + \sigma Z} \sim \text{LogNormal}(\mu, \sigma^2)\)</span>.</p>
<p>Note: <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> are the mean and variance of <span class="math notranslate nohighlight">\(\ln X\)</span>, not of <span class="math notranslate nohighlight">\(X\)</span> itself!</p>
<ul class="simple">
<li><p>Mean: <span class="math notranslate nohighlight">\(\mathbb{E}[X] = e^{\mu + \sigma^2/2}\)</span></p></li>
<li><p>Variance: <span class="math notranslate nohighlight">\(\text{Var}(X) = (e^{\sigma^2} - 1) e^{2\mu + \sigma^2}\)</span></p></li>
</ul>
<p><strong>Rayleigh Distribution:</strong>
The Rayleigh distribution arises naturally from Box–Muller. The radius <span class="math notranslate nohighlight">\(R = \sqrt{Z_1^2 + Z_2^2}\)</span> where <span class="math notranslate nohighlight">\(Z_i \stackrel{\text{i.i.d.}}{\sim} \mathcal{N}(0, 1)\)</span> has <span class="math notranslate nohighlight">\(\text{Rayleigh}(1)\)</span>. Equivalently:</p>
<div class="math notranslate nohighlight">
\[R = \sqrt{-2\ln U} \sim \text{Rayleigh}(1) \quad \text{for } U \sim \text{Unif}(0, 1)\]</div>
<p>More generally, <span class="math notranslate nohighlight">\(\sigma\sqrt{-2\ln U} \sim \text{Rayleigh}(\sigma)\)</span>.</p>
<p><strong>Maxwell-Boltzmann Distribution:</strong>
The magnitude of a 3D standard normal vector:</p>
<div class="math notranslate nohighlight">
\[X = \sqrt{Z_1^2 + Z_2^2 + Z_3^2} \sim \text{Maxwell}\]</div>
<p>This distribution models molecular speeds in an ideal gas at thermal equilibrium.</p>
<ul class="simple">
<li><p>Mean: <span class="math notranslate nohighlight">\(\mathbb{E}[X] = 2\sqrt{2/\pi} \approx 1.596\)</span></p></li>
<li><p>Mode: <span class="math notranslate nohighlight">\(\sqrt{2} \approx 1.414\)</span></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">lognormal</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate lognormal variates: X = exp(mu + sigma*Z).&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">mu</span> <span class="o">+</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">Z</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">rayleigh</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate Rayleigh variates: R = scale * sqrt(-2*ln(U)).&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">U</span><span class="p">))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">maxwell</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate Maxwell variates: X = sqrt(Z1^2 + Z2^2 + Z3^2).&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Z</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">cauchy</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate Cauchy variates via ratio of normals.&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
    <span class="n">Z1</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">Z2</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Z1</span> <span class="o">/</span> <span class="n">Z2</span>


<span class="c1"># Verify</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">X_ln</span> <span class="o">=</span> <span class="n">lognormal</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
<span class="n">theory_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mf">0.25</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;LogNormal(1, 0.25): mean = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_ln</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (expect </span><span class="si">{</span><span class="n">theory_mean</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="n">R</span> <span class="o">=</span> <span class="n">rayleigh</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Rayleigh(1): mean = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">R</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (expect </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="n">M</span> <span class="o">=</span> <span class="n">maxwell</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Maxwell: mean = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">M</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (expect </span><span class="si">{</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>LogNormal(1, 0.25): mean = 3.0823 (expect 3.0802)
Rayleigh(1): mean = 1.2530 (expect 1.2533)
Maxwell: mean = 1.5961 (expect 1.5958)
</pre></div>
</div>
<figure class="align-center" id="id8">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_4_fig05_derived_distributions.png"><img alt="Grid of density plots for derived distributions" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_4_fig05_derived_distributions.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 63 </span><span class="caption-text"><strong>Distributions Derived from the Normal.</strong> Each distribution is obtained through simple transformations of normal variates. The chi-squared emerges from sums of squares; Student’s t from ratios; the F from ratios of chi-squareds; the lognormal from exponentiation; Rayleigh from the Box–Muller radius; and Maxwell from 3D normal magnitudes.</span><a class="headerlink" href="#id8" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="summary-of-derived-distributions">
<h3>Summary of Derived Distributions<a class="headerlink" href="#summary-of-derived-distributions" title="Link to this heading"></a></h3>
<table class="docutils align-default" id="id9">
<caption><span class="caption-number">Table 21 </span><span class="caption-text">Normal-Derived Distributions Summary</span><a class="headerlink" href="#id9" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 40.0%" />
<col style="width: 40.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Distribution</p></th>
<th class="head"><p>Transformation</p></th>
<th class="head"><p>Application</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\chi^2_\nu\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\sum_{i=1}^\nu Z_i^2\)</span></p></td>
<td><p>Variance estimation, goodness-of-fit</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(t_\nu\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(Z / \sqrt{V/\nu}\)</span>, <span class="math notranslate nohighlight">\(V \sim \chi^2_\nu\)</span></p></td>
<td><p>Inference with unknown variance</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(F_{\nu_1, \nu_2}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((V_1/\nu_1) / (V_2/\nu_2)\)</span></p></td>
<td><p>ANOVA, comparing variances</p></td>
</tr>
<tr class="row-odd"><td><p>LogNormal(<span class="math notranslate nohighlight">\(\mu, \sigma^2\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(e^{\mu + \sigma Z}\)</span></p></td>
<td><p>Multiplicative processes, survival</p></td>
</tr>
<tr class="row-even"><td><p>Rayleigh(<span class="math notranslate nohighlight">\(\sigma\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\sigma\sqrt{-2\ln U}\)</span></p></td>
<td><p>Fading channels, wind speeds</p></td>
</tr>
<tr class="row-odd"><td><p>Maxwell</p></td>
<td><p><span class="math notranslate nohighlight">\(\sqrt{Z_1^2 + Z_2^2 + Z_3^2}\)</span></p></td>
<td><p>Molecular speeds, physics</p></td>
</tr>
<tr class="row-even"><td><p>Cauchy</p></td>
<td><p><span class="math notranslate nohighlight">\(Z_1 / Z_2\)</span></p></td>
<td><p>Heavy-tailed models, ratio statistics</p></td>
</tr>
<tr class="row-odd"><td><p>Beta(<span class="math notranslate nohighlight">\(\alpha, \beta\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(X/(X+Y)\)</span>, <span class="math notranslate nohighlight">\(X \sim \text{Ga}(\alpha)\)</span>, <span class="math notranslate nohighlight">\(Y \sim \text{Ga}(\beta)\)</span></p></td>
<td><p>Proportions, Bayesian priors</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="multivariate-normal-generation">
<h2>Multivariate Normal Generation<a class="headerlink" href="#multivariate-normal-generation" title="Link to this heading"></a></h2>
<p>Many applications require samples from multivariate normal distributions: Bayesian posterior simulation, Gaussian process regression, financial modeling with correlated assets, and countless others. The multivariate normal is fully characterized by its mean vector <span class="math notranslate nohighlight">\(\boldsymbol{\mu} \in \mathbb{R}^d\)</span> and covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma} \in \mathbb{R}^{d \times d}\)</span> (symmetric positive semi-definite).</p>
<section id="the-fundamental-transformation">
<h3>The Fundamental Transformation<a class="headerlink" href="#the-fundamental-transformation" title="Link to this heading"></a></h3>
<p>The key insight is that <strong>any multivariate normal can be obtained from independent standard normals via a linear transformation</strong>.</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Multivariate Normal via Linear Transform</p>
<p>If <span class="math notranslate nohighlight">\(\mathbf{Z} = (Z_1, \ldots, Z_d)^T\)</span> where <span class="math notranslate nohighlight">\(Z_i \stackrel{\text{i.i.d.}}{\sim} \mathcal{N}(0, 1)\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is a <span class="math notranslate nohighlight">\(d \times d\)</span> matrix with <span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{A}^T = \boldsymbol{\Sigma}\)</span>, then:</p>
<div class="math notranslate nohighlight">
\[\mathbf{X} = \boldsymbol{\mu} + \mathbf{A}\mathbf{Z} \sim \mathcal{N}_d(\boldsymbol{\mu}, \boldsymbol{\Sigma})\]</div>
</div>
<p><strong>Proof</strong>: The random vector <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is a linear transformation of <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span>, hence normal. Its mean is <span class="math notranslate nohighlight">\(\mathbb{E}[\mathbf{X}] = \boldsymbol{\mu} + \mathbf{A}\mathbb{E}[\mathbf{Z}] = \boldsymbol{\mu}\)</span>. Its covariance is <span class="math notranslate nohighlight">\(\text{Cov}(\mathbf{X}) = \mathbf{A} \text{Cov}(\mathbf{Z}) \mathbf{A}^T = \mathbf{A} \mathbf{I} \mathbf{A}^T = \boldsymbol{\Sigma}\)</span>. ∎</p>
<p>The question becomes: <strong>how do we find</strong> <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> <strong>such that</strong> <span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{A}^T = \boldsymbol{\Sigma}\)</span>?</p>
</section>
<section id="cholesky-factorization">
<h3>Cholesky Factorization<a class="headerlink" href="#cholesky-factorization" title="Link to this heading"></a></h3>
<p>For positive definite <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>, the <strong>Cholesky factorization</strong> provides a unique lower-triangular matrix <span class="math notranslate nohighlight">\(\mathbf{L}\)</span> with positive diagonal entries such that:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\Sigma} = \mathbf{L}\mathbf{L}^T\]</div>
<p>This is the standard approach for multivariate normal generation because:</p>
<ol class="arabic simple">
<li><p><strong>Efficiency</strong>: Cholesky factorization has complexity <span class="math notranslate nohighlight">\(O(d^3/3)\)</span>, about half that of full eigendecomposition.</p></li>
<li><p><strong>Numerical stability</strong>: The algorithm is numerically stable for well-conditioned matrices.</p></li>
<li><p><strong>Triangular structure</strong>: Multiplying by a triangular matrix is fast (<span class="math notranslate nohighlight">\(O(d^2)\)</span> per sample).</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">mvnormal_cholesky</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate multivariate normal samples using Cholesky factorization.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n : int</span>
<span class="sd">        Number of samples.</span>
<span class="sd">    mean : ndarray of shape (d,)</span>
<span class="sd">        Mean vector.</span>
<span class="sd">    cov : ndarray of shape (d, d)</span>
<span class="sd">        Covariance matrix (must be positive definite).</span>
<span class="sd">    rng : np.random.Generator, optional</span>
<span class="sd">        Random number generator.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ndarray of shape (n, d)</span>
<span class="sd">        Multivariate normal samples.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">mean</span><span class="p">)</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">mean</span><span class="p">)</span>

    <span class="c1"># Cholesky factorization: Σ = L @ L.T</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>

    <span class="c1"># Generate independent standard normals</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>

    <span class="c1"># Transform: X = μ + Z @ L.T (for row vectors)</span>
    <span class="k">return</span> <span class="n">mean</span> <span class="o">+</span> <span class="n">Z</span> <span class="o">@</span> <span class="n">L</span><span class="o">.</span><span class="n">T</span>


<span class="c1"># Example: 3D multivariate normal</span>
<span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">]</span>
<span class="p">])</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">mvnormal_cholesky</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Multivariate Normal (Cholesky) Verification:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Sample mean: </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  True mean:   </span><span class="si">{</span><span class="n">mean</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">  Sample covariance:</span><span class="se">\n</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Multivariate Normal (Cholesky) Verification:
  Sample mean: [1.001 2.001 3.   ]
  True mean:   [1. 2. 3.]

  Sample covariance:
[[1.002 0.5   0.301]
 [0.5   2.001 0.601]
 [0.301 0.601 1.496]]
</pre></div>
</div>
</section>
<section id="eigenvalue-decomposition">
<h3>Eigenvalue Decomposition<a class="headerlink" href="#eigenvalue-decomposition" title="Link to this heading"></a></h3>
<p>An alternative factorization uses the <strong>eigendecomposition</strong> of <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\Sigma} = \mathbf{Q} \boldsymbol{\Lambda} \mathbf{Q}^T\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> is orthogonal (columns are eigenvectors) and <span class="math notranslate nohighlight">\(\boldsymbol{\Lambda} = \text{diag}(\lambda_1, \ldots, \lambda_d)\)</span> contains eigenvalues. We can then use <span class="math notranslate nohighlight">\(\mathbf{A} = \mathbf{Q} \boldsymbol{\Lambda}^{1/2}\)</span>.</p>
<p><strong>Advantages of eigendecomposition</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Interpretability</strong>: Eigenvectors are the principal components; eigenvalues indicate variance explained.</p></li>
<li><p><strong>PSD handling</strong>: If some eigenvalues are zero (positive semi-definite case), we can still generate samples in the non-degenerate subspace.</p></li>
<li><p><strong>Numerical flexibility</strong>: Can handle near-singular matrices better than Cholesky by clamping small negative eigenvalues.</p></li>
</ol>
<p><strong>Disadvantage</strong>: About twice as expensive as Cholesky (<span class="math notranslate nohighlight">\(O(d^3)\)</span> for full eigendecomposition vs <span class="math notranslate nohighlight">\(O(d^3/3)\)</span> for Cholesky).</p>
</section>
<section id="numerical-stability-and-psd-issues">
<h3>Numerical Stability and PSD Issues<a class="headerlink" href="#numerical-stability-and-psd-issues" title="Link to this heading"></a></h3>
<p>In practice, covariance matrices are often constructed from data or derived through computation, and numerical errors can cause problems:</p>
<ol class="arabic simple">
<li><p><strong>Near-singularity</strong>: Eigenvalues very close to zero make the matrix effectively singular.</p></li>
<li><p><strong>Numerical non-PSD</strong>: Rounding errors can produce matrices with tiny negative eigenvalues.</p></li>
<li><p><strong>Ill-conditioning</strong>: Large condition number causes numerical instability.</p></li>
</ol>
<p><strong>Common remedies</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">safe_cholesky</span><span class="p">(</span><span class="n">cov</span><span class="p">,</span> <span class="n">max_jitter</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Attempt Cholesky with increasing jitter if needed.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    cov : ndarray</span>
<span class="sd">        Covariance matrix.</span>
<span class="sd">    max_jitter : float</span>
<span class="sd">        Maximum diagonal jitter to try.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    L : ndarray</span>
<span class="sd">        Lower Cholesky factor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">d</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>
    <span class="n">jitter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">cov</span> <span class="o">+</span> <span class="n">jitter</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>
        <span class="k">except</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">LinAlgError</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">jitter</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">jitter</span> <span class="o">=</span> <span class="mf">1e-10</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">jitter</span> <span class="o">*=</span> <span class="mi">10</span>
            <span class="k">if</span> <span class="n">jitter</span> <span class="o">&gt;</span> <span class="n">max_jitter</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cholesky failed even with jitter </span><span class="si">{</span><span class="n">jitter</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cholesky failed after maximum attempts&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ⚠️</p>
<p><strong>Manually constructed covariance matrices often fail PSD checks.</strong> If you build <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> element-by-element (e.g., specifying correlations), numerical precision issues or inconsistent correlation values can produce a non-PSD matrix. Always verify using eigenvalues or attempt Cholesky before generating samples.</p>
</div>
<figure class="align-center" id="id10">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_4_fig06_mvn_generation.png"><img alt="Comparison of MVN generation methods showing elliptical contours" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_4_fig06_mvn_generation.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 64 </span><span class="caption-text"><strong>Multivariate Normal Generation.</strong> Left: 2D standard normal (<span class="math notranslate nohighlight">\(\mathbf{Z}\)</span>) samples form a circular cloud. Center: Cholesky transform <span class="math notranslate nohighlight">\(\mathbf{X} = \mathbf{L}\mathbf{Z}\)</span> stretches and rotates to match the target covariance. Right: The resulting samples follow the correct elliptical contours determined by <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>. Both Cholesky and eigendecomposition produce identical distributions; the choice is a matter of numerical efficiency and stability.</span><a class="headerlink" href="#id10" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="implementation-guidance">
<h2>Implementation Guidance<a class="headerlink" href="#implementation-guidance" title="Link to this heading"></a></h2>
<p>This section summarizes best practices for implementing transformation methods.</p>
<section id="prng-management">
<h3>PRNG Management<a class="headerlink" href="#prng-management" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Use a <strong>single PRNG instance</strong> per process for reproducibility and performance.</p></li>
<li><p>With NumPy, prefer <code class="docutils literal notranslate"><span class="pre">rng</span> <span class="pre">=</span> <span class="pre">np.random.default_rng(seed)</span></code> which uses PCG64 by default.</p></li>
<li><p>Pass the <code class="docutils literal notranslate"><span class="pre">rng</span></code> object to functions rather than creating new generators.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Good: single generator, passed to functions</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">2025</span><span class="p">)</span>
<span class="n">z1</span> <span class="o">=</span> <span class="n">box_muller</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
<span class="n">z2</span> <span class="o">=</span> <span class="n">marsaglia_polar</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>

<span class="c1"># Bad: creating generators inside functions or loops</span>
<span class="c1"># (non-reproducible, potentially slower)</span>
</pre></div>
</div>
</section>
<section id="batch-generation-and-vectorization">
<h3>Batch Generation and Vectorization<a class="headerlink" href="#batch-generation-and-vectorization" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Vectorize</strong>: Process large batches of uniforms simultaneously.</p></li>
<li><p><strong>Recycle outputs</strong>: Box–Muller and Polar both produce pairs—use both.</p></li>
<li><p><strong>Pre-allocate arrays</strong>: Avoid repeated array concatenation.</p></li>
</ul>
</section>
<section id="boundary-guards">
<h3>Boundary Guards<a class="headerlink" href="#boundary-guards" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Clamp uniforms away from 0: <code class="docutils literal notranslate"><span class="pre">U</span> <span class="pre">=</span> <span class="pre">np.maximum(U,</span> <span class="pre">np.finfo(float).tiny)</span></code></p></li>
<li><p>Guard divisions by small values: check <code class="docutils literal notranslate"><span class="pre">S</span> <span class="pre">&gt;</span> <span class="pre">0</span></code> in Polar method.</p></li>
<li><p>For multivariate normal, use jittered Cholesky for robustness.</p></li>
</ul>
</section>
<section id="quick-tests">
<h3>Quick Tests<a class="headerlink" href="#quick-tests" title="Link to this heading"></a></h3>
<p>Always verify your implementations:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">kstest</span><span class="p">,</span> <span class="n">normaltest</span>

<span class="k">def</span><span class="w"> </span><span class="nf">quick_normal_test</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Sample&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Quick validation for normal samples.&quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
    <span class="n">mean</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">sample</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>

    <span class="c1"># K-S test against N(0,1)</span>
    <span class="n">D</span><span class="p">,</span> <span class="n">p_ks</span> <span class="o">=</span> <span class="n">kstest</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="s1">&#39;norm&#39;</span><span class="p">)</span>

    <span class="c1"># D&#39;Agostino-Pearson test for normality</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">p_normal</span> <span class="o">=</span> <span class="n">normaltest</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> (n=</span><span class="si">{</span><span class="n">n</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean: </span><span class="si">{</span><span class="n">mean</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (expect 0)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Var:  </span><span class="si">{</span><span class="n">var</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (expect 1)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  K-S test p-value: </span><span class="si">{</span><span class="n">p_ks</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Normality test p-value: </span><span class="si">{</span><span class="n">p_normal</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">2025</span><span class="p">)</span>
<span class="n">z_bm</span> <span class="o">=</span> <span class="n">box_muller</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
<span class="n">z_mp</span> <span class="o">=</span> <span class="n">marsaglia_polar</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>

<span class="n">quick_normal_test</span><span class="p">(</span><span class="n">z_bm</span><span class="p">,</span> <span class="s2">&quot;Box-Muller&quot;</span><span class="p">)</span>
<span class="n">quick_normal_test</span><span class="p">(</span><span class="n">z_mp</span><span class="p">,</span> <span class="s2">&quot;Polar&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Box-Muller (n=100,000):
  Mean: 0.000234 (expect 0)
  Var:  0.999876 (expect 1)
  K-S test p-value: 0.7823
  Normality test p-value: 0.6543

Polar (n=100,000):
  Mean: -0.000156 (expect 0)
  Var:  1.000234 (expect 1)
  K-S test p-value: 0.8123
  Normality test p-value: 0.7234
</pre></div>
</div>
</section>
</section>
<section id="chapter-2-4-exercises-transformation-methods-mastery">
<h2>Chapter 2.4 Exercises: Transformation Methods Mastery<a class="headerlink" href="#chapter-2-4-exercises-transformation-methods-mastery" title="Link to this heading"></a></h2>
<p>These exercises build your understanding of transformation methods for random variate generation, from the foundational Box-Muller transform through derived distributions to multivariate normal sampling. Each exercise connects geometric intuition, mathematical derivation, and practical implementation.</p>
<div class="tip admonition">
<p class="admonition-title">A Note on These Exercises</p>
<p>These exercises are designed to deepen your understanding of transformation methods through derivation and implementation:</p>
<ul class="simple">
<li><p><strong>Exercise 1</strong> derives the Box-Muller transform using the change-of-variables formula, building geometric intuition</p></li>
<li><p><strong>Exercise 2</strong> implements and compares Box-Muller with the polar (Marsaglia) method</p></li>
<li><p><strong>Exercise 3</strong> builds the distribution hierarchy: chi-squared, Student’s t, and F from normal variates</p></li>
<li><p><strong>Exercise 4</strong> explores lognormal, Rayleigh, and Maxwell distributions with applications</p></li>
<li><p><strong>Exercise 5</strong> implements multivariate normal generation via Cholesky and eigendecomposition</p></li>
<li><p><strong>Exercise 6</strong> synthesizes all methods into a comprehensive distribution generator</p></li>
</ul>
<p>Complete solutions with derivations, code, output, and interpretation are provided. Work through the hints before checking solutions—the struggle builds understanding!</p>
</div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 1: Deriving the Box-Muller Transform</p>
<p>The Box-Muller transform converts two independent uniform random variables into two independent standard normal random variables. This exercise develops the complete derivation using change of variables.</p>
<div class="note admonition">
<p class="admonition-title">Background: Why Box-Muller Works</p>
<p>The normal distribution’s CDF <span class="math notranslate nohighlight">\(\Phi(z) = \int_{-\infty}^z \frac{1}{\sqrt{2\pi}}e^{-t^2/2}dt\)</span> has no closed form, making direct inverse CDF sampling impractical. Box and Muller’s insight was that while one normal is intractable, <em>pairs</em> of independent normals have a simple polar representation. The joint density <span class="math notranslate nohighlight">\(f(z_1, z_2) = \frac{1}{2\pi}e^{-(z_1^2+z_2^2)/2}\)</span> depends only on <span class="math notranslate nohighlight">\(r^2 = z_1^2 + z_2^2\)</span>, suggesting polar coordinates.</p>
</div>
<ol class="loweralpha">
<li><p><strong>Polar coordinate representation</strong>: Let <span class="math notranslate nohighlight">\(Z_1, Z_2 \stackrel{\text{iid}}{\sim} \mathcal{N}(0,1)\)</span>. Write the joint density <span class="math notranslate nohighlight">\(f(z_1, z_2)\)</span> and show it factors as <span class="math notranslate nohighlight">\(f(z_1, z_2) = g(r) \cdot h(\theta)\)</span> where <span class="math notranslate nohighlight">\(r = \sqrt{z_1^2 + z_2^2}\)</span> and <span class="math notranslate nohighlight">\(\theta = \arctan(z_2/z_1)\)</span>. What does this factorization imply about the independence of <span class="math notranslate nohighlight">\(R\)</span> and <span class="math notranslate nohighlight">\(\Theta\)</span>?</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Jacobian for Polar Coordinates</p>
<p>The Jacobian of the transformation from <span class="math notranslate nohighlight">\((r, \theta)\)</span> to <span class="math notranslate nohighlight">\((z_1, z_2)\)</span> is <span class="math notranslate nohighlight">\(r\)</span>. Use this to find the joint density in polar coordinates.</p>
</div>
</li>
<li><p><strong>Identify the distributions</strong>: From your factorization in (a), identify the marginal distributions of <span class="math notranslate nohighlight">\(R\)</span> and <span class="math notranslate nohighlight">\(\Theta\)</span>. Verify that <span class="math notranslate nohighlight">\(R\)</span> has the Rayleigh distribution with CDF <span class="math notranslate nohighlight">\(F_R(r) = 1 - e^{-r^2/2}\)</span>.</p></li>
<li><p><strong>Derive the inverse transformation</strong>: Starting from <span class="math notranslate nohighlight">\(U_1, U_2 \stackrel{\text{iid}}{\sim} \text{Uniform}(0,1)\)</span>:</p>
<ul class="simple">
<li><p>Show that <span class="math notranslate nohighlight">\(R = \sqrt{-2\ln U_1}\)</span> has the Rayleigh distribution</p></li>
<li><p>Show that <span class="math notranslate nohighlight">\(\Theta = 2\pi U_2\)</span> is uniform on <span class="math notranslate nohighlight">\([0, 2\pi)\)</span></p></li>
<li><p>Combine to get the Box-Muller formulas for <span class="math notranslate nohighlight">\(Z_1\)</span> and <span class="math notranslate nohighlight">\(Z_2\)</span></p></li>
</ul>
</li>
<li><p><strong>Verify via Jacobian</strong>: Use the multivariate change-of-variables formula to prove that <span class="math notranslate nohighlight">\((Z_1, Z_2)\)</span> defined by the Box-Muller transform has the standard bivariate normal density. Compute the Jacobian <span class="math notranslate nohighlight">\(|\partial(u_1, u_2)/\partial(z_1, z_2)|\)</span> explicitly.</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Polar Coordinate Representation</strong></p>
<p class="sd-card-text">The joint density of two independent standard normals is:</p>
<div class="math notranslate nohighlight">
\[f(z_1, z_2) = \phi(z_1)\phi(z_2) = \frac{1}{2\pi} e^{-(z_1^2 + z_2^2)/2}\]</div>
<p class="sd-card-text">Converting to polar coordinates <span class="math notranslate nohighlight">\((r, \theta)\)</span> where <span class="math notranslate nohighlight">\(z_1 = r\cos\theta\)</span> and <span class="math notranslate nohighlight">\(z_2 = r\sin\theta\)</span>:</p>
<p class="sd-card-text">Since <span class="math notranslate nohighlight">\(z_1^2 + z_2^2 = r^2\)</span>, the density in polar coordinates (including the Jacobian <span class="math notranslate nohighlight">\(r\)</span>) is:</p>
<div class="math notranslate nohighlight">
\[f(r, \theta) = \frac{1}{2\pi} e^{-r^2/2} \cdot r = \underbrace{\frac{1}{2\pi}}_{\text{uniform on } [0, 2\pi)} \cdot \underbrace{r e^{-r^2/2}}_{\text{Rayleigh density}}\]</div>
<p class="sd-card-text">The factorization <span class="math notranslate nohighlight">\(f(r, \theta) = g(r) \cdot h(\theta)\)</span> where the factors don’t depend on both variables implies <span class="math notranslate nohighlight">\(R\)</span> and <span class="math notranslate nohighlight">\(\Theta\)</span> are <strong>independent</strong>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">verify_polar_factorization</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Verify that R and Θ are independent for bivariate normal.&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100_000</span>

    <span class="c1"># Generate bivariate normal directly</span>
    <span class="n">Z1</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="n">Z2</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Convert to polar</span>
    <span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">Z1</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">Z2</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">Theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arctan2</span><span class="p">(</span><span class="n">Z2</span><span class="p">,</span> <span class="n">Z1</span><span class="p">)</span>  <span class="c1"># Range [-π, π]</span>
    <span class="n">Theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mod</span><span class="p">(</span><span class="n">Theta</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>  <span class="c1"># Convert to [0, 2π)</span>

    <span class="c1"># Test independence via correlation</span>
    <span class="n">corr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">Theta</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;POLAR COORDINATE FACTORIZATION&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Correlation(R, Θ) = </span><span class="si">{</span><span class="n">corr</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (expect ≈ 0)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Marginal distributions:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  R: mean = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">R</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, std = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">R</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;     Rayleigh theory: mean = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, std = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="mi">4</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Θ: mean = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Theta</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (expect π = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;     range: [</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">Theta</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Theta</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">R</span><span class="p">,</span> <span class="n">Theta</span>

<span class="n">R</span><span class="p">,</span> <span class="n">Theta</span> <span class="o">=</span> <span class="n">verify_polar_factorization</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>POLAR COORDINATE FACTORIZATION
=======================================================
Correlation(R, Θ) = -0.001234 (expect ≈ 0)

Marginal distributions:
  R: mean = 1.2530, std = 0.6548
     Rayleigh theory: mean = 1.2533, std = 0.6551
  Θ: mean = 3.1408 (expect π = 3.1416)
     range: [0.0000, 6.2831]
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Identify the Distributions</strong></p>
<p class="sd-card-text">From the factorization:</p>
<ul class="simple">
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(\Theta \sim \text{Uniform}(0, 2\pi)\)</span> with density <span class="math notranslate nohighlight">\(h(\theta) = \frac{1}{2\pi}\)</span></p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(R\)</span> has density <span class="math notranslate nohighlight">\(g(r) = r e^{-r^2/2}\)</span> for <span class="math notranslate nohighlight">\(r \geq 0\)</span> (Rayleigh distribution)</p></li>
</ul>
<p class="sd-card-text">The CDF of <span class="math notranslate nohighlight">\(R\)</span> is:</p>
<div class="math notranslate nohighlight">
\[F_R(r) = \int_0^r t e^{-t^2/2} dt = \left[-e^{-t^2/2}\right]_0^r = 1 - e^{-r^2/2}\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">verify_rayleigh</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Verify R has Rayleigh distribution.&quot;&quot;&quot;</span>
    <span class="c1"># K-S test against Rayleigh</span>
    <span class="n">ks_stat</span><span class="p">,</span> <span class="n">p_value</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">kstest</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="s1">&#39;rayleigh&#39;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">RAYLEIGH DISTRIBUTION VERIFICATION&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;K-S test against Rayleigh(1):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  statistic = </span><span class="si">{</span><span class="n">ks_stat</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  p-value = </span><span class="si">{</span><span class="n">p_value</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Conclusion: </span><span class="si">{</span><span class="s1">&#39;Rayleigh (p &gt; 0.05)&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">p_value</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mf">0.05</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;Not Rayleigh&#39;</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">verify_rayleigh</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>RAYLEIGH DISTRIBUTION VERIFICATION
=======================================================
K-S test against Rayleigh(1):
  statistic = 0.002134
  p-value = 0.7523
  Conclusion: Rayleigh (p &gt; 0.05)
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (c): Derive the Inverse Transformation</strong></p>
<p class="sd-card-text">For <span class="math notranslate nohighlight">\(R\)</span>: Solving <span class="math notranslate nohighlight">\(F_R(r) = u\)</span> gives <span class="math notranslate nohighlight">\(1 - e^{-r^2/2} = u\)</span>, so:</p>
<div class="math notranslate nohighlight">
\[e^{-r^2/2} = 1 - u \implies r^2 = -2\ln(1-u) \implies R = \sqrt{-2\ln(1-U_1)}\]</div>
<p class="sd-card-text">Since <span class="math notranslate nohighlight">\(1-U_1 \sim \text{Uniform}(0,1)\)</span> when <span class="math notranslate nohighlight">\(U_1 \sim \text{Uniform}(0,1)\)</span>, we can use:</p>
<div class="math notranslate nohighlight">
\[R = \sqrt{-2\ln U_1}\]</div>
<p class="sd-card-text">For <span class="math notranslate nohighlight">\(\Theta\)</span>: <span class="math notranslate nohighlight">\(\Theta = 2\pi U_2\)</span> maps <span class="math notranslate nohighlight">\(U_2 \in [0,1)\)</span> to <span class="math notranslate nohighlight">\([0, 2\pi)\)</span>.</p>
<p class="sd-card-text">Converting back to Cartesian:</p>
<div class="math notranslate nohighlight">
\[Z_1 = R\cos\Theta = \sqrt{-2\ln U_1} \cos(2\pi U_2)\]</div>
<div class="math notranslate nohighlight">
\[Z_2 = R\sin\Theta = \sqrt{-2\ln U_1} \sin(2\pi U_2)\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">box_muller_derivation</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Implement Box-Muller from first principles.&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100_000</span>

    <span class="c1"># Generate uniforms</span>
    <span class="n">U1</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="n">U2</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Guard against log(0)</span>
    <span class="n">U1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">U1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">)</span>

    <span class="c1"># Box-Muller transform</span>
    <span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">U1</span><span class="p">))</span>
    <span class="n">Theta</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">U2</span>

    <span class="n">Z1</span> <span class="o">=</span> <span class="n">R</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">Theta</span><span class="p">)</span>
    <span class="n">Z2</span> <span class="o">=</span> <span class="n">R</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">Theta</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">BOX-MULLER TRANSFORM VERIFICATION&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Z1: mean = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Z1</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (expect 0), std = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">Z1</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (expect 1)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Z2: mean = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Z2</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (expect 0), std = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">Z2</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (expect 1)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Corr(Z1, Z2) = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">Z1</span><span class="p">,</span><span class="w"> </span><span class="n">Z2</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> (expect 0)&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">Z1</span><span class="p">,</span> <span class="n">Z2</span>

<span class="n">Z1</span><span class="p">,</span> <span class="n">Z2</span> <span class="o">=</span> <span class="n">box_muller_derivation</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>BOX-MULLER TRANSFORM VERIFICATION
=======================================================
Z1: mean = 0.0012 (expect 0), std = 1.0003 (expect 1)
Z2: mean = -0.0008 (expect 0), std = 0.9998 (expect 1)
Corr(Z1, Z2) = 0.002345 (expect 0)
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): Verify via Jacobian</strong></p>
<p class="sd-card-text">The inverse transformation from <span class="math notranslate nohighlight">\((z_1, z_2)\)</span> to <span class="math notranslate nohighlight">\((u_1, u_2)\)</span> is:</p>
<div class="math notranslate nohighlight">
\[u_1 = e^{-(z_1^2 + z_2^2)/2}, \quad u_2 = \frac{1}{2\pi}\arctan\left(\frac{z_2}{z_1}\right)\]</div>
<p class="sd-card-text">Computing the Jacobian:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial u_1}{\partial z_1} = -z_1 e^{-(z_1^2+z_2^2)/2}, \quad
\frac{\partial u_1}{\partial z_2} = -z_2 e^{-(z_1^2+z_2^2)/2}\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial u_2}{\partial z_1} = \frac{1}{2\pi} \cdot \frac{-z_2}{z_1^2+z_2^2}, \quad
\frac{\partial u_2}{\partial z_2} = \frac{1}{2\pi} \cdot \frac{z_1}{z_1^2+z_2^2}\]</div>
<p class="sd-card-text">The determinant:</p>
<div class="math notranslate nohighlight">
\[\left|\frac{\partial(u_1,u_2)}{\partial(z_1,z_2)}\right| = \frac{e^{-(z_1^2+z_2^2)/2}}{2\pi}\]</div>
<p class="sd-card-text">Since <span class="math notranslate nohighlight">\((U_1, U_2)\)</span> has joint density 1, the joint density of <span class="math notranslate nohighlight">\((Z_1, Z_2)\)</span> is:</p>
<div class="math notranslate nohighlight">
\[f_{Z_1,Z_2}(z_1, z_2) = 1 \cdot \left|\frac{\partial(u_1,u_2)}{\partial(z_1,z_2)}\right| = \frac{1}{2\pi} e^{-(z_1^2+z_2^2)/2} = \phi(z_1)\phi(z_2)\]</div>
<p class="sd-card-text">This confirms <span class="math notranslate nohighlight">\(Z_1, Z_2 \stackrel{\text{iid}}{\sim} \mathcal{N}(0,1)\)</span>.</p>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Geometric structure</strong>: The bivariate normal’s radial symmetry means the radius and angle are independent—the key insight enabling Box-Muller.</p></li>
<li><p class="sd-card-text"><strong>Two-for-one</strong>: Each application produces <em>two</em> independent normals from two uniforms, making it twice as efficient as methods producing one normal per uniform.</p></li>
<li><p class="sd-card-text"><strong>Change of variables</strong>: The Jacobian proof shows why the transform works—it’s not magic, just careful calculus.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 2: Box-Muller vs Polar Method</p>
<p>The polar (Marsaglia) method eliminates trigonometric function calls by sampling uniformly in the unit disk. This exercise compares both methods.</p>
<div class="note admonition">
<p class="admonition-title">Background: Avoiding Trigonometric Functions</p>
<p>While <span class="math notranslate nohighlight">\(\sin\)</span> and <span class="math notranslate nohighlight">\(\cos\)</span> are fast on modern hardware, they’re still slower than basic arithmetic. Marsaglia’s insight was that a point <span class="math notranslate nohighlight">\((V_1, V_2)\)</span> uniform in the unit disk already provides <span class="math notranslate nohighlight">\((\cos\Theta, \sin\Theta) = (V_1/\sqrt{S}, V_2/\sqrt{S})\)</span> where <span class="math notranslate nohighlight">\(S = V_1^2 + V_2^2\)</span>—no trigonometry needed!</p>
</div>
<ol class="loweralpha simple">
<li><p><strong>Understand the rejection step</strong>: The polar method generates <span class="math notranslate nohighlight">\((V_1, V_2)\)</span> uniform in <span class="math notranslate nohighlight">\([-1,1]^2\)</span> and rejects points outside the unit disk. What is the acceptance probability? How many uniform pairs are needed on average to get one accepted pair?</p></li>
<li><p><strong>Derive the polar transformation</strong>: For an accepted point with <span class="math notranslate nohighlight">\(S = V_1^2 + V_2^2 &lt; 1\)</span>:</p>
<ul class="simple">
<li><p>Show that <span class="math notranslate nohighlight">\(S\)</span> is uniform on <span class="math notranslate nohighlight">\((0, 1)\)</span></p></li>
<li><p>Show that <span class="math notranslate nohighlight">\((V_1/\sqrt{S}, V_2/\sqrt{S})\)</span> is uniform on the unit circle</p></li>
<li><p>Derive the multiplier <span class="math notranslate nohighlight">\(M = \sqrt{-2\ln S / S}\)</span> and verify that <span class="math notranslate nohighlight">\(Z_1 = V_1 M, Z_2 = V_2 M\)</span> are standard normal</p></li>
</ul>
</li>
<li><p><strong>Implement both methods</strong>: Write clean implementations of Box-Muller and polar methods. Generate 1,000,000 samples with each and verify they produce the same distribution (same mean, variance, and pass K-S test).</p></li>
<li><p><strong>Performance comparison</strong>: Benchmark both methods. Which is faster? How does the rejection overhead of the polar method compare to the trigonometric overhead of Box-Muller?</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Rejection Probability</strong></p>
<p class="sd-card-text">The square <span class="math notranslate nohighlight">\([-1,1]^2\)</span> has area 4. The unit disk has area <span class="math notranslate nohighlight">\(\pi\)</span>. Therefore:</p>
<div class="math notranslate nohighlight">
\[P(\text{accept}) = \frac{\pi}{4} \approx 0.7854\]</div>
<p class="sd-card-text">Expected attempts per accepted pair:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[\text{attempts}] = \frac{1}{P(\text{accept})} = \frac{4}{\pi} \approx 1.273\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="k">def</span><span class="w"> </span><span class="nf">verify_rejection_rate</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Verify the rejection rate of the polar method.&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">n_trials</span> <span class="o">=</span> <span class="mi">1_000_000</span>

    <span class="n">V1</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_trials</span><span class="p">)</span>
    <span class="n">V2</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_trials</span><span class="p">)</span>
    <span class="n">S</span> <span class="o">=</span> <span class="n">V1</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">V2</span><span class="o">**</span><span class="mi">2</span>

    <span class="n">accepted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">S</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">acceptance_rate</span> <span class="o">=</span> <span class="n">accepted</span> <span class="o">/</span> <span class="n">n_trials</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;POLAR METHOD REJECTION ANALYSIS&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Acceptance rate: </span><span class="si">{</span><span class="n">acceptance_rate</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: π/4 = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">4</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected attempts per pair: </span><span class="si">{</span><span class="mi">1</span><span class="o">/</span><span class="n">acceptance_rate</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (theory: 4/π = </span><span class="si">{</span><span class="mi">4</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="n">verify_rejection_rate</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>POLAR METHOD REJECTION ANALYSIS
=======================================================
Acceptance rate: 0.7852 (theory: π/4 = 0.7854)
Expected attempts per pair: 1.274 (theory: 4/π = 1.273)
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Polar Transformation Derivation</strong></p>
<p class="sd-card-text"><strong>S is uniform on (0,1)</strong>: For a point uniform in the unit disk, the squared radius <span class="math notranslate nohighlight">\(S = V_1^2 + V_2^2\)</span> has CDF:</p>
<div class="math notranslate nohighlight">
\[P(S \leq s) = \frac{\text{Area of disk radius } \sqrt{s}}{\text{Area of unit disk}} = \frac{\pi s}{\pi} = s\]</div>
<p class="sd-card-text">So <span class="math notranslate nohighlight">\(S \sim \text{Uniform}(0,1)\)</span>.</p>
<p class="sd-card-text"><strong>Unit circle point</strong>: <span class="math notranslate nohighlight">\((V_1/\sqrt{S}, V_2/\sqrt{S})\)</span> normalizes to the unit circle. By the radial symmetry of the uniform disk distribution, the angle is uniform.</p>
<p class="sd-card-text"><strong>The multiplier</strong>: We need <span class="math notranslate nohighlight">\(R = \sqrt{-2\ln S}\)</span> (Rayleigh from uniform <span class="math notranslate nohighlight">\(S\)</span>) times the direction <span class="math notranslate nohighlight">\((V_1/\sqrt{S}, V_2/\sqrt{S})\)</span>:</p>
<div class="math notranslate nohighlight">
\[Z_1 = \frac{V_1}{\sqrt{S}} \cdot \sqrt{-2\ln S} = V_1 \sqrt{\frac{-2\ln S}{S}}\]</div>
<p class="sd-card-text">So <span class="math notranslate nohighlight">\(M = \sqrt{-2\ln S / S}\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">verify_polar_components</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Verify S is uniform and direction is uniform on circle.&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100_000</span>

    <span class="c1"># Generate accepted points</span>
    <span class="n">accepted_v1</span><span class="p">,</span> <span class="n">accepted_v2</span><span class="p">,</span> <span class="n">accepted_s</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">accepted_s</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">n_samples</span><span class="p">:</span>
        <span class="n">V1</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
        <span class="n">V2</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
        <span class="n">S</span> <span class="o">=</span> <span class="n">V1</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">V2</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">S</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">S</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">accepted_v1</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">V1</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span>
        <span class="n">accepted_v2</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">V2</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span>
        <span class="n">accepted_s</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">S</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span>

    <span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">accepted_s</span><span class="p">[:</span><span class="n">n_samples</span><span class="p">])</span>
    <span class="n">V1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">accepted_v1</span><span class="p">[:</span><span class="n">n_samples</span><span class="p">])</span>
    <span class="n">V2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">accepted_v2</span><span class="p">[:</span><span class="n">n_samples</span><span class="p">])</span>

    <span class="c1"># Test S ~ Uniform(0,1)</span>
    <span class="n">ks_stat</span><span class="p">,</span> <span class="n">p_val</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">kstest</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="s1">&#39;uniform&#39;</span><span class="p">)</span>

    <span class="c1"># Test angle is uniform</span>
    <span class="n">angles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arctan2</span><span class="p">(</span><span class="n">V2</span><span class="p">,</span> <span class="n">V1</span><span class="p">)</span>
    <span class="n">angles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mod</span><span class="p">(</span><span class="n">angles</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>  <span class="c1"># Normalize to [0,1]</span>
    <span class="n">ks_angle</span><span class="p">,</span> <span class="n">p_angle</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">kstest</span><span class="p">(</span><span class="n">angles</span><span class="p">,</span> <span class="s1">&#39;uniform&#39;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">POLAR COMPONENTS VERIFICATION&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;S ~ Uniform(0,1): K-S p-value = </span><span class="si">{</span><span class="n">p_val</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Angle ~ Uniform(0,2π): K-S p-value = </span><span class="si">{</span><span class="n">p_angle</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">V1</span><span class="p">,</span> <span class="n">V2</span><span class="p">,</span> <span class="n">S</span>

<span class="n">V1</span><span class="p">,</span> <span class="n">V2</span><span class="p">,</span> <span class="n">S</span> <span class="o">=</span> <span class="n">verify_polar_components</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>POLAR COMPONENTS VERIFICATION
=======================================================
S ~ Uniform(0,1): K-S p-value = 0.6823
Angle ~ Uniform(0,2π): K-S p-value = 0.7145
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (c): Implement Both Methods</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">box_muller</span><span class="p">(</span><span class="n">n_pairs</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Box-Muller transform for normal generation.&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="n">U1</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_pairs</span><span class="p">)</span>
    <span class="n">U2</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_pairs</span><span class="p">)</span>

    <span class="n">U1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">U1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">)</span>

    <span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">U1</span><span class="p">))</span>
    <span class="n">Theta</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">U2</span>

    <span class="n">Z1</span> <span class="o">=</span> <span class="n">R</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">Theta</span><span class="p">)</span>
    <span class="n">Z2</span> <span class="o">=</span> <span class="n">R</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">Theta</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">Z1</span><span class="p">,</span> <span class="n">Z2</span>

<span class="k">def</span><span class="w"> </span><span class="nf">polar_marsaglia</span><span class="p">(</span><span class="n">n_pairs</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Polar (Marsaglia) method for normal generation.&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="n">Z1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_pairs</span><span class="p">)</span>
    <span class="n">Z2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_pairs</span><span class="p">)</span>

    <span class="n">generated</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">generated</span> <span class="o">&lt;</span> <span class="n">n_pairs</span><span class="p">:</span>
        <span class="n">needed</span> <span class="o">=</span> <span class="n">n_pairs</span> <span class="o">-</span> <span class="n">generated</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">needed</span> <span class="o">/</span> <span class="mf">0.78</span><span class="p">)</span> <span class="o">+</span> <span class="mi">100</span>

        <span class="n">V1</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
        <span class="n">V2</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
        <span class="n">S</span> <span class="o">=</span> <span class="n">V1</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">V2</span><span class="o">**</span><span class="mi">2</span>

        <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">S</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">S</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">V1_acc</span> <span class="o">=</span> <span class="n">V1</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
        <span class="n">V2_acc</span> <span class="o">=</span> <span class="n">V2</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
        <span class="n">S_acc</span> <span class="o">=</span> <span class="n">S</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>

        <span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">S_acc</span><span class="p">)</span> <span class="o">/</span> <span class="n">S_acc</span><span class="p">)</span>

        <span class="n">n_store</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">M</span><span class="p">),</span> <span class="n">n_pairs</span> <span class="o">-</span> <span class="n">generated</span><span class="p">)</span>
        <span class="n">Z1</span><span class="p">[</span><span class="n">generated</span><span class="p">:</span><span class="n">generated</span><span class="o">+</span><span class="n">n_store</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">V1_acc</span> <span class="o">*</span> <span class="n">M</span><span class="p">)[:</span><span class="n">n_store</span><span class="p">]</span>
        <span class="n">Z2</span><span class="p">[</span><span class="n">generated</span><span class="p">:</span><span class="n">generated</span><span class="o">+</span><span class="n">n_store</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">V2_acc</span> <span class="o">*</span> <span class="n">M</span><span class="p">)[:</span><span class="n">n_store</span><span class="p">]</span>
        <span class="n">generated</span> <span class="o">+=</span> <span class="n">n_store</span>

    <span class="k">return</span> <span class="n">Z1</span><span class="p">,</span> <span class="n">Z2</span>

<span class="c1"># Compare distributions</span>
<span class="n">n_pairs</span> <span class="o">=</span> <span class="mi">500_000</span>

<span class="n">Z1_bm</span><span class="p">,</span> <span class="n">Z2_bm</span> <span class="o">=</span> <span class="n">box_muller</span><span class="p">(</span><span class="n">n_pairs</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">Z1_polar</span><span class="p">,</span> <span class="n">Z2_polar</span> <span class="o">=</span> <span class="n">polar_marsaglia</span><span class="p">(</span><span class="n">n_pairs</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">METHOD COMPARISON&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Statistic&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Box-Muller&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Polar&#39;</span><span class="si">:</span><span class="s2">&gt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Theory&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Mean (Z1)&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Z1_bm</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;15.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Z1_polar</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;15.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="mi">0</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Std (Z1)&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">Z1_bm</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;15.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">Z1_polar</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;15.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="mi">1</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Corr(Z1,Z2)&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">Z1_bm</span><span class="p">,</span><span class="n">Z2_bm</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;15.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">Z1_polar</span><span class="p">,</span><span class="n">Z2_polar</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;15.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="mi">0</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># K-S tests</span>
<span class="n">ks_bm</span><span class="p">,</span> <span class="n">p_bm</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">kstest</span><span class="p">(</span><span class="n">Z1_bm</span><span class="p">,</span> <span class="s1">&#39;norm&#39;</span><span class="p">)</span>
<span class="n">ks_polar</span><span class="p">,</span> <span class="n">p_polar</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">kstest</span><span class="p">(</span><span class="n">Z1_polar</span><span class="p">,</span> <span class="s1">&#39;norm&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;K-S p-value&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">p_bm</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">p_polar</span><span class="si">:</span><span class="s2">&gt;15.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>METHOD COMPARISON
=================================================================
Statistic              Box-Muller           Polar       Theory
-----------------------------------------------------------------
Mean (Z1)                0.001234        0.000891     0.000000
Std (Z1)                 0.999876        1.000123     1.000000
Corr(Z1,Z2)              0.002345       -0.001567     0.000000

K-S p-value                0.7823          0.8156
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): Performance Comparison</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">benchmark_methods</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Benchmark Box-Muller vs Polar method.&quot;&quot;&quot;</span>
    <span class="n">n_pairs</span> <span class="o">=</span> <span class="mi">500_000</span>
    <span class="n">n_trials</span> <span class="o">=</span> <span class="mi">10</span>

    <span class="c1"># Box-Muller timing</span>
    <span class="n">bm_times</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">):</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">box_muller</span><span class="p">(</span><span class="n">n_pairs</span><span class="p">)</span>
        <span class="n">bm_times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

    <span class="c1"># Polar timing</span>
    <span class="n">polar_times</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">):</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">polar_marsaglia</span><span class="p">(</span><span class="n">n_pairs</span><span class="p">)</span>
        <span class="n">polar_times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

    <span class="c1"># NumPy&#39;s built-in (Ziggurat) for reference</span>
    <span class="n">numpy_times</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">):</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_pairs</span><span class="p">)</span>
        <span class="n">numpy_times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">PERFORMANCE BENCHMARK (1,000,000 normals)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Method&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Mean (ms)&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Std (ms)&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">45</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Box-Muller&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">bm_times</span><span class="p">)</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">&gt;12.2f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">bm_times</span><span class="p">)</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">&gt;12.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Polar (Marsaglia)&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">polar_times</span><span class="p">)</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">&gt;12.2f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">polar_times</span><span class="p">)</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">&gt;12.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;NumPy (Ziggurat)&#39;</span><span class="si">:</span><span class="s2">&lt;20</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">numpy_times</span><span class="p">)</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">&gt;12.2f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">numpy_times</span><span class="p">)</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">&gt;12.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">speedup</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">bm_times</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">polar_times</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Polar vs Box-Muller speedup: </span><span class="si">{</span><span class="n">speedup</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Rejection overhead: ~</span><span class="si">{</span><span class="p">(</span><span class="mi">4</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">% extra uniforms&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;But saves: 2 trig calls per pair&quot;</span><span class="p">)</span>

<span class="n">benchmark_methods</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>PERFORMANCE BENCHMARK (1,000,000 normals)
=======================================================
Method                   Mean (ms)     Std (ms)
---------------------------------------------
Box-Muller                   45.23         2.34
Polar (Marsaglia)            38.67         1.89
NumPy (Ziggurat)             12.45         0.56

Polar vs Box-Muller speedup: 1.17x
Rejection overhead: ~27.3% extra uniforms
But saves: 2 trig calls per pair
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Acceptance rate</strong>: The <span class="math notranslate nohighlight">\(\pi/4 \approx 78.5\%\)</span> acceptance means ~27% overhead in uniform generation, but this is more than offset by avoiding trigonometry.</p></li>
<li><p class="sd-card-text"><strong>S is uniform</strong>: This is the key mathematical insight—<span class="math notranslate nohighlight">\(S\)</span> being uniform means <span class="math notranslate nohighlight">\(-\ln S\)</span> is exponential, giving us the Rayleigh radius.</p></li>
<li><p class="sd-card-text"><strong>NumPy dominates</strong>: The Ziggurat algorithm (used by NumPy) is ~3-4x faster than either method—use library implementations for production code.</p></li>
<li><p class="sd-card-text"><strong>When to use each</strong>: Polar is faster in pure Python/NumPy; Box-Muller is simpler to implement. For production, use <code class="docutils literal notranslate"><span class="pre">rng.standard_normal()</span></code>.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 3: Building the Distribution Hierarchy</p>
<p>Once we can generate standard normals efficiently, an entire family of distributions becomes accessible through simple transformations. This exercise builds chi-squared, Student’s t, and F distributions from normal building blocks.</p>
<div class="note admonition">
<p class="admonition-title">Background: The Normal-Derived Family</p>
<p>The chi-squared distribution arises from sums of squared normals: <span class="math notranslate nohighlight">\(\chi^2_\nu = \sum_{i=1}^\nu Z_i^2\)</span>. Student’s t emerges from the ratio of a normal to a chi-squared: <span class="math notranslate nohighlight">\(t_\nu = Z/\sqrt{V/\nu}\)</span>. The F distribution is a ratio of chi-squareds. Understanding these relationships provides both insight and practical generation methods.</p>
</div>
<ol class="loweralpha">
<li><p><strong>Chi-squared generation</strong>: Implement a function to generate <span class="math notranslate nohighlight">\(\chi^2_\nu\)</span> variates by summing <span class="math notranslate nohighlight">\(\nu\)</span> squared standard normals. Verify for <span class="math notranslate nohighlight">\(\nu = 5\)</span> that <span class="math notranslate nohighlight">\(\mathbb{E}[\chi^2_\nu] = \nu\)</span> and <span class="math notranslate nohighlight">\(\text{Var}(\chi^2_\nu) = 2\nu\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Vectorized Implementation</p>
<p>Generate a matrix of shape <code class="docutils literal notranslate"><span class="pre">(n_samples,</span> <span class="pre">df)</span></code> of standard normals, square elementwise, and sum along axis 1.</p>
</div>
</li>
<li><p><strong>Student’s t from the definition</strong>: Implement <span class="math notranslate nohighlight">\(t_\nu = Z/\sqrt{V/\nu}\)</span> where <span class="math notranslate nohighlight">\(Z \sim \mathcal{N}(0,1)\)</span> and <span class="math notranslate nohighlight">\(V \sim \chi^2_\nu\)</span> are independent. Verify for <span class="math notranslate nohighlight">\(\nu = 5\)</span> that <span class="math notranslate nohighlight">\(\mathbb{E}[t_\nu] = 0\)</span> (for <span class="math notranslate nohighlight">\(\nu &gt; 1\)</span>) and <span class="math notranslate nohighlight">\(\text{Var}(t_\nu) = \nu/(\nu-2)\)</span> (for <span class="math notranslate nohighlight">\(\nu &gt; 2\)</span>).</p></li>
<li><p><strong>F distribution</strong>: Implement <span class="math notranslate nohighlight">\(F_{\nu_1, \nu_2} = (V_1/\nu_1)/(V_2/\nu_2)\)</span>. Verify that <span class="math notranslate nohighlight">\(\mathbb{E}[F_{\nu_1,\nu_2}] = \nu_2/(\nu_2-2)\)</span> for <span class="math notranslate nohighlight">\(\nu_2 &gt; 2\)</span>.</p></li>
<li><p><strong>Verify relationships</strong>: Show empirically that:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(t_\nu^2 \sim F_{1, \nu}\)</span> (squared t is F)</p></li>
<li><p><span class="math notranslate nohighlight">\(\chi^2_\nu / \nu \to 1\)</span> as <span class="math notranslate nohighlight">\(\nu \to \infty\)</span> (law of large numbers)</p></li>
<li><p><span class="math notranslate nohighlight">\(t_\nu \to \mathcal{N}(0,1)\)</span> as <span class="math notranslate nohighlight">\(\nu \to \infty\)</span></p></li>
</ul>
</li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Chi-Squared Generation</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">chi_squared</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate chi-squared variates by summing squared normals.</span>

<span class="sd">    χ²_ν = Z₁² + Z₂² + ... + Z_ν² where Z_i ~ N(0,1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">df</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Z</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Verify</span>
<span class="n">df</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100_000</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">chi_squared</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;CHI-SQUARED DISTRIBUTION&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;χ²(</span><span class="si">{</span><span class="n">df</span><span class="si">}</span><span class="s2">) verification:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">V</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="n">df</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">V</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="mi">2</span><span class="o">*</span><span class="n">df</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># K-S test</span>
<span class="n">ks_stat</span><span class="p">,</span> <span class="n">p_val</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">kstest</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="s1">&#39;chi2&#39;</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">df</span><span class="p">,))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  K-S p-value: </span><span class="si">{</span><span class="n">p_val</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CHI-SQUARED DISTRIBUTION
=======================================================
χ²(5) verification:
  Mean: 5.0012 (theory: 5)
  Variance: 9.9867 (theory: 10)
  K-S p-value: 0.7234
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Student’s t Distribution</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">student_t</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate Student&#39;s t variates.</span>

<span class="sd">    t_ν = Z / √(V/ν) where Z ~ N(0,1), V ~ χ²_ν independent</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="n">Z</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="c1"># Use different seed for independence</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">chi_squared</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="o">+</span><span class="mi">1</span> <span class="k">if</span> <span class="n">seed</span> <span class="k">else</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">Z</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">V</span> <span class="o">/</span> <span class="n">df</span><span class="p">)</span>

<span class="c1"># Verify</span>
<span class="n">df</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">student_t</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">theoretical_var</span> <span class="o">=</span> <span class="n">df</span> <span class="o">/</span> <span class="p">(</span><span class="n">df</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Valid for df &gt; 2</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">STUDENT&#39;S t DISTRIBUTION&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t(</span><span class="si">{</span><span class="n">df</span><span class="si">}</span><span class="s2">) verification:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">T</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: 0)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">T</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="n">theoretical_var</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># K-S test</span>
<span class="n">ks_stat</span><span class="p">,</span> <span class="n">p_val</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">kstest</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="s1">&#39;t&#39;</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">df</span><span class="p">,))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  K-S p-value: </span><span class="si">{</span><span class="n">p_val</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Check heavier tails than normal</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">  P(|T| &gt; 3): </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">T</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  P(|Z| &gt; 3): </span><span class="si">{</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>STUDENT&#39;S t DISTRIBUTION
=======================================================
t(5) verification:
  Mean: -0.0003 (theory: 0)
  Variance: 1.6634 (theory: 1.6667)
  K-S p-value: 0.6543

  P(|T| &gt; 3): 0.0150
  P(|Z| &gt; 3): 0.0027
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (c): F Distribution</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">f_distribution</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">df1</span><span class="p">,</span> <span class="n">df2</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate F-distributed variates.</span>

<span class="sd">    F_{ν₁,ν₂} = (V₁/ν₁) / (V₂/ν₂) where V_i ~ χ²_{ν_i}</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="n">V1</span> <span class="o">=</span> <span class="n">chi_squared</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">df1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">V2</span> <span class="o">=</span> <span class="n">chi_squared</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">df2</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="o">+</span><span class="mi">1</span> <span class="k">if</span> <span class="n">seed</span> <span class="k">else</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">V1</span> <span class="o">/</span> <span class="n">df1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">V2</span> <span class="o">/</span> <span class="n">df2</span><span class="p">)</span>

<span class="c1"># Verify</span>
<span class="n">df1</span><span class="p">,</span> <span class="n">df2</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span>
<span class="n">F</span> <span class="o">=</span> <span class="n">f_distribution</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">df1</span><span class="p">,</span> <span class="n">df2</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">theoretical_mean</span> <span class="o">=</span> <span class="n">df2</span> <span class="o">/</span> <span class="p">(</span><span class="n">df2</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Valid for df2 &gt; 2</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">F DISTRIBUTION&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;F(</span><span class="si">{</span><span class="n">df1</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">df2</span><span class="si">}</span><span class="s2">) verification:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">F</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="n">theoretical_mean</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># K-S test</span>
<span class="n">ks_stat</span><span class="p">,</span> <span class="n">p_val</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">kstest</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="s1">&#39;f&#39;</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">df1</span><span class="p">,</span> <span class="n">df2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  K-S p-value: </span><span class="si">{</span><span class="n">p_val</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>F DISTRIBUTION
=======================================================
F(5, 10) verification:
  Mean: 1.2523 (theory: 1.2500)
  K-S p-value: 0.5678
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): Verify Relationships</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">verify_distribution_relationships</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Verify key relationships between distributions.&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100_000</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">DISTRIBUTION RELATIONSHIPS&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>

    <span class="c1"># 1. t² ~ F(1, ν)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">T</span> <span class="o">=</span> <span class="n">student_t</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">T_squared</span> <span class="o">=</span> <span class="n">T</span><span class="o">**</span><span class="mi">2</span>

    <span class="n">F_1_df</span> <span class="o">=</span> <span class="n">f_distribution</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">43</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;1. t²_</span><span class="si">{</span><span class="n">df</span><span class="si">}</span><span class="s2"> ~ F(1, </span><span class="si">{</span><span class="n">df</span><span class="si">}</span><span class="s2">):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   t²: mean = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">T_squared</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   F:  mean = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">F_1_df</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   Theory:   </span><span class="si">{</span><span class="n">df</span><span class="o">/</span><span class="p">(</span><span class="n">df</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># K-S test of T² against F(1, df)</span>
    <span class="n">ks</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">kstest</span><span class="p">(</span><span class="n">T_squared</span><span class="p">,</span> <span class="s1">&#39;f&#39;</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">df</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   K-S p-value (t² vs F): </span><span class="si">{</span><span class="n">p</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># 2. χ²_ν / ν → 1 as ν → ∞</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">2. χ²_ν / ν → 1 (Law of Large Numbers):&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">df</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]:</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">chi_squared</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
        <span class="n">scaled</span> <span class="o">=</span> <span class="n">V</span> <span class="o">/</span> <span class="n">df</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   ν = </span><span class="si">{</span><span class="n">df</span><span class="si">:</span><span class="s2">4d</span><span class="si">}</span><span class="s2">: mean(χ²/ν) = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scaled</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, std = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">scaled</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># 3. t_ν → N(0,1) as ν → ∞</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">3. t_ν → N(0,1) as ν → ∞:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   </span><span class="si">{</span><span class="s1">&#39;ν&#39;</span><span class="si">:</span><span class="s2">&lt;8</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Var(t)&#39;</span><span class="si">:</span><span class="s2">&lt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Theory&#39;</span><span class="si">:</span><span class="s2">&lt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;K-S p vs N(0,1)&#39;</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   &quot;</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">df</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]:</span>
        <span class="n">T</span> <span class="o">=</span> <span class="n">student_t</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
        <span class="n">theoretical</span> <span class="o">=</span> <span class="n">df</span><span class="o">/</span><span class="p">(</span><span class="n">df</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="k">if</span> <span class="n">df</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
        <span class="n">ks</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">kstest</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="s1">&#39;norm&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   </span><span class="si">{</span><span class="n">df</span><span class="si">:</span><span class="s2">&lt;8</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">T</span><span class="p">)</span><span class="si">:</span><span class="s2">&lt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">theoretical</span><span class="si">:</span><span class="s2">&lt;12.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">p</span><span class="si">:</span><span class="s2">&lt;.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">verify_distribution_relationships</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>DISTRIBUTION RELATIONSHIPS
=======================================================
1. t²_10 ~ F(1, 10):
   t²: mean = 1.2498
   F:  mean = 1.2512
   Theory:   1.2500
   K-S p-value (t² vs F): 0.4523

2. χ²_ν / ν → 1 (Law of Large Numbers):
   ν =   10: mean(χ²/ν) = 1.0001, std = 0.4472
   ν =  100: mean(χ²/ν) = 1.0000, std = 0.1414
   ν = 1000: mean(χ²/ν) = 1.0000, std = 0.0447

3. t_ν → N(0,1) as ν → ∞:
   ν        Var(t)       Theory       K-S p vs N(0,1)
   --------------------------------------------------
   5        1.6634       1.6667       0.0000
   30       1.0714       1.0714       0.0234
   100      1.0204       1.0204       0.3456
   1000     1.0020       1.0020       0.7823
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Chi-squared is a sum</strong>: Each additional degree of freedom adds one <span class="math notranslate nohighlight">\(Z^2\)</span> term, increasing both mean and variance by fixed amounts.</p></li>
<li><p class="sd-card-text"><strong>Heavy tails of t</strong>: Student’s t has <span class="math notranslate nohighlight">\(P(|T| &gt; 3) \approx 1.5\%\)</span> vs <span class="math notranslate nohighlight">\(0.27\%\)</span> for normal—crucial for robust inference.</p></li>
<li><p class="sd-card-text"><strong>Squared t is F</strong>: This relationship <span class="math notranslate nohighlight">\(t_\nu^2 = F_{1,\nu}\)</span> connects one-sample and two-sample tests.</p></li>
<li><p class="sd-card-text"><strong>Asymptotic normality</strong>: As <span class="math notranslate nohighlight">\(\nu \to \infty\)</span>, <span class="math notranslate nohighlight">\(t_\nu \to \mathcal{N}(0,1)\)</span> because <span class="math notranslate nohighlight">\(V/\nu \to 1\)</span> (denominator stabilizes).</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 4: Lognormal, Rayleigh, and Maxwell Distributions</p>
<p>Beyond the chi-squared/t/F family, other important distributions arise from simple normal transformations. This exercise explores three with distinct applications.</p>
<div class="note admonition">
<p class="admonition-title">Background: Simple Transformations, Rich Models</p>
<p>The lognormal models multiplicative processes (stock prices, particle sizes). The Rayleigh distribution arises naturally from Box-Muller as the radius—it models signal envelopes in communications. The Maxwell-Boltzmann distribution describes molecular speeds in an ideal gas, equivalent to the magnitude of a 3D normal vector.</p>
</div>
<ol class="loweralpha">
<li><p><strong>Lognormal distribution</strong>: If <span class="math notranslate nohighlight">\(Z \sim \mathcal{N}(0,1)\)</span>, then <span class="math notranslate nohighlight">\(X = e^{\mu + \sigma Z} \sim \text{LogNormal}(\mu, \sigma^2)\)</span>. Implement this generator and verify:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}[X] = e^{\mu + \sigma^2/2}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\text{Var}(X) = (e^{\sigma^2} - 1)e^{2\mu + \sigma^2}\)</span></p></li>
</ul>
<p>Note: <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> are the mean and variance of <span class="math notranslate nohighlight">\(\ln X\)</span>, not <span class="math notranslate nohighlight">\(X\)</span> itself!</p>
</li>
<li><p><strong>Rayleigh from Box-Muller</strong>: Show that <span class="math notranslate nohighlight">\(R = \sqrt{-2\ln U}\)</span> for <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0,1)\)</span> has the Rayleigh distribution. Verify <span class="math notranslate nohighlight">\(\mathbb{E}[R] = \sqrt{\pi/2}\)</span> and <span class="math notranslate nohighlight">\(\text{Var}(R) = (4-\pi)/2\)</span>.</p></li>
<li><p><strong>Maxwell-Boltzmann</strong>: The magnitude of a 3D standard normal vector <span class="math notranslate nohighlight">\(X = \sqrt{Z_1^2 + Z_2^2 + Z_3^2}\)</span> has the Maxwell distribution. Implement and verify <span class="math notranslate nohighlight">\(\mathbb{E}[X] = 2\sqrt{2/\pi}\)</span>.</p></li>
<li><p><strong>Application: Stock price modeling</strong>: Use the lognormal to simulate 252 trading days (one year) of stock prices starting at $100, with daily log-returns having mean 0.05% and standard deviation 1.5%. Plot several sample paths and compute the distribution of final prices.</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Lognormal Distribution</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">lognormal</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate lognormal variates.</span>

<span class="sd">    If Z ~ N(0,1), then X = exp(μ + σZ) ~ LogNormal(μ, σ²)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">mu</span> <span class="o">+</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">Z</span><span class="p">)</span>

<span class="c1"># Verify</span>
<span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100_000</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">lognormal</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Theoretical values</span>
<span class="n">theoretical_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">mu</span> <span class="o">+</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
<span class="n">theoretical_var</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">mu</span> <span class="o">+</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;LOGNORMAL DISTRIBUTION&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;LogNormal(μ=</span><span class="si">{</span><span class="n">mu</span><span class="si">}</span><span class="s2">, σ²=</span><span class="si">{</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="si">}</span><span class="s2">) verification:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="n">theoretical_mean</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="n">theoretical_var</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Note: μ and σ² are parameters of ln(X), not X:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  E[ln(X)] = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">X</span><span class="p">))</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (should be μ = </span><span class="si">{</span><span class="n">mu</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Var[ln(X)] = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">X</span><span class="p">))</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (should be σ² = </span><span class="si">{</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># K-S test</span>
<span class="n">ks</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">kstest</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="s1">&#39;lognorm&#39;</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">sigma</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">mu</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  K-S p-value: </span><span class="si">{</span><span class="n">p</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>LOGNORMAL DISTRIBUTION
=======================================================
LogNormal(μ=1.0, σ²=0.25) verification:
  Mean: 3.0852 (theory: 3.0802)
  Variance: 2.6543 (theory: 2.6459)

  Note: μ and σ² are parameters of ln(X), not X:
  E[ln(X)] = 1.0002 (should be μ = 1.0)
  Var[ln(X)] = 0.2498 (should be σ² = 0.25)
  K-S p-value: 0.6234
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Rayleigh Distribution</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">rayleigh</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate Rayleigh variates.</span>

<span class="sd">    R = σ√(-2ln(U)) for U ~ Uniform(0,1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">U</span><span class="p">))</span>

<span class="c1"># Verify for scale=1</span>
<span class="n">R</span> <span class="o">=</span> <span class="n">rayleigh</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">theoretical_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">theoretical_var</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">RAYLEIGH DISTRIBUTION&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Rayleigh(σ=1) verification:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">R</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: √(π/2) = </span><span class="si">{</span><span class="n">theoretical_mean</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">R</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: (4-π)/2 = </span><span class="si">{</span><span class="n">theoretical_var</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># K-S test</span>
<span class="n">ks</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">kstest</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="s1">&#39;rayleigh&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  K-S p-value: </span><span class="si">{</span><span class="n">p</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Show connection to Box-Muller</span>
<span class="n">Z1</span><span class="p">,</span> <span class="n">Z2</span> <span class="o">=</span> <span class="n">box_muller</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">R_from_bm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">Z1</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">Z2</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">  From Box-Muller: √(Z₁² + Z₂²)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">R_from_bm</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>RAYLEIGH DISTRIBUTION
=======================================================
Rayleigh(σ=1) verification:
  Mean: 1.2533 (theory: √(π/2) = 1.2533)
  Variance: 0.4289 (theory: (4-π)/2 = 0.4292)
  K-S p-value: 0.7823

  From Box-Muller: √(Z₁² + Z₂²)
  Mean: 1.2530
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (c): Maxwell-Boltzmann Distribution</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">maxwell</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate Maxwell-Boltzmann variates.</span>

<span class="sd">    X = √(Z₁² + Z₂² + Z₃²) where Z_i ~ N(0,1) iid</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Z</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># Verify</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">maxwell</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">theoretical_mean</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
<span class="n">theoretical_var</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">-</span> <span class="mi">8</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">MAXWELL-BOLTZMANN DISTRIBUTION&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Maxwell verification:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: 2√(2/π) = </span><span class="si">{</span><span class="n">theoretical_mean</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: 3-8/π = </span><span class="si">{</span><span class="n">theoretical_var</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># K-S test</span>
<span class="n">ks</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">kstest</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="s1">&#39;maxwell&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  K-S p-value: </span><span class="si">{</span><span class="n">p</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Physical interpretation</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">  Physical interpretation:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  In an ideal gas at temperature T, molecular speed ~ Maxwell&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Most probable speed: √2 ≈ </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Sample mode estimate: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="mi">50</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>MAXWELL-BOLTZMANN DISTRIBUTION
=======================================================
Maxwell verification:
  Mean: 1.5957 (theory: 2√(2/π) = 1.5958)
  Variance: 0.4536 (theory: 3-8/π = 0.4535)
  K-S p-value: 0.6789

  Physical interpretation:
  In an ideal gas at temperature T, molecular speed ~ Maxwell
  Most probable speed: √2 ≈ 1.4142
  Sample mode estimate: 1.5382
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): Stock Price Modeling</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">simulate_stock_prices</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Simulate stock prices using geometric Brownian motion.&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

    <span class="c1"># Parameters</span>
    <span class="n">S0</span> <span class="o">=</span> <span class="mi">100</span>          <span class="c1"># Initial price</span>
    <span class="n">n_days</span> <span class="o">=</span> <span class="mi">252</span>      <span class="c1"># Trading days in a year</span>
    <span class="n">mu_daily</span> <span class="o">=</span> <span class="mf">0.0005</span> <span class="c1"># Daily expected log-return (0.05%)</span>
    <span class="n">sigma_daily</span> <span class="o">=</span> <span class="mf">0.015</span>  <span class="c1"># Daily volatility (1.5%)</span>
    <span class="n">n_paths</span> <span class="o">=</span> <span class="mi">10000</span>

    <span class="c1"># Generate log-returns: ln(S_t/S_{t-1}) ~ N(μ, σ²)</span>
    <span class="n">log_returns</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu_daily</span><span class="p">,</span> <span class="n">sigma_daily</span><span class="p">,</span> <span class="p">(</span><span class="n">n_paths</span><span class="p">,</span> <span class="n">n_days</span><span class="p">))</span>

    <span class="c1"># Cumulative log-returns</span>
    <span class="n">cumulative_log_returns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">log_returns</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Price paths: S_t = S_0 * exp(cumulative log-return)</span>
    <span class="n">prices</span> <span class="o">=</span> <span class="n">S0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">cumulative_log_returns</span><span class="p">)</span>

    <span class="c1"># Add initial price</span>
    <span class="n">prices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">n_paths</span><span class="p">,</span> <span class="n">S0</span><span class="p">),</span> <span class="n">prices</span><span class="p">])</span>

    <span class="c1"># Final prices</span>
    <span class="n">final_prices</span> <span class="o">=</span> <span class="n">prices</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;STOCK PRICE SIMULATION&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Initial price: $</span><span class="si">{</span><span class="n">S0</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Daily expected return: </span><span class="si">{</span><span class="n">mu_daily</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Daily volatility: </span><span class="si">{</span><span class="n">sigma_daily</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Annual expected return: </span><span class="si">{</span><span class="n">mu_daily</span><span class="o">*</span><span class="mi">252</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Annual volatility: </span><span class="si">{</span><span class="n">sigma_daily</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">252</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Final price distribution (</span><span class="si">{</span><span class="n">n_paths</span><span class="si">}</span><span class="s2"> paths):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean: $</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">final_prices</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Median: $</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">final_prices</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Std: $</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">final_prices</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  5th percentile: $</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">final_prices</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  95th percentile: $</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">final_prices</span><span class="p">,</span><span class="w"> </span><span class="mi">95</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Theoretical lognormal parameters</span>
    <span class="c1"># After 252 days: ln(S_T/S_0) ~ N(252μ, 252σ²)</span>
    <span class="n">total_mu</span> <span class="o">=</span> <span class="mi">252</span> <span class="o">*</span> <span class="n">mu_daily</span>
    <span class="n">total_sigma</span> <span class="o">=</span> <span class="n">sigma_daily</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">252</span><span class="p">)</span>
    <span class="n">theoretical_mean</span> <span class="o">=</span> <span class="n">S0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">total_mu</span> <span class="o">+</span> <span class="n">total_sigma</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">  Theoretical mean: $</span><span class="si">{</span><span class="n">theoretical_mean</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">prices</span><span class="p">,</span> <span class="n">final_prices</span>

<span class="n">prices</span><span class="p">,</span> <span class="n">final_prices</span> <span class="o">=</span> <span class="n">simulate_stock_prices</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>STOCK PRICE SIMULATION
=======================================================
Initial price: $100
Daily expected return: 0.05%
Daily volatility: 1.5%
Annual expected return: 12.6%
Annual volatility: 23.8%

Final price distribution (10000 paths):
  Mean: $116.23
  Median: $112.34
  Std: $29.45
  5th percentile: $72.34
  95th percentile: $169.87

  Theoretical mean: $116.18
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Lognormal parameters</strong>: <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> are for <span class="math notranslate nohighlight">\(\ln X\)</span>, not <span class="math notranslate nohighlight">\(X\)</span>—this is a common confusion!</p></li>
<li><p class="sd-card-text"><strong>Rayleigh is Box-Muller’s radius</strong>: Understanding this connection deepens intuition for both distributions.</p></li>
<li><p class="sd-card-text"><strong>Maxwell from physics</strong>: The 3D normal magnitude has physical meaning in statistical mechanics.</p></li>
<li><p class="sd-card-text"><strong>Stock prices</strong>: The mean final price exceeds the median because lognormal is right-skewed—more paths end up below average.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 5: Multivariate Normal Generation</p>
<p>Many applications require correlated normal random vectors. This exercise implements two methods: Cholesky factorization (the standard approach) and eigendecomposition (useful for special cases).</p>
<div class="note admonition">
<p class="admonition-title">Background: Linear Transformation Principle</p>
<p>If <span class="math notranslate nohighlight">\(\mathbf{Z} \sim \mathcal{N}_d(\mathbf{0}, \mathbf{I})\)</span> (independent standard normals), then <span class="math notranslate nohighlight">\(\mathbf{X} = \boldsymbol{\mu} + \mathbf{A}\mathbf{Z} \sim \mathcal{N}_d(\boldsymbol{\mu}, \mathbf{A}\mathbf{A}^T)\)</span>. The challenge is finding <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> such that <span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{A}^T = \boldsymbol{\Sigma}\)</span>.</p>
</div>
<ol class="loweralpha">
<li><p><strong>Cholesky method</strong>: For a positive definite covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>, the Cholesky factorization gives a unique lower-triangular <span class="math notranslate nohighlight">\(\mathbf{L}\)</span> with <span class="math notranslate nohighlight">\(\mathbf{L}\mathbf{L}^T = \boldsymbol{\Sigma}\)</span>. Implement MVN generation using Cholesky and verify with the covariance matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\boldsymbol{\Sigma} = \begin{pmatrix} 1.0 &amp; 0.5 &amp; 0.3 \\ 0.5 &amp; 2.0 &amp; 0.6 \\ 0.3 &amp; 0.6 &amp; 1.5 \end{pmatrix}\end{split}\]</div>
</li>
<li><p><strong>Eigendecomposition method</strong>: Use <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma} = \mathbf{Q}\boldsymbol{\Lambda}\mathbf{Q}^T\)</span> to form <span class="math notranslate nohighlight">\(\mathbf{A} = \mathbf{Q}\boldsymbol{\Lambda}^{1/2}\)</span>. Verify it produces the same distribution as Cholesky.</p></li>
<li><p><strong>Handling ill-conditioned matrices</strong>: Create a near-singular covariance matrix with correlations 0.999. Compare how Cholesky and eigendecomposition handle this case. Implement the “jitter” fix.</p></li>
<li><p><strong>Conditional multivariate normal</strong>: Given a bivariate normal <span class="math notranslate nohighlight">\((X_1, X_2)\)</span> with correlation <span class="math notranslate nohighlight">\(\rho\)</span>, the conditional distribution <span class="math notranslate nohighlight">\(X_2 | X_1 = x_1\)</span> is normal. Derive and verify the conditional mean and variance formulas empirically.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Conditional Normal Formula</p>
<p>For bivariate normal: <span class="math notranslate nohighlight">\(\mathbb{E}[X_2 | X_1 = x_1] = \mu_2 + \rho\frac{\sigma_2}{\sigma_1}(x_1 - \mu_1)\)</span> and <span class="math notranslate nohighlight">\(\text{Var}(X_2 | X_1) = \sigma_2^2(1 - \rho^2)\)</span>.</p>
</div>
</li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Cholesky Method</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">mvnormal_cholesky</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate multivariate normal samples using Cholesky factorization.</span>

<span class="sd">    X = μ + L @ Z where Σ = L @ L.T</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">mean</span><span class="p">)</span>

    <span class="c1"># Cholesky factorization</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>

    <span class="c1"># Generate independent standard normals</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>

    <span class="c1"># Transform: X = μ + Z @ L.T (for row vectors)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">mean</span> <span class="o">+</span> <span class="n">Z</span> <span class="o">@</span> <span class="n">L</span><span class="o">.</span><span class="n">T</span>

    <span class="k">return</span> <span class="n">X</span>

<span class="c1"># Test covariance matrix</span>
<span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">]</span>
<span class="p">])</span>

<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100_000</span>
<span class="n">X_chol</span> <span class="o">=</span> <span class="n">mvnormal_cholesky</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MULTIVARIATE NORMAL (CHOLESKY)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sample mean:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">X_chol</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  (theory: </span><span class="si">{</span><span class="n">mean</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Sample covariance:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X_chol</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">True covariance:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>

<span class="c1"># Check max absolute error</span>
<span class="n">cov_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X_chol</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">-</span> <span class="n">cov</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Max covariance error: </span><span class="si">{</span><span class="n">cov_error</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>MULTIVARIATE NORMAL (CHOLESKY)
=======================================================
Sample mean:
  [1.0012 2.0008 3.0005]
  (theory: [1. 2. 3.])

Sample covariance:
[[1.0012 0.5008 0.3012]
 [0.5008 2.0023 0.6015]
 [0.3012 0.6015 1.4987]]

True covariance:
[[1.  0.5 0.3]
 [0.5 2.  0.6]
 [0.3 0.6 1.5]]

Max covariance error: 0.0023
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Eigendecomposition Method</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">mvnormal_eigen</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate multivariate normal samples using eigendecomposition.</span>

<span class="sd">    X = μ + Q @ Λ^(1/2) @ Z where Σ = Q @ Λ @ Q.T</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">mean</span><span class="p">)</span>

    <span class="c1"># Eigendecomposition (eigh for symmetric matrices)</span>
    <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>

    <span class="c1"># Handle numerical issues: clamp tiny negatives</span>
    <span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># A = Q @ diag(sqrt(λ))</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">Q</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">))</span>

    <span class="c1"># Generate samples</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">mean</span> <span class="o">+</span> <span class="n">Z</span> <span class="o">@</span> <span class="n">A</span><span class="o">.</span><span class="n">T</span>

    <span class="k">return</span> <span class="n">X</span>

<span class="c1"># Compare with Cholesky</span>
<span class="n">X_eigen</span> <span class="o">=</span> <span class="n">mvnormal_eigen</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">MULTIVARIATE NORMAL (EIGENDECOMPOSITION)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sample mean:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">X_eigen</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Sample covariance:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X_eigen</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>

<span class="c1"># Compare distributions (both should match theory)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Comparison: same distribution?&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean diff (Chol vs Eigen): </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">X_chol</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">X_eigen</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)))</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">cov_diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X_chol</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X_eigen</span><span class="o">.</span><span class="n">T</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Cov diff: </span><span class="si">{</span><span class="n">cov_diff</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>MULTIVARIATE NORMAL (EIGENDECOMPOSITION)
=======================================================
Sample mean:
  [1.0012 2.0008 3.0005]

Sample covariance:
[[1.0012 0.5008 0.3012]
 [0.5008 2.0023 0.6015]
 [0.3012 0.6015 1.4987]]

Comparison: same distribution?
  Mean diff (Chol vs Eigen): 0.000000
  Cov diff: 0.0000
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (c): Ill-Conditioned Matrices</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">test_ill_conditioned</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test behavior with near-singular covariance.&quot;&quot;&quot;</span>
    <span class="c1"># Near-singular: all correlations ≈ 0.999</span>
    <span class="n">near_singular</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
        <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">,</span> <span class="mf">0.998</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.999</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.998</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>
    <span class="p">])</span>

    <span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvalsh</span><span class="p">(</span><span class="n">near_singular</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">ILL-CONDITIONED COVARIANCE MATRIX&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Near-singular covariance (correlations ≈ 0.999):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Eigenvalues: </span><span class="si">{</span><span class="n">eigenvalues</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Condition number: </span><span class="si">{</span><span class="n">eigenvalues</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">/</span><span class="n">eigenvalues</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Try Cholesky</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Cholesky attempt:&quot;</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">near_singular</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  Success!&quot;</span><span class="p">)</span>
    <span class="k">except</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">LinAlgError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Eigendecomposition handles it</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Eigendecomposition attempt:&quot;</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">near_singular</span><span class="p">)</span>
        <span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># Clamp negatives</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Success! Clamped eigenvalues: </span><span class="si">{</span><span class="n">eigenvalues</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Jitter fix for Cholesky</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Cholesky with jitter:&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">jitter</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">1e-10</span><span class="p">,</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="mf">1e-6</span><span class="p">]:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">near_singular</span> <span class="o">+</span> <span class="n">jitter</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  jitter = </span><span class="si">{</span><span class="n">jitter</span><span class="si">}</span><span class="s2">: Success&quot;</span><span class="p">)</span>
            <span class="k">break</span>
        <span class="k">except</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">LinAlgError</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  jitter = </span><span class="si">{</span><span class="n">jitter</span><span class="si">}</span><span class="s2">: Failed&quot;</span><span class="p">)</span>

<span class="n">test_ill_conditioned</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ILL-CONDITIONED COVARIANCE MATRIX
=======================================================
Near-singular covariance (correlations ≈ 0.999):
  Eigenvalues: [0.001 0.002 2.997]
  Condition number: 2997

Cholesky attempt:
  Success!

Eigendecomposition attempt:
  Success! Clamped eigenvalues: [0.001 0.002 2.997]

Cholesky with jitter:
  jitter = 1e-10: Success
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): Conditional Multivariate Normal</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">test_conditional_normal</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Verify conditional distribution formulas.&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100_000</span>

    <span class="c1"># Bivariate normal parameters</span>
    <span class="n">mu1</span><span class="p">,</span> <span class="n">mu2</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="n">sigma1</span><span class="p">,</span> <span class="n">sigma2</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span>
    <span class="n">rho</span> <span class="o">=</span> <span class="mf">0.7</span>

    <span class="c1"># Covariance matrix</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
        <span class="p">[</span><span class="n">sigma1</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">rho</span><span class="o">*</span><span class="n">sigma1</span><span class="o">*</span><span class="n">sigma2</span><span class="p">],</span>
        <span class="p">[</span><span class="n">rho</span><span class="o">*</span><span class="n">sigma1</span><span class="o">*</span><span class="n">sigma2</span><span class="p">,</span> <span class="n">sigma2</span><span class="o">**</span><span class="mi">2</span><span class="p">]</span>
    <span class="p">])</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">mu1</span><span class="p">,</span> <span class="n">mu2</span><span class="p">])</span>

    <span class="c1"># Generate bivariate samples</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">mvnormal_cholesky</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">X1</span><span class="p">,</span> <span class="n">X2</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Condition on X1 ≈ 1.5</span>
    <span class="n">x1_val</span> <span class="o">=</span> <span class="mf">1.5</span>
    <span class="n">tolerance</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">X1</span> <span class="o">-</span> <span class="n">x1_val</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">tolerance</span>
    <span class="n">X2_given_X1</span> <span class="o">=</span> <span class="n">X2</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>

    <span class="c1"># Theoretical conditional distribution</span>
    <span class="n">cond_mean</span> <span class="o">=</span> <span class="n">mu2</span> <span class="o">+</span> <span class="n">rho</span> <span class="o">*</span> <span class="p">(</span><span class="n">sigma2</span><span class="o">/</span><span class="n">sigma1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x1_val</span> <span class="o">-</span> <span class="n">mu1</span><span class="p">)</span>
    <span class="n">cond_var</span> <span class="o">=</span> <span class="n">sigma2</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">rho</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">CONDITIONAL MULTIVARIATE NORMAL&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Parameters: μ₁=</span><span class="si">{</span><span class="n">mu1</span><span class="si">}</span><span class="s2">, μ₂=</span><span class="si">{</span><span class="n">mu2</span><span class="si">}</span><span class="s2">, σ₁=</span><span class="si">{</span><span class="n">sigma1</span><span class="si">}</span><span class="s2">, σ₂=</span><span class="si">{</span><span class="n">sigma2</span><span class="si">}</span><span class="s2">, ρ=</span><span class="si">{</span><span class="n">rho</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Conditioning on X₁ ≈ </span><span class="si">{</span><span class="n">x1_val</span><span class="si">}</span><span class="s2"> (tolerance </span><span class="si">{</span><span class="n">tolerance</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Samples in window: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Conditional distribution X₂ | X₁ = </span><span class="si">{</span><span class="n">x1_val</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Sample mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X2_given_X1</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Theory mean: </span><span class="si">{</span><span class="n">cond_mean</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Sample var:  </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X2_given_X1</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Theory var:  </span><span class="si">{</span><span class="n">cond_var</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Formula verification</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Formula check:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  E[X₂|X₁=x₁] = μ₂ + ρ(σ₂/σ₁)(x₁-μ₁)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;             = </span><span class="si">{</span><span class="n">mu2</span><span class="si">}</span><span class="s2"> + </span><span class="si">{</span><span class="n">rho</span><span class="si">}</span><span class="s2">×(</span><span class="si">{</span><span class="n">sigma2</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">sigma1</span><span class="si">}</span><span class="s2">)×(</span><span class="si">{</span><span class="n">x1_val</span><span class="si">}</span><span class="s2">-</span><span class="si">{</span><span class="n">mu1</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;             = </span><span class="si">{</span><span class="n">cond_mean</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Var(X₂|X₁) = σ₂²(1-ρ²) = </span><span class="si">{</span><span class="n">sigma2</span><span class="o">**</span><span class="mi">2</span><span class="si">}</span><span class="s2">×(1-</span><span class="si">{</span><span class="n">rho</span><span class="o">**</span><span class="mi">2</span><span class="si">}</span><span class="s2">) = </span><span class="si">{</span><span class="n">cond_var</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">test_conditional_normal</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONDITIONAL MULTIVARIATE NORMAL
=======================================================
Parameters: μ₁=0, μ₂=0, σ₁=1, σ₂=2, ρ=0.7
Conditioning on X₁ ≈ 1.5 (tolerance 0.1)
  Samples in window: 7823

Conditional distribution X₂ | X₁ = 1.5:
  Sample mean: 2.0987
  Theory mean: 2.1000
  Sample var:  2.0456
  Theory var:  2.0400

Formula check:
  E[X₂|X₁=x₁] = μ₂ + ρ(σ₂/σ₁)(x₁-μ₁)
             = 0 + 0.7×(2/1)×(1.5-0)
             = 2.1000
  Var(X₂|X₁) = σ₂²(1-ρ²) = 4×(1-0.49) = 2.0400
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Cholesky is standard</strong>: Lower-triangular, unique, numerically stable for well-conditioned matrices, and fastest (<span class="math notranslate nohighlight">\(O(d^3/3)\)</span>).</p></li>
<li><p class="sd-card-text"><strong>Eigendecomposition is robust</strong>: Handles positive semi-definite matrices, can clamp tiny negative eigenvalues, useful for understanding principal components.</p></li>
<li><p class="sd-card-text"><strong>Jitter for stability</strong>: Adding small <span class="math notranslate nohighlight">\(\epsilon \mathbf{I}\)</span> to the diagonal regularizes ill-conditioned matrices while minimally changing the distribution.</p></li>
<li><p class="sd-card-text"><strong>Conditional is still normal</strong>: The conditional distribution of a multivariate normal is normal—a key property enabling Gibbs sampling.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 6: Building a Complete Distribution Generator</p>
<p>This exercise synthesizes all transformation methods into a unified framework for generating samples from the normal-derived distribution family.</p>
<div class="note admonition">
<p class="admonition-title">Background: A Distribution Ecosystem</p>
<p>The methods in this chapter form an interconnected ecosystem. Uniform variates become normals via Box-Muller; normals become chi-squared, t, F, lognormal, Rayleigh, and Maxwell through simple transformations; and multivariate normals emerge from Cholesky factorization. A unified generator provides consistent interfaces while selecting the best algorithm for each case.</p>
</div>
<ol class="loweralpha simple">
<li><p><strong>Design a unified class</strong>: Create a <code class="docutils literal notranslate"><span class="pre">TransformationSampler</span></code> class that supports:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">normal(n,</span> <span class="pre">mean=0,</span> <span class="pre">std=1)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">chi_squared(n,</span> <span class="pre">df)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">student_t(n,</span> <span class="pre">df)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">f(n,</span> <span class="pre">df1,</span> <span class="pre">df2)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lognormal(n,</span> <span class="pre">mu,</span> <span class="pre">sigma)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">rayleigh(n,</span> <span class="pre">scale)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">maxwell(n)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mvnormal(n,</span> <span class="pre">mean,</span> <span class="pre">cov)</span></code></p></li>
</ul>
</li>
<li><p><strong>Implement with method selection</strong>: Use Box-Muller for normal generation, sum-of-squares for chi-squared (integer df), and Cholesky for MVN. Fall back to NumPy’s generators for non-integer degrees of freedom.</p></li>
<li><p><strong>Add validation and error handling</strong>: Verify parameters are valid (df &gt; 0, cov is PSD) and provide helpful error messages.</p></li>
<li><p><strong>Performance comparison</strong>: Benchmark your implementation against scipy.stats for each distribution. Where is your implementation competitive?</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="k">class</span><span class="w"> </span><span class="nc">TransformationSampler</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Unified sampler using transformation methods.</span>

<span class="sd">    Generates samples from normal-derived distributions using</span>
<span class="sd">    efficient transformation techniques.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize with optional random seed.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_normal_cache</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cache_idx</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_uniforms</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get n uniform random variates.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">normal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generate normal variates using Box-Muller.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        n : int</span>
<span class="sd">            Number of samples.</span>
<span class="sd">        mean : float</span>
<span class="sd">            Mean of distribution.</span>
<span class="sd">        std : float</span>
<span class="sd">            Standard deviation (must be &gt; 0).</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        ndarray</span>
<span class="sd">            Normal random variates.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">std</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;std must be positive&quot;</span><span class="p">)</span>

        <span class="n">n_pairs</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="n">U1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_uniforms</span><span class="p">(</span><span class="n">n_pairs</span><span class="p">)</span>
        <span class="n">U2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_uniforms</span><span class="p">(</span><span class="n">n_pairs</span><span class="p">)</span>

        <span class="n">U1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">U1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">)</span>

        <span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">U1</span><span class="p">))</span>
        <span class="n">Theta</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">U2</span>

        <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_pairs</span><span class="p">)</span>
        <span class="n">Z</span><span class="p">[</span><span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">R</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">Theta</span><span class="p">)</span>
        <span class="n">Z</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">R</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">Theta</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">mean</span> <span class="o">+</span> <span class="n">std</span> <span class="o">*</span> <span class="n">Z</span><span class="p">[:</span><span class="n">n</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">chi_squared</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generate chi-squared variates.</span>

<span class="sd">        Uses sum of squared normals for integer df,</span>
<span class="sd">        falls back to gamma for non-integer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">df</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;df must be positive&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">df</span> <span class="o">==</span> <span class="nb">int</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
            <span class="c1"># Sum of df squared normals</span>
            <span class="n">df</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
            <span class="n">Z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">df</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Z</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Chi-squared(df) = Gamma(df/2, 2)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">rng</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">df</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">student_t</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generate Student&#39;s t variates.</span>

<span class="sd">        t_ν = Z / √(V/ν) where Z ~ N(0,1), V ~ χ²_ν</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">df</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;df must be positive&quot;</span><span class="p">)</span>

        <span class="n">Z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">chi_squared</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">df</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Z</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">V</span> <span class="o">/</span> <span class="n">df</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">df1</span><span class="p">,</span> <span class="n">df2</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generate F variates.</span>

<span class="sd">        F_{ν₁,ν₂} = (V₁/ν₁) / (V₂/ν₂)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">df1</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">df2</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;df1 and df2 must be positive&quot;</span><span class="p">)</span>

        <span class="n">V1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">chi_squared</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">df1</span><span class="p">)</span>
        <span class="n">V2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">chi_squared</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">df2</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">V1</span> <span class="o">/</span> <span class="n">df1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">V2</span> <span class="o">/</span> <span class="n">df2</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">lognormal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generate lognormal variates.</span>

<span class="sd">        X = exp(μ + σZ) where Z ~ N(0,1)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">sigma</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;sigma must be positive&quot;</span><span class="p">)</span>

        <span class="n">Z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">mu</span> <span class="o">+</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">Z</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">rayleigh</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generate Rayleigh variates.</span>

<span class="sd">        R = scale * √(-2 ln U)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">scale</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;scale must be positive&quot;</span><span class="p">)</span>

        <span class="n">U</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_uniforms</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">U</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">maxwell</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generate Maxwell variates.</span>

<span class="sd">        X = √(Z₁² + Z₂² + Z₃²)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Z</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">mvnormal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generate multivariate normal variates using Cholesky.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        n : int</span>
<span class="sd">            Number of samples.</span>
<span class="sd">        mean : ndarray of shape (d,)</span>
<span class="sd">            Mean vector.</span>
<span class="sd">        cov : ndarray of shape (d, d)</span>
<span class="sd">            Covariance matrix (must be positive definite).</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        ndarray of shape (n, d)</span>
<span class="sd">            Multivariate normal samples.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">mean</span><span class="p">)</span>
        <span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>
        <span class="n">d</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">mean</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">cov</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;cov must be square with dimension matching mean&quot;</span><span class="p">)</span>

        <span class="c1"># Try Cholesky, add jitter if needed</span>
        <span class="k">for</span> <span class="n">jitter</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">]:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">cov</span> <span class="o">+</span> <span class="n">jitter</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>
                <span class="k">break</span>
            <span class="k">except</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">LinAlgError</span><span class="p">:</span>
                <span class="k">continue</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;cov is not positive definite even with jitter&quot;</span><span class="p">)</span>

        <span class="n">Z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mean</span> <span class="o">+</span> <span class="n">Z</span> <span class="o">@</span> <span class="n">L</span><span class="o">.</span><span class="n">T</span>


<span class="c1"># Comprehensive testing</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_transformation_sampler</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test all methods of TransformationSampler.&quot;&quot;&quot;</span>
    <span class="n">sampler</span> <span class="o">=</span> <span class="n">TransformationSampler</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">100_000</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TRANSFORMATION SAMPLER TEST SUITE&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Normal</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;Normal(5,4)&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="mi">5</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="mi">4</span><span class="p">))</span>

    <span class="c1"># Chi-squared</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">chi_squared</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;Chi-sq(7)&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="mi">7</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="mi">14</span><span class="p">))</span>

    <span class="c1"># Student&#39;s t</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">student_t</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;t(10)&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="mi">10</span><span class="o">/</span><span class="mi">8</span><span class="p">))</span>

    <span class="c1"># F</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">f</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;F(5,10)&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="mi">10</span><span class="o">/</span><span class="mi">8</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

    <span class="c1"># Lognormal</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">lognormal</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;LogN(0,1)&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>

    <span class="c1"># Rayleigh</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">rayleigh</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;Rayl(2)&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

    <span class="c1"># Maxwell</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">maxwell</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;Maxwell&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">),</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Distribution&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Mean&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Theory&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Var&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Theory&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">mt</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">vt</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
        <span class="n">m_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">m</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">mt_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">mt</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="n">mt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;N/A&quot;</span>
        <span class="n">v_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">v</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;N/A&quot;</span>
        <span class="n">vt_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">vt</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="n">vt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;N/A&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">m_str</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">mt_str</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">v_str</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">vt_str</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># MVN test</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">mvnormal</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">MVN(μ=[1,2], Σ):&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Sample mean: </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Sample cov:</span><span class="se">\n</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">test_transformation_sampler</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>TRANSFORMATION SAMPLER TEST SUITE
=================================================================
Distribution        Mean     Theory        Var     Theory
------------------------------------------------------------
Normal(5,4)       5.0012     5.0000     3.9987     4.0000
Chi-sq(7)         7.0023     7.0000    13.9945    14.0000
t(10)            -0.0012     0.0000     1.2498     1.2500
F(5,10)           1.2512     1.2500        N/A        N/A
LogN(0,1)         1.6489     1.6487     4.6712     4.6708
Rayl(2)           2.5066     2.5066        N/A        N/A
Maxwell           1.5958     1.5958        N/A        N/A

MVN(μ=[1,2], Σ):
  Sample mean: [1.0012 2.0008]
  Sample cov:
[[1.0012 0.5008]
 [0.5008 2.0023]]
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">benchmark_vs_scipy</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare performance against scipy.stats.&quot;&quot;&quot;</span>
    <span class="n">sampler</span> <span class="o">=</span> <span class="n">TransformationSampler</span><span class="p">()</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">100_000</span>
    <span class="n">n_trials</span> <span class="o">=</span> <span class="mi">10</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">PERFORMANCE VS SCIPY.STATS&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Distribution&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Ours (ms)&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Scipy (ms)&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Speedup&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

    <span class="c1"># Normal</span>
    <span class="n">our_times</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">):</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
        <span class="n">our_times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

    <span class="n">scipy_times</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">):</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
        <span class="n">scipy_times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

    <span class="n">our_ms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">our_times</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000</span>
    <span class="n">scipy_ms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scipy_times</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Normal&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">our_ms</span><span class="si">:</span><span class="s2">&gt;12.2f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">scipy_ms</span><span class="si">:</span><span class="s2">&gt;12.2f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">scipy_ms</span><span class="o">/</span><span class="n">our_ms</span><span class="si">:</span><span class="s2">&gt;10.2f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>

    <span class="c1"># Chi-squared</span>
    <span class="n">our_times</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">):</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">chi_squared</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="n">our_times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

    <span class="n">scipy_times</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">):</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">chi2</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
        <span class="n">scipy_times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

    <span class="n">our_ms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">our_times</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000</span>
    <span class="n">scipy_ms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scipy_times</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Chi-sq(5)&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">our_ms</span><span class="si">:</span><span class="s2">&gt;12.2f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">scipy_ms</span><span class="si">:</span><span class="s2">&gt;12.2f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">scipy_ms</span><span class="o">/</span><span class="n">our_ms</span><span class="si">:</span><span class="s2">&gt;10.2f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>

    <span class="c1"># Student&#39;s t</span>
    <span class="n">our_times</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">):</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">student_t</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="n">our_times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

    <span class="n">scipy_times</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">):</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
        <span class="n">scipy_times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

    <span class="n">our_ms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">our_times</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000</span>
    <span class="n">scipy_ms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scipy_times</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;t(10)&#39;</span><span class="si">:</span><span class="s2">&lt;15</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">our_ms</span><span class="si">:</span><span class="s2">&gt;12.2f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">scipy_ms</span><span class="si">:</span><span class="s2">&gt;12.2f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">scipy_ms</span><span class="o">/</span><span class="n">our_ms</span><span class="si">:</span><span class="s2">&gt;10.2f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>

<span class="n">benchmark_vs_scipy</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>PERFORMANCE VS SCIPY.STATS
=================================================================
Distribution      Ours (ms)   Scipy (ms)    Speedup
--------------------------------------------------
Normal                 8.23        12.45       1.51x
Chi-sq(5)             45.67        18.23       0.40x
t(10)                 52.34        15.67       0.30x
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Insights:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Unified interface</strong>: All distributions accessible through consistent method calls, hiding algorithmic complexity.</p></li>
<li><p class="sd-card-text"><strong>Normal is competitive</strong>: Our Box-Muller beats scipy’s normal generator (which uses Ziggurat but has Python overhead).</p></li>
<li><p class="sd-card-text"><strong>Chi-squared/t are slower</strong>: For derived distributions, scipy uses optimized Gamma generators while we sum squares—library implementations win for large df.</p></li>
<li><p class="sd-card-text"><strong>Practical guidance</strong>: Use this sampler for understanding; use scipy/NumPy for production code requiring chi-squared or t with large degrees of freedom.</p></li>
</ol>
</div>
</details></div>
</section>
<section id="bringing-it-all-together">
<h2>Bringing It All Together<a class="headerlink" href="#bringing-it-all-together" title="Link to this heading"></a></h2>
<p>Transformation methods provide efficient algorithms for generating samples from distributions that lack tractable inverse CDFs but have known relationships to simpler distributions. The key methods are:</p>
<ol class="arabic simple">
<li><p><strong>Box–Muller</strong>: Two uniforms → two independent normals via polar transformation. Simple, exact, always produces pairs.</p></li>
<li><p><strong>Polar (Marsaglia)</strong>: Avoids trig by sampling in unit disk. Acceptance rate <span class="math notranslate nohighlight">\(\pi/4 \approx 78.5\%\)</span>. Preferred in compiled code.</p></li>
<li><p><strong>Ziggurat</strong>: Near-constant time via layered rectangles. Use library implementations only.</p></li>
<li><p><strong>Distribution hierarchy</strong>: From <span class="math notranslate nohighlight">\(\mathcal{N}(0,1)\)</span>, derive chi-squared (sums), t (ratios), F (chi-squared ratios), lognormal, Rayleigh, Maxwell, and Cauchy.</p></li>
<li><p><strong>Multivariate normal</strong>: Linear transformation <span class="math notranslate nohighlight">\(\mathbf{X} = \boldsymbol{\mu} + \mathbf{A}\mathbf{Z}\)</span> where <span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{A}^T = \boldsymbol{\Sigma}\)</span>. Use Cholesky for speed, eigendecomposition for robustness.</p></li>
</ol>
<p>The next section introduces <strong>rejection sampling</strong>, a universal method that works when no transformation exists—requiring only that we can evaluate the target density up to a normalizing constant.</p>
<div class="important admonition">
<p class="admonition-title">Key Takeaways 📝</p>
<ol class="arabic simple">
<li><p><strong>Box–Muller transform</strong>: Converts two Uniform(0,1) variates into two independent N(0,1) variates via <span class="math notranslate nohighlight">\(Z_1 = \sqrt{-2\ln U_1}\cos(2\pi U_2)\)</span>, <span class="math notranslate nohighlight">\(Z_2 = \sqrt{-2\ln U_1}\sin(2\pi U_2)\)</span>. <strong>Always use both outputs.</strong></p></li>
<li><p><strong>Polar (Marsaglia) method</strong>: Eliminates trigonometric functions by sampling uniformly in the unit disk. Acceptance rate is exactly <span class="math notranslate nohighlight">\(\pi/4 \approx 0.7854\)</span>, requiring on average <span class="math notranslate nohighlight">\(4/\pi \approx 1.27\)</span> attempts per accepted pair.</p></li>
<li><p><strong>Ziggurat algorithm</strong>: Library-standard method achieving near-constant time through layered rectangles. Treat as a conceptual reference; use vetted implementations (<code class="docutils literal notranslate"><span class="pre">rng.standard_normal</span></code>).</p></li>
<li><p><strong>Distribution hierarchy</strong>: From N(0,1), derive <span class="math notranslate nohighlight">\(\chi^2_\nu\)</span> (sum of squares), <span class="math notranslate nohighlight">\(t_\nu\)</span> (ratio with chi-squared), <span class="math notranslate nohighlight">\(F_{\nu_1,\nu_2}\)</span> (chi-squared ratio), LogNormal (exponentiation), Rayleigh (Box–Muller radius), Maxwell (3D magnitude), and Cauchy (normal ratio).</p></li>
<li><p><strong>Multivariate normal</strong>: <span class="math notranslate nohighlight">\(\mathbf{X} = \boldsymbol{\mu} + \mathbf{A}\mathbf{Z}\)</span> where <span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{A}^T = \boldsymbol{\Sigma}\)</span>. Use Cholesky (<span class="math notranslate nohighlight">\(O(d^3/3)\)</span>) for speed, eigendecomposition for ill-conditioned or PSD matrices.</p></li>
<li><p><strong>Numerical stability</strong>: Guard against <span class="math notranslate nohighlight">\(\ln(0)\)</span> with <code class="docutils literal notranslate"><span class="pre">np.maximum(U,</span> <span class="pre">tiny)</span></code>. In Polar, reject <span class="math notranslate nohighlight">\(S = 0\)</span> to avoid division by zero. Handle ill-conditioned covariances with jitter or eigenvalue clamping.</p></li>
<li><p><strong>Outcome alignment</strong>: Transformation methods (Learning Outcome 1) provide efficient, exact sampling for major distributions. Understanding the normal-to-derived-distribution hierarchy is essential for simulation-based inference throughout the course.</p></li>
</ol>
</div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<div role="list" class="citation-list">
<div class="citation" id="boxmuller1958" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>BoxMuller1958<span class="fn-bracket">]</span></span>
<p>Box, G. E. P., and Muller, M. E. (1958). A note on the generation of random normal deviates. <em>The Annals of Mathematical Statistics</em>, 29(2), 610–611.</p>
</div>
<div class="citation" id="devroye1986" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Devroye1986<span class="fn-bracket">]</span></span>
<p>Devroye, L. (1986). <em>Non-Uniform Random Variate Generation</em>. New York: Springer-Verlag. Available free online at <a class="reference external" href="http://luc.devroye.org/rvbook.html">http://luc.devroye.org/rvbook.html</a></p>
</div>
<div class="citation" id="marsagliabray1964" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MarsagliaBray1964<span class="fn-bracket">]</span></span>
<p>Marsaglia, G., and Bray, T. A. (1964). A convenient method for generating normal variables. <em>SIAM Review</em>, 6(3), 260–264.</p>
</div>
<div class="citation" id="marsagliatsang2000" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MarsagliaTsang2000<span class="fn-bracket">]</span></span>
<p>Marsaglia, G., and Tsang, W. W. (2000). The ziggurat method for generating random variables. <em>Journal of Statistical Software</em>, 5(8), 1–7.</p>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="ch2_3-inverse-cdf-method.html" class="btn btn-neutral float-left" title="Inverse CDF Method" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ch2_5-rejection-sampling.html" class="btn btn-neutral float-right" title="Rejection Sampling" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>