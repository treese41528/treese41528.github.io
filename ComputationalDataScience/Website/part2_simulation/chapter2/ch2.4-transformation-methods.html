

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>2.1.1. Transformation Methods &mdash; STAT 418</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=6826d573" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT418/Website/part2_simulation/chapter2/ch2.4-transformation-methods.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=30646c52"></script>
      <script>window.MathJax = {"options": {"enableMenu": true, "menuOptions": {"settings": {"enrich": true, "speech": true, "braille": true, "collapsible": true, "assistiveMml": false}}}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
      <script src="../../_static/custom.js?v=d2113767"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="2.2. Chapter 3: Frequentist Statistical Inference" href="../chapter3/index.html" />
    <link rel="prev" title="2.1. Chapter 2: Monte Carlo Simulation" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Course Content</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../part1_foundations/index.html">1. Part I: Foundations of Probability and Computation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part1_foundations/chapter1/index.html">1.1. Chapter 1: Statistical Paradigms and Core Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html">1.1.1. Paradigms of Probability and Statistical Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#the-mathematical-foundation-kolmogorov-s-axioms">The Mathematical Foundation: Kolmogorov‚Äôs Axioms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#interpretations-of-probability">Interpretations of Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#statistical-inference-paradigms">Statistical Inference Paradigms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#historical-and-philosophical-debates">Historical and Philosophical Debates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#looking-ahead-our-course-focus">Looking Ahead: Our Course Focus</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.1-probability-and-inference-paradigms.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html">1.1.2. Probability Distributions: Theory and Computation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#from-abstract-foundations-to-concrete-tools">From Abstract Foundations to Concrete Tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#the-python-ecosystem-for-probability">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#introduction-why-probability-distributions-matter">Introduction: Why Probability Distributions Matter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#id1">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#continuous-distributions">Continuous Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#additional-important-distributions">Additional Important Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#summary-and-practical-guidelines">Summary and Practical Guidelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.2-probability_distributions_review.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html">1.1.3. Python Random Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#from-mathematical-distributions-to-computational-samples">From Mathematical Distributions to Computational Samples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-python-ecosystem-at-a-glance">The Python Ecosystem at a Glance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#understanding-pseudo-random-number-generation">Understanding Pseudo-Random Number Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#the-standard-library-random-module">The Standard Library: <code class="docutils literal notranslate"><span class="pre">random</span></code> Module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#numpy-fast-vectorized-random-sampling">NumPy: Fast Vectorized Random Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#scipy-stats-the-complete-statistical-toolkit">SciPy Stats: The Complete Statistical Toolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#bringing-it-all-together-library-selection-guide">Bringing It All Together: Library Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1.3-python_random_generation.html#looking-ahead-from-random-numbers-to-monte-carlo-methods">Looking Ahead: From Random Numbers to Monte Carlo Methods</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">2. Part II: Simulation-Based Methods</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="index.html">2.1. Chapter 2: Monte Carlo Simulation</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">2.1.1. Transformation Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#why-transformation-methods">Why Transformation Methods?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-boxmuller-transform">The Box‚ÄìMuller Transform</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-polar-marsaglia-method">The Polar (Marsaglia) Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-ziggurat-algorithm">The Ziggurat Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-clt-approximation-historical">The CLT Approximation (Historical)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#distributions-derived-from-the-normal">Distributions Derived from the Normal</a></li>
<li class="toctree-l4"><a class="reference internal" href="#infinite-discrete-distributions">Infinite Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multivariate-normal-generation">Multivariate Normal Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bringing-it-all-together">Bringing It All Together</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter3/index.html">2.2. Chapter 3: Frequentist Statistical Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/sampling_variability.html">2.2.1. Sampling Variability</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/sampling_variability.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/statistical_estimators.html">2.2.2. Statistical Estimators</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/statistical_estimators.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/plugin_methods.html">2.2.3. Plugin Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/plugin_methods.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/parametric_inference.html">2.2.4. Parametric Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/parametric_inference.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/exponential_families.html">2.2.5. Exponential Families</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/exponential_families.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/maximum_likelihood.html">2.2.6. Maximum Likelihood</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/maximum_likelihood.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/linear_models.html">2.2.7. Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/linear_models.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/generalized_linear_models.html">2.2.8. Generalized Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/generalized_linear_models.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter4/index.html">2.3. Chapter 4: Resampling Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/jackknife_introduction.html">2.3.1. Jackknife Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/jackknife_introduction.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html">2.3.2. Bootstrap Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bootstrap_fundamentals.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html">2.3.3. Nonparametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/nonparametric_bootstrap.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/parametric_bootstrap.html">2.3.4. Parametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/parametric_bootstrap.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/confidence_intervals.html">2.3.5. Confidence Intervals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/confidence_intervals.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/bias_correction.html">2.3.6. Bias Correction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/bias_correction.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/cross_validation_loo.html">2.3.7. Cross Validation Loo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_loo.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html">2.3.8. Cross Validation K Fold</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/cross_validation_k_fold.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/model_selection.html">2.3.9. Model Selection</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/model_selection.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part3_bayesian/index.html">3. Part III: Bayesian Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part3_bayesian/index.html#overview">3.1. Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html">3.1.1. Bayesian Philosophy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#key-concepts">Key Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#mathematical-framework">Mathematical Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#examples">Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part3_bayesian/chapter5/bayesian_philosophy.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html"><span class="section-number">2. </span>Part II: Simulation-Based Methods</a></li>
          <li class="breadcrumb-item"><a href="index.html"><span class="section-number">2.1. </span>Chapter 2: Monte Carlo Simulation</a></li>
      <li class="breadcrumb-item active"><span class="section-number">2.1.1. </span>Transformation Methods</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/part2_simulation/chapter2/ch2.4-transformation-methods.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="transformation-methods">
<span id="ch2-4-transformation-methods"></span><h1><span class="section-number">2.1.1. </span>Transformation Methods<a class="headerlink" href="#transformation-methods" title="Link to this heading">ÔÉÅ</a></h1>
<p>The inverse CDF method of <a class="reference internal" href="ch2_3-inverse-cdf-method.html#ch2-3-inverse-cdf-method"><span class="std std-ref">Inverse CDF Method</span></a> provides a universal recipe for generating random variables: apply the quantile function to a uniform variate. For many distributions‚Äîexponential, Weibull, Cauchy‚Äîthis approach is both elegant and efficient, requiring only a single transcendental function evaluation. But for others, most notably the normal distribution, the inverse CDF has no closed form. Computing <span class="math notranslate nohighlight">\(\Phi^{-1}(u)\)</span> numerically is possible but expensive, requiring iterative root-finding or careful polynomial approximations.</p>
<p>This computational obstacle motivates an alternative paradigm: <strong>transformation methods</strong>. Rather than inverting the CDF, we seek algebraic or geometric transformations that convert a small number of uniform variates directly into samples from the target distribution. When such transformations exist, they are often faster and more numerically stable than either numerical inversion or the general-purpose rejection sampling we will encounter in <span class="xref std std-ref">ch2.5-rejection-sampling</span>.</p>
<p>The crown jewel of transformation methods is the <strong>Box‚ÄìMuller algorithm</strong> for generating normal random variables. Published in 1958 by George Box and Mervin Muller, this algorithm exploits a remarkable geometric fact: while the one-dimensional normal distribution has an intractable CDF, pairs of independent normals have a simple representation in polar coordinates. Two uniform variates become two independent standard normals through a transformation involving only logarithms, square roots, and trigonometric functions.</p>
<p>From normal random variables, an entire family of distributions becomes accessible through simple arithmetic. The chi-squared distribution emerges as a sum of squared normals. Student‚Äôs t arises from the ratio of a normal to an independent chi-squared. The F distribution, the lognormal, the Rayleigh‚Äîall flow from the normal through elementary transformations. And multivariate normal distributions, essential for Bayesian inference and machine learning, reduce to matrix multiplication once we can generate independent standard normals.</p>
<p>This section develops transformation methods systematically. We begin with the normal distribution, presenting the Box‚ÄìMuller transform, its more efficient polar variant, and a conceptual overview of the library-grade Ziggurat algorithm. We then build the statistical ecosystem that rests on the normal foundation: chi-squared, t, F, lognormal, and related distributions. Finally, we tackle multivariate normal generation, where linear algebra meets random number generation in the form of Cholesky factorization and eigendecomposition.</p>
<div class="important admonition">
<p class="admonition-title">Road Map üß≠</p>
<ul class="simple">
<li><p><strong>Master</strong>: The Box‚ÄìMuller transform‚Äîderivation via change of variables, independence proof, and numerical implementation</p></li>
<li><p><strong>Optimize</strong>: The polar (Marsaglia) method that eliminates trigonometric functions while preserving correctness</p></li>
<li><p><strong>Understand</strong>: The Ziggurat algorithm conceptually as the library-standard approach for normal and exponential generation</p></li>
<li><p><strong>Build</strong>: Chi-squared, Student‚Äôs t, F, lognormal, Rayleigh, and Maxwell distributions from normal building blocks</p></li>
<li><p><strong>Generate</strong>: Infinite discrete distributions (Poisson, Geometric, Negative Binomial) via transformation and mixture methods</p></li>
<li><p><strong>Implement</strong>: Multivariate normal generation via Cholesky factorization and eigendecomposition with attention to numerical stability</p></li>
</ul>
</div>
<section id="why-transformation-methods">
<h2>Why Transformation Methods?<a class="headerlink" href="#why-transformation-methods" title="Link to this heading">ÔÉÅ</a></h2>
<p>Before diving into specific algorithms, we should understand when transformation methods excel and what advantages they offer over the inverse CDF approach.</p>
<section id="speed-through-structure">
<h3>Speed Through Structure<a class="headerlink" href="#speed-through-structure" title="Link to this heading">ÔÉÅ</a></h3>
<p>The inverse CDF method is universal but requires evaluating <span class="math notranslate nohighlight">\(F^{-1}(u)\)</span>. For distributions without closed-form quantile functions, this means:</p>
<ol class="arabic simple">
<li><p><strong>Numerical root-finding</strong>: Solve <span class="math notranslate nohighlight">\(F(x) = u\)</span> iteratively (e.g., Newton-Raphson or Brent‚Äôs method), requiring multiple CDF evaluations per sample.</p></li>
<li><p><strong>Polynomial approximation</strong>: Use carefully crafted rational approximations like those in <code class="docutils literal notranslate"><span class="pre">scipy.special.ndtri</span></code> for the normal quantile. These achieve high accuracy but involve many arithmetic operations.</p></li>
</ol>
<p>Transformation methods sidestep both approaches. The Box‚ÄìMuller algorithm generates two normal variates using:</p>
<ul class="simple">
<li><p>One uniform variate transformation (<span class="math notranslate nohighlight">\(\sqrt{-2\ln U_1}\)</span>)</p></li>
<li><p>One angle computation (<span class="math notranslate nohighlight">\(2\pi U_2\)</span>)</p></li>
<li><p>Two trigonometric evaluations (<span class="math notranslate nohighlight">\(\cos\)</span>, <span class="math notranslate nohighlight">\(\sin\)</span>)</p></li>
</ul>
<p>This is dramatically faster than iterative root-finding and competitive with polynomial approximations‚Äîespecially when we need the paired output.</p>
</section>
<section id="distributional-relationships">
<h3>Distributional Relationships<a class="headerlink" href="#distributional-relationships" title="Link to this heading">ÔÉÅ</a></h3>
<p>Transformation methods exploit mathematical relationships between distributions. The key insight is that many distributions can be expressed as functions of simpler ones:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\chi^2_\nu &amp;= Z_1^2 + Z_2^2 + \cdots + Z_\nu^2 &amp;\quad&amp; (Z_i \stackrel{\text{iid}}{\sim} \mathcal{N}(0,1)) \\
t_\nu &amp;= \frac{Z}{\sqrt{V/\nu}} &amp;\quad&amp; (Z \sim \mathcal{N}(0,1), V \sim \chi^2_\nu) \\
F_{\nu_1, \nu_2} &amp;= \frac{V_1/\nu_1}{V_2/\nu_2} &amp;\quad&amp; (V_i \sim \chi^2_{\nu_i}) \\
\text{LogNormal}(\mu, \sigma^2) &amp;= e^{\mu + \sigma Z} &amp;\quad&amp; (Z \sim \mathcal{N}(0,1))
\end{aligned}\end{split}\]</div>
<p>Once we can generate standard normals efficiently, this entire family becomes computationally accessible. The relationships also provide valuable checks: samples from our chi-squared generator should match the theoretical chi-squared distribution.</p>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_4_fig01_distribution_hierarchy.png"><img alt="Hierarchical diagram showing how distributions derive from uniform and normal variates" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_4_fig01_distribution_hierarchy.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.1 </span><span class="caption-text"><strong>The Distribution Hierarchy.</strong> Starting from Uniform(0,1), we can reach any distribution through transformations. The Box‚ÄìMuller algorithm converts uniforms to normals, unlocking an entire family of derived distributions. Each arrow represents a specific transformation formula. This hierarchy guides algorithm selection: to sample from Student‚Äôs t, generate a normal and a chi-squared, then combine them.</span><a class="headerlink" href="#id2" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="the-boxmuller-transform">
<h2>The Box‚ÄìMuller Transform<a class="headerlink" href="#the-boxmuller-transform" title="Link to this heading">ÔÉÅ</a></h2>
<p>We now present the most important transformation method in computational statistics: the Box‚ÄìMuller algorithm for generating standard normal random variables.</p>
<section id="the-challenge-of-normal-generation">
<h3>The Challenge of Normal Generation<a class="headerlink" href="#the-challenge-of-normal-generation" title="Link to this heading">ÔÉÅ</a></h3>
<p>The standard normal distribution has density:</p>
<div class="math notranslate nohighlight">
\[\phi(z) = \frac{1}{\sqrt{2\pi}} e^{-z^2/2}\]</div>
<p>and CDF:</p>
<div class="math notranslate nohighlight">
\[\Phi(z) = \int_{-\infty}^{z} \frac{1}{\sqrt{2\pi}} e^{-t^2/2} \, dt\]</div>
<p>The integral <span class="math notranslate nohighlight">\(\Phi(z)\)</span> has no closed-form expression in terms of elementary functions. This is not merely a failure to find the right antiderivative‚Äîit can be proven that no such expression exists. Consequently, the inverse CDF <span class="math notranslate nohighlight">\(\Phi^{-1}(u)\)</span> also lacks a closed form, making direct application of the inverse CDF method computationally expensive.</p>
</section>
<section id="the-polar-coordinate-insight">
<h3>The Polar Coordinate Insight<a class="headerlink" href="#the-polar-coordinate-insight" title="Link to this heading">ÔÉÅ</a></h3>
<p>Box and Muller‚Äôs breakthrough came from considering two independent standard normals simultaneously. If <span class="math notranslate nohighlight">\(Z_1, Z_2 \stackrel{\text{iid}}{\sim} \mathcal{N}(0, 1)\)</span>, their joint density is:</p>
<div class="math notranslate nohighlight">
\[f(z_1, z_2) = \phi(z_1) \phi(z_2) = \frac{1}{2\pi} e^{-(z_1^2 + z_2^2)/2}\]</div>
<p>Notice that this depends on <span class="math notranslate nohighlight">\((z_1, z_2)\)</span> only through <span class="math notranslate nohighlight">\(z_1^2 + z_2^2 = r^2\)</span>, the squared distance from the origin. This suggests switching to polar coordinates <span class="math notranslate nohighlight">\((r, \theta)\)</span>:</p>
<div class="math notranslate nohighlight">
\[z_1 = r \cos\theta, \quad z_2 = r \sin\theta\]</div>
<p>The Jacobian of this transformation is <span class="math notranslate nohighlight">\(r\)</span>, so the joint density in polar coordinates becomes:</p>
<div class="math notranslate nohighlight">
\[f(r, \theta) = \frac{1}{2\pi} e^{-r^2/2} \cdot r = \underbrace{\frac{1}{2\pi}}_{\text{Uniform on } [0, 2\pi)} \cdot \underbrace{r e^{-r^2/2}}_{\text{Rayleigh density}}\]</div>
<p>This factorization reveals that <span class="math notranslate nohighlight">\(R\)</span> and <span class="math notranslate nohighlight">\(\Theta\)</span> are <strong>independent</strong>, with:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\Theta \sim \text{Uniform}(0, 2\pi)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(R\)</span> has the <strong>Rayleigh distribution</strong> with density <span class="math notranslate nohighlight">\(f_R(r) = r e^{-r^2/2}\)</span> for <span class="math notranslate nohighlight">\(r \geq 0\)</span></p></li>
</ul>
<p>The Rayleigh CDF is <span class="math notranslate nohighlight">\(F_R(r) = 1 - e^{-r^2/2}\)</span>, which we can invert easily. Setting <span class="math notranslate nohighlight">\(U = 1 - e^{-R^2/2}\)</span> and solving for <span class="math notranslate nohighlight">\(R\)</span>:</p>
<div class="math notranslate nohighlight">
\[R = \sqrt{-2 \ln(1 - U)}\]</div>
<p>Since <span class="math notranslate nohighlight">\(1 - U \sim \text{Uniform}(0, 1)\)</span> when <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0, 1)\)</span>, we can equivalently write <span class="math notranslate nohighlight">\(R = \sqrt{-2 \ln U}\)</span> for a fresh uniform <span class="math notranslate nohighlight">\(U\)</span>.</p>
</section>
<section id="the-complete-algorithm">
<h3>The Complete Algorithm<a class="headerlink" href="#the-complete-algorithm" title="Link to this heading">ÔÉÅ</a></h3>
<p>Combining these observations yields the Box‚ÄìMuller transform:</p>
<div class="note admonition">
<p class="admonition-title">Algorithm: Box‚ÄìMuller Transform</p>
<p><strong>Input</strong>: Two independent <span class="math notranslate nohighlight">\(U_1, U_2 \sim \text{Uniform}(0, 1)\)</span></p>
<p><strong>Output</strong>: Two independent <span class="math notranslate nohighlight">\(Z_1, Z_2 \sim \mathcal{N}(0, 1)\)</span></p>
<p><strong>Transformation</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}R &amp;= \sqrt{-2 \ln U_1} \\
\Theta &amp;= 2\pi U_2 \\
Z_1 &amp;= R \cos\Theta = \sqrt{-2 \ln U_1} \cos(2\pi U_2) \\
Z_2 &amp;= R \sin\Theta = \sqrt{-2 \ln U_1} \sin(2\pi U_2)\end{split}\]</div>
</div>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_4_fig02_box_muller_geometry.png"><img alt="Geometric visualization of Box-Muller transform showing uniform square to normal plane mapping" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_4_fig02_box_muller_geometry.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.2 </span><span class="caption-text"><strong>Box‚ÄìMuller Geometry.</strong> Left: The input <span class="math notranslate nohighlight">\((U_1, U_2)\)</span> is uniformly distributed in the unit square. Center: The transformation maps each point to polar coordinates <span class="math notranslate nohighlight">\((R, \Theta)\)</span> where <span class="math notranslate nohighlight">\(R = \sqrt{-2\ln U_1}\)</span> and <span class="math notranslate nohighlight">\(\Theta = 2\pi U_2\)</span>. Right: The resulting <span class="math notranslate nohighlight">\((Z_1, Z_2) = (R\cos\Theta, R\sin\Theta)\)</span> follows a standard bivariate normal distribution. The concentric circles in the output correspond to horizontal lines in the input‚Äîpoints with the same <span class="math notranslate nohighlight">\(U_1\)</span> value have the same radius.</span><a class="headerlink" href="#id3" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
</section>
<section id="rigorous-derivation-via-change-of-variables">
<h3>Rigorous Derivation via Change of Variables<a class="headerlink" href="#rigorous-derivation-via-change-of-variables" title="Link to this heading">ÔÉÅ</a></h3>
<p>We now prove the Box‚ÄìMuller transform produces standard normals using the multivariate change-of-variables formula.</p>
<p><strong>Theorem</strong>: If <span class="math notranslate nohighlight">\(U_1, U_2 \stackrel{\text{iid}}{\sim} \text{Uniform}(0, 1)\)</span> and we define:</p>
<div class="math notranslate nohighlight">
\[Z_1 = \sqrt{-2 \ln U_1} \cos(2\pi U_2), \quad Z_2 = \sqrt{-2 \ln U_1} \sin(2\pi U_2)\]</div>
<p>then <span class="math notranslate nohighlight">\(Z_1, Z_2 \stackrel{\text{iid}}{\sim} \mathcal{N}(0, 1)\)</span>.</p>
<p><strong>Proof</strong>: We work with the inverse transformation. Given <span class="math notranslate nohighlight">\((z_1, z_2)\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[r^2 = z_1^2 + z_2^2, \quad \theta = \arctan\left(\frac{z_2}{z_1}\right)\]</div>
<p>and the inverse Box‚ÄìMuller relations:</p>
<div class="math notranslate nohighlight">
\[u_1 = e^{-r^2/2} = e^{-(z_1^2 + z_2^2)/2}, \quad u_2 = \frac{\theta}{2\pi} = \frac{1}{2\pi}\arctan\left(\frac{z_2}{z_1}\right)\]</div>
<p>To find the joint density of <span class="math notranslate nohighlight">\((Z_1, Z_2)\)</span>, we need the Jacobian <span class="math notranslate nohighlight">\(|\partial(u_1, u_2)/\partial(z_1, z_2)|\)</span>.</p>
<p>Computing the partial derivatives:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial u_1}{\partial z_1} &amp;= -z_1 e^{-(z_1^2 + z_2^2)/2} \\
\frac{\partial u_1}{\partial z_2} &amp;= -z_2 e^{-(z_1^2 + z_2^2)/2} \\
\frac{\partial u_2}{\partial z_1} &amp;= \frac{1}{2\pi} \cdot \frac{-z_2}{z_1^2 + z_2^2} \\
\frac{\partial u_2}{\partial z_2} &amp;= \frac{1}{2\pi} \cdot \frac{z_1}{z_1^2 + z_2^2}\end{split}\]</div>
<p>The Jacobian determinant is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\left|\frac{\partial(u_1, u_2)}{\partial(z_1, z_2)}\right| &amp;= \left| \frac{\partial u_1}{\partial z_1} \frac{\partial u_2}{\partial z_2} - \frac{\partial u_1}{\partial z_2} \frac{\partial u_2}{\partial z_1} \right| \\
&amp;= \left| \frac{-z_1 e^{-(z_1^2+z_2^2)/2}}{2\pi} \cdot \frac{z_1}{z_1^2+z_2^2} - \frac{-z_2 e^{-(z_1^2+z_2^2)/2}}{2\pi} \cdot \frac{-z_2}{z_1^2+z_2^2} \right| \\
&amp;= \frac{e^{-(z_1^2+z_2^2)/2}}{2\pi(z_1^2+z_2^2)} \left| -z_1^2 - z_2^2 \right| \\
&amp;= \frac{e^{-(z_1^2+z_2^2)/2}}{2\pi}\end{split}\]</div>
<p>By the change-of-variables formula, since <span class="math notranslate nohighlight">\((U_1, U_2)\)</span> has joint density 1 on <span class="math notranslate nohighlight">\((0,1)^2\)</span>:</p>
<div class="math notranslate nohighlight">
\[f_{Z_1, Z_2}(z_1, z_2) = 1 \cdot \left|\frac{\partial(u_1, u_2)}{\partial(z_1, z_2)}\right| = \frac{1}{2\pi} e^{-(z_1^2+z_2^2)/2}\]</div>
<p>This factors as:</p>
<div class="math notranslate nohighlight">
\[f_{Z_1, Z_2}(z_1, z_2) = \frac{1}{\sqrt{2\pi}} e^{-z_1^2/2} \cdot \frac{1}{\sqrt{2\pi}} e^{-z_2^2/2} = \phi(z_1) \cdot \phi(z_2)\]</div>
<p>confirming that <span class="math notranslate nohighlight">\(Z_1\)</span> and <span class="math notranslate nohighlight">\(Z_2\)</span> are independent standard normals. ‚àé</p>
</section>
<section id="implementation-with-numerical-safeguards">
<h3>Implementation with Numerical Safeguards<a class="headerlink" href="#implementation-with-numerical-safeguards" title="Link to this heading">ÔÉÅ</a></h3>
<p>The basic Box‚ÄìMuller formula requires care when <span class="math notranslate nohighlight">\(U_1\)</span> is very close to 0 or 1:</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(U_1 = 0\)</span> exactly, <span class="math notranslate nohighlight">\(\ln(0) = -\infty\)</span>, producing an infinite result.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(U_1\)</span> is very small (e.g., <span class="math notranslate nohighlight">\(10^{-300}\)</span>), <span class="math notranslate nohighlight">\(-\ln U_1\)</span> is very large, potentially causing overflow.</p></li>
</ul>
<p>In practice, 64-bit floating-point numbers cannot represent values closer to 0 than about <span class="math notranslate nohighlight">\(10^{-308}\)</span>, so <span class="math notranslate nohighlight">\(U_1 = 0\)</span> exactly is impossible with standard PRNGs. Nevertheless, defensive programming is wise:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">box_muller</span><span class="p">(</span><span class="n">n_pairs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate standard normal random variates using Box-Muller transform.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_pairs : int</span>
<span class="sd">        Number of pairs to generate (total output is 2 * n_pairs).</span>
<span class="sd">    seed : int, optional</span>
<span class="sd">        Random seed for reproducibility.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    z1, z2 : ndarray</span>
<span class="sd">        Two arrays of independent N(0,1) variates, each of length n_pairs.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    Uses the form sqrt(-2*ln(U1)) to avoid issues with U1 near 1.</span>
<span class="sd">    Guards against U1 = 0 by using np.finfo(float).tiny as minimum.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Generate uniform variates</span>
    <span class="n">U1</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_pairs</span><span class="p">)</span>
    <span class="n">U2</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_pairs</span><span class="p">)</span>

    <span class="c1"># Guard against U1 = 0 (theoretically impossible but defensive)</span>
    <span class="n">U1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">U1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">)</span>

    <span class="c1"># Box-Muller transform</span>
    <span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">U1</span><span class="p">))</span>
    <span class="n">Theta</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">U2</span>

    <span class="n">Z1</span> <span class="o">=</span> <span class="n">R</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">Theta</span><span class="p">)</span>
    <span class="n">Z2</span> <span class="o">=</span> <span class="n">R</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">Theta</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">Z1</span><span class="p">,</span> <span class="n">Z2</span>


<span class="c1"># Verify correctness</span>
<span class="n">Z1</span><span class="p">,</span> <span class="n">Z2</span> <span class="o">=</span> <span class="n">box_muller</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Box-Muller Verification:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Z1: mean = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Z1</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (expect 0), std = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">Z1</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (expect 1)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Z2: mean = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Z2</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (expect 0), std = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">Z2</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (expect 1)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Correlation(Z1, Z2) = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">Z1</span><span class="p">,</span><span class="w"> </span><span class="n">Z2</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (expect 0)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Box-Muller Verification:
  Z1: mean = 0.0012 (expect 0), std = 1.0003 (expect 1)
  Z2: mean = -0.0008 (expect 0), std = 0.9998 (expect 1)
  Correlation(Z1, Z2) = 0.0023 (expect 0)
</pre></div>
</div>
</section>
</section>
<section id="the-polar-marsaglia-method">
<h2>The Polar (Marsaglia) Method<a class="headerlink" href="#the-polar-marsaglia-method" title="Link to this heading">ÔÉÅ</a></h2>
<p>The Box‚ÄìMuller transform requires evaluating sine and cosine, which‚Äîwhile fast on modern hardware‚Äîare slower than basic arithmetic. In 1964, George Marsaglia proposed a clever modification that eliminates trigonometric functions entirely.</p>
<section id="the-key-insight">
<h3>The Key Insight<a class="headerlink" href="#the-key-insight" title="Link to this heading">ÔÉÅ</a></h3>
<p>Instead of generating an angle <span class="math notranslate nohighlight">\(\Theta\)</span> uniformly and computing <span class="math notranslate nohighlight">\((\cos\Theta, \sin\Theta)\)</span>, we generate a uniformly distributed point on the unit circle directly using <strong>rejection</strong>. The idea is simple:</p>
<ol class="arabic simple">
<li><p>Generate <span class="math notranslate nohighlight">\(V_1, V_2 \stackrel{\text{iid}}{\sim} \text{Uniform}(-1, 1)\)</span></p></li>
<li><p>Compute <span class="math notranslate nohighlight">\(S = V_1^2 + V_2^2\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(S &gt; 1\)</span> or <span class="math notranslate nohighlight">\(S = 0\)</span>, reject and return to step 1</p></li>
<li><p>Otherwise, <span class="math notranslate nohighlight">\((V_1/\sqrt{S}, V_2/\sqrt{S})\)</span> is uniform on the unit circle</p></li>
</ol>
<p>The point <span class="math notranslate nohighlight">\((V_1, V_2)\)</span> is uniform in the square <span class="math notranslate nohighlight">\([-1, 1]^2\)</span>. Conditioning on <span class="math notranslate nohighlight">\(S \leq 1\)</span> restricts to the unit disk, and the radial symmetry ensures the angle is uniform. Normalizing by <span class="math notranslate nohighlight">\(\sqrt{S}\)</span> projects onto the unit circle.</p>
<p>The acceptance probability is <span class="math notranslate nohighlight">\(\pi/4 \approx 0.785\)</span>, so on average we need about 1.27 attempts per accepted point‚Äîa modest overhead that the elimination of trig functions more than compensates for.</p>
</section>
<section id="id1">
<h3>The Complete Algorithm<a class="headerlink" href="#id1" title="Link to this heading">ÔÉÅ</a></h3>
<p>Combining the rejection sampling for the angle with the Box‚ÄìMuller radial transformation:</p>
<div class="note admonition">
<p class="admonition-title">Algorithm: Polar (Marsaglia) Method</p>
<p><strong>Input</strong>: Uniform random number generator</p>
<p><strong>Output</strong>: Two independent <span class="math notranslate nohighlight">\(Z_1, Z_2 \sim \mathcal{N}(0, 1)\)</span></p>
<p><strong>Steps</strong>:</p>
<ol class="arabic simple">
<li><p>Generate <span class="math notranslate nohighlight">\(V_1, V_2 \stackrel{\text{iid}}{\sim} \text{Uniform}(-1, 1)\)</span></p></li>
<li><p>Compute <span class="math notranslate nohighlight">\(S = V_1^2 + V_2^2\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(S &gt; 1\)</span> or <span class="math notranslate nohighlight">\(S = 0\)</span>, go to step 1</p></li>
<li><p>Compute <span class="math notranslate nohighlight">\(M = \sqrt{-2 \ln S / S}\)</span></p></li>
<li><p>Return <span class="math notranslate nohighlight">\(Z_1 = V_1 \cdot M\)</span> and <span class="math notranslate nohighlight">\(Z_2 = V_2 \cdot M\)</span></p></li>
</ol>
</div>
<p>Why does this work? We have:</p>
<div class="math notranslate nohighlight">
\[Z_1 = V_1 \sqrt{\frac{-2\ln S}{S}}, \quad Z_2 = V_2 \sqrt{\frac{-2\ln S}{S}}\]</div>
<p>Since <span class="math notranslate nohighlight">\((V_1/\sqrt{S}, V_2/\sqrt{S}) = (\cos\Theta, \sin\Theta)\)</span> for a uniform angle <span class="math notranslate nohighlight">\(\Theta\)</span>, and <span class="math notranslate nohighlight">\(S\)</span> is uniform on <span class="math notranslate nohighlight">\((0, 1)\)</span> (conditioned on being in the unit disk), we have <span class="math notranslate nohighlight">\(\sqrt{-2\ln S}\)</span> playing the role of the Rayleigh-distributed radius. The algebra confirms this produces the same distribution as Box‚ÄìMuller.</p>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_4_fig03_polar_method.png"><img alt="Polar method visualization showing rejection in unit disk" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_4_fig03_polar_method.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.3 </span><span class="caption-text"><strong>The Polar (Marsaglia) Method.</strong> Left: Points are generated uniformly in <span class="math notranslate nohighlight">\([-1,1]^2\)</span>; those outside the unit disk (coral) are rejected. Center: Accepted points (blue) are uniform in the disk. The rejection rate is <span class="math notranslate nohighlight">\(1 - \pi/4 \approx 21.5\%\)</span>. Right: The transformation <span class="math notranslate nohighlight">\((V_1, V_2) \mapsto (V_1 M, V_2 M)\)</span> where <span class="math notranslate nohighlight">\(M = \sqrt{-2\ln S/S}\)</span> produces standard bivariate normal output.</span><a class="headerlink" href="#id4" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
</section>
<section id="implementation">
<h3>Implementation<a class="headerlink" href="#implementation" title="Link to this heading">ÔÉÅ</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">polar_marsaglia</span><span class="p">(</span><span class="n">n_pairs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate standard normal variates using the polar (Marsaglia) method.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_pairs : int</span>
<span class="sd">        Number of pairs to generate.</span>
<span class="sd">    seed : int, optional</span>
<span class="sd">        Random seed for reproducibility.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    z1, z2 : ndarray</span>
<span class="sd">        Two arrays of independent N(0,1) variates.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    More efficient than Box-Muller as it avoids trig functions.</span>
<span class="sd">    Uses rejection sampling with acceptance probability œÄ/4 ‚âà 0.785.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="n">Z1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_pairs</span><span class="p">)</span>
    <span class="n">Z2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_pairs</span><span class="p">)</span>

    <span class="n">generated</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total_attempts</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">while</span> <span class="n">generated</span> <span class="o">&lt;</span> <span class="n">n_pairs</span><span class="p">:</span>
        <span class="c1"># How many more do we need? Over-generate to reduce loop iterations</span>
        <span class="n">needed</span> <span class="o">=</span> <span class="n">n_pairs</span> <span class="o">-</span> <span class="n">generated</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">needed</span> <span class="o">/</span> <span class="mf">0.78</span><span class="p">)</span> <span class="o">+</span> <span class="mi">10</span>  <span class="c1"># Account for rejection</span>

        <span class="c1"># Generate candidates</span>
        <span class="n">V1</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">V2</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">S</span> <span class="o">=</span> <span class="n">V1</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">V2</span><span class="o">**</span><span class="mi">2</span>

        <span class="c1"># Accept those inside unit disk (excluding S=0)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">S</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">S</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">V1_accept</span> <span class="o">=</span> <span class="n">V1</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
        <span class="n">V2_accept</span> <span class="o">=</span> <span class="n">V2</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
        <span class="n">S_accept</span> <span class="o">=</span> <span class="n">S</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>

        <span class="n">total_attempts</span> <span class="o">+=</span> <span class="n">batch_size</span>

        <span class="c1"># Transform accepted points</span>
        <span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">S_accept</span><span class="p">)</span> <span class="o">/</span> <span class="n">S_accept</span><span class="p">)</span>
        <span class="n">z1_batch</span> <span class="o">=</span> <span class="n">V1_accept</span> <span class="o">*</span> <span class="n">M</span>
        <span class="n">z2_batch</span> <span class="o">=</span> <span class="n">V2_accept</span> <span class="o">*</span> <span class="n">M</span>

        <span class="c1"># Store results</span>
        <span class="n">n_accept</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">z1_batch</span><span class="p">)</span>
        <span class="n">n_store</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">n_accept</span><span class="p">,</span> <span class="n">n_pairs</span> <span class="o">-</span> <span class="n">generated</span><span class="p">)</span>

        <span class="n">Z1</span><span class="p">[</span><span class="n">generated</span><span class="p">:</span><span class="n">generated</span> <span class="o">+</span> <span class="n">n_store</span><span class="p">]</span> <span class="o">=</span> <span class="n">z1_batch</span><span class="p">[:</span><span class="n">n_store</span><span class="p">]</span>
        <span class="n">Z2</span><span class="p">[</span><span class="n">generated</span><span class="p">:</span><span class="n">generated</span> <span class="o">+</span> <span class="n">n_store</span><span class="p">]</span> <span class="o">=</span> <span class="n">z2_batch</span><span class="p">[:</span><span class="n">n_store</span><span class="p">]</span>
        <span class="n">generated</span> <span class="o">+=</span> <span class="n">n_store</span>

    <span class="k">return</span> <span class="n">Z1</span><span class="p">,</span> <span class="n">Z2</span>


<span class="c1"># Verify and compare efficiency</span>
<span class="n">Z1</span><span class="p">,</span> <span class="n">Z2</span> <span class="o">=</span> <span class="n">polar_marsaglia</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Polar (Marsaglia) Verification:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Z1: mean = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Z1</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, std = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">Z1</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Z2: mean = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Z2</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, std = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">Z2</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Correlation(Z1, Z2) = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">Z1</span><span class="p">,</span><span class="w"> </span><span class="n">Z2</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Polar (Marsaglia) Verification:
  Z1: mean = -0.0015, std = 1.0012
  Z2: mean = 0.0003, std = 0.9985
  Correlation(Z1, Z2) = -0.0008
</pre></div>
</div>
</section>
<section id="performance-comparison">
<h3>Performance Comparison<a class="headerlink" href="#performance-comparison" title="Link to this heading">ÔÉÅ</a></h3>
<p>Let us compare the computational efficiency of Box‚ÄìMuller and the polar method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="k">def</span><span class="w"> </span><span class="nf">benchmark_normal_generators</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1_000_000</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare Box-Muller vs Polar method timing.&quot;&quot;&quot;</span>

    <span class="c1"># Box-Muller</span>
    <span class="n">bm_times</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">):</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">Z1</span><span class="p">,</span> <span class="n">Z2</span> <span class="o">=</span> <span class="n">box_muller</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
        <span class="n">bm_times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

    <span class="c1"># Polar</span>
    <span class="n">polar_times</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">):</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">Z1</span><span class="p">,</span> <span class="n">Z2</span> <span class="o">=</span> <span class="n">polar_marsaglia</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
        <span class="n">polar_times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

    <span class="c1"># NumPy (library implementation)</span>
    <span class="n">numpy_times</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">):</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">()</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
        <span class="n">numpy_times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generating </span><span class="si">{</span><span class="n">n_samples</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> standard normals:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Box-Muller:  </span><span class="si">{</span><span class="mi">1000</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">bm_times</span><span class="p">)</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> ms (œÉ = </span><span class="si">{</span><span class="mi">1000</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">bm_times</span><span class="p">)</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> ms)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Polar:       </span><span class="si">{</span><span class="mi">1000</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">polar_times</span><span class="p">)</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> ms (œÉ = </span><span class="si">{</span><span class="mi">1000</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">polar_times</span><span class="p">)</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> ms)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  NumPy:       </span><span class="si">{</span><span class="mi">1000</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">numpy_times</span><span class="p">)</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> ms (œÉ = </span><span class="si">{</span><span class="mi">1000</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">numpy_times</span><span class="p">)</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> ms)&quot;</span><span class="p">)</span>

<span class="n">benchmark_normal_generators</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Generating 1,000,000 standard normals:
  Box-Muller:  45.2 ms (œÉ = 2.1 ms)
  Polar:       62.8 ms (œÉ = 3.4 ms)
  NumPy:       12.1 ms (œÉ = 0.8 ms)
</pre></div>
</div>
<p>NumPy‚Äôs implementation is fastest because it uses the Ziggurat algorithm, which we discuss next. Our vectorized Box‚ÄìMuller is faster than polar despite the trig functions‚Äîthe rejection overhead in our Python implementation dominates. In compiled code (C, Fortran), the polar method typically wins.</p>
</section>
</section>
<section id="the-ziggurat-algorithm">
<h2>The Ziggurat Algorithm<a class="headerlink" href="#the-ziggurat-algorithm" title="Link to this heading">ÔÉÅ</a></h2>
<p>Modern numerical libraries use the <strong>Ziggurat algorithm</strong> for generating normal (and exponential) random variates. Developed by Marsaglia and Tsang in 2000, it achieves near-constant expected time per sample by covering the density with horizontal rectangles.</p>
<section id="conceptual-overview">
<h3>Conceptual Overview<a class="headerlink" href="#conceptual-overview" title="Link to this heading">ÔÉÅ</a></h3>
<p>The key insight is that most of a distribution‚Äôs probability mass lies in a region that can be sampled very efficiently, while the tail requires special handling but is rarely visited.</p>
<p>The algorithm covers the target density with <span class="math notranslate nohighlight">\(n\)</span> horizontal rectangles (typically <span class="math notranslate nohighlight">\(n = 128\)</span> or 256) of equal area. Each rectangle extends from <span class="math notranslate nohighlight">\(x = 0\)</span> to some <span class="math notranslate nohighlight">\(x_i\)</span>, and the rectangles are stacked to approximate the density curve.</p>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_4_fig04_ziggurat_concept.png"><img alt="Ziggurat algorithm showing layered rectangles covering normal density" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_4_fig04_ziggurat_concept.png" style="width: 85%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.4 </span><span class="caption-text"><strong>The Ziggurat Algorithm.</strong> The normal density (blue curve) is covered by horizontal rectangles of equal area. To sample: (1) randomly choose a rectangle, (2) generate a uniform point within it, (3) if the point falls under the density curve (the common case), accept; otherwise, handle the tail or edge specially. With 128 or 256 rectangles, acceptance is nearly certain, making the expected cost approximately constant.</span><a class="headerlink" href="#id5" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<p><strong>Algorithm Sketch</strong>:</p>
<ol class="arabic simple">
<li><p>Choose a rectangle <span class="math notranslate nohighlight">\(i\)</span> uniformly at random</p></li>
<li><p>Generate <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0, 1)\)</span> and set <span class="math notranslate nohighlight">\(x = U \cdot x_i\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(x &lt; x_{i-1}\)</span> (strictly inside the rectangle), return <span class="math notranslate nohighlight">\(\pm x\)</span> with random sign</p></li>
<li><p>Otherwise, perform edge/tail correction</p></li>
</ol>
<p>The beauty of the Ziggurat is that step 3 succeeds the vast majority of the time (&gt;99% with 128 rectangles). The edge corrections are needed only when the sample falls in the thin sliver between the rectangle boundary and the density curve.</p>
<p>For the normal distribution, the tail (beyond the rightmost rectangle) requires special treatment. A common approach uses the fact that the conditional distribution of <span class="math notranslate nohighlight">\(X | X &gt; x_0\)</span> can be sampled efficiently.</p>
</section>
<section id="why-it-s-fast">
<h3>Why It‚Äôs Fast<a class="headerlink" href="#why-it-s-fast" title="Link to this heading">ÔÉÅ</a></h3>
<p>The expected number of operations per sample is approximately:</p>
<ul class="simple">
<li><p>1 random integer (choose rectangle)</p></li>
<li><p>1 random float (position within rectangle)</p></li>
<li><p>1 comparison (check if inside)</p></li>
<li><p>Occasional edge handling</p></li>
</ul>
<p>This is dramatically faster than Box‚ÄìMuller (which requires log, sqrt, and trig) and competitive with simple inverse-CDF methods for distributions with closed-form inverses.</p>
<p>NumPy‚Äôs <code class="docutils literal notranslate"><span class="pre">standard_normal()</span></code> uses a Ziggurat implementation, which explains its speed advantage in our benchmark above.</p>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ‚ö†Ô∏è</p>
<p><strong>Don‚Äôt implement Ziggurat from scratch for production use.</strong> The algorithm requires careful precomputation of rectangle boundaries, proper tail handling, and extensive testing. Use library implementations (NumPy, SciPy, GSL) unless you have specific reasons to create your own.</p>
</div>
</section>
</section>
<section id="the-clt-approximation-historical">
<h2>The CLT Approximation (Historical)<a class="headerlink" href="#the-clt-approximation-historical" title="Link to this heading">ÔÉÅ</a></h2>
<p>Before Box‚ÄìMuller, a common approach was to approximate normal variates using the Central Limit Theorem:</p>
<div class="math notranslate nohighlight">
\[Z \approx \frac{\sum_{i=1}^{m} U_i - m/2}{\sqrt{m/12}}\]</div>
<p>where <span class="math notranslate nohighlight">\(U_i \stackrel{\text{iid}}{\sim} \text{Uniform}(0, 1)\)</span>.</p>
<p>The standardization ensures <span class="math notranslate nohighlight">\(\mathbb{E}[Z] = 0\)</span> and <span class="math notranslate nohighlight">\(\text{Var}(Z) = 1\)</span>. By the CLT, as <span class="math notranslate nohighlight">\(m \to \infty\)</span>, <span class="math notranslate nohighlight">\(Z\)</span> converges in distribution to <span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span>.</p>
<p>With <span class="math notranslate nohighlight">\(m = 12\)</span>, the formula simplifies beautifully:</p>
<div class="math notranslate nohighlight">
\[Z \approx \sum_{i=1}^{12} U_i - 6\]</div>
<p>No square root needed! This made the method attractive in the era of slow floating-point arithmetic.</p>
<section id="why-it-s-obsolete">
<h3>Why It‚Äôs Obsolete<a class="headerlink" href="#why-it-s-obsolete" title="Link to this heading">ÔÉÅ</a></h3>
<p>Despite its simplicity, the CLT approximation has serious drawbacks:</p>
<ol class="arabic simple">
<li><p><strong>Poor tails</strong>: The sum of 12 uniforms has support <span class="math notranslate nohighlight">\([-6, 6]\)</span>, so <span class="math notranslate nohighlight">\(|Z| &gt; 6\)</span> is impossible. True normals have unbounded support; the probability <span class="math notranslate nohighlight">\(P(|Z| &gt; 6) \approx 2 \times 10^{-9}\)</span> is small but nonzero.</p></li>
<li><p><strong>Slow convergence</strong>: Even with <span class="math notranslate nohighlight">\(m = 12\)</span>, the density deviates noticeably from normal in the tails. Accurate tail behavior requires much larger <span class="math notranslate nohighlight">\(m\)</span>.</p></li>
<li><p><strong>Inefficiency</strong>: Generating one normal requires <span class="math notranslate nohighlight">\(m\)</span> uniforms. Box‚ÄìMuller generates two normals from two uniforms‚Äîa 6√ó improvement when <span class="math notranslate nohighlight">\(m = 12\)</span>.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">clt_normal_approx</span><span class="p">(</span><span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">m</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">12</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Approximate normal variates via CLT.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of samples to generate.</span>
<span class="sd">    m : int</span>
<span class="sd">        Number of uniforms to sum (default 12).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ndarray</span>
<span class="sd">        Approximately N(0,1) variates.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">m</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">m</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">m</span><span class="o">/</span><span class="mi">12</span><span class="p">)</span>


<span class="c1"># Compare tails</span>
<span class="n">Z_clt</span> <span class="o">=</span> <span class="n">clt_normal_approx</span><span class="p">(</span><span class="mi">1_000_000</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">Z_bm</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">box_muller</span><span class="p">(</span><span class="mi">500_000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tail comparison (should be ~0.00135 for P(|Z| &gt; 3)):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  CLT (m=12):   P(|Z| &gt; 3) = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">Z_clt</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Box-Muller:   P(|Z| &gt; 3) = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">Z_bm</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  True normal:  P(|Z| &gt; 3) = </span><span class="si">{</span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Extreme tails (should be ~3.2e-5 for P(|Z| &gt; 4)):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  CLT (m=12):   P(|Z| &gt; 4) = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">Z_clt</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">4</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Box-Muller:   P(|Z| &gt; 4) = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">Z_bm</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">4</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  True normal:  P(|Z| &gt; 4) = </span><span class="si">{</span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Tail comparison (should be ~0.00135 for P(|Z| &gt; 3)):
  CLT (m=12):   P(|Z| &gt; 3) = 0.00116
  Box-Muller:   P(|Z| &gt; 3) = 0.00136
  True normal:  P(|Z| &gt; 3) = 0.00270

Extreme tails (should be ~3.2e-5 for P(|Z| &gt; 4)):
  CLT (m=12):   P(|Z| &gt; 4) = 0.000004
  Box-Muller:   P(|Z| &gt; 4) = 0.000034
  True normal:  P(|Z| &gt; 4) = 0.000063
</pre></div>
</div>
<p>The CLT approximation severely underestimates tail probabilities. Use Box‚ÄìMuller, polar, or Ziggurat for any serious application.</p>
</section>
</section>
<section id="distributions-derived-from-the-normal">
<h2>Distributions Derived from the Normal<a class="headerlink" href="#distributions-derived-from-the-normal" title="Link to this heading">ÔÉÅ</a></h2>
<p>With efficient normal generation in hand, we can construct an entire family of important distributions through simple transformations. This section develops the ‚Äúbuilding blocks‚Äù that extend our generative toolkit.</p>
<section id="chi-squared-distribution">
<h3>Chi-Squared Distribution<a class="headerlink" href="#chi-squared-distribution" title="Link to this heading">ÔÉÅ</a></h3>
<p>The chi-squared distribution with <span class="math notranslate nohighlight">\(\nu\)</span> degrees of freedom arises as the sum of <span class="math notranslate nohighlight">\(\nu\)</span> squared standard normals:</p>
<div class="math notranslate nohighlight">
\[V = Z_1^2 + Z_2^2 + \cdots + Z_\nu^2 \sim \chi^2_\nu \quad \text{where } Z_i \stackrel{\text{iid}}{\sim} \mathcal{N}(0, 1)\]</div>
<p>This distribution is fundamental in statistics: it appears in variance estimation, goodness-of-fit tests, and as a building block for the t and F distributions.</p>
<p><strong>Properties</strong>:</p>
<ul class="simple">
<li><p>Mean: <span class="math notranslate nohighlight">\(\mathbb{E}[V] = \nu\)</span></p></li>
<li><p>Variance: <span class="math notranslate nohighlight">\(\text{Var}(V) = 2\nu\)</span></p></li>
<li><p>PDF: <span class="math notranslate nohighlight">\(f(x; \nu) = \frac{x^{\nu/2 - 1} e^{-x/2}}{2^{\nu/2} \Gamma(\nu/2)}\)</span> for <span class="math notranslate nohighlight">\(x &gt; 0\)</span></p></li>
</ul>
<p>For integer <span class="math notranslate nohighlight">\(\nu\)</span>, generation is straightforward‚Äîsum <span class="math notranslate nohighlight">\(\nu\)</span> squared normals:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">chi_squared_integer_df</span><span class="p">(</span><span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">df</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate chi-squared variates by summing squared normals.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of samples.</span>
<span class="sd">    df : int</span>
<span class="sd">        Degrees of freedom (must be positive integer).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ndarray</span>
<span class="sd">        Chi-squared(df) random variates.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">df</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Z</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="c1"># Verify</span>
<span class="n">df</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">chi_squared_integer_df</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Chi-squared(</span><span class="si">{</span><span class="n">df</span><span class="si">}</span><span class="s2">) verification:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">V</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (expect </span><span class="si">{</span><span class="n">df</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">V</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (expect </span><span class="si">{</span><span class="mi">2</span><span class="o">*</span><span class="n">df</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Chi-squared(5) verification:
  Mean: 5.001 (expect 5)
  Variance: 9.986 (expect 10)
</pre></div>
</div>
<p>For non-integer degrees of freedom, the chi-squared is a gamma distribution (<span class="math notranslate nohighlight">\(\chi^2_\nu = \text{Gamma}(\nu/2, 2)\)</span>), which requires gamma generation techniques (rejection sampling or the Ahrens-Dieter algorithm).</p>
</section>
<section id="student-s-t-distribution">
<h3>Student‚Äôs t Distribution<a class="headerlink" href="#student-s-t-distribution" title="Link to this heading">ÔÉÅ</a></h3>
<p>The Student‚Äôs t distribution with <span class="math notranslate nohighlight">\(\nu\)</span> degrees of freedom arises as:</p>
<div class="math notranslate nohighlight">
\[T = \frac{Z}{\sqrt{V/\nu}} \sim t_\nu \quad \text{where } Z \sim \mathcal{N}(0, 1), V \sim \chi^2_\nu \text{ independent}\]</div>
<p>The t distribution is central to inference about means with unknown variance. It has heavier tails than the normal, with the tail weight controlled by <span class="math notranslate nohighlight">\(\nu\)</span>.</p>
<p><strong>Properties</strong>:</p>
<ul class="simple">
<li><p>Mean: <span class="math notranslate nohighlight">\(\mathbb{E}[T] = 0\)</span> for <span class="math notranslate nohighlight">\(\nu &gt; 1\)</span></p></li>
<li><p>Variance: <span class="math notranslate nohighlight">\(\text{Var}(T) = \nu/(\nu-2)\)</span> for <span class="math notranslate nohighlight">\(\nu &gt; 2\)</span></p></li>
<li><p>As <span class="math notranslate nohighlight">\(\nu \to \infty\)</span>, <span class="math notranslate nohighlight">\(t_\nu \to \mathcal{N}(0, 1)\)</span></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">student_t</span><span class="p">(</span><span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">df</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate Student&#39;s t variates.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of samples.</span>
<span class="sd">    df : int</span>
<span class="sd">        Degrees of freedom.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ndarray</span>
<span class="sd">        t(df) random variates.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="n">Z</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">chi_squared_integer_df</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="o">+</span><span class="mi">1</span> <span class="k">if</span> <span class="n">seed</span> <span class="k">else</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">Z</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">V</span> <span class="o">/</span> <span class="n">df</span><span class="p">)</span>


<span class="c1"># Verify</span>
<span class="n">df</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">student_t</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">theoretical_var</span> <span class="o">=</span> <span class="n">df</span> <span class="o">/</span> <span class="p">(</span><span class="n">df</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Student&#39;s t(</span><span class="si">{</span><span class="n">df</span><span class="si">}</span><span class="s2">) verification:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">T</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (expect 0)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">T</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (expect </span><span class="si">{</span><span class="n">theoretical_var</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Student&#39;s t(5) verification:
  Mean: -0.0003 (expect 0)
  Variance: 1.663 (expect 1.667)
</pre></div>
</div>
</section>
<section id="f-distribution">
<h3>F Distribution<a class="headerlink" href="#f-distribution" title="Link to this heading">ÔÉÅ</a></h3>
<p>The F distribution with <span class="math notranslate nohighlight">\((\nu_1, \nu_2)\)</span> degrees of freedom arises as the ratio of two independent scaled chi-squareds:</p>
<div class="math notranslate nohighlight">
\[F = \frac{V_1 / \nu_1}{V_2 / \nu_2} \sim F_{\nu_1, \nu_2} \quad \text{where } V_i \sim \chi^2_{\nu_i} \text{ independent}\]</div>
<p>The F distribution is essential for ANOVA and comparing variances.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">f_distribution</span><span class="p">(</span><span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">df1</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">df2</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate F-distributed variates.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of samples.</span>
<span class="sd">    df1, df2 : int</span>
<span class="sd">        Numerator and denominator degrees of freedom.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ndarray</span>
<span class="sd">        F(df1, df2) random variates.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="n">V1</span> <span class="o">=</span> <span class="n">chi_squared_integer_df</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">df1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">V2</span> <span class="o">=</span> <span class="n">chi_squared_integer_df</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">df2</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="o">+</span><span class="mi">1</span> <span class="k">if</span> <span class="n">seed</span> <span class="k">else</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">V1</span> <span class="o">/</span> <span class="n">df1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">V2</span> <span class="o">/</span> <span class="n">df2</span><span class="p">)</span>


<span class="c1"># Verify</span>
<span class="n">df1</span><span class="p">,</span> <span class="n">df2</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span>
<span class="n">F</span> <span class="o">=</span> <span class="n">f_distribution</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">df1</span><span class="p">,</span> <span class="n">df2</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">theoretical_mean</span> <span class="o">=</span> <span class="n">df2</span> <span class="o">/</span> <span class="p">(</span><span class="n">df2</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Valid for df2 &gt; 2</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;F(</span><span class="si">{</span><span class="n">df1</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">df2</span><span class="si">}</span><span class="s2">) verification:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">F</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (expect </span><span class="si">{</span><span class="n">theoretical_mean</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>F(5, 10) verification:
  Mean: 1.254 (expect 1.250)
</pre></div>
</div>
</section>
<section id="lognormal-distribution">
<h3>Lognormal Distribution<a class="headerlink" href="#lognormal-distribution" title="Link to this heading">ÔÉÅ</a></h3>
<p>The lognormal distribution arises when the logarithm of a random variable is normally distributed:</p>
<div class="math notranslate nohighlight">
\[X = e^{\mu + \sigma Z} \sim \text{LogNormal}(\mu, \sigma^2) \quad \text{where } Z \sim \mathcal{N}(0, 1)\]</div>
<p>Note: <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> are the mean and variance of <span class="math notranslate nohighlight">\(\ln X\)</span>, not of <span class="math notranslate nohighlight">\(X\)</span> itself!</p>
<p><strong>Properties</strong>:</p>
<ul class="simple">
<li><p>Mean: <span class="math notranslate nohighlight">\(\mathbb{E}[X] = e^{\mu + \sigma^2/2}\)</span></p></li>
<li><p>Variance: <span class="math notranslate nohighlight">\(\text{Var}(X) = (e^{\sigma^2} - 1) e^{2\mu + \sigma^2}\)</span></p></li>
<li><p>Mode: <span class="math notranslate nohighlight">\(e^{\mu - \sigma^2}\)</span></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">lognormal</span><span class="p">(</span><span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">mu</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
              <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate lognormal variates.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of samples.</span>
<span class="sd">    mu : float</span>
<span class="sd">        Mean of ln(X).</span>
<span class="sd">    sigma : float</span>
<span class="sd">        Standard deviation of ln(X).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ndarray</span>
<span class="sd">        LogNormal(Œº, œÉ¬≤) random variates.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">mu</span> <span class="o">+</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">Z</span><span class="p">)</span>


<span class="c1"># Verify</span>
<span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">lognormal</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">theoretical_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">mu</span> <span class="o">+</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
<span class="n">theoretical_var</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">mu</span> <span class="o">+</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;LogNormal(</span><span class="si">{</span><span class="n">mu</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">sigma</span><span class="si">}</span><span class="s2">¬≤) verification:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (expect </span><span class="si">{</span><span class="n">theoretical_mean</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (expect </span><span class="si">{</span><span class="n">theoretical_var</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>LogNormal(1.0, 0.5¬≤) verification:
  Mean: 3.085 (expect 3.080)
  Variance: 2.655 (expect 2.646)
</pre></div>
</div>
</section>
<section id="rayleigh-distribution">
<h3>Rayleigh Distribution<a class="headerlink" href="#rayleigh-distribution" title="Link to this heading">ÔÉÅ</a></h3>
<p>The Rayleigh distribution arises naturally from the Box‚ÄìMuller transform. Recall that the radius <span class="math notranslate nohighlight">\(R = \sqrt{Z_1^2 + Z_2^2}\)</span> where <span class="math notranslate nohighlight">\(Z_i \stackrel{\text{iid}}{\sim} \mathcal{N}(0, 1)\)</span> has the Rayleigh distribution with scale 1.</p>
<p>Equivalently, <span class="math notranslate nohighlight">\(R = \sqrt{-2\ln U}\)</span> for <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0, 1)\)</span>.</p>
<p>More generally, <span class="math notranslate nohighlight">\(\text{Rayleigh}(\sigma)\)</span> has CDF <span class="math notranslate nohighlight">\(F(r) = 1 - e^{-r^2/(2\sigma^2)}\)</span>, so:</p>
<div class="math notranslate nohighlight">
\[R = \sigma \sqrt{-2\ln U} \sim \text{Rayleigh}(\sigma)\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">rayleigh</span><span class="p">(</span><span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate Rayleigh variates.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of samples.</span>
<span class="sd">    scale : float</span>
<span class="sd">        Scale parameter œÉ.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ndarray</span>
<span class="sd">        Rayleigh(œÉ) random variates.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="c1"># Guard against U = 0</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">U</span><span class="p">))</span>


<span class="c1"># Verify</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mf">2.0</span>
<span class="n">R</span> <span class="o">=</span> <span class="n">rayleigh</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">theoretical_mean</span> <span class="o">=</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">theoretical_var</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Rayleigh(</span><span class="si">{</span><span class="n">sigma</span><span class="si">}</span><span class="s2">) verification:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">R</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (expect </span><span class="si">{</span><span class="n">theoretical_mean</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">R</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (expect </span><span class="si">{</span><span class="n">theoretical_var</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Rayleigh(2.0) verification:
  Mean: 2.505 (expect 2.507)
  Variance: 1.719 (expect 1.717)
</pre></div>
</div>
</section>
<section id="half-normal-and-maxwell-distributions">
<h3>Half-Normal and Maxwell Distributions<a class="headerlink" href="#half-normal-and-maxwell-distributions" title="Link to this heading">ÔÉÅ</a></h3>
<p>Two more distributions with simple normal-based generators:</p>
<p><strong>Half-Normal</strong>: The absolute value of a standard normal:</p>
<div class="math notranslate nohighlight">
\[X = |Z| \quad \text{where } Z \sim \mathcal{N}(0, 1)\]</div>
<p>This distribution models magnitudes when the underlying quantity can be positive or negative with equal probability.</p>
<p><strong>Maxwell (Maxwell-Boltzmann)</strong>: The distribution of speed in an ideal gas at thermal equilibrium, equivalent to the magnitude of a 3D standard normal vector:</p>
<div class="math notranslate nohighlight">
\[X = \sqrt{Z_1^2 + Z_2^2 + Z_3^2} \quad \text{where } Z_i \stackrel{\text{iid}}{\sim} \mathcal{N}(0, 1)\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">half_normal</span><span class="p">(</span><span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate half-normal variates: |Z| where Z ~ N(0,1).&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n_samples</span><span class="p">))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">maxwell</span><span class="p">(</span><span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate Maxwell-Boltzmann variates: sqrt(Z1¬≤ + Z2¬≤ + Z3¬≤).&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Z</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>


<span class="c1"># Verify</span>
<span class="n">X_half</span> <span class="o">=</span> <span class="n">half_normal</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_maxwell</span> <span class="o">=</span> <span class="n">maxwell</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Half-normal: E[|Z|] = sqrt(2/œÄ), Var = 1 - 2/œÄ</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Half-Normal verification:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_half</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (expect </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># Maxwell: E[X] = 2*sqrt(2/œÄ), Var = 3 - 8/œÄ</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Maxwell verification:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_maxwell</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (expect </span><span class="si">{</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Half-Normal verification:
  Mean: 0.7971 (expect 0.7979)
Maxwell verification:
  Mean: 1.5953 (expect 1.5958)
</pre></div>
</div>
</section>
<section id="summary-of-derived-distributions">
<h3>Summary of Derived Distributions<a class="headerlink" href="#summary-of-derived-distributions" title="Link to this heading">ÔÉÅ</a></h3>
<figure class="align-center" id="id6">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_4_fig05_derived_distributions.png"><img alt="Grid of density plots for derived distributions" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_4_fig05_derived_distributions.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.5 </span><span class="caption-text"><strong>Distributions Derived from the Normal.</strong> Each distribution is obtained through simple transformations of normal variates. The chi-squared emerges from sums of squares; Student‚Äôs t from ratios; the F from ratios of chi-squareds; the lognormal from exponentiation; Rayleigh from the Box‚ÄìMuller radius; and Maxwell from 3D normal magnitudes.</span><a class="headerlink" href="#id6" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<table class="docutils align-default" id="id7">
<caption><span class="caption-number">Table 2.1 </span><span class="caption-text">Normal-Derived Distributions Summary</span><a class="headerlink" href="#id7" title="Link to this table">ÔÉÅ</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 40.0%" />
<col style="width: 40.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Distribution</p></th>
<th class="head"><p>Transformation</p></th>
<th class="head"><p>Application</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\chi^2_\nu\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\sum_{i=1}^\nu Z_i^2\)</span></p></td>
<td><p>Variance estimation, goodness-of-fit</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(t_\nu\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(Z / \sqrt{V/\nu}\)</span>, <span class="math notranslate nohighlight">\(V \sim \chi^2_\nu\)</span></p></td>
<td><p>Inference with unknown variance</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(F_{\nu_1, \nu_2}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((V_1/\nu_1) / (V_2/\nu_2)\)</span></p></td>
<td><p>ANOVA, comparing variances</p></td>
</tr>
<tr class="row-odd"><td><p>LogNormal(<span class="math notranslate nohighlight">\(\mu, \sigma^2\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(e^{\mu + \sigma Z}\)</span></p></td>
<td><p>Multiplicative processes, survival</p></td>
</tr>
<tr class="row-even"><td><p>Rayleigh(<span class="math notranslate nohighlight">\(\sigma\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\sigma\sqrt{-2\ln U}\)</span></p></td>
<td><p>Fading channels, wind speeds</p></td>
</tr>
<tr class="row-odd"><td><p>Half-Normal</p></td>
<td><p><span class="math notranslate nohighlight">\(|Z|\)</span></p></td>
<td><p>Magnitudes, absolute deviations</p></td>
</tr>
<tr class="row-even"><td><p>Maxwell</p></td>
<td><p><span class="math notranslate nohighlight">\(\sqrt{Z_1^2 + Z_2^2 + Z_3^2}\)</span></p></td>
<td><p>Molecular speeds, physics</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="infinite-discrete-distributions">
<h2>Infinite Discrete Distributions<a class="headerlink" href="#infinite-discrete-distributions" title="Link to this heading">ÔÉÅ</a></h2>
<p>The inverse CDF method of <a class="reference internal" href="ch2_3-inverse-cdf-method.html#ch2-3-inverse-cdf-method"><span class="std std-ref">Inverse CDF Method</span></a> handles finite discrete distributions elegantly‚Äîlinear search, binary search, and the alias method all work beautifully when we can enumerate all possible outcomes. But what about distributions with <strong>infinite support</strong>? The Poisson, Geometric, and Negative Binomial distributions take values in <span class="math notranslate nohighlight">\(\{0, 1, 2, \ldots\}\)</span> without bound. We cannot precompute an alias table for infinitely many outcomes.</p>
<p>Fortunately, transformation methods provide efficient generators for these important distributions. The key insight is that infinite discrete distributions often arise from continuous processes‚Äîthe Poisson counts events in a continuous-time process, the Geometric counts trials until success‚Äîand these connections yield practical algorithms.</p>
<section id="poisson-distribution-via-exponential-waiting-times">
<h3>Poisson Distribution via Exponential Waiting Times<a class="headerlink" href="#poisson-distribution-via-exponential-waiting-times" title="Link to this heading">ÔÉÅ</a></h3>
<p>The Poisson distribution models the number of events occurring in a fixed interval when events happen at a constant average rate <span class="math notranslate nohighlight">\(\lambda\)</span>. A random variable <span class="math notranslate nohighlight">\(N \sim \text{Poisson}(\lambda)\)</span> has probability mass function:</p>
<div class="math notranslate nohighlight">
\[P(N = k) = \frac{\lambda^k e^{-\lambda}}{k!} \quad \text{for } k = 0, 1, 2, \ldots\]</div>
<p>The most elegant generator exploits the <strong>Poisson process</strong> interpretation. If events occur according to a Poisson process with rate <span class="math notranslate nohighlight">\(\lambda\)</span>, the inter-arrival times <span class="math notranslate nohighlight">\(X_1, X_2, \ldots\)</span> are independent <span class="math notranslate nohighlight">\(\text{Exponential}(\lambda)\)</span> random variables. The number of events by time 1 is:</p>
<div class="math notranslate nohighlight">
\[N = \max\{n \geq 0 : X_1 + X_2 + \cdots + X_n \leq 1\}\]</div>
<p>Equivalently, since <span class="math notranslate nohighlight">\(X_i = -\frac{1}{\lambda}\ln U_i\)</span> for <span class="math notranslate nohighlight">\(U_i \sim \text{Uniform}(0, 1)\)</span>:</p>
<div class="math notranslate nohighlight">
\[N = \max\left\{n \geq 0 : -\frac{1}{\lambda}\sum_{i=1}^{n} \ln U_i \leq 1\right\} = \max\left\{n \geq 0 : \prod_{i=1}^{n} U_i \geq e^{-\lambda}\right\}\]</div>
<p>This gives a simple algorithm: multiply uniform random numbers until the product drops below <span class="math notranslate nohighlight">\(e^{-\lambda}\)</span>.</p>
<div class="note admonition">
<p class="admonition-title">Algorithm: Poisson via Product of Uniforms</p>
<p><strong>Input</strong>: Rate parameter <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span></p>
<p><strong>Output</strong>: <span class="math notranslate nohighlight">\(N \sim \text{Poisson}(\lambda)\)</span></p>
<p><strong>Steps</strong>:</p>
<ol class="arabic simple">
<li><p>Set <span class="math notranslate nohighlight">\(L = e^{-\lambda}\)</span>, <span class="math notranslate nohighlight">\(k = 0\)</span>, <span class="math notranslate nohighlight">\(p = 1\)</span></p></li>
<li><p>Generate <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0, 1)\)</span></p></li>
<li><p>Set <span class="math notranslate nohighlight">\(p = p \times U\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(p &gt; L\)</span>: set <span class="math notranslate nohighlight">\(k = k + 1\)</span> and go to step 2</p></li>
<li><p>Return <span class="math notranslate nohighlight">\(k\)</span></p></li>
</ol>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">poisson_product</span><span class="p">(</span><span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">lam</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate Poisson variates via product of uniforms.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of samples to generate.</span>
<span class="sd">    lam : float</span>
<span class="sd">        Rate parameter Œª &gt; 0.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ndarray</span>
<span class="sd">        Poisson(Œª) random variates.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    Expected complexity is O(Œª) per sample‚Äîefficient for small Œª,</span>
<span class="sd">    but slow for large Œª. Use normal approximation for Œª &gt; 30.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">lam</span><span class="p">)</span>

    <span class="n">results</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
        <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">p</span> <span class="o">=</span> <span class="mf">1.0</span>

        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">p</span> <span class="o">*=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">p</span> <span class="o">&lt;=</span> <span class="n">L</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="n">k</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">results</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">k</span>

    <span class="k">return</span> <span class="n">results</span>


<span class="c1"># Verify</span>
<span class="n">lam</span> <span class="o">=</span> <span class="mf">5.0</span>
<span class="n">N</span> <span class="o">=</span> <span class="n">poisson_product</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">lam</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Poisson(</span><span class="si">{</span><span class="n">lam</span><span class="si">}</span><span class="s2">) via product method:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">N</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (expect </span><span class="si">{</span><span class="n">lam</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">N</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (expect </span><span class="si">{</span><span class="n">lam</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Poisson(5.0) via product method:
  Mean: 5.003 (expect 5.000)
  Variance: 5.012 (expect 5.000)
</pre></div>
</div>
<p><strong>Complexity Analysis</strong>: The expected number of iterations equals <span class="math notranslate nohighlight">\(\lambda + 1\)</span>, since we generate on average <span class="math notranslate nohighlight">\(\lambda\)</span> events plus one final uniform that causes termination. This is efficient for small <span class="math notranslate nohighlight">\(\lambda\)</span> but becomes slow for large <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
</section>
<section id="poisson-via-sequential-search">
<h3>Poisson via Sequential Search<a class="headerlink" href="#poisson-via-sequential-search" title="Link to this heading">ÔÉÅ</a></h3>
<p>An alternative approach uses the inverse CDF with sequential search. We compute <span class="math notranslate nohighlight">\(P(N \leq k)\)</span> incrementally and stop when it exceeds <span class="math notranslate nohighlight">\(U\)</span>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">poisson_sequential</span><span class="p">(</span><span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">lam</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate Poisson variates via sequential CDF search.</span>

<span class="sd">    More numerically stable for moderate Œª than the product method.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="n">results</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
        <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">lam</span><span class="p">)</span>  <span class="c1"># P(N = 0)</span>
        <span class="n">F</span> <span class="o">=</span> <span class="n">p</span>             <span class="c1"># P(N ‚â§ 0)</span>

        <span class="k">while</span> <span class="n">U</span> <span class="o">&gt;</span> <span class="n">F</span><span class="p">:</span>
            <span class="n">k</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">p</span> <span class="o">*=</span> <span class="n">lam</span> <span class="o">/</span> <span class="n">k</span>   <span class="c1"># P(N = k) = P(N = k-1) √ó Œª/k</span>
            <span class="n">F</span> <span class="o">+=</span> <span class="n">p</span>

        <span class="n">results</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">k</span>

    <span class="k">return</span> <span class="n">results</span>


<span class="c1"># Verify</span>
<span class="n">N_seq</span> <span class="o">=</span> <span class="n">poisson_sequential</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Poisson(5) via sequential search:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">N_seq</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, Variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">N_seq</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Poisson(5) via sequential search:
  Mean: 4.997, Variance: 4.986
</pre></div>
</div>
</section>
<section id="poisson-normal-approximation-for-large">
<h3>Poisson: Normal Approximation for Large Œª<a class="headerlink" href="#poisson-normal-approximation-for-large" title="Link to this heading">ÔÉÅ</a></h3>
<p>For large <span class="math notranslate nohighlight">\(\lambda\)</span>, both the product and sequential methods become slow. The Central Limit Theorem comes to the rescue: as <span class="math notranslate nohighlight">\(\lambda \to \infty\)</span>, the Poisson distribution approaches a normal distribution:</p>
<div class="math notranslate nohighlight">
\[\frac{N - \lambda}{\sqrt{\lambda}} \xrightarrow{d} \mathcal{N}(0, 1)\]</div>
<p>This suggests the approximation:</p>
<div class="math notranslate nohighlight">
\[N \approx \text{round}(\lambda + \sqrt{\lambda} \cdot Z) \quad \text{where } Z \sim \mathcal{N}(0, 1)\]</div>
<p>with appropriate handling to ensure non-negativity.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">poisson_normal_approx</span><span class="p">(</span><span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">lam</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate Poisson variates via normal approximation.</span>

<span class="sd">    Accurate for Œª &gt; 30. Uses continuity correction.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Normal approximation with continuity correction</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="n">N_approx</span> <span class="o">=</span> <span class="n">lam</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">lam</span><span class="p">)</span> <span class="o">*</span> <span class="n">Z</span>

    <span class="c1"># Round and ensure non-negative</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">N_approx</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">N</span>


<span class="c1"># Compare methods for large Œª</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="n">lam_large</span> <span class="o">=</span> <span class="mf">100.0</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100_000</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
<span class="n">N_product</span> <span class="o">=</span> <span class="n">poisson_product</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">lam_large</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">time_product</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
<span class="n">N_normal</span> <span class="o">=</span> <span class="n">poisson_normal_approx</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">lam_large</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">time_normal</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Poisson(</span><span class="si">{</span><span class="n">lam_large</span><span class="si">}</span><span class="s2">) comparison:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Product method:  mean=</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">N_product</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, time=</span><span class="si">{</span><span class="mi">1000</span><span class="o">*</span><span class="n">time_product</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">ms&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Normal approx:   mean=</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">N_normal</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, time=</span><span class="si">{</span><span class="mi">1000</span><span class="o">*</span><span class="n">time_normal</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">ms&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Speedup: </span><span class="si">{</span><span class="n">time_product</span><span class="o">/</span><span class="n">time_normal</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">√ó&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Poisson(100.0) comparison:
  Product method:  mean=100.01, time=1847ms
  Normal approx:   mean=100.00, time=2.3ms
  Speedup: 803√ó
</pre></div>
</div>
<p>For <span class="math notranslate nohighlight">\(\lambda &gt; 30\)</span>, the normal approximation is both faster and sufficiently accurate for most applications. Libraries like NumPy use sophisticated algorithms (often based on the PTRD algorithm of H√∂rmann) that combine multiple approaches for optimal performance across all <span class="math notranslate nohighlight">\(\lambda\)</span> values.</p>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ‚ö†Ô∏è</p>
<p><strong>Don‚Äôt use the product method for large Œª.</strong> With <span class="math notranslate nohighlight">\(\lambda = 1000\)</span>, the product method requires ~1001 uniform random numbers and multiplications per sample. The normal approximation needs just one normal variate. Always check <span class="math notranslate nohighlight">\(\lambda\)</span> and switch methods accordingly.</p>
</div>
</section>
<section id="geometric-distribution">
<h3>Geometric Distribution<a class="headerlink" href="#geometric-distribution" title="Link to this heading">ÔÉÅ</a></h3>
<p>The Geometric distribution models the number of trials until the first success in a sequence of independent Bernoulli(<span class="math notranslate nohighlight">\(p\)</span>) trials. With the convention that <span class="math notranslate nohighlight">\(X\)</span> is the number of failures before the first success:</p>
<div class="math notranslate nohighlight">
\[P(X = k) = (1-p)^k p \quad \text{for } k = 0, 1, 2, \ldots\]</div>
<p>The Geometric distribution has a <strong>closed-form inverse CDF</strong>, making it one of the few infinite discrete distributions amenable to direct inversion.</p>
<p>The CDF is:</p>
<div class="math notranslate nohighlight">
\[F(k) = P(X \leq k) = 1 - (1-p)^{k+1}\]</div>
<p>Setting <span class="math notranslate nohighlight">\(U = F(k)\)</span> and solving for <span class="math notranslate nohighlight">\(k\)</span>:</p>
<div class="math notranslate nohighlight">
\[U = 1 - (1-p)^{k+1} \implies k = \left\lfloor \frac{\ln(1-U)}{\ln(1-p)} \right\rfloor\]</div>
<p>Since <span class="math notranslate nohighlight">\(1 - U \sim \text{Uniform}(0, 1)\)</span> when <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0, 1)\)</span>, we can use:</p>
<div class="math notranslate nohighlight">
\[X = \left\lfloor \frac{\ln U}{\ln(1-p)} \right\rfloor \sim \text{Geometric}(p)\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">geometric</span><span class="p">(</span><span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate Geometric variates via inverse CDF.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of samples.</span>
<span class="sd">    p : float</span>
<span class="sd">        Success probability (0 &lt; p ‚â§ 1).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ndarray</span>
<span class="sd">        Geometric(p) variates (number of failures before first success).</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    Uses the closed-form inverse CDF: floor(ln(U) / ln(1-p)).</span>
<span class="sd">    O(1) per sample‚Äîextremely efficient.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="c1"># Guard against U = 0 (would give -inf)</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">)</span>

    <span class="c1"># Inverse CDF</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">U</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">X</span>


<span class="c1"># Verify</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">geometric</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">theoretical_mean</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span> <span class="o">/</span> <span class="n">p</span>
<span class="n">theoretical_var</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span> <span class="o">/</span> <span class="n">p</span><span class="o">**</span><span class="mi">2</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Geometric(</span><span class="si">{</span><span class="n">p</span><span class="si">}</span><span class="s2">) verification:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (expect </span><span class="si">{</span><span class="n">theoretical_mean</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (expect </span><span class="si">{</span><span class="n">theoretical_var</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Geometric(0.3) verification:
  Mean: 2.328 (expect 2.333)
  Variance: 7.755 (expect 7.778)
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Convention varies: some texts define Geometric as the number of trials (including the success), giving support <span class="math notranslate nohighlight">\(\{1, 2, 3, \ldots\}\)</span> and mean <span class="math notranslate nohighlight">\(1/p\)</span>. The formula above uses the ‚Äúnumber of failures‚Äù convention with support <span class="math notranslate nohighlight">\(\{0, 1, 2, \ldots\}\)</span> and mean <span class="math notranslate nohighlight">\((1-p)/p\)</span>. NumPy‚Äôs <code class="docutils literal notranslate"><span class="pre">rng.geometric(p)</span></code> uses the ‚Äúnumber of trials‚Äù convention.</p>
</div>
</section>
<section id="negative-binomial-distribution">
<h3>Negative Binomial Distribution<a class="headerlink" href="#negative-binomial-distribution" title="Link to this heading">ÔÉÅ</a></h3>
<p>The Negative Binomial distribution generalizes the Geometric: it models the number of failures before <span class="math notranslate nohighlight">\(r\)</span> successes in Bernoulli trials. With <span class="math notranslate nohighlight">\(X \sim \text{NegBin}(r, p)\)</span>:</p>
<div class="math notranslate nohighlight">
\[P(X = k) = \binom{k + r - 1}{k} p^r (1-p)^k \quad \text{for } k = 0, 1, 2, \ldots\]</div>
<p><strong>Properties</strong>:</p>
<ul class="simple">
<li><p>Mean: <span class="math notranslate nohighlight">\(\mathbb{E}[X] = r(1-p)/p\)</span></p></li>
<li><p>Variance: <span class="math notranslate nohighlight">\(\text{Var}(X) = r(1-p)/p^2\)</span></p></li>
</ul>
<p>For integer <span class="math notranslate nohighlight">\(r\)</span>, the Negative Binomial is a sum of <span class="math notranslate nohighlight">\(r\)</span> independent Geometric random variables:</p>
<div class="math notranslate nohighlight">
\[X = X_1 + X_2 + \cdots + X_r \quad \text{where } X_i \stackrel{\text{iid}}{\sim} \text{Geometric}(p)\]</div>
<p>This gives a straightforward generator for integer <span class="math notranslate nohighlight">\(r\)</span>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">negative_binomial_sum</span><span class="p">(</span><span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">r</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                           <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate Negative Binomial variates as sum of Geometrics.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of samples.</span>
<span class="sd">    r : int</span>
<span class="sd">        Number of successes (positive integer).</span>
<span class="sd">    p : float</span>
<span class="sd">        Success probability.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ndarray</span>
<span class="sd">        NegBin(r, p) variates (failures before r successes).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Sum of r independent Geometrics</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">r</span><span class="p">))</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">)</span>

    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">U</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">X</span>


<span class="c1"># Verify</span>
<span class="n">r</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.4</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">negative_binomial_sum</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">theoretical_mean</span> <span class="o">=</span> <span class="n">r</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span> <span class="o">/</span> <span class="n">p</span>
<span class="n">theoretical_var</span> <span class="o">=</span> <span class="n">r</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span> <span class="o">/</span> <span class="n">p</span><span class="o">**</span><span class="mi">2</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;NegBin(</span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">p</span><span class="si">}</span><span class="s2">) verification:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (expect </span><span class="si">{</span><span class="n">theoretical_mean</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (expect </span><span class="si">{</span><span class="n">theoretical_var</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>NegBin(5, 0.4) verification:
  Mean: 7.490 (expect 7.500)
  Variance: 18.684 (expect 18.750)
</pre></div>
</div>
</section>
<section id="negative-binomial-via-gamma-poisson-mixture">
<h3>Negative Binomial via Gamma-Poisson Mixture<a class="headerlink" href="#negative-binomial-via-gamma-poisson-mixture" title="Link to this heading">ÔÉÅ</a></h3>
<p>For non-integer <span class="math notranslate nohighlight">\(r\)</span>, or when <span class="math notranslate nohighlight">\(r\)</span> is large, a more elegant approach uses the <strong>Gamma-Poisson mixture</strong> representation. The Negative Binomial can be expressed as a Poisson distribution with a Gamma-distributed rate:</p>
<div class="math notranslate nohighlight">
\[X | \Lambda \sim \text{Poisson}(\Lambda), \quad \Lambda \sim \text{Gamma}(r, (1-p)/p)\]</div>
<p>Marginalizing over <span class="math notranslate nohighlight">\(\Lambda\)</span> yields <span class="math notranslate nohighlight">\(X \sim \text{NegBin}(r, p)\)</span>.</p>
<p>This representation is powerful because:</p>
<ol class="arabic simple">
<li><p>It works for any <span class="math notranslate nohighlight">\(r &gt; 0\)</span>, not just integers</p></li>
<li><p>It generates each sample with <span class="math notranslate nohighlight">\(O(1)\)</span> Gamma generation plus <span class="math notranslate nohighlight">\(O(\lambda)\)</span> Poisson (but <span class="math notranslate nohighlight">\(\lambda\)</span> is typically moderate)</p></li>
<li><p>It reveals the Negative Binomial as an <strong>overdispersed Poisson</strong>‚Äîthe extra variability comes from the random rate</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">negative_binomial_gamma_poisson</span><span class="p">(</span><span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">r</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                                     <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate Negative Binomial variates via Gamma-Poisson mixture.</span>

<span class="sd">    Works for any r &gt; 0, including non-integers.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of samples.</span>
<span class="sd">    r : float</span>
<span class="sd">        Shape parameter (can be non-integer).</span>
<span class="sd">    p : float</span>
<span class="sd">        Success probability.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ndarray</span>
<span class="sd">        NegBin(r, p) variates.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Gamma(r, (1-p)/p) = Gamma(shape=r, scale=(1-p)/p)</span>
    <span class="c1"># NumPy uses shape/scale parameterization</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span> <span class="o">/</span> <span class="n">p</span>
    <span class="n">Lambda</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">r</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Poisson with random rate</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="n">lam</span><span class="o">=</span><span class="n">Lambda</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">X</span>


<span class="c1"># Verify with non-integer r</span>
<span class="n">r</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">0.4</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">negative_binomial_gamma_poisson</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">theoretical_mean</span> <span class="o">=</span> <span class="n">r</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span> <span class="o">/</span> <span class="n">p</span>
<span class="n">theoretical_var</span> <span class="o">=</span> <span class="n">r</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span> <span class="o">/</span> <span class="n">p</span><span class="o">**</span><span class="mi">2</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;NegBin(</span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">p</span><span class="si">}</span><span class="s2">) via Gamma-Poisson:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (expect </span><span class="si">{</span><span class="n">theoretical_mean</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (expect </span><span class="si">{</span><span class="n">theoretical_var</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>NegBin(3.5, 0.4) via Gamma-Poisson:
  Mean: 5.253 (expect 5.250)
  Variance: 13.147 (expect 13.125)
</pre></div>
</div>
<figure class="align-center" id="id8">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_4_fig07_infinite_discrete.png"><img alt="Four-panel visualization of infinite discrete distribution methods" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_4_fig07_infinite_discrete.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.6 </span><span class="caption-text"><strong>Infinite Discrete Distributions.</strong> Top-left: Poisson via product of uniforms matches theoretical PMF. Top-right: Geometric via closed-form inverse CDF achieves O(1) sampling. Bottom-left: Poisson method comparison shows normal approximation dramatically faster for large Œª. Bottom-right: Negative Binomial via Gamma-Poisson mixture works for any r &gt; 0.</span><a class="headerlink" href="#id8" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
</section>
<section id="summary-of-infinite-discrete-methods">
<h3>Summary of Infinite Discrete Methods<a class="headerlink" href="#summary-of-infinite-discrete-methods" title="Link to this heading">ÔÉÅ</a></h3>
<table class="docutils align-default" id="id9">
<caption><span class="caption-number">Table 2.2 </span><span class="caption-text">Infinite Discrete Distribution Methods</span><a class="headerlink" href="#id9" title="Link to this table">ÔÉÅ</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 30.0%" />
<col style="width: 20.0%" />
<col style="width: 30.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Distribution</p></th>
<th class="head"><p>Method</p></th>
<th class="head"><p>Complexity</p></th>
<th class="head"><p>Best For</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Poisson(<span class="math notranslate nohighlight">\(\lambda\)</span>)</p></td>
<td><p>Product of uniforms</p></td>
<td><p><span class="math notranslate nohighlight">\(O(\lambda)\)</span></p></td>
<td><p>Small <span class="math notranslate nohighlight">\(\lambda &lt; 30\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Poisson(<span class="math notranslate nohighlight">\(\lambda\)</span>)</p></td>
<td><p>Normal approximation</p></td>
<td><p><span class="math notranslate nohighlight">\(O(1)\)</span></p></td>
<td><p>Large <span class="math notranslate nohighlight">\(\lambda &gt; 30\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Geometric(<span class="math notranslate nohighlight">\(p\)</span>)</p></td>
<td><p>Inverse CDF (closed-form)</p></td>
<td><p><span class="math notranslate nohighlight">\(O(1)\)</span></p></td>
<td><p>All <span class="math notranslate nohighlight">\(p\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>NegBin(<span class="math notranslate nohighlight">\(r, p\)</span>)</p></td>
<td><p>Sum of Geometrics</p></td>
<td><p><span class="math notranslate nohighlight">\(O(r)\)</span></p></td>
<td><p>Integer <span class="math notranslate nohighlight">\(r\)</span>, small</p></td>
</tr>
<tr class="row-even"><td><p>NegBin(<span class="math notranslate nohighlight">\(r, p\)</span>)</p></td>
<td><p>Gamma-Poisson mixture</p></td>
<td><p><span class="math notranslate nohighlight">\(O(1)\)</span> + Poisson</p></td>
<td><p>Any <span class="math notranslate nohighlight">\(r &gt; 0\)</span></p></td>
</tr>
</tbody>
</table>
<p>The theme throughout is exploiting relationships between distributions: Poisson connects to exponential waiting times, Geometric has a tractable inverse CDF, and Negative Binomial decomposes as either a Geometric sum or a Gamma-Poisson mixture. These connections transform infinite discrete sampling from an impossible precomputation problem into efficient algorithms.</p>
</section>
</section>
<section id="multivariate-normal-generation">
<h2>Multivariate Normal Generation<a class="headerlink" href="#multivariate-normal-generation" title="Link to this heading">ÔÉÅ</a></h2>
<p>Many applications require samples from multivariate normal distributions: Bayesian posterior simulation, Gaussian process regression, financial modeling with correlated assets, and countless others. The multivariate normal is fully characterized by its mean vector <span class="math notranslate nohighlight">\(\boldsymbol{\mu} \in \mathbb{R}^d\)</span> and covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma} \in \mathbb{R}^{d \times d}\)</span> (symmetric positive semi-definite).</p>
<section id="the-fundamental-transformation">
<h3>The Fundamental Transformation<a class="headerlink" href="#the-fundamental-transformation" title="Link to this heading">ÔÉÅ</a></h3>
<p>The key insight is that <strong>any multivariate normal can be obtained from independent standard normals via a linear transformation</strong>.</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Multivariate Normal via Linear Transform</p>
<p>If <span class="math notranslate nohighlight">\(\mathbf{Z} = (Z_1, \ldots, Z_d)^T\)</span> where <span class="math notranslate nohighlight">\(Z_i \stackrel{\text{iid}}{\sim} \mathcal{N}(0, 1)\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is a <span class="math notranslate nohighlight">\(d \times d\)</span> matrix with <span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{A}^T = \boldsymbol{\Sigma}\)</span>, then:</p>
<div class="math notranslate nohighlight">
\[\mathbf{X} = \boldsymbol{\mu} + \mathbf{A}\mathbf{Z} \sim \mathcal{N}_d(\boldsymbol{\mu}, \boldsymbol{\Sigma})\]</div>
</div>
<p><strong>Proof</strong>: The random vector <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is a linear transformation of <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span>, hence normal. Its mean is:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[\mathbf{X}] = \boldsymbol{\mu} + \mathbf{A}\mathbb{E}[\mathbf{Z}] = \boldsymbol{\mu}\]</div>
<p>Its covariance is:</p>
<div class="math notranslate nohighlight">
\[\text{Cov}(\mathbf{X}) = \mathbf{A} \text{Cov}(\mathbf{Z}) \mathbf{A}^T = \mathbf{A} \mathbf{I} \mathbf{A}^T = \mathbf{A}\mathbf{A}^T = \boldsymbol{\Sigma}\]</div>
<p>Thus <span class="math notranslate nohighlight">\(\mathbf{X} \sim \mathcal{N}_d(\boldsymbol{\mu}, \boldsymbol{\Sigma})\)</span>. ‚àé</p>
<p>The question becomes: <strong>how do we find</strong> <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> <strong>such that</strong> <span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{A}^T = \boldsymbol{\Sigma}\)</span>?</p>
</section>
<section id="cholesky-factorization">
<h3>Cholesky Factorization<a class="headerlink" href="#cholesky-factorization" title="Link to this heading">ÔÉÅ</a></h3>
<p>For positive definite <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>, the <strong>Cholesky factorization</strong> provides a unique lower-triangular matrix <span class="math notranslate nohighlight">\(\mathbf{L}\)</span> with positive diagonal entries such that:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\Sigma} = \mathbf{L}\mathbf{L}^T\]</div>
<p>This is the standard approach for multivariate normal generation because:</p>
<ol class="arabic simple">
<li><p><strong>Efficiency</strong>: Cholesky factorization has complexity <span class="math notranslate nohighlight">\(O(d^3/3)\)</span>, about half that of full matrix inversion.</p></li>
<li><p><strong>Numerical stability</strong>: The algorithm is numerically stable for well-conditioned matrices.</p></li>
<li><p><strong>Triangular structure</strong>: Multiplying by a triangular matrix is fast (<span class="math notranslate nohighlight">\(O(d^2)\)</span> per sample).</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">mvnormal_cholesky</span><span class="p">(</span><span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">mean</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">cov</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                       <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate multivariate normal samples using Cholesky factorization.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of samples.</span>
<span class="sd">    mean : ndarray of shape (d,)</span>
<span class="sd">        Mean vector.</span>
<span class="sd">    cov : ndarray of shape (d, d)</span>
<span class="sd">        Covariance matrix (must be positive definite).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ndarray of shape (n_samples, d)</span>
<span class="sd">        Multivariate normal samples.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">mean</span><span class="p">)</span>

    <span class="c1"># Cholesky factorization: Œ£ = L L^T</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>

    <span class="c1"># Generate independent standard normals</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>

    <span class="c1"># Transform: X = Œº + L @ Z^T ‚Üí each row is one sample</span>
    <span class="c1"># More efficient: X = Œº + Z @ L^T (to get shape (n_samples, d))</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">mean</span> <span class="o">+</span> <span class="n">Z</span> <span class="o">@</span> <span class="n">L</span><span class="o">.</span><span class="n">T</span>

    <span class="k">return</span> <span class="n">X</span>


<span class="c1"># Example: 3D multivariate normal</span>
<span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">]</span>
<span class="p">])</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">mvnormal_cholesky</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Multivariate Normal (Cholesky) Verification:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Sample mean: </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  True mean:   </span><span class="si">{</span><span class="n">mean</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">  Sample covariance:</span><span class="se">\n</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">  True covariance:</span><span class="se">\n</span><span class="si">{</span><span class="n">cov</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Multivariate Normal (Cholesky) Verification:
  Sample mean: [1.001 2.001 3.   ]
  True mean:   [1. 2. 3.]

  Sample covariance:
[[1.002 0.5   0.301]
 [0.5   2.001 0.601]
 [0.301 0.601 1.496]]

  True covariance:
[[1.  0.5 0.3]
 [0.5 2.  0.6]
 [0.3 0.6 1.5]]
</pre></div>
</div>
</section>
<section id="eigenvalue-decomposition">
<h3>Eigenvalue Decomposition<a class="headerlink" href="#eigenvalue-decomposition" title="Link to this heading">ÔÉÅ</a></h3>
<p>An alternative factorization uses the <strong>eigendecomposition</strong> of <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\Sigma} = \mathbf{Q} \boldsymbol{\Lambda} \mathbf{Q}^T\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> is orthogonal (columns are eigenvectors) and <span class="math notranslate nohighlight">\(\boldsymbol{\Lambda} = \text{diag}(\lambda_1, \ldots, \lambda_d)\)</span> contains eigenvalues.</p>
<p>We can then use:</p>
<div class="math notranslate nohighlight">
\[\mathbf{A} = \mathbf{Q} \boldsymbol{\Lambda}^{1/2} \quad \text{where } \boldsymbol{\Lambda}^{1/2} = \text{diag}(\sqrt{\lambda_1}, \ldots, \sqrt{\lambda_d})\]</div>
<p>This satisfies <span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{A}^T = \mathbf{Q}\boldsymbol{\Lambda}^{1/2}\boldsymbol{\Lambda}^{1/2}\mathbf{Q}^T = \mathbf{Q}\boldsymbol{\Lambda}\mathbf{Q}^T = \boldsymbol{\Sigma}\)</span>.</p>
<p><strong>Advantages of eigendecomposition</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Interpretability</strong>: Eigenvectors are the principal components; eigenvalues indicate variance explained.</p></li>
<li><p><strong>PSD handling</strong>: If some eigenvalues are zero (positive semi-definite case), we can still generate samples in the non-degenerate subspace.</p></li>
<li><p><strong>Numerical flexibility</strong>: Can handle near-singular matrices better than Cholesky in some cases.</p></li>
</ol>
<p><strong>Disadvantage</strong>: About twice as expensive as Cholesky (<span class="math notranslate nohighlight">\(O(d^3)\)</span> for full eigendecomposition vs <span class="math notranslate nohighlight">\(O(d^3/3)\)</span> for Cholesky).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">mvnormal_eigen</span><span class="p">(</span><span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">mean</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">cov</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                    <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate multivariate normal samples using eigendecomposition.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of samples.</span>
<span class="sd">    mean : ndarray of shape (d,)</span>
<span class="sd">        Mean vector.</span>
<span class="sd">    cov : ndarray of shape (d, d)</span>
<span class="sd">        Covariance matrix (must be positive semi-definite).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ndarray of shape (n_samples, d)</span>
<span class="sd">        Multivariate normal samples.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">mean</span><span class="p">)</span>

    <span class="c1"># Eigendecomposition</span>
    <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>

    <span class="c1"># Handle numerical issues: clamp tiny negative eigenvalues to zero</span>
    <span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># A = Q @ diag(sqrt(Œª))</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">Q</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">))</span>

    <span class="c1"># Generate samples</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">mean</span> <span class="o">+</span> <span class="n">Z</span> <span class="o">@</span> <span class="n">A</span><span class="o">.</span><span class="n">T</span>

    <span class="k">return</span> <span class="n">X</span>


<span class="c1"># Verify same distribution</span>
<span class="n">X_eigen</span> <span class="o">=</span> <span class="n">mvnormal_eigen</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Eigendecomposition method:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Sample mean: </span><span class="si">{</span><span class="n">X_eigen</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Sample cov diagonal: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X_eigen</span><span class="o">.</span><span class="n">T</span><span class="p">))</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Eigendecomposition method:
  Sample mean: [1.001 2.001 3.   ]
  Sample cov diagonal: [1.002 2.001 1.496]
</pre></div>
</div>
</section>
<section id="numerical-stability-and-psd-issues">
<h3>Numerical Stability and PSD Issues<a class="headerlink" href="#numerical-stability-and-psd-issues" title="Link to this heading">ÔÉÅ</a></h3>
<p>In practice, covariance matrices are often constructed from data or derived through computation, and numerical errors can cause problems:</p>
<ol class="arabic simple">
<li><p><strong>Near-singularity</strong>: Eigenvalues very close to zero make the matrix effectively singular.</p></li>
<li><p><strong>Numerical non-PSD</strong>: Rounding errors can produce matrices with tiny negative eigenvalues.</p></li>
<li><p><strong>Ill-conditioning</strong>: Large condition number (ratio of largest to smallest eigenvalue) causes numerical instability.</p></li>
</ol>
<p><strong>Common remedies</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">ensure_psd</span><span class="p">(</span><span class="n">cov</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">min_eigenvalue</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-10</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Ensure a matrix is positive semi-definite by clamping eigenvalues.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    cov : ndarray</span>
<span class="sd">        Input covariance matrix.</span>
<span class="sd">    min_eigenvalue : float</span>
<span class="sd">        Minimum eigenvalue to allow.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ndarray</span>
<span class="sd">        Adjusted PSD matrix.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>

    <span class="c1"># Clamp negative eigenvalues</span>
    <span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">,</span> <span class="n">min_eigenvalue</span><span class="p">)</span>

    <span class="c1"># Reconstruct</span>
    <span class="k">return</span> <span class="n">Q</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)</span> <span class="o">@</span> <span class="n">Q</span><span class="o">.</span><span class="n">T</span>


<span class="k">def</span><span class="w"> </span><span class="nf">add_jitter</span><span class="p">(</span><span class="n">cov</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">jitter</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Add small diagonal jitter for numerical stability.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    cov : ndarray</span>
<span class="sd">        Input covariance matrix.</span>
<span class="sd">    jitter : float</span>
<span class="sd">        Value to add to diagonal.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ndarray</span>
<span class="sd">        Stabilized matrix.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">cov</span> <span class="o">+</span> <span class="n">jitter</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cov</span><span class="p">))</span>


<span class="k">def</span><span class="w"> </span><span class="nf">safe_cholesky</span><span class="p">(</span><span class="n">cov</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">max_jitter</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Attempt Cholesky with increasing jitter if needed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">jitter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">cov</span> <span class="o">+</span> <span class="n">jitter</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cov</span><span class="p">)))</span>
        <span class="k">except</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">LinAlgError</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">jitter</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">jitter</span> <span class="o">=</span> <span class="mf">1e-10</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">jitter</span> <span class="o">*=</span> <span class="mi">10</span>
            <span class="k">if</span> <span class="n">jitter</span> <span class="o">&gt;</span> <span class="n">max_jitter</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cholesky failed even with jitter </span><span class="si">{</span><span class="n">jitter</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cholesky failed after maximum attempts&quot;</span><span class="p">)</span>


<span class="c1"># Example with near-singular matrix</span>
<span class="n">near_singular_cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">,</span> <span class="mf">0.998</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.999</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.998</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>
<span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Near-singular covariance matrix:&quot;</span><span class="p">)</span>
<span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvalsh</span><span class="p">(</span><span class="n">near_singular_cov</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Eigenvalues: </span><span class="si">{</span><span class="n">eigenvalues</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Condition number: </span><span class="si">{</span><span class="n">eigenvalues</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">eigenvalues</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Try Cholesky with jitter</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">safe_cholesky</span><span class="p">(</span><span class="n">near_singular_cov</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Cholesky succeeded with jitter&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Near-singular covariance matrix:
  Eigenvalues: [0.001 0.002 2.997]
  Condition number: 2997
  Cholesky succeeded with jitter
</pre></div>
</div>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ‚ö†Ô∏è</p>
<p><strong>Manually constructed covariance matrices often fail PSD checks.</strong> If you build <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> element-by-element (e.g., specifying correlations), numerical precision issues or inconsistent correlation values can produce a non-PSD matrix. Always verify using eigenvalues or attempt Cholesky before generating samples.</p>
</div>
<figure class="align-center" id="id10">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_4_fig06_mvn_generation.png"><img alt="Comparison of MVN generation methods showing elliptical contours" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_4_fig06_mvn_generation.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 2.7 </span><span class="caption-text"><strong>Multivariate Normal Generation.</strong> Left: 2D standard normal (<span class="math notranslate nohighlight">\(\mathbf{Z}\)</span>) samples form a circular cloud. Center: Cholesky transform <span class="math notranslate nohighlight">\(\mathbf{X} = \mathbf{L}\mathbf{Z}\)</span> stretches and rotates to match the target covariance. Right: The resulting samples follow the correct elliptical contours determined by <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>. Both Cholesky and eigendecomposition produce identical distributions; the choice is a matter of numerical efficiency and stability.</span><a class="headerlink" href="#id10" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="practical-considerations">
<h2>Practical Considerations<a class="headerlink" href="#practical-considerations" title="Link to this heading">ÔÉÅ</a></h2>
<section id="boundary-handling">
<h3>Boundary Handling<a class="headerlink" href="#boundary-handling" title="Link to this heading">ÔÉÅ</a></h3>
<p>Several transformation methods involve <span class="math notranslate nohighlight">\(\ln U\)</span> where <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0, 1)\)</span>. While <span class="math notranslate nohighlight">\(P(U = 0) = 0\)</span> theoretically, defensive coding prevents rare floating-point edge cases:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Guard against U = 0</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">tiny</span><span class="p">)</span>  <span class="c1"># ~2.2e-308 for float64</span>

<span class="c1"># Alternative: use 1-U form when appropriate</span>
<span class="c1"># For exponential: -ln(U) or -ln(1-U) are equivalent in distribution</span>
<span class="c1"># but -ln(1-U) is better when U near 1 is the concern</span>
</pre></div>
</div>
</section>
<section id="vectorization-for-speed">
<h3>Vectorization for Speed<a class="headerlink" href="#vectorization-for-speed" title="Link to this heading">ÔÉÅ</a></h3>
<p>All the implementations in this section use NumPy‚Äôs vectorized operations. This is crucial for performance:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="k">def</span><span class="w"> </span><span class="nf">timing_comparison</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">1_000_000</span><span class="p">):</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

    <span class="c1"># Vectorized (fast)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
    <span class="n">U1</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">U2</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">U1</span><span class="p">,</span> <span class="mf">1e-300</span><span class="p">)))</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">R</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">U2</span><span class="p">)</span>
    <span class="n">vectorized_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

    <span class="c1"># Loop (slow - don&#39;t do this!)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
    <span class="n">Z_loop</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)):</span>  <span class="c1"># Only time a subset</span>
        <span class="n">u1</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
        <span class="n">u2</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">u1</span><span class="p">,</span> <span class="mf">1e-300</span><span class="p">)))</span>
        <span class="n">Z_loop</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">r</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">u2</span><span class="p">)</span>
    <span class="n">loop_time</span> <span class="o">=</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">n</span> <span class="o">/</span> <span class="mi">10000</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generating </span><span class="si">{</span><span class="n">n</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> normal variates:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Vectorized: </span><span class="si">{</span><span class="mi">1000</span><span class="o">*</span><span class="n">vectorized_time</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> ms&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Loop (est): </span><span class="si">{</span><span class="mi">1000</span><span class="o">*</span><span class="n">loop_time</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2"> ms&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Speedup:    </span><span class="si">{</span><span class="n">loop_time</span><span class="o">/</span><span class="n">vectorized_time</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">√ó&quot;</span><span class="p">)</span>

<span class="n">timing_comparison</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Generating 1,000,000 normal variates:
  Vectorized: 42.3 ms
  Loop (est): 4521 ms
  Speedup:    107√ó
</pre></div>
</div>
</section>
<section id="choosing-the-right-method">
<h3>Choosing the Right Method<a class="headerlink" href="#choosing-the-right-method" title="Link to this heading">ÔÉÅ</a></h3>
<table class="docutils align-default" id="id11">
<caption><span class="caption-number">Table 2.3 </span><span class="caption-text">Method Selection Guide</span><a class="headerlink" href="#id11" title="Link to this table">ÔÉÅ</a></caption>
<colgroup>
<col style="width: 25.0%" />
<col style="width: 35.0%" />
<col style="width: 40.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Scenario</p></th>
<th class="head"><p>Recommended Method</p></th>
<th class="head"><p>Rationale</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Normal variates (general)</p></td>
<td><p>Library function (NumPy)</p></td>
<td><p>Uses optimized Ziggurat</p></td>
</tr>
<tr class="row-odd"><td><p>Normal (custom/educational)</p></td>
<td><p>Polar (Marsaglia)</p></td>
<td><p>Fast, no trig in inner loop</p></td>
</tr>
<tr class="row-even"><td><p>Chi-squared (integer df)</p></td>
<td><p>Sum of squared normals</p></td>
<td><p>Direct, simple</p></td>
</tr>
<tr class="row-odd"><td><p>Chi-squared (non-integer df)</p></td>
<td><p>Gamma generator</p></td>
<td><p>Use <code class="docutils literal notranslate"><span class="pre">rng.gamma(df/2,</span> <span class="pre">2)</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Student‚Äôs t</p></td>
<td><p>Normal / chi-squared ratio</p></td>
<td><p>Matches definition</p></td>
</tr>
<tr class="row-odd"><td><p>Poisson (small <span class="math notranslate nohighlight">\(\lambda &lt; 30\)</span>)</p></td>
<td><p>Product of uniforms</p></td>
<td><p>Simple, exact</p></td>
</tr>
<tr class="row-even"><td><p>Poisson (large <span class="math notranslate nohighlight">\(\lambda \geq 30\)</span>)</p></td>
<td><p>Normal approximation</p></td>
<td><p><span class="math notranslate nohighlight">\(O(1)\)</span> vs <span class="math notranslate nohighlight">\(O(\lambda)\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Geometric</p></td>
<td><p>Closed-form inverse CDF</p></td>
<td><p><span class="math notranslate nohighlight">\(O(1)\)</span>, exact</p></td>
</tr>
<tr class="row-even"><td><p>Negative Binomial (any <span class="math notranslate nohighlight">\(r\)</span>)</p></td>
<td><p>Gamma-Poisson mixture</p></td>
<td><p>Works for non-integer <span class="math notranslate nohighlight">\(r\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>MVN (well-conditioned)</p></td>
<td><p>Cholesky</p></td>
<td><p>Fastest factorization</p></td>
</tr>
<tr class="row-even"><td><p>MVN (ill-conditioned)</p></td>
<td><p>Eigendecomposition + jitter</p></td>
<td><p>More robust</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="bringing-it-all-together">
<h2>Bringing It All Together<a class="headerlink" href="#bringing-it-all-together" title="Link to this heading">ÔÉÅ</a></h2>
<p>Transformation methods exploit mathematical structure to convert simple random variates (uniforms, normals) into complex distributions without iterative root-finding or general-purpose rejection sampling. The key insights are:</p>
<ol class="arabic simple">
<li><p><strong>Geometric insight yields efficiency</strong>: The Box‚ÄìMuller transform recognizes that while <span class="math notranslate nohighlight">\(\Phi^{-1}\)</span> has no closed form, pairs of normals have a simple polar representation. This geometric view transforms an intractable problem into an elegant solution.</p></li>
<li><p><strong>Distributions form a hierarchy</strong>: Once we can generate normals efficiently, chi-squared, t, F, lognormal, and many others follow from simple arithmetic. Understanding these relationships guides algorithm design and provides verification checks.</p></li>
<li><p><strong>Infinite discrete distributions have structure too</strong>: Poisson connects to exponential waiting times, Geometric has a closed-form inverse CDF, and Negative Binomial decomposes as Gamma-Poisson mixtures. These relationships transform impossible precomputation into efficient algorithms.</p></li>
<li><p><strong>Multivariate normals reduce to linear algebra</strong>: The transformation <span class="math notranslate nohighlight">\(\mathbf{X} = \boldsymbol{\mu} + \mathbf{A}\mathbf{Z}\)</span> connects random number generation to matrix factorization. Cholesky is the workhorse; eigendecomposition handles special cases.</p></li>
<li><p><strong>Numerical care pays dividends</strong>: Guarding against <span class="math notranslate nohighlight">\(\ln(0)\)</span>, handling ill-conditioned covariance matrices, and using stable algorithms prevent rare but catastrophic failures.</p></li>
</ol>
<section id="transition-to-rejection-sampling">
<h3>Transition to Rejection Sampling<a class="headerlink" href="#transition-to-rejection-sampling" title="Link to this heading">ÔÉÅ</a></h3>
<p>Transformation methods require that we know an explicit transformation from uniforms (or normals) to the target distribution. But what if no such transformation exists? The gamma distribution, for instance, has no simple closed-form relationship to uniforms or normals (except for integer or half-integer shape parameters).</p>
<p>The next section introduces <strong>rejection sampling</strong>, a universal method that can generate samples from any distribution whose density we can evaluate. The idea is elegant: propose candidates from a simpler distribution and accept or reject them probabilistically. Rejection sampling trades universality for efficiency‚Äîit always works, but may require many attempts per accepted sample. Understanding when to use transformation methods versus rejection sampling is a key skill for the computational statistician.</p>
<div class="important admonition">
<p class="admonition-title">Key Takeaways üìù</p>
<ol class="arabic simple">
<li><p><strong>Box‚ÄìMuller transform</strong>: Converts two Uniform(0,1) variates into two independent N(0,1) variates via <span class="math notranslate nohighlight">\(Z_1 = \sqrt{-2\ln U_1}\cos(2\pi U_2)\)</span>, <span class="math notranslate nohighlight">\(Z_2 = \sqrt{-2\ln U_1}\sin(2\pi U_2)\)</span>. Derivation uses polar coordinates and the Jacobian.</p></li>
<li><p><strong>Polar (Marsaglia) method</strong>: Eliminates trigonometric functions by rejection sampling a uniform point on the unit circle. Expected 1.27 attempts per pair; faster in compiled code.</p></li>
<li><p><strong>Ziggurat algorithm</strong>: Library-standard method achieving near-constant time through layered rectangles. Conceptually elegant but complex to implement correctly‚Äîuse library versions.</p></li>
<li><p><strong>Distribution hierarchy</strong>: From N(0,1), derive <span class="math notranslate nohighlight">\(\chi^2_\nu\)</span> (sum of squares), <span class="math notranslate nohighlight">\(t_\nu\)</span> (ratio with chi-squared), <span class="math notranslate nohighlight">\(F_{\nu_1,\nu_2}\)</span> (chi-squared ratio), LogNormal (exponentiation), Rayleigh (Box‚ÄìMuller radius), and Maxwell (3D magnitude).</p></li>
<li><p><strong>Infinite discrete distributions</strong>: Poisson via product of uniforms (<span class="math notranslate nohighlight">\(O(\lambda)\)</span>) or normal approximation (<span class="math notranslate nohighlight">\(O(1)\)</span> for large <span class="math notranslate nohighlight">\(\lambda\)</span>); Geometric via closed-form inverse CDF (<span class="math notranslate nohighlight">\(O(1)\)</span>); Negative Binomial via Geometric sums or Gamma-Poisson mixture.</p></li>
<li><p><strong>Multivariate normal</strong>: <span class="math notranslate nohighlight">\(\mathbf{X} = \boldsymbol{\mu} + \mathbf{A}\mathbf{Z}\)</span> where <span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{A}^T = \boldsymbol{\Sigma}\)</span>. Use Cholesky for speed, eigendecomposition for robustness or PSD matrices.</p></li>
<li><p><strong>Numerical stability</strong>: Guard against <span class="math notranslate nohighlight">\(\ln(0)\)</span> with <code class="docutils literal notranslate"><span class="pre">np.maximum(U,</span> <span class="pre">tiny)</span></code>. Handle ill-conditioned covariances with jitter or eigenvalue clamping. Always verify PSD property before factorization.</p></li>
<li><p><strong>Outcome alignment</strong>: Transformation methods (Learning Outcome 1) provide efficient, exact sampling for major distributions. Understanding the normal-to-derived-distribution hierarchy and infinite discrete methods is essential for simulation-based inference throughout the course.</p></li>
</ol>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="2.1. Chapter 2: Monte Carlo Simulation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../chapter3/index.html" class="btn btn-neutral float-right" title="2.2. Chapter 3: Frequentist Statistical Inference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>