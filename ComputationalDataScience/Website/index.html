

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Computational Methods in Data Science &mdash; STAT 418</title>
      <link rel="stylesheet" type="text/css" href="/ComputationalDataScience/Website/_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="/ComputationalDataScience/Website/_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="/ComputationalDataScience/Website/_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="/ComputationalDataScience/Website/_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="/ComputationalDataScience/Website/_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="/ComputationalDataScience/Website/_static/custom.css?v=3d0abd52" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT418/Website/index.html" />
      <script src="/ComputationalDataScience/Website/_static/jquery.js?v=5d32c60e"></script>
      <script src="/ComputationalDataScience/Website/_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="/ComputationalDataScience/Website/_static/documentation_options.js?v=f2a433a1"></script>
      <script src="/ComputationalDataScience/Website/_static/doctools.js?v=9bcbadda"></script>
      <script src="/ComputationalDataScience/Website/_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="/ComputationalDataScience/Website/_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="/ComputationalDataScience/Website/_static/copybutton.js?v=30646c52"></script>
      <script>let toggleHintShow = 'Show solution';</script>
      <script>let toggleHintHide = 'Hide solution';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="/ComputationalDataScience/Website/_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script src="/ComputationalDataScience/Website/_static/design-tabs.js?v=f930bc37"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>window.MathJax = {"options": {"enableMenu": true, "menuOptions": {"settings": {"enrich": true, "speech": true, "braille": true, "collapsible": true, "assistiveMml": false}}}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
      <script src="/ComputationalDataScience/Website/_static/custom.js?v=8718e0ab"></script>
    <script src="/ComputationalDataScience/Website/_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Part I: Foundations of Probability and Computation" href="part1_foundations/index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="#" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Course Content</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="part1_foundations/index.html">Part I: Foundations of Probability and Computation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="part1_foundations/chapter1/index.html">Chapter 1: Statistical Paradigms and Core Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html">Section 1.1 Paradigms of Probability and Statistical Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html#the-mathematical-foundation-kolmogorov-s-axioms">The Mathematical Foundation: Kolmogorov’s Axioms</a></li>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html#interpretations-of-probability">Interpretations of Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html#statistical-inference-paradigms">Statistical Inference Paradigms</a></li>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html#historical-and-philosophical-debates">Historical and Philosophical Debates</a></li>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html#looking-ahead-our-course-focus">Looking Ahead: Our Course Focus</a></li>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="part1_foundations/chapter1/ch1_2-probability_distributions_review.html">Section 1.2 Probability Distributions: Theory and Computation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_2-probability_distributions_review.html#from-abstract-foundations-to-concrete-tools">From Abstract Foundations to Concrete Tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_2-probability_distributions_review.html#the-python-ecosystem-for-probability">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_2-probability_distributions_review.html#introduction-why-probability-distributions-matter">Introduction: Why Probability Distributions Matter</a></li>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_2-probability_distributions_review.html#id1">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_2-probability_distributions_review.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_2-probability_distributions_review.html#continuous-distributions">Continuous Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_2-probability_distributions_review.html#additional-important-distributions">Additional Important Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_2-probability_distributions_review.html#summary-and-practical-guidelines">Summary and Practical Guidelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_2-probability_distributions_review.html#conclusion">Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_2-probability_distributions_review.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="part1_foundations/chapter1/ch1_3-python_random_generation.html">Section 1.3 Python Random Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_3-python_random_generation.html#from-mathematical-distributions-to-computational-samples">From Mathematical Distributions to Computational Samples</a></li>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_3-python_random_generation.html#the-python-ecosystem-at-a-glance">The Python Ecosystem at a Glance</a></li>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_3-python_random_generation.html#understanding-pseudo-random-number-generation">Understanding Pseudo-Random Number Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_3-python_random_generation.html#the-standard-library-random-module">The Standard Library: <code class="docutils literal notranslate"><span class="pre">random</span></code> Module</a></li>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_3-python_random_generation.html#numpy-fast-vectorized-random-sampling">NumPy: Fast Vectorized Random Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_3-python_random_generation.html#scipy-stats-the-complete-statistical-toolkit">SciPy Stats: The Complete Statistical Toolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_3-python_random_generation.html#bringing-it-all-together-library-selection-guide">Bringing It All Together: Library Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_3-python_random_generation.html#looking-ahead-from-random-numbers-to-monte-carlo-methods">Looking Ahead: From Random Numbers to Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_3-python_random_generation.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="part1_foundations/chapter1/ch1_4-chapter-summary.html">Section 1.4 Chapter 1 Summary: Foundations in Place</a><ul>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_4-chapter-summary.html#the-three-pillars-of-chapter-1">The Three Pillars of Chapter 1</a></li>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_4-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_4-chapter-summary.html#what-lies-ahead-the-road-to-simulation">What Lies Ahead: The Road to Simulation</a></li>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_4-chapter-summary.html#chapter-1-exercises-synthesis-problems">Chapter 1 Exercises: Synthesis Problems</a></li>
<li class="toctree-l4"><a class="reference internal" href="part1_foundations/chapter1/ch1_4-chapter-summary.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="part2_frequentist/index.html">Part II: Frequentist Inference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="part2_frequentist/chapter2/index.html">Chapter 2: Monte Carlo Simulation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="part2_frequentist/chapter2/ch2_1-monte-carlo-fundamentals.html">Section 2.1 Monte Carlo Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_1-monte-carlo-fundamentals.html#the-historical-development-of-monte-carlo-methods">The Historical Development of Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_1-monte-carlo-fundamentals.html#the-core-principle-expectation-as-integration">The Core Principle: Expectation as Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_1-monte-carlo-fundamentals.html#theoretical-foundations">Theoretical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_1-monte-carlo-fundamentals.html#variance-estimation-and-confidence-intervals">Variance Estimation and Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_1-monte-carlo-fundamentals.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_1-monte-carlo-fundamentals.html#comparison-with-deterministic-methods">Comparison with Deterministic Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_1-monte-carlo-fundamentals.html#sample-size-determination">Sample Size Determination</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_1-monte-carlo-fundamentals.html#convergence-diagnostics-and-monitoring">Convergence Diagnostics and Monitoring</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_1-monte-carlo-fundamentals.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_1-monte-carlo-fundamentals.html#chapter-2-1-exercises-monte-carlo-fundamentals-mastery">Chapter 2.1 Exercises: Monte Carlo Fundamentals Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_1-monte-carlo-fundamentals.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_1-monte-carlo-fundamentals.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_1-monte-carlo-fundamentals.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_1-monte-carlo-fundamentals.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="part2_frequentist/chapter2/ch2_2-uniform-random-variates.html">Section 2.2 Uniform Random Variates</a><ul>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_2-uniform-random-variates.html#why-uniform-the-universal-currency-of-randomness">Why Uniform? The Universal Currency of Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_2-uniform-random-variates.html#the-paradox-of-computational-randomness">The Paradox of Computational Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_2-uniform-random-variates.html#chaotic-dynamical-systems-an-instructive-failure">Chaotic Dynamical Systems: An Instructive Failure</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_2-uniform-random-variates.html#linear-congruential-generators">Linear Congruential Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_2-uniform-random-variates.html#shift-register-generators">Shift-Register Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_2-uniform-random-variates.html#the-kiss-generator-combining-strategies">The KISS Generator: Combining Strategies</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_2-uniform-random-variates.html#modern-generators-mersenne-twister-and-pcg">Modern Generators: Mersenne Twister and PCG</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_2-uniform-random-variates.html#statistical-testing-of-random-number-generators">Statistical Testing of Random Number Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_2-uniform-random-variates.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_2-uniform-random-variates.html#chapter-2-2-exercises-uniform-random-variates-mastery">Chapter 2.2 Exercises: Uniform Random Variates Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_2-uniform-random-variates.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_2-uniform-random-variates.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_2-uniform-random-variates.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="part2_frequentist/chapter2/ch2_3-inverse-cdf-method.html">Section 2.3 Inverse CDF Method</a><ul>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_3-inverse-cdf-method.html#mathematical-foundations">Mathematical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_3-inverse-cdf-method.html#continuous-distributions-with-closed-form-inverses">Continuous Distributions with Closed-Form Inverses</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_3-inverse-cdf-method.html#numerical-inversion">Numerical Inversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_3-inverse-cdf-method.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_3-inverse-cdf-method.html#mixed-distributions">Mixed Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_3-inverse-cdf-method.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_3-inverse-cdf-method.html#chapter-2-3-exercises-inverse-cdf-method-mastery">Chapter 2.3 Exercises: Inverse CDF Method Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_3-inverse-cdf-method.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_3-inverse-cdf-method.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_3-inverse-cdf-method.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_3-inverse-cdf-method.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="part2_frequentist/chapter2/ch2_4-transformation-methods.html">Section 2.4 Transformation Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_4-transformation-methods.html#why-transformation-methods">Why Transformation Methods?</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_4-transformation-methods.html#the-boxmuller-transform">The Box–Muller Transform</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_4-transformation-methods.html#the-polar-marsaglia-method">The Polar (Marsaglia) Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_4-transformation-methods.html#method-comparison-boxmuller-vs-polar-vs-ziggurat">Method Comparison: Box–Muller vs Polar vs Ziggurat</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_4-transformation-methods.html#the-ziggurat-algorithm">The Ziggurat Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_4-transformation-methods.html#the-clt-approximation-historical">The CLT Approximation (Historical)</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_4-transformation-methods.html#distributions-derived-from-the-normal">Distributions Derived from the Normal</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_4-transformation-methods.html#multivariate-normal-generation">Multivariate Normal Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_4-transformation-methods.html#implementation-guidance">Implementation Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_4-transformation-methods.html#chapter-2-4-exercises-transformation-methods-mastery">Chapter 2.4 Exercises: Transformation Methods Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_4-transformation-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_4-transformation-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="part2_frequentist/chapter2/ch2_5-rejection-sampling.html">Section 2.5 Rejection Sampling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_5-rejection-sampling.html#the-dartboard-intuition">The Dartboard Intuition</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_5-rejection-sampling.html#the-accept-reject-algorithm">The Accept-Reject Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_5-rejection-sampling.html#efficiency-analysis">Efficiency Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_5-rejection-sampling.html#choosing-the-proposal-distribution">Choosing the Proposal Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_5-rejection-sampling.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_5-rejection-sampling.html#the-squeeze-principle">The Squeeze Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_5-rejection-sampling.html#geometric-example-sampling-from-the-unit-disk">Geometric Example: Sampling from the Unit Disk</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_5-rejection-sampling.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_5-rejection-sampling.html#limitations-and-the-curse-of-dimensionality">Limitations and the Curse of Dimensionality</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_5-rejection-sampling.html#connections-to-other-methods">Connections to Other Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_5-rejection-sampling.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_5-rejection-sampling.html#chapter-2-5-exercises-rejection-sampling-mastery">Chapter 2.5 Exercises: Rejection Sampling Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_5-rejection-sampling.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_5-rejection-sampling.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="part2_frequentist/chapter2/ch2_6-variance-reduction-methods.html">Section 2.6 Variance Reduction Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_6-variance-reduction-methods.html#the-variance-reduction-paradigm">The Variance Reduction Paradigm</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_6-variance-reduction-methods.html#importance-sampling">Importance Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_6-variance-reduction-methods.html#control-variates">Control Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_6-variance-reduction-methods.html#antithetic-variates">Antithetic Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_6-variance-reduction-methods.html#stratified-sampling">Stratified Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_6-variance-reduction-methods.html#common-random-numbers">Common Random Numbers</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_6-variance-reduction-methods.html#conditional-monte-carlo-raoblackwellization">Conditional Monte Carlo (Rao–Blackwellization)</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_6-variance-reduction-methods.html#combining-variance-reduction-techniques">Combining Variance Reduction Techniques</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_6-variance-reduction-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_6-variance-reduction-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_6-variance-reduction-methods.html#chapter-2-6-exercises-variance-reduction-mastery">Chapter 2.6 Exercises: Variance Reduction Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_6-variance-reduction-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="part2_frequentist/chapter2/ch2_7-chapter-summary.html">Section 2.7 Chapter 2 Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_7-chapter-summary.html#the-complete-monte-carlo-workflow">The Complete Monte Carlo Workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_7-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_7-chapter-summary.html#quick-reference-tables">Quick Reference Tables</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_7-chapter-summary.html#common-pitfalls-checklist">Common Pitfalls Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_7-chapter-summary.html#connections-to-later-chapters">Connections to Later Chapters</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_7-chapter-summary.html#learning-outcomes-checklist">Learning Outcomes Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_7-chapter-summary.html#further-reading-optimization-and-missing-data">Further Reading: Optimization and Missing Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_7-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter2/ch2_7-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="part2_frequentist/chapter3/index.html">Chapter 3: Parametric Inference and Likelihood Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="part2_frequentist/chapter3/ch3_1-exponential-families.html">Section 3.1 Exponential Families</a><ul>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_1-exponential-families.html#historical-origins-from-scattered-results-to-unified-theory">Historical Origins: From Scattered Results to Unified Theory</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_1-exponential-families.html#the-canonical-exponential-family">The Canonical Exponential Family</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_1-exponential-families.html#converting-familiar-distributions">Converting Familiar Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_1-exponential-families.html#the-log-partition-function-a-moment-generating-machine">The Log-Partition Function: A Moment-Generating Machine</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_1-exponential-families.html#sufficiency-capturing-all-parameter-information">Sufficiency: Capturing All Parameter Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_1-exponential-families.html#minimal-sufficiency-and-completeness">Minimal Sufficiency and Completeness</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_1-exponential-families.html#conjugate-priors-and-bayesian-inference">Conjugate Priors and Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_1-exponential-families.html#exponential-dispersion-models-and-glms">Exponential Dispersion Models and GLMs</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_1-exponential-families.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_1-exponential-families.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_1-exponential-families.html#chapter-3-1-exercises-exponential-families-mastery">Chapter 3.1 Exercises: Exponential Families Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_1-exponential-families.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_1-exponential-families.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html">Section 3.2 Maximum Likelihood Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#the-likelihood-function">The Likelihood Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#the-score-function">The Score Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#fisher-information">Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#closed-form-maximum-likelihood-estimators">Closed-Form Maximum Likelihood Estimators</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#numerical-optimization-for-mle">Numerical Optimization for MLE</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#asymptotic-properties-of-mles">Asymptotic Properties of MLEs</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#the-cramer-rao-lower-bound">The Cramér-Rao Lower Bound</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#the-invariance-property">The Invariance Property</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#likelihood-based-hypothesis-testing">Likelihood-Based Hypothesis Testing</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#confidence-intervals-from-likelihood">Confidence Intervals from Likelihood</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#connection-to-bayesian-inference">Connection to Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#chapter-3-2-exercises-maximum-likelihood-estimation-mastery">Chapter 3.2 Exercises: Maximum Likelihood Estimation Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_2-maximum-likelihood-estimation.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="part2_frequentist/chapter3/ch3_3-sampling-variability.html">Section 3.3 Sampling Variability and Variance Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_3-sampling-variability.html#statistical-estimators-and-their-properties">Statistical Estimators and Their Properties</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_3-sampling-variability.html#sampling-distributions">Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_3-sampling-variability.html#the-delta-method">The Delta Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_3-sampling-variability.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_3-sampling-variability.html#variance-estimation-methods">Variance Estimation Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_3-sampling-variability.html#applications-and-worked-examples">Applications and Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_3-sampling-variability.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_3-sampling-variability.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_3-sampling-variability.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_3-sampling-variability.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="part2_frequentist/chapter3/ch3_4-linear-models.html">Section 3.4 Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_4-linear-models.html#matrix-calculus-foundations">Matrix Calculus Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_4-linear-models.html#the-linear-model">The Linear Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_4-linear-models.html#ordinary-least-squares-the-calculus-approach">Ordinary Least Squares: The Calculus Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_4-linear-models.html#ordinary-least-squares-the-geometric-approach">Ordinary Least Squares: The Geometric Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_4-linear-models.html#properties-of-the-ols-estimator">Properties of the OLS Estimator</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_4-linear-models.html#the-gauss-markov-theorem">The Gauss-Markov Theorem</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_4-linear-models.html#estimating-the-error-variance">Estimating the Error Variance</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_4-linear-models.html#distributional-results-under-normality">Distributional Results Under Normality</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_4-linear-models.html#diagnostics-and-model-checking">Diagnostics and Model Checking</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_4-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_4-linear-models.html#numerical-stability-qr-decomposition">Numerical Stability: QR Decomposition</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_4-linear-models.html#model-selection-and-information-criteria">Model Selection and Information Criteria</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_4-linear-models.html#regularization-ridge-and-lasso">Regularization: Ridge and LASSO</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_4-linear-models.html#chapter-3-4-exercises-linear-models-mastery">Chapter 3.4 Exercises: Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_4-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="part2_frequentist/chapter3/ch3_5-generalized-linear-models.html">Section 3.5 Generalized Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#historical-context-unification-of-regression-methods">Historical Context: Unification of Regression Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#the-glm-framework-three-components">The GLM Framework: Three Components</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#score-equations-and-fisher-information">Score Equations and Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#iteratively-reweighted-least-squares">Iteratively Reweighted Least Squares</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#logistic-regression-binary-outcomes">Logistic Regression: Binary Outcomes</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#poisson-regression-count-data">Poisson Regression: Count Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#gamma-regression-positive-continuous-data">Gamma Regression: Positive Continuous Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#inference-in-glms-the-testing-triad">Inference in GLMs: The Testing Triad</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#model-diagnostics">Model Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#model-comparison-and-selection">Model Comparison and Selection</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#quasi-likelihood-and-robust-inference">Quasi-Likelihood and Robust Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#further-reading">Further Reading</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#chapter-3-5-exercises-generalized-linear-models-mastery">Chapter 3.5 Exercises: Generalized Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_5-generalized-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="part2_frequentist/chapter3/ch3_6-chapter-summary.html">Section 3.6 Chapter 3 Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_6-chapter-summary.html#the-parametric-inference-pipeline">The Parametric Inference Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_6-chapter-summary.html#the-five-pillars-of-chapter-3">The Five Pillars of Chapter 3</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_6-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_6-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_6-chapter-summary.html#quick-reference-core-formulas">Quick Reference: Core Formulas</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_6-chapter-summary.html#connections-to-future-material">Connections to Future Material</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_6-chapter-summary.html#practical-guidance">Practical Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_6-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter3/ch3_6-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="part2_frequentist/chapter4/index.html">Chapter 4: Resampling Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="part2_frequentist/chapter4/ch4_1-sampling-distribution-problem.html">Section 4.1 The Sampling Distribution Problem</a><ul>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_1-sampling-distribution-problem.html#the-fundamental-target-sampling-distributions">The Fundamental Target: Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_1-sampling-distribution-problem.html#historical-development-the-quest-for-sampling-distributions">Historical Development: The Quest for Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_1-sampling-distribution-problem.html#three-routes-to-the-sampling-distribution">Three Routes to the Sampling Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_1-sampling-distribution-problem.html#when-asymptotics-fail-motivating-the-bootstrap">When Asymptotics Fail: Motivating the Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_1-sampling-distribution-problem.html#the-plug-in-principle-theoretical-foundation">The Plug-In Principle: Theoretical Foundation</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_1-sampling-distribution-problem.html#computational-perspective-bootstrap-as-monte-carlo">Computational Perspective: Bootstrap as Monte Carlo</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_1-sampling-distribution-problem.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_1-sampling-distribution-problem.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_1-sampling-distribution-problem.html#chapter-4-1-exercises">Chapter 4.1 Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_1-sampling-distribution-problem.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="part2_frequentist/chapter4/ch4_2-empirical-distribution-plugin.html">Section 4.2 The Empirical Distribution and Plug-in Principle</a><ul>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_2-empirical-distribution-plugin.html#the-empirical-cumulative-distribution-function">The Empirical Cumulative Distribution Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_2-empirical-distribution-plugin.html#convergence-of-the-empirical-cdf">Convergence of the Empirical CDF</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_2-empirical-distribution-plugin.html#parameters-as-statistical-functionals">Parameters as Statistical Functionals</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_2-empirical-distribution-plugin.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_2-empirical-distribution-plugin.html#when-the-plug-in-principle-fails">When the Plug-in Principle Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_2-empirical-distribution-plugin.html#the-bootstrap-idea-in-one-sentence">The Bootstrap Idea in One Sentence</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_2-empirical-distribution-plugin.html#computational-implementation">Computational Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_2-empirical-distribution-plugin.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_2-empirical-distribution-plugin.html#section-4-2-exercises-ecdf-and-plug-in-mastery">Section 4.2 Exercises: ECDF and Plug-in Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_2-empirical-distribution-plugin.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="part2_frequentist/chapter4/ch4_3-nonparametric-bootstrap.html">Section 4.3 The Nonparametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_3-nonparametric-bootstrap.html#the-bootstrap-principle">The Bootstrap Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-standard-errors">Bootstrap Standard Errors</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-bias-estimation">Bootstrap Bias Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-confidence-intervals">Bootstrap Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-for-regression">Bootstrap for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-diagnostics">Bootstrap Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_3-nonparametric-bootstrap.html#when-bootstrap-fails">When Bootstrap Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_3-nonparametric-bootstrap.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_3-nonparametric-bootstrap.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_3-nonparametric-bootstrap.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_3-nonparametric-bootstrap.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="part2_frequentist/chapter4/ch4_4-parametric-bootstrap.html">Section 4.4: The Parametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_4-parametric-bootstrap.html#the-parametric-bootstrap-principle">The Parametric Bootstrap Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_4-parametric-bootstrap.html#location-scale-families">Location-Scale Families</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_4-parametric-bootstrap.html#parametric-bootstrap-for-regression">Parametric Bootstrap for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_4-parametric-bootstrap.html#confidence-intervals">Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_4-parametric-bootstrap.html#model-checking-and-validation">Model Checking and Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_4-parametric-bootstrap.html#when-parametric-bootstrap-fails">When Parametric Bootstrap Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_4-parametric-bootstrap.html#parametric-vs-nonparametric-a-decision-framework">Parametric vs. Nonparametric: A Decision Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_4-parametric-bootstrap.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_4-parametric-bootstrap.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_4-parametric-bootstrap.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="part2_frequentist/chapter4/ch4_5-jackknife-methods.html">Section 4.5: Jackknife Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_5-jackknife-methods.html#historical-context-and-motivation">Historical Context and Motivation</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_5-jackknife-methods.html#the-delete-1-jackknife">The Delete-1 Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_5-jackknife-methods.html#jackknife-bias-estimation">Jackknife Bias Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_5-jackknife-methods.html#the-delete-d-jackknife">The Delete-<span class="math notranslate nohighlight">\(d\)</span> Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_5-jackknife-methods.html#jackknife-versus-bootstrap">Jackknife versus Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_5-jackknife-methods.html#the-infinitesimal-jackknife">The Infinitesimal Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_5-jackknife-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_5-jackknife-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_5-jackknife-methods.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_5-jackknife-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="part2_frequentist/chapter4/ch4_6-bootstrap-hypothesis-testing.html">Section 4.6 Bootstrap Hypothesis Testing and Permutation Tests</a><ul>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_6-bootstrap-hypothesis-testing.html#from-confidence-intervals-to-hypothesis-tests">From Confidence Intervals to Hypothesis Tests</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_6-bootstrap-hypothesis-testing.html#the-bootstrap-hypothesis-testing-framework">The Bootstrap Hypothesis Testing Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_6-bootstrap-hypothesis-testing.html#permutation-tests-exact-tests-under-exchangeability">Permutation Tests: Exact Tests Under Exchangeability</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_6-bootstrap-hypothesis-testing.html#testing-equality-of-distributions">Testing Equality of Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_6-bootstrap-hypothesis-testing.html#bootstrap-tests-for-regression">Bootstrap Tests for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_6-bootstrap-hypothesis-testing.html#bootstrap-vs-classical-tests">Bootstrap vs Classical Tests</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_6-bootstrap-hypothesis-testing.html#permutation-vs-bootstrap-choosing-the-right-approach">Permutation vs Bootstrap: Choosing the Right Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_6-bootstrap-hypothesis-testing.html#multiple-testing-with-bootstrap">Multiple Testing with Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_6-bootstrap-hypothesis-testing.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_6-bootstrap-hypothesis-testing.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_6-bootstrap-hypothesis-testing.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="part2_frequentist/chapter4/ch4_6-bootstrap-hypothesis-testing.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="part3_bayesian/index.html">Part III: Bayesian Inference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="part3_bayesian/chapter5/index.html">Chapter 5: Bayesian Inference</a><ul class="simple">
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="part4_llms_datascience/index.html">Part IV: Large Language Models in Data Science</a><ul>
<li class="toctree-l2"><a class="reference internal" href="part4_llms_datascience/chapter6/index.html">Chapter 6: LLMs in Data Science Workflows</a><ul class="simple">
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Computational Methods in Data Science</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="computational-methods-in-data-science">
<span id="index"></span><h1>Computational Methods in Data Science<a class="headerlink" href="#computational-methods-in-data-science" title="Link to this heading"></a></h1>
<p><em>STAT 418 · Spring 2026 · Purdue University</em></p>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">Course Content</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="part1_foundations/index.html">Part I: Foundations of Probability and Computation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="part1_foundations/chapter1/index.html">Chapter 1: Statistical Paradigms and Core Concepts</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="part2_frequentist/index.html">Part II: Frequentist Inference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="part2_frequentist/chapter2/index.html">Chapter 2: Monte Carlo Simulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="part2_frequentist/chapter3/index.html">Chapter 3: Parametric Inference and Likelihood Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="part2_frequentist/chapter4/index.html">Chapter 4: Resampling Methods</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="part3_bayesian/index.html">Part III: Bayesian Inference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="part3_bayesian/chapter5/index.html">Chapter 5: Bayesian Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="part4_llms_datascience/index.html">Part IV: Large Language Models in Data Science</a><ul>
<li class="toctree-l2"><a class="reference internal" href="part4_llms_datascience/chapter6/index.html">Chapter 6: LLMs in Data Science Workflows</a></li>
</ul>
</li>
</ul>
</div>
<div class="toctree-wrapper compound">
</div>
<hr class="docutils" />
<p>Modern statistics is computational statistics. The elegant formulas of classical theory—derived under assumptions of normality, independence, and large samples—often fail when confronted with real data: messy, high-dimensional, and decidedly non-normal. Yet the core questions remain: How certain should we be? What can we conclude? How might we be wrong?</p>
<p>This course develops the computational methods that answer these questions without relying on fragile assumptions. We replace analytical derivations with simulation, asymptotic approximations with resampling, and conjugate convenience with general-purpose algorithms. The computer becomes not just a calculator but a laboratory for statistical thought experiments.</p>
<p>The intellectual journey moves through four parts. We begin with <strong>foundations</strong>: what probability means, how distributions behave, and how Python’s scientific stack turns theory into computation. We then develop <strong>frequentist inference</strong> computationally: Monte Carlo simulation as the engine, maximum likelihood as the estimator, and bootstrap resampling as the uncertainty quantifier. Next, we explore <strong>Bayesian inference</strong>: prior beliefs, posterior updating, and Markov chain Monte Carlo for models too complex for analytical treatment. Finally, we address <strong>large language models in data science</strong>: integrating pre-trained models into analytical workflows for text preprocessing, feature extraction, data annotation, and retrieval-augmented generation—with careful attention to responsible use, reliability, and privacy.</p>
<p>Throughout, we emphasize both rigor and practice. Every method receives mathematical justification—you’ll understand <em>why</em> these techniques work, not just <em>how</em> to call the functions. But every derivation leads to working code. By course end, you’ll have built a complete toolkit for modern statistical computation, from foundational simulation through cutting-edge AI integration.</p>
<hr class="docutils" />
<section id="course-information">
<h2>Course Information<a class="headerlink" href="#course-information" title="Link to this heading"></a></h2>
<table class="docutils align-default">
<colgroup>
<col style="width: 20.0%" />
<col style="width: 35.0%" />
<col style="width: 20.0%" />
<col style="width: 25.0%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>Instructor</strong></p></td>
<td><p>Dr. Timothy Reese</p></td>
<td><p><strong>Email</strong></p></td>
<td><p><a class="reference external" href="mailto:reese18&#37;&#52;&#48;purdue&#46;edu">reese18<span>&#64;</span>purdue<span>&#46;</span>edu</a></p></td>
</tr>
<tr class="row-even"><td><p><strong>Office</strong></p></td>
<td><p>MATH 210</p></td>
<td><p><strong>Phone</strong></p></td>
<td><p>765-494-4129</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Lectures</strong></p></td>
<td><p>Tue/Thu 1:30–2:45 PM</p></td>
<td><p><strong>Room</strong></p></td>
<td><p>UNIV 127</p></td>
</tr>
<tr class="row-even"><td><p><strong>Office Hours</strong></p></td>
<td><p>Wed/Fri 1:00–3:00 PM</p></td>
<td><p><strong>Location</strong></p></td>
<td><p>MATH 210</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Credits</strong></p></td>
<td><p>3.00</p></td>
<td><p><strong>Website</strong></p></td>
<td><p><a class="reference external" href="https://treese41528.github.io/ComputationalDataScience/Website/index.html">Course Site</a></p></td>
</tr>
</tbody>
</table>
<section id="prerequisites">
<h3>Prerequisites<a class="headerlink" href="#prerequisites" title="Link to this heading"></a></h3>
<p>Enrollment requires completion of the following with a grade of C- or better:</p>
<dl class="simple">
<dt><strong>Probability</strong> (one of):</dt><dd><ul class="simple">
<li><p>STAT 41600: Probability (undergraduate)</p></li>
<li><p>STAT 51600: Basic Probability and Applications (graduate)</p></li>
</ul>
</dd>
<dt><strong>Statistical Inference</strong> (one of):</dt><dd><ul class="simple">
<li><p>STAT 35000: Introduction to Statistics</p></li>
<li><p>STAT 35500: Statistics for Data Science</p></li>
<li><p>STAT 51100: Statistical Methods (graduate)</p></li>
</ul>
</dd>
<dt><strong>Programming</strong> (one of):</dt><dd><ul class="simple">
<li><p>MA 16290: Integrated Calculus and Linear Algebra II</p></li>
<li><p>CS 38003: Python Programming</p></li>
</ul>
</dd>
</dl>
<p>You should be comfortable with: probability axioms and random variables; discrete and continuous distributions (PMFs, PDFs, CDFs); expectation, variance, and covariance; joint, marginal, and conditional distributions; functions and transformations of random variables; moment generating functions (MGFs); the Law of Large Numbers and Central Limit Theorem; Bayes’ theorem; hypothesis testing and confidence intervals; Python programming with NumPy arrays; and calculus through multiple integrals.</p>
</section>
</section>
<hr class="docutils" />
<section id="course-structure">
<h2>Course Structure<a class="headerlink" href="#course-structure" title="Link to this heading"></a></h2>
<p>The course divides into four parts, each building on the previous:</p>
<p><strong>Part I: Foundations of Probability and Computation</strong></p>
<p>What does probability mean? How do we describe and compute with random variables? Part I establishes the mathematical and philosophical groundwork: Kolmogorov’s axioms, frequentist and Bayesian interpretations, probability distributions and their properties, and Python’s ecosystem for statistical computing. These foundations support everything that follows.</p>
<p><strong>Part II: Frequentist Inference</strong></p>
<p>The frequentist asks: “What would happen if I repeated this procedure many times?” Part II develops this perspective computationally. Monte Carlo simulation provides the engine for approximating expectations and probabilities. Maximum likelihood estimation and generalized linear models provide the parametric toolkit. Bootstrap resampling, the jackknife, and permutation tests provide distribution-free inference when parametric assumptions fail.</p>
<p><strong>Part III: Bayesian Inference</strong></p>
<p>The Bayesian asks: “What should I believe given this evidence?” Part III develops posterior inference from prior specification through MCMC computation. We construct models, check their fit, compare alternatives, and extract predictions—all while properly quantifying uncertainty through posterior distributions.</p>
<p><strong>Part IV: Large Language Models in Data Science</strong></p>
<p>How can we leverage pre-trained language models to enhance data science workflows? Part IV addresses the practical and responsible integration of LLMs into analytical pipelines. We cover text preprocessing and feature extraction using embeddings, leveraging pre-trained models for data annotation and augmentation, retrieval-augmented generation (RAG) for domain-specific applications, and responsible AI practices including prompt engineering, reliability assessment, and privacy considerations.</p>
<p>The course culminates in a <strong>capstone project</strong> where you synthesize these methods to address a substantial data science problem, demonstrating both theoretical understanding and practical implementation skill.</p>
</section>
<hr class="docutils" />
<section id="learning-outcomes">
<h2>Learning Outcomes<a class="headerlink" href="#learning-outcomes" title="Link to this heading"></a></h2>
<p>Upon completing this course, you will be able to:</p>
<ol class="arabic simple">
<li><p><strong>Apply simulation techniques</strong> including Monte Carlo methods, transformation approaches, and rejection sampling to analyze probabilistic behavior in data science applications</p></li>
<li><p><strong>Compare and evaluate frequentist and Bayesian inference</strong> paradigms by examining their theoretical foundations, identifying their strengths and limitations, and explaining their roles in statistical modeling and decision-making</p></li>
<li><p><strong>Design, implement, and assess resampling methods</strong> focusing on both nonparametric and parametric forms of the bootstrap to estimate variability, construct confidence intervals, and improve statistical estimates through bias correction techniques</p></li>
<li><p><strong>Apply cross-validation principles</strong> to compute model performance metrics, detect overfitting and underfitting, and select models with reliable predictive accuracy using Python libraries</p></li>
<li><p><strong>Construct and interpret Bayesian models</strong> including posterior distributions and credible intervals, apply Markov chain Monte Carlo methods to approximate posteriors, and evaluate the role of prior distributions in Bayesian inference</p></li>
<li><p><strong>Utilize large language models</strong> in data science workflows for contextual data augmentation, feature engineering, and integrating structured and unstructured data to enhance predictive models, while addressing challenges of privacy and reliability</p></li>
<li><p><strong>Synthesize course methods</strong> in a capstone project to design, develop, and present robust solutions to real-world data science challenges, demonstrating both theoretical understanding and applied expertise</p></li>
</ol>
</section>
<hr class="docutils" />
<section id="assessment">
<h2>Assessment<a class="headerlink" href="#assessment" title="Link to this heading"></a></h2>
<table class="docutils align-default">
<colgroup>
<col style="width: 25.0%" />
<col style="width: 15.0%" />
<col style="width: 60.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Component</p></th>
<th class="head"><p>Weight</p></th>
<th class="head"><p>Details</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Homework</strong></p></td>
<td><p>40%</p></td>
<td><p>6–7 assignments on ~2-week cadence; lowest score dropped; late submissions accepted up to 3 days with 20% penalty</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Midterm Exams</strong></p></td>
<td><p>30%</p></td>
<td><p>Two exams (15% each): Midterm I covers Chapters 1–3 (Foundations, Monte Carlo, Frequentist Inference); Midterm II covers Chapters 4–5 (Resampling Methods, Bayesian Inference)</p></td>
</tr>
<tr class="row-even"><td><p><strong>Capstone Project</strong></p></td>
<td><p>30%</p></td>
<td><p>Proposal (5%), progress report (10%), final submission (15%); demonstrates synthesis of course methods on substantive problem</p></td>
</tr>
</tbody>
</table>
<p><strong>Academic Integrity</strong>: All work governed by Purdue’s Honor Pledge. Collaboration encouraged on concepts; submitted work must be your own. AI tools (ChatGPT, Copilot, Claude) permitted for debugging, studying, and exploring ideas; prohibited for generating turnkey solutions. Disclose AI assistance; verify all AI-generated content for accuracy.</p>
</section>
<hr class="docutils" />
<section id="schedule-syllabus">
<h2>Schedule &amp; Syllabus<a class="headerlink" href="#schedule-syllabus" title="Link to this heading"></a></h2>
<dl class="simple">
<dt>📅 <a class="reference external" href="https://treese41528.github.io/ComputationalDataScience/Website/STAT_418_Schedule.html">Course Schedule</a></dt><dd><p>Interactive weekly schedule with topics, assignments, and exam dates.</p>
</dd>
<dt>📋 <a class="reference external" href="https://treese41528.github.io/ComputationalDataScience/Website/Computational%20Methods%20in%20Data%20Science%20Syllabus.pdf">Course Syllabus (PDF)</a></dt><dd><p>Complete syllabus with policies, grading breakdown, and academic integrity guidelines.</p>
</dd>
</dl>
</section>
<hr class="docutils" />
<section id="companion-notebooks">
<h2>Companion Notebooks<a class="headerlink" href="#companion-notebooks" title="Link to this heading"></a></h2>
<p>These Jupyter notebooks accompany the course lectures. Each chapter notebook contains worked examples, visualizations, and code you can run and modify. View the rendered HTML online or download the <code class="docutils literal notranslate"><span class="pre">.ipynb</span></code> files to run locally (right-click and “Save Link As” if needed).</p>
<p><em>Additional notebooks will be released as the course progresses.</em></p>
<table class="docutils align-default" style="width:100%">
<thead>
<tr><th>Chapter</th><th>View Online</th><th>Download</th></tr>
</thead>
<tbody>
<tr>
  <td><strong>Chapter 1</strong>: Probability Foundations &amp; Python Review</td>
  <td><a href="https://treese41528.github.io/ComputationalDataScience/Website/Notebooks/PartI/chapter1_review.html">🔗 View HTML</a></td>
  <td><a href="https://treese41528.github.io/ComputationalDataScience/Website/Notebooks/PartI/chapter1_review.ipynb" download="chapter1_review.ipynb">⬇ Download .ipynb</a></td>
</tr>
<tr>
  <td><strong>Chapter 2</strong>: Monte Carlo Methods</td>
  <td><a href="https://treese41528.github.io/ComputationalDataScience/Website/Notebooks/PartII/Chapter2/chapter2_monte_carlo_methods.html">🔗 View HTML</a></td>
  <td><a href="https://treese41528.github.io/ComputationalDataScience/Website/Notebooks/PartII/Chapter2/chapter2_monte_carlo_methods.ipynb" download="chapter2_monte_carlo_methods.ipynb">⬇ Download .ipynb</a></td>
</tr>
<tr>
  <td><strong>Chapter 3</strong>: Parametric Inference</td>
  <td><a href="https://treese41528.github.io/ComputationalDataScience/Website/Notebooks/PartII/Chapter3/chapter3_parametric_inference.html">🔗 View HTML</a></td>
  <td><a href="https://treese41528.github.io/ComputationalDataScience/Website/Notebooks/PartII/Chapter3/chapter3_parametric_inference.ipynb" download="chapter3_parametric_inference.ipynb">⬇ Download .ipynb</a></td>
</tr>
</tbody>
</table></section>
<hr class="docutils" />
<section id="python-tutorials-for-further-study">
<h2>Python Tutorials for Further Study<a class="headerlink" href="#python-tutorials-for-further-study" title="Link to this heading"></a></h2>
<p>The resources below are curated for students who want to deepen their Python skills beyond the course material. All are free, open-source, and maintained by recognized experts or framework developers.</p>
<p><strong>Course Environment Setup</strong></p>
<p><a href="https://treese41528.github.io/ComputationalDataScience/Website/requirements.txt" download="requirements.txt">⬇ Download requirements.txt</a> — Python package dependencies for the course. Install with <code>pip install -r requirements.txt</code> in a virtual environment.</p><p><strong>Data Science Foundations</strong></p>
<ul class="simple">
<li><dl class="simple">
<dt><a class="reference external" href="https://jakevdp.github.io/PythonDataScienceHandbook/">Python Data Science Handbook</a> by Jake VanderPlas</dt><dd><p>Complete free book covering NumPy, Pandas, Matplotlib, and Scikit-learn. All content available as executable Jupyter notebooks. Essential reference for the scientific Python stack.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><a class="reference external" href="https://lectures.scientific-python.org/">Scientific Python Lectures</a></dt><dd><p>From core SciPy contributors. Structured modules progressing from fundamentals to expert topics including memory optimization and performance tuning.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><a class="reference external" href="https://www.labri.fr/perso/nrougier/from-python-to-numpy/">From Python to NumPy</a> by Nicolas Rougier</dt><dd><p>Focused entirely on vectorization techniques—transforming Python loops into efficient NumPy operations. Essential for writing fast numerical code.</p>
</dd>
</dl>
</li>
</ul>
<p><strong>NumPy and Pandas Deep Dives</strong></p>
<ul class="simple">
<li><dl class="simple">
<dt><a class="reference external" href="https://scipy-lectures.org/advanced/advanced_numpy/">SciPy Lecture Notes: Advanced NumPy</a></dt><dd><p>Written by Pauli Virtanen (NumPy core developer). Covers ndarray internals, strides, memory layout, and creating ufuncs. Graduate-level depth.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><a class="reference external" href="https://pandas.pydata.org/docs/user_guide/index.html">Pandas User Guide</a></dt><dd><p>Official comprehensive documentation. Essential sections: <a class="reference external" href="https://pandas.pydata.org/docs/user_guide/groupby.html">GroupBy</a>, <a class="reference external" href="https://pandas.pydata.org/docs/user_guide/reshaping.html">Reshaping</a>, and <a class="reference external" href="https://pandas.pydata.org/docs/user_guide/enhancingperf.html">Enhancing Performance</a>.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><a class="reference external" href="https://numpy.org/doc/stable/reference/random/generator.html">Modern Random Generator API</a></dt><dd><p>Official documentation for <code class="docutils literal notranslate"><span class="pre">np.random.default_rng()</span></code>—the modern approach we use throughout the course.</p>
</dd>
</dl>
</li>
</ul>
<p><strong>Machine Learning</strong></p>
<ul class="simple">
<li><dl class="simple">
<dt><a class="reference external" href="https://inria.github.io/scikit-learn-mooc/">INRIA Scikit-learn MOOC</a></dt><dd><p>Gold standard for ML education—developed by scikit-learn core developers. 70% hands-on notebooks covering model selection, cross-validation, and ensemble methods.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><a class="reference external" href="https://scikit-learn.org/stable/user_guide.html">Scikit-learn User Guide</a></dt><dd><p>Official documentation with mathematical formulations for all algorithms. Includes the famous “Choosing the Right Estimator” flowchart.</p>
</dd>
</dl>
</li>
</ul>
<p><strong>Bayesian Statistics</strong></p>
<ul class="simple">
<li><dl class="simple">
<dt><a class="reference external" href="https://allendowney.github.io/ThinkBayes2/">Think Bayes 2nd Edition</a> by Allen Downey</dt><dd><p>Computational approach to Bayesian statistics using Python code instead of calculus. All Jupyter notebooks available for Colab.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><a class="reference external" href="https://www.pymc.io/projects/docs/en/stable/">PyMC Documentation</a></dt><dd><p>Official tutorials for the probabilistic programming library we’ll use. Start with the <a class="reference external" href="https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/pymc_overview.html">Overview Tutorial</a>.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><a class="reference external" href="https://github.com/pymc-devs/pymc-resources/tree/main/Rethinking">Statistical Rethinking with PyMC</a></dt><dd><p>Full port of McElreath’s acclaimed course materials to Python/PyMC.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><a class="reference external" href="https://python.arviz.org/en/stable/">ArviZ Documentation</a></dt><dd><p>Bayesian model visualization and diagnostics. Works with PyMC, NumPyro, Stan, and other backends.</p>
</dd>
</dl>
</li>
</ul>
<p><strong>Large Language Models</strong></p>
<ul class="simple">
<li><dl class="simple">
<dt><a class="reference external" href="https://huggingface.co/learn/llm-course/">Hugging Face LLM Course</a></dt><dd><p>Comprehensive 12-chapter course covering transformer architecture, fine-tuning, and building applications. Updated for current models.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><a class="reference external" href="https://www.promptingguide.ai/">Prompt Engineering Guide</a></dt><dd><p>Techniques including Chain-of-Thought, ReAct, and RAG. Model-specific guidance for GPT-4, Claude, and open models.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><a class="reference external" href="https://python.langchain.com/docs/tutorials/">LangChain Tutorials</a></dt><dd><p>RAG implementation, agents, and complex LLM workflows. Industry-standard orchestration framework.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><a class="reference external" href="https://cookbook.openai.com/">OpenAI Cookbook</a></dt><dd><p>Production-ready patterns for API integration, embeddings, function calling, and cost optimization.</p>
</dd>
</dl>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="recommended-textbooks">
<h2>Recommended Textbooks<a class="headerlink" href="#recommended-textbooks" title="Link to this heading"></a></h2>
<p>There is no single textbook that covers all course topics in depth. Students seeking one comprehensive resource should start with:</p>
<blockquote>
<div><p>Efron, B. &amp; Hastie, T. (2016). <em>Computer Age Statistical Inference: Algorithms, Evidence, and Data Science</em>. Cambridge University Press. <a class="reference external" href="https://doi.org/10.1017/CBO9781316576533">https://doi.org/10.1017/CBO9781316576533</a></p>
</div></blockquote>
<p>This text bridges classical frequentist methods, bootstrap and resampling, and Bayesian approaches, which partially mirrors the arc of the course.</p>
<p>For deeper study, the following topic-specific texts are recommended. Within each category, texts are ranked by accessibility and relevance to course material (★★★ = primary recommendation).</p>
<p><strong>Statistical Foundations and Inference Theory</strong></p>
<ul class="simple">
<li><p>★★★ Abramovich, F. &amp; Ritov, Y. (2022). <em>Statistical Theory: A Concise Introduction</em> (2nd ed.). Chapman and Hall/CRC. Concise, modern treatment of estimation, hypothesis testing, and asymptotic theory. Best for building theoretical intuition.</p></li>
</ul>
<p><strong>Monte Carlo and Simulation Methods</strong></p>
<ul class="simple">
<li><p>★★★ Robert, C. P. &amp; Casella, G. (2004). <em>Monte Carlo Statistical Methods</em> (2nd ed.). Springer. <a class="reference external" href="https://doi.org/10.1007/978-1-4757-4145-2">https://doi.org/10.1007/978-1-4757-4145-2</a> The definitive reference for simulation techniques. Chapters 2–4 cover foundational methods used in Weeks 2–3.</p></li>
</ul>
<p><strong>Resampling Methods</strong></p>
<ul class="simple">
<li><p>★★★ Efron, B. &amp; Tibshirani, R. J. (1994). <em>An Introduction to the Bootstrap</em>. Chapman and Hall/CRC. <a class="reference external" href="https://doi.org/10.1201/9780429246593">https://doi.org/10.1201/9780429246593</a> The foundational text by the method’s creators. Exceptionally clear exposition; essential reading for Weeks 6–8.</p></li>
<li><p>★★ Shao, J. &amp; Tu, D. (1995). <em>The Jackknife and Bootstrap</em>. Springer. <a class="reference external" href="https://doi.org/10.1007/978-1-4612-0795-5">https://doi.org/10.1007/978-1-4612-0795-5</a> [Advanced] More theoretical treatment with rigorous asymptotic analysis. Recommended after Efron &amp; Tibshirani.</p></li>
</ul>
<p><strong>Bayesian Data Analysis</strong></p>
<ul class="simple">
<li><p>★★★ McElreath, R. (2020). <em>Statistical Rethinking: A Bayesian Course with Examples in R and Stan</em> (2nd ed.). Chapman and Hall/CRC. <a class="reference external" href="https://doi.org/10.1201/9780429029608">https://doi.org/10.1201/9780429029608</a> Outstanding pedagogical approach that builds intuition before formalism. Primary recommendation for Weeks 10–12.</p></li>
<li><p>★★ Martin, O. A. (2024). <em>Bayesian Analysis with Python: A Practical Guide to Probabilistic Modeling</em> (3rd ed.). Packt Publishing. Practical implementation focus using PyMC. Excellent for translating Bayesian concepts into working Python code.</p></li>
<li><p>★★ Gelman, A. et al. (2013). <em>Bayesian Data Analysis</em> (3rd ed.). Chapman and Hall/CRC. <a class="reference external" href="https://doi.org/10.1201/b16018">https://doi.org/10.1201/b16018</a> [Advanced] Comprehensive reference (“BDA3”). More encyclopedic; best used for specific topics or deeper theoretical study.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="part1_foundations/index.html" class="btn btn-neutral float-right" title="Part I: Foundations of Probability and Computation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>