

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Section 2.7 Chapter 2 Summary &mdash; STAT 418</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=3d0abd52" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT418/Website/part2_frequentist/chapter2/ch2_7-chapter-summary.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=30646c52"></script>
      <script>let toggleHintShow = 'Show solution';</script>
      <script>let toggleHintHide = 'Hide solution';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>window.MathJax = {"options": {"enableMenu": true, "menuOptions": {"settings": {"enrich": true, "speech": true, "braille": true, "collapsible": true, "assistiveMml": false}}}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
      <script src="../../_static/custom.js?v=8718e0ab"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Chapter 3: Parametric Inference and Likelihood Methods" href="../chapter3/index.html" />
    <link rel="prev" title="Section 2.6 Variance Reduction Methods" href="ch2_6-variance-reduction-methods.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Course Content</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../part1_foundations/index.html">Part I: Foundations of Probability and Computation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part1_foundations/chapter1/index.html">Chapter 1: Statistical Paradigms and Core Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html">Section 1.1 Paradigms of Probability and Statistical Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html#the-mathematical-foundation-kolmogorov-s-axioms">The Mathematical Foundation: Kolmogorov’s Axioms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html#interpretations-of-probability">Interpretations of Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html#statistical-inference-paradigms">Statistical Inference Paradigms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html#historical-and-philosophical-debates">Historical and Philosophical Debates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html#looking-ahead-our-course-focus">Looking Ahead: Our Course Focus</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html">Section 1.2 Probability Distributions: Theory and Computation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#from-abstract-foundations-to-concrete-tools">From Abstract Foundations to Concrete Tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#the-python-ecosystem-for-probability">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#introduction-why-probability-distributions-matter">Introduction: Why Probability Distributions Matter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#id1">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#continuous-distributions">Continuous Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#additional-important-distributions">Additional Important Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#summary-and-practical-guidelines">Summary and Practical Guidelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#conclusion">Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html">Section 1.3 Python Random Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html#from-mathematical-distributions-to-computational-samples">From Mathematical Distributions to Computational Samples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html#the-python-ecosystem-at-a-glance">The Python Ecosystem at a Glance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html#understanding-pseudo-random-number-generation">Understanding Pseudo-Random Number Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html#the-standard-library-random-module">The Standard Library: <code class="docutils literal notranslate"><span class="pre">random</span></code> Module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html#numpy-fast-vectorized-random-sampling">NumPy: Fast Vectorized Random Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html#scipy-stats-the-complete-statistical-toolkit">SciPy Stats: The Complete Statistical Toolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html#bringing-it-all-together-library-selection-guide">Bringing It All Together: Library Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html#looking-ahead-from-random-numbers-to-monte-carlo-methods">Looking Ahead: From Random Numbers to Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_4-chapter-summary.html">Section 1.4 Chapter 1 Summary: Foundations in Place</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_4-chapter-summary.html#the-three-pillars-of-chapter-1">The Three Pillars of Chapter 1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_4-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_4-chapter-summary.html#what-lies-ahead-the-road-to-simulation">What Lies Ahead: The Road to Simulation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_4-chapter-summary.html#chapter-1-exercises-synthesis-problems">Chapter 1 Exercises: Synthesis Problems</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_4-chapter-summary.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Part II: Frequentist Inference</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Chapter 2: Monte Carlo Simulation</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html">Section 2.1 Monte Carlo Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#the-historical-development-of-monte-carlo-methods">The Historical Development of Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#the-core-principle-expectation-as-integration">The Core Principle: Expectation as Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#theoretical-foundations">Theoretical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#variance-estimation-and-confidence-intervals">Variance Estimation and Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#comparison-with-deterministic-methods">Comparison with Deterministic Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#sample-size-determination">Sample Size Determination</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#convergence-diagnostics-and-monitoring">Convergence Diagnostics and Monitoring</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#chapter-2-1-exercises-monte-carlo-fundamentals-mastery">Chapter 2.1 Exercises: Monte Carlo Fundamentals Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_1-monte-carlo-fundamentals.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_2-uniform-random-variates.html">Section 2.2 Uniform Random Variates</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#why-uniform-the-universal-currency-of-randomness">Why Uniform? The Universal Currency of Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#the-paradox-of-computational-randomness">The Paradox of Computational Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#chaotic-dynamical-systems-an-instructive-failure">Chaotic Dynamical Systems: An Instructive Failure</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#linear-congruential-generators">Linear Congruential Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#shift-register-generators">Shift-Register Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#the-kiss-generator-combining-strategies">The KISS Generator: Combining Strategies</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#modern-generators-mersenne-twister-and-pcg">Modern Generators: Mersenne Twister and PCG</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#statistical-testing-of-random-number-generators">Statistical Testing of Random Number Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#chapter-2-2-exercises-uniform-random-variates-mastery">Chapter 2.2 Exercises: Uniform Random Variates Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_3-inverse-cdf-method.html">Section 2.3 Inverse CDF Method</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#mathematical-foundations">Mathematical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#continuous-distributions-with-closed-form-inverses">Continuous Distributions with Closed-Form Inverses</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#numerical-inversion">Numerical Inversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#mixed-distributions">Mixed Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#chapter-2-3-exercises-inverse-cdf-method-mastery">Chapter 2.3 Exercises: Inverse CDF Method Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_4-transformation-methods.html">Section 2.4 Transformation Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#why-transformation-methods">Why Transformation Methods?</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#the-boxmuller-transform">The Box–Muller Transform</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#the-polar-marsaglia-method">The Polar (Marsaglia) Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#method-comparison-boxmuller-vs-polar-vs-ziggurat">Method Comparison: Box–Muller vs Polar vs Ziggurat</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#the-ziggurat-algorithm">The Ziggurat Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#the-clt-approximation-historical">The CLT Approximation (Historical)</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#distributions-derived-from-the-normal">Distributions Derived from the Normal</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#multivariate-normal-generation">Multivariate Normal Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#implementation-guidance">Implementation Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#chapter-2-4-exercises-transformation-methods-mastery">Chapter 2.4 Exercises: Transformation Methods Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_5-rejection-sampling.html">Section 2.5 Rejection Sampling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#the-dartboard-intuition">The Dartboard Intuition</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#the-accept-reject-algorithm">The Accept-Reject Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#efficiency-analysis">Efficiency Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#choosing-the-proposal-distribution">Choosing the Proposal Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#the-squeeze-principle">The Squeeze Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#geometric-example-sampling-from-the-unit-disk">Geometric Example: Sampling from the Unit Disk</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#limitations-and-the-curse-of-dimensionality">Limitations and the Curse of Dimensionality</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#connections-to-other-methods">Connections to Other Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#chapter-2-5-exercises-rejection-sampling-mastery">Chapter 2.5 Exercises: Rejection Sampling Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_6-variance-reduction-methods.html">Section 2.6 Variance Reduction Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#the-variance-reduction-paradigm">The Variance Reduction Paradigm</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#importance-sampling">Importance Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#control-variates">Control Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#antithetic-variates">Antithetic Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#stratified-sampling">Stratified Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#common-random-numbers">Common Random Numbers</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#conditional-monte-carlo-raoblackwellization">Conditional Monte Carlo (Rao–Blackwellization)</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#combining-variance-reduction-techniques">Combining Variance Reduction Techniques</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#chapter-2-6-exercises-variance-reduction-mastery">Chapter 2.6 Exercises: Variance Reduction Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Section 2.7 Chapter 2 Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#the-complete-monte-carlo-workflow">The Complete Monte Carlo Workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="#quick-reference-tables">Quick Reference Tables</a></li>
<li class="toctree-l4"><a class="reference internal" href="#common-pitfalls-checklist">Common Pitfalls Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="#connections-to-later-chapters">Connections to Later Chapters</a></li>
<li class="toctree-l4"><a class="reference internal" href="#learning-outcomes-checklist">Learning Outcomes Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="#further-reading-optimization-and-missing-data">Further Reading: Optimization and Missing Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter3/index.html">Chapter 3: Parametric Inference and Likelihood Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html">Exponential Families</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#historical-origins-from-scattered-results-to-unified-theory">Historical Origins: From Scattered Results to Unified Theory</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#the-canonical-exponential-family">The Canonical Exponential Family</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#converting-familiar-distributions">Converting Familiar Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#the-log-partition-function-a-moment-generating-machine">The Log-Partition Function: A Moment-Generating Machine</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#sufficiency-capturing-all-parameter-information">Sufficiency: Capturing All Parameter Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#minimal-sufficiency-and-completeness">Minimal Sufficiency and Completeness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#conjugate-priors-and-bayesian-inference">Conjugate Priors and Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#exponential-dispersion-models-and-glms">Exponential Dispersion Models and GLMs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#chapter-3-1-exercises-exponential-families-mastery">Chapter 3.1 Exercises: Exponential Families Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html">Maximum Likelihood Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-likelihood-function">The Likelihood Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-score-function">The Score Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#fisher-information">Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#closed-form-maximum-likelihood-estimators">Closed-Form Maximum Likelihood Estimators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#numerical-optimization-for-mle">Numerical Optimization for MLE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#asymptotic-properties-of-mles">Asymptotic Properties of MLEs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-cramer-rao-lower-bound">The Cramér-Rao Lower Bound</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-invariance-property">The Invariance Property</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#likelihood-based-hypothesis-testing">Likelihood-Based Hypothesis Testing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#confidence-intervals-from-likelihood">Confidence Intervals from Likelihood</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#connection-to-bayesian-inference">Connection to Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#chapter-3-2-exercises-maximum-likelihood-estimation-mastery">Chapter 3.2 Exercises: Maximum Likelihood Estimation Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html">Sampling Variability and Variance Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#statistical-estimators-and-their-properties">Statistical Estimators and Their Properties</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#sampling-distributions">Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#the-delta-method">The Delta Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#variance-estimation-methods">Variance Estimation Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#applications-and-worked-examples">Applications and Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html">Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#matrix-calculus-foundations">Matrix Calculus Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#the-linear-model">The Linear Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#ordinary-least-squares-the-calculus-approach">Ordinary Least Squares: The Calculus Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#ordinary-least-squares-the-geometric-approach">Ordinary Least Squares: The Geometric Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#properties-of-the-ols-estimator">Properties of the OLS Estimator</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#the-gauss-markov-theorem">The Gauss-Markov Theorem</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#estimating-the-error-variance">Estimating the Error Variance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#distributional-results-under-normality">Distributional Results Under Normality</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#diagnostics-and-model-checking">Diagnostics and Model Checking</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#numerical-stability-qr-decomposition">Numerical Stability: QR Decomposition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#model-selection-and-information-criteria">Model Selection and Information Criteria</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#regularization-ridge-and-lasso">Regularization: Ridge and LASSO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#chapter-3-4-exercises-linear-models-mastery">Chapter 3.4 Exercises: Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html">Generalized Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#historical-context-unification-of-regression-methods">Historical Context: Unification of Regression Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#the-glm-framework-three-components">The GLM Framework: Three Components</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#score-equations-and-fisher-information">Score Equations and Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#iteratively-reweighted-least-squares">Iteratively Reweighted Least Squares</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#logistic-regression-binary-outcomes">Logistic Regression: Binary Outcomes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#poisson-regression-count-data">Poisson Regression: Count Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#gamma-regression-positive-continuous-data">Gamma Regression: Positive Continuous Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#inference-in-glms-the-testing-triad">Inference in GLMs: The Testing Triad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#model-diagnostics">Model Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#model-comparison-and-selection">Model Comparison and Selection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#quasi-likelihood-and-robust-inference">Quasi-Likelihood and Robust Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#further-reading">Further Reading</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#chapter-3-5-exercises-generalized-linear-models-mastery">Chapter 3.5 Exercises: Generalized Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html">Chapter Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#the-parametric-inference-pipeline">The Parametric Inference Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#the-five-pillars-of-chapter-3">The Five Pillars of Chapter 3</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#quick-reference-core-formulas">Quick Reference: Core Formulas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#connections-to-future-material">Connections to Future Material</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#practical-guidance">Practical Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter4/index.html">Chapter 4: Resampling Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html">The Sampling Distribution Problem</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#the-fundamental-target-sampling-distributions">The Fundamental Target: Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#historical-development-the-quest-for-sampling-distributions">Historical Development: The Quest for Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#three-routes-to-the-sampling-distribution">Three Routes to the Sampling Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#when-asymptotics-fail-motivating-the-bootstrap">When Asymptotics Fail: Motivating the Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#the-plug-in-principle-theoretical-foundation">The Plug-In Principle: Theoretical Foundation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#computational-perspective-bootstrap-as-monte-carlo">Computational Perspective: Bootstrap as Monte Carlo</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#chapter-4-1-exercises">Chapter 4.1 Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html">The Empirical Distribution and Plug-in Principle</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#the-empirical-cumulative-distribution-function">The Empirical Cumulative Distribution Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#convergence-of-the-empirical-cdf">Convergence of the Empirical CDF</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#parameters-as-statistical-functionals">Parameters as Statistical Functionals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#when-the-plug-in-principle-fails">When the Plug-in Principle Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#the-bootstrap-idea-in-one-sentence">The Bootstrap Idea in One Sentence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#computational-implementation">Computational Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#section-4-2-exercises-ecdf-and-plug-in-mastery">Section 4.2 Exercises: ECDF and Plug-in Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html">The Nonparametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#the-bootstrap-principle">The Bootstrap Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-standard-errors">Bootstrap Standard Errors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-bias-estimation">Bootstrap Bias Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-confidence-intervals">Bootstrap Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-for-regression">Bootstrap for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-diagnostics">Bootstrap Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#when-bootstrap-fails">When Bootstrap Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html">Section 4.4: The Parametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#the-parametric-bootstrap-principle">The Parametric Bootstrap Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#location-scale-families">Location-Scale Families</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#parametric-bootstrap-for-regression">Parametric Bootstrap for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#confidence-intervals">Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#model-checking-and-validation">Model Checking and Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#when-parametric-bootstrap-fails">When Parametric Bootstrap Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#parametric-vs-nonparametric-a-decision-framework">Parametric vs. Nonparametric: A Decision Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html">Section 4.5: Jackknife Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#historical-context-and-motivation">Historical Context and Motivation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#the-delete-1-jackknife">The Delete-1 Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#jackknife-bias-estimation">Jackknife Bias Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#the-delete-d-jackknife">The Delete-<span class="math notranslate nohighlight">\(d\)</span> Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#jackknife-versus-bootstrap">Jackknife versus Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#the-infinitesimal-jackknife">The Infinitesimal Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html">Section 4.6 Bootstrap Hypothesis Testing and Permutation Tests</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#from-confidence-intervals-to-hypothesis-tests">From Confidence Intervals to Hypothesis Tests</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#the-bootstrap-hypothesis-testing-framework">The Bootstrap Hypothesis Testing Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#permutation-tests-exact-tests-under-exchangeability">Permutation Tests: Exact Tests Under Exchangeability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#testing-equality-of-distributions">Testing Equality of Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#bootstrap-tests-for-regression">Bootstrap Tests for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#bootstrap-vs-classical-tests">Bootstrap vs Classical Tests</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#permutation-vs-bootstrap-choosing-the-right-approach">Permutation vs Bootstrap: Choosing the Right Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#multiple-testing-with-bootstrap">Multiple Testing with Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part3_bayesian/index.html">Part III: Bayesian Inference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part3_bayesian/chapter5/index.html">Chapter 5: Bayesian Inference</a><ul class="simple">
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part4_llms_datascience/index.html">Part IV: Large Language Models in Data Science</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part4_llms_datascience/chapter6/index.html">Chapter 6: LLMs in Data Science Workflows</a><ul class="simple">
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Part II: Frequentist Inference</a></li>
          <li class="breadcrumb-item"><a href="index.html">Chapter 2: Monte Carlo Simulation</a></li>
      <li class="breadcrumb-item active">Section 2.7 Chapter 2 Summary</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/part2_frequentist/chapter2/ch2_7-chapter-summary.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="section-2-7-chapter-2-summary">
<span id="ch2-7-chapter-summary"></span><h1>Section 2.7 Chapter 2 Summary<a class="headerlink" href="#section-2-7-chapter-2-summary" title="Link to this heading"></a></h1>
<p>This chapter developed a complete toolkit for Monte Carlo simulation, from the fundamental mathematics of pseudo-random number generation through sophisticated variance reduction techniques. The methods form a coherent pipeline that transforms deterministic computation into stochastic estimation—leveraging randomness as a computational resource rather than viewing it as noise to be eliminated.</p>
<section id="the-complete-monte-carlo-workflow">
<h2>The Complete Monte Carlo Workflow<a class="headerlink" href="#the-complete-monte-carlo-workflow" title="Link to this heading"></a></h2>
<p>Every Monte Carlo simulation follows a four-stage pipeline:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>┌─────────────────────────────────────────────────────────────────────────┐
│                    THE MONTE CARLO PIPELINE                             │
└─────────────────────────────────────────────────────────────────────────┘

Stage 1: PRNG                Stage 2: Variate           Stage 3: MC
(Section 2.2)                Generation                 Estimation
                             (Sections 2.3-2.5)         (Section 2.1)
┌──────────────┐            ┌──────────────┐           ┌──────────────┐
│  Seed        │            │  Transform   │           │  Compute     │
│  ↓           │            │  U → X ~ F   │           │  h(X₁),...   │
│  LCG/MT/PCG  │ ──U[0,1]──→│              │ ──X~F───→ │  Average     │
│  ↓           │            │  • Inverse   │           │  ↓           │
│  U₁,U₂,...   │            │  • Box-Muller│           │  Î = Σh(Xᵢ)/n│
└──────────────┘            │  • Rejection │           └──────────────┘
                            └──────────────┘                  │
                                                              ↓
                                                 Stage 4: Variance Reduction
                                                 (Section 2.6)
                                                 ┌──────────────────────────┐
                                                 │ • Importance Sampling    │
                                                 │ • Control Variates       │
                                                 │ • Antithetic Variates    │
                                                 │ • Stratified / LHS       │
                                                 │ • Common Random Numbers  │
                                                 └──────────────────────────┘
</pre></div>
</div>
<p><strong>Stage 1 — Pseudo-Random Number Generation</strong>: Deterministic algorithms (LCG, Mersenne Twister, PCG) produce sequences that pass statistical tests for randomness. The choice of generator affects period length, speed, and statistical quality. Always use <code class="docutils literal notranslate"><span class="pre">np.random.default_rng()</span></code> with explicit seeds for reproducibility. For parallel jobs, use <code class="docutils literal notranslate"><span class="pre">SeedSequence.spawn</span></code> to create independent streams; counter-based bit-generators (Philox, Threefry) provide reproducible, splittable streams.</p>
<p><strong>Stage 2 — Random Variate Generation</strong>: Transform uniform samples into draws from target distributions. The inverse CDF method works when <span class="math notranslate nohighlight">\(F^{-1}\)</span> is tractable; Box-Muller (or the faster polar method) handles normals—prefer <code class="docutils literal notranslate"><span class="pre">rng.standard_normal()</span></code> or <code class="docutils literal notranslate"><span class="pre">scipy.stats</span></code> for production performance; rejection sampling covers arbitrary densities at the cost of wasted proposals. For large discrete supports, use the Walker/Vose alias method for <span class="math notranslate nohighlight">\(O(1)\)</span> sampling after <span class="math notranslate nohighlight">\(O(K)\)</span> preprocessing.</p>
<p><strong>Stage 3 — Monte Carlo Estimation</strong>: Estimate integrals <span class="math notranslate nohighlight">\(I = \mathbb{E}_f[h(X)]\)</span> by sample averages <span class="math notranslate nohighlight">\(\hat{I}_n = n^{-1}\sum_{i=1}^n h(X_i)\)</span>. The LLN guarantees convergence; the CLT provides the <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> rate and confidence intervals.</p>
<p><strong>Stage 4 — Variance Reduction</strong>: Reduce the constant <span class="math notranslate nohighlight">\(\sigma^2\)</span> in <span class="math notranslate nohighlight">\(\text{Var}(\hat{I}) = \sigma^2/n\)</span> through clever sampling strategies. Methods are multiplicative—combining importance sampling with control variates can achieve variance reductions exceeding 100×.</p>
<p><em>Note: Section numbers reflect chapter order (2.1 introduces Monte Carlo estimation conceptually); the pipeline diagram reflects computational flow.</em></p>
</section>
<section id="method-selection-guide">
<h2>Method Selection Guide<a class="headerlink" href="#method-selection-guide" title="Link to this heading"></a></h2>
<p>Use this decision framework to choose appropriate methods for your problem:</p>
<p><strong>Random Variate Generation</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Need samples from distribution F?
│
├─► Is F⁻¹(u) available in closed form?
│   YES → INVERSE CDF METHOD (Section 2.3)
│         Fast, exact, simple
│
├─► Is F the normal distribution?
│   YES → BOX-MULLER (Section 2.4)
│         Or use scipy.stats / numpy
│
├─► Is F a standard distribution?
│   YES → Use scipy.stats.distribution.rvs()
│         Optimized implementations
│
└─► None of the above?
    → REJECTION SAMPLING (Section 2.5)
      Find proposal g ≥ f/M everywhere
      Accept with probability f(x)/(M·g(x))
</pre></div>
</div>
<p><strong>Variance Reduction Selection</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>What are you estimating?
│
├─► Rare event or tail probability?
│   → IMPORTANCE SAMPLING
│     Shift proposal to important region
│     Target ESS ≥ 0.1n; check weight CV ≤ 1
│
├─► Have auxiliary variable with known E[C]?
│   → CONTROL VARIATES
│     Requires |ρ(H,C)| &gt; 0.5 for good gains
│
├─► Integrand monotone in inputs?
│   → ANTITHETIC VARIATES
│     Free variance reduction
│     Verify ρ &lt; 0 from pilot sample
│
├─► Domain naturally partitioned?
│   → STRATIFIED SAMPLING (low-d) or LATIN HYPERCUBE (high-d)
│     Eliminates between-stratum variance
│
└─► Comparing two systems/parameters?
    → COMMON RANDOM NUMBERS (CRN)
      Use paired-t CI, not two-sample
</pre></div>
</div>
</section>
<section id="quick-reference-tables">
<h2>Quick Reference Tables<a class="headerlink" href="#quick-reference-tables" title="Link to this heading"></a></h2>
<p><strong>Core Formulas</strong></p>
<table class="docutils align-default">
<colgroup>
<col style="width: 35.0%" />
<col style="width: 65.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Concept</p></th>
<th class="head"><p>Formula</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Monte Carlo estimator</p></td>
<td><p><span class="math notranslate nohighlight">\(\hat{I}_n = \frac{1}{n}\sum_{i=1}^n h(X_i)\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Standard error</p></td>
<td><p><span class="math notranslate nohighlight">\(\text{SE} = \frac{s}{\sqrt{n}}\)</span> where <span class="math notranslate nohighlight">\(s^2 = \frac{1}{n-1}\sum(h_i - \bar{h})^2\)</span></p></td>
</tr>
<tr class="row-even"><td><p>95% Confidence interval</p></td>
<td><p><span class="math notranslate nohighlight">\(\hat{I}_n \pm 1.96 \cdot \text{SE}\)</span> (use <span class="math notranslate nohighlight">\(t_{n-1,0.025}\)</span> for small <span class="math notranslate nohighlight">\(n\)</span>; batch means for dependence)</p></td>
</tr>
<tr class="row-odd"><td><p>Inverse CDF method</p></td>
<td><p><span class="math notranslate nohighlight">\(X = F^{-1}(U)\)</span> where <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0,1)\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Box-Muller</p></td>
<td><p><span class="math notranslate nohighlight">\(Z_1 = \sqrt{-2\ln U_1}\cos(2\pi U_2)\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Rejection sampling acceptance</p></td>
<td><p>Accept <span class="math notranslate nohighlight">\(X\)</span> with probability <span class="math notranslate nohighlight">\(\frac{f(X)}{M \cdot g(X)}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Importance weight</p></td>
<td><p><span class="math notranslate nohighlight">\(w(x) = \frac{f(x)}{g(x)}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Effective Sample Size</p></td>
<td><p><span class="math notranslate nohighlight">\(\text{ESS} = \frac{1}{\sum_i \bar{w}_i^2} = \frac{(\sum w_i)^2}{\sum w_i^2}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Control variate (optimal β)</p></td>
<td><p><span class="math notranslate nohighlight">\(\beta^* = \frac{\text{Cov}(H, C)}{\text{Var}(C)}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>CV variance reduction</p></td>
<td><p><span class="math notranslate nohighlight">\(\text{VRF} = \frac{1}{1 - \rho^2}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Antithetic VRF</p></td>
<td><p><span class="math notranslate nohighlight">\(\text{VRF} = \frac{1}{1 + \rho}\)</span> per evaluation; <span class="math notranslate nohighlight">\(\frac{2}{1+\rho}\)</span> per draw (need <span class="math notranslate nohighlight">\(\rho &lt; 0\)</span>)</p></td>
</tr>
<tr class="row-odd"><td><p>Neyman allocation</p></td>
<td><p><span class="math notranslate nohighlight">\(n_k^* \propto p_k \sigma_k\)</span></p></td>
</tr>
</tbody>
</table>
<p><strong>Method Comparison</strong></p>
<table class="docutils align-default">
<colgroup>
<col style="width: 18.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 22.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Variance Effect</p></th>
<th class="head"><p>Overhead</p></th>
<th class="head"><p>Best For</p></th>
<th class="head"><p>Watch Out For</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Importance Sampling</p></td>
<td><p>Orders of magnitude if <span class="math notranslate nohighlight">\(g \approx g^*\)</span></p></td>
<td><p>Proposal design; density evaluation</p></td>
<td><p>Rare events, tails</p></td>
<td><p>Weight degeneracy; light-tailed <span class="math notranslate nohighlight">\(g\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Control Variates</p></td>
<td><p>Factor <span class="math notranslate nohighlight">\(1/(1-\rho^2)\)</span></p></td>
<td><p>Evaluate control; estimate <span class="math notranslate nohighlight">\(\beta\)</span></p></td>
<td><p>Known moments available</p></td>
<td><p>Must know <span class="math notranslate nohighlight">\(\mathbb{E}[C]\)</span> exactly</p></td>
</tr>
<tr class="row-even"><td><p>Antithetic Variates</p></td>
<td><p>Factor <span class="math notranslate nohighlight">\(1/(1+\rho)\)</span></p></td>
<td><p>Pairing only (free)</p></td>
<td><p>Monotone integrands</p></td>
<td><p>Non-monotone → variance <em>increases</em></p></td>
</tr>
<tr class="row-odd"><td><p>Stratified / Latin Hypercube</p></td>
<td><p>Eliminates between-strata variance</p></td>
<td><p>Partition design; conditional sampling</p></td>
<td><p>Heterogeneous integrands</p></td>
<td><p>Curse of dimensionality (full stratification)</p></td>
</tr>
<tr class="row-even"><td><p>Common Random Numbers (CRN)</p></td>
<td><p>Large for similar systems</p></td>
<td><p>Synchronization</p></td>
<td><p>A/B comparisons</p></td>
<td><p>Helps differences only</p></td>
</tr>
</tbody>
</table>
</section>
<section id="common-pitfalls-checklist">
<h2>Common Pitfalls Checklist<a class="headerlink" href="#common-pitfalls-checklist" title="Link to this heading"></a></h2>
<p>Before running a Monte Carlo simulation, verify:</p>
<p><strong>PRNG and Reproducibility</strong></p>
<ul class="simple">
<li><p>☐ Using <code class="docutils literal notranslate"><span class="pre">np.random.default_rng(seed)</span></code> with explicit seed</p></li>
<li><p>☐ Not using legacy <code class="docutils literal notranslate"><span class="pre">np.random.seed()</span></code> API</p></li>
<li><p>☐ Separate RNG instances for parallel streams (use <code class="docutils literal notranslate"><span class="pre">SeedSequence.spawn</span></code>; counter-based generators like Philox provide splittable streams)</p></li>
</ul>
<p><strong>Variate Generation</strong></p>
<ul class="simple">
<li><p>☐ Inverse CDF: Using <code class="docutils literal notranslate"><span class="pre">np.log1p(-u)</span></code> not <code class="docutils literal notranslate"><span class="pre">np.log(1-u)</span></code> for stability</p></li>
<li><p>☐ Rejection sampling: Proposal <span class="math notranslate nohighlight">\(g\)</span> covers target <span class="math notranslate nohighlight">\(f\)</span> support entirely</p></li>
<li><p>☐ Rejection sampling: Bound <span class="math notranslate nohighlight">\(M\)</span> is tight (acceptance rate reasonable)</p></li>
</ul>
<p><strong>Monte Carlo Estimation</strong></p>
<ul class="simple">
<li><p>☐ Reporting SE and CI, not just point estimate</p></li>
<li><p>☐ Sample size adequate: <span class="math notranslate nohighlight">\(n \geq (z_{\alpha/2} \cdot s / \epsilon)^2\)</span> for desired precision <span class="math notranslate nohighlight">\(\epsilon\)</span></p></li>
<li><p>☐ Checked convergence visually (running mean plot)</p></li>
<li><p>☐ For sequential stopping: increase <span class="math notranslate nohighlight">\(n\)</span> until CI half-width <span class="math notranslate nohighlight">\(\leq \epsilon\)</span>, re-checking <span class="math notranslate nohighlight">\(s\)</span> with updated sample</p></li>
</ul>
<p><strong>Importance Sampling</strong></p>
<ul class="simple">
<li><p>☐ Target ESS <span class="math notranslate nohighlight">\(\geq 0.1n\)</span>; also monitor weight CV <span class="math notranslate nohighlight">\(\leq 1\)</span> or Gini coefficient</p></li>
<li><p>☐ Proposal has heavier tails than <span class="math notranslate nohighlight">\(|h|f\)</span>; verify <span class="math notranslate nohighlight">\(\int h^2 f^2 / g \, dx &lt; \infty\)</span></p></li>
<li><p>☐ Using log-space arithmetic with logsumexp</p></li>
</ul>
<p><strong>Control Variates</strong></p>
<ul class="simple">
<li><p>☐ Control mean <span class="math notranslate nohighlight">\(\mu_C\)</span> is known exactly (not estimated)</p></li>
<li><p>☐ Correlation <span class="math notranslate nohighlight">\(|\rho| &gt; 0.5\)</span> for meaningful reduction</p></li>
</ul>
<p><strong>Antithetic Variates</strong></p>
<ul class="simple">
<li><p>☐ Verified <span class="math notranslate nohighlight">\(\rho(h(U), h(1-U)) &lt; 0\)</span> from pilot</p></li>
<li><p>☐ Function is monotone (or near-monotone)</p></li>
</ul>
<p><strong>Stratified Sampling</strong></p>
<ul class="simple">
<li><p>☐ Strata cover entire domain</p></li>
<li><p>☐ For Neyman allocation: estimated <span class="math notranslate nohighlight">\(\sigma_k\)</span> from pilot</p></li>
</ul>
</section>
<section id="connections-to-later-chapters">
<h2>Connections to Later Chapters<a class="headerlink" href="#connections-to-later-chapters" title="Link to this heading"></a></h2>
<p>The Monte Carlo foundations developed here underpin the remainder of the course:</p>
<p><strong>Chapter 3: Parametric Inference and Linear Models</strong></p>
<ul class="simple">
<li><p>Maximum likelihood estimation uses Monte Carlo for complex likelihoods</p></li>
<li><p>Bootstrap standard errors (Chapter 4) rely on MC resampling</p></li>
<li><p>Simulation-based model checking and residual diagnostics</p></li>
</ul>
<p><strong>Chapter 4: Resampling Methods</strong></p>
<ul class="simple">
<li><p><strong>Bootstrap</strong>: Resample data with replacement, compute statistic—pure Monte Carlo</p></li>
<li><p><strong>Jackknife</strong>: Systematic leave-one-out; variance reduction via antithetic ideas</p></li>
<li><p><strong>Cross-validation</strong>: Random partitioning for model selection</p></li>
<li><p>Variance reduction techniques apply directly to bootstrap variance estimation</p></li>
</ul>
<p><strong>Part III: Bayesian Computation</strong></p>
<ul class="simple">
<li><p><strong>Posterior expectations</strong> <span class="math notranslate nohighlight">\(\mathbb{E}[\theta|y]\)</span> are integrals—estimated via MC</p></li>
<li><p><strong>Importance sampling</strong> estimates marginal likelihoods <span class="math notranslate nohighlight">\(p(y)\)</span></p></li>
<li><p><strong>Markov chain Monte Carlo (MCMC)</strong> (Metropolis-Hastings, Gibbs) generates dependent samples when direct sampling fails</p></li>
<li><p><strong>Convergence diagnostics</strong> extend the running mean ideas from Section 2.1</p></li>
<li><p><strong>Effective Sample Size</strong> from Section 2.6 diagnoses MCMC mixing</p></li>
</ul>
<p>The variance reduction techniques are not merely theoretical—they appear throughout modern computational statistics:</p>
<ul class="simple">
<li><p><strong>Sequential Monte Carlo</strong> uses importance sampling with resampling</p></li>
<li><p><strong>Rao-Blackwellization</strong> in MCMC is conditional Monte Carlo</p></li>
<li><p><strong>Control variates</strong> improve MCMC posterior mean estimates</p></li>
<li><p><strong>Antithetic sampling</strong> accelerates bootstrap computations</p></li>
</ul>
</section>
<section id="learning-outcomes-checklist">
<h2>Learning Outcomes Checklist<a class="headerlink" href="#learning-outcomes-checklist" title="Link to this heading"></a></h2>
<p>Upon completing this chapter, you should be able to:</p>
<p><strong>Foundational Understanding</strong></p>
<ul class="simple">
<li><p>☑ Define Monte Carlo integration as estimating <span class="math notranslate nohighlight">\(\mathbb{E}_f[h(X)]\)</span> via sample averaging</p></li>
<li><p>☑ Explain convergence via LLN and uncertainty quantification via CLT</p></li>
<li><p>☑ Analyze why the <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> rate is dimension-independent</p></li>
<li><p>☑ Compare Monte Carlo with deterministic quadrature for different problem classes</p></li>
</ul>
<p><strong>Random Number Generation</strong></p>
<ul class="simple">
<li><p>☑ Implement LCG, explain its limitations, and recognize spectral failures</p></li>
<li><p>☑ Describe Mersenne Twister and PCG trade-offs</p></li>
<li><p>☑ Apply statistical tests (chi-square, runs, serial correlation) to assess PRNG quality</p></li>
<li><p>☑ Design reproducible simulations with proper seed management</p></li>
</ul>
<p><strong>Random Variate Generation</strong></p>
<ul class="simple">
<li><p>☑ Apply inverse CDF for continuous and discrete distributions</p></li>
<li><p>☑ Implement binary search and alias method for discrete sampling</p></li>
<li><p>☑ Derive and implement rejection sampling with optimal proposals</p></li>
<li><p>☑ Use Box-Muller for normal generation; understand its geometric basis</p></li>
</ul>
<p><strong>Variance Reduction</strong></p>
<ul class="simple">
<li><p>☑ Implement importance sampling; diagnose weight degeneracy via ESS</p></li>
<li><p>☑ Apply control variates; derive optimal <span class="math notranslate nohighlight">\(\beta\)</span> and predict variance reduction</p></li>
<li><p>☑ Use antithetic variates for monotone functions; recognize failure modes</p></li>
<li><p>☑ Design stratified sampling schemes; apply Neyman allocation</p></li>
<li><p>☑ Use common random numbers for system comparisons with paired-t inference</p></li>
</ul>
</section>
<section id="further-reading-optimization-and-missing-data">
<h2>Further Reading: Optimization and Missing Data<a class="headerlink" href="#further-reading-optimization-and-missing-data" title="Link to this heading"></a></h2>
<p>The Monte Carlo foundations developed in this chapter extend naturally to two important areas that merit further study: <strong>stochastic optimization</strong> and <strong>missing data models</strong>. These topics, treated comprehensively in Robert and Casella (2004, Chapters 5 and 9), represent sophisticated applications of Monte Carlo methods to problems that resist analytical solution.</p>
<section id="monte-carlo-optimization">
<h3>Monte Carlo Optimization<a class="headerlink" href="#monte-carlo-optimization" title="Link to this heading"></a></h3>
<p>Many statistical problems reduce to optimization: maximum likelihood estimation, posterior mode finding, and loss minimization. When the objective function <span class="math notranslate nohighlight">\(h(\theta)\)</span> has multiple local optima, irregular geometry, or lacks analytical gradients, deterministic optimization methods struggle. Monte Carlo approaches transform optimization into sampling.</p>
<p><strong>Key Techniques:</strong></p>
<ul class="simple">
<li><p><strong>Stochastic exploration</strong>: Sample from a distribution proportional to <span class="math notranslate nohighlight">\(\exp(h(\theta)/T)\)</span> and identify high-density regions. The parameter <span class="math notranslate nohighlight">\(T\)</span> (temperature) controls the trade-off between exploration and exploitation.</p></li>
<li><p><strong>Simulated annealing</strong>: Gradually decrease <span class="math notranslate nohighlight">\(T\)</span> during sampling, allowing the Markov chain to escape local optima early (high <span class="math notranslate nohighlight">\(T\)</span>) while concentrating on the global optimum later (low <span class="math notranslate nohighlight">\(T\)</span>). Under cooling schedules like <span class="math notranslate nohighlight">\(T_n = C/\log(n)\)</span>, convergence to the global optimum is guaranteed.</p></li>
<li><p><strong>Stochastic gradient methods</strong>: Approximate gradients <span class="math notranslate nohighlight">\(\nabla h(\theta)\)</span> using Monte Carlo samples when analytical derivatives are unavailable or data is too large to process at once. The Robbins-Monro algorithm provides convergence guarantees under regularity conditions. Stochastic gradient descent (SGD) and its variants (Adam, RMSprop) are the workhorses of deep learning, where mini-batch gradient estimates replace full-data gradients to enable training on massive datasets.</p></li>
<li><p><strong>Monte Carlo EM</strong>: When the E-step of the EM algorithm lacks a closed form, replace it with Monte Carlo integration. The MCEM algorithm iterates between simulating latent variables and maximizing expected complete-data log-likelihood.</p></li>
</ul>
<p><strong>Connection to Chapter 2</strong>: Importance sampling (Section 2.6) enables efficient gradient estimation in stochastic optimization. Variance reduction techniques accelerate convergence of Monte Carlo EM. The effective sample size diagnostic warns when importance weights degenerate during optimization.</p>
</section>
<section id="missing-data-models">
<h3>Missing Data Models<a class="headerlink" href="#missing-data-models" title="Link to this heading"></a></h3>
<p>Missing data pervades applied statistics: survey nonresponse, censored observations, latent variables in mixture models, and hidden states in time series. The Monte Carlo solution is <strong>data augmentation</strong>—treat missing values as additional parameters and sample them alongside model parameters.</p>
<p><strong>Key Applications:</strong></p>
<ul class="simple">
<li><p><strong>Incomplete observations</strong>: When data are missing at random (MAR), the missing mechanism is ignorable and Gibbs sampling alternates between imputing missing values and updating parameters given the completed data.</p></li>
<li><p><strong>Finite mixtures</strong>: Mixture models <span class="math notranslate nohighlight">\(f(x|\theta) = \sum_{k=1}^K \pi_k f_k(x|\theta_k)\)</span> introduce latent allocation variables <span class="math notranslate nohighlight">\(Z_i \in \{1, \ldots, K\}\)</span>. Gibbs sampling augments the data with allocations, simplifying an intractable marginal likelihood into tractable complete-data conditionals.</p></li>
<li><p><strong>Hidden Markov models</strong>: When observations <span class="math notranslate nohighlight">\(Y_1, \ldots, Y_n\)</span> depend on an unobserved Markov chain <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span>, the forward-backward algorithm computes <span class="math notranslate nohighlight">\(P(X_t | Y_{1:n})\)</span> exactly, enabling efficient Gibbs sampling for HMM parameters.</p></li>
<li><p><strong>Changepoint models</strong>: The number and locations of regime changes are unknown. Reversible jump Markov chain Monte Carlo and product partition models handle the variable-dimensional parameter space.</p></li>
<li><p><strong>Stochastic volatility</strong>: Financial time series exhibit time-varying variance. Since volatility is unobserved, the model becomes a missing data problem where the latent volatility process is sampled alongside observation parameters.</p></li>
</ul>
<p><strong>Connection to Chapter 2</strong>: The variance reduction methods of Section 2.6 apply directly to Markov chain Monte Carlo output. Control variates reduce posterior mean estimation variance. Importance sampling enables model comparison via marginal likelihood estimation. The Rao-Blackwellization technique (conditional Monte Carlo) improves efficiency by integrating out some latent variables analytically.</p>
<p><strong>Recommended Reading:</strong></p>
<ul class="simple">
<li><p>Robert, C. P. and Casella, G. (2004). <em>Monte Carlo Statistical Methods</em> (2nd ed.), Chapters 5 and 9. Springer-Verlag. Comprehensive treatment of stochastic optimization and missing data with full theoretical development.</p></li>
<li><p>McLachlan, G. J. and Krishnan, T. (2008). <em>The EM Algorithm and Extensions</em> (2nd ed.). Wiley. Detailed coverage of EM and its Monte Carlo variants.</p></li>
<li><p>Little, R. J. A. and Rubin, D. B. (2019). <em>Statistical Analysis with Missing Data</em> (3rd ed.). Wiley. The definitive reference on missing data mechanisms and multiple imputation.</p></li>
<li><p>Cappé, O., Moulines, E., and Rydén, T. (2005). <em>Inference in Hidden Markov Models</em>. Springer. Thorough treatment of HMMs with computational algorithms.</p></li>
</ul>
</section>
</section>
<section id="final-perspective">
<h2>Final Perspective<a class="headerlink" href="#final-perspective" title="Link to this heading"></a></h2>
<p>Monte Carlo methods embody a profound insight: randomness, properly harnessed, becomes a precision instrument. The <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> convergence rate—seemingly slow—is actually remarkable because it holds regardless of dimension. Where deterministic methods succumb to the curse of dimensionality, Monte Carlo maintains its steady march toward the true value.</p>
<p>The techniques in this chapter form a complete toolkit:</p>
<ol class="arabic simple">
<li><p><strong>Generate</strong> uniform random numbers (Section 2.2)</p></li>
<li><p><strong>Transform</strong> them to any target distribution (Sections 2.3–2.5)</p></li>
<li><p><strong>Estimate</strong> integrals with quantified uncertainty (Section 2.1)</p></li>
<li><p><strong>Accelerate</strong> convergence through variance reduction (Section 2.6)</p></li>
</ol>
<p>This toolkit is not merely academic. Every posterior computation in Bayesian statistics, every option price in computational finance, every particle transport simulation in physics relies on these methods. The bootstrap and Markov chain Monte Carlo algorithms of later chapters build directly on the Monte Carlo foundations established here.</p>
<p>As we move to parametric inference, resampling methods, and Bayesian computation, the Monte Carlo perspective remains central: when analytical solutions fail, simulation provides the path forward. Master these fundamentals, and you hold the key to modern computational statistics.</p>
<div class="important admonition">
<p class="admonition-title">Key Takeaways 📝</p>
<ol class="arabic simple">
<li><p><strong>The Pipeline</strong>: PRNG → Variate Generation → MC Estimation → Variance Reduction. Each stage builds on the previous; weakness at any stage propagates forward.</p></li>
<li><p><strong>The Core Insight</strong>: Monte Carlo converts integrals to expectations and expectations to sample averages. The LLN guarantees convergence; the CLT quantifies uncertainty.</p></li>
<li><p><strong>The Rate</strong>: <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> is immutable. To improve precision, either increase <span class="math notranslate nohighlight">\(n\)</span> or decrease <span class="math notranslate nohighlight">\(\sigma^2\)</span> through variance reduction—or both.</p></li>
<li><p><strong>The Diagnostics</strong>: Always report SE and CI. Monitor ESS for importance sampling. Verify negative correlation for antithetics. Check convergence visually.</p></li>
<li><p><strong>The Connections</strong>: Bootstrap, Markov chain Monte Carlo, particle filters, and Bayesian computation all rest on these Monte Carlo foundations. The investment in understanding this chapter pays dividends throughout the course.</p></li>
</ol>
</div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<div role="list" class="citation-list">
<div class="citation" id="boxmuller1958" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>BoxMuller1958<span class="fn-bracket">]</span></span>
<p>Box, G. E. P., and Muller, M. E. (1958). A note on the generation of random normal deviates. <em>The Annals of Mathematical Statistics</em>, 29(2), 610–611.</p>
</div>
<div class="citation" id="buffon1777" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Buffon1777<span class="fn-bracket">]</span></span>
<p>Buffon, G.-L. L., Comte de (1777). Essai d’arithmétique morale. <em>Supplément à l’Histoire Naturelle</em>, Vol. 4. Contains the needle problem for estimating π.</p>
</div>
<div class="citation" id="cappeetal2005" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CappeEtAl2005<span class="fn-bracket">]</span></span>
<p>Cappé, O., Moulines, E., and Rydén, T. (2005). <em>Inference in Hidden Markov Models</em>. New York: Springer.</p>
</div>
<div class="citation" id="devroye1986" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Devroye1986<span class="fn-bracket">]</span></span>
<p>Devroye, L. (1986). <em>Non-Uniform Random Variate Generation</em>. New York: Springer-Verlag. Available free online at <a class="reference external" href="https://luc.devroye.org/rnbookindex.html">https://luc.devroye.org/rnbookindex.html</a></p>
</div>
<div class="citation" id="gilksetal1995" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>GilksEtAl1995<span class="fn-bracket">]</span></span>
<p>Gilks, W. R., Best, N. G., and Tan, K. K. C. (1995). Adaptive rejection Metropolis sampling within Gibbs sampling. <em>Journal of the Royal Statistical Society: Series C</em>, 44(4), 455–472.</p>
</div>
<div class="citation" id="gilkswild1992" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>GilksWild1992<span class="fn-bracket">]</span></span>
<p>Gilks, W. R., and Wild, P. (1992). Adaptive rejection sampling for Gibbs sampling. <em>Journal of the Royal Statistical Society: Series C</em>, 41(2), 337–348.</p>
</div>
<div class="citation" id="hulldobell1962" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HullDobell1962<span class="fn-bracket">]</span></span>
<p>Hull, T. E., and Dobell, A. R. (1962). Random number generators. <em>SIAM Review</em>, 4(3), 230–254.</p>
</div>
<div class="citation" id="lecuyersimard2007" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LEcuyerSimard2007<span class="fn-bracket">]</span></span>
<p>L’Ecuyer, P., and Simard, R. (2007). TestU01: A C library for empirical testing of random number generators. <em>ACM Transactions on Mathematical Software</em>, 33(4), Article 22.</p>
</div>
<div class="citation" id="lehmer1951" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Lehmer1951<span class="fn-bracket">]</span></span>
<p>Lehmer, D. H. (1951). Mathematical methods in large-scale computing units. In <em>Proceedings of the Second Symposium on Large-Scale Digital Calculating Machinery</em> (pp. 141–146). Harvard University Press.</p>
</div>
<div class="citation" id="littlerubin2019" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LittleRubin2019<span class="fn-bracket">]</span></span>
<p>Little, R. J. A., and Rubin, D. B. (2019). <em>Statistical Analysis with Missing Data</em> (3rd ed.). Hoboken, NJ: Wiley.</p>
</div>
<div class="citation" id="marsaglia1977" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Marsaglia1977<span class="fn-bracket">]</span></span>
<p>Marsaglia, G. (1977). The squeeze method for generating gamma variates. <em>Computers &amp; Mathematics with Applications</em>, 3(4), 321–325.</p>
</div>
<div class="citation" id="marsagliabray1964" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MarsagliaBray1964<span class="fn-bracket">]</span></span>
<p>Marsaglia, G., and Bray, T. A. (1964). A convenient method for generating normal variables. <em>SIAM Review</em>, 6(3), 260–264.</p>
</div>
<div class="citation" id="marsagliatsang2000" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MarsagliaTsang2000<span class="fn-bracket">]</span></span>
<p>Marsaglia, G., and Tsang, W. W. (2000). The ziggurat method for generating random variables. <em>Journal of Statistical Software</em>, 5(8), 1–7.</p>
</div>
<div class="citation" id="marsagliazaman1993" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MarsagliaZaman1993<span class="fn-bracket">]</span></span>
<p>Marsaglia, G., and Zaman, A. (1993). The KISS generator. Technical report, Department of Statistics, Florida State University.</p>
</div>
<div class="citation" id="matsumotonishimura1998" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MatsumotoNishimura1998<span class="fn-bracket">]</span></span>
<p>Matsumoto, M., and Nishimura, T. (1998). Mersenne Twister: A 623-dimensionally equidistributed uniform pseudo-random number generator. <em>ACM Transactions on Modeling and Computer Simulation</em>, 8(1), 3–30.</p>
</div>
<div class="citation" id="mclachlankrishnan2008" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>McLachlanKrishnan2008<span class="fn-bracket">]</span></span>
<p>McLachlan, G. J., and Krishnan, T. (2008). <em>The EM Algorithm and Extensions</em> (2nd ed.). Hoboken, NJ: Wiley.</p>
</div>
<div class="citation" id="metropolisulam1949" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MetropolisUlam1949<span class="fn-bracket">]</span></span>
<p>Metropolis, N., and Ulam, S. (1949). The Monte Carlo method. <em>Journal of the American Statistical Association</em>, 44(247), 335–341.</p>
</div>
<div class="citation" id="oneill2014" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ONeill2014<span class="fn-bracket">]</span></span>
<p>O’Neill, M. E. (2014). PCG: A family of simple fast space-efficient statistically good algorithms for random number generation. Technical Report HMC-CS-2014-0905, Harvey Mudd College.</p>
</div>
<div class="citation" id="robertcasella2004" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>RobertCasella2004<span class="fn-bracket">]</span></span>
<p>Robert, C. P., and Casella, G. (2004). <em>Monte Carlo Statistical Methods</em> (2nd ed.). New York: Springer.</p>
</div>
<div class="citation" id="vonneumann1951" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>vonNeumann1951<span class="fn-bracket">]</span></span>
<p>von Neumann, J. (1951). Various techniques used in connection with random digits. In A. S. Householder, G. E. Forsythe, &amp; H. H. Germond (Eds.), <em>Monte Carlo method</em> (pp. 36–38). National Bureau of Standards, Applied Mathematics Series (No. 12).</p>
</div>
<div class="citation" id="welford1962" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Welford1962<span class="fn-bracket">]</span></span>
<p>Welford, B. P. (1962). Note on a method for calculating corrected sums of squares and products. <em>Technometrics</em>, 4(3), 419–420.</p>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="ch2_6-variance-reduction-methods.html" class="btn btn-neutral float-left" title="Section 2.6 Variance Reduction Methods" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../chapter3/index.html" class="btn btn-neutral float-right" title="Chapter 3: Parametric Inference and Likelihood Methods" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>