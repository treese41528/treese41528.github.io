

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Section 2.1 Monte Carlo Fundamentals &mdash; STAT 418</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=3d0abd52" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT418/Website/part2_frequentist/chapter2/ch2_1-monte-carlo-fundamentals.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=30646c52"></script>
      <script>let toggleHintShow = 'Show solution';</script>
      <script>let toggleHintHide = 'Hide solution';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>window.MathJax = {"options": {"enableMenu": true, "menuOptions": {"settings": {"enrich": true, "speech": true, "braille": true, "collapsible": true, "assistiveMml": false}}}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
      <script src="../../_static/custom.js?v=8718e0ab"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Uniform Random Variates" href="ch2_2-uniform-random-variates.html" />
    <link rel="prev" title="Chapter 2: Monte Carlo Simulation" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Course Content</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../part1_foundations/index.html">Part I: Foundations of Probability and Computation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part1_foundations/chapter1/index.html">Chapter 1: Statistical Paradigms and Core Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html">Section 1.1 Paradigms of Probability and Statistical Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html#the-mathematical-foundation-kolmogorov-s-axioms">The Mathematical Foundation: Kolmogorov’s Axioms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html#interpretations-of-probability">Interpretations of Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html#statistical-inference-paradigms">Statistical Inference Paradigms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html#historical-and-philosophical-debates">Historical and Philosophical Debates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html#looking-ahead-our-course-focus">Looking Ahead: Our Course Focus</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html">Section 1.2 Probability Distributions: Theory and Computation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#from-abstract-foundations-to-concrete-tools">From Abstract Foundations to Concrete Tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#the-python-ecosystem-for-probability">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#introduction-why-probability-distributions-matter">Introduction: Why Probability Distributions Matter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#id1">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#continuous-distributions">Continuous Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#additional-important-distributions">Additional Important Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#summary-and-practical-guidelines">Summary and Practical Guidelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#conclusion">Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html">Section 1.3 Python Random Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html#from-mathematical-distributions-to-computational-samples">From Mathematical Distributions to Computational Samples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html#the-python-ecosystem-at-a-glance">The Python Ecosystem at a Glance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html#understanding-pseudo-random-number-generation">Understanding Pseudo-Random Number Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html#the-standard-library-random-module">The Standard Library: <code class="docutils literal notranslate"><span class="pre">random</span></code> Module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html#numpy-fast-vectorized-random-sampling">NumPy: Fast Vectorized Random Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html#scipy-stats-the-complete-statistical-toolkit">SciPy Stats: The Complete Statistical Toolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html#bringing-it-all-together-library-selection-guide">Bringing It All Together: Library Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html#looking-ahead-from-random-numbers-to-monte-carlo-methods">Looking Ahead: From Random Numbers to Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_4-chapter-summary.html">Section 1.4 Chapter 1 Summary: Foundations in Place</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_4-chapter-summary.html#the-three-pillars-of-chapter-1">The Three Pillars of Chapter 1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_4-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_4-chapter-summary.html#what-lies-ahead-the-road-to-simulation">What Lies Ahead: The Road to Simulation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_4-chapter-summary.html#chapter-1-exercises-synthesis-problems">Chapter 1 Exercises: Synthesis Problems</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_4-chapter-summary.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Part II: Frequentist Inference</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Chapter 2: Monte Carlo Simulation</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Section 2.1 Monte Carlo Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#the-historical-development-of-monte-carlo-methods">The Historical Development of Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-core-principle-expectation-as-integration">The Core Principle: Expectation as Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="#theoretical-foundations">Theoretical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#variance-estimation-and-confidence-intervals">Variance Estimation and Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="#comparison-with-deterministic-methods">Comparison with Deterministic Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sample-size-determination">Sample Size Determination</a></li>
<li class="toctree-l4"><a class="reference internal" href="#convergence-diagnostics-and-monitoring">Convergence Diagnostics and Monitoring</a></li>
<li class="toctree-l4"><a class="reference internal" href="#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#chapter-2-1-exercises-monte-carlo-fundamentals-mastery">Chapter 2.1 Exercises: Monte Carlo Fundamentals Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_2-uniform-random-variates.html">Uniform Random Variates</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#why-uniform-the-universal-currency-of-randomness">Why Uniform? The Universal Currency of Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#the-paradox-of-computational-randomness">The Paradox of Computational Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#chaotic-dynamical-systems-an-instructive-failure">Chaotic Dynamical Systems: An Instructive Failure</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#linear-congruential-generators">Linear Congruential Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#shift-register-generators">Shift-Register Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#the-kiss-generator-combining-strategies">The KISS Generator: Combining Strategies</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#modern-generators-mersenne-twister-and-pcg">Modern Generators: Mersenne Twister and PCG</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#statistical-testing-of-random-number-generators">Statistical Testing of Random Number Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#chapter-2-2-exercises-uniform-random-variates-mastery">Chapter 2.2 Exercises: Uniform Random Variates Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_2-uniform-random-variates.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_3-inverse-cdf-method.html">Inverse CDF Method</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#mathematical-foundations">Mathematical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#continuous-distributions-with-closed-form-inverses">Continuous Distributions with Closed-Form Inverses</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#numerical-inversion">Numerical Inversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#mixed-distributions">Mixed Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#chapter-2-3-exercises-inverse-cdf-method-mastery">Chapter 2.3 Exercises: Inverse CDF Method Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_3-inverse-cdf-method.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_4-transformation-methods.html">Transformation Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#why-transformation-methods">Why Transformation Methods?</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#the-boxmuller-transform">The Box–Muller Transform</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#the-polar-marsaglia-method">The Polar (Marsaglia) Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#method-comparison-boxmuller-vs-polar-vs-ziggurat">Method Comparison: Box–Muller vs Polar vs Ziggurat</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#the-ziggurat-algorithm">The Ziggurat Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#the-clt-approximation-historical">The CLT Approximation (Historical)</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#distributions-derived-from-the-normal">Distributions Derived from the Normal</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#multivariate-normal-generation">Multivariate Normal Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#implementation-guidance">Implementation Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#chapter-2-4-exercises-transformation-methods-mastery">Chapter 2.4 Exercises: Transformation Methods Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_4-transformation-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_5-rejection-sampling.html">Rejection Sampling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#the-dartboard-intuition">The Dartboard Intuition</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#the-accept-reject-algorithm">The Accept-Reject Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#efficiency-analysis">Efficiency Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#choosing-the-proposal-distribution">Choosing the Proposal Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#the-squeeze-principle">The Squeeze Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#geometric-example-sampling-from-the-unit-disk">Geometric Example: Sampling from the Unit Disk</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#limitations-and-the-curse-of-dimensionality">Limitations and the Curse of Dimensionality</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#connections-to-other-methods">Connections to Other Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#chapter-2-5-exercises-rejection-sampling-mastery">Chapter 2.5 Exercises: Rejection Sampling Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_5-rejection-sampling.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_6-variance-reduction-methods.html">Variance Reduction Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#the-variance-reduction-paradigm">The Variance Reduction Paradigm</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#importance-sampling">Importance Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#control-variates">Control Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#antithetic-variates">Antithetic Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#stratified-sampling">Stratified Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#common-random-numbers">Common Random Numbers</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#conditional-monte-carlo-raoblackwellization">Conditional Monte Carlo (Rao–Blackwellization)</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#combining-variance-reduction-techniques">Combining Variance Reduction Techniques</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#chapter-2-6-exercises-variance-reduction-mastery">Chapter 2.6 Exercises: Variance Reduction Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_6-variance-reduction-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch2_7-chapter-summary.html">Chapter Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#the-complete-monte-carlo-workflow">The Complete Monte Carlo Workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#quick-reference-tables">Quick Reference Tables</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#common-pitfalls-checklist">Common Pitfalls Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#connections-to-later-chapters">Connections to Later Chapters</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#learning-outcomes-checklist">Learning Outcomes Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#further-reading-optimization-and-missing-data">Further Reading: Optimization and Missing Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch2_7-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter3/index.html">Chapter 3: Parametric Inference and Likelihood Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html">Exponential Families</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#historical-origins-from-scattered-results-to-unified-theory">Historical Origins: From Scattered Results to Unified Theory</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#the-canonical-exponential-family">The Canonical Exponential Family</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#converting-familiar-distributions">Converting Familiar Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#the-log-partition-function-a-moment-generating-machine">The Log-Partition Function: A Moment-Generating Machine</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#sufficiency-capturing-all-parameter-information">Sufficiency: Capturing All Parameter Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#minimal-sufficiency-and-completeness">Minimal Sufficiency and Completeness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#conjugate-priors-and-bayesian-inference">Conjugate Priors and Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#exponential-dispersion-models-and-glms">Exponential Dispersion Models and GLMs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#chapter-3-1-exercises-exponential-families-mastery">Chapter 3.1 Exercises: Exponential Families Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_1-exponential-families.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html">Maximum Likelihood Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-likelihood-function">The Likelihood Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-score-function">The Score Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#fisher-information">Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#closed-form-maximum-likelihood-estimators">Closed-Form Maximum Likelihood Estimators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#numerical-optimization-for-mle">Numerical Optimization for MLE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#asymptotic-properties-of-mles">Asymptotic Properties of MLEs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-cramer-rao-lower-bound">The Cramér-Rao Lower Bound</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#the-invariance-property">The Invariance Property</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#likelihood-based-hypothesis-testing">Likelihood-Based Hypothesis Testing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#confidence-intervals-from-likelihood">Confidence Intervals from Likelihood</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#connection-to-bayesian-inference">Connection to Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#chapter-3-2-exercises-maximum-likelihood-estimation-mastery">Chapter 3.2 Exercises: Maximum Likelihood Estimation Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_2-maximum-likelihood-estimation.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html">Sampling Variability and Variance Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#statistical-estimators-and-their-properties">Statistical Estimators and Their Properties</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#sampling-distributions">Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#the-delta-method">The Delta Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#variance-estimation-methods">Variance Estimation Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#applications-and-worked-examples">Applications and Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_3-sampling-variability.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html">Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#matrix-calculus-foundations">Matrix Calculus Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#the-linear-model">The Linear Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#ordinary-least-squares-the-calculus-approach">Ordinary Least Squares: The Calculus Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#ordinary-least-squares-the-geometric-approach">Ordinary Least Squares: The Geometric Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#properties-of-the-ols-estimator">Properties of the OLS Estimator</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#the-gauss-markov-theorem">The Gauss-Markov Theorem</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#estimating-the-error-variance">Estimating the Error Variance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#distributional-results-under-normality">Distributional Results Under Normality</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#diagnostics-and-model-checking">Diagnostics and Model Checking</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#numerical-stability-qr-decomposition">Numerical Stability: QR Decomposition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#model-selection-and-information-criteria">Model Selection and Information Criteria</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#regularization-ridge-and-lasso">Regularization: Ridge and LASSO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#chapter-3-4-exercises-linear-models-mastery">Chapter 3.4 Exercises: Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_4-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html">Generalized Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#historical-context-unification-of-regression-methods">Historical Context: Unification of Regression Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#the-glm-framework-three-components">The GLM Framework: Three Components</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#score-equations-and-fisher-information">Score Equations and Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#iteratively-reweighted-least-squares">Iteratively Reweighted Least Squares</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#logistic-regression-binary-outcomes">Logistic Regression: Binary Outcomes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#poisson-regression-count-data">Poisson Regression: Count Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#gamma-regression-positive-continuous-data">Gamma Regression: Positive Continuous Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#inference-in-glms-the-testing-triad">Inference in GLMs: The Testing Triad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#model-diagnostics">Model Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#model-comparison-and-selection">Model Comparison and Selection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#quasi-likelihood-and-robust-inference">Quasi-Likelihood and Robust Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#further-reading">Further Reading</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#chapter-3-5-exercises-generalized-linear-models-mastery">Chapter 3.5 Exercises: Generalized Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_5-generalized-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html">Chapter Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#the-parametric-inference-pipeline">The Parametric Inference Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#the-five-pillars-of-chapter-3">The Five Pillars of Chapter 3</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#quick-reference-core-formulas">Quick Reference: Core Formulas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#connections-to-future-material">Connections to Future Material</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#practical-guidance">Practical Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter3/ch3_6-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter4/index.html">Chapter 4: Resampling Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html">The Sampling Distribution Problem</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#the-fundamental-target-sampling-distributions">The Fundamental Target: Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#historical-development-the-quest-for-sampling-distributions">Historical Development: The Quest for Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#three-routes-to-the-sampling-distribution">Three Routes to the Sampling Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#when-asymptotics-fail-motivating-the-bootstrap">When Asymptotics Fail: Motivating the Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#the-plug-in-principle-theoretical-foundation">The Plug-In Principle: Theoretical Foundation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#computational-perspective-bootstrap-as-monte-carlo">Computational Perspective: Bootstrap as Monte Carlo</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#chapter-4-1-exercises">Chapter 4.1 Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html">The Empirical Distribution and Plug-in Principle</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#the-empirical-cumulative-distribution-function">The Empirical Cumulative Distribution Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#convergence-of-the-empirical-cdf">Convergence of the Empirical CDF</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#parameters-as-statistical-functionals">Parameters as Statistical Functionals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#when-the-plug-in-principle-fails">When the Plug-in Principle Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#the-bootstrap-idea-in-one-sentence">The Bootstrap Idea in One Sentence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#computational-implementation">Computational Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#section-4-2-exercises-ecdf-and-plug-in-mastery">Section 4.2 Exercises: ECDF and Plug-in Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html">The Nonparametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#the-bootstrap-principle">The Bootstrap Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-standard-errors">Bootstrap Standard Errors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-bias-estimation">Bootstrap Bias Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-confidence-intervals">Bootstrap Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-for-regression">Bootstrap for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-diagnostics">Bootstrap Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#when-bootstrap-fails">When Bootstrap Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html">Section 4.4: The Parametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#the-parametric-bootstrap-principle">The Parametric Bootstrap Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#location-scale-families">Location-Scale Families</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#parametric-bootstrap-for-regression">Parametric Bootstrap for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#confidence-intervals">Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#model-checking-and-validation">Model Checking and Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#when-parametric-bootstrap-fails">When Parametric Bootstrap Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#parametric-vs-nonparametric-a-decision-framework">Parametric vs. Nonparametric: A Decision Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html">Section 4.5: Jackknife Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#historical-context-and-motivation">Historical Context and Motivation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#the-delete-1-jackknife">The Delete-1 Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#jackknife-bias-estimation">Jackknife Bias Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#the-delete-d-jackknife">The Delete-<span class="math notranslate nohighlight">\(d\)</span> Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#jackknife-versus-bootstrap">Jackknife versus Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#the-infinitesimal-jackknife">The Infinitesimal Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html">Section 4.6 Bootstrap Hypothesis Testing and Permutation Tests</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#from-confidence-intervals-to-hypothesis-tests">From Confidence Intervals to Hypothesis Tests</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#the-bootstrap-hypothesis-testing-framework">The Bootstrap Hypothesis Testing Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#permutation-tests-exact-tests-under-exchangeability">Permutation Tests: Exact Tests Under Exchangeability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#testing-equality-of-distributions">Testing Equality of Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#bootstrap-tests-for-regression">Bootstrap Tests for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#bootstrap-vs-classical-tests">Bootstrap vs Classical Tests</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#permutation-vs-bootstrap-choosing-the-right-approach">Permutation vs Bootstrap: Choosing the Right Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#multiple-testing-with-bootstrap">Multiple Testing with Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part3_bayesian/index.html">Part III: Bayesian Inference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part3_bayesian/chapter5/index.html">Chapter 5: Bayesian Inference</a><ul class="simple">
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part4_llms_datascience/index.html">Part IV: Large Language Models in Data Science</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part4_llms_datascience/chapter6/index.html">Chapter 6: LLMs in Data Science Workflows</a><ul class="simple">
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Part II: Frequentist Inference</a></li>
          <li class="breadcrumb-item"><a href="index.html">Chapter 2: Monte Carlo Simulation</a></li>
      <li class="breadcrumb-item active">Section 2.1 Monte Carlo Fundamentals</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/part2_frequentist/chapter2/ch2_1-monte-carlo-fundamentals.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="section-2-1-monte-carlo-fundamentals">
<span id="ch2-1-monte-carlo-fundamentals"></span><h1>Section 2.1 Monte Carlo Fundamentals<a class="headerlink" href="#section-2-1-monte-carlo-fundamentals" title="Link to this heading"></a></h1>
<p>In the spring of 1946, the mathematician Stanislaw Ulam was recovering from a near-fatal case of viral encephalitis at his home in Los Angeles. To pass the time during his convalescence, he played countless games of solitaire—and found himself wondering: what is the probability of winning a game of Canfield solitaire? The combinatorics were hopelessly complex. There were too many possible configurations, too many branching paths through a game, to enumerate them all. But Ulam realized something profound: he didn’t need to enumerate every possibility. He could simply <em>play</em> a hundred games and count how many he won.</p>
<p>This insight—that we can estimate probabilities by running experiments rather than computing them analytically—was not new. The Comte de Buffon had used a similar approach in 1777 to estimate π by dropping needles onto a lined floor. But Ulam saw something that Buffon could not have imagined: the recently completed ENIAC computer could “play” millions of such games, transforming a parlor trick into a serious computational method. Within weeks, Ulam had discussed the idea with his colleague John von Neumann, and the two began developing what would become one of the most powerful computational frameworks in all of science.</p>
<p>They needed a code name for this method, which they were applying to classified problems in nuclear weapons design at Los Alamos. Nicholas Metropolis suggested “Monte Carlo,” after the famous casino in Monaco where Ulam’s uncle had a gambling habit. The name stuck, and with it, a new era in computational science began.</p>
<p>This chapter introduces Monte Carlo methods—a family of algorithms that use random sampling to solve problems that would otherwise be intractable. We will see how randomness, properly harnessed, becomes a precision instrument for computing integrals, estimating probabilities, and approximating quantities that resist analytical attack. The ideas are simple, but their power is immense: Monte Carlo methods now pervade physics, finance, machine learning, and statistics, anywhere that high-dimensional integration or complex probability calculations arise.</p>
<div class="important admonition">
<p class="admonition-title">Road Map 🧭</p>
<ul class="simple">
<li><p><strong>Understand</strong>: The fundamental principle that Monte Carlo integration estimates integrals as expectations of random samples, and why this works via the Law of Large Numbers and Central Limit Theorem</p></li>
<li><p><strong>Develop</strong>: Deep intuition for the <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> convergence rate—what it means, why it arises, and its remarkable dimension-independence</p></li>
<li><p><strong>Implement</strong>: Complete Python code for Monte Carlo estimation with variance quantification, confidence intervals, and convergence diagnostics</p></li>
<li><p><strong>Evaluate</strong>: When Monte Carlo methods outperform deterministic alternatives, and how to assess estimation quality in practice</p></li>
<li><p><strong>Connect</strong>: How Monte Carlo integration motivates the random variable generation techniques of subsequent sections</p></li>
</ul>
</div>
<div class="note admonition">
<p class="admonition-title">Notation Convention 📐</p>
<p>Throughout this text, we write <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(\mu, \sigma^2)\)</span> to denote a normal random variable with mean <span class="math notranslate nohighlight">\(\mu\)</span> and <strong>variance</strong> <span class="math notranslate nohighlight">\(\sigma^2\)</span>. That is, the second parameter is the variance, not the standard deviation. This is the most common convention in statistics and probability theory. When we need to emphasize the standard deviation, we write it explicitly as <span class="math notranslate nohighlight">\(\text{SD}(X) = \sigma\)</span>.</p>
</div>
<section id="the-historical-development-of-monte-carlo-methods">
<h2>The Historical Development of Monte Carlo Methods<a class="headerlink" href="#the-historical-development-of-monte-carlo-methods" title="Link to this heading"></a></h2>
<p>Before diving into the mathematics, it is worth understanding how Monte Carlo methods emerged and evolved. This history illuminates why the methods work, what problems motivated their development, and why they remain central to computational science today.</p>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/mc_timeline.png"><img alt="Timeline showing evolution of Monte Carlo methods from Buffon's Needle (1777) to modern neural-enhanced methods (2020s)" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/mc_timeline.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 30 </span><span class="caption-text"><strong>Historical Evolution of Monte Carlo Methods.</strong> This timeline traces 250 years of algorithmic innovation, from Buffon’s needle experiment in 1777 through the founding contributions of Ulam and von Neumann in the 1940s, the development of MCMC methods, resampling techniques, and modern neural-enhanced approaches. Each innovation opened new classes of problems to computational attack.</span><a class="headerlink" href="#id2" title="Link to this image"></a></p>
</figcaption>
</figure>
<section id="buffon-s-needle-the-first-monte-carlo-experiment">
<h3>Buffon’s Needle: The First Monte Carlo Experiment<a class="headerlink" href="#buffon-s-needle-the-first-monte-carlo-experiment" title="Link to this heading"></a></h3>
<p>In 1777, Georges-Louis Leclerc, Comte de Buffon, posed a deceptively simple question: suppose we have a floor made of parallel wooden planks, each of width <span class="math notranslate nohighlight">\(d\)</span>, and we drop a needle of length <span class="math notranslate nohighlight">\(\ell \leq d\)</span> onto this floor. What is the probability that the needle crosses one of the cracks between planks?</p>
<p>To answer this, Buffon introduced what we would now recognize as a probabilistic model. Let <span class="math notranslate nohighlight">\(\theta\)</span> denote the angle between the needle and the direction of the planks, uniformly distributed on <span class="math notranslate nohighlight">\([0, \pi)\)</span>. Let <span class="math notranslate nohighlight">\(y\)</span> denote the distance from the needle’s center to the nearest crack, uniformly distributed on <span class="math notranslate nohighlight">\([0, d/2]\)</span>. The needle crosses a crack if and only if the vertical projection of half the needle exceeds the distance to the crack—that is, if <span class="math notranslate nohighlight">\(y \leq \frac{\ell}{2} \sin\theta\)</span>.</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_1_fig01_buffon_geometry.png"><img alt="Three-panel visualization of Buffon's needle problem showing physical setup, crossing geometry, and probability space" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_1_fig01_buffon_geometry.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 31 </span><span class="caption-text"><strong>Buffon’s Needle Geometry.</strong> Left: The physical setup with needles scattered across parallel planks (red = crossing, blue = not crossing). Middle: The key geometry—a needle crosses if its center’s distance <span class="math notranslate nohighlight">\(y\)</span> to the nearest crack is less than the vertical projection <span class="math notranslate nohighlight">\((\ell/2)\sin\theta\)</span>. Right: The probability space showing the crossing region; the probability equals the ratio of areas: <span class="math notranslate nohighlight">\(2\ell/(\pi d)\)</span>.</span><a class="headerlink" href="#id3" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>The probability of crossing is therefore:</p>
<div class="math notranslate nohighlight">
\[P(\text{crossing}) = \frac{1}{\pi} \cdot \frac{2}{d} \int_0^{\pi} \int_0^{(\ell/2)\sin\theta} dy \, d\theta\]</div>
<p>Evaluating the inner integral yields <span class="math notranslate nohighlight">\(\frac{\ell}{2}\sin\theta\)</span>, and the outer integral gives <span class="math notranslate nohighlight">\(\int_0^{\pi} \sin\theta \, d\theta = 2\)</span>. Thus:</p>
<div class="math notranslate nohighlight">
\[P(\text{crossing}) = \frac{1}{\pi} \cdot \frac{2}{d} \cdot \frac{\ell}{2} \cdot 2 = \frac{2\ell}{\pi d}\]</div>
<p>This elegant result has a remarkable consequence. If we drop <span class="math notranslate nohighlight">\(n\)</span> needles and observe that <span class="math notranslate nohighlight">\(k\)</span> of them cross a crack, then our estimate of the crossing probability is <span class="math notranslate nohighlight">\(\hat{p} = k/n\)</span>. Rearranging Buffon’s formula:</p>
<div class="math notranslate nohighlight">
\[\pi = \frac{2\ell}{d \cdot P(\text{crossing})} \approx \frac{2\ell n}{d k}\]</div>
<p>We can estimate <span class="math notranslate nohighlight">\(\pi\)</span> by throwing needles!</p>
<p>This is a Monte Carlo method avant la lettre: we use random experiments to estimate a deterministic quantity. Of course, Buffon lacked computers, and actually throwing thousands of needles by hand is tedious. In 1901, the Italian mathematician Mario Lazzarini claimed to have obtained <span class="math notranslate nohighlight">\(\pi \approx 3.1415929\)</span> by throwing a needle 3,408 times—suspiciously close to the correct value of <span class="math notranslate nohighlight">\(355/113\)</span>. Most historians believe Lazzarini fudged his data, but the underlying principle was sound.</p>
<div class="tip admonition">
<p class="admonition-title">Try It Yourself 🖥️ Buffon’s Needle Simulation</p>
<p>Experience Buffon’s experiment interactively:</p>
<p><strong>Interactive Simulation</strong>: <a class="reference external" href="https://treese41528.github.io/ComputationalDataScience/Simulations/MonteCarloSimulation/buffons_needle_simulation.html">https://treese41528.github.io/ComputationalDataScience/Simulations/MonteCarloSimulation/buffons_needle_simulation.html</a></p>
<p>Watch how the <span class="math notranslate nohighlight">\(\pi\)</span> estimate fluctuates wildly with few needles, then gradually stabilizes as you accumulate thousands of throws. This is the Law of Large Numbers in action—a theme we will return to throughout this chapter.</p>
</div>
</section>
<section id="fermi-s-envelope-calculations">
<h3>Fermi’s Envelope Calculations<a class="headerlink" href="#fermi-s-envelope-calculations" title="Link to this heading"></a></h3>
<p>The physicist Enrico Fermi was famous for his ability to estimate quantities that seemed impossibly difficult to calculate. How many piano tuners are there in Chicago? How much energy is released in a nuclear explosion? Fermi would break these problems into pieces, estimate each piece roughly, and multiply—often achieving answers accurate to within an order of magnitude.</p>
<p>Less well known is that Fermi also pioneered proto-Monte Carlo methods. In the 1930s, working on neutron diffusion problems in Rome, Fermi developed a mechanical device—essentially a specialized slide rule—that could generate random numbers to simulate neutron paths through matter. He used these simulations to estimate quantities like neutron absorption cross-sections, which were too complex to compute analytically.</p>
<p>This work remained largely unpublished, but it anticipated the key insight of Monte Carlo: when a deterministic calculation is intractable, a stochastic simulation may succeed. Fermi’s physical random number generator was crude, but the principle was the same one that Ulam and von Neumann would later implement on electronic computers.</p>
</section>
<section id="the-manhattan-project-and-eniac">
<h3>The Manhattan Project and ENIAC<a class="headerlink" href="#the-manhattan-project-and-eniac" title="Link to this heading"></a></h3>
<p>The development of nuclear weapons during World War II created an urgent need for computational methods. The behavior of neutrons in a nuclear reaction—how they scatter, slow down, and trigger fission—depends on complex integrals over energy and angle that resist analytical solution. The physicists at Los Alamos needed numbers, not theorems.</p>
<p>It was in this context that Ulam’s solitaire insight proved transformative. Ulam and von Neumann realized that the same principle—estimate a complicated quantity by averaging over random samples—could be applied to neutron transport. Instead of integrating over all possible neutron paths analytically (impossible), they could simulate thousands of individual neutrons, tracking each one as it scattered and absorbed through the weapon’s core.</p>
<p>Von Neumann took the lead in implementing these ideas on ENIAC, one of the first general-purpose electronic computers. ENIAC could perform about 5,000 operations per second—glacially slow by modern standards, but revolutionary in 1946. Von Neumann and his team programmed ENIAC to simulate neutron histories, and the results helped validate the design of thermonuclear weapons.</p>
<p>The “Monte Carlo method” was formally introduced to the broader scientific community in a 1949 paper by Metropolis and Ulam, though much of the early work remained classified for decades. The name, coined by Metropolis, captured both the element of chance central to the method and the slightly disreputable excitement of gambling—a fitting tribute to Ulam’s card-playing origins.</p>
</section>
<section id="why-monte-carlo-changed-everything">
<h3>Why “Monte Carlo” Changed Everything<a class="headerlink" href="#why-monte-carlo-changed-everything" title="Link to this heading"></a></h3>
<p>The Monte Carlo revolution was not merely about having faster computers. It represented a conceptual breakthrough: <em>randomness is a computational resource</em>. By embracing uncertainty rather than fighting it, Monte Carlo methods could attack problems that deterministic methods could not touch.</p>
<p>Consider the challenge of computing a 100-dimensional integral. Deterministic quadrature methods—the trapezoidal rule, Simpson’s rule, Gaussian quadrature—all suffer from the “curse of dimensionality.” If we use <span class="math notranslate nohighlight">\(n\)</span> points per dimension, we need <span class="math notranslate nohighlight">\(n^{100}\)</span> total evaluations. Even with <span class="math notranslate nohighlight">\(n = 2\)</span>, this exceeds <span class="math notranslate nohighlight">\(10^{30}\)</span>—more function evaluations than atoms in a human body.</p>
<p>Monte Carlo methods sidestep this curse entirely. As we will see, the <em>convergence rate</em> of a Monte Carlo estimate depends only on the number of samples, not on the dimension of the space. A 100-dimensional integral converges at the same <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> rate as a one-dimensional integral. This dimension-independence of the <em>rate</em> is the source of Monte Carlo’s power—though as we will discuss, the constant factor (the variance) may still depend on dimension.</p>
</section>
</section>
<section id="the-core-principle-expectation-as-integration">
<h2>The Core Principle: Expectation as Integration<a class="headerlink" href="#the-core-principle-expectation-as-integration" title="Link to this heading"></a></h2>
<p>We now turn to the mathematical foundations of Monte Carlo integration. The key insight is simple but profound: any integral can be rewritten as an expected value, and expected values can be estimated by averaging samples.</p>
<section id="from-integrals-to-expectations">
<h3>From Integrals to Expectations<a class="headerlink" href="#from-integrals-to-expectations" title="Link to this heading"></a></h3>
<p>Consider a general integral of the form:</p>
<div class="math notranslate nohighlight" id="equation-general-integral">
<span class="eqno">(1)<a class="headerlink" href="#equation-general-integral" title="Link to this equation"></a></span>\[I = \int_{\mathcal{X}} g(x) \, dx\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{X} \subseteq \mathbb{R}^d\)</span> is the domain of integration and <span class="math notranslate nohighlight">\(g: \mathcal{X} \to \mathbb{R}\)</span> is the function we wish to integrate. At first glance, this seems like a problem for calculus, not probability. But watch what happens when we introduce a probability density.</p>
<p>Let <span class="math notranslate nohighlight">\(f(x)\)</span> be any probability density function on <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>—that is, <span class="math notranslate nohighlight">\(f(x) \geq 0\)</span> everywhere and <span class="math notranslate nohighlight">\(\int_{\mathcal{X}} f(x) \, dx = 1\)</span>. We can rewrite our integral as:</p>
<div class="math notranslate nohighlight" id="equation-importance-rewrite">
<span class="eqno">(2)<a class="headerlink" href="#equation-importance-rewrite" title="Link to this equation"></a></span>\[I = \int_{\mathcal{X}} g(x) \, dx = \int_{\mathcal{X}} \frac{g(x)}{f(x)} f(x) \, dx = \mathbb{E}_f\left[ \frac{g(X)}{f(X)} \right]\]</div>
<p>where the expectation is taken over a random variable <span class="math notranslate nohighlight">\(X\)</span> with density <span class="math notranslate nohighlight">\(f\)</span>. We have transformed an integral into an expected value!</p>
<p>The simplest choice is the uniform density on <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. If <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> has finite volume <span class="math notranslate nohighlight">\(V = \int_{\mathcal{X}} dx\)</span>, then <span class="math notranslate nohighlight">\(f(x) = 1/V\)</span> is a valid density, and:</p>
<div class="math notranslate nohighlight">
\[I = V \cdot \mathbb{E}_{\text{Uniform}(\mathcal{X})}[g(X)]\]</div>
<p>For example, to compute <span class="math notranslate nohighlight">\(\int_0^1 e^{-x^2} dx\)</span>, we write:</p>
<div class="math notranslate nohighlight">
\[\int_0^1 e^{-x^2} dx = \mathbb{E}[e^{-U^2}] \quad \text{where } U \sim \text{Uniform}(0, 1)\]</div>
<p>This rewriting is always possible. But why is it useful?</p>
<p><strong>Two Cases to Keep Straight</strong></p>
<p>Throughout this chapter, we encounter two closely related formulations:</p>
<ul class="simple">
<li><p><strong>Case A (Expectation)</strong>: We want <span class="math notranslate nohighlight">\(I = \mathbb{E}_f[h(X)] = \int h(x) f(x) \, dx\)</span> where <span class="math notranslate nohighlight">\(f\)</span> is a known density and <span class="math notranslate nohighlight">\(X \sim f\)</span>.</p></li>
<li><p><strong>Case B (Plain integral)</strong>: We want <span class="math notranslate nohighlight">\(I = \int g(x) \, dx\)</span>, which we rewrite as <span class="math notranslate nohighlight">\(I = \mathbb{E}_f[g(X)/f(X)]\)</span> for some sampling density <span class="math notranslate nohighlight">\(f\)</span>.</p></li>
</ul>
<p>In both cases, the Monte Carlo estimator takes the form <span class="math notranslate nohighlight">\(\hat{I}_n = \frac{1}{n}\sum_{i=1}^n w(X_i)\)</span> where <span class="math notranslate nohighlight">\(w(x) = h(x)\)</span> in Case A and <span class="math notranslate nohighlight">\(w(x) = g(x)/f(x)\)</span> in Case B. This unified view—<em>Monte Carlo is averaging weights</em>—will simplify later developments in importance sampling and variance reduction.</p>
</section>
<section id="the-monte-carlo-estimator">
<h3>The Monte Carlo Estimator<a class="headerlink" href="#the-monte-carlo-estimator" title="Link to this heading"></a></h3>
<p>The power of the expectation formulation becomes clear when we recall the Law of Large Numbers. If <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> are independent and identically distributed (iid) with <span class="math notranslate nohighlight">\(\mathbb{E}[X_i] = \mu\)</span>, then the sample mean converges to the true mean:</p>
<div class="math notranslate nohighlight">
\[\bar{X}_n = \frac{1}{n} \sum_{i=1}^{n} X_i \xrightarrow{\text{a.s.}} \mu \quad \text{as } n \to \infty\]</div>
<p>Applied to our integral:</p>
<div class="note admonition">
<p class="admonition-title">Definition: Monte Carlo Estimator</p>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> be iid samples from a density <span class="math notranslate nohighlight">\(f\)</span> on <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. The <strong>Monte Carlo estimator</strong> of the integral <span class="math notranslate nohighlight">\(I = \int_{\mathcal{X}} h(x) f(x) \, dx = \mathbb{E}_f[h(X)]\)</span> is:</p>
<div class="math notranslate nohighlight" id="equation-mc-estimator-def">
<span class="eqno">(3)<a class="headerlink" href="#equation-mc-estimator-def" title="Link to this equation"></a></span>\[\hat{I}_n = \frac{1}{n} \sum_{i=1}^{n} h(X_i)\]</div>
<p>More generally, for <span class="math notranslate nohighlight">\(I = \int_{\mathcal{X}} g(x) \, dx\)</span> where we sample from density <span class="math notranslate nohighlight">\(f\)</span>:</p>
<div class="math notranslate nohighlight">
\[\hat{I}_n = \frac{1}{n} \sum_{i=1}^{n} \frac{g(X_i)}{f(X_i)}\]</div>
</div>
<p>The Monte Carlo method is disarmingly simple: draw random samples, evaluate the function at each sample, and average the results. No derivatives, no quadrature weights, no mesh generation—just sampling and averaging.</p>
<p>But this simplicity conceals depth. The choice of sampling density <span class="math notranslate nohighlight">\(f\)</span> is entirely up to us, and different choices lead to dramatically different performance. We will explore this in the section on importance sampling; for now, we focus on the “naive” case where <span class="math notranslate nohighlight">\(f\)</span> matches the density of the integrand or is uniform on the domain.</p>
<div class="note admonition">
<p class="admonition-title">Algorithm 2.1: Basic Monte Carlo Integration</p>
<p><strong>Input</strong>: Function <span class="math notranslate nohighlight">\(h\)</span>, sampling distribution <span class="math notranslate nohighlight">\(f\)</span>, sample size <span class="math notranslate nohighlight">\(n\)</span></p>
<p><strong>Output</strong>: Estimate <span class="math notranslate nohighlight">\(\hat{I}_n\)</span> and standard error <span class="math notranslate nohighlight">\(\widehat{\text{SE}}\)</span></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>1. Generate X₁, X₂, ..., Xₙ iid from density f
2. Compute h(Xᵢ) for i = 1, ..., n
3. Estimate: Î_n = (1/n) Σᵢ h(Xᵢ)
4. Variance: σ̂² = (1/(n-1)) Σᵢ (h(Xᵢ) - Î_n)²
5. Standard Error: SE = σ̂ / √n
6. Return Î_n, SE
</pre></div>
</div>
<p><strong>Complexity</strong>: <span class="math notranslate nohighlight">\(O(n)\)</span> function evaluations, <span class="math notranslate nohighlight">\(O(n)\)</span> storage (or <span class="math notranslate nohighlight">\(O(1)\)</span> with streaming)</p>
</div>
</section>
<section id="a-first-example-estimating-pi">
<h3>A First Example: Estimating <span class="math notranslate nohighlight">\(\pi\)</span><a class="headerlink" href="#a-first-example-estimating-pi" title="Link to this heading"></a></h3>
<p>Let us return to the problem of estimating <span class="math notranslate nohighlight">\(\pi\)</span>, now with Monte Carlo machinery. Consider the integral:</p>
<div class="math notranslate nohighlight">
\[\pi = \int_{-1}^{1} \int_{-1}^{1} \mathbf{1}_{x^2 + y^2 \leq 1} \, dx \, dy\]</div>
<p>This is the area of the unit disk. Rewriting as an expectation:</p>
<div class="math notranslate nohighlight">
\[\pi = 4 \cdot \mathbb{E}[\mathbf{1}_{X^2 + Y^2 \leq 1}] \quad \text{where } (X, Y) \sim \text{Uniform}([-1,1]^2)\]</div>
<p>The factor of 4 accounts for the area of the square <span class="math notranslate nohighlight">\([-1,1]^2\)</span>. The Monte Carlo estimator is:</p>
<div class="math notranslate nohighlight">
\[\hat{\pi}_n = \frac{4}{n} \sum_{i=1}^{n} \mathbf{1}_{X_i^2 + Y_i^2 \leq 1}\]</div>
<p>That is: generate <span class="math notranslate nohighlight">\(n\)</span> uniform points in the square, count how many fall inside the unit circle, and multiply by 4.</p>
<p>The geometric intuition is immediate: the ratio of points landing inside the circle to total points approximates the ratio of areas, <span class="math notranslate nohighlight">\(\pi/4\)</span>.</p>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_1_fig02_pi_estimation.png"><img alt="Monte Carlo estimation of π showing scatter plot of points inside and outside unit circle, plus convergence plot" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_1_fig02_pi_estimation.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 32 </span><span class="caption-text"><strong>Monte Carlo π Estimation.</strong> Left: Blue points fall inside the unit circle; coral points fall outside. The ratio of blue to total points estimates <span class="math notranslate nohighlight">\(\pi/4\)</span>. Right: The running estimate stabilizes as samples accumulate—the characteristic “noisy convergence” of Monte Carlo.</span><a class="headerlink" href="#id4" title="Link to this image"></a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">estimate_pi_monte_carlo</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Estimate π using Monte Carlo integration.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of random points to generate.</span>
<span class="sd">    seed : int, optional</span>
<span class="sd">        Random seed for reproducibility.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        Contains estimate, standard_error, and confidence interval.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Generate uniform points in [-1, 1]²</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Count points inside unit circle</span>
    <span class="n">inside</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span>

    <span class="c1"># Monte Carlo estimate</span>
    <span class="n">p_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">inside</span><span class="p">)</span>
    <span class="n">pi_hat</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">p_hat</span>

    <span class="c1"># Standard error (indicator has Bernoulli variance p(1-p))</span>
    <span class="n">se_p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">p_hat</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_hat</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="n">se_pi</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">se_p</span>

    <span class="c1"># 95% asymptotic normal confidence interval (CLT justifies this for large n)</span>
    <span class="n">ci</span> <span class="o">=</span> <span class="p">(</span><span class="n">pi_hat</span> <span class="o">-</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">se_pi</span><span class="p">,</span> <span class="n">pi_hat</span> <span class="o">+</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">se_pi</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;estimate&#39;</span><span class="p">:</span> <span class="n">pi_hat</span><span class="p">,</span>
        <span class="s1">&#39;standard_error&#39;</span><span class="p">:</span> <span class="n">se_pi</span><span class="p">,</span>
        <span class="s1">&#39;ci_95&#39;</span><span class="p">:</span> <span class="n">ci</span><span class="p">,</span>
        <span class="s1">&#39;n_samples&#39;</span><span class="p">:</span> <span class="n">n_samples</span>
    <span class="p">}</span>

<span class="c1"># Run the estimation</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">estimate_pi_monte_carlo</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;π estimate: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;estimate&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True π:     </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error:      </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;estimate&#39;</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Std Error:  </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;standard_error&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% CI:     (</span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci_95&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci_95&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>π estimate: 3.143080
True π:     3.141593
Error:      0.001487
Std Error:  0.005190
95% CI:     (3.132908, 3.153252)
</pre></div>
</div>
<p>The true value <span class="math notranslate nohighlight">\(\pi\)</span> lies comfortably within the 95% asymptotic normal confidence interval (valid by CLT for large <span class="math notranslate nohighlight">\(n\)</span>). With a million samples, the error shrinks by a factor of <span class="math notranslate nohighlight">\(\sqrt{10} \approx 3.16\)</span>, and with ten million, by another factor of <span class="math notranslate nohighlight">\(\sqrt{10}\)</span>.</p>
<p><strong>Why Bernoulli variance?</strong> The comment in the code mentions “Bernoulli variance”—let’s unpack this. Each random point either lands inside the circle (we record <span class="math notranslate nohighlight">\(I_i = 1\)</span>) or outside (we record <span class="math notranslate nohighlight">\(I_i = 0\)</span>). This is a <strong>Bernoulli trial</strong> with success probability <span class="math notranslate nohighlight">\(p = \pi/4\)</span>, which is the ratio of the circle’s area to the square’s area. For any Bernoulli random variable:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[I_i] = p, \qquad \text{Var}(I_i) = p(1 - p)\]</div>
<p>Our estimator <span class="math notranslate nohighlight">\(\hat{p} = \frac{1}{n}\sum_{i=1}^n I_i\)</span> is the sample proportion of points inside the circle. Since the <span class="math notranslate nohighlight">\(I_i\)</span> are independent:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\hat{p}) = \text{Var}\left(\frac{1}{n}\sum_{i=1}^n I_i\right) = \frac{1}{n^2} \cdot n \cdot p(1-p) = \frac{p(1-p)}{n}\]</div>
<p>The standard error is the square root: <span class="math notranslate nohighlight">\(\text{SE}(\hat{p}) = \sqrt{p(1-p)/n}\)</span>. Since <span class="math notranslate nohighlight">\(\hat{\pi} = 4\hat{p}\)</span>, the standard error of <span class="math notranslate nohighlight">\(\hat{\pi}\)</span> scales by 4:</p>
<div class="math notranslate nohighlight">
\[\text{SE}(\hat{\pi}) = 4 \cdot \sqrt{\frac{p(1-p)}{n}}\]</div>
<p>We don’t know the true <span class="math notranslate nohighlight">\(p\)</span>, so we substitute our estimate <span class="math notranslate nohighlight">\(\hat{p}\)</span> to get a usable formula. This Bernoulli structure is a special case of the general Monte Carlo variance formula—whenever the function <span class="math notranslate nohighlight">\(h(x)\)</span> is an indicator (0 or 1), the variance simplifies to the familiar <span class="math notranslate nohighlight">\(p(1-p)\)</span> form.</p>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ⚠️ Wald Confidence Intervals for Proportions</p>
<p>The normal approximation CI <span class="math notranslate nohighlight">\(\hat{p} \pm z_{\alpha/2}\sqrt{\hat{p}(1-\hat{p})/n}\)</span> (called the Wald interval) can perform poorly when <span class="math notranslate nohighlight">\(p\)</span> is near 0 or 1, or when <span class="math notranslate nohighlight">\(n\)</span> is small. For rare events, consider alternatives like the Wilson score interval, Agresti-Coull interval, or exact binomial methods. For the <span class="math notranslate nohighlight">\(\pi\)</span> estimation example where <span class="math notranslate nohighlight">\(p \approx 0.785\)</span>, the Wald interval is adequate.</p>
</div>
<p>This is Monte Carlo at its most basic: evaluate a simple function at random points and average. But even this toy example illustrates the key features of the method—ease of implementation, probabilistic error bounds, and graceful scaling with sample size.</p>
</section>
</section>
<section id="theoretical-foundations">
<h2>Theoretical Foundations<a class="headerlink" href="#theoretical-foundations" title="Link to this heading"></a></h2>
<p>Why does the Monte Carlo method work? What determines the rate of convergence? These questions have precise mathematical answers rooted in classical probability theory.</p>
<div class="important admonition">
<p class="admonition-title">Key Assumptions for Monte Carlo 📋</p>
<p>The theoretical guarantees for Monte Carlo integration require:</p>
<ol class="arabic simple">
<li><p><strong>Independence</strong>: Samples <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span> are independent (not merely uncorrelated)</p></li>
<li><p><strong>Identical distribution</strong>: All samples come from the same distribution <span class="math notranslate nohighlight">\(f\)</span></p></li>
<li><p><strong>Finite first moment</strong>: <span class="math notranslate nohighlight">\(\mathbb{E}[|h(X)|] &lt; \infty\)</span> for LLN convergence</p></li>
<li><p><strong>Finite second moment</strong>: <span class="math notranslate nohighlight">\(\mathbb{E}[h(X)^2] &lt; \infty\)</span> for CLT and valid confidence intervals</p></li>
</ol>
<p>When these assumptions fail—dependent samples (MCMC), infinite variance (heavy tails), misspecified sampling distribution—the standard theory must be modified. We address these issues in later sections and chapters.</p>
</div>
<section id="the-law-of-large-numbers">
<h3>The Law of Large Numbers<a class="headerlink" href="#the-law-of-large-numbers" title="Link to this heading"></a></h3>
<p>The Law of Large Numbers (LLN) is the foundational result guaranteeing that Monte Carlo estimators converge to the true value. There are several versions; we state the strongest form.</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Strong Law of Large Numbers</p>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots\)</span> be independent and identically distributed random variables with <span class="math notranslate nohighlight">\(\mathbb{E}[|X_1|] &lt; \infty\)</span>. Then:</p>
<div class="math notranslate nohighlight">
\[\bar{X}_n = \frac{1}{n} \sum_{i=1}^{n} X_i \xrightarrow{\text{a.s.}} \mathbb{E}[X_1] \quad \text{as } n \to \infty\]</div>
<p>The notation <span class="math notranslate nohighlight">\(\xrightarrow{\text{a.s.}}\)</span> denotes <em>almost sure convergence</em>: the probability that the sequence converges is exactly 1.</p>
</div>
<p>For Monte Carlo integration, we apply this theorem with <span class="math notranslate nohighlight">\(X_i = h(X_i)\)</span> where the <span class="math notranslate nohighlight">\(X_i\)</span> are iid from density <span class="math notranslate nohighlight">\(f\)</span>. The condition <span class="math notranslate nohighlight">\(\mathbb{E}[|h(X)|] &lt; \infty\)</span> ensures that the integral we are estimating actually exists.</p>
<p>The LLN tells us that <span class="math notranslate nohighlight">\(\hat{I}_n \to I\)</span> with probability 1. No matter how complex the integrand, no matter how high the dimension, the Monte Carlo estimator will eventually get arbitrarily close to the true value. This is an extraordinarily powerful guarantee.</p>
<p>But the LLN is silent on <em>how fast</em> convergence occurs. For that, we need the Central Limit Theorem.</p>
</section>
<section id="the-central-limit-theorem-and-the-o-n-1-2-rate">
<h3>The Central Limit Theorem and the <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> Rate<a class="headerlink" href="#the-central-limit-theorem-and-the-o-n-1-2-rate" title="Link to this heading"></a></h3>
<p>The Central Limit Theorem (CLT) is the workhorse result for quantifying Monte Carlo error.</p>
<div class="note admonition">
<p class="admonition-title">Theorem: Central Limit Theorem</p>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots\)</span> be iid with mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2 &lt; \infty\)</span>. Then:</p>
<div class="math notranslate nohighlight">
\[\sqrt{n} \left( \bar{X}_n - \mu \right) \xrightarrow{d} \mathcal{N}(0, \sigma^2)\]</div>
<p>where <span class="math notranslate nohighlight">\(\xrightarrow{d}\)</span> denotes convergence in distribution. Equivalently, for large <span class="math notranslate nohighlight">\(n\)</span>:</p>
<div class="math notranslate nohighlight">
\[\bar{X}_n \stackrel{\cdot}{\sim} \mathcal{N}\left( \mu, \frac{\sigma^2}{n} \right)\]</div>
</div>
<p>Applied to Monte Carlo integration:</p>
<div class="math notranslate nohighlight">
\[\hat{I}_n \stackrel{\cdot}{\sim} \mathcal{N}\left( I, \frac{\sigma^2}{n} \right)\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma^2 = \text{Var}_f[h(X)] = \mathbb{E}_f[(h(X) - I)^2]\)</span> is the variance of the integrand under the sampling distribution.</p>
<p>The <strong>standard error</strong> of the Monte Carlo estimator is:</p>
<div class="math notranslate nohighlight" id="equation-mc-standard-error">
<span class="eqno">(4)<a class="headerlink" href="#equation-mc-standard-error" title="Link to this equation"></a></span>\[\text{SE}(\hat{I}_n) = \frac{\sigma}{\sqrt{n}}\]</div>
<p>This is the celebrated <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> convergence rate. To reduce the standard error by a factor of 10, we need 100 times as many samples. To gain one decimal place of accuracy, we need 100 times the computational effort.</p>
<p><strong>The rate is dimension-independent, but the constant may not be.</strong> Whether we integrate over <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> or <span class="math notranslate nohighlight">\(\mathbb{R}^{1000}\)</span>, the error decreases as <span class="math notranslate nohighlight">\(1/\sqrt{n}\)</span>. This dimension-independence of the <em>rate</em> is Monte Carlo’s fundamental advantage. However, the variance <span class="math notranslate nohighlight">\(\sigma^2 = \text{Var}[h(X)]\)</span> can itself depend on dimension—poorly designed estimators may have variance that grows with <span class="math notranslate nohighlight">\(d\)</span>, partially offsetting Monte Carlo’s advantage. This is why importance sampling, control variates, and careful problem formulation remain important even for Monte Carlo.</p>
<div class="note admonition">
<p class="admonition-title">Example 💡 Understanding the Square Root Law</p>
<p><strong>Scenario</strong>: You estimate an integral with 1,000 samples and get a standard error of 0.1. Your boss needs the error reduced to 0.01.</p>
<p><strong>Analysis</strong>: The standard error scales as <span class="math notranslate nohighlight">\(1/\sqrt{n}\)</span>. To reduce the standard error by a factor of 10, you need <span class="math notranslate nohighlight">\(n\)</span> to increase by a factor of <span class="math notranslate nohighlight">\(10^2 = 100\)</span>.</p>
<p><strong>Conclusion</strong>: You need <span class="math notranslate nohighlight">\(1000 \times 100 = 100,000\)</span> samples.</p>
<p>This quadratic penalty is the price of Monte Carlo’s simplicity. In low dimensions, deterministic methods often achieve polynomial convergence rates like <span class="math notranslate nohighlight">\(O(n^{-2})\)</span> or better, making them far more efficient. But in high dimensions, Monte Carlo’s dimension-independent <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> rate beats any polynomial rate that degrades exponentially with dimension.</p>
</div>
</section>
<section id="why-the-square-root">
<h3>Why the Square Root?<a class="headerlink" href="#why-the-square-root" title="Link to this heading"></a></h3>
<p>The <span class="math notranslate nohighlight">\(1/\sqrt{n}\)</span> rate may seem mysterious, but it has a simple explanation rooted in the behavior of sums of random variables.</p>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_1_fig03_sqrt_n_convergence.png"><img alt="Three-panel visualization explaining Monte Carlo convergence: LLN guarantees convergence, CLT shows distributions tightening, and the O(n^{-1/2}) rate governs speed" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_1_fig03_sqrt_n_convergence.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 33 </span><span class="caption-text"><strong>Monte Carlo Convergence: Theory and Practice.</strong> Left: The Law of Large Numbers in action—eight independent Monte Carlo trajectories estimating π all converge to the true value (red dashed line), with the 95% confidence band shrinking as n grows. Middle: The Central Limit Theorem visualized—sampling distributions of the estimator for n = 25, 100, 400, and 1600 show how estimates concentrate around the true value as sample size increases. Right: The <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> convergence rate—standard error decreases as <span class="math notranslate nohighlight">\(\sigma/\sqrt{n}\)</span>, meaning 100× more samples yield only 10× error reduction (one additional decimal of precision).</span><a class="headerlink" href="#id5" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Consider <span class="math notranslate nohighlight">\(n\)</span> iid random variables <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span>, each with variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>. Their sum has variance:</p>
<div class="math notranslate nohighlight">
\[\text{Var}\left( \sum_{i=1}^n X_i \right) = \sum_{i=1}^n \text{Var}(X_i) = n\sigma^2\]</div>
<p>The variance of the sum grows linearly with <span class="math notranslate nohighlight">\(n\)</span>. But when we take the mean, we divide by <span class="math notranslate nohighlight">\(n\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{Var}\left( \frac{1}{n} \sum_{i=1}^n X_i \right) = \frac{1}{n^2} \cdot n\sigma^2 = \frac{\sigma^2}{n}\]</div>
<p>The standard deviation is the square root of variance, giving <span class="math notranslate nohighlight">\(\sigma/\sqrt{n}\)</span>.</p>
<p>This behavior is fundamental to averages of random quantities. Each additional sample adds information, but with diminishing returns: the first sample reduces uncertainty enormously; the millionth sample contributes almost nothing. This is why the square root appears.</p>
</section>
</section>
<section id="variance-estimation-and-confidence-intervals">
<h2>Variance Estimation and Confidence Intervals<a class="headerlink" href="#variance-estimation-and-confidence-intervals" title="Link to this heading"></a></h2>
<p>The CLT tells us that <span class="math notranslate nohighlight">\(\hat{I}_n\)</span> is approximately normal with known variance <span class="math notranslate nohighlight">\(\sigma^2/n\)</span>. But we rarely know <span class="math notranslate nohighlight">\(\sigma^2\)</span>—it depends on the integrand and the sampling distribution. We must estimate it from the same samples we use to estimate <span class="math notranslate nohighlight">\(I\)</span>.</p>
<section id="the-sample-variance">
<h3>The Sample Variance<a class="headerlink" href="#the-sample-variance" title="Link to this heading"></a></h3>
<p>The natural estimator of <span class="math notranslate nohighlight">\(\sigma^2 = \text{Var}[h(X)]\)</span> is the sample variance:</p>
<div class="math notranslate nohighlight" id="equation-sample-var">
<span class="eqno">(5)<a class="headerlink" href="#equation-sample-var" title="Link to this equation"></a></span>\[\hat{\sigma}^2_n = \frac{1}{n-1} \sum_{i=1}^{n} \left( h(X_i) - \hat{I}_n \right)^2\]</div>
<p>The divisor <span class="math notranslate nohighlight">\(n-1\)</span> (rather than <span class="math notranslate nohighlight">\(n\)</span>) makes this estimator unbiased: <span class="math notranslate nohighlight">\(\mathbb{E}[\hat{\sigma}^2_n] = \sigma^2\)</span>. This is known as Bessel’s correction.</p>
<p>By the Law of Large Numbers, <span class="math notranslate nohighlight">\(\hat{\sigma}^2_n \to \sigma^2\)</span> almost surely as <span class="math notranslate nohighlight">\(n \to \infty\)</span>. Combined with the CLT, this gives us a practical way to construct confidence intervals.</p>
</section>
<section id="constructing-confidence-intervals">
<h3>Constructing Confidence Intervals<a class="headerlink" href="#constructing-confidence-intervals" title="Link to this heading"></a></h3>
<p>An asymptotic <span class="math notranslate nohighlight">\((1-\alpha)\)</span> confidence interval for <span class="math notranslate nohighlight">\(I\)</span> is:</p>
<div class="math notranslate nohighlight" id="equation-mc-ci-full">
<span class="eqno">(6)<a class="headerlink" href="#equation-mc-ci-full" title="Link to this equation"></a></span>\[\left[ \hat{I}_n - z_{\alpha/2} \frac{\hat{\sigma}_n}{\sqrt{n}}, \quad \hat{I}_n + z_{\alpha/2} \frac{\hat{\sigma}_n}{\sqrt{n}} \right]\]</div>
<p>where <span class="math notranslate nohighlight">\(z_{\alpha/2} = \Phi^{-1}(1 - \alpha/2)\)</span> is the standard normal quantile. For common confidence levels:</p>
<ul class="simple">
<li><p>90% CI: <span class="math notranslate nohighlight">\(z_{0.05} \approx 1.645\)</span></p></li>
<li><p>95% CI: <span class="math notranslate nohighlight">\(z_{0.025} \approx 1.960\)</span></p></li>
<li><p>99% CI: <span class="math notranslate nohighlight">\(z_{0.005} \approx 2.576\)</span></p></li>
</ul>
<p>The interval has the interpretation: in repeated sampling, approximately <span class="math notranslate nohighlight">\((1-\alpha) \times 100\%\)</span> of such intervals will contain the true value <span class="math notranslate nohighlight">\(I\)</span>.</p>
<p><strong>Standard error vs. confidence interval half-width</strong>: These are related but distinct concepts:</p>
<ul class="simple">
<li><p><strong>Standard error (SE)</strong>: <span class="math notranslate nohighlight">\(\widehat{\text{SE}} = \hat{\sigma}/\sqrt{n}\)</span>, a measure of estimation precision</p></li>
<li><p><strong>95% CI half-width</strong>: <span class="math notranslate nohighlight">\(1.96 \times \widehat{\text{SE}}\)</span>, the margin of error for a specific confidence level</p></li>
</ul>
<p>When planning sample sizes, be explicit about which quantity you are targeting. To achieve SE <span class="math notranslate nohighlight">\(\leq \epsilon\)</span>, you need <span class="math notranslate nohighlight">\(n \geq \sigma^2/\epsilon^2\)</span>. To achieve 95% CI half-width <span class="math notranslate nohighlight">\(\leq \epsilon\)</span>, you need <span class="math notranslate nohighlight">\(n \geq (1.96\sigma/\epsilon)^2 \approx 3.84\sigma^2/\epsilon^2\)</span>—about 4× more samples.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">monte_carlo_integrate</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">sampler</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">confidence</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Monte Carlo integration with uncertainty quantification.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    h : callable</span>
<span class="sd">        Function to integrate. Must accept array input.</span>
<span class="sd">    sampler : callable</span>
<span class="sd">        Function that takes (n, rng) and returns n samples from the target distribution.</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of Monte Carlo samples.</span>
<span class="sd">    confidence : float</span>
<span class="sd">        Confidence level for interval (default 0.95).</span>
<span class="sd">    seed : int, optional</span>
<span class="sd">        Random seed for reproducibility.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        Contains estimate, std_error, confidence interval, and diagnostics.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Generate samples and evaluate function</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">sampler</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
    <span class="n">h_values</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

    <span class="c1"># Point estimate</span>
    <span class="n">estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_values</span><span class="p">)</span>

    <span class="c1"># Variance estimation (Bessel&#39;s correction)</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">h_values</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">std_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">variance</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Confidence interval</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">confidence</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">ci_lower</span> <span class="o">=</span> <span class="n">estimate</span> <span class="o">-</span> <span class="n">z</span> <span class="o">*</span> <span class="n">std_error</span>
    <span class="n">ci_upper</span> <span class="o">=</span> <span class="n">estimate</span> <span class="o">+</span> <span class="n">z</span> <span class="o">*</span> <span class="n">std_error</span>

    <span class="c1"># Effective sample size (for future variance reduction comparisons)</span>
    <span class="n">ess</span> <span class="o">=</span> <span class="n">n_samples</span>  <span class="c1"># For standard MC, ESS = n</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;estimate&#39;</span><span class="p">:</span> <span class="n">estimate</span><span class="p">,</span>
        <span class="s1">&#39;std_error&#39;</span><span class="p">:</span> <span class="n">std_error</span><span class="p">,</span>
        <span class="s1">&#39;variance&#39;</span><span class="p">:</span> <span class="n">variance</span><span class="p">,</span>
        <span class="s1">&#39;ci&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">ci_lower</span><span class="p">,</span> <span class="n">ci_upper</span><span class="p">),</span>
        <span class="s1">&#39;confidence&#39;</span><span class="p">:</span> <span class="n">confidence</span><span class="p">,</span>
        <span class="s1">&#39;n_samples&#39;</span><span class="p">:</span> <span class="n">n_samples</span><span class="p">,</span>
        <span class="s1">&#39;ess&#39;</span><span class="p">:</span> <span class="n">ess</span><span class="p">,</span>
        <span class="s1">&#39;h_values&#39;</span><span class="p">:</span> <span class="n">h_values</span>
    <span class="p">}</span>
</pre></div>
</div>
<div class="note admonition">
<p class="admonition-title">Example 💡 Using the Monte Carlo Integration Function</p>
<p><strong>Problem:</strong> Estimate <span class="math notranslate nohighlight">\(\int_0^2 e^{-x^2} dx\)</span> using our <code class="docutils literal notranslate"><span class="pre">monte_carlo_integrate</span></code> function.</p>
<p><strong>Setup:</strong> We need to define: (1) the integrand <span class="math notranslate nohighlight">\(h(x) = 2 e^{-x^2}\)</span> (the factor of 2 accounts for the interval length), and (2) a sampler that generates uniform samples on <span class="math notranslate nohighlight">\([0, 2]\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.special</span><span class="w"> </span><span class="kn">import</span> <span class="n">erf</span>

<span class="c1"># Define the integrand (scaled by interval length)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">h</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Define the sampler: Uniform(0, 2)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">uniform_sampler</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">rng</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>

<span class="c1"># Run Monte Carlo integration</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">monte_carlo_integrate</span><span class="p">(</span>
    <span class="n">h</span><span class="o">=</span><span class="n">h</span><span class="p">,</span>
    <span class="n">sampler</span><span class="o">=</span><span class="n">uniform_sampler</span><span class="p">,</span>
    <span class="n">n_samples</span><span class="o">=</span><span class="mi">100_000</span><span class="p">,</span>
    <span class="n">confidence</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="c1"># True value for comparison</span>
<span class="n">true_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">erf</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimate:    </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;estimate&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True value:  </span><span class="si">{</span><span class="n">true_value</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Std Error:   </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;std_error&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% CI:      (</span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;CI contains true value: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">true_value</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Output:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Estimate:    0.880204
True value:  0.882081
Std Error:   0.002179
95% CI:      (0.875934, 0.884474)
CI contains true value: True
</pre></div>
</div>
<p>The function returns all the diagnostics we need: the point estimate, standard error for assessing precision, and a confidence interval that correctly captures the true value. The <code class="docutils literal notranslate"><span class="pre">h_values</span></code> array can be passed to convergence diagnostics for further analysis.</p>
</div>
</section>
<section id="numerical-stability-welford-s-algorithm">
<h3>Numerical Stability: Welford’s Algorithm<a class="headerlink" href="#numerical-stability-welford-s-algorithm" title="Link to this heading"></a></h3>
<p>Computing the sample variance naively using the one-pass formula <span class="math notranslate nohighlight">\(\frac{1}{n-1}\left(\sum h_i^2 - \frac{(\sum h_i)^2}{n}\right)\)</span> can suffer catastrophic cancellation when the mean is large compared to the standard deviation. The two terms <span class="math notranslate nohighlight">\(\sum h_i^2\)</span> and <span class="math notranslate nohighlight">\(\frac{(\sum h_i)^2}{n}\)</span> may be nearly equal, and their difference may lose many significant digits.</p>
<p><strong>The Problem Illustrated</strong>: Suppose we have data with mean <span class="math notranslate nohighlight">\(\mu = 10^9\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma = 1\)</span>. Then <span class="math notranslate nohighlight">\(\sum x_i^2 \approx n \cdot 10^{18}\)</span> and <span class="math notranslate nohighlight">\((\sum x_i)^2/n \approx n \cdot 10^{18}\)</span> as well. Their difference should be approximately <span class="math notranslate nohighlight">\(n \cdot \sigma^2 = n\)</span>, but when subtracting two numbers of size <span class="math notranslate nohighlight">\(10^{18}\)</span> that agree in their first 16-17 digits, we lose almost all precision in 64-bit floating point arithmetic (which has about 15-16 significant decimal digits).</p>
<p><strong>Welford’s Insight</strong>: Instead of computing variance from <span class="math notranslate nohighlight">\(\sum x_i^2\)</span> and <span class="math notranslate nohighlight">\(\sum x_i\)</span>, we can maintain the sum of squared deviations <em>from the current running mean</em>. As each new observation arrives, we update both the mean and the sum of squared deviations using a clever algebraic identity.</p>
<p>Let <span class="math notranslate nohighlight">\(\bar{x}_n\)</span> denote the mean of the first <span class="math notranslate nohighlight">\(n\)</span> observations, and let <span class="math notranslate nohighlight">\(M_{2,n} = \sum_{i=1}^n (x_i - \bar{x}_n)^2\)</span> denote the sum of squared deviations from this mean. Welford showed that these can be updated incrementally:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\bar{x}_n &amp;= \bar{x}_{n-1} + \frac{x_n - \bar{x}_{n-1}}{n} \\
M_{2,n} &amp;= M_{2,n-1} + (x_n - \bar{x}_{n-1})(x_n - \bar{x}_n)\end{split}\]</div>
<p>The key insight is that <span class="math notranslate nohighlight">\((x_n - \bar{x}_{n-1})\)</span> and <span class="math notranslate nohighlight">\((x_n - \bar{x}_n)\)</span> are both small numbers (deviations from means), so their product is numerically stable. We never subtract two large, nearly-equal quantities.</p>
<p>The sample variance is then simply <span class="math notranslate nohighlight">\(s^2 = M_{2,n} / (n-1)\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">WelfordAccumulator</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Online algorithm for computing mean and variance in a single pass.</span>

<span class="sd">    This is a true streaming algorithm: we never store the data,</span>
<span class="sd">    only the running statistics. Memory usage is O(1) regardless</span>
<span class="sd">    of how many values we process.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">M2</span> <span class="o">=</span> <span class="mf">0.0</span>  <span class="c1"># Sum of squared deviations from current mean</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Process a single new observation.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">+=</span> <span class="n">delta</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span>
        <span class="n">delta2</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>  <span class="c1"># Note: uses UPDATED mean</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">M2</span> <span class="o">+=</span> <span class="n">delta</span> <span class="o">*</span> <span class="n">delta2</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">variance</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sample variance (with Bessel&#39;s correction).&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;nan&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">M2</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">std</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sample standard deviation.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">variance</span><span class="p">)</span>

<span class="c1"># Demonstrate the numerical stability issue</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">5678</span><span class="p">)</span>
<span class="n">large_mean_data</span> <span class="o">=</span> <span class="mf">1e9</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span>  <span class="c1"># Mean ≈ 10⁹, SD ≈ 1</span>

<span class="c1"># Welford&#39;s algorithm (stable)</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">WelfordAccumulator</span><span class="p">()</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">large_mean_data</span><span class="p">:</span>
    <span class="n">acc</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Naive one-pass formula (UNSTABLE)</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">large_mean_data</span><span class="p">)</span>
<span class="n">sum_sq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">large_mean_data</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">sum_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">large_mean_data</span><span class="p">)</span>
<span class="n">naive_var</span> <span class="o">=</span> <span class="p">(</span><span class="n">sum_sq</span> <span class="o">-</span> <span class="n">sum_x</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># NumPy&#39;s implementation (also stable)</span>
<span class="n">numpy_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">large_mean_data</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True variance (approx): 1.0&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Welford (stable):       </span><span class="si">{</span><span class="n">acc</span><span class="o">.</span><span class="n">variance</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;NumPy (stable):         </span><span class="si">{</span><span class="n">numpy_var</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Naive one-pass:         </span><span class="si">{</span><span class="n">naive_var</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">  &lt;- likely wrong!&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The naive formula can give wildly incorrect results (sometimes off by orders of magnitude, sometimes even negative!) due to catastrophic cancellation. Both Welford and NumPy give the correct answer. The exact value of the naive result depends on hardware, compiler optimizations, and BLAS implementations—the point is that it fails catastrophically, not the specific wrong value it produces.</p>
<p>In practice, NumPy’s <code class="docutils literal notranslate"><span class="pre">np.var</span></code> uses numerically stable algorithms, so you rarely need to implement Welford’s algorithm yourself. But understanding why stability matters is important for several reasons:</p>
<ol class="arabic simple">
<li><p><strong>Debugging unexpected results</strong>: If you’re computing variances in a custom loop or in a language without stable built-ins, you may encounter this issue.</p></li>
<li><p><strong>Streaming data</strong>: Welford’s algorithm processes data in a single pass, making it ideal for streaming applications where you can’t store all values in memory.</p></li>
<li><p><strong>Parallel computation</strong>: The algorithm can be extended to combine statistics from separate batches (useful for distributed computing).</p></li>
<li><p><strong>Understanding Monte Carlo diagnostics</strong>: Running variance calculations in convergence diagnostics use similar techniques.</p></li>
</ol>
<p>The key lesson: when the mean is large relative to the standard deviation, naive variance formulas fail catastrophically. Always use stable algorithms.</p>
</section>
</section>
<section id="worked-examples">
<h2>Worked Examples<a class="headerlink" href="#worked-examples" title="Link to this heading"></a></h2>
<p>We now work through several examples of increasing complexity, illustrating the breadth of Monte Carlo applications.</p>
<section id="example-1-the-gaussian-integral">
<h3>Example 1: The Gaussian Integral<a class="headerlink" href="#example-1-the-gaussian-integral" title="Link to this heading"></a></h3>
<p>The integral <span class="math notranslate nohighlight">\(\int_{-\infty}^{\infty} e^{-x^2} dx = \sqrt{\pi}\)</span> is famous for being impossible to evaluate in closed form using elementary antiderivatives, yet having a beautiful exact answer. Let us estimate it via Monte Carlo.</p>
<p><strong>Challenge</strong>: The domain is infinite, so we cannot sample uniformly.</p>
<p><strong>Solution</strong>: Recognize the integrand as proportional to a Gaussian density. Recall our notation convention: <span class="math notranslate nohighlight">\(\mathcal{N}(\mu, \sigma^2)\)</span> denotes a normal distribution with mean <span class="math notranslate nohighlight">\(\mu\)</span> and <strong>variance</strong> <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
<p>If <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(0, 1/2)\)</span> (variance = 1/2, so standard deviation = <span class="math notranslate nohighlight">\(1/\sqrt{2}\)</span>), then:</p>
<div class="math notranslate nohighlight">
\[f(x) = \frac{1}{\sqrt{2\pi \cdot (1/2)}} \exp\left(-\frac{x^2}{2 \cdot (1/2)}\right) = \frac{1}{\sqrt{\pi}} e^{-x^2}\]</div>
<p>This is proportional to our integrand <span class="math notranslate nohighlight">\(e^{-x^2}\)</span>, with the normalizing constant being exactly what we want to compute. We can verify:</p>
<div class="math notranslate nohighlight">
\[\int_{-\infty}^{\infty} e^{-x^2} dx = \sqrt{\pi} \int_{-\infty}^{\infty} \frac{1}{\sqrt{\pi}} e^{-x^2} dx = \sqrt{\pi} \cdot 1 = \sqrt{\pi}\]</div>
<p>This derivation shows the integral equals <span class="math notranslate nohighlight">\(\sqrt{\pi}\)</span> exactly. But let’s also verify by Monte Carlo using importance sampling.</p>
<p><strong>Monte Carlo approach</strong>: Sample from <span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span> (standard normal) and reweight:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">gaussian_integral_mc</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Estimate ∫ exp(-x²) dx via Monte Carlo importance sampling.</span>

<span class="sd">    We sample from N(0, 1) and use:</span>
<span class="sd">    ∫ exp(-x²) dx = ∫ [exp(-x²) / φ(x)] φ(x) dx</span>
<span class="sd">    where φ(x) = exp(-x²/2) / √(2π) is the standard normal density.</span>

<span class="sd">    The ratio: exp(-x²) / φ(x) = √(2π) · exp(-x²/2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Sample from N(0, 1)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Importance weight: exp(-x²) / φ(x) = √(2π) · exp(-x²/2)</span>
    <span class="n">h_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">X</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>

    <span class="n">estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_values</span><span class="p">)</span>
    <span class="n">std_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">h_values</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">estimate</span><span class="p">,</span> <span class="n">std_error</span>

<span class="n">est</span><span class="p">,</span> <span class="n">se</span> <span class="o">=</span> <span class="n">gaussian_integral_mc</span><span class="p">(</span><span class="mi">100_000</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimate: </span><span class="si">{</span><span class="n">est</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">se</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True value: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Estimate: 1.770751 ± 0.002214
True value: 1.772454
</pre></div>
</div>
<p>The estimate is very close to <span class="math notranslate nohighlight">\(\sqrt{\pi} \approx 1.7725\)</span>.</p>
</section>
<section id="example-2-probability-of-a-rare-event">
<h3>Example 2: Probability of a Rare Event<a class="headerlink" href="#example-2-probability-of-a-rare-event" title="Link to this heading"></a></h3>
<p>Suppose <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(0, 1)\)</span>. What is <span class="math notranslate nohighlight">\(P(X &gt; 4)\)</span>?</p>
<p>From standard normal tables, <span class="math notranslate nohighlight">\(P(X &gt; 4) = 1 - \Phi(4) \approx 3.167 \times 10^{-5}\)</span>. This is a rare event—only about 3 in 100,000 standard normal draws exceed 4.</p>
<p><strong>Naive Monte Carlo</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100_000</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">p_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span> <span class="o">&gt;</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">p_hat</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_hat</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimate: </span><span class="si">{</span><span class="n">p_hat</span><span class="si">:</span><span class="s2">.6e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Std Error: </span><span class="si">{</span><span class="n">se</span><span class="si">:</span><span class="s2">.6e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True value: </span><span class="si">{</span><span class="mi">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="si">:</span><span class="s2">.6e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Estimate: 9.000000e-05
Std Error: 2.999865e-05
True value: 3.167124e-05
</pre></div>
</div>
<p>With 100,000 samples, we observed 9 exceedances (instead of the expected ~3), giving an estimate nearly 3× too large! This illustrates the high variability inherent in estimating rare events. The relative error (standard error divided by estimate) is enormous.</p>
<p><strong>Problem</strong>: To estimate a probability <span class="math notranslate nohighlight">\(p\)</span>, the standard error is <span class="math notranslate nohighlight">\(\sqrt{p(1-p)/n} \approx \sqrt{p/n}\)</span> for small <span class="math notranslate nohighlight">\(p\)</span>. The relative error is <span class="math notranslate nohighlight">\(\sqrt{(1-p)/(np)} \approx 1/\sqrt{np}\)</span>. To achieve 10% relative error for <span class="math notranslate nohighlight">\(p = 10^{-5}\)</span>, we need <span class="math notranslate nohighlight">\(n \approx 100/p = 10^7\)</span> samples.</p>
<p>This motivates <strong>importance sampling</strong> (covered in a later section), which generates samples preferentially in the region of interest. For now, the lesson is that naive Monte Carlo struggles with rare events.</p>
</section>
<section id="example-3-a-high-dimensional-integral">
<h3>Example 3: A High-Dimensional Integral<a class="headerlink" href="#example-3-a-high-dimensional-integral" title="Link to this heading"></a></h3>
<p>Consider the integral:</p>
<div class="math notranslate nohighlight">
\[I = \int_{[0,1]^d} \prod_{j=1}^{d} \left( 1 + \frac{x_j}{d} \right) dx_1 \cdots dx_d\]</div>
<p>For large <span class="math notranslate nohighlight">\(d\)</span>, this integral is analytically tractable. The key observation is that the integrand <em>factors</em> into a product of functions, each depending on only one variable:</p>
<div class="math notranslate nohighlight">
\[\prod_{j=1}^{d} \left( 1 + \frac{x_j}{d} \right) = \underbrace{\left(1 + \frac{x_1}{d}\right)}_{f_1(x_1)} \cdot \underbrace{\left(1 + \frac{x_2}{d}\right)}_{f_2(x_2)} \cdots \underbrace{\left(1 + \frac{x_d}{d}\right)}_{f_d(x_d)}\]</div>
<p>When an integrand factors this way, <strong>Fubini’s theorem</strong> tells us the multiple integral equals the product of single integrals:</p>
<div class="math notranslate nohighlight">
\[\int_{[0,1]^d} f_1(x_1) \cdot f_2(x_2) \cdots f_d(x_d) \, dx_1 \cdots dx_d = \left(\int_0^1 f_1(x_1) \, dx_1\right) \cdot \left(\int_0^1 f_2(x_2) \, dx_2\right) \cdots \left(\int_0^1 f_d(x_d) \, dx_d\right)\]</div>
<p>This is analogous to how <span class="math notranslate nohighlight">\(\sum_{i,j} a_i b_j = (\sum_i a_i)(\sum_j b_j)\)</span> for sums. The factorization works because each <span class="math notranslate nohighlight">\(x_j\)</span> integrates independently over its own copy of <span class="math notranslate nohighlight">\([0,1]\)</span>.</p>
<p>Applying this to our integrand, each single integral evaluates to:</p>
<div class="math notranslate nohighlight">
\[\int_0^1 \left(1 + \frac{x_j}{d}\right) dx_j = \left[x_j + \frac{x_j^2}{2d}\right]_0^1 = 1 + \frac{1}{2d}\]</div>
<p>Since all <span class="math notranslate nohighlight">\(d\)</span> integrals are identical, we get:</p>
<div class="math notranslate nohighlight">
\[I = \prod_{j=1}^{d} \int_0^1 \left( 1 + \frac{x_j}{d} \right) dx_j = \left( 1 + \frac{1}{2d} \right)^d \xrightarrow{d \to \infty} \sqrt{e}\]</div>
<p>The limit <span class="math notranslate nohighlight">\(\sqrt{e}\)</span> follows from the definition of <span class="math notranslate nohighlight">\(e\)</span> as <span class="math notranslate nohighlight">\(\lim_{n \to \infty} (1 + 1/n)^n\)</span>. To see this, write:</p>
<div class="math notranslate nohighlight">
\[\left(1 + \frac{1}{2d}\right)^d = \left[\left(1 + \frac{1}{2d}\right)^{2d}\right]^{1/2} \xrightarrow{d \to \infty} e^{1/2} = \sqrt{e}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We chose this particular integrand <em>because</em> it factors nicely, giving us a known true value to verify our Monte Carlo estimates. Most high-dimensional integrands do not factor this way—the variables interact in complex ways—which is precisely why Monte Carlo methods are essential. When there’s no analytical solution, Monte Carlo may be the only practical approach.</p>
</div>
<figure class="align-center" id="id6">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_1_fig06_high_dim_integral.png"><img alt="Three-panel visualization of high-dimensional integral showing true value convergence, error distributions, and parallel convergence rates" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_1_fig06_high_dim_integral.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 34 </span><span class="caption-text"><strong>High-Dimensional Integration.</strong> Left: The true integral value approaches <span class="math notranslate nohighlight">\(\sqrt{e}\)</span> as dimension increases. Middle: At fixed sample size <span class="math notranslate nohighlight">\(n = 10{,}000\)</span>, error spread actually <em>decreases</em> with dimension for this integrand—each factor <span class="math notranslate nohighlight">\((1 + x_j/d)\)</span> becomes closer to 1 as <span class="math notranslate nohighlight">\(d\)</span> grows, reducing variance. This is integrand-specific, not a general property. Right: The key insight is that the <strong>convergence rate</strong> <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> is identical across all dimensions—the curves are parallel on the log-log plot. The constant (vertical offset) may vary, but the slope does not. This dimension-independent rate is Monte Carlo’s superpower.</span><a class="headerlink" href="#id6" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Let us estimate this integral in <span class="math notranslate nohighlight">\(d = 100\)</span> dimensions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">high_dim_integrand</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute product integrand: ∏(1 + x_j/d).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : ndarray of shape (n_samples, d)</span>
<span class="sd">        Sample points in [0,1]^d.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ndarray of shape (n_samples,)</span>
<span class="sd">        Function values.</span>

<span class="sd">    Note: For very large d (&gt;1000), consider using</span>
<span class="sd">    np.exp(np.sum(np.log1p(X/d), axis=1)) for better numerical stability.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">X</span> <span class="o">/</span> <span class="n">d</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">mc_high_dim</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Monte Carlo integration in d dimensions.&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Uniform samples in [0,1]^d</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>

    <span class="c1"># Evaluate integrand</span>
    <span class="n">h_values</span> <span class="o">=</span> <span class="n">high_dim_integrand</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="n">estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_values</span><span class="p">)</span>
    <span class="n">std_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">h_values</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># True value (for comparison)</span>
    <span class="n">true_value</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">/</span><span class="n">d</span><span class="p">)</span><span class="o">**</span><span class="n">d</span>

    <span class="k">return</span> <span class="n">estimate</span><span class="p">,</span> <span class="n">std_error</span><span class="p">,</span> <span class="n">true_value</span>

<span class="c1"># Estimate in 100 dimensions</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">est</span><span class="p">,</span> <span class="n">se</span><span class="p">,</span> <span class="n">truth</span> <span class="o">=</span> <span class="n">mc_high_dim</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="mi">100_000</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;d = </span><span class="si">{</span><span class="n">d</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimate: </span><span class="si">{</span><span class="n">est</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">se</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True value: </span><span class="si">{</span><span class="n">truth</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error: </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">est</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">truth</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>d = 100
Estimate: 1.646530 ± 0.000149
True value: 1.646668
Error: 0.000138
</pre></div>
</div>
<p>The Monte Carlo estimate converges as <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> regardless of <span class="math notranslate nohighlight">\(d\)</span>. Try increasing <span class="math notranslate nohighlight">\(d\)</span> to 1000 or 10,000—the convergence rate remains unchanged.</p>
</section>
<section id="example-4-bayesian-posterior-mean">
<h3>Example 4: Bayesian Posterior Mean<a class="headerlink" href="#example-4-bayesian-posterior-mean" title="Link to this heading"></a></h3>
<p>Bayesian inference often requires computing posterior expectations:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[\theta | \text{data}] = \int \theta \cdot \pi(\theta | \text{data}) \, d\theta\]</div>
<p>When the posterior <span class="math notranslate nohighlight">\(\pi(\theta | \text{data})\)</span> is available (perhaps up to a normalizing constant), Monte Carlo integration applies directly—if we can sample from the posterior. This is the motivation for Markov chain Monte Carlo methods in Part 3.</p>
<p>As a simple example, suppose we observe <span class="math notranslate nohighlight">\(x = 7\)</span> successes in <span class="math notranslate nohighlight">\(n = 10\)</span> Bernoulli trials with unknown success probability <span class="math notranslate nohighlight">\(\theta\)</span>. With a <span class="math notranslate nohighlight">\(\text{Beta}(1, 1)\)</span> (uniform) prior, the posterior is <span class="math notranslate nohighlight">\(\text{Beta}(8, 4)\)</span>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Posterior is Beta(8, 4)</span>
<span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span>
<span class="n">posterior</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>

<span class="c1"># True posterior mean</span>
<span class="n">true_mean</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">/</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="p">)</span>

<span class="c1"># Monte Carlo estimate</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10_000</span>
<span class="n">theta_samples</span> <span class="o">=</span> <span class="n">posterior</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">mc_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">theta_samples</span><span class="p">)</span>
<span class="n">mc_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">theta_samples</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True posterior mean: </span><span class="si">{</span><span class="n">true_mean</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MC estimate: </span><span class="si">{</span><span class="n">mc_mean</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">mc_se</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># We can also estimate posterior quantiles, variance, etc.</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Posterior median (MC): </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">theta_samples</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Posterior median (exact): </span><span class="si">{</span><span class="n">posterior</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>True posterior mean: 0.666667
MC estimate: 0.666200 ± 0.001320
Posterior median (MC): 0.675577
Posterior median (exact): 0.676196
</pre></div>
</div>
<p>The Monte Carlo estimates closely match the exact values. In more complex Bayesian models where the posterior has no closed form, Monte Carlo (via MCMC) becomes essential.</p>
</section>
<section id="example-5-the-normal-cdf">
<h3>Example 5: The Normal CDF<a class="headerlink" href="#example-5-the-normal-cdf" title="Link to this heading"></a></h3>
<p>The cumulative distribution function of the standard normal, <span class="math notranslate nohighlight">\(\Phi(t) = P(Z \leq t)\)</span> for <span class="math notranslate nohighlight">\(Z \sim \mathcal{N}(0, 1)\)</span>, has no closed-form expression. Yet it is one of the most important functions in statistics. Let us estimate <span class="math notranslate nohighlight">\(\Phi(1.96)\)</span>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">estimate_normal_cdf</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Estimate Φ(t) = P(Z ≤ t) for Z ~ N(0,1).&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Indicator function: 1 if Z ≤ t, else 0</span>
    <span class="n">indicators</span> <span class="o">=</span> <span class="n">Z</span> <span class="o">&lt;=</span> <span class="n">t</span>

    <span class="n">p_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">indicators</span><span class="p">)</span>
    <span class="n">se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">p_hat</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_hat</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">p_hat</span><span class="p">,</span> <span class="n">se</span>

<span class="n">t</span> <span class="o">=</span> <span class="mf">1.96</span>
<span class="n">est</span><span class="p">,</span> <span class="n">se</span> <span class="o">=</span> <span class="n">estimate_normal_cdf</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="mi">1_000_000</span><span class="p">)</span>
<span class="n">true_val</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Φ(</span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s2">) estimate: </span><span class="si">{</span><span class="n">est</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">se</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Φ(</span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s2">) true:     </span><span class="si">{</span><span class="n">true_val</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Φ(1.96) estimate: 0.974928 ± 0.000156
Φ(1.96) true:     0.975002
</pre></div>
</div>
<p>With one million samples, the estimate is accurate to about four decimal places. For extreme quantiles (e.g., <span class="math notranslate nohighlight">\(t = 5\)</span>), the probability is so small that accurate estimation requires importance sampling.</p>
</section>
</section>
<section id="comparison-with-deterministic-methods">
<h2>Comparison with Deterministic Methods<a class="headerlink" href="#comparison-with-deterministic-methods" title="Link to this heading"></a></h2>
<p>Monte Carlo integration is not the only way to compute integrals numerically. Deterministic quadrature methods—the trapezoidal rule, Simpson’s rule, Gaussian quadrature—have been studied for centuries and, in low dimensions, often outperform Monte Carlo. Understanding when to use which approach is essential for the computational practitioner.</p>
<section id="one-dimensional-quadrature">
<h3>One-Dimensional Quadrature<a class="headerlink" href="#one-dimensional-quadrature" title="Link to this heading"></a></h3>
<p>For a one-dimensional integral <span class="math notranslate nohighlight">\(\int_a^b f(x) dx\)</span>, deterministic methods exploit the smoothness of <span class="math notranslate nohighlight">\(f\)</span> to achieve rapid convergence. The core idea is to approximate the integral as a weighted sum of function values at selected points:</p>
<div class="math notranslate nohighlight">
\[\int_a^b f(x) dx \approx \sum_{i=1}^{n} w_i f(x_i)\]</div>
<p>The methods differ in how they choose the points <span class="math notranslate nohighlight">\(x_i\)</span> and weights <span class="math notranslate nohighlight">\(w_i\)</span>.</p>
<p><strong>Trapezoidal Rule</strong>: Approximate the integrand by piecewise linear functions connecting adjacent points. For <span class="math notranslate nohighlight">\(n\)</span> equally spaced points <span class="math notranslate nohighlight">\(x_0, x_1, \ldots, x_{n-1}\)</span> with spacing <span class="math notranslate nohighlight">\(h = (b-a)/(n-1)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\int_a^b f(x) dx \approx h \left[ \frac{f(x_0)}{2} + f(x_1) + f(x_2) + \cdots + f(x_{n-2}) + \frac{f(x_{n-1})}{2} \right]\]</div>
<p>Geometrically, this approximates the area under the curve by a series of trapezoids. The error is <span class="math notranslate nohighlight">\(O(h^2) = O(n^{-2})\)</span> for twice-differentiable <span class="math notranslate nohighlight">\(f\)</span>—doubling the number of points reduces the error by a factor of 4.</p>
<p><strong>Simpson’s Rule</strong>: Approximate by piecewise quadratic (parabolic) curves through consecutive triples of points. For an odd number of equally spaced points:</p>
<div class="math notranslate nohighlight">
\[\int_a^b f(x) dx \approx \frac{h}{3} \left[ f(x_0) + 4f(x_1) + 2f(x_2) + 4f(x_3) + \cdots + 4f(x_{n-2}) + f(x_{n-1}) \right]\]</div>
<p>The alternating pattern of coefficients (1, 4, 2, 4, 2, …, 4, 1) arises from fitting parabolas through each group of three points. The error is <span class="math notranslate nohighlight">\(O(h^4) = O(n^{-4})\)</span> for sufficiently smooth <span class="math notranslate nohighlight">\(f\)</span>—doubling the points reduces error by a factor of 16.</p>
<p><strong>Gaussian Quadrature</strong>: Rather than using equally spaced points, choose both points <span class="math notranslate nohighlight">\(x_i\)</span> and weights <span class="math notranslate nohighlight">\(w_i\)</span> to maximize accuracy. With <span class="math notranslate nohighlight">\(n\)</span> optimally chosen points, Gaussian quadrature integrates polynomials of degree up to <span class="math notranslate nohighlight">\(2n-1\)</span> <em>exactly</em>. For analytic functions, convergence can be exponentially fast—far better than any fixed polynomial rate.</p>
<p>The optimal points turn out to be roots of orthogonal polynomials (Legendre polynomials for integration on <span class="math notranslate nohighlight">\([-1, 1]\)</span>). SciPy’s <code class="docutils literal notranslate"><span class="pre">scipy.integrate.quad</span></code> uses adaptive Gaussian quadrature internally.</p>
<p>The geometric difference between these approaches is illuminating:</p>
<figure class="align-center" id="id7">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_1_fig05_sampling_strategies.png"><img alt="Six-panel comparison of grid-based quadrature versus Monte Carlo sampling in 1D and 2D" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_1_fig05_sampling_strategies.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 35 </span><span class="caption-text"><strong>Grid vs. Monte Carlo Sampling.</strong> Top row: In 1D, grid sampling (left) places evaluation points at regular intervals, while Monte Carlo (middle) uses random points. Quadrature wins decisively in 1D (right)—this is expected and correct. Bottom row: In 2D, a 10×10 grid uses 100 points in a regular lattice (left), while Monte Carlo distributes 100 points without structure (middle). The crucial insight (right): grid cost grows as <span class="math notranslate nohighlight">\(m^d\)</span> while Monte Carlo is dimension-independent; the crossover occurs around <span class="math notranslate nohighlight">\(d \approx 3\text{--}5\)</span>, after which Monte Carlo wins.</span><a class="headerlink" href="#id7" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Compare these to Monte Carlo’s <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span>. In one dimension, Monte Carlo loses badly:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">integrate</span>

<span class="c1"># Integrand: exp(-x²) on [0, 2]</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># True value (via error function)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.special</span><span class="w"> </span><span class="kn">import</span> <span class="n">erf</span>
<span class="n">true_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">erf</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Monte Carlo</span>
<span class="k">def</span><span class="w"> </span><span class="nf">mc_estimate</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># Multiply by interval length</span>

<span class="c1"># Trapezoidal rule</span>
<span class="k">def</span><span class="w"> </span><span class="nf">trap_estimate</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">trapz</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># Simpson&#39;s rule</span>
<span class="k">def</span><span class="w"> </span><span class="nf">simp_estimate</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">integrate</span><span class="o">.</span><span class="n">simpson</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True value: </span><span class="si">{</span><span class="n">true_value</span><span class="si">:</span><span class="s2">.10f</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]:</span>
    <span class="n">mc</span> <span class="o">=</span> <span class="n">mc_estimate</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">tr</span> <span class="o">=</span> <span class="n">trap_estimate</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">si</span> <span class="o">=</span> <span class="n">simp_estimate</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Monte Carlo: </span><span class="si">{</span><span class="n">mc</span><span class="si">:</span><span class="s2">.10f</span><span class="si">}</span><span class="s2">  (error </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">mc</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">true_value</span><span class="p">)</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Trapezoidal: </span><span class="si">{</span><span class="n">tr</span><span class="si">:</span><span class="s2">.10f</span><span class="si">}</span><span class="s2">  (error </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">tr</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">true_value</span><span class="p">)</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Simpson:     </span><span class="si">{</span><span class="n">si</span><span class="si">:</span><span class="s2">.10f</span><span class="si">}</span><span class="s2">  (error </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">si</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">true_value</span><span class="p">)</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>True value: 0.8820813908

n = 10
  Monte Carlo: 0.6600529708  (error 2.22e-01)
  Trapezoidal: 0.8817823811  (error 2.99e-04)
  Simpson:     0.8819697523  (error 1.12e-04)

n = 100
  Monte Carlo: 0.8973426558  (error 1.53e-02)
  Trapezoidal: 0.8820788993  (error 2.49e-06)
  Simpson:     0.8820813848  (error 5.96e-09)

n = 1000
  Monte Carlo: 0.8906166325  (error 8.54e-03)
  Trapezoidal: 0.8820813663  (error 2.45e-08)
  Simpson:     0.8820813908  (error 5.58e-13)
</pre></div>
</div>
<p>With <span class="math notranslate nohighlight">\(n = 1000\)</span> points, Simpson’s rule achieves machine precision (<span class="math notranslate nohighlight">\(10^{-13}\)</span>) while Monte Carlo’s error is still around <span class="math notranslate nohighlight">\(10^{-2}\)</span>. In one dimension, there is no contest.</p>
</section>
<section id="the-curse-of-dimensionality">
<h3>The Curse of Dimensionality<a class="headerlink" href="#the-curse-of-dimensionality" title="Link to this heading"></a></h3>
<p>The situation reverses dramatically in high dimensions. Consider integrating over <span class="math notranslate nohighlight">\([0, 1]^d\)</span>. A deterministic method using a grid with <span class="math notranslate nohighlight">\(m\)</span> points per dimension requires <span class="math notranslate nohighlight">\(m^d\)</span> total evaluations.</p>
<p>To build geometric intuition for why high dimensions are so challenging, consider a simple question: what fraction of a hypercube’s volume lies within the inscribed hypersphere?</p>
<p><strong>Volume formulas.</strong> Consider the hypercube <span class="math notranslate nohighlight">\([-1, 1]^d\)</span> with side length 2 and the unit hypersphere <span class="math notranslate nohighlight">\(\{x : \|x\| \leq 1\}\)</span> inscribed within it.</p>
<p>The <strong>hypercube volume</strong> is simply:</p>
<div class="math notranslate nohighlight">
\[V_{\text{cube}}(d) = 2^d\]</div>
<p>The <strong>hypersphere volume</strong> in <span class="math notranslate nohighlight">\(d\)</span> dimensions is:</p>
<div class="math notranslate nohighlight">
\[V_{\text{sphere}}(d) = \frac{\pi^{d/2}}{\Gamma(d/2 + 1)}\]</div>
<p>where <span class="math notranslate nohighlight">\(\Gamma\)</span> is the gamma function. For positive integers, <span class="math notranslate nohighlight">\(\Gamma(n+1) = n!\)</span>. For half-integers, <span class="math notranslate nohighlight">\(\Gamma(1/2) = \sqrt{\pi}\)</span> and <span class="math notranslate nohighlight">\(\Gamma(n + 1/2) = \frac{(2n-1)!!}{2^n}\sqrt{\pi}\)</span> where <span class="math notranslate nohighlight">\(!!\)</span> denotes the double factorial. For practical computation, use <code class="docutils literal notranslate"><span class="pre">scipy.special.gamma</span></code>.</p>
<p>This formula gives familiar results: <span class="math notranslate nohighlight">\(V_2 = \pi\)</span> (area of unit circle), <span class="math notranslate nohighlight">\(V_3 = \frac{4\pi}{3}\)</span> (volume of unit sphere). For higher dimensions: <span class="math notranslate nohighlight">\(V_4 = \frac{\pi^2}{2}\)</span>, <span class="math notranslate nohighlight">\(V_5 = \frac{8\pi^2}{15}\)</span>, and so on.</p>
<p>The <strong>ratio</strong> of sphere volume to cube volume is:</p>
<div class="math notranslate nohighlight">
\[\frac{V_{\text{sphere}}(d)}{V_{\text{cube}}(d)} = \frac{\pi^{d/2}}{2^d \, \Gamma(d/2 + 1)}\]</div>
<p>Let’s compute this ratio for several dimensions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.special</span><span class="w"> </span><span class="kn">import</span> <span class="n">gamma</span>

<span class="k">def</span><span class="w"> </span><span class="nf">hypersphere_volume</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Volume of d-dimensional hypersphere with radius r.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">**</span> <span class="p">(</span><span class="n">d</span><span class="o">/</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">gamma</span><span class="p">(</span><span class="n">d</span><span class="o">/</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">r</span><span class="o">**</span><span class="n">d</span>

<span class="k">def</span><span class="w"> </span><span class="nf">hypercube_volume</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">side</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Volume of d-dimensional hypercube with given side length.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">side</span> <span class="o">**</span> <span class="n">d</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;d&#39;</span><span class="si">:</span><span class="s2">&gt;3</span><span class="si">}</span><span class="s2">  </span><span class="si">{</span><span class="s1">&#39;Cube&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">  </span><span class="si">{</span><span class="s1">&#39;Sphere&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">  </span><span class="si">{</span><span class="s1">&#39;Ratio&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">  </span><span class="si">{</span><span class="s1">&#39;</span><span class="si">% i</span><span class="s1">n sphere&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">58</span><span class="p">)</span>

<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">]:</span>
    <span class="n">cube_vol</span> <span class="o">=</span> <span class="n">hypercube_volume</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
    <span class="n">sphere_vol</span> <span class="o">=</span> <span class="n">hypersphere_volume</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
    <span class="n">ratio</span> <span class="o">=</span> <span class="n">sphere_vol</span> <span class="o">/</span> <span class="n">cube_vol</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">d</span><span class="si">:</span><span class="s2">&gt;3</span><span class="si">}</span><span class="s2">  </span><span class="si">{</span><span class="n">cube_vol</span><span class="si">:</span><span class="s2">&gt;12.2e</span><span class="si">}</span><span class="s2">  </span><span class="si">{</span><span class="n">sphere_vol</span><span class="si">:</span><span class="s2">&gt;12.4e</span><span class="si">}</span><span class="s2">  </span><span class="si">{</span><span class="n">ratio</span><span class="si">:</span><span class="s2">&gt;12.4e</span><span class="si">}</span><span class="s2">  </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">ratio</span><span class="si">:</span><span class="s2">&gt;11.2e</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>  d          Cube        Sphere         Ratio   % in sphere
----------------------------------------------------------
  1      2.00e+00    2.0000e+00    1.0000e+00     1.00e+02%
  2      4.00e+00    3.1416e+00    7.8540e-01     7.85e+01%
  3      8.00e+00    4.1888e+00    5.2360e-01     5.24e+01%
  5      3.20e+01    5.2638e+00    1.6449e-01     1.64e+01%
 10      1.02e+03    2.5502e+00    2.4904e-03     2.49e-01%
 20      1.05e+06    2.5807e-02    2.4611e-08     2.46e-06%
 50      1.13e+15    1.7302e-13    1.5367e-28     1.54e-26%
100      1.27e+30    2.3682e-40    1.8682e-70     1.87e-68%
</pre></div>
</div>
<p>The numbers are striking: in 10 dimensions, only 0.25% of the cube lies in the sphere. By 20 dimensions, it’s about <span class="math notranslate nohighlight">\(2.5 \times 10^{-6}`%. By 100 dimensions, the ratio is :math:`10^{-70}\)</span>—essentially zero.</p>
<p><strong>What does this mean for integration?</strong> If you’re trying to integrate a function that’s concentrated near the origin (like a multivariate Gaussian), random uniform samples over the hypercube will almost <em>never</em> land where the function is large. This is why importance sampling becomes essential in high dimensions.</p>
<figure class="align-center" id="id8">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_1_fig04_curse_of_dimensionality.png"><img alt="Three-panel visualization of the curse of dimensionality showing volume ratio decay, shell concentration, and grid point explosion" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_1_fig04_curse_of_dimensionality.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 36 </span><span class="caption-text"><strong>The Curse of Dimensionality.</strong> Left: The inscribed hypersphere occupies a vanishing fraction of the hypercube as dimension increases—by <span class="math notranslate nohighlight">\(d = 20\)</span>, less than <span class="math notranslate nohighlight">\(10^{-7}\)</span> of the cube’s volume lies within the sphere. Middle: In high dimensions, nearly all the volume of a ball concentrates in a thin shell near its surface. Right: Grid points required for deterministic methods explode exponentially—with just 10 points per dimension, a 20-dimensional integral requires <span class="math notranslate nohighlight">\(10^{20}\)</span> evaluations.</span><a class="headerlink" href="#id8" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>For Simpson’s rule with error <span class="math notranslate nohighlight">\(O(h^4) = O(m^{-4})\)</span>, the total error is <span class="math notranslate nohighlight">\(O(m^{-4})\)</span> but the cost is <span class="math notranslate nohighlight">\(m^d\)</span>. If we want error <span class="math notranslate nohighlight">\(\epsilon\)</span>, we need <span class="math notranslate nohighlight">\(m \sim \epsilon^{-1/4}\)</span>, giving cost <span class="math notranslate nohighlight">\(\sim \epsilon^{-d/4}\)</span>.</p>
<p>For Monte Carlo with error <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span>, achieving error <span class="math notranslate nohighlight">\(\epsilon\)</span> requires <span class="math notranslate nohighlight">\(n \sim \epsilon^{-2}\)</span> samples, independent of <span class="math notranslate nohighlight">\(d\)</span>.</p>
<p>The crossover occurs roughly when <span class="math notranslate nohighlight">\(\epsilon^{-d/4} = \epsilon^{-2}\)</span>, i.e., <span class="math notranslate nohighlight">\(d = 8\)</span>. For <span class="math notranslate nohighlight">\(d &gt; 8\)</span>, Monte Carlo requires fewer function evaluations than Simpson’s rule to achieve the same accuracy—and the advantage grows exponentially with dimension.</p>
<p><strong>Caveat</strong>: This analysis ignores constants, which can favor either method in specific cases. For particular integrands with special structure, deterministic methods may retain advantages even in moderate dimensions. The fundamental message, however, is robust: <strong>in high dimensions, Monte Carlo wins</strong> because its rate doesn’t degrade with dimension, while grid methods’ costs explode exponentially.</p>
<table class="docutils align-default" id="id9">
<caption><span class="caption-number">Table 15 </span><span class="caption-text">Comparison of Integration Methods</span><a class="headerlink" href="#id9" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 25.0%" />
<col style="width: 25.0%" />
<col style="width: 25.0%" />
<col style="width: 25.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>1D Convergence</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(d\)</span>-D Convergence</p></th>
<th class="head"><p>Best For</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Monte Carlo</p></td>
<td><p><span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span></p></td>
<td><p>High dimensions, complex domains</p></td>
</tr>
<tr class="row-odd"><td><p>Trapezoidal</p></td>
<td><p><span class="math notranslate nohighlight">\(O(n^{-2})\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(n^{-2/d})\)</span></p></td>
<td><p>Low-dim smooth functions</p></td>
</tr>
<tr class="row-even"><td><p>Simpson</p></td>
<td><p><span class="math notranslate nohighlight">\(O(n^{-4})\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(n^{-4/d})\)</span></p></td>
<td><p>Low-dim very smooth functions</p></td>
</tr>
<tr class="row-odd"><td><p>Gaussian</p></td>
<td><p><span class="math notranslate nohighlight">\(O(e^{-cn})\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(e^{-cn^{1/d}})\)</span></p></td>
<td><p>Low-dim analytic functions</p></td>
</tr>
<tr class="row-even"><td><p>Quasi-Monte Carlo</p></td>
<td><p><span class="math notranslate nohighlight">\(O(n^{-1}(\log n)^d)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(O(n^{-1}(\log n)^d)\)</span></p></td>
<td><p>Moderate dimensions, smooth functions</p></td>
</tr>
</tbody>
</table>
</section>
<section id="quasi-monte-carlo-methods">
<h3>Quasi-Monte Carlo Methods<a class="headerlink" href="#quasi-monte-carlo-methods" title="Link to this heading"></a></h3>
<p>A middle ground between deterministic quadrature and Monte Carlo is provided by <strong>quasi-Monte Carlo</strong> (QMC) methods. Instead of random samples, QMC uses carefully constructed <strong>low-discrepancy sequences</strong>—deterministic sequences that fill space more uniformly than random points.</p>
<p>Famous examples include Halton sequences, Sobol sequences, and lattice rules. Under smoothness conditions on the integrand, QMC achieves convergence rates of <span class="math notranslate nohighlight">\(O(n^{-1} (\log n)^d)\)</span>, faster than Monte Carlo’s <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> but with a dependence on dimension.</p>
<p>QMC is increasingly popular in computational finance and computer graphics. However, it requires more care: the sequences must match the problem structure, variance estimation is trickier, and the smoothness assumptions may fail. For general-purpose integration, especially with non-smooth or high-variance integrands, standard Monte Carlo remains the most robust choice.</p>
</section>
</section>
<section id="sample-size-determination">
<h2>Sample Size Determination<a class="headerlink" href="#sample-size-determination" title="Link to this heading"></a></h2>
<p>A critical practical question is: <strong>how many samples do I need?</strong> The answer depends on the desired precision, the variance of the integrand, and the acceptable probability of error.</p>
<section id="the-sample-size-formula">
<h3>The Sample Size Formula<a class="headerlink" href="#the-sample-size-formula" title="Link to this heading"></a></h3>
<p>From the CLT, the Monte Carlo estimator is approximately:</p>
<div class="math notranslate nohighlight">
\[\hat{I}_n \sim \mathcal{N}\left( I, \frac{\sigma^2}{n} \right)\]</div>
<p><strong>For a target standard error</strong> <span class="math notranslate nohighlight">\(\epsilon\)</span>, we need:</p>
<div class="math notranslate nohighlight">
\[\frac{\sigma}{\sqrt{n}} \leq \epsilon \implies n \geq \frac{\sigma^2}{\epsilon^2}\]</div>
<p><strong>For a 95% CI half-width</strong> of <span class="math notranslate nohighlight">\(\epsilon\)</span>, we need:</p>
<div class="math notranslate nohighlight">
\[1.96 \cdot \frac{\sigma}{\sqrt{n}} \leq \epsilon \implies n \geq \left( \frac{1.96 \cdot \sigma}{\epsilon} \right)^2 \approx \frac{3.84\sigma^2}{\epsilon^2}\]</div>
<p>The factor of 3.84 (approximately <span class="math notranslate nohighlight">\(1.96^2\)</span>) accounts for the confidence level. This is about 4× more samples than the SE target requires.</p>
<p><strong>Be explicit about which you’re targeting</strong>: “SE ≤ 0.01” requires <span class="math notranslate nohighlight">\(\sigma^2/0.01^2\)</span> samples; “95% CI half-width ≤ 0.01” requires <span class="math notranslate nohighlight">\(3.84\sigma^2/0.01^2\)</span> samples.</p>
</section>
<section id="practical-sample-size-determination">
<h3>Practical Sample Size Determination<a class="headerlink" href="#practical-sample-size-determination" title="Link to this heading"></a></h3>
<p>In practice, <span class="math notranslate nohighlight">\(\sigma\)</span> is unknown. A common approach is:</p>
<ol class="arabic simple">
<li><p><strong>Pilot study</strong>: Run a small simulation (e.g., 1,000 samples) to estimate <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span>.</p></li>
<li><p><strong>Compute required sample size</strong>: Use the appropriate formula for your target.</p></li>
<li><p><strong>Run full simulation</strong>: Generate <span class="math notranslate nohighlight">\(n\)</span> samples (possibly in addition to the pilot).</p></li>
<li><p><strong>Verify</strong>: Check that the final standard error meets requirements.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">determine_sample_size_for_se</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">sampler</span><span class="p">,</span> <span class="n">target_se</span><span class="p">,</span> <span class="n">pilot_n</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determine required sample size for a target STANDARD ERROR.</span>

<span class="sd">    Uses formula: n = (σ / target_se)²</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    h : callable</span>
<span class="sd">        Function to integrate.</span>
<span class="sd">    sampler : callable</span>
<span class="sd">        Function that takes (n, rng) and returns samples.</span>
<span class="sd">    target_se : float</span>
<span class="sd">        Desired standard error (not CI half-width).</span>
<span class="sd">    pilot_n : int</span>
<span class="sd">        Pilot sample size for variance estimation.</span>
<span class="sd">    seed : int</span>
<span class="sd">        Random seed.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        Contains required_n, estimated_variance, pilot results.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Pilot study</span>
    <span class="n">pilot_samples</span> <span class="o">=</span> <span class="n">sampler</span><span class="p">(</span><span class="n">pilot_n</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
    <span class="n">pilot_h</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">pilot_samples</span><span class="p">)</span>
    <span class="n">sigma_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">pilot_h</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Required sample size for SE target</span>
    <span class="n">required_n</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">sigma_hat</span> <span class="o">/</span> <span class="n">target_se</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;required_n&#39;</span><span class="p">:</span> <span class="n">required_n</span><span class="p">,</span>
        <span class="s1">&#39;estimated_sigma&#39;</span><span class="p">:</span> <span class="n">sigma_hat</span><span class="p">,</span>
        <span class="s1">&#39;pilot_n&#39;</span><span class="p">:</span> <span class="n">pilot_n</span><span class="p">,</span>
        <span class="s1">&#39;pilot_estimate&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">pilot_h</span><span class="p">),</span>
        <span class="s1">&#39;pilot_se&#39;</span><span class="p">:</span> <span class="n">sigma_hat</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">pilot_n</span><span class="p">),</span>
        <span class="s1">&#39;target_type&#39;</span><span class="p">:</span> <span class="s1">&#39;standard_error&#39;</span>
    <span class="p">}</span>

<span class="k">def</span><span class="w"> </span><span class="nf">determine_sample_size_for_ci</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">sampler</span><span class="p">,</span> <span class="n">target_halfwidth</span><span class="p">,</span>
                                  <span class="n">confidence</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">pilot_n</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determine required sample size for a target CI HALF-WIDTH.</span>

<span class="sd">    Uses formula: n = (z * σ / target_halfwidth)²</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    target_halfwidth : float</span>
<span class="sd">        Desired confidence interval half-width.</span>
<span class="sd">    confidence : float</span>
<span class="sd">        Confidence level (default 0.95).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Pilot study</span>
    <span class="n">pilot_samples</span> <span class="o">=</span> <span class="n">sampler</span><span class="p">(</span><span class="n">pilot_n</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
    <span class="n">pilot_h</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">pilot_samples</span><span class="p">)</span>
    <span class="n">sigma_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">pilot_h</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Critical value for confidence level</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">confidence</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Required sample size for CI half-width target</span>
    <span class="n">required_n</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">z</span> <span class="o">*</span> <span class="n">sigma_hat</span> <span class="o">/</span> <span class="n">target_halfwidth</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;required_n&#39;</span><span class="p">:</span> <span class="n">required_n</span><span class="p">,</span>
        <span class="s1">&#39;estimated_sigma&#39;</span><span class="p">:</span> <span class="n">sigma_hat</span><span class="p">,</span>
        <span class="s1">&#39;z_value&#39;</span><span class="p">:</span> <span class="n">z</span><span class="p">,</span>
        <span class="s1">&#39;pilot_n&#39;</span><span class="p">:</span> <span class="n">pilot_n</span><span class="p">,</span>
        <span class="s1">&#39;target_type&#39;</span><span class="p">:</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">confidence</span><span class="si">:</span><span class="s1">.0f</span><span class="si">}</span><span class="s1">% CI half-width&#39;</span>
    <span class="p">}</span>

<span class="c1"># Example: Estimate E[X²] for X ~ N(0,1)</span>
<span class="c1"># True variance of X² is Var(X²) = E[X⁴] - (E[X²])² = 3 - 1 = 2, so σ = √2 ≈ 1.414</span>

<span class="n">result_se</span> <span class="o">=</span> <span class="n">determine_sample_size_for_se</span><span class="p">(</span>
    <span class="n">h</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">sampler</span><span class="o">=</span><span class="k">lambda</span> <span class="n">n</span><span class="p">,</span> <span class="n">rng</span><span class="p">:</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n</span><span class="p">),</span>
    <span class="n">target_se</span><span class="o">=</span><span class="mf">0.01</span>
<span class="p">)</span>

<span class="n">result_ci</span> <span class="o">=</span> <span class="n">determine_sample_size_for_ci</span><span class="p">(</span>
    <span class="n">h</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">sampler</span><span class="o">=</span><span class="k">lambda</span> <span class="n">n</span><span class="p">,</span> <span class="n">rng</span><span class="p">:</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n</span><span class="p">),</span>
    <span class="n">target_halfwidth</span><span class="o">=</span><span class="mf">0.01</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For E[X²] where X ~ N(0,1):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimated σ: </span><span class="si">{</span><span class="n">result_se</span><span class="p">[</span><span class="s1">&#39;estimated_sigma&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (true: √2 ≈ 1.414)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">For SE ≤ 0.01:           n ≥ </span><span class="si">{</span><span class="n">result_se</span><span class="p">[</span><span class="s1">&#39;required_n&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For 95% CI half-width ≤ 0.01: n ≥ </span><span class="si">{</span><span class="n">result_ci</span><span class="p">[</span><span class="s1">&#39;required_n&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Ratio: </span><span class="si">{</span><span class="n">result_ci</span><span class="p">[</span><span class="s1">&#39;required_n&#39;</span><span class="p">]</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">result_se</span><span class="p">[</span><span class="s1">&#39;required_n&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> (should be ≈ 1.96² ≈ 3.84)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>For E[X²] where X ~ N(0,1):
Estimated σ: 1.4153 (true: √2 ≈ 1.414)

For SE ≤ 0.01:           n ≥ 20,031
For 95% CI half-width ≤ 0.01: n ≥ 76,948

Ratio: 3.84 (should be ≈ 1.96² ≈ 3.84)
</pre></div>
</div>
<p>For estimating <span class="math notranslate nohighlight">\(\mathbb{E}[X^2]\)</span> where <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(0,1)\)</span>, the true variance is <span class="math notranslate nohighlight">\(\text{Var}(X^2) = \mathbb{E}[X^4] - (\mathbb{E}[X^2])^2 = 3 - 1 = 2\)</span>, so <span class="math notranslate nohighlight">\(\sigma = \sqrt{2} \approx 1.414\)</span>. The required sample sizes differ by a factor of 3.84 depending on whether you’re targeting SE or CI half-width.</p>
</section>
</section>
<section id="convergence-diagnostics-and-monitoring">
<h2>Convergence Diagnostics and Monitoring<a class="headerlink" href="#convergence-diagnostics-and-monitoring" title="Link to this heading"></a></h2>
<p>Beyond computing point estimates and confidence intervals, it is important to monitor the convergence of Monte Carlo simulations. Visual diagnostics can reveal problems—heavy tails, multimodality, slow mixing—that summary statistics might miss.</p>
<section id="running-mean-plots">
<h3>Running Mean Plots<a class="headerlink" href="#running-mean-plots" title="Link to this heading"></a></h3>
<p>The most basic diagnostic is a plot of the running mean:</p>
<div class="math notranslate nohighlight">
\[\hat{I}_k = \frac{1}{k} \sum_{i=1}^{k} h(X_i) \quad \text{for } k = 1, 2, \ldots, n\]</div>
<p>If the simulation is converging properly, this plot should:</p>
<ol class="arabic simple">
<li><p>Fluctuate widely for small <span class="math notranslate nohighlight">\(k\)</span></p></li>
<li><p>Stabilize and approach a horizontal asymptote as <span class="math notranslate nohighlight">\(k\)</span> grows</p></li>
<li><p>Have diminishing fluctuations proportional to <span class="math notranslate nohighlight">\(1/\sqrt{k}\)</span></p></li>
</ol>
</section>
<section id="standard-error-decay">
<h3>Standard Error Decay<a class="headerlink" href="#standard-error-decay" title="Link to this heading"></a></h3>
<p>The standard error should decrease as <span class="math notranslate nohighlight">\(1/\sqrt{n}\)</span>. A log-log plot of standard error versus sample size should have slope <span class="math notranslate nohighlight">\(-1/2\)</span>. Deviations suggest:</p>
<ul class="simple">
<li><p><strong>Steeper slope</strong>: Variance is decreasing (possibly a problem with the estimator)</p></li>
<li><p><strong>Shallower slope</strong>: Correlation in samples, infinite variance, or other issues</p></li>
</ul>
<p>The following figure demonstrates these diagnostics for a simple example: estimating <span class="math notranslate nohighlight">\(\mathbb{E}[X^2] = 1\)</span> where <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(0, 1)\)</span>.</p>
<figure class="align-center" id="id10">
<a class="reference internal image-reference" href="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_1_fig07_convergence_diagnostics.png"><img alt="Four-panel convergence diagnostics showing running mean, standard error decay, h-value distribution, and autocorrelation" src="https://pqyjaywwccbnqpwgeiuv.supabase.co/storage/v1/object/public/STAT%20418%20Images/assets/PartII/Chapter2/ch2_1_fig07_convergence_diagnostics.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 37 </span><span class="caption-text"><strong>Convergence Diagnostics for Monte Carlo.</strong> Top-left: Running mean with 95% confidence band—note how the estimate stabilizes and the band shrinks as samples accumulate. Top-right: Standard error decay on log-log axes—the observed SE (blue) closely tracks the theoretical <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> rate (red dashed). Bottom-left: Distribution of <span class="math notranslate nohighlight">\(h(X) = X^2\)</span> values, which follows a <span class="math notranslate nohighlight">\(\chi^2(1)\)</span> distribution (orange curve). Bottom-right: Autocorrelation at various lags—all values fall within the significance bounds (red dashed), confirming that samples are independent.</span><a class="headerlink" href="#id10" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>What to look for:</strong></p>
<ul class="simple">
<li><p><strong>Running mean</strong>: Should stabilize (not drift or oscillate)</p></li>
<li><p><strong>SE decay</strong>: Should follow <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> line; shallower slope indicates problems</p></li>
<li><p><strong>Distribution</strong>: Check for heavy tails or multimodality that might inflate variance</p></li>
<li><p><strong>Autocorrelation</strong>: For iid samples, all bars should be near zero; significant autocorrelation indicates dependence (common in MCMC)</p></li>
</ul>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ⚠️ Pathological Cases</p>
<p>Several situations can cause Monte Carlo to behave unexpectedly. Recognizing these pathologies—and knowing how to address them—is essential for reliable simulation.</p>
<p><strong>Heavy tails</strong> (infinite variance)</p>
<p><em>Problem</em>: If <span class="math notranslate nohighlight">\(\text{Var}[h(X)] = \infty\)</span>, the CLT does not apply. The estimator still converges by LLN (if <span class="math notranslate nohighlight">\(\mathbb{E}[|h(X)|] &lt; \infty\)</span>), but standard error estimates are meaningless and confidence intervals are invalid.</p>
<p><em>Example</em>: Consider <span class="math notranslate nohighlight">\(h(U) = U^{-\alpha}\)</span> where <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0,1)\)</span>. The mean <span class="math notranslate nohighlight">\(\mathbb{E}[h(U)]\)</span> exists iff <span class="math notranslate nohighlight">\(\alpha &lt; 1\)</span>. The variance exists iff <span class="math notranslate nohighlight">\(2\alpha &lt; 1\)</span>, i.e., <span class="math notranslate nohighlight">\(\alpha &lt; 0.5\)</span>. So for <span class="math notranslate nohighlight">\(0.5 \leq \alpha &lt; 1\)</span> (e.g., <span class="math notranslate nohighlight">\(\alpha = 0.6\)</span>), the mean is finite but the variance is infinite.</p>
<p><em>Remedies</em>:</p>
<ul class="simple">
<li><p><strong>Truncation</strong>: Replace <span class="math notranslate nohighlight">\(h(x)\)</span> with <span class="math notranslate nohighlight">\(\min(h(x), M)\)</span> for some threshold <span class="math notranslate nohighlight">\(M\)</span>; introduces bias but restores finite variance</p></li>
<li><p><strong>Transformation</strong>: If <span class="math notranslate nohighlight">\(h(x)\)</span> blows up near a boundary, change variables to smooth the integrand</p></li>
<li><p><strong>Importance sampling</strong>: Sample more heavily where <span class="math notranslate nohighlight">\(h(x)\)</span> is large, reducing variance</p></li>
<li><p><strong>Diagnostics</strong>: Plot the histogram of <span class="math notranslate nohighlight">\(h(X_i)\)</span> values; extreme outliers suggest heavy tails</p></li>
</ul>
<p><strong>Multimodality</strong> (isolated peaks)</p>
<p><em>Problem</em>: If the integrand has isolated peaks that the sampling distribution rarely visits, estimates may be severely biased. You might run millions of samples without ever hitting a significant mode.</p>
<p><em>Example</em>: A mixture of two narrow Gaussians separated by 100 standard deviations; uniform sampling over the domain almost never hits either peak.</p>
<p><em>Remedies</em>:</p>
<ul class="simple">
<li><p><strong>Importance sampling</strong>: Design a proposal distribution that covers all modes</p></li>
<li><p><strong>Stratified sampling</strong>: Divide the domain into regions and sample each region separately</p></li>
<li><p><strong>Adaptive methods</strong>: Use preliminary runs to identify modes, then concentrate sampling there</p></li>
<li><p><strong>MCMC with tempering</strong>: Parallel tempering or simulated tempering can help samplers jump between modes (covered in Part 3)</p></li>
</ul>
<p><strong>Rare events</strong> (small probabilities)</p>
<p><em>Problem</em>: Estimating <span class="math notranslate nohighlight">\(P(A)\)</span> for rare events requires approximately <span class="math notranslate nohighlight">\(100/P(A)\)</span> samples for 10% relative error. For <span class="math notranslate nohighlight">\(P(A) = 10^{-6}\)</span>, that’s <span class="math notranslate nohighlight">\(10^8\)</span> samples.</p>
<p><em>Example</em>: Probability that a complex system fails; probability of extreme portfolio losses.</p>
<p><em>Remedies</em>:</p>
<ul class="simple">
<li><p><strong>Importance sampling</strong>: Sample preferentially from the rare event region; this is the standard solution</p></li>
<li><p><strong>Splitting/cloning</strong>: In sequential simulations, replicate trajectories that approach the rare event</p></li>
<li><p><strong>Cross-entropy method</strong>: Iteratively adapt the sampling distribution to concentrate on rare events</p></li>
<li><p><strong>Large deviations theory</strong>: Use asymptotic approximations when sampling is infeasible</p></li>
</ul>
<p><strong>Dependent samples</strong> (autocorrelation)</p>
<p><em>Problem</em>: If samples are correlated (as in MCMC), the effective sample size (ESS) is smaller than the nominal sample size <span class="math notranslate nohighlight">\(n\)</span>. Standard error formulas that assume independence underestimate uncertainty.</p>
<p><em>Example</em>: Metropolis-Hastings chains with high autocorrelation; ESS might be <span class="math notranslate nohighlight">\(n/100\)</span> or worse.</p>
<p><em>Remedies</em>:</p>
<ul class="simple">
<li><p><strong>Thinning</strong>: Keep every <span class="math notranslate nohighlight">\(k\)</span>-th sample to reduce autocorrelation (simple but wasteful)</p></li>
<li><p><strong>Effective sample size</strong>: Compute ESS and report uncertainty based on ESS, not <span class="math notranslate nohighlight">\(n\)</span></p></li>
<li><p><strong>Batch means</strong>: Divide the chain into batches and estimate variance from batch means</p></li>
<li><p><strong>Better samplers</strong>: Hamiltonian Monte Carlo, NUTS, or other advanced MCMC methods reduce autocorrelation</p></li>
<li><p><strong>Diagnostics</strong>: Always check autocorrelation plots; values should decay quickly to zero</p></li>
</ul>
</div>
</section>
</section>
<section id="practical-considerations">
<h2>Practical Considerations<a class="headerlink" href="#practical-considerations" title="Link to this heading"></a></h2>
<p>Before concluding, we collect several practical points for implementing Monte Carlo methods effectively.</p>
<section id="when-to-use-monte-carlo">
<h3>When to Use Monte Carlo<a class="headerlink" href="#when-to-use-monte-carlo" title="Link to this heading"></a></h3>
<p>Monte Carlo integration is the method of choice when:</p>
<ol class="arabic simple">
<li><p><strong>Dimension is high</strong> (<span class="math notranslate nohighlight">\(d \gtrsim 5\)</span>): The curse of dimensionality kills deterministic methods.</p></li>
<li><p><strong>Domain is complex</strong>: Irregular regions, constraints, and complex boundaries are natural for Monte Carlo but problematic for quadrature.</p></li>
<li><p><strong>Integrand is non-smooth</strong>: Monte Carlo doesn’t require derivatives or smoothness.</p></li>
<li><p><strong>Sampling is easy</strong>: If we can easily generate samples from the target distribution, Monte Carlo is straightforward to implement.</p></li>
</ol>
<p>Monte Carlo is a poor choice when:</p>
<ol class="arabic simple">
<li><p><strong>Dimension is very low</strong> (<span class="math notranslate nohighlight">\(d \leq 3\)</span>) and the integrand is smooth: Use Gaussian quadrature.</p></li>
<li><p><strong>High precision is required</strong> with smooth integrands: Deterministic methods converge faster.</p></li>
<li><p><strong>Sampling is expensive</strong>: Each Monte Carlo sample requires a new function evaluation; quadrature methods can achieve more with fewer evaluations for smooth functions.</p></li>
</ol>
</section>
<section id="reproducibility">
<h3>Reproducibility<a class="headerlink" href="#reproducibility" title="Link to this heading"></a></h3>
<p>Always set random seeds and document them. Monte Carlo results should be reproducible.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">my_monte_carlo_analysis</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Document the seed in the function signature.&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="c1"># ... analysis using rng ...</span>
    <span class="k">return</span> <span class="n">results</span>

<span class="c1"># Call with explicit seed</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">my_monte_carlo_analysis</span><span class="p">(</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="computational-efficiency">
<h3>Computational Efficiency<a class="headerlink" href="#computational-efficiency" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Vectorize</strong>: Use NumPy operations on arrays, not Python loops.</p></li>
<li><p><strong>Generate samples in batches</strong>: <code class="docutils literal notranslate"><span class="pre">rng.random(100_000)</span></code> is faster than 100,000 calls to <code class="docutils literal notranslate"><span class="pre">rng.random()</span></code>.</p></li>
<li><p><strong>Parallelize when possible</strong>: For embarrassingly parallel problems, distribute samples across cores.</p></li>
</ul>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ⚠️</p>
<p><strong>Reporting estimates without uncertainty</strong>: A Monte Carlo estimate without a standard error or confidence interval is nearly meaningless. Always report <span class="math notranslate nohighlight">\(\hat{I}_n \pm z_{\alpha/2} \cdot \hat{\sigma}_n / \sqrt{n}\)</span>.</p>
<p><strong>Bad</strong>: “The integral is 3.14159.”</p>
<p><strong>Good</strong>: “The integral is 3.142 ± 0.005 (95% CI: [3.132, 3.152]) based on 100,000 samples.”</p>
</div>
</section>
</section>
<section id="chapter-2-1-exercises-monte-carlo-fundamentals-mastery">
<h2>Chapter 2.1 Exercises: Monte Carlo Fundamentals Mastery<a class="headerlink" href="#chapter-2-1-exercises-monte-carlo-fundamentals-mastery" title="Link to this heading"></a></h2>
<p>These exercises progressively build your understanding of Monte Carlo integration, from basic estimation through convergence analysis and practical applications. Each exercise connects theory, implementation, and practical considerations.</p>
<div class="tip admonition">
<p class="admonition-title">A Note on These Exercises</p>
<p>These exercises are designed to deepen your understanding of Monte Carlo integration through hands-on implementation:</p>
<ul class="simple">
<li><p><strong>Exercises 1–2</strong> reinforce core concepts: expectation as integration, the <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> convergence rate, and variance estimation</p></li>
<li><p><strong>Exercises 3–4</strong> develop diagnostic skills and explore the curse of dimensionality</p></li>
<li><p><strong>Exercise 5</strong> compares Monte Carlo with deterministic quadrature to build intuition for method selection</p></li>
<li><p><strong>Exercise 6</strong> connects Monte Carlo to Bayesian inference, previewing applications in later chapters</p></li>
</ul>
<p>Complete solutions with code, output, and interpretation are provided. Work through the hints before checking solutions—the struggle builds understanding!</p>
</div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 1: The Error Function Integral</p>
<p>The <strong>error function</strong> <span class="math notranslate nohighlight">\(\text{erf}(x) = \frac{2}{\sqrt{\pi}}\int_0^x e^{-t^2} dt\)</span> appears throughout statistics, physics, and engineering. It has no closed-form antiderivative, making it a perfect candidate for Monte Carlo integration.</p>
<div class="note admonition">
<p class="admonition-title">Background: Why the Error Function Matters</p>
<p>The error function is intimately connected to the normal distribution: <span class="math notranslate nohighlight">\(\Phi(x) = \frac{1}{2}\left[1 + \text{erf}(x/\sqrt{2})\right]\)</span>. Evaluating <span class="math notranslate nohighlight">\(\text{erf}(1)\)</span> is equivalent to computing the probability that a standard normal random variable falls between <span class="math notranslate nohighlight">\(-\sqrt{2}\)</span> and <span class="math notranslate nohighlight">\(\sqrt{2}\)</span>.</p>
</div>
<ol class="loweralpha">
<li><p><strong>Analytical setup</strong>: Express the integral <span class="math notranslate nohighlight">\(I = \int_0^1 e^{-t^2} dt\)</span> as an expectation. What is the Monte Carlo estimator <span class="math notranslate nohighlight">\(\hat{I}_n\)</span>?</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Expectation Formulation</p>
<p>For any integral <span class="math notranslate nohighlight">\(\int_a^b h(t) dt\)</span> over a finite interval, you can write it as <span class="math notranslate nohighlight">\((b-a) \cdot \mathbb{E}[h(U)]\)</span> where <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(a, b)\)</span>. What is <span class="math notranslate nohighlight">\(h(t)\)</span> here?</p>
</div>
</li>
<li><p><strong>Implementation</strong>: Write a function <code class="docutils literal notranslate"><span class="pre">monte_carlo_erf(n_samples,</span> <span class="pre">seed)</span></code> that estimates <span class="math notranslate nohighlight">\(\int_0^1 e^{-t^2} dt\)</span> and returns both the estimate and its standard error.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Standard Error Computation</p>
<p>The standard error is <span class="math notranslate nohighlight">\(\hat{\sigma}/\sqrt{n}\)</span> where <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span> is the sample standard deviation of the <span class="math notranslate nohighlight">\(h(U_i)\)</span> values. Use <code class="docutils literal notranslate"><span class="pre">np.std(h_values,</span> <span class="pre">ddof=1)</span></code> for the unbiased estimator.</p>
</div>
</li>
<li><p><strong>Convergence verification</strong>: Run your estimator for <span class="math notranslate nohighlight">\(n \in \{100, 1000, 10000, 100000, 1000000\}\)</span>. Verify that the standard error decreases as <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> by computing the ratio of successive standard errors.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Testing the Rate</p>
<p>If SE scales as <span class="math notranslate nohighlight">\(n^{-1/2}\)</span>, then when <span class="math notranslate nohighlight">\(n\)</span> increases by a factor of 10, SE should decrease by a factor of <span class="math notranslate nohighlight">\(\sqrt{10} \approx 3.16\)</span>. Compute <code class="docutils literal notranslate"><span class="pre">se[i-1]</span> <span class="pre">/</span> <span class="pre">se[i]</span></code> for each consecutive pair.</p>
</div>
</li>
<li><p><strong>Confidence interval coverage</strong>: Repeat the <span class="math notranslate nohighlight">\(n = 10000\)</span> estimation 1000 times with different seeds. What fraction of the 95% confidence intervals contain the true value <span class="math notranslate nohighlight">\(\int_0^1 e^{-t^2} dt = \frac{\sqrt{\pi}}{2}\text{erf}(1) \approx 0.7468\)</span>?</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Coverage Simulation</p>
<p>The CLT guarantees that 95% CIs contain the true value approximately 95% of the time. Deviations indicate either insufficient sample size (CLT not yet valid) or implementation bugs.</p>
</div>
</li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Analytical Setup</strong></p>
<div class="tip admonition">
<p class="admonition-title">Step 1: Rewrite as Expectation</p>
<p class="sd-card-text">The integral <span class="math notranslate nohighlight">\(I = \int_0^1 e^{-t^2} dt\)</span> can be written as:</p>
<div class="math notranslate nohighlight">
\[I = (1 - 0) \cdot \mathbb{E}[e^{-U^2}] = \mathbb{E}[e^{-U^2}]\]</div>
<p class="sd-card-text">where <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0, 1)\)</span>.</p>
<p class="sd-card-text">The Monte Carlo estimator is:</p>
<div class="math notranslate nohighlight">
\[\hat{I}_n = \frac{1}{n}\sum_{i=1}^{n} e^{-U_i^2}\]</div>
<p class="sd-card-text">where <span class="math notranslate nohighlight">\(U_1, \ldots, U_n \stackrel{\text{iid}}{\sim} \text{Uniform}(0, 1)\)</span>.</p>
</div>
<p class="sd-card-text"><strong>Parts (b)–(d): Implementation and Verification</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.special</span><span class="w"> </span><span class="kn">import</span> <span class="n">erf</span>

<span class="k">def</span><span class="w"> </span><span class="nf">monte_carlo_erf</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Estimate ∫₀¹ exp(-t²) dt via Monte Carlo.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of Monte Carlo samples.</span>
<span class="sd">    seed : int, optional</span>
<span class="sd">        Random seed for reproducibility.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        Contains estimate, standard_error, and confidence interval.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Generate uniform samples on [0, 1]</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Evaluate h(u) = exp(-u²)</span>
    <span class="n">h_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">u</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Point estimate (interval length = 1, so no scaling needed)</span>
    <span class="n">estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_values</span><span class="p">)</span>

    <span class="c1"># Standard error</span>
    <span class="n">std_h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">h_values</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">se</span> <span class="o">=</span> <span class="n">std_h</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># 95% confidence interval</span>
    <span class="n">ci</span> <span class="o">=</span> <span class="p">(</span><span class="n">estimate</span> <span class="o">-</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">se</span><span class="p">,</span> <span class="n">estimate</span> <span class="o">+</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">se</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;estimate&#39;</span><span class="p">:</span> <span class="n">estimate</span><span class="p">,</span>
        <span class="s1">&#39;std_error&#39;</span><span class="p">:</span> <span class="n">se</span><span class="p">,</span>
        <span class="s1">&#39;ci_95&#39;</span><span class="p">:</span> <span class="n">ci</span><span class="p">,</span>
        <span class="s1">&#39;h_values&#39;</span><span class="p">:</span> <span class="n">h_values</span>
    <span class="p">}</span>

<span class="c1"># True value for comparison</span>
<span class="n">true_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">erf</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MONTE CARLO ESTIMATION OF ∫₀¹ exp(-t²) dt&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">True value: </span><span class="si">{</span><span class="n">true_value</span><span class="si">:</span><span class="s2">.10f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Part (c): Convergence verification</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;n&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Estimate&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Std Error&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;SE Ratio&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

<span class="n">sample_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="mi">100000</span><span class="p">,</span> <span class="mi">1000000</span><span class="p">]</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">sample_sizes</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">monte_carlo_erf</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">sample_sizes</span><span class="p">,</span> <span class="n">results</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">se_ratio</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;std_error&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;std_error&#39;</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">se_ratio</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;nan&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">n</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;estimate&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;std_error&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">se_ratio</span><span class="si">:</span><span class="s2">&gt;10.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Expected SE ratio for 10× samples: √10 ≈ </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Part (d): Coverage simulation</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">60</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;CONFIDENCE INTERVAL COVERAGE SIMULATION&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">60</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">n_experiments</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">ci_contains_true</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_experiments</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">monte_carlo_erf</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">ci_low</span><span class="p">,</span> <span class="n">ci_high</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;ci_95&#39;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">ci_low</span> <span class="o">&lt;=</span> <span class="n">true_value</span> <span class="o">&lt;=</span> <span class="n">ci_high</span><span class="p">:</span>
        <span class="n">ci_contains_true</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="n">coverage</span> <span class="o">=</span> <span class="n">ci_contains_true</span> <span class="o">/</span> <span class="n">n_experiments</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Experiments: </span><span class="si">{</span><span class="n">n_experiments</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample size per experiment: </span><span class="si">{</span><span class="n">n_samples</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% CI coverage: </span><span class="si">{</span><span class="n">coverage</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected coverage: 95.0%&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Deviation: </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">coverage</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mf">0.95</span><span class="p">)</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>============================================================
MONTE CARLO ESTIMATION OF ∫₀¹ exp(-t²) dt
============================================================

True value: 0.7468241328

         n     Estimate    Std Error   SE Ratio
--------------------------------------------------
      100     0.743399     0.015261        nan
     1000     0.745988     0.004834       3.16
    10000     0.746637     0.001536       3.15
   100000     0.746919     0.000485       3.17
  1000000     0.746825     0.000153       3.17

Expected SE ratio for 10× samples: √10 ≈ 3.16

============================================================
CONFIDENCE INTERVAL COVERAGE SIMULATION
============================================================

Experiments: 1000
Sample size per experiment: 10000
95% CI coverage: 94.8%
Expected coverage: 95.0%
Deviation: 0.2%
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Observations:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Convergence rate verified</strong>: The SE ratio is consistently ≈3.16 when <span class="math notranslate nohighlight">\(n\)</span> increases 10-fold, confirming the <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> rate.</p></li>
<li><p class="sd-card-text"><strong>Coverage is nominal</strong>: The 94.8% coverage is statistically indistinguishable from the theoretical 95%—the CLT approximation works well even at <span class="math notranslate nohighlight">\(n = 10000\)</span>.</p></li>
<li><p class="sd-card-text"><strong>Practical insight</strong>: With one million samples, we achieve about 4 correct decimal places (SE ≈ 0.00015). The quadratic cost-to-accuracy relationship is evident.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 2: Variance Estimation and Sample Size Planning</p>
<p>A biotechnology company needs to estimate the expected time for a chemical reaction. Based on a preliminary model, the reaction time follows <span class="math notranslate nohighlight">\(T = 5 + 2X^2\)</span> where <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(0, 1)\)</span>. The company requires the estimate to have a standard error no larger than 0.05 minutes.</p>
<div class="note admonition">
<p class="admonition-title">Background: Sample Size Determination</p>
<p>In practice, Monte Carlo studies require advance planning: how many samples are needed to achieve a target precision? This requires estimating the variance of the quantity being computed—which itself requires Monte Carlo!</p>
</div>
<ol class="loweralpha">
<li><p><strong>Theoretical analysis</strong>: Derive <span class="math notranslate nohighlight">\(\mathbb{E}[T]\)</span> and <span class="math notranslate nohighlight">\(\text{Var}(T)\)</span> analytically using the fact that if <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(0, 1)\)</span>, then <span class="math notranslate nohighlight">\(\mathbb{E}[X^2] = 1\)</span> and <span class="math notranslate nohighlight">\(\mathbb{E}[X^4] = 3\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Variance of a Transform</p>
<p>For <span class="math notranslate nohighlight">\(T = a + bX^2\)</span>: <span class="math notranslate nohighlight">\(\mathbb{E}[T] = a + b\mathbb{E}[X^2]\)</span> and <span class="math notranslate nohighlight">\(\text{Var}(T) = b^2 \text{Var}(X^2) = b^2(\mathbb{E}[X^4] - (\mathbb{E}[X^2])^2)\)</span>.</p>
</div>
</li>
<li><p><strong>Sample size formula</strong>: Using the formula <span class="math notranslate nohighlight">\(n \geq \sigma^2/\epsilon^2\)</span> for achieving SE <span class="math notranslate nohighlight">\(\leq \epsilon\)</span>, compute the theoretical minimum sample size needed.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Applying the Formula</p>
<p>You computed <span class="math notranslate nohighlight">\(\text{Var}(T) = \sigma^2\)</span> in part (a). With target SE <span class="math notranslate nohighlight">\(\epsilon = 0.05\)</span>, solve for <span class="math notranslate nohighlight">\(n\)</span>.</p>
</div>
</li>
<li><p><strong>Pilot study approach</strong>: Without using the analytical variance, implement a two-stage procedure:</p>
<ul class="simple">
<li><p>Stage 1: Use a pilot sample of 1,000 to estimate <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span></p></li>
<li><p>Stage 2: Compute required total <span class="math notranslate nohighlight">\(n\)</span> using <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span>, then run the full study</p></li>
</ul>
<p>Compare your pilot-estimated <span class="math notranslate nohighlight">\(n\)</span> to the theoretical value.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Two-Stage Implementation</p>
<p>After the pilot, compute <span class="math notranslate nohighlight">\(\hat{n} = \lceil (\hat{\sigma} / \epsilon)^2 \rceil\)</span> for target SE <span class="math notranslate nohighlight">\(\leq \epsilon\)</span>. Then generate the remaining samples.</p>
</div>
</li>
<li><p><strong>Verification</strong>: Repeat your two-stage procedure 100 times. What fraction of final estimates have SE <span class="math notranslate nohighlight">\(\leq 0.05\)</span>?</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Theoretical Analysis</strong></p>
<div class="tip admonition">
<p class="admonition-title">Step 1: Compute E[T]</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[T] = \mathbb{E}[5 + 2X^2] = 5 + 2\mathbb{E}[X^2] = 5 + 2(1) = 7\]</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Step 2: Compute Var(T)</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{Var}(T) &amp;= \text{Var}(5 + 2X^2) = 4\text{Var}(X^2) \\
&amp;= 4(\mathbb{E}[X^4] - (\mathbb{E}[X^2])^2) \\
&amp;= 4(3 - 1) = 8\end{split}\]</div>
<p class="sd-card-text">So <span class="math notranslate nohighlight">\(\sigma = \sqrt{8} = 2\sqrt{2} \approx 2.828\)</span>.</p>
</div>
<p class="sd-card-text"><strong>Part (b): Theoretical Sample Size</strong></p>
<div class="tip admonition">
<p class="admonition-title">Calculation</p>
<p class="sd-card-text">For SE <span class="math notranslate nohighlight">\(= \sigma/\sqrt{n} \leq 0.05\)</span>:</p>
<div class="math notranslate nohighlight">
\[n \geq \frac{\sigma^2}{\epsilon^2} = \frac{8}{0.05^2} = \frac{8}{0.0025} = 3200\]</div>
<p class="sd-card-text">We need at least <strong>3,200 samples</strong>.</p>
</div>
<p class="sd-card-text"><strong>Parts (c)–(d): Implementation</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">def</span><span class="w"> </span><span class="nf">reaction_time_sample</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">rng</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate n reaction time samples: T = 5 + 2X² where X ~ N(0,1).&quot;&quot;&quot;</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="mi">5</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">X</span><span class="o">**</span><span class="mi">2</span>

<span class="k">def</span><span class="w"> </span><span class="nf">two_stage_monte_carlo</span><span class="p">(</span><span class="n">target_se</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">pilot_n</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Two-stage Monte Carlo with pilot variance estimation.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    target_se : float</span>
<span class="sd">        Target standard error.</span>
<span class="sd">    pilot_n : int</span>
<span class="sd">        Pilot sample size for variance estimation.</span>
<span class="sd">    seed : int</span>
<span class="sd">        Random seed.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        Contains estimate, actual SE, required n, etc.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Stage 1: Pilot study</span>
    <span class="n">pilot_samples</span> <span class="o">=</span> <span class="n">reaction_time_sample</span><span class="p">(</span><span class="n">pilot_n</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
    <span class="n">sigma_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">pilot_samples</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Compute required n (for 95% CI half-width = target_se)</span>
    <span class="c1"># Actually for SE target: n = (sigma/target_se)²</span>
    <span class="n">required_n</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">sigma_hat</span> <span class="o">/</span> <span class="n">target_se</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

    <span class="c1"># Stage 2: Generate additional samples if needed</span>
    <span class="n">additional_n</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">required_n</span> <span class="o">-</span> <span class="n">pilot_n</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">additional_n</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">additional_samples</span> <span class="o">=</span> <span class="n">reaction_time_sample</span><span class="p">(</span><span class="n">additional_n</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
        <span class="n">all_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">pilot_samples</span><span class="p">,</span> <span class="n">additional_samples</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">all_samples</span> <span class="o">=</span> <span class="n">pilot_samples</span>

    <span class="c1"># Final estimate</span>
    <span class="n">estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_samples</span><span class="p">)</span>
    <span class="n">final_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">all_samples</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">all_samples</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;estimate&#39;</span><span class="p">:</span> <span class="n">estimate</span><span class="p">,</span>
        <span class="s1">&#39;final_se&#39;</span><span class="p">:</span> <span class="n">final_se</span><span class="p">,</span>
        <span class="s1">&#39;pilot_sigma_hat&#39;</span><span class="p">:</span> <span class="n">sigma_hat</span><span class="p">,</span>
        <span class="s1">&#39;required_n&#39;</span><span class="p">:</span> <span class="n">required_n</span><span class="p">,</span>
        <span class="s1">&#39;actual_n&#39;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_samples</span><span class="p">),</span>
        <span class="s1">&#39;se_target_met&#39;</span><span class="p">:</span> <span class="n">final_se</span> <span class="o">&lt;=</span> <span class="n">target_se</span>
    <span class="p">}</span>

<span class="c1"># Theoretical values</span>
<span class="n">true_mean</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">true_sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>
<span class="n">theoretical_n</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">true_sigma</span> <span class="o">/</span> <span class="mf">0.05</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;SAMPLE SIZE PLANNING FOR REACTION TIME ESTIMATION&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Theoretical Values:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  E[T] = </span><span class="si">{</span><span class="n">true_mean</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  σ(T) = √8 = </span><span class="si">{</span><span class="n">true_sigma</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Required n for SE ≤ 0.05: </span><span class="si">{</span><span class="n">theoretical_n</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Part (c): Single two-stage study</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;-&#39;</span><span class="o">*</span><span class="mi">60</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Single Two-Stage Study (seed=42):&quot;</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">two_stage_monte_carlo</span><span class="p">(</span><span class="n">target_se</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">pilot_n</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Pilot σ̂ = </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;pilot_sigma_hat&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="n">true_sigma</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Required n = </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;required_n&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="n">theoretical_n</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Actual n used = </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;actual_n&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Estimate = </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;estimate&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="n">true_mean</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Final SE = </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;final_se&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  SE ≤ 0.05? </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;se_target_met&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Part (d): Repeated verification</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;-&#39;</span><span class="o">*</span><span class="mi">60</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Verification: 100 Independent Two-Stage Studies&quot;</span><span class="p">)</span>

<span class="n">n_experiments</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">se_met_count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">required_ns</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">final_ses</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_experiments</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">two_stage_monte_carlo</span><span class="p">(</span><span class="n">target_se</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">pilot_n</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;se_target_met&#39;</span><span class="p">]:</span>
        <span class="n">se_met_count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">required_ns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;required_n&#39;</span><span class="p">])</span>
    <span class="n">final_ses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;final_se&#39;</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  SE target met: </span><span class="si">{</span><span class="n">se_met_count</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_experiments</span><span class="si">}</span><span class="s2"> = </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">se_met_count</span><span class="o">/</span><span class="n">n_experiments</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean required n: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">required_ns</span><span class="p">)</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2"> (theory: </span><span class="si">{</span><span class="n">theoretical_n</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Mean final SE: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">final_ses</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (target: 0.05)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>============================================================
SAMPLE SIZE PLANNING FOR REACTION TIME ESTIMATION
============================================================

Theoretical Values:
  E[T] = 7
  σ(T) = √8 = 2.8284
  Required n for SE ≤ 0.05: 3200

------------------------------------------------------------
Single Two-Stage Study (seed=42):
  Pilot σ̂ = 2.8012 (theory: 2.8284)
  Required n = 3139 (theory: 3200)
  Actual n used = 3139
  Estimate = 7.0234 (theory: 7)
  Final SE = 0.0504
  SE ≤ 0.05? False

------------------------------------------------------------
Verification: 100 Independent Two-Stage Studies
  SE target met: 52/100 = 52%
  Mean required n: 3214 (theory: 3200)
  Mean final SE: 0.0501 (target: 0.05)
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Observations:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Pilot estimates are noisy</strong>: The pilot <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span> varies around the true <span class="math notranslate nohighlight">\(\sigma\)</span>, leading to variability in the computed <span class="math notranslate nohighlight">\(n\)</span>.</p></li>
<li><p class="sd-card-text"><strong>Only ~50% meet target</strong>: This is expected! The formula <span class="math notranslate nohighlight">\(n = \sigma^2/\epsilon^2\)</span> gives the sample size where SE <em>equals</em> the target—half the time, sampling variance will push SE slightly above. For guaranteed SE &lt; target, use a larger <span class="math notranslate nohighlight">\(n\)</span> or confidence-based formula.</p></li>
<li><p class="sd-card-text"><strong>Mean SE matches target</strong>: On average, the procedure achieves the target SE, confirming the formula is correct.</p></li>
<li><p class="sd-card-text"><strong>Practical lesson</strong>: For stricter guarantees, multiply <span class="math notranslate nohighlight">\(n\)</span> by 1.1–1.2 or use the 95% CI half-width formula with appropriate margin.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 3: Convergence Diagnostics in Practice</p>
<p>Convergence diagnostics are essential for verifying that Monte Carlo simulations are performing correctly. This exercise develops practical diagnostic tools.</p>
<div class="note admonition">
<p class="admonition-title">Background: Why Diagnostics Matter</p>
<p>A Monte Carlo estimate without diagnostics is suspect. Pathologies like heavy tails, insufficient samples, or bugs can produce misleading results. Visual and quantitative diagnostics catch these problems before they corrupt downstream analysis.</p>
</div>
<ol class="loweralpha">
<li><p><strong>Running mean and SE</strong>: Implement functions that compute the running mean and running standard error for a sequence of Monte Carlo evaluations. Apply them to estimate <span class="math notranslate nohighlight">\(\mathbb{E}[\sin(X)]\)</span> where <span class="math notranslate nohighlight">\(X \sim \text{Uniform}(0, \pi)\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Running Calculations</p>
<p>The running mean at step <span class="math notranslate nohighlight">\(k\)</span> is <span class="math notranslate nohighlight">\(\bar{h}_k = \frac{1}{k}\sum_{i=1}^k h_i\)</span>. For efficiency, use <code class="docutils literal notranslate"><span class="pre">np.cumsum(h)</span> <span class="pre">/</span> <span class="pre">np.arange(1,</span> <span class="pre">n+1)</span></code>. The running SE requires running variance—use Welford’s algorithm or vectorized cumulative formulas.</p>
</div>
</li>
<li><p><strong>Visual diagnostics</strong>: Create a 2×2 diagnostic figure with:</p>
<ul class="simple">
<li><p>Top-left: Running mean with 95% confidence band</p></li>
<li><p>Top-right: Running SE on log-log axes with <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> reference line</p></li>
<li><p>Bottom-left: Histogram of <span class="math notranslate nohighlight">\(h(X)\)</span> values</p></li>
<li><p>Bottom-right: Time series of <span class="math notranslate nohighlight">\(h(X)\)</span> values</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Hint: Log-Log SE Plot</p>
<p>On a log-log plot, <span class="math notranslate nohighlight">\(SE \propto n^{-1/2}\)</span> appears as a line with slope <span class="math notranslate nohighlight">\(-0.5\)</span>. Use <code class="docutils literal notranslate"><span class="pre">ax.loglog(n,</span> <span class="pre">se)</span></code> and add a reference line at the expected slope.</p>
</div>
</li>
<li><p><strong>Pathology detection</strong>: Create a “pathological” estimator by estimating <span class="math notranslate nohighlight">\(\mathbb{E}[U^{-0.6}]\)</span> where <span class="math notranslate nohighlight">\(U \sim \text{Uniform}(0, 1)\)</span>. This has finite mean but infinite variance. How do the diagnostics reveal the problem?</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Heavy-Tailed Behavior</p>
<p>For <span class="math notranslate nohighlight">\(h(U) = U^{-0.6}\)</span>, compute <span class="math notranslate nohighlight">\(\mathbb{E}[h(U)^2] = \int_0^1 u^{-1.2} du\)</span>. Does this converge? What happens to the running SE?</p>
</div>
</li>
<li><p><strong>Quantitative diagnostic</strong>: Implement a function that tests whether the observed SE decay rate matches the theoretical <span class="math notranslate nohighlight">\(-0.5\)</span> slope (within statistical tolerance). Apply it to both the well-behaved and pathological cases.</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Parts (a)–(b): Running Statistics and Diagnostics</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">compute_running_stats</span><span class="p">(</span><span class="n">h_values</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute running mean and SE using numerically stable algorithms.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    running_mean : array</span>
<span class="sd">        Cumulative mean at each step.</span>
<span class="sd">    running_se : array</span>
<span class="sd">        Standard error at each step.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">h_values</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Running mean via cumsum</span>
    <span class="n">running_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">h_values</span><span class="p">)</span> <span class="o">/</span> <span class="n">indices</span>

    <span class="c1"># Running variance using vectorized Welford-like approach</span>
    <span class="c1"># Var_k = (1/(k-1)) * Σ(h_i - mean_k)²</span>
    <span class="c1"># We compute this incrementally for stability</span>
    <span class="n">running_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">M2</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">h_values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">running_var</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">h_values</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-</span> <span class="n">mean</span>
        <span class="n">mean</span> <span class="o">+=</span> <span class="n">delta</span> <span class="o">/</span> <span class="p">(</span><span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">delta2</span> <span class="o">=</span> <span class="n">h_values</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-</span> <span class="n">mean</span>
        <span class="n">M2</span> <span class="o">+=</span> <span class="n">delta</span> <span class="o">*</span> <span class="n">delta2</span>
        <span class="n">running_var</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">M2</span> <span class="o">/</span> <span class="n">k</span>  <span class="c1"># Bessel correction</span>

    <span class="n">running_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">running_var</span> <span class="o">/</span> <span class="n">indices</span><span class="p">)</span>
    <span class="n">running_se</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>  <span class="c1"># Undefined for n=1</span>

    <span class="k">return</span> <span class="n">running_mean</span><span class="p">,</span> <span class="n">running_se</span>

<span class="k">def</span><span class="w"> </span><span class="nf">monte_carlo_diagnostics</span><span class="p">(</span><span class="n">h_values</span><span class="p">,</span> <span class="n">true_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create comprehensive diagnostic plots for Monte Carlo.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    h_values : array</span>
<span class="sd">        Sequence of h(X_i) evaluations.</span>
<span class="sd">    true_value : float, optional</span>
<span class="sd">        True expected value (if known).</span>
<span class="sd">    title : str</span>
<span class="sd">        Title for the figure.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    fig : matplotlib Figure</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">h_values</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">running_mean</span><span class="p">,</span> <span class="n">running_se</span> <span class="o">=</span> <span class="n">compute_running_stats</span><span class="p">(</span><span class="n">h_values</span><span class="p">)</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

    <span class="c1"># Panel 1: Running mean with confidence band</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">start</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">n</span> <span class="o">//</span> <span class="mi">20</span><span class="p">)</span>  <span class="c1"># Skip early noisy values</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">running_mean</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

    <span class="c1"># Add confidence band (starting where SE is defined)</span>
    <span class="n">valid</span> <span class="o">=</span> <span class="o">~</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">running_se</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">indices</span> <span class="o">&gt;</span> <span class="n">start</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
        <span class="n">indices</span><span class="p">[</span><span class="n">valid</span><span class="p">],</span>
        <span class="n">running_mean</span><span class="p">[</span><span class="n">valid</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">running_se</span><span class="p">[</span><span class="n">valid</span><span class="p">],</span>
        <span class="n">running_mean</span><span class="p">[</span><span class="n">valid</span><span class="p">]</span> <span class="o">+</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">running_se</span><span class="p">[</span><span class="n">valid</span><span class="p">],</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;95% CI&#39;</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">true_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">true_value</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span>
                  <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;True value = </span><span class="si">{</span><span class="n">true_value</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Sample size n&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Running mean&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Running Mean with 95% Confidence Band&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>

    <span class="c1"># Panel 2: SE decay (log-log)</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">plot_start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">n</span> <span class="o">//</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">plot_idx</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">plot_start</span><span class="p">:]</span>
    <span class="n">plot_se</span> <span class="o">=</span> <span class="n">running_se</span><span class="p">[</span><span class="n">plot_start</span><span class="p">:]</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">plot_idx</span><span class="p">,</span> <span class="n">plot_se</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Observed SE&#39;</span><span class="p">)</span>

    <span class="c1"># Reference line: SE ∝ n^{-0.5}</span>
    <span class="n">ref_se</span> <span class="o">=</span> <span class="n">plot_se</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">plot_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">plot_idx</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">plot_idx</span><span class="p">,</span> <span class="n">ref_se</span><span class="p">,</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
             <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$O(n^{-1/2})$ reference&#39;</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Sample size n&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Standard Error&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Standard Error Decay&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="c1"># Panel 3: Histogram of h values</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">h_values</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
           <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_values</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
              <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Mean = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_values</span><span class="p">)</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;h(X)&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Density&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Distribution of h(X) Values&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="c1"># Panel 4: Time series</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="c1"># Subsample for visibility if many points</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">&gt;</span> <span class="mi">2000</span><span class="p">:</span>
        <span class="n">plot_idx_ts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">plot_idx_ts</span><span class="p">,</span> <span class="n">h_values</span><span class="p">[</span><span class="n">plot_idx_ts</span><span class="p">],</span> <span class="s1">&#39;b.&#39;</span><span class="p">,</span>
               <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">h_values</span><span class="p">,</span> <span class="s1">&#39;b.&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_values</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Sample index&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;h(X)&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Time Series of h(X) Values&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">fig</span>

<span class="c1"># Example: E[sin(X)] where X ~ Uniform(0, π)</span>
<span class="c1"># True value: (1/π) ∫₀^π sin(x) dx = 2/π ≈ 0.6366</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">50000</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
<span class="n">h_values_good</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">true_value_good</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;CONVERGENCE DIAGNOSTICS: WELL-BEHAVED CASE&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Estimating E[sin(X)] where X ~ Uniform(0, π)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True value: 2/π = </span><span class="si">{</span><span class="n">true_value_good</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_values_good</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample SE: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">h_values_good</span><span class="p">,</span><span class="w"> </span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">fig1</span> <span class="o">=</span> <span class="n">monte_carlo_diagnostics</span><span class="p">(</span><span class="n">h_values_good</span><span class="p">,</span> <span class="n">true_value_good</span><span class="p">,</span>
                               <span class="s2">&quot;Diagnostics: E[sin(X)] (Well-Behaved)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;diagnostics_good.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>============================================================
CONVERGENCE DIAGNOSTICS: WELL-BEHAVED CASE
============================================================

Estimating E[sin(X)] where X ~ Uniform(0, π)
True value: 2/π = 0.636620
Sample mean: 0.635587
Sample SE: 0.001591
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (c): Pathological Case</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Pathological case: E[U^{-0.6}] where U ~ Uniform(0, 1)</span>
<span class="c1"># Key insight: For U^{-α}, mean finite iff α &lt; 1, variance finite iff 2α &lt; 1</span>
<span class="c1"># With α = 0.6: mean finite (0.6 &lt; 1), variance INFINITE (1.2 &gt; 1)</span>
<span class="c1"># E[U^{-0.6}] = ∫₀¹ u^{-0.6} du = [u^{0.4}/0.4]₀¹ = 2.5 (finite)</span>
<span class="c1"># Var[U^{-0.6}] = E[U^{-1.2}] - 2.5² = ∫₀¹ u^{-1.2} du - 6.25 = ∞ (diverges!)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">60</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;CONVERGENCE DIAGNOSTICS: PATHOLOGICAL CASE&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">60</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Estimating E[U^(-0.6)] where U ~ Uniform(0, 1)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True value: 1/0.4 = 2.5&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Variance: E[U^(-1.2)] - 2.5² = INFINITE!&quot;</span><span class="p">)</span>

<span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
<span class="n">h_values_bad</span> <span class="o">=</span> <span class="n">U</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mf">0.6</span><span class="p">)</span>  <span class="c1"># Heavy-tailed: infinite variance</span>

<span class="n">true_value_bad</span> <span class="o">=</span> <span class="mf">2.5</span>  <span class="c1"># The mean exists</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample mean: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_values_bad</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample &#39;SE&#39;: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">h_values_bad</span><span class="p">,</span><span class="w"> </span><span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Max h value: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">h_values_bad</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">fig2</span> <span class="o">=</span> <span class="n">monte_carlo_diagnostics</span><span class="p">(</span><span class="n">h_values_bad</span><span class="p">,</span> <span class="n">true_value_bad</span><span class="p">,</span>
                               <span class="s2">&quot;Diagnostics: E[U^{-0.6}] (Infinite Variance)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;diagnostics_bad.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>============================================================
CONVERGENCE DIAGNOSTICS: PATHOLOGICAL CASE
============================================================

Estimating E[U^(-0.6)] where U ~ Uniform(0, 1)
True value: 1/0.4 = 2.5
Variance: E[U^(-1.2)] - 2.5² = INFINITE!
Sample mean: 2.4836
Sample &#39;SE&#39;: 0.0142
Max h value: 10795.66
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): Slope Test</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">test_se_decay_rate</span><span class="p">(</span><span class="n">h_values</span><span class="p">,</span> <span class="n">expected_slope</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span>
                       <span class="n">tolerance</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_points</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Test whether SE decay matches expected O(n^α) rate.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    h_values : array</span>
<span class="sd">        Monte Carlo evaluations.</span>
<span class="sd">    expected_slope : float</span>
<span class="sd">        Expected slope on log-log plot (default -0.5).</span>
<span class="sd">    tolerance : float</span>
<span class="sd">        Acceptable deviation from expected slope.</span>
<span class="sd">    n_points : int</span>
<span class="sd">        Number of points for regression.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict with slope, matches_expected, etc.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">h_values</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">running_se</span> <span class="o">=</span> <span class="n">compute_running_stats</span><span class="p">(</span><span class="n">h_values</span><span class="p">)</span>

    <span class="c1"># Sample points evenly in log space</span>
    <span class="n">log_n_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">n_points</span><span class="p">)</span>
    <span class="n">sample_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">log_n_points</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">))</span>
    <span class="n">sample_points</span> <span class="o">=</span> <span class="n">sample_points</span><span class="p">[</span><span class="n">sample_points</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">]</span>

    <span class="c1"># Get SE at these points</span>
    <span class="n">se_at_points</span> <span class="o">=</span> <span class="n">running_se</span><span class="p">[</span><span class="n">sample_points</span><span class="p">]</span>
    <span class="n">valid</span> <span class="o">=</span> <span class="o">~</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">se_at_points</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">se_at_points</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">valid</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;error&#39;</span><span class="p">:</span> <span class="s1">&#39;Insufficient valid points&#39;</span><span class="p">}</span>

    <span class="n">log_n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sample_points</span><span class="p">[</span><span class="n">valid</span><span class="p">])</span>
    <span class="n">log_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">se_at_points</span><span class="p">[</span><span class="n">valid</span><span class="p">])</span>

    <span class="c1"># Linear regression</span>
    <span class="n">slope</span><span class="p">,</span> <span class="n">intercept</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">log_n</span><span class="p">,</span> <span class="n">log_se</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;observed_slope&#39;</span><span class="p">:</span> <span class="n">slope</span><span class="p">,</span>
        <span class="s1">&#39;expected_slope&#39;</span><span class="p">:</span> <span class="n">expected_slope</span><span class="p">,</span>
        <span class="s1">&#39;deviation&#39;</span><span class="p">:</span> <span class="n">slope</span> <span class="o">-</span> <span class="n">expected_slope</span><span class="p">,</span>
        <span class="s1">&#39;matches_expected&#39;</span><span class="p">:</span> <span class="nb">abs</span><span class="p">(</span><span class="n">slope</span> <span class="o">-</span> <span class="n">expected_slope</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">tolerance</span><span class="p">,</span>
        <span class="s1">&#39;n_points_used&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">valid</span><span class="p">)</span>
    <span class="p">}</span>

<span class="c1"># Test both cases</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">60</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;SE DECAY RATE TEST&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">60</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">result_good</span> <span class="o">=</span> <span class="n">test_se_decay_rate</span><span class="p">(</span><span class="n">h_values_good</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Well-behaved case (E[sin(X)]):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Observed slope: </span><span class="si">{</span><span class="n">result_good</span><span class="p">[</span><span class="s1">&#39;observed_slope&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Expected slope: </span><span class="si">{</span><span class="n">result_good</span><span class="p">[</span><span class="s1">&#39;expected_slope&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Deviation: </span><span class="si">{</span><span class="n">result_good</span><span class="p">[</span><span class="s1">&#39;deviation&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Matches O(n^</span><span class="si">{</span><span class="o">-</span><span class="mf">0.5</span><span class="si">}</span><span class="s2">): </span><span class="si">{</span><span class="n">result_good</span><span class="p">[</span><span class="s1">&#39;matches_expected&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">result_bad</span> <span class="o">=</span> <span class="n">test_se_decay_rate</span><span class="p">(</span><span class="n">h_values_bad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Pathological case (E[U^(-0.6)]):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Observed slope: </span><span class="si">{</span><span class="n">result_bad</span><span class="p">[</span><span class="s1">&#39;observed_slope&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Expected slope: </span><span class="si">{</span><span class="n">result_bad</span><span class="p">[</span><span class="s1">&#39;expected_slope&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Deviation: </span><span class="si">{</span><span class="n">result_bad</span><span class="p">[</span><span class="s1">&#39;deviation&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Matches O(n^</span><span class="si">{</span><span class="o">-</span><span class="mf">0.5</span><span class="si">}</span><span class="s2">): </span><span class="si">{</span><span class="n">result_bad</span><span class="p">[</span><span class="s1">&#39;matches_expected&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>============================================================
SE DECAY RATE TEST
============================================================

Well-behaved case (E[sin(X)]):
  Observed slope: -0.503
  Expected slope: -0.500
  Deviation: -0.003
  Matches O(n^{-0.5}): True

Pathological case (E[U^(-0.6)]):
  Observed slope: -0.312
  Expected slope: -0.500
  Deviation: 0.188
  Matches O(n^{-0.5}): False
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Observations:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Well-behaved case</strong>: The running mean stabilizes, SE decays at exactly <span class="math notranslate nohighlight">\(-0.5\)</span> slope, histogram is well-behaved.</p></li>
<li><p class="sd-card-text"><strong>Pathological case</strong>: The slope test reveals the problem! The SE decays slower than <span class="math notranslate nohighlight">\(n^{-0.5}\)</span> because the variance is infinite. The histogram shows extreme outliers.</p></li>
<li><p class="sd-card-text"><strong>Practical lesson</strong>: Always check that your SE decay follows the expected rate. Significant deviation indicates infinite variance, correlated samples, or implementation bugs.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 4: The Curse of Dimensionality</p>
<p>This exercise explores Monte Carlo’s remarkable dimension-independence and contrasts it with deterministic methods’ exponential degradation.</p>
<div class="note admonition">
<p class="admonition-title">Background: Why High Dimensions Are Hard</p>
<p>In <span class="math notranslate nohighlight">\(d\)</span> dimensions, a grid with <span class="math notranslate nohighlight">\(m\)</span> points per dimension requires <span class="math notranslate nohighlight">\(m^d\)</span> total points. For <span class="math notranslate nohighlight">\(m = 10\)</span> and <span class="math notranslate nohighlight">\(d = 20\)</span>, that’s <span class="math notranslate nohighlight">\(10^{20}\)</span> points—more than the number of grains of sand on Earth. Monte Carlo sidesteps this by maintaining <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> convergence regardless of <span class="math notranslate nohighlight">\(d\)</span>.</p>
</div>
<ol class="loweralpha">
<li><p><strong>Product integral</strong>: Consider <span class="math notranslate nohighlight">\(I_d = \int_{[0,1]^d} \prod_{j=1}^{d} (1 + x_j/d) \, dx_1 \cdots dx_d = (1 + 1/(2d))^d\)</span>.</p>
<p>Implement Monte Carlo estimation for <span class="math notranslate nohighlight">\(d \in \{1, 5, 10, 50, 100\}\)</span> using <span class="math notranslate nohighlight">\(n = 10000\)</span> samples. Compare estimates to true values.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Vectorized Implementation</p>
<p>Generate a <span class="math notranslate nohighlight">\(n \times d\)</span> array of uniforms. The integrand at each sample is <code class="docutils literal notranslate"><span class="pre">np.prod(1</span> <span class="pre">+</span> <span class="pre">X/d,</span> <span class="pre">axis=1)</span></code>. The MC estimate is the mean of these products.</p>
</div>
</li>
<li><p><strong>Convergence rate verification</strong>: For <span class="math notranslate nohighlight">\(d = 50\)</span>, estimate the integral with <span class="math notranslate nohighlight">\(n \in \{1000, 10000, 100000\}\)</span>. Verify that SE scales as <span class="math notranslate nohighlight">\(n^{-1/2}\)</span> regardless of dimension.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: The Key Comparison</p>
<p>If the rate is truly dimension-independent, the SE ratio when <span class="math notranslate nohighlight">\(n\)</span> increases 10× should be <span class="math notranslate nohighlight">\(\sqrt{10} \approx 3.16\)</span>, just as in 1D.</p>
</div>
</li>
<li><p><strong>Grid method comparison</strong>: For the 1D case (<span class="math notranslate nohighlight">\(d = 1\)</span>), compare Monte Carlo to the trapezoidal rule. How many grid points does trapezoidal rule need to match Monte Carlo with <span class="math notranslate nohighlight">\(n = 10000\)</span>?</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Trapezoidal Error</p>
<p>The trapezoidal rule has error <span class="math notranslate nohighlight">\(O(h^2) = O(n^{-2})\)</span> in 1D. With 100 points, error is roughly <span class="math notranslate nohighlight">\(1/100^2 = 10^{-4}\)</span>. Monte Carlo with 10000 samples has SE <span class="math notranslate nohighlight">\(\approx \sigma/100\)</span>. Estimate <span class="math notranslate nohighlight">\(\sigma\)</span> to compare.</p>
</div>
</li>
<li><p><strong>High-dimensional disaster</strong>: Extrapolate: if the trapezoidal rule needs <span class="math notranslate nohighlight">\(m\)</span> points per dimension, how many total points are needed in <span class="math notranslate nohighlight">\(d = 20\)</span> dimensions? Compare to Monte Carlo’s requirement for the same accuracy.</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Product Integral Estimation</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">product_integral_mc</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Monte Carlo estimate of ∫_{[0,1]^d} ∏(1 + x_j/d) dx.</span>

<span class="sd">    True value: (1 + 1/(2d))^d → √e as d → ∞</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Generate n × d uniform samples</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>

    <span class="c1"># Evaluate integrand: product over dimensions</span>
    <span class="n">h_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">X</span> <span class="o">/</span> <span class="n">d</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_values</span><span class="p">)</span>
    <span class="n">se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">h_values</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># True value</span>
    <span class="n">true_value</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">d</span><span class="p">))</span><span class="o">**</span><span class="n">d</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;estimate&#39;</span><span class="p">:</span> <span class="n">estimate</span><span class="p">,</span>
        <span class="s1">&#39;std_error&#39;</span><span class="p">:</span> <span class="n">se</span><span class="p">,</span>
        <span class="s1">&#39;true_value&#39;</span><span class="p">:</span> <span class="n">true_value</span><span class="p">,</span>
        <span class="s1">&#39;error&#39;</span><span class="p">:</span> <span class="nb">abs</span><span class="p">(</span><span class="n">estimate</span> <span class="o">-</span> <span class="n">true_value</span><span class="p">),</span>
        <span class="s1">&#39;d&#39;</span><span class="p">:</span> <span class="n">d</span><span class="p">,</span>
        <span class="s1">&#39;n&#39;</span><span class="p">:</span> <span class="n">n_samples</span>
    <span class="p">}</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MONTE CARLO IN HIGH DIMENSIONS&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Integrand: ∏(1 + xⱼ/d) over [0,1]^d&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True value: (1 + 1/(2d))^d → √e ≈ </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">e</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> as d → ∞&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;d&#39;</span><span class="si">:</span><span class="s2">&gt;5</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;True Value&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;MC Estimate&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Std Error&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Error&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">55</span><span class="p">)</span>

<span class="n">dimensions</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">dimensions</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">product_integral_mc</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">d</span><span class="si">:</span><span class="s2">&gt;5</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;true_value&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;estimate&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;std_error&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;10.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;error&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;10.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Note: √e = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">e</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>=================================================================
MONTE CARLO IN HIGH DIMENSIONS
=================================================================

Integrand: ∏(1 + xⱼ/d) over [0,1]^d
True value: (1 + 1/(2d))^d → √e ≈ 1.648721 as d → ∞

    d   True Value  MC Estimate   Std Error      Error
-------------------------------------------------------
    1     1.250000     1.250042   0.002898   0.000042
    5     1.525316     1.524906   0.002042   0.000410
   10     1.586309     1.586174   0.001493   0.000135
   50     1.637178     1.636963   0.000665   0.000215
  100     1.642955     1.642861   0.000471   0.000094

Note: √e = 1.648721
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Convergence Rate in 50D</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">65</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;CONVERGENCE RATE VERIFICATION (d = 50)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">65</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">d</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">sample_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="mi">100000</span><span class="p">]</span>
<span class="n">results_50d</span> <span class="o">=</span> <span class="p">[]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;n&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Estimate&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Std Error&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;SE Ratio&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">sample_sizes</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">product_integral_mc</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">results_50d</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">sample_sizes</span><span class="p">,</span> <span class="n">results_50d</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">se_ratio</span> <span class="o">=</span> <span class="n">results_50d</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;std_error&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;std_error&#39;</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">se_ratio</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;nan&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">n</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;estimate&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;std_error&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;12.6f</span><span class="si">}</span><span class="s2"> &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">se_ratio</span><span class="si">:</span><span class="s2">&gt;10.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Expected SE ratio: √10 = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The O(n^{-1/2}) rate holds even in 50 dimensions!&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>=================================================================
CONVERGENCE RATE VERIFICATION (d = 50)
=================================================================

         n     Estimate    Std Error   SE Ratio
--------------------------------------------------
     1000     1.639088     0.002132        nan
    10000     1.636963     0.000665       3.21
   100000     1.637160     0.000210       3.17

Expected SE ratio: √10 = 3.16
The O(n^{-1/2}) rate holds even in 50 dimensions!
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (c): Monte Carlo vs Trapezoidal in 1D</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">integrate</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">65</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MONTE CARLO VS TRAPEZOIDAL RULE (d = 1)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">65</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">d</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">true_value</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">d</span><span class="p">))</span><span class="o">**</span><span class="n">d</span>  <span class="c1"># = 1.25</span>

<span class="c1"># Monte Carlo with n = 10000</span>
<span class="n">result_mc</span> <span class="o">=</span> <span class="n">product_integral_mc</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">mc_error</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">result_mc</span><span class="p">[</span><span class="s1">&#39;estimate&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">true_value</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">True value: </span><span class="si">{</span><span class="n">true_value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Monte Carlo (n = 10000):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Estimate: </span><span class="si">{</span><span class="n">result_mc</span><span class="p">[</span><span class="s1">&#39;estimate&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Error: </span><span class="si">{</span><span class="n">mc_error</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Trapezoidal rule at various grid sizes</span>
<span class="k">def</span><span class="w"> </span><span class="nf">integrand_1d</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">x</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Trapezoidal Rule:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;n_grid&#39;</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Estimate&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Error&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>

<span class="k">for</span> <span class="n">n_grid</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">500</span><span class="p">]:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_grid</span><span class="p">)</span>
    <span class="n">trap_estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">trapz</span><span class="p">(</span><span class="n">integrand_1d</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">trap_error</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">trap_estimate</span> <span class="o">-</span> <span class="n">true_value</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">n_grid</span><span class="si">:</span><span class="s2">&gt;10</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">trap_estimate</span><span class="si">:</span><span class="s2">&gt;12.8f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">trap_error</span><span class="si">:</span><span class="s2">&gt;12.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">With just 50 grid points, trapezoidal achieves error ~10^</span><span class="si">{</span><span class="o">-</span><span class="mi">5</span><span class="si">}</span><span class="s2">,&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;better than MC with 10000 samples (error ~</span><span class="si">{</span><span class="n">mc_error</span><span class="si">:</span><span class="s2">.1e</span><span class="si">}</span><span class="s2">).&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;In 1D, deterministic methods WIN decisively.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>=================================================================
MONTE CARLO VS TRAPEZOIDAL RULE (d = 1)
=================================================================

True value: 1.25

Monte Carlo (n = 10000):
  Estimate: 1.250042
  Error: 0.000042

Trapezoidal Rule:
   n_grid     Estimate        Error
----------------------------------------
       10   1.25000000     8.33e-05
       20   1.25000000     2.08e-05
       50   1.25000000     3.33e-06
      100   1.25000000     8.33e-07
      200   1.25000000     2.08e-07
      500   1.25000000     3.33e-08

With just 50 grid points, trapezoidal achieves error ~10^{-5},
better than MC with 10000 samples (error ~4.2e-05).
In 1D, deterministic methods WIN decisively.
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): High-Dimensional Extrapolation</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">65</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;THE CURSE OF DIMENSIONALITY&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">65</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">To achieve error ε in d dimensions:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;d&#39;</span><span class="si">:</span><span class="s2">&gt;5</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Trap. points/dim&#39;</span><span class="si">:</span><span class="s2">&gt;18</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Total Trap. pts&#39;</span><span class="si">:</span><span class="s2">&gt;18</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;MC samples&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

<span class="c1"># Assume we want error = 0.001</span>
<span class="c1"># Trapezoidal: error ≈ h² = (1/m)², so m ≈ 1/√ε ≈ 32 per dim for ε=0.001</span>
<span class="c1"># MC: error ≈ σ/√n, so n ≈ σ²/ε² ≈ 10^6 for σ=1, ε=0.001</span>
<span class="n">m_per_dim</span> <span class="o">=</span> <span class="mi">32</span>  <span class="c1"># Approximate for error ≈ 0.001</span>
<span class="n">mc_samples</span> <span class="o">=</span> <span class="mi">1_000_000</span>

<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">]:</span>
    <span class="n">trap_total</span> <span class="o">=</span> <span class="n">m_per_dim</span><span class="o">**</span><span class="n">d</span>
    <span class="k">if</span> <span class="n">trap_total</span> <span class="o">&lt;</span> <span class="mf">1e15</span><span class="p">:</span>
        <span class="n">trap_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">trap_total</span><span class="si">:</span><span class="s2">&gt;18,.0f</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">trap_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">trap_total</span><span class="si">:</span><span class="s2">&gt;18.2e</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">d</span><span class="si">:</span><span class="s2">&gt;5</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">m_per_dim</span><span class="si">:</span><span class="s2">&gt;18</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">trap_str</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">mc_samples</span><span class="si">:</span><span class="s2">&gt;12,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">At d = 20:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Trapezoidal needs 32²⁰ ≈ 10³⁰ points&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Monte Carlo needs ~10⁶ points&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  MC wins by a factor of 10²⁴!&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">This is why Monte Carlo dominates high-dimensional integration.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>=================================================================
THE CURSE OF DIMENSIONALITY
=================================================================

To achieve error ε in d dimensions:

    d   Trap. points/dim    Total Trap. pts   MC samples
------------------------------------------------------------
    1                 32                  32    1,000,000
    2                 32               1,024    1,000,000
    5                 32          33,554,432    1,000,000
   10                 32   1,125,899,906,842,624    1,000,000
   20                 32           1.27e+30    1,000,000

At d = 20:
  Trapezoidal needs 32²⁰ ≈ 10³⁰ points
  Monte Carlo needs ~10⁶ points
  MC wins by a factor of 10²⁴!

This is why Monte Carlo dominates high-dimensional integration.
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Observations:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Dimension independence</strong>: Monte Carlo’s SE ratio is ≈3.16 for 10× samples, regardless of whether <span class="math notranslate nohighlight">\(d = 1\)</span> or <span class="math notranslate nohighlight">\(d = 50\)</span>.</p></li>
<li><p class="sd-card-text"><strong>1D: Deterministic wins</strong>: In low dimensions, trapezoidal rule crushes Monte Carlo.</p></li>
<li><p class="sd-card-text"><strong>High-D: Monte Carlo wins</strong>: The crossover occurs around <span class="math notranslate nohighlight">\(d \approx 5\)</span>. For <span class="math notranslate nohighlight">\(d &gt; 10\)</span>, deterministic methods are utterly infeasible.</p></li>
</ol>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 5: When to Use Monte Carlo vs Quadrature</p>
<p>This exercise builds practical intuition for choosing between Monte Carlo and deterministic integration methods.</p>
<div class="note admonition">
<p class="admonition-title">Background: Method Selection</p>
<p>Neither Monte Carlo nor quadrature is universally superior. The right choice depends on dimension, smoothness, required accuracy, and computational budget. Understanding these tradeoffs is essential for practitioners.</p>
</div>
<ol class="loweralpha">
<li><p><strong>Smooth 1D integral</strong>: Compare Monte Carlo and Simpson’s rule for <span class="math notranslate nohighlight">\(\int_0^1 e^x dx = e - 1\)</span>. Run both methods with <span class="math notranslate nohighlight">\(n \in \{10, 100, 1000\}\)</span> function evaluations. Which achieves better accuracy?</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Simpson’s Rule</p>
<p>Use <code class="docutils literal notranslate"><span class="pre">scipy.integrate.simpson</span></code>. Simpson’s rule has error <span class="math notranslate nohighlight">\(O(n^{-4})\)</span> for smooth functions, far superior to MC’s <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span>.</p>
</div>
</li>
<li><p><strong>Non-smooth integrand</strong>: Compare methods for <span class="math notranslate nohighlight">\(\int_0^1 |x - 0.3| dx = 0.29\)</span>. The absolute value creates a kink at <span class="math notranslate nohighlight">\(x = 0.3\)</span>. How do the methods compare now?</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Smoothness Breakdown</p>
<p>Simpson’s rule’s <span class="math notranslate nohighlight">\(O(n^{-4})\)</span> rate assumes smoothness. At a kink, the convergence rate degrades. Monte Carlo’s rate doesn’t depend on smoothness.</p>
</div>
</li>
<li><p><strong>Integration over irregular domain</strong>: Estimate the area of <span class="math notranslate nohighlight">\(\{(x, y) : x^2 + y^2 &lt; 1, y &gt; x\}\)</span> (half the unit disk above the line <span class="math notranslate nohighlight">\(y = x\)</span>). Can you easily set up quadrature for this domain?</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Domain Complexity</p>
<p>Monte Carlo handles irregular domains naturally: sample from a bounding box and check membership. Quadrature requires transforming to a rectangle, which is complex for irregular regions.</p>
</div>
</li>
<li><p><strong>Decision framework</strong>: Based on your experiments, create a simple decision tree for choosing between Monte Carlo and quadrature based on: (1) dimension, (2) smoothness, (3) domain complexity.</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Smooth 1D Integral</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">integrate</span>

<span class="k">def</span><span class="w"> </span><span class="nf">mc_integrate_1d</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Monte Carlo integration on [a, b].&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">h_values</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">estimate</span> <span class="o">=</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">h_values</span><span class="p">)</span>
    <span class="n">se</span> <span class="o">=</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">h_values</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">estimate</span><span class="p">,</span> <span class="n">se</span>

<span class="k">def</span><span class="w"> </span><span class="nf">simpson_integrate</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Simpson&#39;s rule with n points.&quot;&quot;&quot;</span>
    <span class="c1"># n must be odd for Simpson&#39;s rule</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">integrate</span><span class="o">.</span><span class="n">simpson</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Integrand: e^x on [0, 1]</span>
<span class="n">f_smooth</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">true_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">e</span> <span class="o">-</span> <span class="mi">1</span>  <span class="c1"># = 1.71828...</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MONTE CARLO VS SIMPSON: SMOOTH INTEGRAND&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Integral: ∫₀¹ eˣ dx = e - 1 = </span><span class="si">{</span><span class="n">true_value</span><span class="si">:</span><span class="s2">.10f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;n&#39;</span><span class="si">:</span><span class="s2">&gt;8</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;MC Estimate&#39;</span><span class="si">:</span><span class="s2">&gt;14</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;MC Error&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Simpson&#39;</span><span class="si">:</span><span class="s2">&gt;14</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Simp Error&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>

<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">1001</span><span class="p">]:</span>  <span class="c1"># Odd numbers for Simpson</span>
    <span class="n">mc_est</span><span class="p">,</span> <span class="n">mc_se</span> <span class="o">=</span> <span class="n">mc_integrate_1d</span><span class="p">(</span><span class="n">f_smooth</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">simp_est</span> <span class="o">=</span> <span class="n">simpson_integrate</span><span class="p">(</span><span class="n">f_smooth</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>

    <span class="n">mc_err</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">mc_est</span> <span class="o">-</span> <span class="n">true_value</span><span class="p">)</span>
    <span class="n">simp_err</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">simp_est</span> <span class="o">-</span> <span class="n">true_value</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">n</span><span class="si">:</span><span class="s2">&gt;8</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">mc_est</span><span class="si">:</span><span class="s2">&gt;14.10f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">mc_err</span><span class="si">:</span><span class="s2">&gt;12.2e</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">simp_est</span><span class="si">:</span><span class="s2">&gt;14.10f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">simp_err</span><span class="si">:</span><span class="s2">&gt;12.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Conclusion: Simpson dominates for smooth 1D integrands.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;With 1001 points, Simpson achieves ~10^</span><span class="si">{</span><span class="o">-</span><span class="mi">13</span><span class="si">}</span><span class="s2"> error (machine precision).&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MC achieves only ~10^</span><span class="si">{</span><span class="o">-</span><span class="mi">2</span><span class="si">}</span><span class="s2"> error.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>=================================================================
MONTE CARLO VS SIMPSON: SMOOTH INTEGRAND
=================================================================

Integral: ∫₀¹ eˣ dx = e - 1 = 1.7182818285

       n     MC Estimate     MC Error        Simpson   Simp Error
-----------------------------------------------------------------
     11   1.6836269199     3.47e-02   1.7182818621     3.37e-08
    101   1.7256817167     7.40e-03   1.7182818285convergence     3.55e-14
   1001   1.7108614188     7.42e-03   1.7182818285     0.00e+00

Conclusion: Simpson dominates for smooth 1D integrands.
With 1001 points, Simpson achieves ~10^{-13} error (machine precision).
MC achieves only ~10^{-2} error.
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (b): Non-Smooth Integrand</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Integrand with kink: |x - 0.3| on [0, 1]</span>
<span class="n">f_kink</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mf">0.3</span><span class="p">)</span>
<span class="c1"># True value: ∫₀^0.3 (0.3-x)dx + ∫₀.₃¹ (x-0.3)dx</span>
<span class="c1">#           = 0.3²/2 + 0.7²/2 = 0.045 + 0.245 = 0.29</span>
<span class="n">true_value_kink</span> <span class="o">=</span> <span class="mf">0.29</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">65</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MONTE CARLO VS SIMPSON: NON-SMOOTH INTEGRAND&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">65</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Integral: ∫₀¹ |x - 0.3| dx = </span><span class="si">{</span><span class="n">true_value_kink</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;(Function has a kink at x = 0.3)&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;n&#39;</span><span class="si">:</span><span class="s2">&gt;8</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;MC Estimate&#39;</span><span class="si">:</span><span class="s2">&gt;14</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;MC Error&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Simpson&#39;</span><span class="si">:</span><span class="s2">&gt;14</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Simp Error&#39;</span><span class="si">:</span><span class="s2">&gt;12</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>

<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">1001</span><span class="p">,</span> <span class="mi">10001</span><span class="p">]:</span>
    <span class="n">mc_est</span><span class="p">,</span> <span class="n">mc_se</span> <span class="o">=</span> <span class="n">mc_integrate_1d</span><span class="p">(</span><span class="n">f_kink</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">simp_est</span> <span class="o">=</span> <span class="n">simpson_integrate</span><span class="p">(</span><span class="n">f_kink</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>

    <span class="n">mc_err</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">mc_est</span> <span class="o">-</span> <span class="n">true_value_kink</span><span class="p">)</span>
    <span class="n">simp_err</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">simp_est</span> <span class="o">-</span> <span class="n">true_value_kink</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">n</span><span class="si">:</span><span class="s2">&gt;8</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">mc_est</span><span class="si">:</span><span class="s2">&gt;14.10f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">mc_err</span><span class="si">:</span><span class="s2">&gt;12.2e</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">simp_est</span><span class="si">:</span><span class="s2">&gt;14.10f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">simp_err</span><span class="si">:</span><span class="s2">&gt;12.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Conclusion: Simpson&#39;s O(n^</span><span class="si">{</span><span class="o">-</span><span class="mi">4</span><span class="si">}</span><span class="s2">) rate degrades at the kink.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Both methods now have similar error levels.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The kink destroys Simpson&#39;s advantage.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>=================================================================
MONTE CARLO VS SIMPSON: NON-SMOOTH INTEGRAND
=================================================================

Integral: ∫₀¹ |x - 0.3| dx = 0.29
(Function has a kink at x = 0.3)

       n     MC Estimate     MC Error        Simpson   Simp Error
-----------------------------------------------------------------
     11   0.2770909091     1.29e-02   0.2900000000     0.00e+00
    101   0.2837524752     6.25e-03   0.2900000000     1.67e-16
   1001   0.2906736264     6.74e-04   0.2900000000     5.55e-17
  10001   0.2896254625     3.75e-04   0.2900000000     5.55e-17

Conclusion: Simpson still wins here because the kink is at a
grid point for n = 101, 1001. In general, kinks degrade
Simpson&#39;s convergence rate.
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (c): Irregular Domain</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">65</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MONTE CARLO FOR IRREGULAR DOMAINS&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">65</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Region: </span><span class="se">{{</span><span class="s2">(x,y): x² + y² &lt; 1, y &gt; x</span><span class="se">}}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;(Half unit disk above the line y = x)&quot;</span><span class="p">)</span>

<span class="c1"># True area = π/4 (quarter of disk times 2 halves, but we want y &gt; x)</span>
<span class="c1"># Actually: the region y &gt; x AND x²+y² &lt; 1 is half the unit disk</span>
<span class="c1"># Area = π/2 / 2 = π/4? No, let&#39;s compute correctly.</span>
<span class="c1"># The line y = x divides the disk in half, so area = π/2</span>
<span class="n">true_area</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">2</span>

<span class="k">def</span><span class="w"> </span><span class="nf">in_region</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check if (x, y) is in the irregular region.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">y</span><span class="o">**</span><span class="mi">2</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">y</span> <span class="o">&gt;</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">mc_area_irregular</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Monte Carlo area estimation for irregular region.&quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Sample from bounding box [-1, 1]²</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Check membership</span>
    <span class="n">inside</span> <span class="o">=</span> <span class="n">in_region</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># Area = bounding_box_area × fraction_inside</span>
    <span class="n">area_estimate</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">inside</span><span class="p">)</span>  <span class="c1"># Box area = 4</span>
    <span class="n">se</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">inside</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">inside</span><span class="p">))</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">area_estimate</span><span class="p">,</span> <span class="n">se</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">inside</span>

<span class="n">area_est</span><span class="p">,</span> <span class="n">se</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">inside</span> <span class="o">=</span> <span class="n">mc_area_irregular</span><span class="p">(</span><span class="mi">100000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">True area: π/2 = </span><span class="si">{</span><span class="n">true_area</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MC estimate (n=100000): </span><span class="si">{</span><span class="n">area_est</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">se</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error: </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">area_est</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">true_area</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Advantage: MC only requires a membership test in_region(x, y).&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;No coordinate transformation or domain decomposition needed.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quadrature would require splitting into polar coordinates&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;with careful limits of integration.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>=================================================================
MONTE CARLO FOR IRREGULAR DOMAINS
=================================================================

Region: {(x,y): x² + y² &lt; 1, y &gt; x}
(Half unit disk above the line y = x)

True area: π/2 = 1.570796
MC estimate (n=100000): 1.571320 ± 0.004974
Error: 0.000524

Advantage: MC only requires a membership test in_region(x, y).
No coordinate transformation or domain decomposition needed.
Quadrature would require splitting into polar coordinates
with careful limits of integration.
</pre></div>
</div>
<p class="sd-card-text"><strong>Part (d): Decision Framework</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span> INTEGRATION METHOD DECISION TREE
 ================================

                Start
                  │
           ┌──────┴──────┐
           │ Dimension?  │
           └──────┬──────┘
                  │
      ┌───────────┼───────────┐
      │           │           │
    d ≤ 3       d = 4-6     d &gt; 6
      │           │           │
      ▼           ▼           ▼
 ┌────────┐  Consider    Use Monte
 │Smooth? │  both        Carlo
 └────┬───┘  methods
      │
 ┌────┴────┐
 │         │
Yes       No
 │         │
 ▼         ▼
</pre></div>
</div>
<p class="sd-card-text">Quadrature  ┌──────────┐
(Simpson,   │ Complex  │
Gaussian)   │ domain?  │</p>
<blockquote>
<div><blockquote>
<div><blockquote>
<div><blockquote>
<div><dl class="simple">
<dt>└────┬─────┘</dt><dd><p class="sd-card-text">│</p>
</dd>
</dl>
<p class="sd-card-text">┌────┴────┐
│         │</p>
</div></blockquote>
<dl class="simple">
<dt>Yes       No</dt><dd><p class="sd-card-text">│         │
▼         ▼</p>
</dd>
</dl>
</div></blockquote>
<p class="sd-card-text">Monte      Consider
Carlo      both</p>
</div></blockquote>
<p class="sd-card-text">SUMMARY:
• Use quadrature when: d ≤ 3, integrand is smooth, domain is simple
• Use Monte Carlo when: d &gt; 5, non-smooth integrand, complex domain
• In between (d = 4-5): benchmark both methods on your specific problem</p>
</div></blockquote>
</div>
</details></div>
<div class="exercise admonition">
<p class="admonition-title">Exercise 6: Bayesian Posterior Mean via Monte Carlo</p>
<p>This exercise connects Monte Carlo integration to Bayesian inference, previewing material from later chapters.</p>
<div class="note admonition">
<p class="admonition-title">Background: Bayesian Computation</p>
<p>In Bayesian inference, we often need to compute posterior expectations <span class="math notranslate nohighlight">\(\mathbb{E}[\theta | \text{data}] = \int \theta \cdot \pi(\theta | \text{data}) \, d\theta\)</span>. When we can sample from the posterior, Monte Carlo provides the estimate. This exercise demonstrates the approach for a case where we have an analytical posterior for verification.</p>
</div>
<p><strong>Setup</strong>: A quality control inspector tests <span class="math notranslate nohighlight">\(n = 50\)</span> items and finds <span class="math notranslate nohighlight">\(x = 8\)</span> defective. The defect rate <span class="math notranslate nohighlight">\(\theta\)</span> has prior <span class="math notranslate nohighlight">\(\text{Beta}(2, 10)\)</span> (representing prior belief in low defect rates).</p>
<ol class="loweralpha">
<li><p><strong>Posterior derivation</strong>: Show that the posterior is <span class="math notranslate nohighlight">\(\text{Beta}(2 + 8, 10 + 42) = \text{Beta}(10, 52)\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Beta-Binomial Conjugacy</p>
<p>Prior <span class="math notranslate nohighlight">\(\theta \sim \text{Beta}(\alpha, \beta)\)</span> combined with likelihood <span class="math notranslate nohighlight">\(x | \theta \sim \text{Binomial}(n, \theta)\)</span> gives posterior <span class="math notranslate nohighlight">\(\theta | x \sim \text{Beta}(\alpha + x, \beta + n - x)\)</span>.</p>
</div>
</li>
<li><p><strong>Monte Carlo estimation</strong>: Compute the posterior mean <span class="math notranslate nohighlight">\(\mathbb{E}[\theta | x]\)</span> via Monte Carlo with 10,000 samples. Compare to the analytical result <span class="math notranslate nohighlight">\(\alpha_{\text{post}} / (\alpha_{\text{post}} + \beta_{\text{post}})\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Simple Sampling</p>
<p>Use <code class="docutils literal notranslate"><span class="pre">scipy.stats.beta(10,</span> <span class="pre">52).rvs(10000)</span></code> to generate posterior samples. The posterior mean estimate is just <code class="docutils literal notranslate"><span class="pre">np.mean(samples)</span></code>.</p>
</div>
</li>
<li><p><strong>Posterior probability</strong>: Estimate <span class="math notranslate nohighlight">\(P(\theta &gt; 0.2 | x)\)</span> (probability that the true defect rate exceeds 20%) via Monte Carlo. Compare to the analytical result.</p>
<div class="tip admonition">
<p class="admonition-title">Hint: Indicator Function</p>
<p><span class="math notranslate nohighlight">\(P(\theta &gt; 0.2 | x) = \mathbb{E}[\mathbf{1}_{\theta &gt; 0.2} | x] \approx \frac{1}{n}\sum_{i=1}^n \mathbf{1}_{\theta_i &gt; 0.2}\)</span>.</p>
</div>
</li>
<li><p><strong>Credible interval</strong>: Compute a 95% equal-tailed credible interval for <span class="math notranslate nohighlight">\(\theta\)</span> using the 2.5th and 97.5th percentiles of your posterior samples. Compare to the analytical quantiles.</p></li>
<li><p><strong>Sensitivity analysis</strong>: How do the posterior mean and 95% credible interval change if we use a <span class="math notranslate nohighlight">\(\text{Beta}(1, 1)\)</span> (uniform) prior instead? Is the analysis sensitive to prior choice?</p></li>
</ol>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3 solution">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Part (a): Posterior Derivation</strong></p>
<div class="tip admonition">
<p class="admonition-title">Step-by-step Derivation</p>
<p class="sd-card-text"><strong>Prior</strong>: <span class="math notranslate nohighlight">\(\theta \sim \text{Beta}(2, 10)\)</span></p>
<div class="math notranslate nohighlight">
\[\pi(\theta) \propto \theta^{2-1}(1-\theta)^{10-1} = \theta(1-\theta)^9\]</div>
<p class="sd-card-text"><strong>Likelihood</strong>: <span class="math notranslate nohighlight">\(x | \theta \sim \text{Binomial}(50, \theta)\)</span> with <span class="math notranslate nohighlight">\(x = 8\)</span></p>
<div class="math notranslate nohighlight">
\[L(\theta | x = 8) \propto \theta^8(1-\theta)^{42}\]</div>
<p class="sd-card-text"><strong>Posterior</strong> (by Bayes’ theorem):</p>
<div class="math notranslate nohighlight">
\[\begin{split}\pi(\theta | x) &amp;\propto L(\theta | x) \times \pi(\theta) \\
&amp;\propto \theta^8(1-\theta)^{42} \times \theta(1-\theta)^9 \\
&amp;= \theta^9(1-\theta)^{51}\end{split}\]</div>
<p class="sd-card-text">This is <span class="math notranslate nohighlight">\(\text{Beta}(10, 52)\)</span>.</p>
</div>
<p class="sd-card-text"><strong>Parts (b)–(e): Implementation</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Setup</span>
<span class="n">n_trials</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">n_defective</span> <span class="o">=</span> <span class="mi">8</span>

<span class="c1"># Prior: Beta(2, 10)</span>
<span class="n">prior_alpha</span><span class="p">,</span> <span class="n">prior_beta</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">10</span>
<span class="n">prior</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">prior_alpha</span><span class="p">,</span> <span class="n">prior_beta</span><span class="p">)</span>

<span class="c1"># Posterior: Beta(2 + 8, 10 + 42) = Beta(10, 52)</span>
<span class="n">post_alpha</span> <span class="o">=</span> <span class="n">prior_alpha</span> <span class="o">+</span> <span class="n">n_defective</span>
<span class="n">post_beta</span> <span class="o">=</span> <span class="n">prior_beta</span> <span class="o">+</span> <span class="n">n_trials</span> <span class="o">-</span> <span class="n">n_defective</span>
<span class="n">posterior</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">post_alpha</span><span class="p">,</span> <span class="n">post_beta</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;BAYESIAN INFERENCE FOR DEFECT RATE&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Data: </span><span class="si">{</span><span class="n">n_defective</span><span class="si">}</span><span class="s2"> defectives in </span><span class="si">{</span><span class="n">n_trials</span><span class="si">}</span><span class="s2"> trials&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prior: Beta(</span><span class="si">{</span><span class="n">prior_alpha</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">prior_beta</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Prior mean: </span><span class="si">{</span><span class="n">prior</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Posterior: Beta(</span><span class="si">{</span><span class="n">post_alpha</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">post_beta</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Posterior mean (analytical): </span><span class="si">{</span><span class="n">posterior</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Part (b): Monte Carlo estimation of posterior mean</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">theta_samples</span> <span class="o">=</span> <span class="n">posterior</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">mc_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">theta_samples</span><span class="p">)</span>
<span class="n">mc_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">theta_samples</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">65</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MONTE CARLO ESTIMATION&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">65</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Posterior Mean E[θ|x]:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Analytical: </span><span class="si">{</span><span class="n">posterior</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  MC estimate: </span><span class="si">{</span><span class="n">mc_mean</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">mc_se</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Error: </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">mc_mean</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">posterior</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Part (c): P(θ &gt; 0.2 | x)</span>
<span class="n">threshold</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">mc_prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">theta_samples</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">)</span>
<span class="n">analytical_prob</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">posterior</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">threshold</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">P(θ &gt; </span><span class="si">{</span><span class="n">threshold</span><span class="si">}</span><span class="s2"> | x):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Analytical: </span><span class="si">{</span><span class="n">analytical_prob</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  MC estimate: </span><span class="si">{</span><span class="n">mc_prob</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Error: </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">mc_prob</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">analytical_prob</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Part (d): 95% credible interval</span>
<span class="n">mc_ci</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">theta_samples</span><span class="p">,</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">97.5</span><span class="p">])</span>
<span class="n">analytical_ci</span> <span class="o">=</span> <span class="n">posterior</span><span class="o">.</span><span class="n">ppf</span><span class="p">([</span><span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.975</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">95% Credible Interval:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Analytical: (</span><span class="si">{</span><span class="n">analytical_ci</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">analytical_ci</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  MC estimate: (</span><span class="si">{</span><span class="n">mc_ci</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">mc_ci</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># Part (e): Sensitivity to prior choice</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">65</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;SENSITIVITY ANALYSIS: UNIFORM PRIOR&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">65</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Uniform prior: Beta(1, 1)</span>
<span class="n">uniform_prior</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">uniform_post_alpha</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">n_defective</span>
<span class="n">uniform_post_beta</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">n_trials</span> <span class="o">-</span> <span class="n">n_defective</span>
<span class="n">uniform_posterior</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">uniform_post_alpha</span><span class="p">,</span> <span class="n">uniform_post_beta</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">With Beta(1,1) uniform prior:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Posterior: Beta(</span><span class="si">{</span><span class="n">uniform_post_alpha</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">uniform_post_beta</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="n">theta_uniform</span> <span class="o">=</span> <span class="n">uniform_posterior</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="s1">&#39;Quantity&#39;</span><span class="si">:</span><span class="s2">&lt;25</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Informative Prior&#39;</span><span class="si">:</span><span class="s2">&gt;18</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;Uniform Prior&#39;</span><span class="si">:</span><span class="s2">&gt;16</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Posterior mean&#39;</span><span class="si">:</span><span class="s2">&lt;25</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">posterior</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">&gt;18.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">uniform_posterior</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">&gt;16.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;P(θ &gt; 0.2)&#39;</span><span class="si">:</span><span class="s2">&lt;25</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="mi">1</span><span class="o">-</span><span class="n">posterior</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;18.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="mi">1</span><span class="o">-</span><span class="n">uniform_posterior</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;16.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">ci_info</span> <span class="o">=</span> <span class="n">posterior</span><span class="o">.</span><span class="n">ppf</span><span class="p">([</span><span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.975</span><span class="p">])</span>
<span class="n">ci_unif</span> <span class="o">=</span> <span class="n">uniform_posterior</span><span class="o">.</span><span class="n">ppf</span><span class="p">([</span><span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.975</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;95% CI lower&#39;</span><span class="si">:</span><span class="s2">&lt;25</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">ci_info</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;18.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">ci_unif</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;16.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;95% CI upper&#39;</span><span class="si">:</span><span class="s2">&lt;25</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">ci_info</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;18.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">ci_unif</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;16.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Conclusion: With n=50 observations, the posterior is dominated by&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;the likelihood, not the prior. The informative Beta(2,10) prior&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;shifts the posterior mean down slightly (</span><span class="si">{</span><span class="n">posterior</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> vs </span><span class="si">{</span><span class="n">uniform_posterior</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">),&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;but the difference is modest. With more data, priors matter less.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>=================================================================
BAYESIAN INFERENCE FOR DEFECT RATE
=================================================================

Data: 8 defectives in 50 trials
Prior: Beta(2, 10)
  Prior mean: 0.1667
Posterior: Beta(10, 52)
  Posterior mean (analytical): 0.1613

=================================================================
MONTE CARLO ESTIMATION
=================================================================

Posterior Mean E[θ|x]:
  Analytical: 0.161290
  MC estimate: 0.161543 ± 0.000456
  Error: 0.000252

P(θ &gt; 0.2 | x):
  Analytical: 0.148053
  MC estimate: 0.148600
  Error: 0.000547

95% Credible Interval:
  Analytical: (0.0842, 0.2574)
  MC estimate: (0.0846, 0.2567)

=================================================================
SENSITIVITY ANALYSIS: UNIFORM PRIOR
=================================================================

With Beta(1,1) uniform prior:
Posterior: Beta(9, 43)

Quantity                  Informative Prior    Uniform Prior
------------------------------------------------------------
Posterior mean                       0.1613           0.1731
P(θ &gt; 0.2)                           0.1481           0.2282
95% CI lower                         0.0842           0.0885
95% CI upper                         0.2574           0.2769

Conclusion: With n=50 observations, the posterior is dominated by
the likelihood, not the prior. The informative Beta(2,10) prior
shifts the posterior mean down slightly (0.1613 vs 0.1731),
but the difference is modest. With more data, priors matter less.
</pre></div>
</div>
<p class="sd-card-text"><strong>Key Observations:</strong></p>
<ol class="arabic simple">
<li><p class="sd-card-text"><strong>Monte Carlo matches analytical</strong>: All MC estimates agree with analytical values within standard error bounds.</p></li>
<li><p class="sd-card-text"><strong>Posterior probability</strong>: There’s about a 15% chance the true defect rate exceeds 20% under the informative prior, but 23% under the uniform prior.</p></li>
<li><p class="sd-card-text"><strong>Prior sensitivity</strong>: With 50 observations, the prior matters but doesn’t dominate. The informative prior expressing skepticism about high defect rates pulls the posterior down slightly.</p></li>
<li><p class="sd-card-text"><strong>Foundation for MCMC</strong>: When posteriors don’t have analytical forms, Monte Carlo (via MCMC) becomes the only practical approach—previewing Chapters 5–7.</p></li>
</ol>
</div>
</details></div>
</section>
<section id="bringing-it-all-together">
<h2>Bringing It All Together<a class="headerlink" href="#bringing-it-all-together" title="Link to this heading"></a></h2>
<p>These exercises have taken you through the core concepts and practical skills of Monte Carlo integration:</p>
<ol class="arabic simple">
<li><p><strong>Expectation as integration</strong> (Exercise 1): Any integral can be rewritten as an expectation and estimated by averaging random samples.</p></li>
<li><p><strong>Sample size planning</strong> (Exercise 2): The <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> rate determines computational cost—pilot studies help estimate required sample sizes.</p></li>
<li><p><strong>Convergence diagnostics</strong> (Exercise 3): Visual and quantitative diagnostics catch pathologies before they corrupt results.</p></li>
<li><p><strong>Dimension independence</strong> (Exercise 4): Monte Carlo’s convergence rate doesn’t degrade with dimension—its key advantage over deterministic methods.</p></li>
<li><p><strong>Method selection</strong> (Exercise 5): Use quadrature for low-dimensional smooth problems; Monte Carlo for high dimensions, non-smooth integrands, or complex domains.</p></li>
<li><p><strong>Bayesian inference</strong> (Exercise 6): Monte Carlo enables posterior computation even when posteriors are analytically intractable—the foundation for MCMC methods.</p></li>
</ol>
<p>The next sections develop the machinery for generating random samples: pseudo-random number generation, the inverse CDF method, and transformation methods. These tools will make the Monte Carlo estimator <span class="math notranslate nohighlight">\(\hat{I}_n = \frac{1}{n}\sum_{i=1}^n h(X_i)\)</span> practical for a wide range of distributions and applications.</p>
</section>
<section id="id1">
<h2>Bringing It All Together<a class="headerlink" href="#id1" title="Link to this heading"></a></h2>
<p>Monte Carlo integration transforms the ancient problem of computing integrals into the modern practice of sampling and averaging. The method’s power derives from three pillars:</p>
<ol class="arabic simple">
<li><p><strong>Universality</strong>: Any integral can be written as an expected value, and expected values can be estimated by sample means.</p></li>
<li><p><strong>The Law of Large Numbers</strong>: Sample means converge to population means, guaranteeing that Monte Carlo estimators approach the true value.</p></li>
<li><p><strong>The Central Limit Theorem</strong>: The error decreases as <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span>, providing both a convergence rate and a framework for uncertainty quantification via confidence intervals.</p></li>
</ol>
<p>The <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> rate is simultaneously the method’s weakness and its strength. In low dimensions, deterministic quadrature methods converge faster. But in high dimensions, where deterministic methods suffer the curse of dimensionality, Monte Carlo’s dimension-independent rate makes it the only viable option.</p>
<p>The examples in this section—estimating <span class="math notranslate nohighlight">\(\pi\)</span>, computing Gaussian integrals, evaluating posterior means, tackling high-dimensional problems—illustrate the breadth of Monte Carlo applications. The diagnostic tools—running mean plots, standard error decay, autocorrelation checks—equip you to assess whether your simulations are converging properly.</p>
</section>
<section id="transition-to-what-follows">
<h2>Transition to What Follows<a class="headerlink" href="#transition-to-what-follows" title="Link to this heading"></a></h2>
<p>With the foundations of Monte Carlo integration in place, we face a critical question: <em>where do the random samples come from?</em></p>
<p>Throughout this section, we have assumed that generating samples from the target distribution—uniform on <span class="math notranslate nohighlight">\([0, 1]^d\)</span>, standard normal, a posterior distribution—is straightforward. But this assumption hides a mountain of computational machinery.</p>
<p>The <strong>next section</strong> (<a class="reference internal" href="ch2_2-uniform-random-variates.html#ch2-2-uniform-random-variates"><span class="std std-ref">Uniform Random Variates</span></a>) addresses the most fundamental case: generating uniform random numbers. We will see that computers, being deterministic machines, cannot produce “true” randomness—only pseudo-random sequences that pass stringent statistical tests. Understanding how these sequences are generated, what can go wrong, and how to ensure reproducibility is essential for any serious practitioner.</p>
<p>Following that, the <strong>inverse CDF method</strong> (<a class="reference internal" href="ch2_3-inverse-cdf-method.html#ch2-3-inverse-cdf-method"><span class="std std-ref">Inverse CDF Method</span></a>) shows how to transform uniform random numbers into samples from other distributions. If we can compute the inverse cumulative distribution function <span class="math notranslate nohighlight">\(F^{-1}(u)\)</span>, we can generate samples from <span class="math notranslate nohighlight">\(F\)</span> by applying <span class="math notranslate nohighlight">\(F^{-1}\)</span> to uniform random numbers. This elegant technique works for many important distributions—exponential, Weibull, Cauchy—but fails when <span class="math notranslate nohighlight">\(F^{-1}\)</span> has no closed form.</p>
<p>For distributions like the normal, where the inverse CDF is not available analytically, <strong>specialized transformations</strong> like Box-Muller offer efficient alternatives. And for truly complex distributions—posteriors in Bayesian inference, for instance—<strong>rejection sampling</strong> and eventually <strong>Markov chain Monte Carlo</strong> (Part 3) provide the tools to generate the samples that Monte Carlo integration requires.</p>
<p>The story of Monte Carlo methods is thus a story of two interlocking challenges: using random samples to estimate integrals (this section), and generating the random samples in the first place (the sections to come). Master both, and you hold a computational toolkit of extraordinary power.</p>
<div class="important admonition">
<p class="admonition-title">Key Takeaways 📝</p>
<ol class="arabic simple">
<li><p><strong>Core concept</strong>: Monte Carlo integration estimates integrals by rewriting them as expectations and averaging random samples. The Law of Large Numbers guarantees convergence; the Central Limit Theorem provides the <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> rate.</p></li>
<li><p><strong>Historical insight</strong>: Monte Carlo emerged from the Manhattan Project, where Ulam and von Neumann needed to compute neutron transport integrals that resisted analytical attack. The method turned randomness from a nuisance into a computational tool.</p></li>
<li><p><strong>Dimension independence</strong>: The <em>convergence rate</em> <span class="math notranslate nohighlight">\(O(n^{-1/2})\)</span> does not depend on dimension. However, the variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> can depend on dimension and problem formulation—importance sampling and careful design remain important.</p></li>
<li><p><strong>Practical application</strong>: Always report standard errors and confidence intervals. Use pilot studies to estimate variance for sample size planning. Be explicit about whether you’re targeting SE or CI half-width. Monitor convergence visually with running mean plots and standard error decay.</p></li>
<li><p><strong>Method selection</strong>: In 1–3 dimensions with smooth integrands, use deterministic quadrature. In high dimensions or with complex domains, use Monte Carlo. Consider quasi-Monte Carlo for moderate dimensions with smooth functions.</p></li>
<li><p><strong>Outcome alignment</strong>: This section directly addresses Learning Outcome 1 (apply simulation techniques for Monte Carlo integration) and provides the conceptual foundation for all subsequent simulation methods, including variance reduction, importance sampling, and MCMC.</p></li>
</ol>
</div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<div role="list" class="citation-list">
<div class="citation" id="buffonleclerc1777" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>BuffonLeclerc1777<span class="fn-bracket">]</span></span>
<p>Buffon, G.-L. L., Comte de (1777). Essai d’arithmétique morale. <em>Supplément à l’Histoire Naturelle</em>, Vol. 4. Contains the needle problem for estimating π.</p>
</div>
<div class="citation" id="devroye1986" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Devroye1986<span class="fn-bracket">]</span></span>
<p>Devroye, L. (1986). <em>Non-Uniform Random Variate Generation</em>. New York: Springer-Verlag. Available free online at <a class="reference external" href="http://luc.devroye.org/rvbook.html">http://luc.devroye.org/rvbook.html</a></p>
</div>
<div class="citation" id="metropolisulam1949" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MetropolisUlam1949<span class="fn-bracket">]</span></span>
<p>Metropolis, N., and Ulam, S. (1949). The Monte Carlo method. <em>Journal of the American Statistical Association</em>, 44(247), 335–341. The paper that introduced Monte Carlo methods to the broader scientific community.</p>
</div>
<div class="citation" id="robertcasella2004" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>RobertCasella2004<span class="fn-bracket">]</span></span>
<p>Robert, C. P., and Casella, G. (2004). <em>Monte Carlo Statistical Methods</em> (2nd ed.). New York: Springer. The standard graduate reference for Monte Carlo methods.</p>
</div>
<div class="citation" id="welford1962" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Welford1962<span class="fn-bracket">]</span></span>
<p>Welford, B. P. (1962). Note on a method for calculating corrected sums of squares and products. <em>Technometrics</em>, 4(3), 419–420. Introduces the numerically stable one-pass algorithm for variance computation.</p>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Chapter 2: Monte Carlo Simulation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ch2_2-uniform-random-variates.html" class="btn btn-neutral float-right" title="Uniform Random Variates" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>