

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Section 3.6 Chapter 3 Summary &mdash; STAT 418</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=3d0abd52" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT418/Website/part2_frequentist/chapter3/ch3_6-chapter-summary.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=30646c52"></script>
      <script>let toggleHintShow = 'Show solution';</script>
      <script>let toggleHintHide = 'Hide solution';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>window.MathJax = {"options": {"enableMenu": true, "menuOptions": {"settings": {"enrich": true, "speech": true, "braille": true, "collapsible": true, "assistiveMml": false}}}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
      <script src="../../_static/custom.js?v=8718e0ab"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Chapter 4: Resampling Methods" href="../chapter4/index.html" />
    <link rel="prev" title="Section 3.5 Generalized Linear Models" href="ch3_5-generalized-linear-models.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Course Content</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../part1_foundations/index.html">Part I: Foundations of Probability and Computation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part1_foundations/chapter1/index.html">Chapter 1: Statistical Paradigms and Core Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html">Section 1.1 Paradigms of Probability and Statistical Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html#the-mathematical-foundation-kolmogorov-s-axioms">The Mathematical Foundation: Kolmogorov’s Axioms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html#interpretations-of-probability">Interpretations of Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html#statistical-inference-paradigms">Statistical Inference Paradigms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html#historical-and-philosophical-debates">Historical and Philosophical Debates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html#looking-ahead-our-course-focus">Looking Ahead: Our Course Focus</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_1-probability-and-inference-paradigms.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html">Section 1.2 Probability Distributions: Theory and Computation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#from-abstract-foundations-to-concrete-tools">From Abstract Foundations to Concrete Tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#the-python-ecosystem-for-probability">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#introduction-why-probability-distributions-matter">Introduction: Why Probability Distributions Matter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#id1">The Python Ecosystem for Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#continuous-distributions">Continuous Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#additional-important-distributions">Additional Important Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#summary-and-practical-guidelines">Summary and Practical Guidelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#conclusion">Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_2-probability_distributions_review.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html">Section 1.3 Python Random Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html#from-mathematical-distributions-to-computational-samples">From Mathematical Distributions to Computational Samples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html#the-python-ecosystem-at-a-glance">The Python Ecosystem at a Glance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html#understanding-pseudo-random-number-generation">Understanding Pseudo-Random Number Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html#the-standard-library-random-module">The Standard Library: <code class="docutils literal notranslate"><span class="pre">random</span></code> Module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html#numpy-fast-vectorized-random-sampling">NumPy: Fast Vectorized Random Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html#scipy-stats-the-complete-statistical-toolkit">SciPy Stats: The Complete Statistical Toolkit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html#bringing-it-all-together-library-selection-guide">Bringing It All Together: Library Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html#looking-ahead-from-random-numbers-to-monte-carlo-methods">Looking Ahead: From Random Numbers to Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_3-python_random_generation.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_4-chapter-summary.html">Section 1.4 Chapter 1 Summary: Foundations in Place</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_4-chapter-summary.html#the-three-pillars-of-chapter-1">The Three Pillars of Chapter 1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_4-chapter-summary.html#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_4-chapter-summary.html#what-lies-ahead-the-road-to-simulation">What Lies Ahead: The Road to Simulation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_4-chapter-summary.html#chapter-1-exercises-synthesis-problems">Chapter 1 Exercises: Synthesis Problems</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../part1_foundations/chapter1/ch1_4-chapter-summary.html#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Part II: Frequentist Inference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../chapter2/index.html">Chapter 2: Monte Carlo Simulation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html">Section 2.1 Monte Carlo Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#the-historical-development-of-monte-carlo-methods">The Historical Development of Monte Carlo Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#the-core-principle-expectation-as-integration">The Core Principle: Expectation as Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#theoretical-foundations">Theoretical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#variance-estimation-and-confidence-intervals">Variance Estimation and Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#comparison-with-deterministic-methods">Comparison with Deterministic Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#sample-size-determination">Sample Size Determination</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#convergence-diagnostics-and-monitoring">Convergence Diagnostics and Monitoring</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#chapter-2-1-exercises-monte-carlo-fundamentals-mastery">Chapter 2.1 Exercises: Monte Carlo Fundamentals Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_1-monte-carlo-fundamentals.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html">Section 2.2 Uniform Random Variates</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#why-uniform-the-universal-currency-of-randomness">Why Uniform? The Universal Currency of Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#the-paradox-of-computational-randomness">The Paradox of Computational Randomness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#chaotic-dynamical-systems-an-instructive-failure">Chaotic Dynamical Systems: An Instructive Failure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#linear-congruential-generators">Linear Congruential Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#shift-register-generators">Shift-Register Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#the-kiss-generator-combining-strategies">The KISS Generator: Combining Strategies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#modern-generators-mersenne-twister-and-pcg">Modern Generators: Mersenne Twister and PCG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#statistical-testing-of-random-number-generators">Statistical Testing of Random Number Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#chapter-2-2-exercises-uniform-random-variates-mastery">Chapter 2.2 Exercises: Uniform Random Variates Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_2-uniform-random-variates.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html">Section 2.3 Inverse CDF Method</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#mathematical-foundations">Mathematical Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#continuous-distributions-with-closed-form-inverses">Continuous Distributions with Closed-Form Inverses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#numerical-inversion">Numerical Inversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#discrete-distributions">Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#mixed-distributions">Mixed Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#chapter-2-3-exercises-inverse-cdf-method-mastery">Chapter 2.3 Exercises: Inverse CDF Method Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#id1">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#transition-to-what-follows">Transition to What Follows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_3-inverse-cdf-method.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html">Section 2.4 Transformation Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#why-transformation-methods">Why Transformation Methods?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-boxmuller-transform">The Box–Muller Transform</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-polar-marsaglia-method">The Polar (Marsaglia) Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#method-comparison-boxmuller-vs-polar-vs-ziggurat">Method Comparison: Box–Muller vs Polar vs Ziggurat</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-ziggurat-algorithm">The Ziggurat Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#the-clt-approximation-historical">The CLT Approximation (Historical)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#distributions-derived-from-the-normal">Distributions Derived from the Normal</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#multivariate-normal-generation">Multivariate Normal Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#implementation-guidance">Implementation Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#chapter-2-4-exercises-transformation-methods-mastery">Chapter 2.4 Exercises: Transformation Methods Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_4-transformation-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html">Section 2.5 Rejection Sampling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-dartboard-intuition">The Dartboard Intuition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-accept-reject-algorithm">The Accept-Reject Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#efficiency-analysis">Efficiency Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#choosing-the-proposal-distribution">Choosing the Proposal Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#the-squeeze-principle">The Squeeze Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#geometric-example-sampling-from-the-unit-disk">Geometric Example: Sampling from the Unit Disk</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#worked-examples">Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#limitations-and-the-curse-of-dimensionality">Limitations and the Curse of Dimensionality</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#connections-to-other-methods">Connections to Other Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#chapter-2-5-exercises-rejection-sampling-mastery">Chapter 2.5 Exercises: Rejection Sampling Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_5-rejection-sampling.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html">Section 2.6 Variance Reduction Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#the-variance-reduction-paradigm">The Variance Reduction Paradigm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#importance-sampling">Importance Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#control-variates">Control Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#antithetic-variates">Antithetic Variates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#stratified-sampling">Stratified Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#common-random-numbers">Common Random Numbers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#conditional-monte-carlo-raoblackwellization">Conditional Monte Carlo (Rao–Blackwellization)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#combining-variance-reduction-techniques">Combining Variance Reduction Techniques</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#chapter-2-6-exercises-variance-reduction-mastery">Chapter 2.6 Exercises: Variance Reduction Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_6-variance-reduction-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html">Section 2.7 Chapter 2 Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#the-complete-monte-carlo-workflow">The Complete Monte Carlo Workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#quick-reference-tables">Quick Reference Tables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#common-pitfalls-checklist">Common Pitfalls Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#connections-to-later-chapters">Connections to Later Chapters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#learning-outcomes-checklist">Learning Outcomes Checklist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#further-reading-optimization-and-missing-data">Further Reading: Optimization and Missing Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter2/ch2_7-chapter-summary.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Chapter 3: Parametric Inference and Likelihood Methods</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="ch3_1-exponential-families.html">Section 3.1 Exponential Families</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#historical-origins-from-scattered-results-to-unified-theory">Historical Origins: From Scattered Results to Unified Theory</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#the-canonical-exponential-family">The Canonical Exponential Family</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#converting-familiar-distributions">Converting Familiar Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#the-log-partition-function-a-moment-generating-machine">The Log-Partition Function: A Moment-Generating Machine</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#sufficiency-capturing-all-parameter-information">Sufficiency: Capturing All Parameter Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#minimal-sufficiency-and-completeness">Minimal Sufficiency and Completeness</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#conjugate-priors-and-bayesian-inference">Conjugate Priors and Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#exponential-dispersion-models-and-glms">Exponential Dispersion Models and GLMs</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#python-implementation">Python Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#chapter-3-1-exercises-exponential-families-mastery">Chapter 3.1 Exercises: Exponential Families Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_1-exponential-families.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html">Section 3.2 Maximum Likelihood Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#the-likelihood-function">The Likelihood Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#the-score-function">The Score Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#fisher-information">Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#closed-form-maximum-likelihood-estimators">Closed-Form Maximum Likelihood Estimators</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#numerical-optimization-for-mle">Numerical Optimization for MLE</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#asymptotic-properties-of-mles">Asymptotic Properties of MLEs</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#the-cramer-rao-lower-bound">The Cramér-Rao Lower Bound</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#the-invariance-property">The Invariance Property</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#likelihood-based-hypothesis-testing">Likelihood-Based Hypothesis Testing</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#confidence-intervals-from-likelihood">Confidence Intervals from Likelihood</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#connection-to-bayesian-inference">Connection to Bayesian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#chapter-3-2-exercises-maximum-likelihood-estimation-mastery">Chapter 3.2 Exercises: Maximum Likelihood Estimation Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_2-maximum-likelihood-estimation.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch3_3-sampling-variability.html">Section 3.3 Sampling Variability and Variance Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#statistical-estimators-and-their-properties">Statistical Estimators and Their Properties</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#sampling-distributions">Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#the-delta-method">The Delta Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#variance-estimation-methods">Variance Estimation Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#applications-and-worked-examples">Applications and Worked Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_3-sampling-variability.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch3_4-linear-models.html">Section 3.4 Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#matrix-calculus-foundations">Matrix Calculus Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#the-linear-model">The Linear Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#ordinary-least-squares-the-calculus-approach">Ordinary Least Squares: The Calculus Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#ordinary-least-squares-the-geometric-approach">Ordinary Least Squares: The Geometric Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#properties-of-the-ols-estimator">Properties of the OLS Estimator</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#the-gauss-markov-theorem">The Gauss-Markov Theorem</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#estimating-the-error-variance">Estimating the Error Variance</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#distributional-results-under-normality">Distributional Results Under Normality</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#diagnostics-and-model-checking">Diagnostics and Model Checking</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#numerical-stability-qr-decomposition">Numerical Stability: QR Decomposition</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#model-selection-and-information-criteria">Model Selection and Information Criteria</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#regularization-ridge-and-lasso">Regularization: Ridge and LASSO</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#chapter-3-4-exercises-linear-models-mastery">Chapter 3.4 Exercises: Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_4-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="ch3_5-generalized-linear-models.html">Section 3.5 Generalized Linear Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#historical-context-unification-of-regression-methods">Historical Context: Unification of Regression Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#the-glm-framework-three-components">The GLM Framework: Three Components</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#score-equations-and-fisher-information">Score Equations and Fisher Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#iteratively-reweighted-least-squares">Iteratively Reweighted Least Squares</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#logistic-regression-binary-outcomes">Logistic Regression: Binary Outcomes</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#poisson-regression-count-data">Poisson Regression: Count Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#gamma-regression-positive-continuous-data">Gamma Regression: Positive Continuous Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#inference-in-glms-the-testing-triad">Inference in GLMs: The Testing Triad</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#model-diagnostics">Model Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#model-comparison-and-selection">Model Comparison and Selection</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#quasi-likelihood-and-robust-inference">Quasi-Likelihood and Robust Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#further-reading">Further Reading</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#chapter-3-5-exercises-generalized-linear-models-mastery">Chapter 3.5 Exercises: Generalized Linear Models Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="ch3_5-generalized-linear-models.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Section 3.6 Chapter 3 Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#the-parametric-inference-pipeline">The Parametric Inference Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-five-pillars-of-chapter-3">The Five Pillars of Chapter 3</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-the-pillars-connect">How the Pillars Connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="#method-selection-guide">Method Selection Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="#quick-reference-core-formulas">Quick Reference: Core Formulas</a></li>
<li class="toctree-l4"><a class="reference internal" href="#connections-to-future-material">Connections to Future Material</a></li>
<li class="toctree-l4"><a class="reference internal" href="#practical-guidance">Practical Guidance</a></li>
<li class="toctree-l4"><a class="reference internal" href="#final-perspective">Final Perspective</a></li>
<li class="toctree-l4"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../chapter4/index.html">Chapter 4: Resampling Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html">The Sampling Distribution Problem</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#the-fundamental-target-sampling-distributions">The Fundamental Target: Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#historical-development-the-quest-for-sampling-distributions">Historical Development: The Quest for Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#three-routes-to-the-sampling-distribution">Three Routes to the Sampling Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#when-asymptotics-fail-motivating-the-bootstrap">When Asymptotics Fail: Motivating the Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#the-plug-in-principle-theoretical-foundation">The Plug-In Principle: Theoretical Foundation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#computational-perspective-bootstrap-as-monte-carlo">Computational Perspective: Bootstrap as Monte Carlo</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#chapter-4-1-exercises">Chapter 4.1 Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_1-sampling-distribution-problem.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html">The Empirical Distribution and Plug-in Principle</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#the-empirical-cumulative-distribution-function">The Empirical Cumulative Distribution Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#convergence-of-the-empirical-cdf">Convergence of the Empirical CDF</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#parameters-as-statistical-functionals">Parameters as Statistical Functionals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#the-plug-in-principle">The Plug-in Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#when-the-plug-in-principle-fails">When the Plug-in Principle Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#the-bootstrap-idea-in-one-sentence">The Bootstrap Idea in One Sentence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#computational-implementation">Computational Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#section-4-2-exercises-ecdf-and-plug-in-mastery">Section 4.2 Exercises: ECDF and Plug-in Mastery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_2-empirical-distribution-plugin.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html">The Nonparametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#the-bootstrap-principle">The Bootstrap Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-standard-errors">Bootstrap Standard Errors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-bias-estimation">Bootstrap Bias Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-confidence-intervals">Bootstrap Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-for-regression">Bootstrap for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bootstrap-diagnostics">Bootstrap Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#when-bootstrap-fails">When Bootstrap Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_3-nonparametric-bootstrap.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html">Section 4.4: The Parametric Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#the-parametric-bootstrap-principle">The Parametric Bootstrap Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#location-scale-families">Location-Scale Families</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#parametric-bootstrap-for-regression">Parametric Bootstrap for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#confidence-intervals">Confidence Intervals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#model-checking-and-validation">Model Checking and Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#when-parametric-bootstrap-fails">When Parametric Bootstrap Fails</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#parametric-vs-nonparametric-a-decision-framework">Parametric vs. Nonparametric: A Decision Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_4-parametric-bootstrap.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html">Section 4.5: Jackknife Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#historical-context-and-motivation">Historical Context and Motivation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#the-delete-1-jackknife">The Delete-1 Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#jackknife-bias-estimation">Jackknife Bias Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#the-delete-d-jackknife">The Delete-<span class="math notranslate nohighlight">\(d\)</span> Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#jackknife-versus-bootstrap">Jackknife versus Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#the-infinitesimal-jackknife">The Infinitesimal Jackknife</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_5-jackknife-methods.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html">Section 4.6 Bootstrap Hypothesis Testing and Permutation Tests</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#from-confidence-intervals-to-hypothesis-tests">From Confidence Intervals to Hypothesis Tests</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#the-bootstrap-hypothesis-testing-framework">The Bootstrap Hypothesis Testing Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#permutation-tests-exact-tests-under-exchangeability">Permutation Tests: Exact Tests Under Exchangeability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#testing-equality-of-distributions">Testing Equality of Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#bootstrap-tests-for-regression">Bootstrap Tests for Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#bootstrap-vs-classical-tests">Bootstrap vs Classical Tests</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#permutation-vs-bootstrap-choosing-the-right-approach">Permutation vs Bootstrap: Choosing the Right Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#multiple-testing-with-bootstrap">Multiple Testing with Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#practical-considerations">Practical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#bringing-it-all-together">Bringing It All Together</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#exercises">Exercises</a></li>
<li class="toctree-l4"><a class="reference internal" href="../chapter4/ch4_6-bootstrap-hypothesis-testing.html#references">References</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part3_bayesian/index.html">Part III: Bayesian Inference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part3_bayesian/chapter5/index.html">Chapter 5: Bayesian Inference</a><ul class="simple">
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../part4_llms_datascience/index.html">Part IV: Large Language Models in Data Science</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../part4_llms_datascience/chapter6/index.html">Chapter 6: LLMs in Data Science Workflows</a><ul class="simple">
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Part II: Frequentist Inference</a></li>
          <li class="breadcrumb-item"><a href="index.html">Chapter 3: Parametric Inference and Likelihood Methods</a></li>
      <li class="breadcrumb-item active">Section 3.6 Chapter 3 Summary</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/part2_frequentist/chapter3/ch3_6-chapter-summary.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="section-3-6-chapter-3-summary">
<span id="ch3-6-chapter-summary"></span><h1>Section 3.6 Chapter 3 Summary<a class="headerlink" href="#section-3-6-chapter-3-summary" title="Link to this heading"></a></h1>
<p>This chapter developed the complete framework for parametric inference—the art and science of learning about model parameters from data. Starting with the unifying structure of exponential families, we built maximum likelihood estimation theory from first principles, established the statistical foundations for quantifying uncertainty, and extended these ideas to the workhorse models of applied statistics: linear regression and generalized linear models. The result is a coherent toolkit where the same mathematical principles—likelihood, score equations, Fisher information—apply across an extraordinary range of applications.</p>
<section id="the-parametric-inference-pipeline">
<h2>The Parametric Inference Pipeline<a class="headerlink" href="#the-parametric-inference-pipeline" title="Link to this heading"></a></h2>
<p>Every parametric inference problem follows a unified workflow:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>┌─────────────────────────────────────────────────────────────────────────┐
│                    THE PARAMETRIC INFERENCE PIPELINE                    │
└─────────────────────────────────────────────────────────────────────────┘

Stage 1: MODEL             Stage 2: ESTIMATION      Stage 3: UNCERTAINTY
(Section 3.1)              (Section 3.2)            (Section 3.3)
┌──────────────┐           ┌──────────────┐         ┌──────────────┐
│ Choose       │           │ Maximize     │         │ Quantify     │
│ Distribution │           │ Likelihood   │         │ Variability  │
│              │ ──f(x|θ)──│              │ ──θ̂────│              │
│ • Exp Family │→          │ • Score = 0  │→        │ • Fisher I   │
│ • EDM        │           │ • Newton     │         │ • Delta      │
│ • GLM        │           │ • IRLS       │         │ • Sandwich   │
└──────────────┘           └──────────────┘         └──────────────┘
       │                                                   │
       └───────────────────────┬───────────────────────────┘
                               ↓
                   Stage 4: INFERENCE
                   (Sections 3.4-3.5)
                   ┌─────────────────────────────────────────┐
                   │ • Confidence Intervals                  │
                   │ • Hypothesis Tests (LRT, Wald, Score)   │
                   │ • Model Comparison                      │
                   │ • Diagnostics &amp; Validation              │
                   └─────────────────────────────────────────┘
</pre></div>
</div>
<p><strong>Stage 1 — Model Selection</strong>: Identify the appropriate probability model for your data. The exponential family (Section 3.1) provides a unified framework encompassing Normal, Poisson, Bernoulli, Gamma, and many other distributions. The canonical form <span class="math notranslate nohighlight">\(f(x|\eta) = h(x)\exp\{\eta^\top T(x) - A(\eta)\}\)</span> reveals sufficient statistics, connects moments to derivatives of <span class="math notranslate nohighlight">\(A(\eta)\)</span>, and guarantees concave log-likelihoods in the natural parameter <span class="math notranslate nohighlight">\(\eta\)</span>.</p>
<p><strong>Stage 2 — Parameter Estimation</strong>: Find the parameter values that best explain the observed data. Maximum likelihood estimation (Section 3.2) provides a principled approach: maximize <span class="math notranslate nohighlight">\(L(\theta) = \prod_i f(x_i|\theta)\)</span> or equivalently solve the score equation <span class="math notranslate nohighlight">\(U(\theta) = 0\)</span>. For exponential families, this reduces to matching observed sufficient statistics to their expectations: <span class="math notranslate nohighlight">\(\bar{T} = \nabla A(\hat{\eta})\)</span>. Newton-Raphson and Fisher scoring provide computational algorithms.</p>
<p><strong>Stage 3 — Uncertainty Quantification</strong>: A point estimate alone is incomplete; we need standard errors and confidence intervals. Sampling variability theory (Section 3.3) shows that <span class="math notranslate nohighlight">\(\sqrt{n}(\hat{\theta} - \theta_0) \xrightarrow{d} \mathcal{N}(0, I_1(\theta_0)^{-1})\)</span>. The delta method extends this to transformed parameters. Fisher information, observed information, and sandwich estimators provide variance estimates under different assumptions.</p>
<p><strong>Stage 4 — Statistical Inference</strong>: Apply the fitted model to answer scientific questions. Linear models (Section 3.4) enable regression analysis with t-tests and F-tests under Gauss-Markov optimality. Generalized linear models (Section 3.5) extend these ideas to binary, count, and other non-normal responses through the link function and IRLS algorithm.</p>
</section>
<section id="the-five-pillars-of-chapter-3">
<h2>The Five Pillars of Chapter 3<a class="headerlink" href="#the-five-pillars-of-chapter-3" title="Link to this heading"></a></h2>
<p><strong>Pillar 1: Exponential Families (Section 3.1)</strong></p>
<p>The exponential family provides the mathematical foundation for all that follows. Its canonical form:</p>
<div class="math notranslate nohighlight">
\[f(x|\eta) = h(x) \exp\left\{ \eta^\top T(x) - A(\eta) \right\}\]</div>
<p>yields extraordinary theoretical and computational benefits:</p>
<ul class="simple">
<li><p><strong>Sufficient statistics</strong> <span class="math notranslate nohighlight">\(T(X)\)</span> capture all information about <span class="math notranslate nohighlight">\(\eta\)</span>—the Neyman-Fisher factorization theorem ensures no information loss when we reduce data to <span class="math notranslate nohighlight">\(T\)</span>.</p></li>
<li><p><strong>Moment generation</strong> via the log-partition function: <span class="math notranslate nohighlight">\(\mathbb{E}[T(X)] = \nabla A(\eta)\)</span> and <span class="math notranslate nohighlight">\(\text{Cov}[T(X)] = \nabla^2 A(\eta)\)</span>. No distribution-specific derivations needed.</p></li>
<li><p><strong>Fisher information</strong> equals the Hessian of the log-partition function: <span class="math notranslate nohighlight">\(I_1(\eta) = \nabla^2 A(\eta)\)</span> per observation. For a sample of size <span class="math notranslate nohighlight">\(n\)</span>, <span class="math notranslate nohighlight">\(I_n(\eta) = n\nabla^2 A(\eta)\)</span>. The convexity of <span class="math notranslate nohighlight">\(A(\eta)\)</span> guarantees positive definiteness and ensures concave log-likelihoods in <span class="math notranslate nohighlight">\(\eta\)</span>.</p></li>
<li><p><strong>Conjugate priors</strong> take the form <span class="math notranslate nohighlight">\(\pi(\eta) \propto \exp\{\eta^\top \nu_0 - n_0 A(\eta)\}\)</span>, enabling closed-form Bayesian updating.</p></li>
</ul>
<p>The exponential dispersion family extends this framework with a dispersion parameter <span class="math notranslate nohighlight">\(\phi\)</span>, creating the foundation for generalized linear models.</p>
<p><strong>Pillar 2: Maximum Likelihood Estimation (Section 3.2)</strong></p>
<p>Maximum likelihood is the workhorse of parametric inference. Given data <span class="math notranslate nohighlight">\(x_1, \ldots, x_n\)</span>, the MLE <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> maximizes:</p>
<div class="math notranslate nohighlight">
\[L(\theta) = \prod_{i=1}^n f(x_i|\theta) \quad \text{or equivalently} \quad \ell(\theta) = \sum_{i=1}^n \log f(x_i|\theta)\]</div>
<p>Key theoretical results:</p>
<ul class="simple">
<li><p><strong>Consistency</strong>: <span class="math notranslate nohighlight">\(\hat{\theta}_n \xrightarrow{P} \theta_0\)</span> as <span class="math notranslate nohighlight">\(n \to \infty\)</span></p></li>
<li><p><strong>Asymptotic normality</strong>: <span class="math notranslate nohighlight">\(\sqrt{n}(\hat{\theta}_n - \theta_0) \xrightarrow{d} \mathcal{N}(0, I_1(\theta_0)^{-1})\)</span></p></li>
<li><p><strong>Asymptotic efficiency</strong>: MLEs achieve the Cramér-Rao lower bound, <span class="math notranslate nohighlight">\(\text{Var}(\hat{\theta}) \geq [nI_1(\theta)]^{-1}\)</span></p></li>
<li><p><strong>Invariance</strong>: If <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is MLE of <span class="math notranslate nohighlight">\(\theta\)</span>, then <span class="math notranslate nohighlight">\(g(\hat{\theta})\)</span> is MLE of <span class="math notranslate nohighlight">\(g(\theta)\)</span></p></li>
</ul>
<p>For exponential families, the score equation <span class="math notranslate nohighlight">\(U(\hat{\eta}) = 0\)</span> reduces to the moment-matching condition <span class="math notranslate nohighlight">\(\nabla A(\hat{\eta}) = \bar{T}\)</span>, providing elegant closed-form solutions for many common distributions.</p>
<p><strong>Pillar 3: Sampling Variability (Section 3.3)</strong></p>
<p>Understanding how estimators vary across samples is essential for valid inference:</p>
<ul class="simple">
<li><p><strong>Bias</strong>: <span class="math notranslate nohighlight">\(\text{Bias}(\hat{\theta}) = \mathbb{E}[\hat{\theta}] - \theta\)</span>. MLEs are generally biased but asymptotically unbiased.</p></li>
<li><p><strong>Variance</strong>: <span class="math notranslate nohighlight">\(\text{Var}(\hat{\theta}) = \mathbb{E}[(\hat{\theta} - \mathbb{E}[\hat{\theta}])^2]\)</span>. Decreases with sample size as <span class="math notranslate nohighlight">\(O(1/n)\)</span>.</p></li>
<li><p><strong>Mean Squared Error</strong>: <span class="math notranslate nohighlight">\(\text{MSE} = \text{Bias}^2 + \text{Variance}\)</span>. The fundamental bias-variance tradeoff.</p></li>
<li><p><strong>Consistency</strong>: <span class="math notranslate nohighlight">\(\hat{\theta}_n \xrightarrow{P} \theta_0\)</span>. Estimators converge to the truth.</p></li>
</ul>
<p>The <strong>delta method</strong> propagates uncertainty through transformations:</p>
<div class="math notranslate nohighlight">
\[\sqrt{n}(g(\hat{\theta}) - g(\theta_0)) \xrightarrow{d} \mathcal{N}\left(0, [g'(\theta_0)]^2 \cdot I_1(\theta_0)^{-1}\right)\]</div>
<p>For multivariate parameters:</p>
<div class="math notranslate nohighlight">
\[\sqrt{n}(g(\hat{\boldsymbol{\theta}}) - g(\boldsymbol{\theta}_0)) \xrightarrow{d} \mathcal{N}\left(0, \nabla g(\boldsymbol{\theta}_0)^\top I_1(\boldsymbol{\theta}_0)^{-1} \nabla g(\boldsymbol{\theta}_0)\right)\]</div>
<p>Variance estimation methods include Fisher information (theoretical), observed information (data-adaptive), and sandwich estimators (robust to misspecification).</p>
<p><strong>Pillar 4: Linear Models (Section 3.4)</strong></p>
<p>Linear regression applies the likelihood framework to continuous responses:</p>
<div class="math notranslate nohighlight">
\[Y_i = \mathbf{x}_i^\top \boldsymbol{\beta} + \varepsilon_i, \quad \varepsilon_i \sim \mathcal{N}(0, \sigma^2)\]</div>
<p>The <strong>OLS estimator</strong> minimizes sum of squared errors:</p>
<div class="math notranslate nohighlight">
\[\hat{\boldsymbol{\beta}} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}\]</div>
<p>Key theoretical results:</p>
<ul class="simple">
<li><p><strong>Gauss-Markov Theorem</strong>: Under assumptions of linearity, exogeneity, homoskedasticity, and no perfect multicollinearity, OLS is BLUE (Best Linear Unbiased Estimator).</p></li>
<li><p><strong>Distributional theory</strong>: <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}} \sim \mathcal{N}(\boldsymbol{\beta}, \sigma^2(\mathbf{X}^\top\mathbf{X})^{-1})\)</span> under normality.</p></li>
<li><p><strong>Hypothesis testing</strong>: t-tests for individual coefficients, F-tests for nested model comparison.</p></li>
<li><p><strong>Residual analysis</strong>: Diagnostic tools detect violations of assumptions.</p></li>
</ul>
<p>The geometric interpretation—OLS projects <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> onto the column space of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>—provides deep insight into the estimator’s properties.</p>
<p><strong>Pillar 5: Generalized Linear Models (Section 3.5)</strong></p>
<p>GLMs extend linear regression to non-normal responses through three components:</p>
<ol class="arabic simple">
<li><p><strong>Random component</strong>: Response <span class="math notranslate nohighlight">\(Y_i\)</span> from exponential dispersion family with mean <span class="math notranslate nohighlight">\(\mu_i\)</span></p></li>
<li><p><strong>Systematic component</strong>: Linear predictor <span class="math notranslate nohighlight">\(\eta_i = \mathbf{x}_i^\top \boldsymbol{\beta}\)</span></p></li>
<li><p><strong>Link function</strong>: <span class="math notranslate nohighlight">\(g(\mu_i) = \eta_i\)</span> connecting mean to linear predictor</p></li>
</ol>
<table class="docutils align-default" id="id1">
<caption><span class="caption-number">Table 36 </span><span class="caption-text">Common GLM Configurations</span><a class="headerlink" href="#id1" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 25.0%" />
<col style="width: 35.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Response Type</p></th>
<th class="head"><p>Distribution</p></th>
<th class="head"><p>Canonical Link</p></th>
<th class="head"><p>Application</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Binary</p></td>
<td><p>Bernoulli</p></td>
<td><p>Logit: <span class="math notranslate nohighlight">\(\log\frac{\mu}{1-\mu}\)</span></p></td>
<td><p>Classification, propensity</p></td>
</tr>
<tr class="row-odd"><td><p>Count</p></td>
<td><p>Poisson</p></td>
<td><p>Log: <span class="math notranslate nohighlight">\(\log \mu\)</span></p></td>
<td><p>Event rates, frequencies</p></td>
</tr>
<tr class="row-even"><td><p>Continuous+</p></td>
<td><p>Gamma</p></td>
<td><p>Reciprocal: <span class="math notranslate nohighlight">\(-1/\mu\)</span></p></td>
<td><p>Duration, costs, insurance</p></td>
</tr>
<tr class="row-odd"><td><p>Continuous</p></td>
<td><p>Normal</p></td>
<td><p>Identity: <span class="math notranslate nohighlight">\(\mu\)</span></p></td>
<td><p>Standard regression</p></td>
</tr>
</tbody>
</table>
<p>The <strong>IRLS algorithm</strong> (Iteratively Reweighted Least Squares) provides unified computation:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Initialize: β⁽⁰⁾
Repeat until convergence:
  1. Compute η⁽ᵗ⁾ = Xβ⁽ᵗ⁾
  2. Compute μ⁽ᵗ⁾ = g⁻¹(η⁽ᵗ⁾)
  3. Compute working weights W⁽ᵗ⁾ and working response z⁽ᵗ⁾
  4. Update: β⁽ᵗ⁺¹⁾ = (X&#39;W⁽ᵗ⁾X)⁻¹X&#39;W⁽ᵗ⁾z⁽ᵗ⁾
Return: β̂
</pre></div>
</div>
<p>This algorithm is Fisher scoring applied to the GLM likelihood—the same principles from Section 3.2 yield a unified estimation procedure for all exponential family responses.</p>
</section>
<section id="how-the-pillars-connect">
<h2>How the Pillars Connect<a class="headerlink" href="#how-the-pillars-connect" title="Link to this heading"></a></h2>
<p>These five pillars form an integrated framework where each builds on the others:</p>
<p><strong>Exponential families enable elegant MLE.</strong> For an exponential family, the score equation becomes <span class="math notranslate nohighlight">\(\nabla A(\hat{\eta}) = \bar{T}\)</span>—match expected sufficient statistics to observed values. This moment-matching principle yields closed-form solutions for Normal, Poisson, Exponential, and many other distributions. When closed forms don’t exist, the concavity of the log-likelihood in <span class="math notranslate nohighlight">\(\eta\)</span> (guaranteed by convexity of <span class="math notranslate nohighlight">\(A\)</span>) ensures that Newton’s method converges to the unique global maximum.</p>
<p><strong>MLE theory provides sampling distributions.</strong> The asymptotic normality theorem <span class="math notranslate nohighlight">\(\sqrt{n}(\hat{\theta} - \theta_0) \xrightarrow{d} \mathcal{N}(0, I_1^{-1})\)</span> is proven using properties of the score function and Fisher information. This theorem transforms point estimates into confidence intervals and hypothesis tests.</p>
<p><strong>Sampling variability extends to transformations.</strong> The delta method converts asymptotic normality of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> into asymptotic normality of <span class="math notranslate nohighlight">\(g(\hat{\theta})\)</span>. This enables inference on interpretable quantities like odds ratios (logistic regression), rate ratios (Poisson regression), and elasticities (log-transformed regression).</p>
<p><strong>Linear models are normal-theory GLMs.</strong> The classical linear model <span class="math notranslate nohighlight">\(Y \sim \mathcal{N}(\mathbf{X}\boldsymbol{\beta}, \sigma^2 I)\)</span> is a GLM with normal distribution, identity link, and estimated dispersion. OLS and MLE coincide. The Gauss-Markov theorem provides finite-sample optimality that complements MLE’s asymptotic efficiency.</p>
<p><strong>GLMs unify regression for all exponential families.</strong> Logistic, Poisson, and Gamma regression—previously treated as separate topics—are all special cases of the same framework. IRLS is Fisher scoring specialized to GLMs. Deviance analysis parallels F-tests. The canonical link ensures that <span class="math notranslate nohighlight">\(\eta = \theta\)</span>, making the connection to exponential family theory explicit.</p>
<div class="note admonition">
<p class="admonition-title">Example 💡 Integrated Analysis</p>
<p><strong>Problem</strong>: Analyze whether study hours predict exam pass/fail for 200 students.</p>
<p><strong>Stage 1 (Model)</strong>: Binary response → Bernoulli distribution → Exponential family with <span class="math notranslate nohighlight">\(\theta = \log\frac{p}{1-p}\)</span> and <span class="math notranslate nohighlight">\(A(\theta) = \log(1 + e^\theta)\)</span>.</p>
<p><strong>Stage 2 (Estimation)</strong>: Logistic regression via IRLS. The canonical link equates <span class="math notranslate nohighlight">\(\eta_i = \mathbf{x}_i^\top \boldsymbol{\beta}\)</span> to <span class="math notranslate nohighlight">\(\theta_i\)</span>, so the score equation is <span class="math notranslate nohighlight">\(\mathbf{X}^\top(\mathbf{y} - \boldsymbol{\mu}) = \mathbf{0}\)</span>.</p>
<p><strong>Stage 3 (Uncertainty)</strong>: Fisher information <span class="math notranslate nohighlight">\(I(\boldsymbol{\beta}) = \mathbf{X}^\top \mathbf{W} \mathbf{X}\)</span> where <span class="math notranslate nohighlight">\(W_{ii} = \mu_i(1-\mu_i)\)</span>. Standard errors from <span class="math notranslate nohighlight">\([\hat{I}(\hat{\boldsymbol{\beta}})]^{-1}\)</span>.</p>
<p><strong>Stage 4 (Inference)</strong>: Wald test for <span class="math notranslate nohighlight">\(H_0: \beta_{\text{hours}} = 0\)</span>. Odds ratio <span class="math notranslate nohighlight">\(e^{\hat{\beta}_{\text{hours}}}\)</span> with 95% CI via delta method. Deviance test comparing to null model.</p>
<p><strong>Code</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>

<span class="c1"># Simulated data</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">hours</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">prob_pass</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">hours</span><span class="p">)))</span>
<span class="n">passed</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">prob_pass</span><span class="p">)</span>

<span class="c1"># Fit logistic regression</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">hours</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">passed</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Binomial</span><span class="p">())</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Intercept: </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (SE: </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">bse</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Hours coef: </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> (SE: </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">bse</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Odds ratio per hour: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;95% CI for OR: (</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">conf_int</span><span class="p">()[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, &quot;</span>
      <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">conf_int</span><span class="p">()[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Output</strong>:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Intercept: -2.687 (SE: 0.537)
Hours coef: 0.432 (SE: 0.074)
Odds ratio per hour: 1.540
95% CI for OR: (1.332, 1.781)
</pre></div>
</div>
</div>
</section>
<section id="method-selection-guide">
<h2>Method Selection Guide<a class="headerlink" href="#method-selection-guide" title="Link to this heading"></a></h2>
<p>Use these decision frameworks to choose appropriate methods:</p>
<p><strong>Choosing the Distribution (Random Component)</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>What type of response variable?
│
├─► Continuous, unbounded?
│   └─► Normal distribution (identity link)
│
├─► Binary (0/1)?
│   └─► Bernoulli distribution (logit link)
│
├─► Count (0, 1, 2, ...)?
│   ├─► Mean ≈ Variance? → Poisson (log link)
│   └─► Variance &gt; Mean? → Negative Binomial or Quasi-Poisson
│
├─► Proportion (0 to 1)?
│   └─► Beta distribution (logit link)
│
├─► Strictly positive continuous?
│   ├─► Constant CV? → Gamma (log link)
│   └─► CV increases with mean? → Inverse Gaussian
│
└─► Bounded continuous [a, b]?
    └─► Transform to (0,1), use Beta
</pre></div>
</div>
<p><strong>Choosing the Variance Estimator</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>What assumptions can you make?
│
├─► Model correctly specified, large n?
│   └─► Fisher Information (theoretical)
│       SE = 1/√(nI₁(θ̂))
│
├─► Model correct, prefer data-adaptive?
│   └─► Observed Information
│       SE = 1/√J(θ̂) where J = -∂²ℓ/∂θ²
│
├─► Possible misspecification?
│   └─► Sandwich Estimator (robust)
│       Var(θ̂) = A⁻¹BA⁻¹ where A = E[-∂²ℓ/∂θ²], B = E[(∂ℓ/∂θ)²]
│
└─► Small sample, exact inference needed?
    └─► Bootstrap (Chapter 4)
</pre></div>
</div>
<p><strong>Choosing the Hypothesis Test</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Testing H₀: θ = θ₀?
│
├─► Large n, quick computation?
│   └─► Wald Test: W = (θ̂ - θ₀)² / Var(θ̂) ~ χ²₁
│
├─► Better finite-sample properties?
│   └─► Likelihood Ratio Test: LRT = 2[ℓ(θ̂) - ℓ(θ₀)] ~ χ²₁
│
└─► Only need to evaluate at θ₀?
    └─► Score Test: S = U(θ₀)² / I(θ₀) ~ χ²₁
</pre></div>
</div>
</section>
<section id="quick-reference-core-formulas">
<h2>Quick Reference: Core Formulas<a class="headerlink" href="#quick-reference-core-formulas" title="Link to this heading"></a></h2>
<p><strong>Exponential Family</strong></p>
<table class="docutils align-default">
<colgroup>
<col style="width: 35.0%" />
<col style="width: 65.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Quantity</p></th>
<th class="head"><p>Formula</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Canonical form</p></td>
<td><p><span class="math notranslate nohighlight">\(f(x|\eta) = h(x)\exp\{\eta^\top T(x) - A(\eta)\}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Mean of sufficient statistic</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbb{E}[T(X)] = \nabla A(\eta)\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Variance of sufficient statistic</p></td>
<td><p><span class="math notranslate nohighlight">\(\text{Cov}[T(X)] = \nabla^2 A(\eta)\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Fisher information</p></td>
<td><p><span class="math notranslate nohighlight">\(I(\eta) = \nabla^2 A(\eta)\)</span></p></td>
</tr>
<tr class="row-even"><td><p>MLE condition</p></td>
<td><p><span class="math notranslate nohighlight">\(\nabla A(\hat{\eta}) = \bar{T}\)</span></p></td>
</tr>
</tbody>
</table>
<p><strong>Maximum Likelihood Estimation</strong></p>
<table class="docutils align-default">
<colgroup>
<col style="width: 35.0%" />
<col style="width: 65.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Quantity</p></th>
<th class="head"><p>Formula</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Log-likelihood</p></td>
<td><p><span class="math notranslate nohighlight">\(\ell(\theta) = \sum_{i=1}^n \log f(x_i|\theta)\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Score function</p></td>
<td><p><span class="math notranslate nohighlight">\(U(\theta) = \nabla \ell(\theta)\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Fisher information (per obs)</p></td>
<td><p><span class="math notranslate nohighlight">\(I_1(\theta) = \mathbb{E}[U_1(\theta)^2] = -\mathbb{E}[\nabla^2 \log f(X|\theta)]\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Asymptotic variance</p></td>
<td><p><span class="math notranslate nohighlight">\(\text{Var}(\hat{\theta}) \approx [nI_1(\theta)]^{-1}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Cramér-Rao bound</p></td>
<td><p><span class="math notranslate nohighlight">\(\text{Var}(\hat{\theta}) \geq [nI_1(\theta)]^{-1}\)</span></p></td>
</tr>
</tbody>
</table>
<p><strong>Sampling Variability</strong></p>
<table class="docutils align-default">
<colgroup>
<col style="width: 35.0%" />
<col style="width: 65.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Quantity</p></th>
<th class="head"><p>Formula</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Bias</p></td>
<td><p><span class="math notranslate nohighlight">\(\text{Bias}(\hat{\theta}) = \mathbb{E}[\hat{\theta}] - \theta\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Mean squared error</p></td>
<td><p><span class="math notranslate nohighlight">\(\text{MSE}(\hat{\theta}) = \text{Var}(\hat{\theta}) + \text{Bias}(\hat{\theta})^2\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Delta method (scalar)</p></td>
<td><p><span class="math notranslate nohighlight">\(\text{Var}(g(\hat{\theta})) \approx [g'(\theta)]^2 \text{Var}(\hat{\theta})\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Delta method (vector)</p></td>
<td><p><span class="math notranslate nohighlight">\(\text{Var}(g(\hat{\boldsymbol{\theta}})) \approx \nabla g^\top \boldsymbol{\Sigma} \nabla g\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Sandwich estimator</p></td>
<td><p><span class="math notranslate nohighlight">\(\hat{\text{Var}}(\hat{\theta}) = \hat{A}^{-1}\hat{B}\hat{A}^{-1}\)</span></p></td>
</tr>
</tbody>
</table>
<p><strong>Linear Models</strong></p>
<table class="docutils align-default">
<colgroup>
<col style="width: 35.0%" />
<col style="width: 65.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Quantity</p></th>
<th class="head"><p>Formula</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>OLS estimator</p></td>
<td><p><span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Variance of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\text{Cov}(\hat{\boldsymbol{\beta}}) = \sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Residual variance estimator</p></td>
<td><p><span class="math notranslate nohighlight">\(\hat{\sigma}^2 = \frac{\mathbf{e}^\top\mathbf{e}}{n-p} = \frac{\text{RSS}}{n-p}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>t-statistic for <span class="math notranslate nohighlight">\(\beta_j\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(t = \frac{\hat{\beta}_j - \beta_{j,0}}{\text{SE}(\hat{\beta}_j)} \sim t_{n-p}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>F-statistic (nested models)</p></td>
<td><p><span class="math notranslate nohighlight">\(F = \frac{(\text{RSS}_R - \text{RSS}_F)/(p_F - p_R)}{\text{RSS}_F/(n-p_F)} \sim F_{p_F-p_R, n-p_F}\)</span></p></td>
</tr>
</tbody>
</table>
<p><strong>Generalized Linear Models</strong></p>
<table class="docutils align-default">
<colgroup>
<col style="width: 35.0%" />
<col style="width: 65.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Quantity</p></th>
<th class="head"><p>Formula</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Mean-variance relation</p></td>
<td><p><span class="math notranslate nohighlight">\(\text{Var}(Y_i) = \phi \cdot V(\mu_i)\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Link function</p></td>
<td><p><span class="math notranslate nohighlight">\(g(\mu_i) = \eta_i = \mathbf{x}_i^\top\boldsymbol{\beta}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Score equation (canonical link)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{X}^\top(\mathbf{y} - \boldsymbol{\mu}) = \mathbf{0}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Score equation (general link)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{W}\mathbf{G}(\mathbf{y} - \boldsymbol{\mu}) = \mathbf{0}\)</span> where <span class="math notranslate nohighlight">\(G_{ii} = \frac{d\eta_i}{d\mu_i}\)</span>, <span class="math notranslate nohighlight">\(W_{ii} = \frac{1}{V(\mu_i)}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>IRLS update</p></td>
<td><p><span class="math notranslate nohighlight">\(\boldsymbol{\beta}^{(t+1)} = (\mathbf{X}^\top\mathbf{W}^{(t)}\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{W}^{(t)}\mathbf{z}^{(t)}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Deviance</p></td>
<td><p><span class="math notranslate nohighlight">\(D = 2[\ell(\hat{\boldsymbol{\mu}}_{\text{saturated}}) - \ell(\hat{\boldsymbol{\mu}}_{\text{model}})]\)</span>; scaled deviance <span class="math notranslate nohighlight">\(D^* = D/\phi\)</span></p></td>
</tr>
</tbody>
</table>
</section>
<section id="connections-to-future-material">
<h2>Connections to Future Material<a class="headerlink" href="#connections-to-future-material" title="Link to this heading"></a></h2>
<p>The parametric inference framework developed in this chapter provides essential foundations for the remainder of the course.</p>
<section id="bootstrap-methods-chapter-4">
<h3>Bootstrap Methods (Chapter 4)<a class="headerlink" href="#bootstrap-methods-chapter-4" title="Link to this heading"></a></h3>
<p>Chapter 4 develops resampling methods that complement and extend parametric inference:</p>
<ul class="simple">
<li><p><strong>When parametric assumptions fail</strong>: The bootstrap provides valid standard errors and confidence intervals without distributional assumptions. When the model is misspecified, bootstrap variance estimates can be more reliable than Fisher information.</p></li>
<li><p><strong>Complex statistics</strong>: For statistics without tractable asymptotic distributions (medians, ratios, eigenvalues), the bootstrap provides a general-purpose solution where delta method approximations may be poor.</p></li>
<li><p><strong>Small samples</strong>: Asymptotic normality may be inadequate for small <span class="math notranslate nohighlight">\(n\)</span>. Bootstrap percentile and BCa intervals can have better coverage than Wald intervals.</p></li>
<li><p><strong>Model validation</strong>: Cross-validation, a resampling technique, assesses predictive performance and guards against overfitting—complementing the in-sample fit measures (deviance, AIC) introduced in this chapter.</p></li>
</ul>
<p><strong>Connection to Chapter 3</strong>: The parametric bootstrap samples from <span class="math notranslate nohighlight">\(\hat{F}_\theta = F_{\hat{\theta}}\)</span>—the fitted parametric model. It combines the efficiency of parametric assumptions with the flexibility of resampling. Understanding MLE and Fisher information enables comparison of bootstrap and asymptotic standard errors.</p>
</section>
<section id="bayesian-inference-chapter-5">
<h3>Bayesian Inference (Chapter 5)<a class="headerlink" href="#bayesian-inference-chapter-5" title="Link to this heading"></a></h3>
<p>Chapter 5 presents the Bayesian alternative to maximum likelihood:</p>
<ul class="simple">
<li><p><strong>Prior distributions</strong>: Where MLE treats <span class="math notranslate nohighlight">\(\theta\)</span> as fixed but unknown, Bayesian inference places a prior distribution <span class="math notranslate nohighlight">\(\pi(\theta)\)</span> on parameters. Conjugate priors for exponential families (Section 3.1) enable closed-form posteriors.</p></li>
<li><p><strong>Posterior inference</strong>: Bayes’ theorem yields <span class="math notranslate nohighlight">\(\pi(\theta|x) \propto L(\theta)\pi(\theta)\)</span>—the posterior combines likelihood with prior. MLE emerges as the posterior mode under a flat prior.</p></li>
<li><p><strong>Credible intervals vs. confidence intervals</strong>: Bayesian 95% credible intervals contain the parameter with 95% posterior probability. Frequentist 95% confidence intervals cover the true parameter in 95% of repeated samples. The interpretations differ philosophically but often coincide numerically.</p></li>
<li><p><strong>MCMC computation</strong>: When posteriors lack closed forms, Markov chain Monte Carlo provides samples from <span class="math notranslate nohighlight">\(\pi(\theta|x)\)</span>. The Metropolis-Hastings algorithm uses proposal distributions analogous to importance sampling (Chapter 2).</p></li>
</ul>
<p><strong>Connection to Chapter 3</strong>: Fisher information determines the asymptotic variance of both MLE and the posterior. Under regularity conditions, the Bernstein-von Mises theorem shows <span class="math notranslate nohighlight">\(\pi(\theta|x) \to \mathcal{N}(\hat{\theta}_{\text{MLE}}, [nI_1]^{-1})\)</span>. Bayesian and frequentist inference converge as <span class="math notranslate nohighlight">\(n \to \infty\)</span>.</p>
</section>
<section id="llms-and-modern-methods-chapters-13-15">
<h3>LLMs and Modern Methods (Chapters 13-15)<a class="headerlink" href="#llms-and-modern-methods-chapters-13-15" title="Link to this heading"></a></h3>
<p>The parametric foundations extend to modern machine learning contexts:</p>
<ul class="simple">
<li><p><strong>Logistic regression in classification</strong>: Logistic regression (GLM with Bernoulli response) remains the interpretable baseline for binary classification. Understanding its likelihood foundation enables principled comparison with neural networks.</p></li>
<li><p><strong>Regularization</strong>: Ridge and LASSO regression add penalty terms to the log-likelihood, corresponding to specific prior distributions in the Bayesian interpretation.</p></li>
<li><p><strong>Uncertainty quantification</strong>: Modern deep learning increasingly emphasizes uncertainty estimates. The Fisher information and delta method concepts from Chapter 3 inform variance propagation through neural networks.</p></li>
</ul>
</section>
</section>
<section id="practical-guidance">
<h2>Practical Guidance<a class="headerlink" href="#practical-guidance" title="Link to this heading"></a></h2>
<p><strong>Best Practices for Parametric Inference</strong></p>
<ol class="arabic simple">
<li><p><strong>Visualize first</strong>: Plot your data before fitting models. Histograms reveal distributional shape; scatterplots show relationships; residual plots expose assumption violations.</p></li>
<li><p><strong>Check assumptions systematically</strong>: For linear models, verify linearity (residuals vs. fitted), homoskedasticity (scale-location plot), normality (Q-Q plot), and influential points (Cook’s distance).</p></li>
<li><p><strong>Use robust standard errors by default</strong>: The sandwich estimator protects against misspecification with minimal efficiency loss when the model is correct.</p></li>
<li><p><strong>Report confidence intervals, not just p-values</strong>: Intervals convey both statistical significance and practical magnitude. A significant effect may be too small to matter; a non-significant effect may have a wide interval consistent with meaningful effects.</p></li>
<li><p><strong>Consider model comparison holistically</strong>: Likelihood ratio tests compare nested models; AIC and BIC enable comparison of non-nested models; cross-validation assesses predictive performance.</p></li>
<li><p><strong>Mind the sample size</strong>: Asymptotic results require “large enough” <span class="math notranslate nohighlight">\(n\)</span>. For small samples with binary outcomes, consider exact methods or Firth’s penalized likelihood for separation issues.</p></li>
</ol>
<p><strong>Common Pitfalls to Avoid</strong></p>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ⚠️ Interpreting Coefficients Incorrectly</p>
<p>GLM coefficients are on the link scale, not the response scale:</p>
<ul class="simple">
<li><p><strong>Logistic</strong>: <span class="math notranslate nohighlight">\(\beta_j\)</span> is the log-odds ratio, not the change in probability</p></li>
<li><p><strong>Poisson</strong>: <span class="math notranslate nohighlight">\(\beta_j\)</span> is the log-rate ratio, not the change in count</p></li>
<li><p><strong>Linear</strong>: <span class="math notranslate nohighlight">\(\beta_j\)</span> is the actual change in response (identity link)</p></li>
</ul>
<p>Transform coefficients appropriately for interpretation: <span class="math notranslate nohighlight">\(e^{\beta_j}\)</span> gives odds ratios (logistic) or rate ratios (Poisson).</p>
</div>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ⚠️ Ignoring Overdispersion</p>
<p>For count and binary data, the variance may exceed what the model predicts. Symptoms include:</p>
<ul class="simple">
<li><p>Residual deviance much larger than degrees of freedom</p></li>
<li><p>Many observations with large Pearson residuals</p></li>
</ul>
<p>Remedies: quasi-likelihood methods, negative binomial (for counts), or beta-binomial (for proportions).</p>
</div>
<div class="warning admonition">
<p class="admonition-title">Common Pitfall ⚠️ Perfect Separation in Logistic Regression</p>
<p>When a predictor perfectly separates classes (all successes above some threshold), MLE does not exist—coefficients diverge to <span class="math notranslate nohighlight">\(\pm\infty\)</span>. Symptoms include:</p>
<ul class="simple">
<li><p>Extremely large coefficient estimates</p></li>
<li><p>Huge standard errors</p></li>
<li><p>Convergence warnings</p></li>
</ul>
<p>Remedies: Firth’s penalized likelihood, Bayesian approaches with proper priors, or regularization (ridge/LASSO).</p>
</div>
</section>
<section id="final-perspective">
<h2>Final Perspective<a class="headerlink" href="#final-perspective" title="Link to this heading"></a></h2>
<p>Parametric inference embodies a fundamental tradeoff: we assume a probability model to gain statistical power and interpretability, but suffer if the model is wrong. The exponential family provides the mathematical structure that makes parametric inference tractable—sufficient statistics compress data without information loss, log-partition functions generate moments automatically, and concave log-likelihoods ensure well-behaved optimization.</p>
<p>The techniques in this chapter form the statistical backbone of data science:</p>
<ol class="arabic simple">
<li><p><strong>Model</strong> data with exponential families (Section 3.1)</p></li>
<li><p><strong>Estimate</strong> parameters via maximum likelihood (Section 3.2)</p></li>
<li><p><strong>Quantify</strong> uncertainty through sampling variability (Section 3.3)</p></li>
<li><p><strong>Analyze</strong> continuous responses with linear regression (Section 3.4)</p></li>
<li><p><strong>Extend</strong> to non-normal responses with GLMs (Section 3.5)</p></li>
</ol>
<p>This toolkit is not merely academic. Every logistic regression in click-through prediction, every Poisson model in epidemiology, every linear regression in econometrics relies on these principles. The bootstrap (Chapter 4) provides robustness when parametric assumptions fail. Bayesian methods (Chapter 5) offer an alternative philosophical foundation with distinct computational techniques. Modern machine learning (Chapters 13-15) builds on these foundations while relaxing some assumptions in exchange for flexibility.</p>
<p>Master parametric inference, and you hold the key to principled statistical modeling. The combination of theoretical rigor and computational tractability makes these methods indispensable—equally valuable for understanding why methods work and for applying them in practice.</p>
<div class="important admonition">
<p class="admonition-title">Key Takeaways 📝</p>
<ol class="arabic simple">
<li><p><strong>The Unifying Framework</strong>: Exponential families provide a common mathematical structure for diverse distributions. The canonical form <span class="math notranslate nohighlight">\(f(x|\eta) = h(x)\exp\{\eta^\top T(x) - A(\eta)\}\)</span> yields sufficient statistics, moment formulas, and Fisher information from a single framework.</p></li>
<li><p><strong>The Estimation Principle</strong>: Maximum likelihood chooses parameters that make observed data most probable. For exponential families, this reduces to matching sufficient statistics: <span class="math notranslate nohighlight">\(\nabla A(\hat{\eta}) = \bar{T}\)</span>. Asymptotic theory guarantees consistency, normality, and efficiency.</p></li>
<li><p><strong>The Uncertainty Machinery</strong>: Sampling variability quantifies estimator precision. The delta method propagates uncertainty through transformations. Fisher information, observed information, and sandwich estimators provide variance estimates under different assumptions.</p></li>
<li><p><strong>The Model Extensions</strong>: Linear models provide optimal estimation under Gauss-Markov conditions. GLMs extend to binary, count, and other non-normal responses through the link function. IRLS provides unified computation across all exponential family responses.</p></li>
<li><p><strong>The Course Connections</strong>: This chapter provides the likelihood foundations for bootstrap methods (Chapter 4, where parametric bootstrap uses <span class="math notranslate nohighlight">\(\hat{F}_{\hat{\theta}}\)</span>), Bayesian inference (Chapter 5, where posterior ∝ likelihood × prior), and modern ML (where cross-entropy loss is negative log-likelihood). [LO 1, 2, 4]</p></li>
</ol>
</div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<p><strong>Foundational Works on Maximum Likelihood (Section 3.2)</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="fisher1912" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Fisher1912<span class="fn-bracket">]</span></span>
<p>Fisher, R. A. (1912). On an absolute criterion for fitting frequency curves. <em>Messenger of Mathematics</em>, 41, 155–160. Fisher’s earliest work on maximum likelihood.</p>
</div>
<div class="citation" id="fisher1922" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Fisher1922<span class="fn-bracket">]</span></span>
<p>Fisher, R. A. (1922). On the mathematical foundations of theoretical statistics. <em>Philosophical Transactions of the Royal Society A</em>, 222, 309–368. The foundational paper introducing maximum likelihood, sufficiency, and efficiency—concepts that remain central to statistical inference.</p>
</div>
<div class="citation" id="fisher1925" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Fisher1925<span class="fn-bracket">]</span></span>
<p>Fisher, R. A. (1925). Theory of statistical estimation. <em>Mathematical Proceedings of the Cambridge Philosophical Society</em>, 22(5), 700–725. Develops the asymptotic theory of maximum likelihood including asymptotic normality and efficiency.</p>
</div>
<div class="citation" id="rao1945" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Rao1945<span class="fn-bracket">]</span></span>
<p>Rao, C. R. (1945). Information and the accuracy attainable in the estimation of statistical parameters. <em>Bulletin of the Calcutta Mathematical Society</em>, 37, 81–89. Independently establishes the information inequality (Cramér-Rao bound).</p>
</div>
<div class="citation" id="cramer1946" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Cramer1946<span class="fn-bracket">]</span></span>
<p>Cramér, H. (1946). <em>Mathematical Methods of Statistics</em>. Princeton University Press. Classic synthesis of statistical theory including rigorous treatment of the Cramér-Rao inequality.</p>
</div>
</div>
<p><strong>Exponential Families (Section 3.1)</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="koopman1936" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Koopman1936<span class="fn-bracket">]</span></span>
<p>Koopman, B. O. (1936). On distributions admitting a sufficient statistic. <em>Transactions of the American Mathematical Society</em>, 39(3), 399–409. One of the three independent proofs of the Pitman-Koopman-Darmois theorem.</p>
</div>
<div class="citation" id="pitman1936" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Pitman1936<span class="fn-bracket">]</span></span>
<p>Pitman, E. J. G. (1936). Sufficient statistics and intrinsic accuracy. <em>Proceedings of the Cambridge Philosophical Society</em>, 32(4), 567–579. Establishes the theorem connecting sufficiency to exponential family structure.</p>
</div>
<div class="citation" id="darmois1935" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Darmois1935<span class="fn-bracket">]</span></span>
<p>Darmois, G. (1935). Sur les lois de probabilité à estimation exhaustive. <em>Comptes Rendus de l’Académie des Sciences</em>, 200, 1265–1266. The French contribution to the simultaneous independent discovery.</p>
</div>
<div class="citation" id="brown1986" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Brown1986<span class="fn-bracket">]</span></span>
<p>Brown, L. D. (1986). <em>Fundamentals of Statistical Exponential Families with Applications in Statistical Decision Theory</em>. Institute of Mathematical Statistics Lecture Notes–Monograph Series, Vol. 9. The definitive mathematical reference on exponential families.</p>
</div>
<div class="citation" id="barndorffnielsen1978" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>BarndorffNielsen1978<span class="fn-bracket">]</span></span>
<p>Barndorff-Nielsen, O. E. (1978). <em>Information and Exponential Families in Statistical Theory</em>. Wiley. Comprehensive treatment emphasizing information geometry.</p>
</div>
</div>
<p><strong>Sampling Variability and Robust Methods (Section 3.3)</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="huber1967" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Huber1967<span class="fn-bracket">]</span></span>
<p>Huber, P. J. (1967). The behavior of maximum likelihood estimates under nonstandard conditions. In <em>Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</em>, Vol. 1, 221–233. University of California Press. Foundational work introducing the sandwich variance estimator.</p>
</div>
<div class="citation" id="white1980" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>White1980<span class="fn-bracket">]</span></span>
<p>White, H. (1980). A heteroskedasticity-consistent covariance matrix estimator and a direct test for heteroskedasticity. <em>Econometrica</em>, 48(4), 817–838. Develops heteroskedasticity-consistent standard errors now standard in regression.</p>
</div>
<div class="citation" id="white1982" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>White1982<span class="fn-bracket">]</span></span>
<p>White, H. (1982). Maximum likelihood estimation of misspecified models. <em>Econometrica</em>, 50(1), 1–25. Establishes quasi-maximum likelihood theory under model misspecification.</p>
</div>
<div class="citation" id="serfling1980" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Serfling1980<span class="fn-bracket">]</span></span>
<p>Serfling, R. J. (1980). <em>Approximation Theorems of Mathematical Statistics</em>. Wiley. Comprehensive treatment of the delta method and asymptotic approximations.</p>
</div>
<div class="citation" id="efron1978" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Efron1978<span class="fn-bracket">]</span></span>
<p>Efron, B., and Hinkley, D. V. (1978). Assessing the accuracy of the maximum likelihood estimator: Observed versus expected Fisher information. <em>Biometrika</em>, 65(3), 457–487. Compares information-based variance estimation methods.</p>
</div>
</div>
<p><strong>Linear Models (Section 3.4)</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="legendre1805" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Legendre1805<span class="fn-bracket">]</span></span>
<p>Legendre, A. M. (1805). <em>Nouvelles méthodes pour la détermination des orbites des comètes</em>. Firmin Didot, Paris. First published account of the method of least squares.</p>
</div>
<div class="citation" id="gauss1809" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Gauss1809<span class="fn-bracket">]</span></span>
<p>Gauss, C. F. (1809). <em>Theoria Motus Corporum Coelestium in Sectionibus Conicis Solem Ambientium</em>. Perthes and Besser, Hamburg. Gauss’s probabilistic justification of least squares under normally distributed errors.</p>
</div>
<div class="citation" id="gauss1821" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Gauss1821<span class="fn-bracket">]</span></span>
<p>Gauss, C. F. (1821–1823). <em>Theoria Combinationis Observationum Erroribus Minimis Obnoxiae</em>. English translation by G. W. Stewart (1995), SIAM. Proves the original Gauss-Markov theorem without assuming normality.</p>
</div>
<div class="citation" id="aitken1935" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Aitken1935<span class="fn-bracket">]</span></span>
<p>Aitken, A. C. (1935). On least squares and linear combination of observations. <em>Proceedings of the Royal Society of Edinburgh</em>, 55, 42–48. Generalizes least squares to handle correlated errors.</p>
</div>
<div class="citation" id="cochran1934" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Cochran1934<span class="fn-bracket">]</span></span>
<p>Cochran, W. G. (1934). The distribution of quadratic forms in a normal system. <em>Mathematical Proceedings of the Cambridge Philosophical Society</em>, 30(2), 178–191. Cochran’s theorem fundamental for F-tests and ANOVA.</p>
</div>
<div class="citation" id="cook1977" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Cook1977<span class="fn-bracket">]</span></span>
<p>Cook, R. D. (1977). Detection of influential observation in linear regression. <em>Technometrics</em>, 19(1), 15–18. Introduces Cook’s distance for identifying influential observations.</p>
</div>
</div>
<p><strong>Generalized Linear Models (Section 3.5)</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="nelderwedderburn1972" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>NelderWedderburn1972<span class="fn-bracket">]</span></span>
<p>Nelder, J. A., and Wedderburn, R. W. M. (1972). Generalized linear models. <em>Journal of the Royal Statistical Society, Series A</em>, 135(3), 370–384. The foundational paper introducing the unified GLM framework and IRLS algorithm.</p>
</div>
<div class="citation" id="wedderburn1974" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Wedderburn1974<span class="fn-bracket">]</span></span>
<p>Wedderburn, R. W. M. (1974). Quasi-likelihood functions, generalized linear models, and the Gauss-Newton method. <em>Biometrika</em>, 61(3), 439–447. Extends GLM theory to quasi-likelihood.</p>
</div>
<div class="citation" id="mccullaghnelder1989" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>McCullaghNelder1989<span class="fn-bracket">]</span></span>
<p>McCullagh, P., and Nelder, J. A. (1989). <em>Generalized Linear Models</em> (2nd ed.). Chapman and Hall. The definitive reference on GLM theory covering exponential dispersion models, deviance, and diagnostics.</p>
</div>
<div class="citation" id="firth1993" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Firth1993<span class="fn-bracket">]</span></span>
<p>Firth, D. (1993). Bias reduction of maximum likelihood estimates. <em>Biometrika</em>, 80(1), 27–38. Introduces penalized likelihood to address separation problems in logistic regression.</p>
</div>
<div class="citation" id="hosmer2013" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Hosmer2013<span class="fn-bracket">]</span></span>
<p>Hosmer, D. W., Lemeshow, S., and Sturdivant, R. X. (2013). <em>Applied Logistic Regression</em> (3rd ed.). Wiley. Comprehensive applied treatment of logistic regression.</p>
</div>
</div>
<p><strong>Asymptotic Theory</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="wald1949" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Wald1949<span class="fn-bracket">]</span></span>
<p>Wald, A. (1949). Note on the consistency of the maximum likelihood estimate. <em>Annals of Mathematical Statistics</em>, 20(4), 595–601. Establishes conditions for MLE consistency.</p>
</div>
<div class="citation" id="lecam1953" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LeCam1953<span class="fn-bracket">]</span></span>
<p>Le Cam, L. (1953). On some asymptotic properties of maximum likelihood estimates. <em>University of California Publications in Statistics</em>, 1, 277–329. Establishes local asymptotic normality.</p>
</div>
<div class="citation" id="vandervaart1998" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>VanDerVaart1998<span class="fn-bracket">]</span></span>
<p>van der Vaart, A. W. (1998). <em>Asymptotic Statistics</em>. Cambridge University Press. Definitive modern treatment of asymptotic statistical theory.</p>
</div>
<div class="citation" id="lehmanncasella1998" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LehmannCasella1998<span class="fn-bracket">]</span></span>
<p>Lehmann, E. L., and Casella, G. (1998). <em>Theory of Point Estimation</em> (2nd ed.). Springer. Graduate-level treatment including comprehensive coverage of exponential families.</p>
</div>
</div>
<p><strong>Comprehensive Texts</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="casellaberger2002" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CasellaBerger2002<span class="fn-bracket">]</span></span>
<p>Casella, G., and Berger, R. L. (2002). <em>Statistical Inference</em> (2nd ed.). Duxbury Press. Accessible introduction to sufficiency, exponential families, and inference.</p>
</div>
<div class="citation" id="efronhastie2016" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>EfronHastie2016<span class="fn-bracket">]</span></span>
<p>Efron, B., and Hastie, T. (2016). <em>Computer Age Statistical Inference</em>. Cambridge University Press. Contemporary perspective integrating classical and modern methods.</p>
</div>
<div class="citation" id="wasserman2004" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Wasserman2004<span class="fn-bracket">]</span></span>
<p>Wasserman, L. (2004). <em>All of Statistics: A Concise Course in Statistical Inference</em>. Springer. Modern introduction to statistical inference.</p>
</div>
<div class="citation" id="davison2003" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Davison2003<span class="fn-bracket">]</span></span>
<p>Davison, A. C. (2003). <em>Statistical Models</em>. Cambridge University Press. Modern treatment including information-based standard errors.</p>
</div>
<div class="citation" id="dobson2018" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Dobson2018<span class="fn-bracket">]</span></span>
<p>Dobson, A. J., and Barnett, A. G. (2018). <em>An Introduction to Generalized Linear Models</em> (4th ed.). CRC Press. Accessible introduction to GLMs.</p>
</div>
</div>
<p><strong>Historical Perspectives</strong></p>
<div role="list" class="citation-list">
<div class="citation" id="stigler1986" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Stigler1986<span class="fn-bracket">]</span></span>
<p>Stigler, S. M. (1986). <em>The History of Statistics: The Measurement of Uncertainty before 1900</em>. Harvard University Press. Historical context for the development of statistical methods.</p>
</div>
<div class="citation" id="hand2015" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Hand2015<span class="fn-bracket">]</span></span>
<p>Hand, D. J. (2015). From evidence to understanding: A commentary on Fisher (1922). <em>Philosophical Transactions of the Royal Society A</em>, 373(2039), 20140249. Modern perspective on Fisher’s foundational contributions.</p>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="ch3_5-generalized-linear-models.html" class="btn btn-neutral float-left" title="Section 3.5 Generalized Linear Models" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../chapter4/index.html" class="btn btn-neutral float-right" title="Chapter 4: Resampling Methods" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>