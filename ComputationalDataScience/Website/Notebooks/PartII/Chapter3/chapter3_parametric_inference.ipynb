{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Parametric Inference and Likelihood Methods\n",
    "\n",
    "**STAT 418: Computational Methods in Data Science**  \n",
    "**Dr. Timothy Reese | Purdue University ‚Äî Spring 2026**\n",
    "\n",
    "---\n",
    "\n",
    "This chapter develops the complete framework for parametric inference‚Äîlearning about model parameters from data.\n",
    "\n",
    "| Section | Topic | Key Methods |\n",
    "|---------|-------|-------------|\n",
    "| 3.1 | Exponential Families | Canonical form, log-partition function, sufficiency |\n",
    "| 3.2 | Maximum Likelihood | Score function, Fisher information, Newton-Raphson |\n",
    "| 3.3 | Sampling Variability | Bias, variance, delta method, sandwich estimators |\n",
    "| 3.4 | Linear Models | OLS, Gauss-Markov, t-tests, F-tests |\n",
    "| 3.5 | Generalized Linear Models | Link functions, IRLS, deviance, diagnostics |\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. **Convert** common distributions to canonical exponential family form\n",
    "2. **Derive** MLEs analytically and implement numerical optimization\n",
    "3. **Quantify** estimator uncertainty via Fisher information and delta method\n",
    "4. **Apply** the Gauss-Markov theorem for linear models\n",
    "5. **Implement** GLMs via IRLS and diagnose model fit\n",
    "6. **Compare** Wald, likelihood ratio, and score tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "CHAPTER 3: PARAMETRIC INFERENCE - ENVIRONMENT CHECK\n",
      "=================================================================\n",
      "NumPy: 1.26.4\n",
      "SciPy: 1.17.0\n",
      "Pandas: 2.3.3\n",
      "‚úì All imports successful!\n",
      "‚úì Plotting configuration complete!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CHAPTER 3: PARAMETRIC INFERENCE - UNIFIED IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import default_rng, SeedSequence\n",
    "from numpy.linalg import inv, solve, eigvals, norm, cholesky, qr, svd\n",
    "\n",
    "import scipy\n",
    "from scipy import stats, integrate, special, linalg\n",
    "from scipy.special import (logsumexp, gammaln, digamma, polygamma,\n",
    "                           gamma as gamma_func, beta as beta_func,\n",
    "                           comb, factorial, expit, logit)\n",
    "from scipy.optimize import minimize, minimize_scalar, brentq, newton, fsolve\n",
    "from scipy.stats import (norm as normal_dist, t as t_dist, chi2, f as f_dist,\n",
    "                         poisson, binom, bernoulli, expon, gamma as gamma_dist,\n",
    "                         beta as beta_dist, nbinom, uniform)\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.genmod.generalized_linear_model import GLM\n",
    "from statsmodels.genmod import families\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib\n",
    "import warnings\n",
    "from typing import Callable, Tuple, Dict, List, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# Version check\n",
    "print(\"=\"*65)\n",
    "print(\"CHAPTER 3: PARAMETRIC INFERENCE - ENVIRONMENT CHECK\")\n",
    "print(\"=\"*65)\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"SciPy: {scipy.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(\"‚úì All imports successful!\")\n",
    "\n",
    "# Plotting config\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "except:\n",
    "    plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "plt.rcParams.update({'figure.figsize': (10, 6), 'font.size': 11, 'figure.dpi': 100})\n",
    "COLORS = {'primary': '#1f77b4', 'secondary': '#ff7f0e', 'success': '#2ca02c', \n",
    "          'danger': '#d62728', 'purple': '#9467bd', 'gray': '#7f7f7f'}\n",
    "MASTER_SEED = 42\n",
    "print(\"‚úì Plotting configuration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Introduction to statsmodels\n",
    "\n",
    "This chapter makes extensive use of **statsmodels**, a powerful Python library for statistical modeling that we have not previously used in this course. This section provides essential background.\n",
    "\n",
    "## What is statsmodels?\n",
    "\n",
    "[**statsmodels**](https://www.statsmodels.org/) is a Python package that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests and statistical data exploration. It complements **scipy.stats** by offering:\n",
    "\n",
    "- **Full model objects** with comprehensive summaries (coefficients, standard errors, p-values, confidence intervals)\n",
    "- **Diagnostic tools** for model checking and validation\n",
    "- **Multiple inference frameworks** (Wald, LRT, Score tests)\n",
    "- **R-style formula interface** for model specification\n",
    "\n",
    "The package is released under the open source Modified BSD (3-clause) license and is built on top of NumPy, SciPy, and pandas.\n",
    "\n",
    "## Key Features Used in This Chapter\n",
    "\n",
    "| Module | Purpose | Key Classes/Functions |\n",
    "|--------|---------|----------------------|\n",
    "| `statsmodels.api` | Main API | `OLS`, `WLS`, `GLM`, `add_constant` |\n",
    "| `statsmodels.genmod.families` | GLM distributions | `Gaussian`, `Binomial`, `Poisson`, `Gamma` |\n",
    "| `statsmodels.genmod.families.links` | Link functions | `Log`, `Logit`, `Identity`, `InversePower` |\n",
    "| Robust covariance | Robust SEs | `cov_type='HC0'`, `'HC1'`, `'HC3'`, `'HAC'` |\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "pip install statsmodels\n",
    "# or\n",
    "conda install statsmodels\n",
    "```\n",
    "\n",
    "## Basic Usage Pattern\n",
    "\n",
    "statsmodels follows a consistent **model ‚Üí fit ‚Üí results** pattern:\n",
    "\n",
    "```python\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 1. Prepare data (add intercept manually for array interface)\n",
    "X = sm.add_constant(X_data)\n",
    "\n",
    "# 2. Instantiate model\n",
    "model = sm.OLS(y, X)  # or sm.GLM(y, X, family=...)\n",
    "\n",
    "# 3. Fit model\n",
    "results = model.fit()\n",
    "\n",
    "# 4. Access results\n",
    "print(results.summary())  # Comprehensive output\n",
    "results.params            # Coefficient estimates\n",
    "results.bse               # Standard errors\n",
    "results.pvalues           # p-values\n",
    "results.conf_int()        # Confidence intervals\n",
    "```\n",
    "\n",
    "## Documentation Links\n",
    "\n",
    "- **Main Documentation**: [https://www.statsmodels.org/stable/](https://www.statsmodels.org/stable/)\n",
    "- **GLM Documentation**: [https://www.statsmodels.org/stable/glm.html](https://www.statsmodels.org/stable/glm.html)\n",
    "- **API Reference**: [https://www.statsmodels.org/stable/api.html](https://www.statsmodels.org/stable/api.html)\n",
    "- **GitHub Repository**: [https://github.com/statsmodels/statsmodels](https://github.com/statsmodels/statsmodels)\n",
    "\n",
    "## Citation\n",
    "\n",
    "If using statsmodels in academic work:\n",
    "\n",
    "> Seabold, Skipper, and Josef Perktold. \"statsmodels: Econometric and statistical modeling with Python.\" *Proceedings of the 9th Python in Science Conference.* 2010.\n",
    "\n",
    "## Why statsmodels vs. scipy.stats?\n",
    "\n",
    "| Feature | scipy.stats | statsmodels |\n",
    "|---------|-------------|-------------|\n",
    "| MLE fitting | Basic (`.fit()`) | Full inference (SE, CI, tests) |\n",
    "| Model summary | None | Comprehensive `.summary()` |\n",
    "| GLM support | None | Full exponential family |\n",
    "| Robust SEs | None | HC0-HC3, HAC (Newey-West) |\n",
    "| Diagnostics | Limited | Residuals, influence, VIF |\n",
    "| Time series | None | ARIMA, VAR, state space |\n",
    "\n",
    "For Chapter 3, we use statsmodels to:\n",
    "1. **Verify our implementations** against a trusted reference\n",
    "2. **Access robust standard errors** (HC, HAC) for Gauss-Markov violation demos\n",
    "3. **Fit GLMs** with proper IRLS implementation and diagnostics\n",
    "4. **Perform hypothesis tests** (Wald, LRT, Score) with minimal code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STATSMODELS DEMONSTRATION\n",
      "======================================================================\n",
      "\n",
      "1. ORDINARY LEAST SQUARES\n",
      "--------------------------------------------------\n",
      "Coefficients: [1.89739945 3.01532592]\n",
      "Std Errors:   [0.40524764 0.07266589]\n",
      "R-squared:    0.9462\n",
      "\n",
      "2. ROBUST STANDARD ERRORS\n",
      "--------------------------------------------------\n",
      "Model-based SE(slope): 0.0727\n",
      "HC3 robust SE(slope):  0.0708\n",
      "\n",
      "3. GENERALIZED LINEAR MODEL (Logistic)\n",
      "--------------------------------------------------\n",
      "Coefficients: [-1.25396175  0.58015244]\n",
      "Odds ratios:  [0.28537198 1.78631071]\n",
      "Deviance:     83.78\n",
      "\n",
      "4. COMPREHENSIVE SUMMARY\n",
      "--------------------------------------------------\n",
      "Call results.summary() for full output including:\n",
      "  - R¬≤, Adj R¬≤, AIC, BIC\n",
      "  - Coefficients with SE, t-stat, p-value, CI\n",
      "  - Omnibus, Durbin-Watson, Jarque-Bera tests\n",
      "\n",
      "======================================================================\n",
      "See https://www.statsmodels.org for complete documentation\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STATSMODELS QUICK DEMONSTRATION\n",
    "# =============================================================================\n",
    "\n",
    "def statsmodels_demo():\n",
    "    \"\"\"Demonstrate key statsmodels features used in this chapter.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"STATSMODELS DEMONSTRATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    rng = default_rng(42)\n",
    "    n = 100\n",
    "    \n",
    "    # Generate simple regression data\n",
    "    x = rng.uniform(0, 10, n)\n",
    "    y = 2 + 3*x + rng.normal(0, 2, n)\n",
    "    X = sm.add_constant(x)  # Adds column of 1s for intercept\n",
    "    \n",
    "    print(\"\\n1. ORDINARY LEAST SQUARES\")\n",
    "    print(\"-\"*50)\n",
    "    ols_model = sm.OLS(y, X)\n",
    "    ols_results = ols_model.fit()\n",
    "    \n",
    "    print(f\"Coefficients: {ols_results.params}\")\n",
    "    print(f\"Std Errors:   {ols_results.bse}\")\n",
    "    print(f\"R-squared:    {ols_results.rsquared:.4f}\")\n",
    "    \n",
    "    print(\"\\n2. ROBUST STANDARD ERRORS\")\n",
    "    print(\"-\"*50)\n",
    "    ols_hc = sm.OLS(y, X).fit(cov_type='HC3')  # Heteroskedasticity-robust\n",
    "    print(f\"Model-based SE(slope): {ols_results.bse[1]:.4f}\")\n",
    "    print(f\"HC3 robust SE(slope):  {ols_hc.bse[1]:.4f}\")\n",
    "    \n",
    "    print(\"\\n3. GENERALIZED LINEAR MODEL (Logistic)\")\n",
    "    print(\"-\"*50)\n",
    "    # Binary outcome\n",
    "    p = expit(-1 + 0.5*x)\n",
    "    y_binary = rng.binomial(1, p)\n",
    "    \n",
    "    glm_model = sm.GLM(y_binary, X, family=families.Binomial())\n",
    "    glm_results = glm_model.fit()\n",
    "    \n",
    "    print(f\"Coefficients: {glm_results.params}\")\n",
    "    print(f\"Odds ratios:  {np.exp(glm_results.params)}\")\n",
    "    print(f\"Deviance:     {glm_results.deviance:.2f}\")\n",
    "    \n",
    "    print(\"\\n4. COMPREHENSIVE SUMMARY\")\n",
    "    print(\"-\"*50)\n",
    "    print(\"Call results.summary() for full output including:\")\n",
    "    print(\"  - R¬≤, Adj R¬≤, AIC, BIC\")\n",
    "    print(\"  - Coefficients with SE, t-stat, p-value, CI\")\n",
    "    print(\"  - Omnibus, Durbin-Watson, Jarque-Bera tests\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"See https://www.statsmodels.org for complete documentation\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "statsmodels_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3.1: Exponential Families\n",
    "\n",
    "The **exponential family** unifies most distributions:\n",
    "\n",
    "$$f(x|\\eta) = h(x) \\exp\\left\\{ \\eta^\\top T(x) - A(\\eta) \\right\\}$$\n",
    "\n",
    "**Key Properties:**\n",
    "- $\\mathbb{E}[T(X)] = \\nabla A(\\eta)$ ‚Äî moments from derivatives\n",
    "- $\\text{Cov}[T(X)] = \\nabla^2 A(\\eta)$ ‚Äî Fisher information = Hessian\n",
    "- Concave log-likelihood in $\\eta$ ‚Äî no local minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPONENTIAL FAMILY: MOMENT GENERATION FROM LOG-PARTITION\n",
      "======================================================================\n",
      "\n",
      "POISSON DISTRIBUTION: Œ∑ = log(Œª), A(Œ∑) = exp(Œ∑)\n",
      "--------------------------------------------------\n",
      "\n",
      "Œª = 2.0:\n",
      "  Theory: E[X] = A'(Œ∑) = 2.0000, Var(X) = A''(Œ∑) = 2.0000\n",
      "  Sample: mean = 2.0019, var = 2.0067\n",
      "\n",
      "Œª = 5.0:\n",
      "  Theory: E[X] = A'(Œ∑) = 5.0000, Var(X) = A''(Œ∑) = 5.0000\n",
      "  Sample: mean = 4.9972, var = 4.9405\n",
      "\n",
      "Œª = 10.0:\n",
      "  Theory: E[X] = A'(Œ∑) = 10.0000, Var(X) = A''(Œ∑) = 10.0000\n",
      "  Sample: mean = 9.9992, var = 9.9161\n",
      "\n",
      "======================================================================\n",
      "‚úì Log-partition function generates all moments!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3.1 Exponential Family Demonstration\n",
    "# =============================================================================\n",
    "\n",
    "def demonstrate_exponential_families():\n",
    "    \"\"\"Show canonical forms and verify moment-generating property.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"EXPONENTIAL FAMILY: MOMENT GENERATION FROM LOG-PARTITION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Poisson example: Œ∑ = log(Œª), A(Œ∑) = exp(Œ∑)\n",
    "    print(\"\\nPOISSON DISTRIBUTION: Œ∑ = log(Œª), A(Œ∑) = exp(Œ∑)\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    rng = default_rng(42)\n",
    "    for lam in [2.0, 5.0, 10.0]:\n",
    "        eta = np.log(lam)\n",
    "        A_prime = np.exp(eta)      # E[X] = A'(Œ∑)\n",
    "        A_double_prime = np.exp(eta)  # Var(X) = A''(Œ∑)\n",
    "        \n",
    "        samples = rng.poisson(lam, 50000)\n",
    "        print(f\"\\nŒª = {lam}:\")\n",
    "        print(f\"  Theory: E[X] = A'(Œ∑) = {A_prime:.4f}, Var(X) = A''(Œ∑) = {A_double_prime:.4f}\")\n",
    "        print(f\"  Sample: mean = {np.mean(samples):.4f}, var = {np.var(samples):.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úì Log-partition function generates all moments!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "demonstrate_exponential_families()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3.2: Maximum Likelihood Estimation\n",
    "\n",
    "**The MLE**: $\\hat{\\theta} = \\arg\\max_\\theta \\ell(\\theta) = \\arg\\max_\\theta \\sum_i \\log f(x_i|\\theta)$\n",
    "\n",
    "**Key Objects:**\n",
    "- Score: $U(\\theta) = \\nabla\\ell(\\theta)$ ‚Äî solve $U(\\hat{\\theta}) = 0$\n",
    "- Fisher Information: $I(\\theta) = -\\mathbb{E}[\\nabla^2\\ell(\\theta)]$\n",
    "\n",
    "**Asymptotic Properties:**\n",
    "- Consistency: $\\hat{\\theta}_n \\xrightarrow{P} \\theta_0$\n",
    "- Normality: $\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\xrightarrow{d} N(0, I_1(\\theta_0)^{-1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GAMMA MLE: NEWTON-RAPHSON vs FISHER SCORING\n",
      "======================================================================\n",
      "\n",
      "True: Œ±=3.0, Œ≤=2.0\n",
      "Initial (MoM): Œ±=3.1761, Œ≤=2.0482\n",
      "\n",
      "Newton-Raphson MLE: Œ±ÃÇ=3.2931, Œ≤ÃÇ=2.1236\n",
      "scipy.stats.gamma: Œ±ÃÇ=3.2931, Œ≤ÃÇ=2.1236\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3.2 Newton-Raphson and Fisher Scoring for Gamma MLE\n",
    "# =============================================================================\n",
    "\n",
    "def gamma_mle_comparison():\n",
    "    \"\"\"Compare Newton-Raphson and Fisher Scoring for Gamma distribution.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"GAMMA MLE: NEWTON-RAPHSON vs FISHER SCORING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    rng = default_rng(42)\n",
    "    alpha_true, beta_true = 3.0, 2.0\n",
    "    data = rng.gamma(alpha_true, 1/beta_true, size=200)\n",
    "    n = len(data)\n",
    "    \n",
    "    log_sum = np.sum(np.log(data))\n",
    "    sum_x = np.sum(data)\n",
    "    \n",
    "    def log_lik(theta):\n",
    "        alpha, beta = theta\n",
    "        if alpha <= 0 or beta <= 0: return -np.inf\n",
    "        return n*alpha*np.log(beta) - n*gammaln(alpha) + (alpha-1)*log_sum - beta*sum_x\n",
    "    \n",
    "    def score(theta):\n",
    "        alpha, beta = theta\n",
    "        return np.array([n*np.log(beta) - n*digamma(alpha) + log_sum,\n",
    "                        n*alpha/beta - sum_x])\n",
    "    \n",
    "    def hessian(theta):\n",
    "        alpha, beta = theta\n",
    "        return np.array([[-n*polygamma(1, alpha), n/beta],\n",
    "                        [n/beta, -n*alpha/beta**2]])\n",
    "    \n",
    "    # Method of moments initialization\n",
    "    x_bar, s2 = np.mean(data), np.var(data)\n",
    "    theta = np.array([x_bar**2/s2, x_bar/s2])\n",
    "    \n",
    "    print(f\"\\nTrue: Œ±={alpha_true}, Œ≤={beta_true}\")\n",
    "    print(f\"Initial (MoM): Œ±={theta[0]:.4f}, Œ≤={theta[1]:.4f}\")\n",
    "    \n",
    "    # Newton-Raphson\n",
    "    for i in range(10):\n",
    "        grad = score(theta)\n",
    "        hess = hessian(theta)\n",
    "        delta = np.linalg.solve(hess, -grad)\n",
    "        theta = theta + delta\n",
    "        if np.linalg.norm(delta) < 1e-8: break\n",
    "    \n",
    "    print(f\"\\nNewton-Raphson MLE: Œ±ÃÇ={theta[0]:.4f}, Œ≤ÃÇ={theta[1]:.4f}\")\n",
    "    \n",
    "    # Compare with scipy\n",
    "    fit_alpha, _, fit_scale = gamma_dist.fit(data, floc=0)\n",
    "    print(f\"scipy.stats.gamma: Œ±ÃÇ={fit_alpha:.4f}, Œ≤ÃÇ={1/fit_scale:.4f}\")\n",
    "\n",
    "gamma_mle_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3.3: Sampling Variability\n",
    "\n",
    "**The Delta Method**: For $g(\\hat{\\theta})$ where $\\sqrt{n}(\\hat{\\theta}-\\theta) \\to N(0,\\sigma^2)$:\n",
    "\n",
    "$$\\sqrt{n}(g(\\hat{\\theta}) - g(\\theta)) \\xrightarrow{d} N(0, [g'(\\theta)]^2\\sigma^2)$$\n",
    "\n",
    "**Variance Estimation:**\n",
    "1. Fisher Information: $\\text{Var}(\\hat{\\theta}) \\approx [nI_1(\\theta)]^{-1}$\n",
    "2. Sandwich: Robust to misspecification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DELTA METHOD: VARIANCE PROPAGATION\n",
      "======================================================================\n",
      "\n",
      "Exponential(Œª=0.5) data, n=100\n",
      "XÃÑ = 1.7981, ŒªÃÇ = 1/XÃÑ = 0.5561\n",
      "\n",
      "Delta Method:\n",
      "  SE(ŒªÃÇ) = |g'(XÃÑ)| √ó SE(XÃÑ) = 0.0547\n",
      "\n",
      "Simulation verification (10,000 reps):\n",
      "  Simulated SE(ŒªÃÇ) = 0.0508\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3.3 Delta Method Demonstration\n",
    "# =============================================================================\n",
    "\n",
    "def delta_method_demo():\n",
    "    \"\"\"Demonstrate delta method for transformed parameters.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"DELTA METHOD: VARIANCE PROPAGATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    rng = default_rng(42)\n",
    "    lambda_true = 0.5\n",
    "    n = 100\n",
    "    \n",
    "    # Exponential data\n",
    "    data = rng.exponential(1/lambda_true, n)\n",
    "    x_bar = np.mean(data)\n",
    "    lambda_hat = 1/x_bar\n",
    "    \n",
    "    # Delta method: g(x) = 1/x, g'(x) = -1/x¬≤\n",
    "    se_xbar = np.std(data)/np.sqrt(n)\n",
    "    se_lambda_delta = (1/x_bar**2) * se_xbar\n",
    "    \n",
    "    print(f\"\\nExponential(Œª={lambda_true}) data, n={n}\")\n",
    "    print(f\"XÃÑ = {x_bar:.4f}, ŒªÃÇ = 1/XÃÑ = {lambda_hat:.4f}\")\n",
    "    print(f\"\\nDelta Method:\")\n",
    "    print(f\"  SE(ŒªÃÇ) = |g'(XÃÑ)| √ó SE(XÃÑ) = {se_lambda_delta:.4f}\")\n",
    "    \n",
    "    # Verify via simulation\n",
    "    lambda_estimates = [1/np.mean(rng.exponential(1/lambda_true, n)) for _ in range(10000)]\n",
    "    print(f\"\\nSimulation verification (10,000 reps):\")\n",
    "    print(f\"  Simulated SE(ŒªÃÇ) = {np.std(lambda_estimates):.4f}\")\n",
    "\n",
    "delta_method_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3.4: Linear Models\n",
    "\n",
    "**OLS Estimator**: $\\hat{\\beta} = (X'X)^{-1}X'y$\n",
    "\n",
    "**Gauss-Markov**: Under linearity, exogeneity, homoskedasticity, OLS is **BLUE**.\n",
    "\n",
    "**Under Normality**: $\\hat{\\beta} \\sim N(\\beta, \\sigma^2(X'X)^{-1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "OLS FROM FIRST PRINCIPLES\n",
      "======================================================================\n",
      "\n",
      "True model: y = 2.0 + 3.0x‚ÇÅ + -1.5x‚ÇÇ + Œµ\n",
      "\n",
      "Param      True         Estimate     SE          \n",
      "--------------------------------------------------\n",
      "Œ≤‚ÇÄ         2.0000       1.2338       0.3050      \n",
      "Œ≤‚ÇÅ         3.0000       3.1430       0.0548      \n",
      "Œ≤‚ÇÇ         -1.5000      -1.4460      0.0514      \n",
      "\n",
      "œÉÃÇ¬≤ = 2.1843 (true œÉ¬≤ = 2.2500)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3.4 OLS Implementation\n",
    "# =============================================================================\n",
    "\n",
    "def ols_demonstration():\n",
    "    \"\"\"OLS from first principles.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"OLS FROM FIRST PRINCIPLES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    rng = default_rng(42)\n",
    "    n = 100\n",
    "    beta_true = np.array([2.0, 3.0, -1.5])\n",
    "    sigma = 1.5\n",
    "    \n",
    "    x1, x2 = rng.uniform(0, 10, n), rng.uniform(-5, 5, n)\n",
    "    X = np.column_stack([np.ones(n), x1, x2])\n",
    "    y = X @ beta_true + rng.normal(0, sigma, n)\n",
    "    \n",
    "    # OLS via normal equations\n",
    "    beta_hat = solve(X.T @ X, X.T @ y)\n",
    "    \n",
    "    # Variance estimation\n",
    "    resid = y - X @ beta_hat\n",
    "    sigma2_hat = np.sum(resid**2) / (n - 3)\n",
    "    se_beta = np.sqrt(np.diag(sigma2_hat * inv(X.T @ X)))\n",
    "    \n",
    "    print(f\"\\nTrue model: y = {beta_true[0]} + {beta_true[1]}x‚ÇÅ + {beta_true[2]}x‚ÇÇ + Œµ\")\n",
    "    print(f\"\\n{'Param':<10} {'True':<12} {'Estimate':<12} {'SE':<12}\")\n",
    "    print(\"-\"*50)\n",
    "    for i, name in enumerate(['Œ≤‚ÇÄ', 'Œ≤‚ÇÅ', 'Œ≤‚ÇÇ']):\n",
    "        print(f\"{name:<10} {beta_true[i]:<12.4f} {beta_hat[i]:<12.4f} {se_beta[i]:<12.4f}\")\n",
    "    \n",
    "    print(f\"\\nœÉÃÇ¬≤ = {sigma2_hat:.4f} (true œÉ¬≤ = {sigma**2:.4f})\")\n",
    "\n",
    "ols_demonstration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3.5: Generalized Linear Models\n",
    "\n",
    "**GLM Components:**\n",
    "1. Random: $Y \\sim$ Exponential Dispersion Family\n",
    "2. Systematic: $\\eta = X\\beta$\n",
    "3. Link: $g(\\mu) = \\eta$\n",
    "\n",
    "**IRLS**: Iteratively Reweighted Least Squares for fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOGISTIC REGRESSION VIA IRLS\n",
      "======================================================================\n",
      "\n",
      "Param      True         IRLS         SE           Odds Ratio  \n",
      "------------------------------------------------------------\n",
      "Œ≤‚ÇÄ         -1.0000      -0.8573      0.1310       0.4243      \n",
      "Œ≤‚ÇÅ         2.0000       1.7265       0.1791       5.6209      \n",
      "Œ≤‚ÇÇ         -1.5000      -1.4663      0.1639       0.2308      \n",
      "\n",
      "statsmodels verification: Œ≤ = [-0.8573  1.7265 -1.4663]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3.5 Logistic Regression via IRLS\n",
    "# =============================================================================\n",
    "\n",
    "def logistic_irls():\n",
    "    \"\"\"Logistic regression via IRLS.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"LOGISTIC REGRESSION VIA IRLS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    rng = default_rng(42)\n",
    "    n = 500\n",
    "    x1, x2 = rng.normal(0, 1, n), rng.normal(0, 1, n)\n",
    "    X = np.column_stack([np.ones(n), x1, x2])\n",
    "    \n",
    "    beta_true = np.array([-1.0, 2.0, -1.5])\n",
    "    p_true = expit(X @ beta_true)\n",
    "    y = rng.binomial(1, p_true)\n",
    "    \n",
    "    # IRLS\n",
    "    beta = np.zeros(3)\n",
    "    for iteration in range(25):\n",
    "        eta = X @ beta\n",
    "        mu = np.clip(expit(eta), 1e-10, 1-1e-10)\n",
    "        W = np.diag(mu * (1 - mu))\n",
    "        z = eta + (y - mu) / (mu * (1 - mu))\n",
    "        beta_new = solve(X.T @ W @ X, X.T @ W @ z)\n",
    "        if np.max(np.abs(beta_new - beta)) < 1e-8: break\n",
    "        beta = beta_new\n",
    "    \n",
    "    se = np.sqrt(np.diag(inv(X.T @ W @ X)))\n",
    "    \n",
    "    print(f\"\\n{'Param':<10} {'True':<12} {'IRLS':<12} {'SE':<12} {'Odds Ratio':<12}\")\n",
    "    print(\"-\"*60)\n",
    "    for i, name in enumerate(['Œ≤‚ÇÄ', 'Œ≤‚ÇÅ', 'Œ≤‚ÇÇ']):\n",
    "        print(f\"{name:<10} {beta_true[i]:<12.4f} {beta[i]:<12.4f} {se[i]:<12.4f} {np.exp(beta[i]):<12.4f}\")\n",
    "    \n",
    "    # Verify with statsmodels\n",
    "    model = sm.GLM(y, X, family=families.Binomial()).fit()\n",
    "    print(f\"\\nstatsmodels verification: Œ≤ = {model.params.round(4)}\")\n",
    "\n",
    "logistic_irls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ Chapter 3 Exercises ‚Äî Complete Solutions\n",
    "\n",
    "The following exercises demonstrate key concepts with full solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1: Beta Distribution as Exponential Family\n",
    "\n",
    "**Problem**: Convert Beta(Œ±, Œ≤) to canonical exponential family form and verify moment-generating property.\n",
    "\n",
    "**Solution**: \n",
    "- Natural parameters: $\\eta = (\\alpha-1, \\beta-1)$\n",
    "- Sufficient statistics: $T(x) = (\\log x, \\log(1-x))$\n",
    "- Log-partition: $A(\\eta) = \\log\\Gamma(\\eta_1+1) + \\log\\Gamma(\\eta_2+1) - \\log\\Gamma(\\eta_1+\\eta_2+2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXERCISE 3.1: BETA DISTRIBUTION AS EXPONENTIAL FAMILY\n",
      "======================================================================\n",
      "\n",
      "DERIVATION:\n",
      "-----------\n",
      "Beta PDF: f(x|Œ±,Œ≤) = [Œì(Œ±+Œ≤)/(Œì(Œ±)Œì(Œ≤))] x^(Œ±-1) (1-x)^(Œ≤-1)\n",
      "\n",
      "Taking log and rearranging:\n",
      "    log f = (Œ±-1)log(x) + (Œ≤-1)log(1-x) - log B(Œ±,Œ≤)\n",
      "\n",
      "Exponential family form f(x) = h(x)exp{Œ∑'T(x) - A(Œ∑)}:\n",
      "    Œ∑ = (Œ∑‚ÇÅ, Œ∑‚ÇÇ) = (Œ±-1, Œ≤-1)\n",
      "    T(x) = (log x, log(1-x))\n",
      "    A(Œ∑) = log Œì(Œ∑‚ÇÅ+1) + log Œì(Œ∑‚ÇÇ+1) - log Œì(Œ∑‚ÇÅ+Œ∑‚ÇÇ+2)\n",
      "    h(x) = 1\n",
      "\n",
      "Moment-generating property:\n",
      "    E[log X] = ‚àÇA/‚àÇŒ∑‚ÇÅ = œà(Œ±) - œà(Œ±+Œ≤)\n",
      "    E[log(1-X)] = ‚àÇA/‚àÇŒ∑‚ÇÇ = œà(Œ≤) - œà(Œ±+Œ≤)\n",
      "    \n",
      "\n",
      "Numerical Verification (Œ±=3.0, Œ≤=5.0, n=100,000):\n",
      "\n",
      "Quantity             Theory ‚àáA(Œ∑)    Monte Carlo     Error          \n",
      "-----------------------------------------------------------------\n",
      "E[log X]             -1.092857       -1.090536       0.002321       \n",
      "E[log(1-X)]          -0.509524       -0.510890       0.001366       \n",
      "\n",
      "======================================================================\n",
      "‚úì Beta is a 2-parameter exponential family. Moments verified!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXERCISE 3.1 SOLUTION: Beta as Exponential Family\n",
    "# =============================================================================\n",
    "\n",
    "def exercise_3_1_solution():\n",
    "    \"\"\"Beta distribution as exponential family with verification.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"EXERCISE 3.1: BETA DISTRIBUTION AS EXPONENTIAL FAMILY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\"\"\n",
    "DERIVATION:\n",
    "-----------\n",
    "Beta PDF: f(x|Œ±,Œ≤) = [Œì(Œ±+Œ≤)/(Œì(Œ±)Œì(Œ≤))] x^(Œ±-1) (1-x)^(Œ≤-1)\n",
    "\n",
    "Taking log and rearranging:\n",
    "    log f = (Œ±-1)log(x) + (Œ≤-1)log(1-x) - log B(Œ±,Œ≤)\n",
    "\n",
    "Exponential family form f(x) = h(x)exp{Œ∑'T(x) - A(Œ∑)}:\n",
    "    Œ∑ = (Œ∑‚ÇÅ, Œ∑‚ÇÇ) = (Œ±-1, Œ≤-1)\n",
    "    T(x) = (log x, log(1-x))\n",
    "    A(Œ∑) = log Œì(Œ∑‚ÇÅ+1) + log Œì(Œ∑‚ÇÇ+1) - log Œì(Œ∑‚ÇÅ+Œ∑‚ÇÇ+2)\n",
    "    h(x) = 1\n",
    "\n",
    "Moment-generating property:\n",
    "    E[log X] = ‚àÇA/‚àÇŒ∑‚ÇÅ = œà(Œ±) - œà(Œ±+Œ≤)\n",
    "    E[log(1-X)] = ‚àÇA/‚àÇŒ∑‚ÇÇ = œà(Œ≤) - œà(Œ±+Œ≤)\n",
    "    \"\"\")\n",
    "    \n",
    "    # Numerical verification\n",
    "    rng = default_rng(42)\n",
    "    alpha, beta_param = 3.0, 5.0\n",
    "    n_samples = 100000\n",
    "    \n",
    "    samples = rng.beta(alpha, beta_param, n_samples)\n",
    "    \n",
    "    # Theoretical vs Monte Carlo\n",
    "    E_logx_theory = digamma(alpha) - digamma(alpha + beta_param)\n",
    "    E_log1mx_theory = digamma(beta_param) - digamma(alpha + beta_param)\n",
    "    \n",
    "    E_logx_mc = np.mean(np.log(samples))\n",
    "    E_log1mx_mc = np.mean(np.log(1 - samples))\n",
    "    \n",
    "    print(f\"\\nNumerical Verification (Œ±={alpha}, Œ≤={beta_param}, n={n_samples:,}):\")\n",
    "    print(f\"\\n{'Quantity':<20} {'Theory ‚àáA(Œ∑)':<15} {'Monte Carlo':<15} {'Error':<15}\")\n",
    "    print(\"-\"*65)\n",
    "    print(f\"{'E[log X]':<20} {E_logx_theory:<15.6f} {E_logx_mc:<15.6f} {abs(E_logx_theory-E_logx_mc):<15.6f}\")\n",
    "    print(f\"{'E[log(1-X)]':<20} {E_log1mx_theory:<15.6f} {E_log1mx_mc:<15.6f} {abs(E_log1mx_theory-E_log1mx_mc):<15.6f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úì Beta is a 2-parameter exponential family. Moments verified!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "exercise_3_1_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2: Gamma MLE via Profile Likelihood\n",
    "\n",
    "**Problem**: Implement profile likelihood for Gamma(Œ±, Œ≤) MLE.\n",
    "\n",
    "**Solution**: For fixed Œ±, $\\hat{\\beta}(\\alpha) = \\alpha/\\bar{x}$. Profile likelihood reduces 2D to 1D optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXERCISE 3.2: GAMMA MLE VIA PROFILE LIKELIHOOD\n",
      "======================================================================\n",
      "\n",
      "Initial Œ± (MoM): 2.6168\n",
      "\n",
      "Profile MLE Results:\n",
      "  True:     Œ± = 2.5000, Œ≤ = 1.5000\n",
      "  Profile:  Œ±ÃÇ = 95.7507, Œ≤ÃÇ = 55.4664\n",
      "  scipy:    Œ±ÃÇ = 2.7180, Œ≤ÃÇ = 1.5745\n",
      "\n",
      "Standard Errors:\n",
      "  SE(Œ±ÃÇ) = 9.5585\n",
      "  SE(Œ≤ÃÇ) = 5.5515\n",
      "\n",
      "======================================================================\n",
      "‚úì Profile likelihood reduces 2D optimization to 1D!\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_223755/128453705.py:34: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  alpha_new = alpha - score/hess\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXERCISE 3.2 SOLUTION: Gamma Profile Likelihood\n",
    "# =============================================================================\n",
    "\n",
    "def exercise_3_2_solution():\n",
    "    \"\"\"Gamma MLE via profile likelihood.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"EXERCISE 3.2: GAMMA MLE VIA PROFILE LIKELIHOOD\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    rng = default_rng(42)\n",
    "    alpha_true, beta_true = 2.5, 1.5\n",
    "    n = 200\n",
    "    data = rng.gamma(alpha_true, 1/beta_true, n)\n",
    "    \n",
    "    x_bar = np.mean(data)\n",
    "    log_x_bar = np.mean(np.log(data))\n",
    "    \n",
    "    # Profile log-likelihood: only function of Œ±\n",
    "    def profile_ll(alpha):\n",
    "        if alpha <= 0: return -np.inf\n",
    "        return n * (alpha * np.log(alpha/x_bar) - gammaln(alpha) + (alpha-1)*log_x_bar - alpha)\n",
    "    \n",
    "    def profile_score(alpha):\n",
    "        return n * (np.log(alpha/x_bar) + 1 - digamma(alpha) + log_x_bar)\n",
    "    \n",
    "    # Newton's method on profile likelihood\n",
    "    alpha = x_bar**2 / np.var(data)  # MoM init\n",
    "    print(f\"\\nInitial Œ± (MoM): {alpha:.4f}\")\n",
    "    \n",
    "    for i in range(15):\n",
    "        score = profile_score(alpha)\n",
    "        hess = n * (1/alpha - polygamma(1, alpha))\n",
    "        alpha_new = alpha - score/hess\n",
    "        alpha_new = max(alpha_new, 0.01)\n",
    "        if abs(alpha_new - alpha) < 1e-10: break\n",
    "        alpha = alpha_new\n",
    "    \n",
    "    beta_hat = alpha / x_bar\n",
    "    \n",
    "    print(f\"\\nProfile MLE Results:\")\n",
    "    print(f\"  True:     Œ± = {alpha_true:.4f}, Œ≤ = {beta_true:.4f}\")\n",
    "    print(f\"  Profile:  Œ±ÃÇ = {alpha:.4f}, Œ≤ÃÇ = {beta_hat:.4f}\")\n",
    "    \n",
    "    # Verify\n",
    "    fit_alpha, _, fit_scale = gamma_dist.fit(data, floc=0)\n",
    "    print(f\"  scipy:    Œ±ÃÇ = {fit_alpha:.4f}, Œ≤ÃÇ = {1/fit_scale:.4f}\")\n",
    "    \n",
    "    # Standard errors via Fisher information\n",
    "    I_aa = n * polygamma(1, alpha)\n",
    "    I_ab = -n / beta_hat\n",
    "    I_bb = n * alpha / beta_hat**2\n",
    "    fisher = np.array([[I_aa, I_ab], [I_ab, I_bb]])\n",
    "    var_mat = inv(fisher)\n",
    "    \n",
    "    print(f\"\\nStandard Errors:\")\n",
    "    print(f\"  SE(Œ±ÃÇ) = {np.sqrt(var_mat[0,0]):.4f}\")\n",
    "    print(f\"  SE(Œ≤ÃÇ) = {np.sqrt(var_mat[1,1]):.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úì Profile likelihood reduces 2D optimization to 1D!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "exercise_3_2_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.3: Delta Method for Log Odds Ratio\n",
    "\n",
    "**Problem**: Derive variance of log odds ratio from 2√ó2 table using multivariate delta method.\n",
    "\n",
    "**Solution**: $\\text{Var}(\\log OR) = 1/n_{11} + 1/n_{12} + 1/n_{21} + 1/n_{22}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXERCISE 3.3: DELTA METHOD FOR LOG ODDS RATIO\n",
      "======================================================================\n",
      "\n",
      "DERIVATION:\n",
      "-----------\n",
      "2√ó2 table with cells n‚ÇÅ‚ÇÅ, n‚ÇÅ‚ÇÇ, n‚ÇÇ‚ÇÅ, n‚ÇÇ‚ÇÇ\n",
      "\n",
      "Log OR: g(n) = log(n‚ÇÅ‚ÇÅ) + log(n‚ÇÇ‚ÇÇ) - log(n‚ÇÅ‚ÇÇ) - log(n‚ÇÇ‚ÇÅ)\n",
      "\n",
      "Gradient: ‚àág = (1/n‚ÇÅ‚ÇÅ, -1/n‚ÇÅ‚ÇÇ, -1/n‚ÇÇ‚ÇÅ, 1/n‚ÇÇ‚ÇÇ)\n",
      "\n",
      "Under Poisson approximation: Var(n·µ¢‚±º) ‚âà n·µ¢‚±º\n",
      "\n",
      "Delta method: Var(log OR) = Œ£·µ¢‚±º (‚àÇg/‚àÇn·µ¢‚±º)¬≤ √ó Var(n·µ¢‚±º)\n",
      "            = 1/n‚ÇÅ‚ÇÅ + 1/n‚ÇÅ‚ÇÇ + 1/n‚ÇÇ‚ÇÅ + 1/n‚ÇÇ‚ÇÇ\n",
      "    \n",
      "\n",
      "2√ó2 Table Example:\n",
      "              Event+  Event-\n",
      "  Exposed+       40      60\n",
      "  Exposed-       80     120\n",
      "\n",
      "Results:\n",
      "  OR = 1.0000\n",
      "  log(OR) = 0.0000\n",
      "  SE(log OR) = ‚àö(1/40 + 1/60 + 1/80 + 1/120) = 0.2500\n",
      "  95% CI for OR: (0.6126, 1.6323)\n",
      "\n",
      "Simulation Verification (10,000 reps):\n",
      "  Simulated SD(log OR) = 0.2517\n",
      "  Delta method SE = 0.2500\n",
      "  Ratio = 1.0067\n",
      "\n",
      "======================================================================\n",
      "‚úì Classic formula Var(log OR) = Œ£(1/n·µ¢‚±º) is delta method!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXERCISE 3.3 SOLUTION: Delta Method for Odds Ratio\n",
    "# =============================================================================\n",
    "\n",
    "def exercise_3_3_solution():\n",
    "    \"\"\"Delta method for log odds ratio variance.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"EXERCISE 3.3: DELTA METHOD FOR LOG ODDS RATIO\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\"\"\n",
    "DERIVATION:\n",
    "-----------\n",
    "2√ó2 table with cells n‚ÇÅ‚ÇÅ, n‚ÇÅ‚ÇÇ, n‚ÇÇ‚ÇÅ, n‚ÇÇ‚ÇÇ\n",
    "\n",
    "Log OR: g(n) = log(n‚ÇÅ‚ÇÅ) + log(n‚ÇÇ‚ÇÇ) - log(n‚ÇÅ‚ÇÇ) - log(n‚ÇÇ‚ÇÅ)\n",
    "\n",
    "Gradient: ‚àág = (1/n‚ÇÅ‚ÇÅ, -1/n‚ÇÅ‚ÇÇ, -1/n‚ÇÇ‚ÇÅ, 1/n‚ÇÇ‚ÇÇ)\n",
    "\n",
    "Under Poisson approximation: Var(n·µ¢‚±º) ‚âà n·µ¢‚±º\n",
    "\n",
    "Delta method: Var(log OR) = Œ£·µ¢‚±º (‚àÇg/‚àÇn·µ¢‚±º)¬≤ √ó Var(n·µ¢‚±º)\n",
    "            = 1/n‚ÇÅ‚ÇÅ + 1/n‚ÇÅ‚ÇÇ + 1/n‚ÇÇ‚ÇÅ + 1/n‚ÇÇ‚ÇÇ\n",
    "    \"\"\")\n",
    "    \n",
    "    # Example data\n",
    "    n11, n12, n21, n22 = 40, 60, 80, 120\n",
    "    \n",
    "    print(f\"\\n2√ó2 Table Example:\")\n",
    "    print(f\"              Event+  Event-\")\n",
    "    print(f\"  Exposed+     {n11:>4}    {n12:>4}\")\n",
    "    print(f\"  Exposed-     {n21:>4}    {n22:>4}\")\n",
    "    \n",
    "    OR = (n11 * n22) / (n12 * n21)\n",
    "    log_OR = np.log(OR)\n",
    "    var_log_OR = 1/n11 + 1/n12 + 1/n21 + 1/n22\n",
    "    se_log_OR = np.sqrt(var_log_OR)\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  OR = {OR:.4f}\")\n",
    "    print(f\"  log(OR) = {log_OR:.4f}\")\n",
    "    print(f\"  SE(log OR) = ‚àö(1/{n11} + 1/{n12} + 1/{n21} + 1/{n22}) = {se_log_OR:.4f}\")\n",
    "    print(f\"  95% CI for OR: ({np.exp(log_OR - 1.96*se_log_OR):.4f}, {np.exp(log_OR + 1.96*se_log_OR):.4f})\")\n",
    "    \n",
    "    # Simulation verification\n",
    "    rng = default_rng(42)\n",
    "    n_total = n11 + n12 + n21 + n22\n",
    "    p_true = np.array([n11, n12, n21, n22]) / n_total\n",
    "    \n",
    "    log_ors = []\n",
    "    for _ in range(10000):\n",
    "        counts = rng.multinomial(n_total, p_true)\n",
    "        if np.all(counts > 0):\n",
    "            log_ors.append(np.log(counts[0]*counts[3]/(counts[1]*counts[2])))\n",
    "    \n",
    "    print(f\"\\nSimulation Verification (10,000 reps):\")\n",
    "    print(f\"  Simulated SD(log OR) = {np.std(log_ors):.4f}\")\n",
    "    print(f\"  Delta method SE = {se_log_OR:.4f}\")\n",
    "    print(f\"  Ratio = {np.std(log_ors)/se_log_OR:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úì Classic formula Var(log OR) = Œ£(1/n·µ¢‚±º) is delta method!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "exercise_3_3_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.4: Gauss-Markov Violations\n",
    "\n",
    "**Problem**: Investigate heteroskedasticity and autocorrelation effects on OLS.\n",
    "\n",
    "**Key Findings**:\n",
    "- OLS remains unbiased under both violations\n",
    "- Model-based SEs are WRONG\n",
    "- Use HC (heteroskedasticity) or HAC (autocorrelation) robust SEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXERCISE 3.4: GAUSS-MARKOV VIOLATIONS\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "PART 1: HETEROSKEDASTICITY (Var(Œµ|x) = x¬≤)\n",
      "======================================================================\n",
      "\n",
      "Œ≤‚ÇÅ estimation (2000 simulations):\n",
      "  Mean(Œ≤ÃÇ‚ÇÅ) = 2.0020 (true = 2.0000) - UNBIASED ‚úì\n",
      "  True SE = 0.1711\n",
      "  Model SE = 0.1646 (ratio = 0.96) - WRONG!\n",
      "  HC1 SE = 0.1752 (ratio = 1.02) - CORRECT ‚úì\n",
      "\n",
      "======================================================================\n",
      "PART 2: AUTOCORRELATION (AR(1) with œÅ=0.7)\n",
      "======================================================================\n",
      "\n",
      "Œ≤‚ÇÅ estimation (2000 simulations):\n",
      "  Mean(Œ≤ÃÇ‚ÇÅ) = 2.0009 (true = 2.0000) - UNBIASED ‚úì\n",
      "  True SE = 0.0850\n",
      "  Model SE = 0.0369 (ratio = 0.43) - WRONG!\n",
      "  Newey-West = 0.0634 (ratio = 0.75) - BETTER ‚úì\n",
      "\n",
      "======================================================================\n",
      "SUMMARY: OLS is unbiased but SEs require correction!\n",
      "  - Heteroskedasticity: Use HC (White) robust SEs\n",
      "  - Autocorrelation: Use HAC (Newey-West) robust SEs\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXERCISE 3.4 SOLUTION: Gauss-Markov Violations\n",
    "# =============================================================================\n",
    "\n",
    "def exercise_3_4_solution():\n",
    "    \"\"\"Investigate Gauss-Markov violations via simulation.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"EXERCISE 3.4: GAUSS-MARKOV VIOLATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    rng = default_rng(42)\n",
    "    n, n_sim = 200, 2000\n",
    "    beta_true = np.array([1.0, 2.0])\n",
    "    \n",
    "    # Fixed design\n",
    "    x = np.linspace(1, 10, n)\n",
    "    X = sm.add_constant(x)\n",
    "    \n",
    "    # Part 1: Heteroskedasticity\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PART 1: HETEROSKEDASTICITY (Var(Œµ|x) = x¬≤)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    ols_ests, ols_ses, hc_ses = [], [], []\n",
    "    for _ in range(n_sim):\n",
    "        epsilon = rng.normal(0, x)  # SD proportional to x\n",
    "        y = beta_true[0] + beta_true[1]*x + epsilon\n",
    "        \n",
    "        ols = sm.OLS(y, X).fit()\n",
    "        ols_ests.append(ols.params[1])\n",
    "        ols_ses.append(ols.bse[1])\n",
    "        \n",
    "        ols_hc = sm.OLS(y, X).fit(cov_type='HC1')\n",
    "        hc_ses.append(ols_hc.bse[1])\n",
    "    \n",
    "    true_se = np.std(ols_ests)\n",
    "    print(f\"\\nŒ≤‚ÇÅ estimation ({n_sim} simulations):\")\n",
    "    print(f\"  Mean(Œ≤ÃÇ‚ÇÅ) = {np.mean(ols_ests):.4f} (true = {beta_true[1]:.4f}) - UNBIASED ‚úì\")\n",
    "    print(f\"  True SE = {true_se:.4f}\")\n",
    "    print(f\"  Model SE = {np.mean(ols_ses):.4f} (ratio = {np.mean(ols_ses)/true_se:.2f}) - WRONG!\")\n",
    "    print(f\"  HC1 SE = {np.mean(hc_ses):.4f} (ratio = {np.mean(hc_ses)/true_se:.2f}) - CORRECT ‚úì\")\n",
    "    \n",
    "    # Part 2: Autocorrelation\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PART 2: AUTOCORRELATION (AR(1) with œÅ=0.7)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    rho = 0.7\n",
    "    ols_ests_ar, ols_ses_ar, nw_ses = [], [], []\n",
    "    \n",
    "    for _ in range(n_sim):\n",
    "        # Generate AR(1) errors\n",
    "        eps = np.zeros(n)\n",
    "        u = rng.normal(0, 1, n)\n",
    "        eps[0] = u[0]/np.sqrt(1-rho**2)\n",
    "        for t in range(1, n):\n",
    "            eps[t] = rho*eps[t-1] + u[t]\n",
    "        \n",
    "        y = beta_true[0] + beta_true[1]*x + eps\n",
    "        \n",
    "        ols = sm.OLS(y, X).fit()\n",
    "        ols_ests_ar.append(ols.params[1])\n",
    "        ols_ses_ar.append(ols.bse[1])\n",
    "        \n",
    "        ols_nw = sm.OLS(y, X).fit(cov_type='HAC', cov_kwds={'maxlags': 5})\n",
    "        nw_ses.append(ols_nw.bse[1])\n",
    "    \n",
    "    true_se_ar = np.std(ols_ests_ar)\n",
    "    print(f\"\\nŒ≤‚ÇÅ estimation ({n_sim} simulations):\")\n",
    "    print(f\"  Mean(Œ≤ÃÇ‚ÇÅ) = {np.mean(ols_ests_ar):.4f} (true = {beta_true[1]:.4f}) - UNBIASED ‚úì\")\n",
    "    print(f\"  True SE = {true_se_ar:.4f}\")\n",
    "    print(f\"  Model SE = {np.mean(ols_ses_ar):.4f} (ratio = {np.mean(ols_ses_ar)/true_se_ar:.2f}) - WRONG!\")\n",
    "    print(f\"  Newey-West = {np.mean(nw_ses):.4f} (ratio = {np.mean(nw_ses)/true_se_ar:.2f}) - BETTER ‚úì\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SUMMARY: OLS is unbiased but SEs require correction!\")\n",
    "    print(\"  - Heteroskedasticity: Use HC (White) robust SEs\")\n",
    "    print(\"  - Autocorrelation: Use HAC (Newey-West) robust SEs\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "exercise_3_4_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.5: Gamma Regression via IRLS\n",
    "\n",
    "**Problem**: Implement IRLS for Gamma regression with log link.\n",
    "\n",
    "**Key insight**: For Gamma with log link, weights are constant ($W=1$) making IRLS very simple!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXERCISE 3.5: GAMMA REGRESSION VIA IRLS\n",
      "======================================================================\n",
      "\n",
      "GAMMA GLM WITH LOG LINK:\n",
      "------------------------\n",
      "Distribution: Y ~ Gamma(shape=Œ±, scale=Œº/Œ±), E[Y]=Œº, Var(Y)=Œº¬≤/Œ±\n",
      "Link: log(Œº) = XŒ≤, so Œº = exp(XŒ≤)\n",
      "Variance function: V(Œº) = Œº¬≤\n",
      "\n",
      "IRLS components:\n",
      "  Œ∑ = XŒ≤\n",
      "  Œº = exp(Œ∑)\n",
      "  g'(Œº) = 1/Œº\n",
      "  W = 1/(V(Œº)¬∑g'(Œº)¬≤) = 1/(Œº¬≤¬∑1/Œº¬≤) = 1  (constant weights!)\n",
      "  z = Œ∑ + (y-Œº)/Œº  (working response)\n",
      "    \n",
      "\n",
      "Simulated medical cost data (n=300):\n",
      "  True model: log(Œº) = 6.0 + 0.02¬∑age + 0.05¬∑BMI\n",
      "  Mean cost: $3804, Range: $507 - $14203\n",
      "\n",
      "IRLS Results (6 iterations):\n",
      "\n",
      "Parameter    True       IRLS       SE        \n",
      "---------------------------------------------\n",
      "Intercept    6.0000     5.9987     0.1611    \n",
      "Age          0.0200     0.0205     0.0018    \n",
      "BMI          0.0500     0.0475     0.0053    \n",
      "\n",
      "Dispersion: œÜÃÇ = 0.1990 (true 1/shape = 0.2000)\n",
      "\n",
      "statsmodels verification: Œ≤ = [5.9987 0.0205 0.0475]\n",
      "\n",
      "======================================================================\n",
      "‚úì Gamma with log link has constant weights - IRLS is just iterated OLS!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXERCISE 3.5 SOLUTION: Gamma Regression via IRLS\n",
    "# =============================================================================\n",
    "\n",
    "def exercise_3_5_solution():\n",
    "    \"\"\"Gamma regression with log link via IRLS.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"EXERCISE 3.5: GAMMA REGRESSION VIA IRLS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\"\"\n",
    "GAMMA GLM WITH LOG LINK:\n",
    "------------------------\n",
    "Distribution: Y ~ Gamma(shape=Œ±, scale=Œº/Œ±), E[Y]=Œº, Var(Y)=Œº¬≤/Œ±\n",
    "Link: log(Œº) = XŒ≤, so Œº = exp(XŒ≤)\n",
    "Variance function: V(Œº) = Œº¬≤\n",
    "\n",
    "IRLS components:\n",
    "  Œ∑ = XŒ≤\n",
    "  Œº = exp(Œ∑)\n",
    "  g'(Œº) = 1/Œº\n",
    "  W = 1/(V(Œº)¬∑g'(Œº)¬≤) = 1/(Œº¬≤¬∑1/Œº¬≤) = 1  (constant weights!)\n",
    "  z = Œ∑ + (y-Œº)/Œº  (working response)\n",
    "    \"\"\")\n",
    "    \n",
    "    rng = default_rng(42)\n",
    "    n = 300\n",
    "    \n",
    "    # Predictors: age and BMI for medical costs\n",
    "    age = rng.uniform(20, 70, n)\n",
    "    bmi = rng.uniform(18, 35, n)\n",
    "    X = np.column_stack([np.ones(n), age, bmi])\n",
    "    \n",
    "    # True model: log(cost) = 6 + 0.02*age + 0.05*bmi\n",
    "    beta_true = np.array([6.0, 0.02, 0.05])\n",
    "    mu_true = np.exp(X @ beta_true)\n",
    "    shape = 5.0\n",
    "    y = rng.gamma(shape, mu_true/shape, n)\n",
    "    \n",
    "    print(f\"\\nSimulated medical cost data (n={n}):\")\n",
    "    print(f\"  True model: log(Œº) = {beta_true[0]} + {beta_true[1]}¬∑age + {beta_true[2]}¬∑BMI\")\n",
    "    print(f\"  Mean cost: ${np.mean(y):.0f}, Range: ${y.min():.0f} - ${y.max():.0f}\")\n",
    "    \n",
    "    # IRLS for Gamma with log link\n",
    "    beta = np.linalg.lstsq(X, np.log(y), rcond=None)[0]  # OLS on log(y) init\n",
    "    \n",
    "    for iteration in range(25):\n",
    "        eta = X @ beta\n",
    "        mu = np.exp(eta)\n",
    "        z = eta + (y - mu)/mu  # Working response\n",
    "        # W = I for Gamma log link, so this is just OLS\n",
    "        beta_new = np.linalg.lstsq(X, z, rcond=None)[0]\n",
    "        if np.max(np.abs(beta_new - beta)) < 1e-8: break\n",
    "        beta = beta_new\n",
    "    \n",
    "    mu_final = np.exp(X @ beta)\n",
    "    pearson_resid = (y - mu_final)/mu_final\n",
    "    phi_hat = np.sum(pearson_resid**2)/(n - 3)\n",
    "    se = np.sqrt(phi_hat * np.diag(inv(X.T @ X)))\n",
    "    \n",
    "    print(f\"\\nIRLS Results ({iteration+1} iterations):\")\n",
    "    print(f\"\\n{'Parameter':<12} {'True':<10} {'IRLS':<10} {'SE':<10}\")\n",
    "    print(\"-\"*45)\n",
    "    for i, name in enumerate(['Intercept', 'Age', 'BMI']):\n",
    "        print(f\"{name:<12} {beta_true[i]:<10.4f} {beta[i]:<10.4f} {se[i]:<10.4f}\")\n",
    "    \n",
    "    print(f\"\\nDispersion: œÜÃÇ = {phi_hat:.4f} (true 1/shape = {1/shape:.4f})\")\n",
    "    \n",
    "    # Verify with statsmodels\n",
    "    model = sm.GLM(y, X, family=families.Gamma(link=families.links.Log())).fit()\n",
    "    print(f\"\\nstatsmodels verification: Œ≤ = {model.params.round(4)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úì Gamma with log link has constant weights - IRLS is just iterated OLS!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "exercise_3_5_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.6: Complete GLM Analysis Workflow\n",
    "\n",
    "**Problem**: Full GLM analysis of hospital readmission data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXERCISE 3.6: COMPLETE GLM ANALYSIS WORKFLOW\n",
      "======================================================================\n",
      "\n",
      "Simulated hospital readmission data:\n",
      "  n = 1000, Readmission rate = 35.6%\n",
      "\n",
      "True model: logit(p) = -2.5 + 0.05¬∑LOS + 0.3¬∑comorbid + 0.02¬∑age + -0.5¬∑education\n",
      "\n",
      "======================================================================\n",
      "MODEL SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Variable        True Œ≤     Œ≤ÃÇ         SE         OR         p-value   \n",
      "----------------------------------------------------------------------\n",
      "Intercept       -2.500     -2.514     0.432      0.081      0.0000    \n",
      "LOS             0.050      0.052      0.013      1.054      0.0001    \n",
      "Comorbid        0.300      0.339      0.049      1.404      0.0000    \n",
      "Age             0.020      0.019      0.006      1.019      0.0011    \n",
      "Education       -0.500     -0.604     0.140      0.547      0.0000    \n",
      "\n",
      "======================================================================\n",
      "MODEL FIT DIAGNOSTICS\n",
      "======================================================================\n",
      "  Deviance = 1215.26\n",
      "  Pearson œá¬≤ = 996.28\n",
      "  df = 995\n",
      "  Deviance/df = 1.221 (should be ‚âà 1)\n",
      "\n",
      "======================================================================\n",
      "LIKELIHOOD RATIO TEST FOR EDUCATION EFFECT\n",
      "======================================================================\n",
      "  H‚ÇÄ: Œ≤_education = 0\n",
      "  LRT statistic = 18.6549\n",
      "  p-value = 0.0000\n",
      "  Conclusion: Reject H‚ÇÄ at Œ±=0.05\n",
      "\n",
      "======================================================================\n",
      "INTERPRETATION\n",
      "======================================================================\n",
      "  ‚Ä¢ Each additional day in hospital increases odds by 5.4%\n",
      "  ‚Ä¢ Each additional comorbidity increases odds by 40%\n",
      "  ‚Ä¢ Discharge education REDUCES odds by 45%\n",
      "\n",
      "======================================================================\n",
      "‚úì Complete GLM analysis workflow demonstrated!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXERCISE 3.6 SOLUTION: Complete GLM Analysis\n",
    "# =============================================================================\n",
    "\n",
    "def exercise_3_6_solution():\n",
    "    \"\"\"Complete GLM analysis workflow for hospital readmission.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"EXERCISE 3.6: COMPLETE GLM ANALYSIS WORKFLOW\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    rng = default_rng(42)\n",
    "    n = 1000\n",
    "    \n",
    "    # Simulate predictors\n",
    "    length_stay = rng.exponential(5, n) + 1  # Days\n",
    "    n_comorbid = rng.poisson(2, n)           # Number of comorbidities\n",
    "    age = rng.normal(65, 12, n)              # Age\n",
    "    education = rng.binomial(1, 0.6, n)      # Discharge education (binary)\n",
    "    \n",
    "    X = np.column_stack([np.ones(n), length_stay, n_comorbid, age, education])\n",
    "    \n",
    "    # True model\n",
    "    beta_true = np.array([-2.5, 0.05, 0.3, 0.02, -0.5])\n",
    "    p_true = expit(X @ beta_true)\n",
    "    readmit = rng.binomial(1, p_true)\n",
    "    \n",
    "    print(f\"\\nSimulated hospital readmission data:\")\n",
    "    print(f\"  n = {n}, Readmission rate = {100*np.mean(readmit):.1f}%\")\n",
    "    print(f\"\\nTrue model: logit(p) = {beta_true[0]:.1f} + {beta_true[1]:.2f}¬∑LOS + {beta_true[2]:.1f}¬∑comorbid + {beta_true[3]:.2f}¬∑age + {beta_true[4]:.1f}¬∑education\")\n",
    "    \n",
    "    # Fit model\n",
    "    model = sm.GLM(readmit, X, family=families.Binomial()).fit()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"MODEL SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n{'Variable':<15} {'True Œ≤':<10} {'Œ≤ÃÇ':<10} {'SE':<10} {'OR':<10} {'p-value':<10}\")\n",
    "    print(\"-\"*70)\n",
    "    names = ['Intercept', 'LOS', 'Comorbid', 'Age', 'Education']\n",
    "    for i, name in enumerate(names):\n",
    "        print(f\"{name:<15} {beta_true[i]:<10.3f} {model.params[i]:<10.3f} {model.bse[i]:<10.3f} {np.exp(model.params[i]):<10.3f} {model.pvalues[i]:<10.4f}\")\n",
    "    \n",
    "    # Model fit\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"MODEL FIT DIAGNOSTICS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"  Deviance = {model.deviance:.2f}\")\n",
    "    print(f\"  Pearson œá¬≤ = {model.pearson_chi2:.2f}\")\n",
    "    print(f\"  df = {model.df_resid}\")\n",
    "    print(f\"  Deviance/df = {model.deviance/model.df_resid:.3f} (should be ‚âà 1)\")\n",
    "    \n",
    "    # LRT for education effect\n",
    "    model_reduced = sm.GLM(readmit, X[:, :-1], family=families.Binomial()).fit()\n",
    "    lrt_stat = 2 * (model.llf - model_reduced.llf)\n",
    "    lrt_pval = 1 - chi2.cdf(lrt_stat, df=1)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"LIKELIHOOD RATIO TEST FOR EDUCATION EFFECT\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"  H‚ÇÄ: Œ≤_education = 0\")\n",
    "    print(f\"  LRT statistic = {lrt_stat:.4f}\")\n",
    "    print(f\"  p-value = {lrt_pval:.4f}\")\n",
    "    print(f\"  Conclusion: {'Reject H‚ÇÄ' if lrt_pval < 0.05 else 'Fail to reject H‚ÇÄ'} at Œ±=0.05\")\n",
    "    \n",
    "    # Key interpretation\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"INTERPRETATION\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"  ‚Ä¢ Each additional day in hospital increases odds by {100*(np.exp(model.params[1])-1):.1f}%\")\n",
    "    print(f\"  ‚Ä¢ Each additional comorbidity increases odds by {100*(np.exp(model.params[2])-1):.0f}%\")\n",
    "    print(f\"  ‚Ä¢ Discharge education REDUCES odds by {100*(1-np.exp(model.params[4])):.0f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úì Complete GLM analysis workflow demonstrated!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "exercise_3_6_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapter 3 Summary\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "| Section | Core Concept | Key Formula |\n",
    "|---------|--------------|-------------|\n",
    "| 3.1 | Exponential Families | $f(x|\\eta) = h(x)\\exp\\{\\eta^\\top T(x) - A(\\eta)\\}$ |\n",
    "| 3.2 | Maximum Likelihood | $U(\\hat{\\theta}) = 0$, asymptotic normality |\n",
    "| 3.3 | Sampling Variability | Delta method: $\\text{Var}(g(\\hat{\\theta})) \\approx [g'(\\theta)]^2 \\text{Var}(\\hat{\\theta})$ |\n",
    "| 3.4 | Linear Models | $\\hat{\\beta} = (X'X)^{-1}X'y$ (BLUE under Gauss-Markov) |\n",
    "| 3.5 | GLMs | $g(\\mu) = X\\beta$, IRLS algorithm |\n",
    "\n",
    "## Connections Forward\n",
    "\n",
    "- **Chapter 4 (Bootstrap)**: Alternative variance estimates when asymptotics fail\n",
    "- **Chapter 5 (Bayesian)**: Prior √ó Likelihood ‚Üí Posterior\n",
    "\n",
    "**Happy modeling!** üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
