<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Bernoulli Family Tree: Distribution Relationships</title>
    
    <!-- KaTeX for Math Rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,400;0,500;0,600;0,700;1,400&family=Source+Sans+3:wght@300;400;500;600&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    
    <style>
        :root {
            --bg-primary: #faf8f5;
            --bg-secondary: #f0ebe3;
            --bg-card: #ffffff;
            --text-primary: #1a1a2e;
            --text-secondary: #4a4a68;
            --text-muted: #666680;
            --accent-discrete: #2d5a7b;
            --accent-discrete-light: #4a8ab8;
            --accent-continuous: #7b4a2d;
            --accent-continuous-light: #b86a4a;
            --accent-bernoulli: #5a2d7b;
            --accent-bernoulli-light: #8a4ab8;
            --border-color: #e0dcd4;
            --shadow-sm: 0 1px 3px rgba(0,0,0,0.08);
            --shadow-md: 0 4px 12px rgba(0,0,0,0.1);
            --shadow-lg: 0 8px 30px rgba(0,0,0,0.12);
            --transition-fast: 0.2s ease;
            --transition-med: 0.4s cubic-bezier(0.4, 0, 0.2, 1);
        }
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Source Sans 3', sans-serif;
            background: var(--bg-primary);
            color: var(--text-primary);
            line-height: 1.7;
            font-size: 16px;
        }
        
        /* Typography */
        h1, h2, h3, h4 {
            font-family: 'Crimson Pro', serif;
            font-weight: 600;
            line-height: 1.3;
        }
        
        h1 {
            font-size: clamp(2.5rem, 5vw, 4rem);
            letter-spacing: -0.02em;
        }
        
        h2 {
            font-size: 1.75rem;
            margin-bottom: 1rem;
        }
        
        h3 {
            font-size: 1.35rem;
            margin-bottom: 0.75rem;
        }
        
        /* Header */
        header {
            background: linear-gradient(135deg, var(--accent-bernoulli) 0%, #3d1a5c 100%);
            color: white;
            padding: 4rem 2rem;
            text-align: center;
            position: relative;
            overflow: hidden;
        }
        
        header::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: url("data:image/svg+xml,%3Csvg width='60' height='60' viewBox='0 0 60 60' xmlns='http://www.w3.org/2000/svg'%3E%3Cg fill='none' fill-rule='evenodd'%3E%3Cg fill='%23ffffff' fill-opacity='0.05'%3E%3Cpath d='M36 34v-4h-2v4h-4v2h4v4h2v-4h4v-2h-4zm0-30V0h-2v4h-4v2h4v4h2V6h4V4h-4zM6 34v-4H4v4H0v2h4v4h2v-4h4v-2H6zM6 4V0H4v4H0v2h4v4h2V6h4V4H6z'/%3E%3C/g%3E%3C/g%3E%3C/svg%3E");
            opacity: 0.5;
        }
        
        header h1 {
            position: relative;
            z-index: 1;
            text-shadow: 0 2px 20px rgba(0,0,0,0.3);
        }
        
        header .subtitle {
            position: relative;
            z-index: 1;
            font-size: 1.25rem;
            opacity: 0.9;
            margin-top: 1rem;
            font-weight: 300;
        }
        
        /* Main Container */
        .container {
            max-width: 2000px;
            margin: 0 auto;
            padding: 2rem;
        }
        
        /* Derivation Chain */
        .derivation-chain {
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
            padding: 1rem 1.25rem;
            background: linear-gradient(135deg, rgba(90, 45, 123, 0.08) 0%, rgba(138, 74, 184, 0.04) 100%);
            border-radius: 8px;
            margin-bottom: 1rem;
            border: 1px solid rgba(90, 45, 123, 0.15);
        }
        
        .derivation-chain .chain-label {
            font-size: 0.75rem;
            font-weight: 600;
            color: var(--accent-bernoulli);
            text-transform: uppercase;
            letter-spacing: 0.05em;
            margin-right: 0.5rem;
        }
        
        .derivation-chain .chain-item {
            font-size: 0.85rem;
            color: var(--text-secondary);
            padding: 0.25rem 0.5rem;
            background: rgba(255, 255, 255, 0.7);
            border-radius: 4px;
            font-family: 'Crimson Pro', serif;
        }
        
        .derivation-chain .chain-arrow {
            color: var(--accent-bernoulli-light);
            font-size: 0.9rem;
        }
        
        /* Introduction */
        .intro {
            background: var(--bg-card);
            border-radius: 12px;
            padding: 2.5rem;
            margin: -3rem auto 2rem;
            max-width: 900px;
            position: relative;
            z-index: 10;
            box-shadow: var(--shadow-lg);
            border: 1px solid var(--border-color);
        }
        
        .intro p {
            font-size: 1.1rem;
            color: var(--text-secondary);
            margin-bottom: 1rem;
        }
        
        .intro p:last-child {
            margin-bottom: 0;
        }
        
        /* Bernoulli Central Card */
        .bernoulli-central {
            background: linear-gradient(145deg, var(--accent-bernoulli-light), var(--accent-bernoulli));
            color: white;
            border-radius: 16px;
            padding: 2.5rem;
            text-align: center;
            margin: 2rem auto;
            max-width: 700px;
            box-shadow: var(--shadow-lg), 0 0 60px rgba(90, 45, 123, 0.2);
            position: relative;
            overflow: hidden;
        }
        
        .bernoulli-central::after {
            content: '';
            position: absolute;
            top: -50%;
            left: -50%;
            width: 200%;
            height: 200%;
            background: radial-gradient(circle, rgba(255,255,255,0.1) 0%, transparent 60%);
            animation: shimmer 8s infinite linear;
        }
        
        @keyframes shimmer {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
        
        .bernoulli-central h2 {
            position: relative;
            z-index: 1;
            font-size: 2rem;
            margin-bottom: 1rem;
        }
        
        .bernoulli-central .math-display {
            position: relative;
            z-index: 1;
            background: rgba(255,255,255,0.15);
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            font-size: 1.2rem;
        }
        
        .bernoulli-central p {
            position: relative;
            z-index: 1;
            opacity: 0.95;
        }
        
        /* Mode Toggle */
        .mode-toggle {
            display: flex;
            justify-content: center;
            gap: 0;
            margin: 3rem auto;
            background: var(--bg-secondary);
            border-radius: 50px;
            padding: 6px;
            max-width: 500px;
            box-shadow: var(--shadow-sm);
        }
        
        .mode-btn {
            flex: 1;
            padding: 1rem 2rem;
            border: none;
            background: transparent;
            font-family: 'Crimson Pro', serif;
            font-size: 1.1rem;
            font-weight: 600;
            cursor: pointer;
            border-radius: 50px;
            transition: var(--transition-med);
            color: var(--text-muted);
        }
        
        .mode-btn.active {
            box-shadow: var(--shadow-md);
        }
        
        .mode-btn.discrete.active {
            background: var(--accent-discrete);
            color: white;
        }
        
        .mode-btn.continuous.active {
            background: var(--accent-continuous);
            color: white;
        }
        
        .mode-btn:hover:not(.active) {
            color: var(--text-primary);
        }
        
        /* Distribution Sections */
        .distribution-section {
            display: none;
            animation: fadeIn 0.5s ease;
        }
        
        .distribution-section.active {
            display: block;
        }
        
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(20px); }
            to { opacity: 1; transform: translateY(0); }
        }
        
        .section-header {
            text-align: center;
            margin: 3rem 0 2rem;
            padding-bottom: 1rem;
            border-bottom: 2px solid var(--border-color);
        }
        
        .section-header.discrete h2 {
            color: var(--accent-discrete);
        }
        
        .section-header.continuous h2 {
            color: var(--accent-continuous);
        }
        
        /* Relationship Flow */
        .relationship-flow {
            display: flex;
            flex-direction: column;
            gap: 2rem;
            margin: 2rem 0;
        }
        
        .flow-level {
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            gap: 2rem;
        }
        
        .flow-arrow {
            display: flex;
            justify-content: center;
            padding: 0.5rem;
            color: var(--text-muted);
            font-size: 1.5rem;
        }
        
        /* Distribution Cards */
        .dist-card {
            background: var(--bg-card);
            border-radius: 12px;
            padding: 2rem 3rem;
            width: 950px;
            min-width: 850px;
            max-width: 95vw;
            box-shadow: var(--shadow-sm);
            border: 1px solid var(--border-color);
            transition: var(--transition-med);
            position: relative;
        }
        
        .dist-card:hover {
            box-shadow: var(--shadow-md);
            transform: translateY(-4px);
        }
        
        .dist-card.discrete {
            border-left: 4px solid var(--accent-discrete);
        }
        
        .dist-card.continuous {
            border-left: 4px solid var(--accent-continuous);
        }
        
        .dist-card h3 {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .dist-card.discrete h3 {
            color: var(--accent-discrete);
        }
        
        .dist-card.continuous h3 {
            color: var(--accent-continuous);
        }
        
        .dist-badge {
            font-size: 0.7rem;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
            font-family: 'Source Sans 3', sans-serif;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.05em;
        }
        
        .dist-badge.discrete {
            background: rgba(45, 90, 123, 0.1);
            color: var(--accent-discrete);
        }
        
        .dist-badge.continuous {
            background: rgba(123, 74, 45, 0.1);
            color: var(--accent-continuous);
        }
        
        .dist-formula {
            background: var(--bg-secondary);
            border-radius: 8px;
            padding: 1.25rem;
            margin: 1rem 0;
            font-size: 1rem;
        }
        
        .dist-params {
            font-size: 0.9rem;
            color: var(--text-secondary);
            margin: 0.75rem 0;
        }
        
        .dist-params strong {
            color: var(--text-primary);
        }
        
        /* Python Code Blocks */
        .python-code {
            background: #1e1e2e;
            border-radius: 6px;
            padding: 0.75rem 1rem;
            margin-top: 1rem;
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.8rem;
            color: #cdd6f4;
            overflow-x: auto;
            border: 1px solid #313244;
        }
        
        .python-code .comment {
            color: #a9b1d6;
        }
        
        .python-code .keyword {
            color: #cba6f7;
        }
        
        .python-code .function {
            color: #89b4fa;
        }
        
        .python-code .string {
            color: #a6e3a1;
        }
        
        .python-code .number {
            color: #fab387;
        }
        
        .python-label {
            font-size: 0.75rem;
            font-weight: 600;
            color: var(--text-muted);
            text-transform: uppercase;
            letter-spacing: 0.05em;
            margin-top: 1rem;
            margin-bottom: 0.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .python-label::before {
            content: 'üêç';
            font-size: 0.9rem;
        }
        
        /* Connection Boxes */
        .connection-box {
            background: linear-gradient(135deg, var(--bg-secondary) 0%, var(--bg-card) 100%);
            border-radius: 12px;
            padding: 2rem;
            margin: 2rem auto;
            max-width: 1100px;
            border: 1px solid var(--border-color);
            box-shadow: var(--shadow-sm);
        }
        
        .connection-box h3 {
            color: var(--accent-bernoulli);
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.75rem;
        }
        
        .connection-icon {
            font-size: 1.5rem;
        }
        
        /* Derivation Toggles */
        .derivation-toggle {
            margin-top: 1rem;
        }
        
        .derivation-btn {
            background: none;
            border: 1px solid var(--border-color);
            padding: 0.5rem 1rem;
            border-radius: 6px;
            font-family: 'Source Sans 3', sans-serif;
            font-size: 0.85rem;
            cursor: pointer;
            transition: var(--transition-fast);
            color: var(--text-secondary);
            display: flex;
            align-items: center;
            gap: 0.5rem;
            width: 100%;
            justify-content: space-between;
        }
        
        .derivation-btn:hover {
            background: var(--bg-secondary);
            border-color: var(--accent-bernoulli-light);
            color: var(--text-primary);
        }
        
        .derivation-btn .arrow {
            transition: transform 0.3s ease;
        }
        
        .derivation-btn.open .arrow {
            transform: rotate(180deg);
        }
        
        .derivation-content {
            display: none;
            margin-top: 1rem;
            padding: 1.75rem;
            background: var(--bg-secondary);
            border-radius: 8px;
            animation: slideDown 0.3s ease;
            font-size: 0.95rem;
        }
        
        .derivation-content.open {
            display: block;
        }
        
        @keyframes slideDown {
            from { opacity: 0; transform: translateY(-10px); }
            to { opacity: 1; transform: translateY(0); }
        }
        
        .derivation-content h4 {
            font-size: 1.1rem;
            color: var(--accent-bernoulli);
            margin-bottom: 1rem;
            margin-top: 1.5rem;
            font-family: 'Source Sans 3', sans-serif;
            font-weight: 600;
        }
        
        .derivation-content h4:first-child {
            margin-top: 0;
        }
        
        .derivation-content p {
            margin-bottom: 1rem;
            color: var(--text-secondary);
            line-height: 1.7;
        }
        
        .derivation-content .math-block {
            background: var(--bg-card);
            padding: 1.5rem 2rem;
            border-radius: 8px;
            margin: 1.5rem 0;
            border: 1px solid var(--border-color);
            font-size: 1.1rem;
            text-align: center;
            overflow-x: auto;
        }
        
        .step {
            margin: 1.5rem 0;
            padding: 1.25rem 1.5rem 1.25rem 1.75rem;
            border-left: 4px solid var(--accent-bernoulli-light);
            background: rgba(255, 255, 255, 0.6);
            border-radius: 0 8px 8px 0;
        }
        
        .step-number {
            font-weight: 600;
            color: var(--accent-bernoulli);
            font-size: 0.9rem;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            display: block;
            margin-bottom: 0.75rem;
        }
        
        .step p {
            margin-bottom: 0.75rem;
            line-height: 1.7;
        }
        
        .step .math-block {
            margin: 1rem 0;
            font-size: 1.05rem;
        }
        
        /* Transition Cards */
        .transition-card {
            background: linear-gradient(135deg, var(--bg-card) 0%, var(--bg-secondary) 100%);
            border-radius: 12px;
            padding: 2rem;
            margin: 2rem auto;
            max-width: 1200px;
            box-shadow: var(--shadow-md);
            border: 2px dashed var(--accent-bernoulli-light);
            overflow: hidden;
        }
        
        .transition-card h3 {
            text-align: center;
            color: var(--accent-bernoulli);
            margin-bottom: 1.5rem;
        }
        
        .transition-grid {
            display: grid;
            grid-template-columns: 1fr auto 1fr;
            gap: 1.5rem;
            align-items: stretch;
            background: rgba(255, 255, 255, 0.5);
            border-radius: 8px;
            padding: 1.5rem;
            border: 1px solid var(--border-color);
        }
        
        .transition-grid .dist-card {
            margin: 0;
            min-height: 100%;
            box-shadow: var(--shadow-sm);
            /* Override the fixed widths for cards inside transition grids */
            width: 100% !important;
            min-width: unset !important;
            max-width: 100% !important;
        }
        
        .transition-arrow {
            font-size: 2rem;
            color: var(--accent-bernoulli);
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            gap: 0.5rem;
            padding: 0 1rem;
        }
        
        .transition-arrow span {
            font-size: 0.8rem;
            color: var(--text-muted);
            text-align: center;
        }
        
        /* Summary Table */
        .summary-table {
            width: 100%;
            border-collapse: collapse;
            margin: 2rem 0;
            font-size: 0.9rem;
        }
        
        .summary-table th,
        .summary-table td {
            padding: 1rem;
            text-align: left;
            border-bottom: 1px solid var(--border-color);
        }
        
        .summary-table th {
            background: var(--bg-secondary);
            font-family: 'Crimson Pro', serif;
            font-weight: 600;
            color: var(--text-primary);
        }
        
        .summary-table tr:hover td {
            background: rgba(90, 45, 123, 0.03);
        }


        .sr-only {
            position: absolute;
            width: 1px;
            height: 1px;
            padding: 0;
            margin: -1px;
            overflow: hidden;
            clip: rect(0, 0, 0, 0);
            white-space: nowrap;
            border: 0;
        }
        
        /* Key Insight Boxes */
        .key-insight {
            background: linear-gradient(135deg, rgba(90, 45, 123, 0.08) 0%, rgba(138, 74, 184, 0.05) 100%);
            border-left: 4px solid var(--accent-bernoulli);
            padding: 1.5rem;
            margin: 2rem 0;
            border-radius: 0 8px 8px 0;
        }
        
        .key-insight h4 {
            color: var(--accent-bernoulli);
            margin-bottom: 0.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        /* Footer */
        footer {
            background: var(--text-primary);
            color: rgba(255,255,255,0.7);
            padding: 3rem 2rem;
            margin-top: 4rem;
            text-align: center;
        }
        
        footer p {
            max-width: 700px;
            margin: 0 auto;
            font-size: 0.95rem;
        }
                
        .skip-link {
            position: absolute;
            top: -40px;
            left: 0;
            background: var(--accent-bernoulli);
            color: white;
            padding: 8px 16px;
            z-index: 100;
            transition: top 0.3s;
        }

        .skip-link:focus {
            top: 0;
        }

        :focus {
            outline: 2px solid var(--accent-bernoulli);
            outline-offset: 2px;
        }

        :focus:not(:focus-visible) {
            outline: none;
        }

        :focus-visible {
            outline: 2px solid var(--accent-bernoulli);
            outline-offset: 2px;
        }
        /* Responsive */
        @media (max-width: 1024px) {
            .dist-card {
                width: 100%;
                min-width: unset;
                max-width: 500px;
            }
            
            .flow-level {
                flex-direction: column;
                align-items: center;
            }
        }
        
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }
            
            .dist-card {
                width: 100%;
                max-width: 100%;
            }
            
            .transition-grid {
                grid-template-columns: 1fr;
            }
            
            .transition-arrow {
                transform: rotate(90deg);
            }
            
            .mode-toggle {
                flex-direction: column;
                border-radius: 12px;
            }
            
            .mode-btn {
                border-radius: 8px;
            }
        }
        
        /* Print styles */
        @media print {
            .derivation-content {
                display: block !important;
            }
            
            .derivation-btn {
                display: none;
            }
        }
    </style>
</head>
<body>
    <a href="#main-content" class="skip-link">Skip to main content</a>
    <header>
        <h1>The Bernoulli Family Tree</h1>
        <p class="subtitle">How One Coin Flip Generates All of Probability Theory</p>
    </header>
    
    <main id="main-content">
    <div class="container">
        <div class="intro">
            <p><strong>The Bernoulli distribution is the atom of probability.</strong> From this simplest random experiment‚Äîa single trial with probability $p$ of success‚Äîwe can derive every major probability distribution through combinations, limits, and transformations.</p>
            <p>This interactive guide shows how discrete distributions arise from sums and sequences of Bernoulli trials, how continuous distributions emerge as limiting cases, and the mathematical machinery (moment generating functions, limits, and transformations) that connects them all.</p>
        </div>
        
        <!-- Central Bernoulli Card -->
        <div class="bernoulli-central">
            <h2>The Bernoulli Distribution</h2>
            <div class="math-display">
                $$X \sim \text{Bernoulli}(p), \quad P(X = k) = p^k(1-p)^{1-k}, \quad k \in \{0, 1\}$$
            </div>
            <p><strong>Parameters:</strong> $p \in [0,1]$ (probability of success)</p>
            <p><strong>MGF:</strong> $M_X(t) = (1-p) + pe^t = 1 + p(e^t - 1)$</p>
            <p><strong>Mean:</strong> $\mathbb{E}[X] = p$ &nbsp;|&nbsp; <strong>Variance:</strong> $\text{Var}(X) = p(1-p)$</p>
            <div style="margin-top: 1rem; background: rgba(0,0,0,0.2); border-radius: 8px; padding: 1rem; font-family: 'JetBrains Mono', monospace; font-size: 0.85rem; text-align: left;">
                <span style="color: #6c7086;"># NumPy</span><br>
                rng.choice([0, 1], p=[1-p, p])<br>
                <span style="color: #6c7086;"># SciPy</span><br>
                scipy.stats.bernoulli(p).rvs()
            </div>
        </div>
        
        <!-- Mode Toggle -->
        <div class="mode-toggle" role="tablist" aria-label="Distribution type">
            <button class="mode-btn discrete active" 
                    role="tab" 
                    aria-selected="true" 
                    aria-controls="discrete-section"
                    id="discrete-tab"
                    onclick="showSection('discrete')">
                Discrete Distributions
            </button>
            <button class="mode-btn continuous" 
                    role="tab" 
                    aria-selected="false" 
                    aria-controls="continuous-section"
                    id="continuous-tab"
                    onclick="showSection('continuous')">
                Continuous Distributions
            </button>
        </div>
        
        <!-- DISCRETE SECTION -->
        <div id="discrete-section" class="distribution-section active">
            <div class="section-header discrete">
                <h2>Discrete Distributions from Bernoulli</h2>
                <p>Sums, sequences, and counting arguments generate the discrete family</p>
            </div>
            
            <!-- Level 1: Direct from Bernoulli -->
            <div class="connection-box">
                <h3><span class="connection-icon">üé≤</span> The Uniform Foundation</h3>
                <p>The discrete uniform distribution provides equal probability to each outcome‚Äîthe unbiased die roll that underlies all random generation.</p>
            </div>
            
            <div class="flow-level">
                <!-- Discrete Uniform -->
                <div class="dist-card discrete">
                    <h3>Discrete Uniform <span class="dist-badge discrete">Discrete</span></h3>
                    <div class="dist-formula">
                        $$X \sim \text{DiscreteUniform}(a, b)$$
                        $$P(X = k) = \frac{1}{b - a + 1}, \quad k \in \{a, a+1, \ldots, b\}$$
                    </div>
                    <p class="dist-params"><strong>Construction:</strong> Equal probability on integers $\{a, \ldots, b\}$</p>
                    <p class="dist-params"><strong>Special case:</strong> $\text{Bernoulli}(0.5) = \text{DiscreteUniform}(0, 1)$</p>
                    
                        <div class="derivation-toggle">
                                <button class="derivation-btn" onclick="toggleDerivation(this)" aria-expanded="false" aria-controls="derivation-discrete-uniform">
                                <span>Show Derivation from Bernoulli</span>
                                <span class="arrow" aria-hidden="true">‚ñº</span>
                            </button>
                            <div class="derivation-content" id="derivation-discrete-uniform"aria-hidden="true">
                            <div class="derivation-chain">
                                <span class="chain-label">Path:</span>
                                <span class="chain-item">Bernoulli$(0.5)$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">Binary expansion</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item"><strong>DiscreteUniform$(a,b)$</strong></span>
                            </div>
                            
                            <h4>Binary Expansion Construction</h4>
                            <p>When the range size $n = b - a + 1$ is a power of 2, discrete uniform distributions can be constructed exactly from independent fair coin flips (Bernoulli$(0.5)$) using binary representation.</p>
                            
                            <div class="step">
                                <span class="step-number">Step 1: Define the range (power of 2)</span>
                                <p>Suppose $n = 2^m$ for some integer $m$. We need exactly $m$ bits.</p>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 2: Generate binary digits from Bernoulli</span>
                                <p>Let $B_1, B_2, \ldots, B_m \stackrel{\text{iid}}{\sim} \text{Bernoulli}(0.5)$. Construct:</p>
                                <div class="math-block">
                                    $$Y = \sum_{i=1}^{m} B_i \cdot 2^{i-1} \sim \text{DiscreteUniform}(0, 2^m - 1)$$
                                </div>
                                <p>Each of the $2^m$ binary strings is equally likely, so $Y$ is uniform on $\{0, 1, \ldots, 2^m - 1\}$.</p>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 3: Shift to desired range</span>
                                <p>Return $X = a + Y$, giving DiscreteUniform$(a, a + 2^m - 1)$.</p>
                                <div class="math-block">
                                    $$X = a + \sum_{i=1}^{m} B_i \cdot 2^{i-1}$$
                                </div>
                            </div>
                            
                            <div class="step" style="background: rgba(123, 74, 45, 0.1); border-left-color: var(--accent-continuous);">
                                <span class="step-number">Non-power-of-2 ranges</span>
                                <p>When $n$ is not a power of 2, additional techniques are required to avoid bias. These methods (including rejection sampling) are covered in <strong>Chapter 2: Monte Carlo Methods</strong>.</p>
                            </div>
                            
                            <h4>Moments</h4>
                            <div class="math-block">
                                $$\mathbb{E}[X] = \frac{a + b}{2}, \qquad \text{Var}(X) = \frac{(b - a + 1)^2 - 1}{12}$$
                            </div>
                            
                            <h4>MGF</h4>
                            <div class="math-block">
                                $$M_X(t) = \frac{e^{at}(1 - e^{(b-a+1)t})}{(b-a+1)(1 - e^t)}$$
                            </div>
                        </div>
                    </div>
                    
                    <div class="python-label">Python</div>
                    <div class="python-code">
                        <span class="comment"># === Sampling ===</span><br>
                        rng.integers(a, b+1)  <span class="comment"># single sample</span><br>
                        rng.integers(a, b+1, size=1000)  <span class="comment"># array</span><br><br>
                        <span class="comment"># === SciPy distribution object ===</span><br>
                        dist = scipy.stats.randint(a, b+1)<br>
                        dist.rvs(size=1000)  <span class="comment"># sampling</span><br>
                        dist.pmf(k)  <span class="comment"># P(X = k)</span><br>
                        dist.cdf(k)  <span class="comment"># P(X ‚â§ k)</span><br>
                        dist.ppf(q)  <span class="comment"># quantile (inverse CDF)</span><br>
                        dist.mean(), dist.var()  <span class="comment"># moments</span>
                    </div>
                </div>
            </div>
            
            <div class="flow-arrow">‚¨á</div>
            
            <div class="connection-box">
                <h3><span class="connection-icon">üîó</span> Direct Constructions from Bernoulli</h3>
                <p>The first generation of distributions arises directly from operations on independent Bernoulli trials.</p>
            </div>
            
            <div class="flow-level">
                <!-- Binomial -->
                <div class="dist-card discrete">
                    <h3>Binomial <span class="dist-badge discrete">Discrete</span></h3>
                    <div class="dist-formula">
                        $$S_n \sim \text{Binomial}(n, p)$$
                        $$P(S_n = k) = \binom{n}{k}p^k(1-p)^{n-k}, \quad k = 0, 1, \ldots, n$$
                    </div>
                    <p class="dist-params"><strong>Functional form:</strong> $S_n = \sum_{i=1}^n X_i$ where $X_i \stackrel{\text{iid}}{\sim} \text{Bernoulli}(p)$</p>
                    <p class="dist-params"><strong>Interpretation:</strong> Count of successes in $n$ independent Bernoulli trials</p>
                    
                    <div class="derivation-toggle">
                        <button class="derivation-btn" onclick="toggleDerivation(this)" aria-expanded="false" aria-controls="derivation-binomial">
                            <span>Show Derivation from Bernoulli</span>
                            <span class="arrow">‚ñº</span>
                        </button>
                        <div class="derivation-content" id="derivation-binomial" aria-hidden="true">
                            <div class="derivation-chain">
                                <span class="chain-label">Path:</span>
                                <span class="chain-item">$X_1, \ldots, X_n \stackrel{\text{iid}}{\sim}$ Bernoulli$(p)$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">$S_n = \sum_{i=1}^n X_i$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item"><strong>Binomial$(n,p)$</strong></span>
                            </div>
                            
                            <h4>Derivation from Bernoulli via MGF</h4>
                            <p>Let $X_1, X_2, \ldots, X_n \stackrel{\text{iid}}{\sim} \text{Bernoulli}(p)$ and define $S_n = \sum_{i=1}^n X_i$.</p>
                            
                            <div class="step">
                                <span class="step-number">Step 1: Start with Bernoulli MGF</span>
                                <p>Each Bernoulli random variable has MGF:</p>
                                <div class="math-block">
                                    $$M_{X_i}(t) = \mathbb{E}[e^{tX_i}] = (1-p) \cdot e^{t \cdot 0} + p \cdot e^{t \cdot 1} = (1-p) + pe^t$$
                                </div>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 2: Apply sum property for independent RVs</span>
                                <p>For independent random variables, the MGF of the sum equals the product of MGFs:</p>
                                <div class="math-block">
                                    $$M_{S_n}(t) = \prod_{i=1}^n M_{X_i}(t) = \left[(1-p) + pe^t\right]^n$$
                                </div>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 3: Identify the Binomial distribution</span>
                                <p>This is exactly the MGF of Binomial$(n, p)$. Since MGFs uniquely determine distributions:</p>
                                <div class="math-block">
                                    $$S_n = \sum_{i=1}^n X_i \sim \text{Binomial}(n, p)$$
                                </div>
                            </div>
                            
                            <h4>Alternative: Direct Combinatorial Derivation</h4>
                            <p>The PMF follows from counting: choose which $k$ of the $n$ trials are successes.</p>
                            <div class="math-block">
                                $$P(S_n = k) = \underbrace{\binom{n}{k}}_{\substack{\text{ways to choose}\\\text{which } k \text{ succeed}}} \times \underbrace{p^k}_{\substack{\text{prob. of } k\\\text{successes}}} \times \underbrace{(1-p)^{n-k}}_{\substack{\text{prob. of } n-k\\\text{failures}}}$$
                            </div>
                        </div>
                    </div>
                    
                    <div class="python-label">Python</div>
                    <div class="python-code">
                        <span class="comment"># === Sampling ===</span><br>
                        rng.binomial(n, p)  <span class="comment"># single sample</span><br>
                        rng.binomial(n, p, size=1000)  <span class="comment"># array</span><br><br>
                        <span class="comment"># === SciPy distribution object ===</span><br>
                        dist = scipy.stats.binom(n, p)<br>
                        dist.rvs(size=1000)  <span class="comment"># sampling</span><br>
                        dist.pmf(k)  <span class="comment"># P(X = k)</span><br>
                        dist.cdf(k)  <span class="comment"># P(X ‚â§ k)</span><br>
                        dist.sf(k)   <span class="comment"># P(X > k) survival</span><br>
                        dist.ppf(q)  <span class="comment"># quantile (inverse CDF)</span><br>
                        dist.mean(), dist.var()  <span class="comment"># np, np(1-p)</span><br>
                        dist.interval(0.95)  <span class="comment"># 95% interval</span>
                    </div>
                </div>
                
                <!-- Multinomial -->
                <div class="dist-card discrete">
                    <h3>Multinomial <span class="dist-badge discrete">Discrete</span></h3>
                    <div class="dist-formula">
                        $$\mathbf{X} \sim \text{Multinomial}(n, \mathbf{p})$$
                        $$P(X_1 = x_1, \ldots, X_k = x_k) = \frac{n!}{x_1! \cdots x_k!} p_1^{x_1} \cdots p_k^{x_k}$$
                    </div>
                    <p class="dist-params"><strong>Constraint:</strong> $\sum_{i=1}^k x_i = n$ and $\sum_{i=1}^k p_i = 1$</p>
                    <p class="dist-params"><strong>Generalization:</strong> Binomial$(n, p)$ is Multinomial$(n, (p, 1-p))$ with $k = 2$</p>
                    
                    <div class="derivation-toggle">
                        <button class="derivation-btn" onclick="toggleDerivation(this)" aria-expanded="false" aria-controls="derivation-multinomial">
                            <span>Show Derivation from Bernoulli</span>
                            <span class="arrow">‚ñº</span>
                        </button>
                        <div class="derivation-content" id="derivation-multinomial" aria-hidden="true">
                            <div class="derivation-chain">
                                <span class="chain-label">Path:</span>
                                <span class="chain-item">$n$ iid categorical trials</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">Count each category</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item"><strong>Multinomial$(n, \mathbf{p})$</strong></span>
                            </div>
                            
                            <h4>From Bernoulli to Multinomial</h4>
                            <p>The Multinomial generalizes the Binomial from 2 categories to $k$ categories. Just as Binomial counts successes vs failures, Multinomial counts outcomes in each of $k$ possible categories.</p>
                            
                            <div class="step">
                                <span class="step-number">Step 1: Generalized Bernoulli (Categorical) trial</span>
                                <p>A single trial results in one of $k$ mutually exclusive outcomes with probabilities $p_1, p_2, \ldots, p_k$ where $\sum_{i=1}^k p_i = 1$.</p>
                                <p>This is a <strong>Categorical</strong> distribution‚Äîthe multi-outcome analog of Bernoulli.</p>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 2: Perform $n$ independent trials</span>
                                <p>Let $X_j$ = count of trials resulting in category $j$, for $j = 1, \ldots, k$.</p>
                                <p>By construction: $\sum_{j=1}^k X_j = n$ (every trial lands in exactly one category).</p>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 3: Derive the PMF</span>
                                <p>To get exactly $(x_1, \ldots, x_k)$ counts:</p>
                                <div class="math-block">
                                    $$P(\mathbf{X} = \mathbf{x}) = \underbrace{\binom{n}{x_1, \ldots, x_k}}_{\text{multinomial coeff.}} \times \underbrace{p_1^{x_1} \cdots p_k^{x_k}}_{\text{probability of one such sequence}}$$
                                </div>
                                <p>The multinomial coefficient $\frac{n!}{x_1! \cdots x_k!}$ counts the ways to arrange $n$ trials into groups of sizes $x_1, \ldots, x_k$.</p>
                            </div>
                            
                            <h4>Connection to Binomial</h4>
                            <div class="step">
                                <span class="step-number">Special case: $k = 2$</span>
                                <p>With two categories (success/failure) and $\mathbf{p} = (p, 1-p)$:</p>
                                <div class="math-block">
                                    $$P(X_1 = x, X_2 = n-x) = \frac{n!}{x!(n-x)!}p^x(1-p)^{n-x} = \binom{n}{x}p^x(1-p)^{n-x}$$
                                </div>
                                <p>This is exactly Binomial$(n, p)$!</p>
                            </div>
                            
                            <h4>Marginal Distributions</h4>
                            <div class="step">
                                <span class="step-number">Each component is Binomial</span>
                                <p>Marginally, each $X_j \sim \text{Binomial}(n, p_j)$. But the $X_j$'s are <em>not</em> independent‚Äîthey must sum to $n$.</p>
                                <div class="math-block">
                                    $$\mathbb{E}[X_j] = np_j, \qquad \text{Var}(X_j) = np_j(1 - p_j)$$
                                </div>
                                <div class="math-block">
                                    $$\text{Cov}(X_i, X_j) = -np_i p_j \quad \text{for } i \neq j$$
                                </div>
                                <p>The negative covariance reflects the constraint: more in one category means fewer in others.</p>
                            </div>
                            
                            <h4>Conjugate Prior: Dirichlet</h4>
                            <p>Just as Beta is conjugate to Binomial, the <strong>Dirichlet</strong> distribution is conjugate to Multinomial‚Äîit's the multivariate generalization of Beta for probability vectors.</p>
                        </div>
                    </div>
                    
                    <div class="python-label">Python</div>
                    <div class="python-code">
                        <span class="comment"># === Sampling ===</span><br>
                        <span class="comment"># p = [p1, p2, ..., pk] probability vector</span><br>
                        rng.multinomial(n, p)  <span class="comment"># returns [x1, ..., xk]</span><br>
                        rng.multinomial(n, p, size=1000)  <span class="comment"># shape (1000, k)</span><br><br>
                        <span class="comment"># === SciPy distribution object ===</span><br>
                        dist = scipy.stats.multinomial(n, p)<br>
                        dist.rvs(size=1000)  <span class="comment"># sampling</span><br>
                        dist.pmf([x1, x2, x3])  <span class="comment"># P(X = x)</span><br>
                        dist.mean()  <span class="comment"># [np1, np2, ..., npk]</span><br>
                        dist.cov()   <span class="comment"># covariance matrix</span>
                    </div>
                </div>
                
                <!-- Geometric -->
                <div class="dist-card discrete">
                    <h3>Geometric <span class="dist-badge discrete">Discrete</span></h3>
                    <div class="dist-formula">
                        $$Y \sim \text{Geometric}(p)$$
                        $$P(Y = k) = (1-p)^{k-1}p, \quad k = 1, 2, \ldots$$
                    </div>
                    <p class="dist-params"><strong>Functional form:</strong> $Y = \min\{k \geq 1 : X_k = 1\}$ where $X_k \stackrel{\text{iid}}{\sim} \text{Bernoulli}(p)$</p>
                    <p class="dist-params"><strong>Interpretation:</strong> First hitting time of success in Bernoulli sequence</p>
                    
                    <div class="derivation-toggle">
                        <button class="derivation-btn" onclick="toggleDerivation(this)" aria-expanded="false" aria-controls="derivation-geometric">
                            <span>Show Derivation from Bernoulli</span>
                            <span class="arrow">‚ñº</span>
                        </button>
                        <div class="derivation-content" id="derivation-geometric" aria-hidden="true">
                            <div class="derivation-chain">
                                <span class="chain-label">Path:</span>
                                <span class="chain-item">$X_1, X_2, \ldots \stackrel{\text{iid}}{\sim}$ Bernoulli$(p)$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">$Y = \min\{k : X_k = 1\}$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item"><strong>Geometric$(p)$</strong></span>
                            </div>
                            
                            <h4>Explicit Functional Construction</h4>
                            <p>Let $X_1, X_2, X_3, \ldots$ be an infinite sequence of iid Bernoulli$(p)$ trials. Define:</p>
                            <div class="math-block">
                                $$Y = \min\{k \in \mathbb{N} : X_k = 1\} = \text{index of first success}$$
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 1: Characterize the event $\{Y = k\}$</span>
                                <p>The event $\{Y = k\}$ occurs if and only if:</p>
                                <div class="math-block">
                                    $$\{Y = k\} = \{X_1 = 0, X_2 = 0, \ldots, X_{k-1} = 0, X_k = 1\}$$
                                </div>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 2: Apply independence</span>
                                <p>Since all $X_i$ are independent:</p>
                                <div class="math-block">
                                    $$P(Y = k) = P(X_1 = 0) \cdots P(X_{k-1} = 0) \cdot P(X_k = 1) = (1-p)^{k-1} \cdot p$$
                                </div>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 3: Verify PMF sums to 1</span>
                                <div class="math-block">
                                    $$\sum_{k=1}^{\infty} p(1-p)^{k-1} = p \sum_{j=0}^{\infty} (1-p)^{j} = p \cdot \frac{1}{p} = 1 \checkmark$$
                                </div>
                            </div>
                            
                            <h4>Survival Function (from Bernoulli product)</h4>
                            <div class="math-block">
                                $$P(Y > k) = P(X_1 = 0, \ldots, X_k = 0) = (1-p)^k$$
                            </div>
                            
                            <h4>Key Property: Memoryless</h4>
                            <p>Geometric is the <em>only</em> discrete memoryless distribution:</p>
                            <div class="math-block">
                                $$P(Y > m + n \mid Y > m) = P(Y > n)$$
                            </div>
                        </div>
                    </div>
                    
                    <div class="python-label">Python</div>
                    <div class="python-code">
                        <span class="comment"># === Sampling ===</span><br>
                        <span class="comment"># NumPy: returns # of trials (1,2,3,...)</span><br>
                        rng.geometric(p, size=1000)<br><br>
                        <span class="comment"># === SciPy distribution object ===</span><br>
                        dist = scipy.stats.geom(p)<br>
                        dist.rvs(size=1000)  <span class="comment"># sampling</span><br>
                        dist.pmf(k)  <span class="comment"># P(Y = k)</span><br>
                        dist.cdf(k)  <span class="comment"># P(Y ‚â§ k)</span><br>
                        dist.sf(k)   <span class="comment"># P(Y > k) = (1-p)^k</span><br>
                        dist.ppf(q)  <span class="comment"># quantile</span><br>
                        dist.mean(), dist.var()  <span class="comment"># 1/p, (1-p)/p¬≤</span>
                    </div>
                </div>
            </div>
            
            <div class="flow-arrow">‚¨á</div>
            
            <!-- Level 2: Second generation -->
            <div class="connection-box">
                <h3><span class="connection-icon">üîó</span> Second-Generation Distributions</h3>
                <p>Extending the counting: sums of geometrics and limits of binomials.</p>
            </div>
            
            <div class="flow-level">
                <!-- Negative Binomial -->
                <div class="dist-card discrete">
                    <h3>Negative Binomial <span class="dist-badge discrete">Discrete</span></h3>
                    <div class="dist-formula">
                        $$X \sim \text{NegBin}(r, p)$$
                        $$P(X = k) = \binom{k-1}{r-1}p^r(1-p)^{k-r}, \quad k = r, r+1, \ldots$$
                    </div>
                    <p class="dist-params"><strong>Functional form:</strong> $X = \sum_{i=1}^r Y_i$ where $Y_i = \min\{k : X_k^{(i)} = 1\} \stackrel{\text{iid}}{\sim} \text{Geom}(p)$</p>
                    <p class="dist-params"><strong>Interpretation:</strong> Trial index of the $r$-th success in Bernoulli sequence</p>
                    
                    <div class="derivation-toggle">
                        <button class="derivation-btn" onclick="toggleDerivation(this)" aria-expanded="false" aria-controls="derivation-negbin">
                            <span>Show Derivation from Bernoulli</span>
                            <span class="arrow">‚ñº</span>
                        </button>
                        <div class="derivation-content" id="derivation-negbin" aria-hidden="true">
                            <div class="derivation-chain">
                                <span class="chain-label">Path:</span>
                                <span class="chain-item">Bernoulli$(p)$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">Geometric$(p)$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">Sum $r$ copies</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item"><strong>NegBin$(r,p)$</strong></span>
                            </div>
                            
                            <h4>Derivation via Geometric Sum (from Bernoulli)</h4>
                            <p>Let $Y_1, \ldots, Y_r \stackrel{\text{iid}}{\sim} \text{Geometric}(p)$ (each itself from Bernoulli trials), and define $X = \sum_{i=1}^r Y_i$.</p>
                            
                            <div class="step">
                                <span class="step-number">Step 1: Recall the Geometric MGF (from Bernoulli)</span>
                                <p>From the Geometric derivation, each waiting time has MGF:</p>
                                <div class="math-block">
                                    $$M_{Y_i}(t) = \frac{pe^t}{1 - (1-p)e^t}$$
                                </div>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 2: Apply product rule for independent sums</span>
                                <p>For the sum of $r$ independent Geometrics:</p>
                                <div class="math-block">
                                    $$M_X(t) = \prod_{i=1}^r M_{Y_i}(t) = \left[\frac{pe^t}{1 - (1-p)e^t}\right]^r$$
                                </div>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 3: Identify the Negative Binomial</span>
                                <p>This is exactly the MGF of NegBin$(r, p)$, confirming:</p>
                                <div class="math-block">
                                    $$X = \sum_{i=1}^r Y_i \sim \text{NegBin}(r, p)$$
                                </div>
                            </div>
                            
                            <h4>Alternative: Direct from Bernoulli Trials</h4>
                            <p>The event $\{X = k\}$ means: in $k$ Bernoulli trials, exactly $r-1$ successes occur in the first $k-1$ trials, then the $k$-th trial is the $r$-th success:</p>
                            <div class="math-block">
                                $$P(X = k) = \underbrace{\binom{k-1}{r-1}}_{\substack{\text{ways to place}\\\text{$r-1$ successes}}} \cdot \underbrace{p^{r-1}(1-p)^{k-r}}_{\substack{\text{first } k-1\\\text{trials}}} \cdot \underbrace{p}_{\substack{k\text{-th}\\\text{trial}}} = \binom{k-1}{r-1}p^r(1-p)^{k-r}$$
                            </div>
                        </div>
                    </div>
                    
                    <div class="python-label">Python</div>
                    <div class="python-code">
                        <span class="comment"># === Sampling ===</span><br>
                        <span class="comment"># NumPy: returns # failures before r successes</span><br>
                        rng.negative_binomial(r, p, size=1000)<br><br>
                        <span class="comment"># === SciPy distribution object ===</span><br>
                        <span class="comment"># nbinom(n, p) = # failures before n successes</span><br>
                        dist = scipy.stats.nbinom(r, p)<br>
                        dist.rvs(size=1000)  <span class="comment"># sampling</span><br>
                        dist.pmf(k)  <span class="comment"># P(X = k)</span><br>
                        dist.cdf(k)  <span class="comment"># P(X ‚â§ k)</span><br>
                        dist.ppf(q)  <span class="comment"># quantile</span><br>
                        dist.mean(), dist.var()  <span class="comment"># r(1-p)/p, r(1-p)/p¬≤</span>
                    </div>
                </div>
                
                <!-- Poisson -->
                <div class="dist-card discrete">
                    <h3>Poisson <span class="dist-badge discrete">Discrete</span></h3>
                    <div class="dist-formula">
                        $$X \sim \text{Poisson}(\lambda)$$
                        $$P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}, \quad k = 0, 1, 2, \ldots$$
                    </div>
                    <p class="dist-params"><strong>Construction:</strong> Limit of Binomial$(n, p)$ as $n \to \infty$, $p \to 0$, $np = \lambda$</p>
                    <p class="dist-params"><strong>Interpretation:</strong> Count of rare events in fixed interval</p>
                    
                    <div class="derivation-toggle">
                        <button class="derivation-btn" onclick="toggleDerivation(this)" aria-expanded="false" aria-controls="derivation-poisson">
                            <span>Show Derivation from Bernoulli</span>
                            <span class="arrow">‚ñº</span>
                        </button>
                        <div class="derivation-content" id="derivation-poisson" aria-hidden="true">
                            <div class="derivation-chain">
                                <span class="chain-label">Path:</span>
                                <span class="chain-item">Bernoulli$(p)$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">Binomial$(n,p)$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">$n \to \infty$, $np \to \lambda$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item"><strong>Poisson$(\lambda)$</strong></span>
                            </div>
                            
                            <h4>The Poisson Limit Theorem (Law of Rare Events)</h4>
                            <p>Starting from Bernoulli: Let $X_n \sim \text{Binomial}(n, p_n)$ (sum of $n$ Bernoullis with $p_n = \lambda/n$).</p>
                            
                            <div class="step">
                                <span class="step-number">Step 1: Start with Binomial PMF (from Bernoulli sum)</span>
                                <div class="math-block">
                                    $$P(X_n = k) = \binom{n}{k}p_n^k(1-p_n)^{n-k}$$
                                </div>
                                <p>Substitute $p_n = \lambda/n$ (rare events: probability vanishes as $n$ grows):</p>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 2: Expand and rearrange</span>
                                <div class="math-block">
                                    $$P(X_n = k) = \frac{n!}{k!(n-k)!} \cdot \frac{\lambda^k}{n^k} \cdot \left(1 - \frac{\lambda}{n}\right)^{n-k}$$
                                </div>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 3: Take the limit $n \to \infty$</span>
                                <p>Each factor has a well-defined limit:</p>
                                <div class="math-block">
                                    $$\frac{n(n-1)\cdots(n-k+1)}{n^k} \to 1, \quad \left(1 - \frac{\lambda}{n}\right)^{n} \to e^{-\lambda}, \quad \left(1 - \frac{\lambda}{n}\right)^{-k} \to 1$$
                                </div>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 4: Combine to get Poisson PMF</span>
                                <div class="math-block">
                                    $$\lim_{n \to \infty} P(X_n = k) = \frac{\lambda^k}{k!} e^{-\lambda} = P(Y = k) \text{ where } Y \sim \text{Poisson}(\lambda)$$
                                </div>
                            </div>
                            
                            <h4>Alternative: MGF Limit Approach</h4>
                            <p>Start from the Binomial MGF (which comes from Bernoulli MGF):</p>
                            <div class="math-block">
                                $$M_{\text{Bin}(n, \lambda/n)}(t) = \left[(1-p) + pe^t\right]^n = \left[1 + \frac{\lambda(e^t - 1)}{n}\right]^n \xrightarrow{n \to \infty} e^{\lambda(e^t - 1)}$$
                            </div>
                            <p>This is exactly the Poisson$(\lambda)$ MGF, confirming convergence in distribution.</p>
                        </div>
                    </div>
                    
                    <div class="python-label">Python</div>
                    <div class="python-code">
                        <span class="comment"># === Sampling ===</span><br>
                        rng.poisson(lam, size=1000)<br><br>
                        <span class="comment"># === SciPy distribution object ===</span><br>
                        dist = scipy.stats.poisson(lam)<br>
                        dist.rvs(size=1000)  <span class="comment"># sampling</span><br>
                        dist.pmf(k)  <span class="comment"># P(X = k)</span><br>
                        dist.cdf(k)  <span class="comment"># P(X ‚â§ k)</span><br>
                        dist.sf(k)   <span class="comment"># P(X > k)</span><br>
                        dist.ppf(q)  <span class="comment"># quantile</span><br>
                        dist.mean(), dist.var()  <span class="comment"># both equal Œª</span>
                    </div>
                </div>
            </div>
            
            <div class="key-insight">
                <h4>üí° Key Insight: The Counting Hierarchy</h4>
                <p>Bernoulli counts success in 1 trial ‚Üí Binomial counts successes in $n$ trials ‚Üí <strong>Multinomial</strong> generalizes to $k$ categories ‚Üí Geometric counts trials to 1st success ‚Üí Negative Binomial counts trials to $r$-th success ‚Üí Poisson counts rare events (infinitely many trials with vanishing probability).</p>
            </div>
            
            <!-- Hypergeometric Connection -->
            <div class="connection-box">
                <h3><span class="connection-icon">üîó</span> The Hypergeometric Connection</h3>
                <p>Sampling without replacement modifies the Bernoulli/Binomial relationship.</p>
            </div>
            
            <div class="flow-level">
                <div class="dist-card discrete">
                    <h3>Hypergeometric <span class="dist-badge discrete">Discrete</span></h3>
                    <div class="dist-formula">
                        $$X \sim \text{Hypergeometric}(N, K, n)$$
                        $$P(X = k) = \frac{\binom{K}{k}\binom{N-K}{n-k}}{\binom{N}{n}}$$
                    </div>
                    <p class="dist-params"><strong>Construction:</strong> $n$ draws without replacement from $N$ items ($K$ successes)</p>
                    <p class="dist-params"><strong>Limit:</strong> As $N \to \infty$ with $K/N \to p$: Hypergeometric $\to$ Binomial</p>
                    
                    <div class="derivation-toggle">
                        <button class="derivation-btn" onclick="toggleDerivation(this)" aria-expanded="false" aria-controls="derivation-hypergeometric">
                            <span>Show Derivation from Bernoulli</span>
                            <span class="arrow">‚ñº</span>
                        </button>
                        <div class="derivation-content" id="derivation-hypergeometric" aria-hidden="true">
                            <div class="derivation-chain">
                                <span class="chain-label">Path:</span>
                                <span class="chain-item">$N$ iid Bernoulli$(p)$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">Two Binomials</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">Condition on total</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item"><strong>Hypergeometric</strong></span>
                            </div>
                            
                            <h4>The Key Insight: How Independent Draws Become Dependent</h4>
                            <p>The Hypergeometric arises when we <strong>condition</strong> independent Bernoulli trials on a constraint. This is how independent draws create dependent distributions‚Äîa fundamental idea in probability.</p>
                            
                            <div class="step">
                                <span class="step-number">Setup: Two Independent Binomials</span>
                                <p>Imagine $N$ independent Bernoulli$(p)$ trials, partitioned into two groups:</p>
                                <ul style="margin: 0.5rem 0 0.5rem 1.5rem;">
                                    <li>First $n$ trials: $X \sim \text{Binomial}(n, p)$ = successes in sample</li>
                                    <li>Remaining $N - n$ trials: $Y \sim \text{Binomial}(N-n, p)$ = successes outside sample</li>
                                </ul>
                                <p>Since trials are independent, $X$ and $Y$ are <em>independent</em> Binomials.</p>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 1: Total successes is also Binomial</span>
                                <p>The total number of successes in all $N$ trials:</p>
                                <div class="math-block">
                                    $$X + Y \sim \text{Binomial}(N, p)$$
                                </div>
                                <p>Now, suppose we <strong>observe</strong> that exactly $K$ total successes occurred: $X + Y = K$.</p>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 2: Condition on total successes</span>
                                <p>What is the distribution of $X$ (successes in sample) given $X + Y = K$?</p>
                                <div class="math-block">
                                    $$P(X = k \mid X + Y = K) = \frac{P(X = k, Y = K - k)}{P(X + Y = K)}$$
                                </div>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 3: Apply independence of X and Y</span>
                                <p>Since $X \perp Y$, the joint factors:</p>
                                <div class="math-block">
                                    $$= \frac{P(X = k) \cdot P(Y = K - k)}{P(X + Y = K)} = \frac{\binom{n}{k}p^k(1-p)^{n-k} \cdot \binom{N-n}{K-k}p^{K-k}(1-p)^{N-n-K+k}}{\binom{N}{K}p^K(1-p)^{N-K}}$$
                                </div>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 4: The p terms cancel!</span>
                                <p>The $p^k \cdot p^{K-k} = p^K$ terms cancel with the denominator, as do the $(1-p)$ terms:</p>
                                <div class="math-block">
                                    $$P(X = k \mid X + Y = K) = \frac{\binom{n}{k}\binom{N-n}{K-k}}{\binom{N}{K}}$$
                                </div>
                                <p>This is exactly the <strong>Hypergeometric$(N, K, n)$</strong> PMF!</p>
                            </div>
                            
                            <div class="key-insight" style="margin: 1.5rem 0;">
                                <h4>üí° The Deep Insight</h4>
                                <p>The parameter $p$ completely <em>disappears</em> from the conditional distribution. This reveals why Hypergeometric arises in sampling without replacement: conditioning on the total successes $K$ is equivalent to fixing the population composition, which then constrains our sample. <strong>Independent Bernoulli trials become dependent when we condition on their sum.</strong></p>
                            </div>
                            
                            <h4>Equivalence to Sampling Without Replacement</h4>
                            <p>Drawing $n$ items from a population of $N$ (where $K$ are "successes") <em>without replacement</em> is probabilistically identical to:</p>
                            <ol style="margin: 0.5rem 0 0.5rem 1.5rem;">
                                <li>Assigning each item an iid Bernoulli$(p)$ label</li>
                                <li>Conditioning on exactly $K$ items being labeled "success"</li>
                                <li>Counting how many of the first $n$ items are labeled "success"</li>
                            </ol>
                            
                            <h4>Convergence to Binomial (Large Population Limit)</h4>
                            <p>When $N \to \infty$ with $K/N \to p$ fixed, the conditioning becomes negligible‚Äîlearning the total $K$ barely constrains any finite sample:</p>
                            <div class="math-block">
                                $$\lim_{N \to \infty} \frac{\binom{n}{k}\binom{N-n}{K-k}}{\binom{N}{K}} = \binom{n}{k}p^k(1-p)^{n-k}$$
                            </div>
                            <p><strong>Rule of thumb:</strong> Use Binomial approximation when $n < 0.05N$ (sampling fraction under 5%).</p>
                        </div>
                    </div>
                    
                    <div class="python-label">Python</div>
                    <div class="python-code">
                        <span class="comment"># === Sampling ===</span><br>
                        <span class="comment"># NumPy: ngood=K, nbad=N-K, nsample=n</span><br>
                        rng.hypergeometric(K, N-K, n, size=1000)<br><br>
                        <span class="comment"># === SciPy distribution object ===</span><br>
                        <span class="comment"># hypergeom(M, n, N) = pop M, success n, draws N</span><br>
                        dist = scipy.stats.hypergeom(N, K, n)<br>
                        dist.rvs(size=1000)  <span class="comment"># sampling</span><br>
                        dist.pmf(k)  <span class="comment"># P(X = k)</span><br>
                        dist.cdf(k)  <span class="comment"># P(X ‚â§ k)</span><br>
                        dist.ppf(q)  <span class="comment"># quantile</span><br>
                        dist.mean(), dist.var()  <span class="comment"># nK/N, ...</span>
                    </div>
                </div>
            </div>
        </div>
        
        <!-- CONTINUOUS SECTION -->
        <div id="continuous-section" class="distribution-section">
            <div class="section-header continuous">
                <h2>Continuous Distributions from Discrete</h2>
                <p>Limits and transformations bridge the discrete-continuous divide</p>
            </div>
            
            <!-- Continuous Uniform Foundation -->
            <div class="connection-box">
                <h3><span class="connection-icon">üé≤</span> The Continuous Uniform Foundation</h3>
                <p>The continuous uniform on $[0, 1]$ is the fundamental building block of all continuous random generation‚Äîevery other distribution can be generated from it via transformations. The general Uniform$(a, b)$ extends this to any bounded interval.</p>
            </div>
            
            <div class="flow-level">
                <div class="dist-card continuous">
                    <h3>Continuous Uniform <span class="dist-badge continuous">Continuous</span></h3>
                    <div class="dist-formula">
                        $$U \sim \text{Uniform}(a, b)$$
                        $$f(x) = \frac{1}{b-a}, \quad F(x) = \frac{x - a}{b - a}, \quad x \in [a, b]$$
                    </div>
                    <p class="dist-params"><strong>Standard case:</strong> $U \sim \text{Uniform}(0,1)$ is the foundation of all random generation</p>
                    <p class="dist-params"><strong>Linear transformation:</strong> If $U \sim \text{Uniform}(0,1)$, then $X = a + (b-a)U \sim \text{Uniform}(a,b)$</p>
                    <p class="dist-params"><strong>Key method:</strong> Inverse CDF: $X = F^{-1}(U)$ has CDF $F$</p>
                    
                    <div class="derivation-toggle">
                        <button class="derivation-btn" onclick="toggleDerivation(this)" aria-expanded="false" aria-controls="derivation-uniform-continuous">
                            <span>Show Derivation from Bernoulli</span>
                            <span class="arrow">‚ñº</span>
                        </button>
                        <div class="derivation-content" id="derivation-uniform-continuous" aria-hidden="true">
                            <div class="derivation-chain">
                                <span class="chain-label">Path:</span>
                                <span class="chain-item">Bernoulli$(0.5)$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">DiscreteUniform$(0,1)$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">$n \to \infty$ points</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item"><strong>Uniform$(0,1)$</strong></span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">Linear transform</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item"><strong>Uniform$(a,b)$</strong></span>
                            </div>
                            
                            <h4>Connection from Discrete Uniform / Bernoulli</h4>
                            <p>A fair coin flip is Bernoulli$(0.5)$ = DiscreteUniform$(0,1)$. As we increase the number of equally-likely outcomes and rescale, the continuous uniform emerges.</p>
                            
                            <div class="step">
                                <span class="step-number">Limiting construction</span>
                                <p>Consider $X_n \sim \text{DiscreteUniform}\{0, 1, 2, \ldots, n-1\}$. Then $U_n = X_n / n$ takes values in $\{0, 1/n, 2/n, \ldots, (n-1)/n\}$.</p>
                                <div class="math-block">
                                    $$U_n \xrightarrow{d} U \sim \text{Uniform}(0, 1) \quad \text{as } n \to \infty$$
                                </div>
                            </div>
                            
                            <h4>General Uniform$(a, b)$ via Linear Transform</h4>
                            <div class="step">
                                <span class="step-number">From standard to general</span>
                                <p>If $U \sim \text{Uniform}(0,1)$, then $X = a + (b-a)U$ satisfies:</p>
                                <div class="math-block">
                                    $$P(X \leq x) = P\left(U \leq \frac{x-a}{b-a}\right) = \frac{x-a}{b-a}, \quad x \in [a, b]$$
                                </div>
                                <p>So $X \sim \text{Uniform}(a, b)$. Any bounded uniform arises this way.</p>
                            </div>
                            
                            <h4>Moments</h4>
                            <div class="step">
                                <span class="step-number">Mean and Variance</span>
                                <div class="math-block">
                                    $$\mathbb{E}[X] = \frac{a + b}{2}, \qquad \text{Var}(X) = \frac{(b - a)^2}{12}$$
                                </div>
                                <p>For standard $U(0,1)$: Mean $= 1/2$, Variance $= 1/12$.</p>
                            </div>
                            
                            <h4>The Probability Integral Transform</h4>
                            <p>This fundamental theorem connects <em>any</em> continuous distribution back to Uniform$(0,1)$:</p>
                            <div class="math-block">
                                $$\text{If } X \text{ has continuous CDF } F, \text{ then } F(X) \sim \text{Uniform}(0, 1)$$
                            </div>
                            
                            <h4>The Inverse CDF Method (Universality)</h4>
                            <p>Conversely, we can generate <em>any</em> distribution from Uniform$(0,1)$:</p>
                            <div class="math-block">
                                $$\text{If } U \sim \text{Uniform}(0, 1), \text{ then } X = F^{-1}(U) \text{ has CDF } F$$
                            </div>
                            <p>This is the <strong>fundamental theorem of random variate generation</strong>‚Äîthe bridge from one uniform to all distributions.</p>
                        </div>
                    </div>
                    
                    <div class="python-label">Python</div>
                    <div class="python-code">
                        <span class="comment"># === Sampling ===</span><br>
                        rng.random(size=1000)  <span class="comment"># U(0,1)</span><br>
                        rng.uniform(a, b, size=1000)  <span class="comment"># U(a,b)</span><br><br>
                        <span class="comment"># === SciPy (loc=a, scale=b-a) ===</span><br>
                        dist = scipy.stats.uniform(a, b-a)<br>
                        dist.rvs(size=1000)  <span class="comment"># sampling</span><br>
                        dist.pdf(x)  <span class="comment"># density = 1/(b-a)</span><br>
                        dist.cdf(x)  <span class="comment"># (x-a)/(b-a)</span><br>
                        dist.ppf(q)  <span class="comment"># a + q(b-a)</span><br><br>
                        <span class="comment"># === Python stdlib ===</span><br>
                        random.random()  <span class="comment"># U(0,1)</span><br>
                        random.uniform(a, b)  <span class="comment"># U(a,b)</span>
                    </div>
                </div>
            </div>
            
            <div class="flow-arrow">‚¨á</div>
            
            <!-- Bridge: Discrete to Continuous -->
            <div class="transition-card">
                <h3>üåâ The Central Limit Theorem Bridge</h3>
                <p style="text-align: center; margin-bottom: 1.5rem;">The CLT transforms discrete counts into continuous distributions.</p>
                
                <div class="transition-grid">
                    <div class="dist-card discrete" style="width: 100%;">
                        <h3>Binomial$(n, p)$</h3>
                        <div class="dist-formula">
                            $$S_n = \sum_{i=1}^n X_i, \quad X_i \stackrel{\text{iid}}{\sim} \text{Bernoulli}(p)$$
                        </div>
                        <p class="dist-params">Mean: $np$, Variance: $np(1-p)$</p>
                    </div>
                    
                    <div class="transition-arrow">
                        ‚ûú
                        <span>$n \to \infty$<br>CLT</span>
                    </div>
                    
                    <div class="dist-card continuous" style="width: 100%;">
                        <h3>Normal$(0, 1)$</h3>
                        <div class="dist-formula">
                            $$Z_n = \frac{S_n - np}{\sqrt{np(1-p)}} \xrightarrow{d} N(0,1)$$
                        </div>
                        <p class="dist-params">The standardized sum converges in distribution</p>
                    </div>
                </div>
                
                <div class="derivation-toggle" style="margin-top: 1.5rem;">
                    <button class="derivation-btn" onclick="toggleDerivation(this)" aria-expanded="false" aria-controls="derivation-clt-proof">
                        <span>Show CLT Proof Sketch via MGF</span>
                        <span class="arrow">‚ñº</span>
                    </button>
                    <div class="derivation-content" id="derivation-clt-proof" aria-hidden="true">
                        <h4>CLT for Bernoulli via MGF</h4>
                        
                        <div class="step">
                            <span class="step-number">Step 1: Standardize</span>
                            <p>Let $Z_n = \frac{S_n - np}{\sqrt{np(1-p)}}$ where $S_n \sim \text{Binomial}(n,p)$.</p>
                        </div>
                        
                        <div class="step">
                            <span class="step-number">Step 2: MGF of standardized sum</span>
                            <div class="math-block">
                                $$M_{Z_n}(t) = e^{-t\sqrt{np/(1-p)}} \left[(1-p) + pe^{t/\sqrt{np(1-p)}}\right]^n$$
                            </div>
                        </div>
                        
                        <div class="step">
                            <span class="step-number">Step 3: Taylor expansion</span>
                            <p>Let $s = t/\sqrt{np(1-p)}$. Then $e^s \approx 1 + s + s^2/2 + O(s^3)$.</p>
                            <div class="math-block">
                                $$(1-p) + pe^s \approx 1 + ps + ps^2/2$$
                            </div>
                        </div>
                        
                        <div class="step">
                            <span class="step-number">Step 4: Take logarithm and limit</span>
                            <div class="math-block">
                                $$\ln M_{Z_n}(t) = -t\sqrt{\frac{np}{1-p}} + n\ln\left[1 + ps + \frac{ps^2}{2}\right]$$
                            </div>
                            <p>Using $\ln(1+x) \approx x - x^2/2$, this simplifies to $t^2/2$ as $n \to \infty$.</p>
                        </div>
                        
                        <div class="step">
                            <span class="step-number">Step 5: Identify limit</span>
                            <div class="math-block">
                                $$M_{Z_n}(t) \to e^{t^2/2} = M_{N(0,1)}(t)$$
                            </div>
                        </div>
                        
                        <p>By continuity theorem: $Z_n \xrightarrow{d} N(0,1)$.</p>
                    </div>
                </div>
            </div>
            
            <!-- Normal Distribution -->
            <div class="connection-box">
                <h3><span class="connection-icon">üîó</span> The Normal Distribution: Universal Attractor</h3>
                <p>The normal distribution is the limit of sums‚Äîthe hub of continuous probability.</p>
            </div>
            
            <div class="flow-level">
                <div class="dist-card continuous">
                    <h3>Normal <span class="dist-badge continuous">Continuous</span></h3>
                    <div class="dist-formula">
                        $$X \sim N(\mu, \sigma^2)$$
                        $$f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$$
                    </div>
                    <p class="dist-params"><strong>From Bernoulli:</strong> CLT limit of standardized binomial sums</p>
                    <p class="dist-params"><strong>MGF:</strong> $M(t) = \exp(\mu t + \sigma^2 t^2/2)$</p>
                    
                    <div class="derivation-toggle">
                        <button class="derivation-btn" onclick="toggleDerivation(this)" aria-expanded="false" aria-controls="derivation-normal">
                            <span>Show Derivation from Bernoulli</span>
                            <span class="arrow">‚ñº</span>
                        </button>
                        <div class="derivation-content" id="derivation-normal" aria-hidden="true">
                            <div class="derivation-chain">
                                <span class="chain-label">Path:</span>
                                <span class="chain-item">Bernoulli$(p)$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">Binomial$(n,p)$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">Standardize, $n \to \infty$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item"><strong>Normal$(0,1)$</strong></span>
                            </div>
                            
                            <h4>Primary Derivation: CLT from Bernoulli</h4>
                            <p>The Normal emerges as the limiting distribution of standardized Bernoulli sums.</p>
                            
                            <div class="step">
                                <span class="step-number">Step 1: Start with Bernoulli sum</span>
                                <p>Let $S_n = \sum_{i=1}^n X_i$ where $X_i \stackrel{\text{iid}}{\sim} \text{Bernoulli}(p)$, so $S_n \sim \text{Binomial}(n, p)$.</p>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 2: Standardize</span>
                                <p>Subtract mean, divide by standard deviation:</p>
                                <div class="math-block">
                                    $$Z_n = \frac{S_n - np}{\sqrt{np(1-p)}} = \frac{S_n - \mathbb{E}[S_n]}{\sqrt{\text{Var}(S_n)}}$$
                                </div>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 3: Apply Central Limit Theorem</span>
                                <div class="math-block">
                                    $$Z_n \xrightarrow{d} N(0,1) \quad \text{as } n \to \infty$$
                                </div>
                            </div>
                            
                            <h4>Alternative Paths to Normal (all trace back to CLT)</h4>
                            
                            <div class="step">
                                <span class="step-number">From Poisson (via Binomial limit)</span>
                                <div class="math-block">
                                    $$\frac{\text{Poisson}(\lambda) - \lambda}{\sqrt{\lambda}} \xrightarrow{d} N(0,1) \text{ as } \lambda \to \infty$$
                                </div>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">General CLT statement</span>
                                <p>For <em>any</em> iid $X_i$ with mean $\mu$, variance $\sigma^2 < \infty$:</p>
                                <div class="math-block">
                                    $$\frac{\bar{X}_n - \mu}{\sigma/\sqrt{n}} \xrightarrow{d} N(0,1)$$
                                </div>
                                <p>The Normal is the <strong>universal attractor</strong> for standardized sums.</p>
                            </div>
                        </div>
                    </div>
                    
                    <div class="python-label">Python</div>
                    <div class="python-code">
                        <span class="comment"># === Sampling ===</span><br>
                        rng.normal(mu, sigma, size=1000)<br>
                        rng.standard_normal(size=1000)  <span class="comment"># N(0,1)</span><br><br>
                        <span class="comment"># === SciPy distribution object ===</span><br>
                        dist = scipy.stats.norm(mu, sigma)<br>
                        dist.rvs(size=1000)  <span class="comment"># sampling</span><br>
                        dist.pdf(x)  <span class="comment"># density œÜ(x)</span><br>
                        dist.cdf(x)  <span class="comment"># Œ¶(x)</span><br>
                        dist.sf(x)   <span class="comment"># 1 - Œ¶(x)</span><br>
                        dist.ppf(q)  <span class="comment"># Œ¶‚Åª¬π(q) quantile</span><br>
                        dist.isf(q)  <span class="comment"># Œ¶‚Åª¬π(1-q)</span><br><br>
                        <span class="comment"># === Python stdlib ===</span><br>
                        random.gauss(mu, sigma)<br>
                        random.normalvariate(mu, sigma)
                    </div>
                </div>
            </div>
            
            <div class="flow-arrow">‚¨á</div>
            
            <!-- From Normal -->
            <div class="connection-box">
                <h3><span class="connection-icon">üîó</span> Distributions Derived from Normal</h3>
                <p>Squares, ratios, and sums of normals generate the inferential distributions.</p>
            </div>
            
            <div class="flow-level">
                <!-- Chi-squared -->
                <div class="dist-card continuous">
                    <h3>Chi-Squared <span class="dist-badge continuous">Continuous</span></h3>
                    <div class="dist-formula">
                        $$X \sim \chi^2_k$$
                        $$f(x) = \frac{x^{k/2-1}e^{-x/2}}{2^{k/2}\Gamma(k/2)}, \quad x > 0$$
                    </div>
                    <p class="dist-params"><strong>Construction:</strong> Sum of $k$ squared standard normals</p>
                    <p class="dist-params"><strong>From Bernoulli:</strong> Binomial ‚Üí Normal ‚Üí Chi-squared</p>
                    
                    <div class="derivation-toggle">
                        <button class="derivation-btn" onclick="toggleDerivation(this)" aria-expanded="false" aria-controls="derivation-chisquared">
                            <span>Show Derivation from Bernoulli</span>
                            <span class="arrow">‚ñº</span>
                        </button>
                        <div class="derivation-content" id="derivation-chisquared" aria-hidden="true">
                            <div class="derivation-chain">
                                <span class="chain-label">Path:</span>
                                <span class="chain-item">Bernoulli$(p)$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">Binomial$(n,p)$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">Normal$(0,1)$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">Square</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item"><strong>$\chi^2_k$</strong></span>
                            </div>
                            
                            <h4>Derivation: Sum of Squared Normals (from CLT)</h4>
                            <p>Let $Z_1, \ldots, Z_k \stackrel{\text{iid}}{\sim} N(0,1)$ (each a CLT limit of Bernoulli sums). Define $Q = \sum_{i=1}^k Z_i^2$.</p>
                            
                            <div class="step">
                                <span class="step-number">Step 1: MGF of a squared standard normal</span>
                                <p>For $Z \sim N(0,1)$, compute the MGF of $Z^2$:</p>
                                <div class="math-block">
                                    $$M_{Z^2}(t) = \mathbb{E}[e^{tZ^2}] = \int_{-\infty}^{\infty} e^{tz^2} \frac{1}{\sqrt{2\pi}} e^{-z^2/2} dz = \frac{1}{\sqrt{1-2t}}, \quad t < \frac{1}{2}$$
                                </div>
                                <p>This is the MGF of Gamma$(1/2, 1/2)$, which is $\chi^2_1$.</p>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 2: MGF of sum of $k$ independent squared normals</span>
                                <div class="math-block">
                                    $$M_Q(t) = \prod_{i=1}^k M_{Z_i^2}(t) = \left[\frac{1}{\sqrt{1-2t}}\right]^k = (1-2t)^{-k/2}$$
                                </div>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 3: Identify the distribution</span>
                                <p>This is the MGF of Gamma$(k/2, 1/2) = \chi^2_k$. Thus:</p>
                                <div class="math-block">
                                    $$Q = \sum_{i=1}^k Z_i^2 \sim \chi^2_k$$
                                </div>
                            </div>
                            
                            <h4>Additivity Property</h4>
                            <p>If $X \sim \chi^2_m$ and $Y \sim \chi^2_n$ are independent, then $X + Y \sim \chi^2_{m+n}$.</p>
                        </div>
                    </div>
                    
                    <div class="python-label">Python</div>
                    <div class="python-code">
                        <span class="comment"># === Sampling ===</span><br>
                        rng.chisquare(df, size=1000)<br><br>
                        <span class="comment"># === SciPy distribution object ===</span><br>
                        dist = scipy.stats.chi2(df)<br>
                        dist.rvs(size=1000)  <span class="comment"># sampling</span><br>
                        dist.pdf(x)  <span class="comment"># density</span><br>
                        dist.cdf(x)  <span class="comment"># P(X ‚â§ x)</span><br>
                        dist.ppf(q)  <span class="comment"># quantile</span><br>
                        dist.mean(), dist.var()  <span class="comment"># df, 2¬∑df</span>
                    </div>
                </div>
                
                <!-- Student's t -->
                <div class="dist-card continuous">
                    <h3>Student's t <span class="dist-badge continuous">Continuous</span></h3>
                    <div class="dist-formula">
                        $$T \sim t_\nu$$
                        $$f(t) = \frac{\Gamma((\nu+1)/2)}{\sqrt{\nu\pi}\,\Gamma(\nu/2)}\left(1 + \frac{t^2}{\nu}\right)^{-(\nu+1)/2}$$
                    </div>
                    <p class="dist-params"><strong>Construction:</strong> $T = Z / \sqrt{V/\nu}$ where $Z \sim N(0,1)$, $V \sim \chi^2_\nu$, independent</p>
                    
                    <div class="derivation-toggle">
                        <button class="derivation-btn" onclick="toggleDerivation(this)" aria-expanded="false" aria-controls="derivation-t">
                            <span>Show Derivation from Bernoulli</span>
                            <span class="arrow">‚ñº</span>
                        </button>
                        <div class="derivation-content" id="derivation-t" aria-hidden="true">
                            <div class="derivation-chain">
                                <span class="chain-label">Path:</span>
                                <span class="chain-item">Bernoulli$(p)$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">Binomial</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">$N(0,1)$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">$\chi^2_\nu$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">Ratio</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item"><strong>$t_\nu$</strong></span>
                            </div>
                            
                            <h4>Ratio Construction (from Normal and Chi-squared)</h4>
                            <p>Let $Z \sim N(0,1)$ (CLT limit of Bernoulli sums) and $V \sim \chi^2_\nu$ (sum of squared normals), independent.</p>
                            
                            <div class="step">
                                <span class="step-number">Step 1: Define the ratio</span>
                                <div class="math-block">
                                    $$T = \frac{Z}{\sqrt{V/\nu}}$$
                                </div>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 2: Write joint density</span>
                                <p>Since $Z$ and $V$ are independent:</p>
                                <div class="math-block">
                                    $$f_{Z,V}(z,v) = \frac{1}{\sqrt{2\pi}}e^{-z^2/2} \cdot \frac{v^{\nu/2-1}e^{-v/2}}{2^{\nu/2}\Gamma(\nu/2)}$$
                                </div>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 3: Transform and integrate out $V$</span>
                                <p>Substitute $T = Z/\sqrt{V/\nu}$, $W = V$, with Jacobian $|J| = \sqrt{w/\nu}$. After integrating out $W$:</p>
                                <div class="math-block">
                                    $$f_T(t) = \frac{\Gamma((\nu+1)/2)}{\sqrt{\nu\pi}\,\Gamma(\nu/2)}\left(1 + \frac{t^2}{\nu}\right)^{-(\nu+1)/2}$$
                                </div>
                            </div>
                            
                            <h4>Key Properties</h4>
                            <p><strong>Convergence to Normal:</strong> As $\nu \to \infty$, $t_\nu \xrightarrow{d} N(0,1)$ (heavier tails disappear).</p>
                            <p><strong>Application:</strong> Inference with estimated variance (sample mean divided by sample SE).</p>
                        </div>
                    </div>
                    
                    <div class="python-label">Python</div>
                    <div class="python-code">
                        <span class="comment"># === Sampling ===</span><br>
                        rng.standard_t(df, size=1000)  <span class="comment"># t(df)</span><br><br>
                        <span class="comment"># === SciPy distribution object ===</span><br>
                        dist = scipy.stats.t(df, loc=mu, scale=sigma)<br>
                        dist.rvs(size=1000)  <span class="comment"># sampling</span><br>
                        dist.pdf(x)  <span class="comment"># density</span><br>
                        dist.cdf(x)  <span class="comment"># P(T ‚â§ x)</span><br>
                        dist.ppf(q)  <span class="comment"># quantile (t-critical)</span><br>
                        dist.interval(0.95)  <span class="comment"># 95% interval</span>
                    </div>
                </div>
                
                <!-- F distribution -->
                <div class="dist-card continuous">
                    <h3>F Distribution <span class="dist-badge continuous">Continuous</span></h3>
                    <div class="dist-formula">
                        $$X \sim F_{d_1, d_2}$$
                        $$F = \frac{U/d_1}{V/d_2}$$
                    </div>
                    <p class="dist-params"><strong>Construction:</strong> Ratio of two independent chi-squareds (scaled)</p>
                    <p class="dist-params"><strong>Connection:</strong> $T^2 \sim F_{1, \nu}$ where $T \sim t_\nu$</p>
                    
                    <div class="derivation-toggle">
                        <button class="derivation-btn" onclick="toggleDerivation(this)" aria-expanded="false" aria-controls="derivation-f">
                            <span>Show Derivation from Bernoulli</span>
                            <span class="arrow">‚ñº</span>
                        </button>
                        <div class="derivation-content" id="derivation-f" aria-hidden="true">
                            <div class="derivation-chain">
                                <span class="chain-label">Path:</span>
                                <span class="chain-item">Bernoulli$(p)$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">Binomial</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">$N(0,1)$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">$\chi^2_{d_1}, \chi^2_{d_2}$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">Ratio</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item"><strong>$F_{d_1,d_2}$</strong></span>
                            </div>
                            
                            <h4>Definition: Ratio of Chi-squareds</h4>
                            <p>Let $U \sim \chi^2_{d_1}$, $V \sim \chi^2_{d_2}$ (both from squared normals, which are CLT limits of Bernoulli sums), independent. Then:</p>
                            <div class="math-block">
                                $$F = \frac{U/d_1}{V/d_2} \sim F_{d_1, d_2}$$
                            </div>
                            
                            <div class="step">
                                <span class="step-number">PDF</span>
                                <div class="math-block">
                                    $$f(x) = \frac{\sqrt{\frac{(d_1 x)^{d_1} d_2^{d_2}}{(d_1 x + d_2)^{d_1+d_2}}}}{x\, B(d_1/2, d_2/2)}, \quad x > 0$$
                                </div>
                            </div>
                            
                            <h4>Key Properties</h4>
                            <div class="step">
                                <span class="step-number">Reciprocal symmetry</span>
                                <div class="math-block">
                                    $$\text{If } X \sim F_{d_1,d_2}, \text{ then } 1/X \sim F_{d_2, d_1}$$
                                </div>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Connection to $t$-distribution</span>
                                <div class="math-block">
                                    $$T^2 \sim F_{1, \nu} \quad \text{where } T \sim t_\nu$$
                                </div>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Limit to Chi-squared</span>
                                <p>As $d_2 \to \infty$: $d_1 \cdot F_{d_1, d_2} \xrightarrow{d} \chi^2_{d_1}$</p>
                            </div>
                            
                            <p><strong>Application:</strong> ANOVA F-tests compare ratio of between-group to within-group variance.</p>
                        </div>
                    </div>
                    
                    <div class="python-label">Python</div>
                    <div class="python-code">
                        <span class="comment"># === Sampling ===</span><br>
                        rng.f(dfnum, dfden, size=1000)<br><br>
                        <span class="comment"># === SciPy distribution object ===</span><br>
                        dist = scipy.stats.f(dfnum, dfden)<br>
                        dist.rvs(size=1000)  <span class="comment"># sampling</span><br>
                        dist.pdf(x)  <span class="comment"># density</span><br>
                        dist.cdf(x)  <span class="comment"># P(F ‚â§ x)</span><br>
                        dist.ppf(q)  <span class="comment"># F-critical value</span><br>
                        dist.sf(x)   <span class="comment"># p-value = 1 - CDF</span>
                    </div>
                </div>
                
                <!-- Cauchy -->
                <div class="dist-card continuous">
                    <h3>Cauchy <span class="dist-badge continuous">Continuous</span></h3>
                    <div class="dist-formula">
                        $$X \sim \text{Cauchy}(x_0, \gamma)$$
                        $$f(x) = \frac{1}{\pi\gamma\left[1 + \left(\frac{x - x_0}{\gamma}\right)^2\right]}$$
                    </div>
                    <p class="dist-params"><strong>Construction:</strong> $t_1 = $ Student's $t$ with $\nu = 1$ degree of freedom</p>
                    <p class="dist-params"><strong>Key property:</strong> No mean or variance (moments do not exist!)</p>
                    
                    <div class="derivation-toggle">
                        <button class="derivation-btn" onclick="toggleDerivation(this)" aria-expanded="false" aria-controls="derivation-cauchy">
                            <span>Show Derivation from Bernoulli</span>
                            <span class="arrow">‚ñº</span>
                        </button>
                        <div class="derivation-content" id="derivation-cauchy" aria-hidden="true">
                            <div class="derivation-chain">
                                <span class="chain-label">Path:</span>
                                <span class="chain-item">Bernoulli$(p)$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">Binomial</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">$N(0,1)$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">$\chi^2_1$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">$t_1$ ratio</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item"><strong>Cauchy</strong></span>
                            </div>
                            
                            <h4>Cauchy as $t_1$: The Extreme Case</h4>
                            <p>The $t$-distribution with $\nu = 1$ degree of freedom is the Cauchy distribution. Recall that $T = Z/\sqrt{V/\nu}$ where $Z \sim N(0,1)$, $V \sim \chi^2_\nu$.</p>
                            
                            <div class="step">
                                <span class="step-number">Step 1: Set $\nu = 1$</span>
                                <p>With $\nu = 1$: $T = Z/\sqrt{V}$ where $V \sim \chi^2_1 = Z'^2$ for $Z' \sim N(0,1)$.</p>
                                <div class="math-block">
                                    $$T = \frac{Z}{|Z'|} \quad \text{(ratio of two independent standard normals)}$$
                                </div>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 2: Substitute into $t_\nu$ PDF</span>
                                <p>The $t_\nu$ PDF with $\nu = 1$:</p>
                                <div class="math-block">
                                    $$f(t) = \frac{\Gamma(1)}{\sqrt{\pi}\,\Gamma(1/2)}\left(1 + t^2\right)^{-1} = \frac{1}{\pi(1 + t^2)}$$
                                </div>
                                <p>This is exactly the standard Cauchy (location $x_0 = 0$, scale $\gamma = 1$).</p>
                            </div>
                            
                            <h4>Why Moments Don't Exist</h4>
                            <p>The tails are so heavy that $\int |x| f(x)\,dx = \infty$. Even the <em>mean</em> is undefined! The integral:</p>
                            <div class="math-block">
                                $$\int_{-\infty}^{\infty} \frac{x}{\pi(1 + x^2)}\,dx \quad \text{diverges}$$
                            </div>
                            <p>This makes Cauchy a crucial counterexample: CLT does not apply, sample means don't converge to a constant.</p>
                            
                            <h4>Alternative Construction: Ratio of Normals</h4>
                            <div class="step">
                                <span class="step-number">Direct ratio</span>
                                <p>If $Z_1, Z_2 \stackrel{\text{iid}}{\sim} N(0,1)$, then $Z_1/Z_2 \sim \text{Cauchy}(0, 1)$.</p>
                            </div>
                            
                            <h4>Stability Property</h4>
                            <p>Cauchy is a <em>stable distribution</em>: if $X_1, X_2 \stackrel{\text{iid}}{\sim} \text{Cauchy}$, then $\frac{X_1 + X_2}{2} \sim \text{Cauchy}$ (not a narrower distribution!). The sample mean has the <em>same</em> distribution as a single observation‚Äîaveraging doesn't help.</p>
                        </div>
                    </div>
                    
                    <div class="python-label">Python</div>
                    <div class="python-code">
                        <span class="comment"># === Sampling ===</span><br>
                        rng.standard_cauchy(size=1000)  <span class="comment"># Cauchy(0,1)</span><br><br>
                        <span class="comment"># === SciPy (loc=x‚ÇÄ, scale=Œ≥) ===</span><br>
                        dist = scipy.stats.cauchy(loc=x0, scale=gamma)<br>
                        dist.rvs(size=1000)  <span class="comment"># sampling</span><br>
                        dist.pdf(x)  <span class="comment"># density</span><br>
                        dist.cdf(x)  <span class="comment"># P(X ‚â§ x)</span><br>
                        dist.ppf(q)  <span class="comment"># quantile</span><br>
                        dist.median()  <span class="comment"># = x‚ÇÄ (mean undefined!)</span><br><br>
                        <span class="comment"># === Via t-distribution (ŒΩ=1) ===</span><br>
                        scipy.stats.t(df=1)  <span class="comment"># equivalent!</span>
                    </div>
                </div>
            </div>
            
            <div class="flow-arrow">‚¨á</div>
            
            <!-- Exponential Family -->
            <div class="connection-box">
                <h3><span class="connection-icon">üîó</span> The Waiting Time Path: Geometric ‚Üí Exponential ‚Üí Gamma</h3>
                <p>Continuous waiting times emerge as limits of discrete counts.</p>
            </div>
            
            <div class="transition-card">
                <h3>üåâ Geometric to Exponential Limit</h3>
                
                <div class="transition-grid">
                    <div class="dist-card discrete" style="width: 100%;">
                        <h3>Geometric$(p)$</h3>
                        <div class="dist-formula">
                            $$P(X > k) = (1-p)^k$$
                        </div>
                        <p class="dist-params">Waiting time: discrete trials</p>
                    </div>
                    
                    <div class="transition-arrow">
                        ‚ûú
                        <span>$p \to 0$<br>$np \to \lambda$</span>
                    </div>
                    
                    <div class="dist-card continuous" style="width: 100%;">
                        <h3>Exponential$(\lambda)$</h3>
                        <div class="dist-formula">
                            $$P(T > t) = e^{-\lambda t}$$
                        </div>
                        <p class="dist-params">Waiting time: continuous</p>
                    </div>
                </div>
                
                <div class="derivation-toggle" style="margin-top: 1.5rem;">
                    <button class="derivation-btn" onclick="toggleDerivation(this)" aria-expanded="false" aria-controls="derivation-geom-exp-limit">
                        <span>Show Limit Derivation</span>
                        <span class="arrow">‚ñº</span>
                    </button>
                    <div class="derivation-content" id="derivation-geom-exp-limit" aria-hidden="true">
                        <h4>Discretization Argument</h4>
                        <p>Partition time $[0, t]$ into $n$ intervals of length $\Delta = t/n$. In each interval, success occurs with probability $p = \lambda \Delta$.</p>
                        
                        <div class="step">
                            <span class="step-number">Step 1: No success in $[0,t]$</span>
                            <div class="math-block">
                                $$P(\text{no success in } n \text{ intervals}) = (1-p)^n = \left(1 - \frac{\lambda t}{n}\right)^n$$
                            </div>
                        </div>
                        
                        <div class="step">
                            <span class="step-number">Step 2: Take limit</span>
                            <div class="math-block">
                                $$\lim_{n \to \infty} \left(1 - \frac{\lambda t}{n}\right)^n = e^{-\lambda t}$$
                            </div>
                        </div>
                        
                        <div class="step">
                            <span class="step-number">Step 3: Identify</span>
                            <p>$P(T > t) = e^{-\lambda t}$ is the survival function of Exponential$(\lambda)$.</p>
                        </div>
                        
                        <h4>Memoryless Property Preserved</h4>
                        <p>Both Geometric and Exponential are memoryless‚Äîthey are the <em>only</em> discrete and continuous distributions with this property.</p>
                    </div>
                </div>
            </div>
            
            <div class="flow-level">
                <!-- Gamma (includes Exponential as special case) -->
                <div class="dist-card continuous">
                    <h3>Gamma <span class="dist-badge continuous">Continuous</span></h3>
                    <div class="dist-formula">
                        $$X \sim \text{Gamma}(\alpha, \beta)$$
                        $$f(x) = \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}, \quad x > 0$$
                    </div>
                    <p class="dist-params"><strong>Construction:</strong> Sum of $\alpha$ iid Exponential$(\beta)$ when $\alpha \in \mathbb{N}$</p>
                    <p class="dist-params"><strong>Special cases:</strong> Exp$(\lambda) = $ Gamma$(1, \lambda)$; $\chi^2_k = $ Gamma$(k/2, 1/2)$</p>
                    
                    <div class="derivation-toggle">
                        <button class="derivation-btn" onclick="toggleDerivation(this)" aria-expanded="false" aria-controls="derivation-gamma">
                            <span>Show Derivation from Bernoulli</span>
                            <span class="arrow">‚ñº</span>
                        </button>
                        <div class="derivation-content" id="derivation-gamma" aria-hidden="true">
                            <div class="derivation-chain">
                                <span class="chain-label">Path:</span>
                                <span class="chain-item">Bernoulli$(p)$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">Geometric$(p)$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">$p \to 0$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">Exponential$(\lambda)$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">Sum $\alpha$ copies</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item"><strong>Gamma$(\alpha,\lambda)$</strong></span>
                            </div>
                            
                            <h4>Exponential: The $\alpha = 1$ Case</h4>
                            <p>Gamma$(1, \lambda) = $ Exponential$(\lambda)$ with PDF $f(x) = \lambda e^{-\lambda x}$ for $x \geq 0$.</p>
                            <div class="math-block">
                                $$\text{Mean} = 1/\lambda, \quad \text{Var} = 1/\lambda^2, \quad \text{MGF} = \frac{\lambda}{\lambda - t}$$
                            </div>
                            <p>This is the continuous analog of Geometric‚Äîthe waiting time for a Poisson event. Both are memoryless: $P(X > s+t \mid X > s) = P(X > t)$.</p>
                            
                            <h4>Derivation: Sum of Exponentials (from Geometric limit)</h4>
                            <p>Let $X_1, \ldots, X_n \stackrel{\text{iid}}{\sim} \text{Exp}(\lambda)$ (each a limit of Geometric as $p \to 0$) and define $S_n = \sum_{i=1}^n X_i$.</p>
                            
                            <div class="step">
                                <span class="step-number">Step 1: Exponential MGF</span>
                                <div class="math-block">
                                    $$M_{X_i}(t) = \frac{\lambda}{\lambda - t}, \quad t < \lambda$$
                                </div>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 2: Product of MGFs for sum</span>
                                <div class="math-block">
                                    $$M_{S_n}(t) = \prod_{i=1}^n M_{X_i}(t) = \left(\frac{\lambda}{\lambda - t}\right)^n$$
                                </div>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 3: Identify as Gamma</span>
                                <p>This is exactly the MGF of Gamma$(n, \lambda)$:</p>
                                <div class="math-block">
                                    $$S_n = \sum_{i=1}^n X_i \sim \text{Gamma}(n, \lambda)$$
                                </div>
                            </div>
                            
                            <h4>Chi-squared: The $\beta = 1/2$ Case</h4>
                            <p>Gamma$(k/2, 1/2) = \chi^2_k$ (sum of squared normals, which are CLT limits of Bernoulli sums).</p>
                        </div>
                    </div>
                    
                    <div class="python-label">Python</div>
                    <div class="python-code">
                        <span class="comment"># === Exponential (Gamma with Œ±=1) ===</span><br>
                        rng.exponential(scale=1/lam, size=1000)<br>
                        scipy.stats.expon(scale=1/lam)  <span class="comment"># full dist object</span><br>
                        random.expovariate(lam)  <span class="comment"># stdlib</span><br><br>
                        <span class="comment"># === General Gamma (shape=Œ±, scale=1/Œ≤) ===</span><br>
                        rng.gamma(shape=alpha, scale=1/beta, size=1000)<br><br>
                        <span class="comment"># === SciPy (a=shape, scale=1/rate) ===</span><br>
                        dist = scipy.stats.gamma(alpha, scale=1/beta)<br>
                        dist.rvs(size=1000)  <span class="comment"># sampling</span><br>
                        dist.pdf(x)  <span class="comment"># density</span><br>
                        dist.cdf(x)  <span class="comment"># P(X ‚â§ x)</span><br>
                        dist.ppf(q)  <span class="comment"># quantile</span><br>
                        dist.mean(), dist.var()  <span class="comment"># Œ±/Œ≤, Œ±/Œ≤¬≤</span><br><br>
                        <span class="comment"># === Python stdlib ===</span><br>
                        random.gammavariate(alpha, 1/beta)
                    </div>
                </div>
            </div>
            
            <div class="flow-arrow">‚¨á</div>
            
            <!-- Beta Distribution -->
            <div class="connection-box">
                <h3><span class="connection-icon">üîó</span> The Beta Distribution: Continuous Analog of Bernoulli</h3>
                <p>The Bernoulli distribution models a binary outcome $\{0, 1\}$ with probability $p$. The Beta distribution is its continuous analog: it lives on $[0, 1]$ and models <strong>the probability parameter $p$ itself</strong>. Just as Bernoulli is the atom of discrete probability, Beta is the atom of continuous probability on bounded intervals‚Äîand they are intimately connected through Bayesian conjugacy.</p>
            </div>
            
            <div class="flow-level">
                <div class="dist-card continuous">
                    <h3>Beta <span class="dist-badge continuous">Continuous</span></h3>
                    <div class="dist-formula">
                        $$X \sim \text{Beta}(\alpha, \beta)$$
                        $$f(x) = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha, \beta)}, \quad x \in [0,1]$$
                    </div>
                    <p class="dist-params"><strong>Bernoulli parallel:</strong> Bernoulli PMF $p^k(1-p)^{1-k}$ ‚Üî Beta PDF $\propto x^{\alpha-1}(1-x)^{\beta-1}$</p>
                    <p class="dist-params"><strong>Construction:</strong> $X/(X+Y)$ where $X \sim \text{Gamma}(\alpha, 1)$, $Y \sim \text{Gamma}(\beta, 1)$ independent</p>
                    <p class="dist-params"><strong>Special case:</strong> Beta$(1,1) = $ Uniform$(0,1)$</p>
                    
                    <div class="derivation-toggle">
                        <button class="derivation-btn" onclick="toggleDerivation(this)" aria-expanded="false" aria-controls="derivation-beta">
                            <span>Show Derivation from Bernoulli</span>
                            <span class="arrow">‚ñº</span>
                        </button>
                        <div class="derivation-content" id="derivation-beta" aria-hidden="true">
                            <div class="derivation-chain">
                                <span class="chain-label">Path:</span>
                                <span class="chain-item">Bernoulli$(p)$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">Geometric</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">Exponential</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">Gamma$(\alpha,1)$, Gamma$(\beta,1)$</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item">Ratio</span>
                                <span class="chain-arrow">‚Üí</span>
                                <span class="chain-item"><strong>Beta$(\alpha,\beta)$</strong></span>
                            </div>
                            
                            <h4>Why Beta is the Continuous Bernoulli</h4>
                            <p>Compare the functional forms:</p>
                            <div class="math-block">
                                $$\text{Bernoulli: } P(X = k) = p^k(1-p)^{1-k}, \quad k \in \{0, 1\}$$
                                $$\text{Beta: } f(x) \propto x^{\alpha-1}(1-x)^{\beta-1}, \quad x \in [0, 1]$$
                            </div>
                            <p>The structural parallel is striking: both have the form $(\text{something})^a \cdot (1 - \text{something})^b$. Bernoulli allocates probability mass to two points; Beta spreads density across the continuum. When $\alpha = \beta = 1$, Beta becomes Uniform‚Äîno preference across $[0,1]$‚Äîjust as Bernoulli$(0.5)$ shows no preference between 0 and 1.</p>
                            
                            <h4>Construction from Gammas (rooted in Bernoulli)</h4>
                            <p>Let $X \sim \text{Gamma}(\alpha, 1)$ and $Y \sim \text{Gamma}(\beta, 1)$ be independent. (Recall: Gamma is a sum of Exponentials, which are limits of Geometrics from Bernoulli.)</p>
                            
                            <div class="step">
                                <span class="step-number">Step 1: Joint density of independent Gammas</span>
                                <div class="math-block">
                                    $$f_{X,Y}(x,y) = \frac{x^{\alpha-1}e^{-x}}{\Gamma(\alpha)} \cdot \frac{y^{\beta-1}e^{-y}}{\Gamma(\beta)}$$
                                </div>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 2: Transform to proportion and total</span>
                                <p>Let $U = X/(X+Y)$ (proportion) and $V = X + Y$ (total).</p>
                                <p>Inverse: $X = UV$, $Y = V(1-U)$. Jacobian: $|J| = V$.</p>
                            </div>
                            
                            <div class="step">
                                <span class="step-number">Step 3: Integrate out the total $V$</span>
                                <div class="math-block">
                                    $$f_U(u) = \frac{u^{\alpha-1}(1-u)^{\beta-1}}{\Gamma(\alpha)\Gamma(\beta)} \int_0^\infty v^{\alpha+\beta-1}e^{-v}\,dv = \frac{u^{\alpha-1}(1-u)^{\beta-1}}{B(\alpha,\beta)}$$
                                </div>
                            </div>
                            
                            <h4>The Beta-Bernoulli Conjugacy: The Deepest Connection</h4>
                            <p>Beta is the <em>conjugate prior</em> for the Bernoulli/Binomial likelihood‚Äîmeaning Beta is the natural "uncertainty distribution" for a Bernoulli probability parameter:</p>
                            <div class="math-block">
                                $$\text{Prior: } p \sim \text{Beta}(\alpha, \beta)$$
                                $$\text{Likelihood: } X_1, \ldots, X_n \mid p \stackrel{\text{iid}}{\sim} \text{Bernoulli}(p)$$
                                $$\text{Posterior: } p \mid X_1, \ldots, X_n \sim \text{Beta}(\alpha + \sum X_i, \beta + n - \sum X_i)$$
                            </div>
                            <p>The posterior stays in the Beta family! This isn't coincidence‚Äîit reflects the deep structural kinship: <strong>Beta quantifies uncertainty about the Bernoulli parameter, just as Bernoulli quantifies uncertainty about a binary outcome.</strong></p>
                            
                            <h4>Order Statistics Connection</h4>
                            <p>If $U_1, \ldots, U_n \stackrel{\text{iid}}{\sim} \text{Uniform}(0,1)$, then the $k$-th order statistic $U_{(k)} \sim \text{Beta}(k, n-k+1)$. This provides another bridge: uniform random variables (the foundation of all simulation) naturally generate Beta distributions through ordering.</p>
                        </div>
                    </div>
                    
                    <div class="python-label">Python</div>
                    <div class="python-code">
                        <span class="comment"># === Sampling ===</span><br>
                        rng.beta(alpha, beta, size=1000)<br><br>
                        <span class="comment"># === SciPy distribution object ===</span><br>
                        dist = scipy.stats.beta(alpha, beta)<br>
                        dist.rvs(size=1000)  <span class="comment"># sampling</span><br>
                        dist.pdf(x)  <span class="comment"># density</span><br>
                        dist.cdf(x)  <span class="comment"># P(X ‚â§ x)</span><br>
                        dist.ppf(q)  <span class="comment"># quantile</span><br>
                        dist.mean(), dist.var()  <span class="comment"># Œ±/(Œ±+Œ≤), ...</span><br>
                        dist.interval(0.95)  <span class="comment"># credible interval</span><br><br>
                        <span class="comment"># === Python stdlib ===</span><br>
                        random.betavariate(alpha, beta)
                    </div>
                </div>
            </div>
        </div>
        
        <!-- Summary Section -->
        <div class="connection-box" style="margin-top: 3rem;">
            <h3><span class="connection-icon">üìä</span> Complete Distribution Hierarchy</h3>
            
            <table class="summary-table">
                <thead>
                    <tr>
                        <th>Distribution</th>
                        <th>Type</th>
                        <th>Connection to Bernoulli</th>
                        <th>Key Transform</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Discrete Uniform$(a,b)$</strong></td>
                        <td>Discrete</td>
                        <td>Binary expansion of Bernoulli$(0.5)$</td>
                        <td>$\sum B_i \cdot 2^{i-1}$</td>
                    </tr>
                    <tr>
                        <td><strong>Bernoulli$(p)$</strong></td>
                        <td>Discrete</td>
                        <td>Foundation (binary)</td>
                        <td>‚Äî</td>
                    </tr>
                    <tr>
                        <td><strong>Binomial$(n,p)$</strong></td>
                        <td>Discrete</td>
                        <td>Sum of $n$ Bernoullis</td>
                        <td>$S = \sum_{i=1}^n X_i$</td>
                    </tr>
                    <tr>
                        <td><strong>Multinomial$(n, \mathbf{p})$</strong></td>
                        <td>Discrete</td>
                        <td>$k$-category generalization of Binomial</td>
                        <td>Count each of $k$ outcomes in $n$ trials</td>
                    </tr>
                    <tr>
                        <td><strong>Geometric$(p)$</strong></td>
                        <td>Discrete</td>
                        <td>First success in Bernoulli seq.</td>
                        <td>$Y = \min\{k: X_k = 1\}$</td>
                    </tr>
                    <tr>
                        <td><strong>Negative Binomial$(r,p)$</strong></td>
                        <td>Discrete</td>
                        <td>Sum of $r$ Geometrics</td>
                        <td>$S = \sum_{i=1}^r Y_i$</td>
                    </tr>
                    <tr>
                        <td><strong>Poisson$(\lambda)$</strong></td>
                        <td>Discrete</td>
                        <td>Binomial limit: $n\to\infty$, $np\to\lambda$</td>
                        <td>Rare events limit</td>
                    </tr>
                    <tr>
                        <td><strong>Uniform$(a,b)$</strong></td>
                        <td>Continuous</td>
                        <td>Foundation for random generation</td>
                        <td>Inverse CDF: $X = F^{-1}(U)$</td>
                    </tr>
                    <tr>
                        <td><strong>Normal$(0,1)$</strong></td>
                        <td>Continuous</td>
                        <td>CLT limit of standardized Binomial</td>
                        <td>$(S_n - np)/\sqrt{np(1-p)}$</td>
                    </tr>
                    <tr>
                        <td><strong>Exponential$(\lambda)$</strong></td>
                        <td>Continuous</td>
                        <td>Geometric limit (continuous time)</td>
                        <td>$p \to 0$, $np \to \lambda$</td>
                    </tr>
                    <tr>
                        <td><strong>Gamma$(\alpha, \beta)$</strong></td>
                        <td>Continuous</td>
                        <td>Sum of Exponentials; NegBin limit</td>
                        <td>$S = \sum \text{Exp}_i$</td>
                    </tr>
                    <tr>
                        <td><strong>Chi-squared$_k$</strong></td>
                        <td>Continuous</td>
                        <td>Sum of squared Normals</td>
                        <td>$Q = \sum Z_i^2$</td>
                    </tr>
                    <tr>
                        <td><strong>Student's $t_\nu$</strong></td>
                        <td>Continuous</td>
                        <td>Normal/Chi-squared ratio</td>
                        <td>$Z/\sqrt{V/\nu}$</td>
                    </tr>
                    <tr>
                        <td><strong>$F_{d_1, d_2}$</strong></td>
                        <td>Continuous</td>
                        <td>Chi-squared ratio</td>
                        <td>$(U/d_1)/(V/d_2)$</td>
                    </tr>
                    <tr>
                        <td><strong>Cauchy$(x_0, \gamma)$</strong></td>
                        <td>Continuous</td>
                        <td>$t_1$ (Student's $t$ with $\nu=1$)</td>
                        <td>$Z_1/Z_2$ for iid Normals</td>
                    </tr>
                    <tr>
                        <td><strong>Beta$(\alpha, \beta)$</strong></td>
                        <td>Continuous</td>
                        <td>Gamma ratio; Bernoulli conjugate</td>
                        <td>$X/(X+Y)$; continuous analog of Bernoulli</td>
                    </tr>
                    <tr>
                        <td><strong>Hypergeometric$(N,K,n)$</strong></td>
                        <td>Discrete</td>
                        <td>Conditional Binomial; sampling w/o replacement</td>
                        <td>$P(X=k \mid X+Y=K)$</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="key-insight">
            <h4>üí° The Grand Unified View</h4>
            <p>Every distribution in classical probability can be traced back to the Bernoulli trial and the Uniform distribution. The <strong>Uniform$(0,1)$</strong> is the foundation of all random generation via the inverse CDF method. The key operations connecting distributions are: <strong>summation</strong> (Binomial, Negative Binomial, Gamma, Chi-squared), <strong>generalization</strong> (Multinomial extends Binomial to $k$ categories), <strong>waiting/counting</strong> (Geometric, Exponential), <strong>limits</strong> (Poisson, Normal, Exponential), <strong>ratios</strong> (t, F, Beta, Cauchy), and <strong>conditioning</strong> (Hypergeometric). The CLT is the great bridge between the discrete and continuous worlds, making the Normal distribution the universal attractor for sums. The Beta distribution plays a special role as the continuous analog of Bernoulli‚Äîliving on $[0,1]$ just as Bernoulli probabilities do, while Dirichlet generalizes Beta to probability vectors just as Multinomial generalizes Binomial.</p>
        </div>
        
        <!-- Parallel Seeding Section -->
        <div class="connection-box" style="margin-top: 3rem;">
            <h3><span class="connection-icon">‚ö°</span> Reproducibility & Parallel Random Generation</h3>
            <p>Proper seeding ensures reproducible results. For parallel computing, use <code>SeedSequence.spawn()</code> to create independent streams‚Äînever use sequential seeds like 1, 2, 3.</p>
            
            <div class="derivation-toggle" style="margin-top: 1.5rem;">
                <button class="derivation-btn" onclick="toggleDerivation(this)" aria-expanded="false" aria-controls="derivation-parallel-seeding">
                    <span>Show Parallel Seeding Code</span>
                    <span class="arrow">‚ñº</span>
                </button>
                <div class="derivation-content" id="derivation-parallel-seeding" aria-hidden="true">
                    <h4>Modern NumPy Random Generator API</h4>
                    <div class="python-code" style="font-size: 0.9rem;">
                        <span class="comment"># === Basic Setup (ALWAYS use default_rng) ===</span><br>
                        <span class="keyword">import</span> numpy <span class="keyword">as</span> np<br>
                        <span class="keyword">from</span> numpy.random <span class="keyword">import</span> default_rng, SeedSequence<br><br>
                        
                        <span class="comment"># Reproducible single-stream</span><br>
                        rng = default_rng(seed=<span class="number">42</span>)<br>
                        samples = rng.normal(<span class="number">0</span>, <span class="number">1</span>, size=<span class="number">1000</span>)
                    </div>
                    
                    <h4>Parallel Independent Streams (The Right Way)</h4>
                    <div class="python-code" style="font-size: 0.9rem;">
                        <span class="comment"># === Create independent generators for parallel workers ===</span><br>
                        <span class="keyword">def</span> <span class="function">create_parallel_generators</span>(master_seed, n_workers):<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;<span class="string">"""Create n independent generators from one master seed."""</span><br>
                        &nbsp;&nbsp;&nbsp;&nbsp;ss = SeedSequence(master_seed)<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;child_seeds = ss.spawn(n_workers)<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">return</span> [default_rng(s) <span class="keyword">for</span> s <span class="keyword">in</span> child_seeds]<br><br>
                        
                        <span class="comment"># Create 8 independent generators</span><br>
                        generators = create_parallel_generators(<span class="number">42</span>, <span class="number">8</span>)<br><br>
                        
                        <span class="comment"># Each worker gets its own generator</span><br>
                        <span class="keyword">for</span> i, rng <span class="keyword">in</span> <span class="function">enumerate</span>(generators):<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;<span class="function">print</span>(<span class="string">f"Worker {i}: {rng.random(3).round(4)}"</span>)
                    </div>
                    
                    <h4>Complete Parallel Monte Carlo Example</h4>
                    <div class="python-code" style="font-size: 0.9rem;">
                        <span class="keyword">from</span> concurrent.futures <span class="keyword">import</span> ProcessPoolExecutor<br><br>
                        
                        <span class="keyword">def</span> <span class="function">pi_worker</span>(child_seed, n_points):<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;<span class="string">"""Estimate œÄ using independent RNG stream."""</span><br>
                        &nbsp;&nbsp;&nbsp;&nbsp;rng = default_rng(child_seed)<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;x, y = rng.random(n_points), rng.random(n_points)<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;inside = np.sum(x**<span class="number">2</span> + y**<span class="number">2</span> <= <span class="number">1</span>)<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">return</span> inside, n_points<br><br>
                        
                        <span class="keyword">def</span> <span class="function">parallel_pi</span>(master_seed=<span class="number">42</span>, n_workers=<span class="number">4</span>, n_per_worker=<span class="number">1_000_000</span>):<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;ss = SeedSequence(master_seed)<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;child_seeds = ss.spawn(n_workers)<br><br>
                        &nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">with</span> ProcessPoolExecutor(n_workers) <span class="keyword">as</span> ex:<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;results = <span class="function">list</span>(ex.map(pi_worker, child_seeds,<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[n_per_worker]*n_workers))<br><br>
                        &nbsp;&nbsp;&nbsp;&nbsp;total_in = <span class="function">sum</span>(r[<span class="number">0</span>] <span class="keyword">for</span> r <span class="keyword">in</span> results)<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;total_n = <span class="function">sum</span>(r[<span class="number">1</span>] <span class="keyword">for</span> r <span class="keyword">in</span> results)<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">return</span> <span class="number">4</span> * total_in / total_n<br><br>
                        
                        pi_est = parallel_pi()  <span class="comment"># Reproducible across runs!</span>
                    </div>
                    
                    <div class="step" style="background: rgba(220, 53, 69, 0.1); border-left-color: #dc3545;">
                        <span class="step-number" style="color: #dc3545;">‚ö†Ô∏è Common Mistake: Sequential Seeds</span>
                        <p>Never create parallel generators with sequential seeds like <code>default_rng(0), default_rng(1), ...</code>‚Äîthey may have subtle correlations. Always use <code>SeedSequence.spawn()</code> for guaranteed independence.</p>
                    </div>
                </div>
            </div>
        </div>
        
        <!-- Documentation Links -->
        <div class="connection-box" style="margin-top: 2rem;">
            <h3><span class="connection-icon">üìö</span> Library Documentation & Resources</h3>
            
            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 1.5rem; margin-top: 1.5rem;">
                <div style="background: var(--bg-secondary); padding: 1.25rem; border-radius: 8px;">
                    <h4 style="color: var(--accent-discrete); margin-bottom: 0.75rem;">NumPy Random</h4>
                    <ul style="list-style: none; padding: 0; margin: 0; font-size: 0.9rem;">
                        <li style="margin-bottom: 0.5rem;">üìñ <a href="https://numpy.org/doc/stable/reference/random/index.html" target="_blank" style="color: var(--text-secondary);">Random Sampling Guide</a></li>
                        <li style="margin-bottom: 0.5rem;">üìñ <a href="https://numpy.org/doc/stable/reference/random/generator.html" target="_blank" style="color: var(--text-secondary);">Generator API Reference</a></li>
                        <li style="margin-bottom: 0.5rem;">üìñ <a href="https://numpy.org/doc/stable/reference/random/parallel.html" target="_blank" style="color: var(--text-secondary);">Parallel Random Generation</a></li>
                        <li>üìñ <a href="https://numpy.org/doc/stable/reference/random/bit_generators/index.html" target="_blank" style="color: var(--text-secondary);">Bit Generators (PCG64, etc.)</a></li>
                    </ul>
                </div>
                
                <div style="background: var(--bg-secondary); padding: 1.25rem; border-radius: 8px;">
                    <h4 style="color: var(--accent-continuous); margin-bottom: 0.75rem;">SciPy Statistics</h4>
                    <ul style="list-style: none; padding: 0; margin: 0; font-size: 0.9rem;">
                        <li style="margin-bottom: 0.5rem;">üìñ <a href="https://docs.scipy.org/doc/scipy/reference/stats.html" target="_blank" style="color: var(--text-secondary);">scipy.stats Module</a></li>
                        <li style="margin-bottom: 0.5rem;">üìñ <a href="https://docs.scipy.org/doc/scipy/tutorial/stats.html" target="_blank" style="color: var(--text-secondary);">Statistics Tutorial</a></li>
                        <li style="margin-bottom: 0.5rem;">üìñ <a href="https://docs.scipy.org/doc/scipy/tutorial/stats/continuous.html" target="_blank" style="color: var(--text-secondary);">Continuous Distributions</a></li>
                        <li>üìñ <a href="https://docs.scipy.org/doc/scipy/tutorial/stats/discrete.html" target="_blank" style="color: var(--text-secondary);">Discrete Distributions</a></li>
                    </ul>
                </div>
                
                <div style="background: var(--bg-secondary); padding: 1.25rem; border-radius: 8px;">
                    <h4 style="color: var(--accent-bernoulli); margin-bottom: 0.75rem;">Python Standard Library</h4>
                    <ul style="list-style: none; padding: 0; margin: 0; font-size: 0.9rem;">
                        <li style="margin-bottom: 0.5rem;">üìñ <a href="https://docs.python.org/3/library/random.html" target="_blank" style="color: var(--text-secondary);">random Module</a></li>
                        <li style="margin-bottom: 0.5rem;">üìñ <a href="https://docs.python.org/3/library/secrets.html" target="_blank" style="color: var(--text-secondary);">secrets Module (crypto)</a></li>
                        <li>üìñ <a href="https://docs.python.org/3/library/statistics.html" target="_blank" style="color: var(--text-secondary);">statistics Module</a></li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
    </main>
    <footer>
        <p><strong>STAT 418: Computational Methods in Data Science</strong><br>
        Dr. Timothy Reese | Purdue University | Spring 2026</p>
        <p style="margin-top: 1rem; font-size: 0.85rem;">Understanding distribution relationships is fundamental to statistical inference, simulation, and Bayesian analysis.</p>
    </footer>
    
    <script>
        // Initialize KaTeX
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false}
                ],
                throwOnError: false,
                output: 'mathml'  // Adds MathML for assistive tech
            });
        });
        
        // Toggle between discrete and continuous sections
        function showSection(section) {
            // Update buttons
            document.querySelectorAll('.mode-btn').forEach(btn => {
                btn.classList.remove('active');
            });
            document.querySelector(`.mode-btn.${section}`).classList.add('active');
            
            // Update sections
            document.querySelectorAll('.distribution-section').forEach(sec => {
                sec.classList.remove('active');
            });
            document.getElementById(`${section}-section`).classList.add('active');
        }
        
        // Toggle derivation content
        function toggleDerivation(button) {
            const content = button.nextElementSibling;
            const isOpen = content.classList.contains('open');
            // Update ARIA states
            button.setAttribute('aria-expanded', !isOpen);
            content.setAttribute('aria-hidden', isOpen);
            // Close all other derivations in the same card
            const card = button.closest('.dist-card, .transition-card, .connection-box');
            if (card) {
                card.querySelectorAll('.derivation-content.open').forEach(el => {
                    if (el !== content) {
                        el.classList.remove('open');
                        el.previousElementSibling.classList.remove('open');
                    }
                });
            }
            
            // Toggle current
            button.classList.toggle('open', !isOpen);
            content.classList.toggle('open', !isOpen);
            
            // Update button text
            const span = button.querySelector('span:first-child');
            if (!isOpen) {
                span.textContent = span.textContent.replace('Show', 'Hide');
            } else {
                span.textContent = span.textContent.replace('Hide', 'Show');
            }
        }
        
        // Smooth scroll for internal links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function(e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    target.scrollIntoView({ behavior: 'smooth', block: 'start' });
                }
            });
        });
    </script>
</body>
</html>
