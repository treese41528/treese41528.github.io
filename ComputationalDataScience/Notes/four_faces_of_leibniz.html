<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Four Faces of Leibniz</title>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body, {
    delimiters: [
      {left: '$$', right: '$$', display: true},
      {left: '$', right: '$', display: false}
    ],
    throwOnError: false
  });"></script>
<link href="https://fonts.googleapis.com/css2?family=Newsreader:ital,opsz,wght@0,6..72,300;0,6..72,400;0,6..72,500;0,6..72,600;1,6..72,300;1,6..72,400&family=JetBrains+Mono:wght@400;500&family=DM+Sans:ital,opsz,wght@0,9..40,400;0,9..40,500;0,9..40,600;1,9..40,400&display=swap" rel="stylesheet">
<style>
  :root {
    --paper: #faf8f4;
    --paper-deep: #f3efe8;
    --ink: #1a1612;
    --ink-light: #4a453d;
    --ink-faint: #8a8478;
    --rule: #d4cfc6;
    --accent-score: #8b3a3a;
    --accent-partition: #2d5a3d;
    --accent-reinforce: #3a4f8b;
    --accent-matching: #6b4a8b;
    --accent-score-bg: #f9f0f0;
    --accent-partition-bg: #eef5f0;
    --accent-reinforce-bg: #eef1f9;
    --accent-matching-bg: #f3eef9;
  }

  * { margin: 0; padding: 0; box-sizing: border-box; }

  body {
    font-family: 'Newsreader', Georgia, serif;
    font-size: 17px;
    line-height: 1.72;
    color: var(--ink);
    background: var(--paper);
    -webkit-font-smoothing: antialiased;
  }

  /* --- HERO --- */
  .hero {
    padding: 6rem 2rem 4rem;
    text-align: center;
    position: relative;
    overflow: hidden;
  }
  .hero::before {
    content: 'âˆ«';
    position: absolute;
    font-size: 42rem;
    font-weight: 300;
    color: rgba(26, 22, 18, 0.022);
    top: -10rem;
    left: 50%;
    transform: translateX(-50%);
    pointer-events: none;
    font-family: 'Newsreader', serif;
  }
  .hero-label {
    font-family: 'DM Sans', sans-serif;
    font-size: 0.72rem;
    font-weight: 600;
    letter-spacing: 0.2em;
    text-transform: uppercase;
    color: var(--ink-faint);
    margin-bottom: 1.5rem;
  }
  .hero h1 {
    font-size: clamp(2.4rem, 5.5vw, 3.8rem);
    font-weight: 300;
    line-height: 1.15;
    margin-bottom: 1.5rem;
    letter-spacing: -0.01em;
  }
  .hero h1 em {
    font-style: italic;
    font-weight: 400;
  }
  .hero-sub {
    font-size: 1.15rem;
    color: var(--ink-light);
    max-width: 40rem;
    margin: 0 auto;
    font-weight: 300;
    line-height: 1.65;
  }

  /* --- MAIN CONTAINER --- */
  .container {
    max-width: 52rem;
    margin: 0 auto;
    padding: 0 2rem 6rem;
  }

  /* --- CASE SECTIONS --- */
  .case-section {
    margin-bottom: 4.5rem;
    position: relative;
  }
  .case-header {
    display: flex;
    align-items: baseline;
    gap: 1rem;
    margin-bottom: 2rem;
    padding-bottom: 0.8rem;
    border-bottom: 1.5px solid var(--rule);
  }
  .case-number {
    font-family: 'DM Sans', sans-serif;
    font-size: 0.68rem;
    font-weight: 600;
    letter-spacing: 0.15em;
    text-transform: uppercase;
    white-space: nowrap;
    padding: 0.25rem 0.7rem;
    border-radius: 3px;
    flex-shrink: 0;
  }
  .case-section[data-case="score"] .case-number {
    background: var(--accent-score-bg);
    color: var(--accent-score);
  }
  .case-section[data-case="partition"] .case-number {
    background: var(--accent-partition-bg);
    color: var(--accent-partition);
  }
  .case-section[data-case="reinforce"] .case-number {
    background: var(--accent-reinforce-bg);
    color: var(--accent-reinforce);
  }
  .case-section[data-case="matching"] .case-number {
    background: var(--accent-matching-bg);
    color: var(--accent-matching);
  }
  .case-title {
    font-size: 1.55rem;
    font-weight: 400;
    letter-spacing: -0.01em;
  }

  /* --- MATH BLOCKS --- */
  .math-block {
    background: var(--paper-deep);
    border-left: 3px solid var(--rule);
    padding: 1.4rem 1.8rem;
    margin: 1.6rem 0;
    border-radius: 0 6px 6px 0;
    overflow-x: auto;
  }
  .case-section[data-case="score"] .math-block { border-left-color: var(--accent-score); }
  .case-section[data-case="partition"] .math-block { border-left-color: var(--accent-partition); }
  .case-section[data-case="reinforce"] .math-block { border-left-color: var(--accent-reinforce); }
  .case-section[data-case="matching"] .math-block { border-left-color: var(--accent-matching); }

  .result-block {
    border: 1.5px solid var(--rule);
    background: #fff;
    padding: 1.2rem 1.8rem;
    margin: 1.6rem 0;
    border-radius: 6px;
    position: relative;
  }
  .result-block::before {
    content: attr(data-label);
    position: absolute;
    top: -0.6rem;
    left: 1.2rem;
    background: #fff;
    padding: 0 0.5rem;
    font-family: 'DM Sans', sans-serif;
    font-size: 0.68rem;
    font-weight: 600;
    letter-spacing: 0.12em;
    text-transform: uppercase;
  }
  .case-section[data-case="score"] .result-block { border-color: var(--accent-score); }
  .case-section[data-case="score"] .result-block::before { color: var(--accent-score); }
  .case-section[data-case="partition"] .result-block { border-color: var(--accent-partition); }
  .case-section[data-case="partition"] .result-block::before { color: var(--accent-partition); }
  .case-section[data-case="reinforce"] .result-block { border-color: var(--accent-reinforce); }
  .case-section[data-case="reinforce"] .result-block::before { color: var(--accent-reinforce); }
  .case-section[data-case="matching"] .result-block { border-color: var(--accent-matching); }
  .case-section[data-case="matching"] .result-block::before { color: var(--accent-matching); }

  /* --- STEP LABELS --- */
  .step-label {
    font-family: 'DM Sans', sans-serif;
    font-size: 0.78rem;
    font-weight: 600;
    color: var(--ink-light);
    margin-top: 2rem;
    margin-bottom: 0.6rem;
    letter-spacing: 0.03em;
  }

  /* --- GIVES-YOU --- */
  .gives-you {
    margin-top: 1.8rem;
    padding: 1rem 1.4rem;
    background: var(--paper-deep);
    border-radius: 6px;
    font-size: 0.92rem;
  }
  .gives-you strong {
    font-family: 'DM Sans', sans-serif;
    font-size: 0.72rem;
    font-weight: 600;
    letter-spacing: 0.1em;
    text-transform: uppercase;
    color: var(--ink-faint);
  }

  /* --- SUMMARY TABLE --- */
  .summary-section {
    margin-top: 5rem;
    padding-top: 3rem;
    border-top: 2px solid var(--ink);
  }
  .summary-section h2 {
    font-size: 1.8rem;
    font-weight: 300;
    margin-bottom: 0.6rem;
  }
  .summary-section .summary-sub {
    color: var(--ink-light);
    font-weight: 300;
    margin-bottom: 2.5rem;
  }
  .summary-table {
    width: 100%;
    border-collapse: collapse;
    font-size: 0.88rem;
    line-height: 1.55;
  }
  .summary-table thead th {
    font-family: 'DM Sans', sans-serif;
    font-size: 0.7rem;
    font-weight: 600;
    letter-spacing: 0.12em;
    text-transform: uppercase;
    color: var(--ink-faint);
    text-align: left;
    padding: 0.7rem 1rem;
    border-bottom: 2px solid var(--ink);
  }
  .summary-table tbody td {
    padding: 1rem;
    border-bottom: 1px solid var(--rule);
    vertical-align: top;
  }
  .summary-table tbody tr:last-child td {
    border-bottom: none;
  }
  .summary-table .case-dot {
    display: inline-block;
    width: 8px;
    height: 8px;
    border-radius: 50%;
    margin-right: 0.5rem;
    vertical-align: middle;
    position: relative;
    top: -1px;
  }
  .dot-score { background: var(--accent-score); }
  .dot-partition { background: var(--accent-partition); }
  .dot-reinforce { background: var(--accent-reinforce); }
  .dot-matching { background: var(--accent-matching); }

  .summary-table .app-tags {
    display: flex;
    flex-wrap: wrap;
    gap: 0.3rem;
  }
  .summary-table .app-tag {
    font-family: 'DM Sans', sans-serif;
    font-size: 0.65rem;
    font-weight: 500;
    padding: 0.15rem 0.5rem;
    border-radius: 3px;
    background: var(--paper-deep);
    color: var(--ink-light);
    white-space: nowrap;
  }

  /* --- FOOTER --- */
  .footer {
    margin-top: 5rem;
    padding: 2rem 0;
    border-top: 1px solid var(--rule);
    font-family: 'DM Sans', sans-serif;
    font-size: 0.75rem;
    color: var(--ink-faint);
    text-align: center;
  }

  /* --- UTILITIES --- */
  p { margin-bottom: 0.9rem; }
  .katex-display { margin: 0.8rem 0 !important; }
  .katex { font-size: 1.05em; }

  /* --- RESPONSIVE --- */
  @media (max-width: 700px) {
    .hero { padding: 4rem 1.5rem 3rem; }
    .container { padding: 0 1.2rem 4rem; }
    .case-header { flex-direction: column; gap: 0.5rem; }
    .math-block, .result-block { padding: 1rem 1.2rem; }
    .summary-table { font-size: 0.78rem; }
    .summary-table thead th, .summary-table tbody td { padding: 0.6rem 0.5rem; }
  }

  /* --- SCROLL ANIMATIONS --- */
  .case-section, .summary-section {
    opacity: 0;
    transform: translateY(24px);
    transition: opacity 0.6s ease, transform 0.6s ease;
  }
  .case-section.visible, .summary-section.visible {
    opacity: 1;
    transform: translateY(0);
  }
</style>
</head>
<body>

<!-- ========== HERO ========== -->
<header class="hero">
  <div class="hero-label">Computational Methods in Data Science</div>
  <h1>Four Faces of <em>Leibniz</em></h1>
  <p class="hero-sub">
    Each identity starts from an integral that equals something known,
    passes&nbsp;$\partial/\partial(\cdot)$ through&nbsp;$\int$, and extracts a useful relationship
    from the constraint that the integral is constant.
    We don&rsquo;t learn these rules just to find the MLE of a coin flip.
    We learn them because they are the mathematical engines that make reinforcement learning,
    generative AI, and complex spatial modeling computationally possible.
  </p>
</header>

<main class="container">

<p style="font-size: 0.92rem; color: var(--ink-light); font-style: italic; margin-bottom: 3rem; line-height: 1.7;">
  Throughout Cases 1&ndash;3, interchanging differentiation and integration is valid under standard regularity conditions: fixed support in the parameter, differentiability a.e., and a dominating integrable bound. When these fail &mdash; $\text{Uniform}(0,\theta)$ violates fixed support, $\text{Laplace}(\theta)$ violates smoothness &mdash; the downstream identities can fail. In a computational pipeline, these failures are not abstract: Monte Carlo gradient estimates become biased, Fisher information matrices become singular or negative definite, and optimizers receive corrupted curvature information &mdash; what a theorist calls a &ldquo;regularity violation&rdquo; shows up as gradients that explode, vanish, or point the wrong way. Case 4 requires a boundary condition instead (stated explicitly below).
</p>

<!-- ========================================= -->
<!-- CASE 1: CLASSICAL SCORE                   -->
<!-- ========================================= -->
<section class="case-section" data-case="score">
  <div class="case-header">
    <span class="case-number">Case 1</span>
    <h2 class="case-title">Classical Score</h2>
  </div>

  <p class="step-label">Starting identity</p>
  <div class="math-block">
    $$\int f(x|\theta)\,dx = 1 \quad \text{for all } \theta$$
  </div>

  <p class="step-label">First Leibniz &mdash; pass $\frac{\partial}{\partial\theta}$ through $\int$</p>
  <div class="math-block">
    $$0 \;=\; \frac{\partial}{\partial\theta}\int f(x|\theta)\,dx
       \;=\; \int \frac{\partial f}{\partial\theta}\,dx
       \;=\; \int \frac{\partial \log f}{\partial\theta}\cdot f\,dx
       \;=\; \mathbb{E}_\theta\!\left[\frac{\partial \log f}{\partial\theta}\right]$$
  </div>

  <div class="result-block" data-label="Result">
    $$\mathbb{E}_\theta\!\left[U(\theta)\right] = 0$$
  </div>

  <p class="step-label">Second Leibniz &mdash; pass $\frac{\partial}{\partial\theta}$ through $\int$ again</p>

  <p>Starting from $\int \frac{\partial \log f}{\partial\theta} \cdot f\,dx = 0$, differentiate the integrand via the product rule:</p>

  <div class="math-block">
    $$\frac{\partial}{\partial\theta}\!\!\left(\frac{\partial \log f}{\partial\theta}\cdot f\right) = \frac{\partial^2 \log f}{\partial\theta^2}\cdot f \;+\; \left(\frac{\partial \log f}{\partial\theta}\right)^{\!2}\cdot f$$
  </div>

  <p>Integrate both sides (passing $\frac{\partial}{\partial\theta}$ through $\int$):</p>

  <div class="math-block">
    $$0 = \mathbb{E}_\theta\!\left[\frac{\partial^2 \log f}{\partial\theta^2}\right] + \mathbb{E}_\theta\!\left[\left(\frac{\partial \log f}{\partial\theta}\right)^{\!2}\right]$$
  </div>

  <div class="result-block" data-label="Definition (always valid)">
    $$I(\theta) = \mathbb{E}\!\left[\left(\frac{\partial \log f}{\partial\theta}\right)^{\!2}\right]$$
  </div>

  <div class="result-block" data-label="Under regularity (second interchange)">
    $$I(\theta) = -\,\mathbb{E}\!\left[\frac{\partial^2 \log f}{\partial\theta^2}\right]$$
  </div>

  <p>The definition is a variance &mdash; always non-negative, requires only $\mathbb{E}[U]=0$. The second form is a theorem: it requires the second interchange to be valid. When it fails &mdash; $\text{Uniform}(0,\theta)$, $\text{Laplace}(\theta)$ &mdash; the two expressions disagree, and the definition governs.</p>

  <div class="gives-you">
    <strong>What it gives you</strong><br>
    MLE theory, Fisher information, Cram&eacute;r&ndash;Rao bound, score tests, natural gradient.
  </div>
</section>

<!-- ========================================= -->
<!-- CASE 2: LOG-PARTITION                     -->
<!-- ========================================= -->
<section class="case-section" data-case="partition">
  <div class="case-header">
    <span class="case-number">Case 2</span>
    <h2 class="case-title">Log-Partition Function</h2>
  </div>

  <p class="step-label">Starting identity</p>
  <div class="math-block">
    $$e^{A(\eta)} = \int h(x)\exp\!\big\{\eta^\top T(x)\big\}\,dx$$
  </div>
  <p>where $A(\eta) = \log \int h(x)\exp\{\eta^\top T(x)\}\,dx$ is the log-partition function. (For discrete $X$, replace $\int\cdots\,dx$ with $\sum_x$ throughout; the parameter $\eta$ is always continuous, so all derivatives with respect to $\eta$ proceed unchanged.)</p>

  <p class="step-label">First Leibniz &mdash; pass $\frac{\partial}{\partial\eta_j}$ through $\int$</p>
  <div class="math-block">
    $$\frac{\partial}{\partial\eta_j}e^{A(\eta)} = \int h(x)\,T_j(x)\,\exp\!\big\{\eta^\top T(x)\big\}\,dx$$
  </div>
  <p>Left side: $A'_j(\eta)\,e^{A(\eta)}$. Right side: $e^{A(\eta)}\,\mathbb{E}_\eta[T_j(X)]$. Cancel $e^{A(\eta)}$:</p>

  <div class="result-block" data-label="First derivatives = moments">
    $$\nabla A(\eta) = \mathbb{E}_\eta[T(X)]$$
  </div>

  <p class="step-label">Second Leibniz &mdash; pass $\frac{\partial}{\partial\eta_k}$ through $\int$ again</p>
  <p>Differentiate $\frac{\partial A}{\partial\eta_j} = \mathbb{E}_\eta[T_j] = \int T_j(x)\,f(x|\eta)\,dx$ with respect to $\eta_k$. By Leibniz:</p>
  <div class="math-block">
    $$\frac{\partial^2 A}{\partial\eta_j\,\partial\eta_k} = \int T_j(x)\,\frac{\partial f(x|\eta)}{\partial\eta_k}\,dx$$
  </div>
  <p>We need $\frac{\partial f}{\partial\eta_k}$. Since $f(x|\eta) = h(x)\exp\{\eta^\top T(x) - A(\eta)\}$, the chain rule gives:</p>
  <div class="math-block">
    $$\frac{\partial f}{\partial\eta_k} = f(x|\eta)\cdot\!\left(T_k(x) - \frac{\partial A}{\partial\eta_k}\right) = f(x|\eta)\cdot\!\left(T_k(x) - \mathbb{E}_\eta[T_k]\right)$$
  </div>
  <p>where the last step uses the first Leibniz result $\frac{\partial A}{\partial\eta_k} = \mathbb{E}_\eta[T_k]$. Substituting back:</p>
  <div class="math-block">
    $$\frac{\partial^2 A}{\partial\eta_j\,\partial\eta_k} = \int T_j(x)\big(T_k(x) - \mathbb{E}_\eta[T_k]\big)\,f(x|\eta)\,dx = \mathbb{E}_\eta[T_j\,T_k] - \mathbb{E}_\eta[T_j]\,\mathbb{E}_\eta[T_k]$$
  </div>

  <div class="result-block" data-label="Hessian = covariance">
    $$\nabla^2 A(\eta) = \mathrm{Var}_\eta\!\big(T(X)\big) \;\succeq\; 0$$
  </div>

  <p>$A$ is convex. Fisher information in natural parameters: $I(\eta) = A''(\eta) = \mathrm{Var}_\eta(T)$. Moments of any order by further differentiation.</p>

  <div class="gives-you">
    <strong>What it gives you</strong><br>
    Exponential family theory, moment-generating properties, convexity of the log-partition, Fisher information via $A''(\eta)$.
  </div>
</section>

<!-- ========================================= -->
<!-- CASE 3: REINFORCE                         -->
<!-- ========================================= -->
<section class="case-section" data-case="reinforce">
  <div class="case-header">
    <span class="case-number">Case 3</span>
    <h2 class="case-title">REINFORCE / Policy Gradient</h2>
  </div>

  <p class="step-label">Starting identity</p>
  <div class="math-block">
    $$V(\theta) = \mathbb{E}_{p(x|\theta)}[f(x)] = \int f(x)\,p(x|\theta)\,dx$$
  </div>
  <p>Here $f(x)$ is a reward that does not depend on $\theta$, and $p(x|\theta)$ is a policy or sampling distribution parameterized by $\theta$. The goal is to find $\theta$ that <em>maximizes</em> $V(\theta)$ &mdash; we want the policy that produces, on average, the highest reward. The natural approach is gradient ascent: $\theta \leftarrow \theta + \alpha\,\nabla_\theta V(\theta)$. So we need $\nabla_\theta V(\theta)$.</p>

  <p class="step-label">The problem &mdash; gradient of an expectation you can only sample from</p>
  <p>We want $\nabla_\theta V(\theta)$, but the expectation is taken over $p(x|\theta)$ &mdash; the distribution we're differentiating with respect to. We can draw samples from $p(x|\theta)$, but we cannot differentiate through the sampling process itself. If $x$ is discrete, or if $p$ is a black-box simulator, a computer looking at the sampling procedure is completely blind &mdash; there is no path to push $\nabla_\theta$ inside naively. The Leibniz interchange is what makes this computable at all: without it, there is no gradient signal, and gradient ascent cannot even start.</p>

  <p class="step-label">The solution &mdash; same idea as Chapter 2</p>
  <p>Recall the fundamental Monte Carlo move from Chapter 2.1: to estimate an integral, rewrite it as an expectation under a distribution you can sample from, then average over samples. Algorithm 2.1 computes $\hat{I}_n = \frac{1}{n}\sum_i h(X_i)$ where $X_i \sim f$. REINFORCE does the same thing for a <em>gradient</em>: it rewrites $\nabla_\theta V$ as $\mathbb{E}_{p(x|\theta)}[\,\cdot\,]$, giving a Monte Carlo estimator</p>
  <div class="math-block">
    $$\widehat{\nabla_\theta V} = \frac{1}{n}\sum_{i=1}^{n} f(x_i)\,\nabla_\theta \log p(x_i|\theta), \qquad x_i \sim p(\cdot|\theta)$$
  </div>
  <p>Draw samples, evaluate $f(x_i)\,\nabla_\theta \log p(x_i|\theta)$ at each one, average. The only new ingredient is the Leibniz step that makes the rewrite possible &mdash; the estimation itself is Algorithm 2.1 applied to a vector-valued integrand.</p>

  <p class="step-label">Leibniz &mdash; pass $\nabla_\theta$ through $\int$</p>
  <div class="math-block">
    $$\nabla_\theta V(\theta) = \int f(x)\,\nabla_\theta p(x|\theta)\,dx$$
  </div>
  <p>Apply the same identity as Case 1: $\nabla_\theta p = p\,\nabla_\theta \log p$:</p>
  <div class="math-block">
    $$= \int f(x)\,p(x|\theta)\,\nabla_\theta \log p(x|\theta)\,dx$$
  </div>

  <div class="result-block" data-label="REINFORCE">
    $$\nabla_\theta\,\mathbb{E}_{p(x|\theta)}[f(x)] = \mathbb{E}_{p(x|\theta)}\!\left[f(x)\,\nabla_\theta \log p(x|\theta)\right]$$
  </div>

  <p class="step-label">Variance reduction via Case 1</p>
  <p>The estimator above is unbiased, but just as Chapter 2 warned that naive Monte Carlo has high variance for rare events, the REINFORCE estimator can have enormous variance &mdash; $f(x)$ can be large and $\nabla_\theta \log p(x|\theta)$ fluctuates across samples. The baseline trick exploits Case 1 to reduce this variance without introducing bias.</p>
  <p>Since $\mathbb{E}_{p(x|\theta)}[\nabla_\theta \log p(x|\theta)] = 0$ (from Case 1), for any constant $b$ that does not depend on $x$:</p>
  <div class="math-block">
    $$\mathbb{E}_{p(x|\theta)}\!\left[b\,\nabla_\theta \log p(x|\theta)\right] = b\,\mathbb{E}_{p(x|\theta)}\!\left[\nabla_\theta \log p(x|\theta)\right] = 0$$
  </div>
  <p>So we can subtract $b$ from $f(x)$ without changing the expected gradient:</p>
  <div class="math-block">
    $$\nabla_\theta V(\theta) = \mathbb{E}_{p(x|\theta)}\!\left[\big(f(x) - b\big)\,\nabla_\theta \log p(x|\theta)\right]$$
  </div>
  <p>This helps because the variance of the estimator scales with $\mathbb{E}[(f(x) - b)^2 \|\nabla_\theta \log p(x|\theta)\|^2]$. By centering $f(x)$ around $b$, we shrink the magnitude of the terms being averaged. The optimal scalar baseline minimizes this variance:</p>
  <div class="result-block" data-label="Optimal baseline">
    $$b^* = \frac{\mathbb{E}_{p(x|\theta)}\!\left[f(x)\,\|\nabla_\theta \log p(x|\theta)\|^2\right]}{\mathbb{E}_{p(x|\theta)}\!\left[\|\nabla_\theta \log p(x|\theta)\|^2\right]}$$
  </div>
  <p>In practice, $b^*$ is estimated from samples, and simpler choices work well: the running mean of rewards in RL, or a learned value function $V_\phi(s)$ in actor-critic methods. The key point is that <em>any</em> $b$ independent of $x$ is valid &mdash; the mean-zero property of the score guarantees unbiasedness.</p>

  <div class="gives-you">
    <strong>What it gives you</strong><br>
    Policy gradients in RL, RLHF fine-tuning of language models, score function estimator for ELBO gradients in variational inference, discrete latent variable models where reparameterization is unavailable.
  </div>
</section>

<!-- ========================================= -->
<!-- CASE 4: SCORE MATCHING                    -->
<!-- ========================================= -->
<section class="case-section" data-case="matching">
  <div class="case-header">
    <span class="case-number">Case 4</span>
    <h2 class="case-title">Score Matching</h2>
  </div>

  <p class="step-label">The problem &mdash; flexible models with intractable normalizing constants</p>
  <p>Suppose you want to model a complex distribution by specifying how &ldquo;unlikely&rdquo; each configuration $x$ is through an <em>energy function</em> $E_\theta(x)$ &mdash; a scalar-valued function where low energy means high probability and high energy means low probability. The model takes the Boltzmann form $p_\theta(x) = \exp(-E_\theta(x))\,/\,Z(\theta)$, where the normalizing constant $Z(\theta) = \int \exp(-E_\theta(x))\,dx$ ensures the density integrates to 1. The energy can be as flexible as you like &mdash; a neural network, a pairwise interaction model, any function that captures the structure you believe is in the data.</p>
  <p>The catch: MLE requires evaluating the likelihood, which requires the normalizing constant $Z(\theta) = \int \exp(-E_\theta(x))\,dx$. For any model rich enough to be interesting, this integral is intractable &mdash; it is a sum or integral over all possible configurations of $x$. This is not a rare corner case. It arises in:</p>
  <ul style="margin: 0.8rem 0 0.8rem 1.5rem; line-height: 1.8;">
    <li><strong>Undirected graphical models</strong> (Markov random fields, Boltzmann machines) &mdash; the partition function sums over exponentially many configurations</li>
    <li><strong>Spatial and network models</strong> &mdash; autologistic models for lattice data, exponential random graph models (ERGMs) for network data</li>
    <li><strong>Energy-based models</strong> in machine learning &mdash; any model of the form $p_\theta(x) \propto \exp(-E_\theta(x))$ with a flexible $E_\theta$</li>
  </ul>
  <p>Without $Z(\theta)$, you cannot compute the likelihood, so you cannot do MLE, you cannot do Bayesian updates, you cannot compare models by AIC or BIC. Score matching offers an escape: fit the model without ever evaluating $Z(\theta)$.</p>

  <p class="step-label">The key observation &mdash; $\nabla_x$ kills the normalizing constant</p>
  <p>We want to fit a model $p_\theta(x)$ to data from unknown $p_{\text{data}}(x)$.
  The <em>data-side score</em> is $\mathbf{s}(x) = \nabla_x \log p(x)$ &mdash; gradient w.r.t. $x$, not $\theta$.
  Unlike the classical score from Case 1 (a scalar measuring sensitivity to the parameter), this is a <em>vector field over data space</em>: at every point $x$, it points in the direction where the density increases fastest. Think of the density $p(x)$ as a landscape &mdash; the data-side score is the slope at every point, telling you which way is &ldquo;uphill&rdquo; toward higher probability. Score matching fits this vector field rather than the probability surface itself, and this is precisely what lets the normalizing constant drop out.</p>
  <p>If $p_\theta(x) = q_\theta(x)/Z(\theta)$, then:</p>
  <div class="math-block">
    $$\nabla_x \log p_\theta(x) = \nabla_x \log q_\theta(x) - \underbrace{\nabla_x \log Z(\theta)}_{=\;0} = \nabla_x \log q_\theta(x)$$
  </div>
  <p>$Z(\theta)$ does not depend on $x$, so its $x$-gradient vanishes.</p>

  <p class="step-label">The Fisher divergence</p>
  <div class="math-block">
    $$D_F(p_{\text{data}} \| p_\theta) = \tfrac{1}{2}\,\mathbb{E}_{p_{\text{data}}}\!\left[\left\|\nabla_x \log p_{\text{data}}(x) - \nabla_x \log p_\theta(x)\right\|^2\right]$$
  </div>
  <p>Expanding the square: the first term is constant in $\theta$, the third is computable. The cross term requires the unknown $\nabla_x \log p_{\text{data}}$.</p>

  <p class="step-label">Integration by parts in $x$ &mdash; the key move</p>
  <p>For each component $i$:</p>
  <div class="math-block">
    $$\mathbb{E}_{p_{\text{data}}}\!\left[\frac{\partial \log p_{\text{data}}}{\partial x_i}\cdot\frac{\partial \log p_\theta}{\partial x_i}\right] = \int \frac{\partial \log p_\theta}{\partial x_i}\cdot\frac{\partial p_{\text{data}}}{\partial x_i}\,dx$$
  </div>
  <p>Integrate by parts in $x_i$ (assuming $p_{\text{data}}(x)\,\frac{\partial \log p_\theta(x)}{\partial x_i} \to 0$ as $\|x\| \to \infty$, so the boundary term vanishes):</p>
  <div class="math-block">
    $$= -\int p_{\text{data}}(x)\,\frac{\partial^2 \log p_\theta(x)}{\partial x_i^2}\,dx = -\,\mathbb{E}_{p_{\text{data}}}\!\left[\frac{\partial^2 \log p_\theta}{\partial x_i^2}\right]$$
  </div>

  <div class="result-block" data-label="Hyv&auml;rinen, 2005">
    $$D_F(p_{\text{data}} \| p_\theta) = \mathbb{E}_{p_{\text{data}}}\!\left[\sum_{i=1}^d \frac{\partial^2 \log p_\theta}{\partial x_i^2} + \frac{1}{2}\!\left(\frac{\partial \log p_\theta}{\partial x_i}\right)^{\!2}\right] + \text{const}$$
  </div>

  <p>The right side involves only derivatives of $\log p_\theta$ evaluated at data points &mdash; no $p_{\text{data}}$ needed, no normalizing constant needed.</p>

  <p class="step-label">Where is Leibniz? Integration by parts is Leibniz</p>
  <p>In Cases 1&ndash;3, we used $\int \frac{\partial}{\partial\theta}[\cdots]\,dx = \frac{\partial}{\partial\theta}\int[\cdots]\,dx$ &mdash; moving a $\theta$-derivative past an $x$-integral. Here the derivative is in $x$, the same variable we integrate over, so the tool is integration by parts rather than Leibniz in its standard form. But it is the same structural move. Start from the product rule:</p>
  <div class="math-block">
    $$\frac{\partial}{\partial x_i}\!\left[p_{\text{data}}(x)\,\frac{\partial \log p_\theta(x)}{\partial x_i}\right] = \frac{\partial p_{\text{data}}}{\partial x_i}\cdot\frac{\partial \log p_\theta}{\partial x_i} \;+\; p_{\text{data}}\cdot\frac{\partial^2 \log p_\theta}{\partial x_i^2}$$
  </div>
  <p>Integrate both sides over all $x$. The left side is a total derivative &mdash; it integrates to a boundary term $\big[p_{\text{data}}\cdot\frac{\partial \log p_\theta}{\partial x_i}\big]_{-\infty}^{+\infty}$ that vanishes when $p_{\text{data}}(x)\,\frac{\partial \log p_\theta}{\partial x_i} \to 0$ as $\|x\|\to\infty$:</p>
  <div class="math-block">
    $$0 = \int \frac{\partial p_{\text{data}}}{\partial x_i}\cdot\frac{\partial \log p_\theta}{\partial x_i}\,dx \;+\; \int p_{\text{data}}\cdot\frac{\partial^2 \log p_\theta}{\partial x_i^2}\,dx$$
  </div>
  <p>This is exactly the pattern: an integral equals something known (zero, because the boundary terms vanish), and the constraint transfers a derivative from one factor ($p_{\text{data}}$, which we don't know) to another ($\log p_\theta$, which we do). In Cases 1&ndash;3, the known constant was $\int f\,dx = 1$. Here it is $\big[p_{\text{data}}\cdot\frac{\partial \log p_\theta}{\partial x_i}\big]_{-\infty}^{+\infty} = 0$. Different constant, same move.</p>

  <div class="gives-you">
    <strong>What it gives you</strong><br>
    Parameter estimation for any model where the normalizing constant is intractable: undirected graphical models, exponential random graph models, energy-based models, non-Gaussian spatial models. This same idea &mdash; matching scores instead of likelihoods &mdash; is the foundation of several important methods you may encounter beyond this course, including denoising score matching, sliced score matching, and the diffusion models (e.g., DALL-E, Stable Diffusion) that drive modern generative AI.
  </div>
</section>

<!-- ========================================= -->
<!-- SUMMARY TABLE                             -->
<!-- ========================================= -->
<section class="summary-section">
  <h2>One Move, Four Domains</h2>
  <p class="summary-sub">The same structural trick &mdash; differentiate an integral that equals something known &mdash; across statistics, exponential family theory, reinforcement learning, and generative modeling.</p>

  <table class="summary-table">
    <thead>
      <tr>
        <th>Case</th>
        <th>Integral</th>
        <th>Diff. w.r.t.</th>
        <th>Key identity</th>
        <th>Applications</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><span class="case-dot dot-score"></span>Score</td>
        <td>$\int f(x|\theta)\,dx = 1$</td>
        <td>$\theta$</td>
        <td>$\mathbb{E}_\theta[\nabla_\theta \log f] = 0$</td>
        <td>
          <div class="app-tags">
            <span class="app-tag">MLE</span>
            <span class="app-tag">Cram&eacute;r&ndash;Rao</span>
            <span class="app-tag">Fisher info</span>
            <span class="app-tag">Natural gradient</span>
          </div>
        </td>
      </tr>
      <tr>
        <td><span class="case-dot dot-partition"></span>Log-partition</td>
        <td>$e^{A(\eta)} = \int h\,e^{\eta^\top T}\,dx$</td>
        <td>$\eta$</td>
        <td>$\nabla A = \mathbb{E}[T]$,&ensp;$\nabla^2 A = \mathrm{Var}(T)$</td>
        <td>
          <div class="app-tags">
            <span class="app-tag">Exp. families</span>
            <span class="app-tag">Convexity</span>
            <span class="app-tag">Moment generation</span>
          </div>
        </td>
      </tr>
      <tr>
        <td><span class="case-dot dot-reinforce"></span>REINFORCE</td>
        <td>$V = \int f(x)\,p(x|\theta)\,dx$</td>
        <td>$\theta$</td>
        <td>$\nabla_\theta V = \mathbb{E}[f\cdot\nabla_\theta\!\log p]$</td>
        <td>
          <div class="app-tags">
            <span class="app-tag">Policy gradients</span>
            <span class="app-tag">RLHF</span>
            <span class="app-tag">Variational inference</span>
          </div>
        </td>
      </tr>
      <tr>
        <td><span class="case-dot dot-matching"></span>Score matching</td>
        <td>$\big[p_{\text{data}}\,\partial_{x_i}\!\log p_\theta\big]_{-\infty}^{\infty}\!=0$</td>
        <td>$x$</td>
        <td>cross term $= -\mathbb{E}[\nabla_x^2\!\log p_\theta]$</td>
        <td>
          <div class="app-tags">
            <span class="app-tag">Energy-based models</span>
            <span class="app-tag">Graphical models</span>
            <span class="app-tag">ERGMs</span>
          </div>
        </td>
      </tr>
    </tbody>
  </table>
</section>

<footer class="footer">
  STAT 418 &middot; Computational Methods in Data Science &middot; Purdue University
</footer>

</main>

<script>
// Scroll-triggered fade-in
const observer = new IntersectionObserver((entries) => {
  entries.forEach(e => {
    if (e.isIntersecting) {
      e.target.classList.add('visible');
    }
  });
}, { threshold: 0.08 });

document.querySelectorAll('.case-section, .summary-section').forEach(el => observer.observe(el));
</script>
</body>
</html>
