<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Complete guide to building an AI-powered pedagogical video segmentation toolkit for educational content">
    <title>Building an AI-Powered Video Segmentation Toolkit for Education</title>
    <style>
        :root {
            --purdue-gold: #CFB991;
            --purdue-black: #000000;
            --primary: #2563eb;
            --primary-dark: #1e40af;
            --secondary: #64748b;
            --bg: #ffffff;
            --bg-alt: #f8fafc;
            --text: #1e293b;
            --text-light: #475569;
            --border: #e2e8f0;
            --code-bg: #f1f5f9;
            --success: #10b981;
            --warning: #f59e0b;
            --danger: #ef4444;
            --info: #3b82f6;
            --sidebar-width: 300px;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            font-size: 17px;
            line-height: 1.7;
            color: var(--text);
            background: var(--bg);
        }

        /* Page Layout */
        .page-wrapper {
            display: flex;
            min-height: 100vh;
        }

        /* Sidebar Navigation */
        .sidebar {
            width: var(--sidebar-width);
            background: linear-gradient(135deg, var(--purdue-black) 0%, #1a1a1a 100%);
            position: fixed;
            height: 100vh;
            overflow-y: auto;
            padding: 0;
            z-index: 100;
        }

        .sidebar-header {
            background: var(--purdue-gold);
            padding: 24px 20px;
            text-align: center;
        }

        .sidebar-header h1 {
            font-size: 16px;
            font-weight: 700;
            color: var(--purdue-black);
            text-transform: uppercase;
            letter-spacing: 0.05em;
            margin-bottom: 4px;
        }

        .sidebar-header .subtitle {
            font-size: 12px;
            color: #333;
            font-weight: 500;
        }

        .sidebar nav {
            padding: 20px 0;
        }

        .sidebar nav ul {
            list-style: none;
        }

        .sidebar nav li {
            margin: 0;
        }

        .sidebar nav a {
            display: flex;
            align-items: center;
            color: #9ca3af;
            text-decoration: none;
            font-size: 14px;
            font-weight: 500;
            padding: 12px 24px;
            border-left: 3px solid transparent;
            transition: all 0.2s ease;
        }

        .sidebar nav a:hover {
            color: var(--purdue-gold);
            background: rgba(207, 185, 145, 0.1);
            border-left-color: var(--purdue-gold);
        }

        .sidebar nav a.active {
            color: var(--purdue-gold);
            background: rgba(207, 185, 145, 0.15);
            border-left-color: var(--purdue-gold);
        }

        .sidebar nav .section-number {
            display: inline-flex;
            align-items: center;
            justify-content: center;
            width: 24px;
            height: 24px;
            background: rgba(255,255,255,0.1);
            border-radius: 4px;
            font-size: 12px;
            font-weight: 600;
            margin-right: 12px;
            flex-shrink: 0;
        }

        /* Main Content */
        .main-content {
            flex: 1;
            margin-left: var(--sidebar-width);
            min-height: 100vh;
        }

        /* Header */
        .header {
            background: linear-gradient(135deg, var(--purdue-black) 0%, #1a1a1a 50%, var(--purdue-gold) 100%);
            color: white;
            padding: 60px 48px;
            position: relative;
            overflow: hidden;
        }

        .header::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: url("data:image/svg+xml,%3Csvg width='60' height='60' viewBox='0 0 60 60' xmlns='http://www.w3.org/2000/svg'%3E%3Cg fill='none' fill-rule='evenodd'%3E%3Cg fill='%23CFB991' fill-opacity='0.05'%3E%3Cpath d='M36 34v-4h-2v4h-4v2h4v4h2v-4h4v-2h-4zm0-30V0h-2v4h-4v2h4v4h2V6h4V4h-4zM6 34v-4H4v4H0v2h4v4h2v-4h4v-2H6zM6 4V0H4v4H0v2h4v4h2V6h4V4H6z'/%3E%3C/g%3E%3C/g%3E%3C/svg%3E");
            pointer-events: none;
        }

        .header-content {
            position: relative;
            z-index: 1;
            max-width: 900px;
        }

        .header h1 {
            font-size: 2.5rem;
            font-weight: 700;
            margin-bottom: 16px;
            line-height: 1.2;
        }

        .header .tagline {
            font-size: 1.25rem;
            opacity: 0.9;
            margin-bottom: 24px;
            max-width: 700px;
        }

        .header-stats {
            display: flex;
            gap: 32px;
            flex-wrap: wrap;
        }

        .stat {
            text-align: left;
        }

        .stat-value {
            font-size: 1.75rem;
            font-weight: 700;
            color: var(--purdue-gold);
        }

        .stat-label {
            font-size: 0.875rem;
            opacity: 0.8;
            text-transform: uppercase;
            letter-spacing: 0.05em;
        }

        /* Content Sections */
        .content {
            max-width: 900px;
            padding: 48px;
        }

        section {
            margin-bottom: 64px;
        }

        h2 {
            font-size: 1.875rem;
            font-weight: 700;
            color: var(--purdue-black);
            margin-bottom: 24px;
            padding-bottom: 12px;
            border-bottom: 3px solid var(--purdue-gold);
        }

        h3 {
            font-size: 1.375rem;
            font-weight: 600;
            color: var(--text);
            margin-top: 32px;
            margin-bottom: 16px;
        }

        h4 {
            font-size: 1.125rem;
            font-weight: 600;
            color: var(--text);
            margin-top: 24px;
            margin-bottom: 12px;
        }

        p {
            margin-bottom: 16px;
        }

        ul, ol {
            margin-bottom: 16px;
            padding-left: 24px;
        }

        li {
            margin-bottom: 8px;
        }

        /* Code Blocks */
        .code-block {
            background: var(--code-bg);
            border: 1px solid var(--border);
            border-radius: 8px;
            margin: 24px 0;
            overflow: hidden;
        }

        .code-header {
            background: #e2e8f0;
            padding: 10px 16px;
            font-size: 13px;
            font-weight: 600;
            color: var(--text-light);
            border-bottom: 1px solid var(--border);
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .code-header .language {
            background: var(--primary);
            color: white;
            padding: 2px 8px;
            border-radius: 4px;
            font-size: 11px;
            text-transform: uppercase;
        }

        pre {
            margin: 0;
            padding: 20px;
            overflow-x: auto;
            font-family: 'SF Mono', Monaco, 'Cascadia Code', 'Roboto Mono', Consolas, monospace;
            font-size: 14px;
            line-height: 1.6;
        }

        code {
            font-family: 'SF Mono', Monaco, 'Cascadia Code', 'Roboto Mono', Consolas, monospace;
            font-size: 0.9em;
        }

        p code, li code {
            background: var(--code-bg);
            padding: 2px 6px;
            border-radius: 4px;
            color: #c7254e;
        }

        /* Callout Boxes */
        .callout {
            padding: 20px 24px;
            border-radius: 8px;
            margin: 24px 0;
            border-left: 4px solid;
        }

        .callout-title {
            font-weight: 600;
            margin-bottom: 8px;
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .callout.success {
            background: #ecfdf5;
            border-color: var(--success);
        }
        .callout.success .callout-title { color: #065f46; }

        .callout.warning {
            background: #fffbeb;
            border-color: var(--warning);
        }
        .callout.warning .callout-title { color: #92400e; }

        .callout.danger {
            background: #fef2f2;
            border-color: var(--danger);
        }
        .callout.danger .callout-title { color: #991b1b; }

        .callout.info {
            background: #eff6ff;
            border-color: var(--info);
        }
        .callout.info .callout-title { color: #1e40af; }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 24px 0;
            font-size: 0.95rem;
        }

        th, td {
            padding: 12px 16px;
            text-align: left;
            border: 1px solid var(--border);
        }

        th {
            background: var(--bg-alt);
            font-weight: 600;
            color: var(--text);
        }

        tr:hover {
            background: var(--bg-alt);
        }

        /* Feature Cards */
        .feature-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 20px;
            margin: 24px 0;
        }

        .feature-card {
            background: var(--bg-alt);
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 24px;
            transition: transform 0.2s, box-shadow 0.2s;
        }

        .feature-card:hover {
            transform: translateY(-2px);
            box-shadow: 0 8px 25px rgba(0,0,0,0.1);
        }

        .feature-card h4 {
            margin-top: 0;
            color: var(--purdue-black);
        }

        .feature-card p {
            color: var(--text-light);
            font-size: 0.95rem;
            margin-bottom: 0;
        }

        /* Architecture Diagram */
        .architecture-diagram {
            background: var(--bg-alt);
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 32px;
            margin: 24px 0;
            font-family: 'SF Mono', Monaco, monospace;
            font-size: 13px;
            line-height: 1.4;
            overflow-x: auto;
            white-space: pre;
        }

        /* Footer */
        footer {
            background: var(--purdue-black);
            color: white;
            padding: 32px 48px;
            text-align: center;
        }

        footer a {
            color: var(--purdue-gold);
            text-decoration: none;
        }

        footer a:hover {
            text-decoration: underline;
        }

        /* Responsive */
        @media (max-width: 1024px) {
            .sidebar {
                transform: translateX(-100%);
                transition: transform 0.3s ease;
            }

            .sidebar.open {
                transform: translateX(0);
            }

            .main-content {
                margin-left: 0;
            }

            .header {
                padding: 40px 24px;
            }

            .content {
                padding: 32px 24px;
            }

            .mobile-toggle {
                display: block;
                position: fixed;
                top: 16px;
                left: 16px;
                z-index: 200;
                background: var(--purdue-black);
                color: var(--purdue-gold);
                border: none;
                padding: 12px;
                border-radius: 8px;
                cursor: pointer;
            }
        }

        @media (min-width: 1025px) {
            .mobile-toggle {
                display: none;
            }
        }

        /* Print Styles */
        @media print {
            .sidebar, .mobile-toggle {
                display: none;
            }
            .main-content {
                margin-left: 0;
            }
            .header {
                background: white;
                color: black;
            }
            .header h1 {
                color: var(--purdue-black);
            }
        }

        /* Workflow Steps */
        .workflow-steps {
            counter-reset: step;
            list-style: none;
            padding-left: 0;
        }

        .workflow-steps li {
            counter-increment: step;
            padding-left: 48px;
            position: relative;
            margin-bottom: 24px;
        }

        .workflow-steps li::before {
            content: counter(step);
            position: absolute;
            left: 0;
            top: 0;
            width: 32px;
            height: 32px;
            background: var(--purdue-gold);
            color: var(--purdue-black);
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: 700;
            font-size: 14px;
        }

        /* File Tree */
        .file-tree {
            background: var(--code-bg);
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 20px 24px;
            font-family: 'SF Mono', Monaco, monospace;
            font-size: 14px;
            line-height: 1.8;
        }

        .file-tree .folder {
            color: var(--primary);
            font-weight: 600;
        }

        .file-tree .file {
            color: var(--text-light);
        }

        .file-tree .comment {
            color: #9ca3af;
            font-style: italic;
        }
    </style>
</head>
<body>
    <button class="mobile-toggle" onclick="document.querySelector('.sidebar').classList.toggle('open')">
        â˜° Menu
    </button>

    <div class="page-wrapper">
        <!-- Sidebar Navigation -->
        <aside class="sidebar">
            <div class="sidebar-header">
                <h1>Video Segmentation</h1>
                <div class="subtitle">AI-Powered Educational Toolkit</div>
            </div>
            <nav>
                <ul>
                    <li><a href="#overview"><span class="section-number">1</span>Overview</a></li>
                    <li><a href="#architecture"><span class="section-number">2</span>Architecture</a></li>
                    <li><a href="#prerequisites"><span class="section-number">3</span>Prerequisites</a></li>
                    <li><a href="#schema"><span class="section-number">4</span>Schema Design</a></li>
                    <li><a href="#segmentation"><span class="section-number">5</span>Core Segmentation</a></li>
                    <li><a href="#prompts"><span class="section-number">6</span>LLM Prompting</a></li>
                    <li><a href="#youtube"><span class="section-number">7</span>YouTube Mapping</a></li>
                    <li><a href="#viewer"><span class="section-number">8</span>Video Viewer</a></li>
                    <li><a href="#annotation"><span class="section-number">9</span>Manual Annotation</a></li>
                    <li><a href="#utilities"><span class="section-number">10</span>Utility Scripts</a></li>
                    <li><a href="#deployment"><span class="section-number">11</span>Deployment</a></li>
                    <li><a href="#extensions"><span class="section-number">12</span>Extensions</a></li>
                </ul>
            </nav>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <header class="header">
                <div class="header-content">
                    <h1>Building an AI-Powered Video Segmentation Toolkit</h1>
                    <p class="tagline">Transform lecture recordings into structured, navigable educational content using LLMs, Python, and modern web technologies.</p>
                    <div class="header-stats">
                        <div class="stat">
                            <div class="stat-value">10+</div>
                            <div class="stat-label">Python Scripts</div>
                        </div>
                        <div class="stat">
                            <div class="stat-value">3,000+</div>
                            <div class="stat-label">Lines of Code</div>
                        </div>
                        <div class="stat">
                            <div class="stat-value">60 min</div>
                            <div class="stat-label">Build Time</div>
                        </div>
                        <div class="stat">
                            <div class="stat-value">âˆ</div>
                            <div class="stat-label">Lectures Processed</div>
                        </div>
                    </div>
                </div>
            </header>

            <div class="content">
                <!-- Section 1: Overview -->
                <section id="overview">
                    <h2>1. Overview</h2>
                    
                    <p>This tutorial guides you through building a complete toolkit for transforming lecture video transcripts into AI-analyzed pedagogical segments. The system processes SRT subtitle files, uses large language models to identify educational boundaries, maps content to YouTube uploads, and generates interactive viewing experiences for students.</p>

                    <h3>What You'll Build</h3>
                    <div class="feature-grid">
                        <div class="feature-card">
                            <h4>ğŸ¯ AI Segmentation Engine</h4>
                            <p>Process transcripts through LLMs to identify introductions, concept explanations, examples, and summaries with pedagogical metadata.</p>
                        </div>
                        <div class="feature-card">
                            <h4>ğŸ“Š Structured JSON Output</h4>
                            <p>Generate validated JSON with learning objectives, prerequisites, difficulty levels, and engagement tips for each segment.</p>
                        </div>
                        <div class="feature-card">
                            <h4>ğŸ¬ YouTube Integration</h4>
                            <p>Automatically map segmentation data to YouTube video IDs for embedded playback with timestamp navigation.</p>
                        </div>
                        <div class="feature-card">
                            <h4>ğŸ–¥ï¸ Interactive Viewer</h4>
                            <p>Single-file HTML viewer with role-based content, search functionality, and theater mode for immersive learning.</p>
                        </div>
                    </div>

                    <h3>Design Principles</h3>
                    <ul>
                        <li><strong>Bounded Expertise Oracle:</strong> AI analysis anchored to transcript content with mandatory timecode citations</li>
                        <li><strong>Graceful Degradation:</strong> Token truncation with clear markers when transcripts exceed limits</li>
                        <li><strong>Human-in-the-Loop:</strong> Flask-based annotation tool for timestamp corrections</li>
                        <li><strong>Schema Enforcement:</strong> Pydantic validation ensures reliable JSON parsing</li>
                        <li><strong>Idempotent Operations:</strong> Backup files before any mutation</li>
                    </ul>

                    <div class="callout info">
                        <div class="callout-title">ğŸ’¡ Educational Use Case</div>
                        <p>This toolkit was developed for STAT 350 at Purdue University, processing 70+ lecture videos into a searchable, navigable learning platform. The same architecture applies to any course with video content.</p>
                    </div>
                </section>

                <!-- Section 2: Architecture -->
                <section id="architecture">
                    <h2>2. System Architecture</h2>

                    <p>The toolkit follows a pipeline architecture where each component produces artifacts consumed by downstream processes. This design enables incremental processing and easy debugging.</p>

                    <div class="architecture-diagram">
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SRT Transcripts â”‚â”€â”€â”€â”€â–¶â”‚ srt_pedagogical_     â”‚â”€â”€â”€â”€â–¶â”‚ *_segments.json     â”‚
â”‚  (lecture videos)â”‚     â”‚ segmentation.py      â”‚     â”‚ (AI-analyzed)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ + estimate_limit.py  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
                                                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ YouTube upload  â”‚â”€â”€â”€â”€â–¶â”‚ create_youtube_      â”‚â”€â”€â”€â”€â–¶â”‚ youtube_mapping.jsonâ”‚
â”‚ log (CSV)       â”‚     â”‚ mapping_smart.py     â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
                                                                â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ lecture_segment_     â”‚â—€â”€â”€â”€â–¶â”‚ Corrected segments  â”‚
                        â”‚ annotator.py (Flask) â”‚     â”‚ (human-verified)    â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â–¼
â”‚ fix_*.py        â”‚â”€â”€â”€â”€â–¶â”‚ Clean, repair,       â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ merge_*.py      â”‚     â”‚ synchronize JSONs    â”‚     â”‚ video_viewer.html   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ (student interface) â”‚
                                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    </div>

                    <h3>Component Responsibilities</h3>
                    <table>
                        <thead>
                            <tr>
                                <th>Component</th>
                                <th>Input</th>
                                <th>Output</th>
                                <th>Purpose</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><code>srt_pedagogical_segmentation.py</code></td>
                                <td>SRT files</td>
                                <td>JSON segments</td>
                                <td>Core AI analysis engine</td>
                            </tr>
                            <tr>
                                <td><code>segmentation_schema.py</code></td>
                                <td>JSON data</td>
                                <td>Validated objects</td>
                                <td>Schema enforcement</td>
                            </tr>
                            <tr>
                                <td><code>estimate_limit.py</code></td>
                                <td>SRT file</td>
                                <td>Token count</td>
                                <td>Context window sizing</td>
                            </tr>
                            <tr>
                                <td><code>create_youtube_mapping_smart.py</code></td>
                                <td>Upload log + JSON</td>
                                <td>Mapping JSON</td>
                                <td>Video-to-lecture matching</td>
                            </tr>
                            <tr>
                                <td><code>generate_segmentation_report_youtube.py</code></td>
                                <td>JSON + mapping</td>
                                <td>HTML viewer</td>
                                <td>Interactive interface</td>
                            </tr>
                            <tr>
                                <td><code>lecture_segment_annotator.py</code></td>
                                <td>JSON segments</td>
                                <td>Corrected JSON</td>
                                <td>Manual timestamp refinement</td>
                            </tr>
                        </tbody>
                    </table>
                </section>

                <!-- Section 3: Prerequisites -->
                <section id="prerequisites">
                    <h2>3. Prerequisites</h2>

                    <h3>Required Software</h3>
                    <div class="code-block">
                        <div class="code-header">
                            <span>requirements.txt</span>
                            <span class="language">Text</span>
                        </div>
                        <pre># Core dependencies
pandas>=1.5.0
pydantic>=2.0.0
tiktoken>=0.5.0

# LLM providers (install one or both)
openai>=1.0.0
anthropic>=0.18.0

# Web application (for annotation tool)
flask>=3.0.0
flask-cors>=4.0.0

# Optional: video probing
# ffprobe (from ffmpeg) - install via system package manager</pre>
                    </div>

                    <h3>API Keys</h3>
                    <p>Configure environment variables for your chosen LLM provider:</p>
                    <div class="code-block">
                        <div class="code-header">
                            <span>.env</span>
                            <span class="language">Environment</span>
                        </div>
                        <pre># OpenAI (for GPT-4o, o1, o3 models)
OPENAI_API_KEY=sk-your-key-here

# Anthropic (for Claude models)
ANTHROPIC_API_KEY=sk-ant-your-key-here

# Or use a local/custom endpoint
OPENAI_BASE_URL=https://your-endpoint.com/v1</pre>
                    </div>

                    <h3>Project Structure</h3>
                    <div class="file-tree">
<span class="folder">video-segmentation-toolkit/</span>
â”œâ”€â”€ <span class="folder">scripts/</span>
â”‚   â”œâ”€â”€ <span class="file">srt_pedagogical_segmentation.py</span>  <span class="comment"># Core engine</span>
â”‚   â”œâ”€â”€ <span class="file">segmentation_schema.py</span>          <span class="comment"># Pydantic models</span>
â”‚   â”œâ”€â”€ <span class="file">estimate_limit.py</span>               <span class="comment"># Token calculator</span>
â”‚   â”œâ”€â”€ <span class="file">create_youtube_mapping_smart.py</span>  <span class="comment"># Video matcher</span>
â”‚   â”œâ”€â”€ <span class="file">generate_segmentation_report_youtube.py</span>
â”‚   â”œâ”€â”€ <span class="file">lecture_segment_annotator.py</span>    <span class="comment"># Flask app</span>
â”‚   â””â”€â”€ <span class="folder">utilities/</span>
â”‚       â”œâ”€â”€ <span class="file">fix_json_line_breaks.py</span>
â”‚       â”œâ”€â”€ <span class="file">fix_lecture_indices.py</span>
â”‚       â”œâ”€â”€ <span class="file">merge_split_items.py</span>
â”‚       â””â”€â”€ <span class="file">rebuild_course_context.py</span>
â”œâ”€â”€ <span class="folder">data/</span>
â”‚   â”œâ”€â”€ <span class="folder">transcripts/</span>                    <span class="comment"># Input SRT files</span>
â”‚   â””â”€â”€ <span class="file">lecture_mapping.xlsx</span>            <span class="comment"># Video-transcript pairs</span>
â”œâ”€â”€ <span class="folder">output/</span>
â”‚   â”œâ”€â”€ <span class="folder">json/</span>                           <span class="comment"># Segment JSON files</span>
â”‚   â”œâ”€â”€ <span class="folder">corrected/</span>                      <span class="comment"># Human-verified JSONs</span>
â”‚   â””â”€â”€ <span class="file">youtube_mapping.json</span>
â”œâ”€â”€ <span class="folder">templates/</span>
â”‚   â””â”€â”€ <span class="file">segment_annotate.html</span>           <span class="comment"># Flask template</span>
â”œâ”€â”€ <span class="file">requirements.txt</span>
â””â”€â”€ <span class="file">.env</span>
                    </div>
                </section>

                <!-- Section 4: Schema Design -->
                <section id="schema">
                    <h2>4. Schema Design</h2>

                    <p>Reliable LLM output parsing requires strict schema enforcement. We use Pydantic models to validate AI-generated JSON and provide clear error messages when output doesn't conform.</p>

                    <h3>Segment Types</h3>
                    <p>The toolkit recognizes ten pedagogical segment types, each with distinct visual styling and semantic meaning:</p>

                    <table>
                        <thead>
                            <tr>
                                <th>Type</th>
                                <th>Icon</th>
                                <th>Description</th>
                                <th>Color</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr><td><code>introduction</code></td><td>ğŸ¯</td><td>Topic introduction and learning objectives</td><td>#2E86DE</td></tr>
                            <tr><td><code>concept_explanation</code></td><td>ğŸ’¡</td><td>Core concept explanation and theory</td><td>#5F27CD</td></tr>
                            <tr><td><code>example</code></td><td>ğŸ“Š</td><td>Worked examples and demonstrations</td><td>#00B894</td></tr>
                            <tr><td><code>deep_reasoning</code></td><td>ğŸ§ </td><td>Deep reasoning and intuition building</td><td>#D63031</td></tr>
                            <tr><td><code>common_mistakes</code></td><td>âš ï¸</td><td>Common mistakes and misconceptions</td><td>#E17055</td></tr>
                            <tr><td><code>practice_problem</code></td><td>âœï¸</td><td>Practice problems and exercises</td><td>#00CEC9</td></tr>
                            <tr><td><code>real_world_application</code></td><td>ğŸŒ</td><td>Real-world applications and context</td><td>#A29BFE</td></tr>
                            <tr><td><code>summary</code></td><td>ğŸ“</td><td>Summary and key takeaways</td><td>#6C5CE7</td></tr>
                            <tr><td><code>q_and_a</code></td><td>â“</td><td>Student questions and answers</td><td>#00B894</td></tr>
                            <tr><td><code>transition</code></td><td>â¡ï¸</td><td>Topic transitions and administrative content</td><td>#636E72</td></tr>
                        </tbody>
                    </table>

                    <h3>Pydantic Models</h3>
                    <div class="code-block">
                        <div class="code-header">
                            <span>segmentation_schema.py</span>
                            <span class="language">Python</span>
                        </div>
                        <pre>from typing import Dict, List, Optional
from pydantic import BaseModel, Field, validator
from enum import Enum

class SegmentType(str, Enum):
    """Valid segment types."""
    INTRODUCTION = "introduction"
    CONCEPT_EXPLANATION = "concept_explanation"
    EXAMPLE = "example"
    DEEP_REASONING = "deep_reasoning"
    COMMON_MISTAKES = "common_mistakes"
    PRACTICE_PROBLEM = "practice_problem"
    REAL_WORLD_APPLICATION = "real_world_application"
    SUMMARY = "summary"
    Q_AND_A = "q_and_a"
    TRANSITION = "transition"

class DifficultyLevel(str, Enum):
    """Valid difficulty levels."""
    EASY = "Easy"
    MEDIUM = "Medium"
    HARD = "Hard"

class TimeRange(BaseModel):
    """Represents a time range in HH:MM:SS,mmm format."""
    start: str = Field(..., pattern=r'^\d{2}:\d{2}:\d{2}[,;]\d{2,3}$')
    end: str = Field(..., pattern=r'^\d{2}:\d{2}:\d{2}[,;]\d{2,3}$')

class SegmentSchema(BaseModel):
    """Schema for a single segment."""
    time_range: TimeRange
    segment_type: SegmentType
    title: str = Field(..., min_length=1, max_length=200)
    description: str = Field(..., min_length=1, max_length=500)
    key_concepts: List[str] = Field(default_factory=list, max_items=10)
    learning_objectives: List[str] = Field(default_factory=list, max_items=5)
    prerequisites: List[str] = Field(default_factory=list, max_items=5)
    difficulty: DifficultyLevel = Field(default=DifficultyLevel.MEDIUM)
    engagement_tips: List[str] = Field(default_factory=list, max_items=5)
    microlecture_suitable: bool = Field(default=False)
    
    class Config:
        use_enum_values = True

class SegmentationAnalysis(BaseModel):
    """Complete schema for segmentation analysis output."""
    overview: LectureOverview
    segments: List[SegmentSchema] = Field(..., min_items=1)
    interactive_opportunities: List[InteractiveOpportunity] = Field(default_factory=list)
    microlecture_recommendations: List[MicrolectureRecommendation] = Field(default_factory=list)</pre>
                    </div>

                    <div class="callout warning">
                        <div class="callout-title">âš ï¸ Validation Strategy</div>
                        <p>Always validate LLM output before storing. Use <code>validate_analysis(data)</code> to catch malformed responses early. LLMs occasionally produce invalid JSON, especially with complex schemas.</p>
                    </div>
                </section>

                <!-- Section 5: Core Segmentation -->
                <section id="segmentation">
                    <h2>5. Core Segmentation Engine</h2>

                    <p>The segmentation engine processes SRT transcripts through several stages: parsing, enhancement, LLM analysis, and output generation.</p>

                    <h3>Timestamp Handling</h3>
                    <p>SRT files use millisecond timestamps (<code>HH:MM:SS,mmm</code>), while video editing uses frame-based timecodes. The toolkit handles both formats:</p>

                    <div class="code-block">
                        <div class="code-header">
                            <span>Timecode Conversion</span>
                            <span class="language">Python</span>
                        </div>
                        <pre>def tc_to_seconds(tc: str, fps: float = 30.0) -> float:
    """Convert timecode to seconds - handles all formats."""
    try:
        if ',' in tc:  # Millisecond format: HH:MM:SS,mmm
            h, m, rest = tc.split(':')
            s, ms = rest.split(',')
            return int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000
        else:  # Frame format: HH:MM:SS:FF or HH:MM:SS;FF
            parts = re.split(r'[:;]', tc)
            if len(parts) == 4:
                h, m, s, ff = parts
                return int(h) * 3600 + int(m) * 60 + int(s) + int(ff) / fps
            else:
                h, m, s = parts
                return int(h) * 3600 + int(m) * 60 + int(s)
    except Exception as e:
        logger.error(f"Error parsing timecode '{tc}': {e}")
        return 0.0

def seconds_to_tc(sec: float, fps: float = 30.0, mode: str = "ms") -> str:
    """Convert seconds to timecode string."""
    h, rem = divmod(sec, 3600)
    m, s_full = divmod(rem, 60)
    s = int(s_full)
    
    if mode == "frames":
        frames = int(round((s_full - s) * fps))
        return f"{int(h):02d}:{int(m):02d}:{int(s):02d}:{frames:02d}"
    else:  # milliseconds
        ms = int((sec - int(sec)) * 1000)
        return f"{int(h):02d}:{int(m):02d}:{int(s):02d},{ms:03d}"</pre>
                    </div>

                    <h3>Transcript Enhancement</h3>
                    <p>Before sending to the LLM, we enhance transcripts with embedded timestamp markers. This anchors the AI to actual timecodes rather than hallucinating times:</p>

                    <div class="code-block">
                        <div class="code-header">
                            <span>Transcript Enhancement</span>
                            <span class="language">Python</span>
                        </div>
                        <pre>def enhance_transcript_with_timestamps(srt_text: str) -> Tuple[str, List]:
    """
    Add timestamp markers to transcript for AI reference.
    Returns: (enhanced_text, timestamp_mapping)
    """
    enhanced_lines = []
    timestamp_mapping = []
    
    # Split into caption blocks
    blocks = re.split(r'\n\n+', srt_text.strip())
    
    for block in blocks:
        lines = block.strip().split('\n')
        if len(lines) < 3:
            continue
        
        # Extract timestamp
        timestamp_match = TIMESTAMP_RE.search(lines[1])
        if timestamp_match:
            start_tc, end_tc = timestamp_match.groups()
            
            # Get caption text (lines after timestamp)
            text = ' '.join(lines[2:])
            
            # Add to enhanced transcript with timestamp marker
            enhanced_lines.append(f"[{start_tc}] {text}")
            timestamp_mapping.append((start_tc, end_tc, text))
    
    return '\n'.join(enhanced_lines), timestamp_mapping</pre>
                    </div>

                    <h3>Token Budget Management</h3>
                    <p>Long lectures may exceed model context windows. Use <code>estimate_limit.py</code> to calculate token requirements:</p>

                    <div class="code-block">
                        <div class="code-header">
                            <span>Usage</span>
                            <span class="language">Bash</span>
                        </div>
                        <pre>python estimate_limit.py --srt lecture_42.srt --model gpt-4o

# Output:
# Transcript characters : 45,230
# Exact tokens (gpt-4o) : 12,847
# ---------------------------------------------------------
# Recommended config.token_limit  : 19,764  (largest safe model window)
# Minimum-for-current heuristic   : 12,848  (fits transcript + 1-token reply)</pre>
                    </div>

                    <div class="callout info">
                        <div class="callout-title">ğŸ’¡ Token Budget Split</div>
                        <p>The toolkit uses a 65/25/10 heuristic: 65% of context for transcript, 25% for model output, 10% safety margin. If transcripts exceed limits, they're truncated with a clear <code>[TRANSCRIPT TRUNCATED]</code> marker.</p>
                    </div>
                </section>

                <!-- Section 6: LLM Prompting -->
                <section id="prompts">
                    <h2>6. LLM Prompting Strategy</h2>

                    <p>Effective prompting is critical for reliable segmentation. The toolkit uses a structured prompt that constrains the LLM to use actual transcript timestamps.</p>

                    <h3>Model Configuration</h3>
                    <div class="code-block">
                        <div class="code-header">
                            <span>Model Configurations</span>
                            <span class="language">Python</span>
                        </div>
                        <pre>MODEL_CONFIGS = {
    'gpt-4o': {
        'temperature': 0.1,      # Low temp for consistent output
        'max_tokens': 4000,
        'token_limit': 128000,
        'provider': 'openai'
    },
    'claude-sonnet-4': {
        'temperature': 0.1,
        'max_tokens': 4000,
        'token_limit': 200000,
        'provider': 'claude'
    },
    'o3': {
        'temperature': None,     # O-series ignores temperature
        'max_completion_tokens': 25000,
        'token_limit': 200000,
        'provider': 'openai',
        'reasoning_effort': 'high'
    }
}</pre>
                    </div>

                    <h3>System Prompt Structure</h3>
                    <p>The prompt follows a specific structure to maximize reliability:</p>

                    <div class="code-block">
                        <div class="code-header">
                            <span>Prompt Template (Abbreviated)</span>
                            <span class="language">Text</span>
                        </div>
                        <pre>You are an expert educational content analyst specializing in 
pedagogical segmentation of lecture videos.

## CRITICAL TIMESTAMP RULES
1. ONLY use timestamps that appear in [HH:MM:SS,mmm] markers in the transcript
2. Segment boundaries MUST align with actual caption timestamps
3. DO NOT interpolate or invent timestamps
4. If uncertain about a boundary, use the nearest visible timestamp

## Segment Types
- introduction: Topic setup and learning objectives
- concept_explanation: Core theory and definitions
- example: Worked problems and demonstrations
- deep_reasoning: Intuition building and "why" explanations
- common_mistakes: Pitfalls and misconceptions
- practice_problem: Student exercises
- real_world_application: Practical applications
- summary: Key takeaways and review
- q_and_a: Student questions
- transition: Topic changes and administrative content

## Output Format
Return valid JSON matching this schema:
{
  "overview": {
    "learning_objectives": ["..."],
    "prerequisites": ["..."],
    "key_takeaways": ["..."]
  },
  "segments": [
    {
      "time_range": {"start": "HH:MM:SS,mmm", "end": "HH:MM:SS,mmm"},
      "segment_type": "concept_explanation",
      "title": "Descriptive Title",
      "description": "1-2 sentence summary",
      "key_concepts": ["concept1", "concept2"],
      "difficulty": "Easy|Medium|Hard"
    }
  ]
}</pre>
                    </div>

                    <div class="callout danger">
                        <div class="callout-title">â›” Timestamp Hallucination</div>
                        <p>LLMs frequently hallucinate timestamps when not properly constrained. The embedded <code>[HH:MM:SS,mmm]</code> markers in enhanced transcripts and explicit instructions to only use visible timestamps significantly reduce this problem, but human verification remains essential.</p>
                    </div>
                </section>

                <!-- Section 7: YouTube Mapping -->
                <section id="youtube">
                    <h2>7. YouTube Video Mapping</h2>

                    <p>After uploading lectures to YouTube, you need to map video IDs to lecture indices. The <code>create_youtube_mapping_smart.py</code> script automates this using a two-pass matching algorithm.</p>

                    <h3>Input: YouTube Upload Log</h3>
                    <p>Most YouTube upload tools generate a CSV log. The script expects this format:</p>

                    <div class="code-block">
                        <div class="code-header">
                            <span>youtube_upload_log.csv</span>
                            <span class="language">CSV</span>
                        </div>
                        <pre>Video File,YouTube Video ID,Status
"/path/to/STAT 350 - Chapter 6.3.1 Intro.mp4",dQw4w9WgXcQ,UPLOAD SUCCESS
"/path/to/STAT 350 - Chapter 6.3.2 Examples.mp4",9bZkp7q19f0,UPLOAD SUCCESS
"/path/to/STAT 350 - Chapter 7.1 Overview.mp4",kJQP7kiw5Fk,UPLOAD SUCCESS</pre>
                    </div>

                    <h3>Matching Algorithm</h3>
                    <ol class="workflow-steps">
                        <li>
                            <strong>Extract Chapter Information:</strong> Parse chapter numbers from filenames using regex patterns like <code>Chapter\s*(\d+(?:\.\d+)*)</code>.
                        </li>
                        <li>
                            <strong>Pass 1 - Chapter Group Matching:</strong> Group videos and lectures by base chapter (e.g., "6.3") and match in order. This handles sub-chapters like 6.3.1, 6.3.2.
                        </li>
                        <li>
                            <strong>Pass 2 - Fuzzy Title Matching:</strong> For remaining unmatched items, use <code>difflib.SequenceMatcher</code> to find best title matches above a similarity threshold (default 0.7).
                        </li>
                        <li>
                            <strong>Generate Mapping:</strong> Output <code>youtube_mapping.json</code> with lecture index â†’ video ID pairs.
                        </li>
                    </ol>

                    <div class="code-block">
                        <div class="code-header">
                            <span>Usage</span>
                            <span class="language">Bash</span>
                        </div>
                        <pre>python create_youtube_mapping_smart.py \
    --log-file youtube_upload_log.csv \
    --json-dir segmentation_reports/json \
    --output youtube_mapping.json \
    --threshold 0.7

# Output files:
# - youtube_mapping.json           (primary mapping)
# - youtube_mapping_detailed.csv   (for human review)
# - unmatched_videos.csv           (failed matches)</pre>
                    </div>

                    <h3>Output Format</h3>
                    <div class="code-block">
                        <div class="code-header">
                            <span>youtube_mapping.json</span>
                            <span class="language">JSON</span>
                        </div>
                        <pre>{
  "1": "dQw4w9WgXcQ",
  "2": "9bZkp7q19f0",
  "3": "kJQP7kiw5Fk",
  "38": "abc123xyz",
  "39": "def456uvw"
}</pre>
                    </div>
                </section>

                <!-- Section 8: Video Viewer -->
                <section id="viewer">
                    <h2>8. Interactive Video Viewer</h2>

                    <p>The HTML viewer is a self-contained single-file application that renders all segmentation data with embedded YouTube playback.</p>

                    <h3>Key Features</h3>
                    <div class="feature-grid">
                        <div class="feature-card">
                            <h4>ğŸ­ Role-Based Views</h4>
                            <p>Students see simplified content; instructors see engagement tips, difficulty badges, and analytics.</p>
                        </div>
                        <div class="feature-card">
                            <h4>ğŸ” Full-Text Search</h4>
                            <p>Search across all lectures by title, concept, description, or learning objective.</p>
                        </div>
                        <div class="feature-card">
                            <h4>ğŸ¬ Theater Mode</h4>
                            <p>Immersive full-width video playback with hidden sidebar.</p>
                        </div>
                        <div class="feature-card">
                            <h4>ğŸ“Š Visual Timeline</h4>
                            <p>Color-coded segment bars showing lecture structure at a glance.</p>
                        </div>
                    </div>

                    <h3>Generation Command</h3>
                    <div class="code-block">
                        <div class="code-header">
                            <span>Generate Viewer</span>
                            <span class="language">Bash</span>
                        </div>
                        <pre>python generate_segmentation_report_youtube.py \
    --json-dir segmentation_reports/json \
    --youtube-mapping youtube_mapping.json \
    --out video_viewer.html</pre>
                    </div>

                    <h3>Data Embedding</h3>
                    <p>Segment data is embedded directly in the HTML as JavaScript objects:</p>

                    <div class="code-block">
                        <div class="code-header">
                            <span>Embedded Data Structure</span>
                            <span class="language">JavaScript</span>
                        </div>
                        <pre>// Embedded in generated HTML
window.segmentData = {
    "1": {
        "lecture_index": 1,
        "lecture_title": "Chapter 1.1: Introduction to Statistics",
        "total_duration": 1847.5,
        "segments": [
            {
                "start_time": 0,
                "end_time": 185.2,
                "start_tc": "00:00:00,000",
                "end_tc": "00:03:05,200",
                "segment_type": "introduction",
                "title": "Course Overview",
                "description": "Introduction to the course...",
                "key_concepts": ["statistics", "data analysis"],
                "difficulty_level": "Easy"
            }
            // ... more segments
        ]
    }
};

const YOUTUBE_MAPPING = {
    "1": "dQw4w9WgXcQ",
    "2": "9bZkp7q19f0"
};</pre>
                    </div>

                    <div class="callout success">
                        <div class="callout-title">âœ… Single-File Deployment</div>
                        <p>The viewer is entirely self-containedâ€”no server required. Host it on GitHub Pages, drop it in an LMS, or open directly in a browser. All styles, scripts, and data are inline.</p>
                    </div>
                </section>

                <!-- Section 9: Manual Annotation -->
                <section id="annotation">
                    <h2>9. Manual Annotation Tool</h2>

                    <p>AI-generated timestamps often need refinement. The Flask-based annotation tool provides a side-by-side interface for human correction.</p>

                    <h3>Starting the Server</h3>
                    <div class="code-block">
                        <div class="code-header">
                            <span>Launch Annotation Tool</span>
                            <span class="language">Bash</span>
                        </div>
                        <pre># Start the Flask server
python lecture_segment_annotator.py

# Server runs at http://localhost:5005
# Open in browser to begin annotation</pre>
                    </div>

                    <h3>Workflow</h3>
                    <ol class="workflow-steps">
                        <li>
                            <strong>Initialize:</strong> On first run, the tool copies all original JSONs to a <code>corrected/</code> directory.
                        </li>
                        <li>
                            <strong>Select Lecture:</strong> Browse available lectures in the sidebar. Videos with corrections show a badge.
                        </li>
                        <li>
                            <strong>Adjust Timestamps:</strong> Play the YouTube video and click on segments to adjust start/end times.
                        </li>
                        <li>
                            <strong>Save Corrections:</strong> Changes are saved to the corrected directory with metadata timestamps.
                        </li>
                        <li>
                            <strong>Export:</strong> Download all corrected segments as a ZIP for deployment.
                        </li>
                    </ol>

                    <h3>API Endpoints</h3>
                    <table>
                        <thead>
                            <tr>
                                <th>Endpoint</th>
                                <th>Method</th>
                                <th>Purpose</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><code>/api/lectures</code></td>
                                <td>GET</td>
                                <td>List all available lectures with correction status</td>
                            </tr>
                            <tr>
                                <td><code>/api/segments/&lt;filename&gt;</code></td>
                                <td>GET</td>
                                <td>Get segments for a specific lecture</td>
                            </tr>
                            <tr>
                                <td><code>/api/segments/save</code></td>
                                <td>POST</td>
                                <td>Save corrected timestamps</td>
                            </tr>
                            <tr>
                                <td><code>/api/reset/&lt;filename&gt;</code></td>
                                <td>POST</td>
                                <td>Reset to original timestamps</td>
                            </tr>
                            <tr>
                                <td><code>/api/export</code></td>
                                <td>GET</td>
                                <td>Download all segments as ZIP</td>
                            </tr>
                        </tbody>
                    </table>
                </section>

                <!-- Section 10: Utility Scripts -->
                <section id="utilities">
                    <h2>10. Utility Scripts</h2>

                    <p>LLM output often contains artifacts that need post-processing. These utilities clean and repair JSON files.</p>

                    <h3>fix_json_line_breaks.py</h3>
                    <p>Removes unwanted line breaks and HTML artifacts from text fields:</p>

                    <div class="code-block">
                        <div class="code-header">
                            <span>Usage</span>
                            <span class="language">Bash</span>
                        </div>
                        <pre>python fix_json_line_breaks.py --json-dir segmentation_reports/json

# Fixes:
# - Removes \n, \r, \t sequences
# - Cleans HTML entities (&lt;br/&gt;)
# - Normalizes whitespace
# - Creates .bak backups</pre>
                    </div>

                    <h3>merge_split_items.py</h3>
                    <p>LLMs sometimes split list items incorrectly. This script merges fragments:</p>

                    <div class="code-block">
                        <div class="code-header">
                            <span>Before/After</span>
                            <span class="language">JSON</span>
                        </div>
                        <pre>// Before (incorrectly split)
"prerequisites": [
    "High",
    "school algebra",
    "Basic probability"
]

// After (merged)
"prerequisites": [
    "High-school algebra",
    "Basic probability"
]</pre>
                    </div>

                    <h3>fix_lecture_indices.py</h3>
                    <p>Ensures <code>lecture_index</code> fields match filename prefixes:</p>

                    <div class="code-block">
                        <div class="code-header">
                            <span>Usage</span>
                            <span class="language">Bash</span>
                        </div>
                        <pre># Preview changes
python fix_lecture_indices.py --json-dir output/json --dry-run

# Apply changes
python fix_lecture_indices.py --json-dir output/json

# File: 038_STAT_350_Chapter_7_segments.json
# Before: lecture_index: 1
# After:  lecture_index: 38</pre>
                    </div>

                    <h3>rebuild_course_context.py</h3>
                    <p>After reprocessing individual lectures, rebuild the cross-lecture concept graph:</p>

                    <div class="code-block">
                        <div class="code-header">
                            <span>Usage</span>
                            <span class="language">Bash</span>
                        </div>
                        <pre>python rebuild_course_context.py \
    --input-dir segmentation_analysis \
    --verify \
    --enhanced-summary

# Outputs:
# - course_context.json (concept tracking)
# - course_summary.md (human-readable overview)</pre>
                    </div>
                </section>

                <!-- Section 11: Deployment -->
                <section id="deployment">
                    <h2>11. Deployment</h2>

                    <h3>Static Hosting (GitHub Pages)</h3>
                    <p>The viewer is a single HTML file that can be hosted anywhere:</p>

                    <div class="code-block">
                        <div class="code-header">
                            <span>GitHub Pages Deployment</span>
                            <span class="language">Bash</span>
                        </div>
                        <pre># Create a docs folder for GitHub Pages
mkdir -p docs
cp video_viewer.html docs/index.html

# Commit and push
git add docs/
git commit -m "Deploy video viewer"
git push

# Enable GitHub Pages in repository settings
# Source: Deploy from branch â†’ main â†’ /docs</pre>
                    </div>

                    <h3>LMS Integration</h3>
                    <p>For Brightspace, Canvas, or Blackboard:</p>
                    <ol>
                        <li>Upload <code>video_viewer.html</code> to course files</li>
                        <li>Create a content item linking to the uploaded file</li>
                        <li>Or embed in an iframe if your LMS allows</li>
                    </ol>

                    <div class="callout warning">
                        <div class="callout-title">âš ï¸ YouTube API Restrictions</div>
                        <p>The YouTube IFrame API requires the page to be served over HTTP/HTTPS. Opening the HTML file directly (<code>file://</code>) may not load videos. Use a local server for development:</p>
                    </div>

                    <div class="code-block">
                        <div class="code-header">
                            <span>Local Development Server</span>
                            <span class="language">Bash</span>
                        </div>
                        <pre># Python 3
python -m http.server 8000

# Then open http://localhost:8000/video_viewer.html</pre>
                    </div>
                </section>

                <!-- Section 12: Extensions -->
                <section id="extensions">
                    <h2>12. Extensions & Future Work</h2>

                    <h3>Potential Enhancements</h3>
                    <div class="feature-grid">
                        <div class="feature-card">
                            <h4>ğŸ”— Chatbot Integration</h4>
                            <p>Connect segmentation data to an AI tutor that can answer questions about specific video segments.</p>
                        </div>
                        <div class="feature-card">
                            <h4>ğŸ“ˆ Learning Analytics</h4>
                            <p>Track which segments students watch, rewind, or skip to identify difficult content.</p>
                        </div>
                        <div class="feature-card">
                            <h4>ğŸ¬ Auto-Clip Generation</h4>
                            <p>Automatically extract "microlecture" clips based on segment boundaries using ffmpeg.</p>
                        </div>
                        <div class="feature-card">
                            <h4>ğŸ“ Quiz Generation</h4>
                            <p>Use LLMs to generate comprehension questions for each segment.</p>
                        </div>
                    </div>

                    <h3>EDL Export for Video Editing</h3>
                    <p>The toolkit can export Edit Decision Lists for Adobe Premiere Pro:</p>

                    <div class="code-block">
                        <div class="code-header">
                            <span>EDL Export</span>
                            <span class="language">Python</span>
                        </div>
                        <pre>def export_segmentation_edl(segmentation, output_path, video_title, fps):
    """Export segmentation as EDL file for Adobe Premiere Pro."""
    with open(output_path, 'w') as f:
        f.write(f"TITLE: {video_title}_segments\n")
        f.write("FCM: NON-DROP FRAME\n\n")
        
        for i, segment in enumerate(segmentation.segments, 1):
            start_tc = seconds_to_tc(segment.start_time, fps, 'frames')
            end_tc = seconds_to_tc(segment.end_time, fps, 'frames')
            
            f.write(f"{i:03d}  AX       V     C        ")
            f.write(f"{start_tc} {end_tc} {start_tc} {end_tc}\n")
            f.write(f"* COMMENT: TYPE={segment.segment_type} | {segment.title}\n\n")</pre>
                    </div>

                    <h3>Contributing</h3>
                    <p>This toolkit is designed to be modular and extensible. Key extension points include:</p>
                    <ul>
                        <li><strong>New segment types:</strong> Add entries to <code>SEGMENT_TYPES</code> dictionary</li>
                        <li><strong>Custom LLM providers:</strong> Implement provider adapters in segmentation engine</li>
                        <li><strong>Alternative viewers:</strong> Generate React, Vue, or native mobile apps from JSON</li>
                        <li><strong>Assessment integration:</strong> Connect to LMS gradebooks via LTI</li>
                    </ul>

                    <div class="callout success">
                        <div class="callout-title">ğŸ‰ Congratulations!</div>
                        <p>You now have a complete understanding of the pedagogical video segmentation toolkit. Start with a single lecture, validate the output, then scale to your full course library. The human-in-the-loop annotation ensures quality while AI handles the heavy lifting.</p>
                    </div>
                </section>
            </div>

            <footer>
                <p>Pedagogical Video Segmentation Toolkit Tutorial | Department of Statistics, Purdue University | 2025</p>
                <p style="margin-top: 8px; font-size: 0.875rem;">
                    <a href="https://github.com/treese41528/STAT350-LatticeAI" target="_blank">Example Project</a> â€¢ 
                    <a href="https://www.purdue.edu" target="_blank">Purdue University</a>
                </p>
            </footer>
        </main>
    </div>

    <script>
        // Smooth scrolling for navigation
        document.querySelectorAll('.sidebar nav a').forEach(link => {
            link.addEventListener('click', function(e) {
                e.preventDefault();
                const targetId = this.getAttribute('href').substring(1);
                const target = document.getElementById(targetId);
                if (target) {
                    target.scrollIntoView({ behavior: 'smooth', block: 'start' });
                    // Update active state
                    document.querySelectorAll('.sidebar nav a').forEach(a => a.classList.remove('active'));
                    this.classList.add('active');
                    // Close mobile menu
                    document.querySelector('.sidebar').classList.remove('open');
                }
            });
        });

        // Highlight current section on scroll
        const sections = document.querySelectorAll('section[id]');
        const navLinks = document.querySelectorAll('.sidebar nav a');

        function highlightNav() {
            const scrollPos = window.scrollY + 100;
            
            sections.forEach(section => {
                const top = section.offsetTop;
                const height = section.offsetHeight;
                const id = section.getAttribute('id');
                
                if (scrollPos >= top && scrollPos < top + height) {
                    navLinks.forEach(link => {
                        link.classList.remove('active');
                        if (link.getAttribute('href') === '#' + id) {
                            link.classList.add('active');
                        }
                    });
                }
            });
        }

        window.addEventListener('scroll', highlightNav);
        highlightNav(); // Initial call
    </script>
</body>
</html>