

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Lecture 02.1 – Image Formation &mdash; Fitting SMPL to IMU Optimization</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
    <link rel="canonical" href="https://treese41528.github.io/VirtualHumans/lecture_02_1_image_formation.html" />
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=f2a433a1"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Lecture 02.2 – Rotations and Kinematic Chains" href="lecture_02_2_rotations_kinematic_chains.html" />
    <link rel="prev" title="Lecture 01.3 – Introduction to Human Models (Overview)" href="lecture_01_3_introduction_to_human_models_continued.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Virtual Humans Lecture
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="lecture_01_1_historical_body_models.html">Lecture 01.1 – Historical Body Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#early-origins-simplified-primitives-and-kinematic-skeletons-1970s1980s">Early Origins: Simplified Primitives and Kinematic Skeletons (1970s–1980s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#advances-in-the-1990s-superquadrics-differentiable-fitting-and-physical-models">Advances in the 1990s: Superquadrics, Differentiable Fitting, and Physical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#the-impact-of-3d-scanning-and-data-from-anthropometry-to-statistical-models-1990s2000s">The Impact of 3D Scanning and Data: From Anthropometry to Statistical Models (1990s–2000s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#scape-and-the-emergence-of-pose-aware-models-mid-2000s">SCAPE and the Emergence of Pose-Aware Models (Mid-2000s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#consolidation-in-the-2010s-smpl-and-integration-with-learning-based-methods">Consolidation in the 2010s: SMPL and Integration with Learning-Based Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#deep-learning-and-neural-implicit-models-late-2010spresent">Deep Learning and Neural Implicit Models (Late 2010s–Present)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#timeline-summary-of-milestones">Timeline Summary of Milestones</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html">Lecture 01.2 – Introduction to Human Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#historical-context-of-human-body-modeling">1. Historical Context of Human Body Modeling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#early-developments">Early Developments</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#mid-20th-century-approaches">Mid-20th Century Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#data-driven-revolution-1990s-2000s">Data-Driven Revolution (1990s-2000s)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#mathematical-foundations-of-human-body-models">2. Mathematical Foundations of Human Body Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#the-smpl-model">The SMPL Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#pca-based-statistical-shape-modeling">PCA-Based Statistical Shape Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#kinematic-modeling">Kinematic Modeling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#applications-of-human-body-models">3. Applications of Human Body Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computer-animation-and-visual-effects">Computer Animation and Visual Effects</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#virtual-humans-and-avatars">Virtual Humans and Avatars</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#biomechanics-and-ergonomics">Biomechanics and Ergonomics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#human-computer-interaction-hci">Human-Computer Interaction (HCI)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computer-vision-and-ai">Computer Vision and AI</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#education-and-training">Education and Training</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#challenges-and-future-directions">4. Challenges and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computational-efficiency">Computational Efficiency</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#accuracy-and-detail">Accuracy and Detail</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#generalization">Generalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#clothing-and-accessories">Clothing and Accessories</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#emerging-approaches">Emerging Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html">Lecture 01.3 – Introduction to Human Models (Overview)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#historical-context">1. Historical Context</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#early-scientific-studies">Early Scientific Studies</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#mid-20th-century-to-digital-era">Mid-20th Century to Digital Era</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#st-century-advances">21st Century Advances</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#mathematical-foundations">2. Mathematical Foundations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#parametric-body-models">Parametric Body Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-surface-representations">Implicit Surface Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#kinematic-modeling">Kinematic Modeling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#image-formation-and-rendering">3. Image Formation and Rendering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#camera-models">Camera Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#shading-and-visibility">Shading and Visibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#differentiable-rendering">Differentiable Rendering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#surface-representation-methods">4. Surface Representation Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#explicit-mesh-models">Explicit Mesh Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-function-models">Implicit Function Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#motion-capture-and-behavior-synthesis">5. Motion Capture and Behavior Synthesis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#capturing-human-motion">Capturing Human Motion</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#behavior-synthesis">Behavior Synthesis</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#clothing-modeling">6. Clothing Modeling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physically-based-simulation">Physically-Based Simulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#data-driven-approaches">Data-Driven Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-clothing-models">Implicit Clothing Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#human-object-interaction">7. Human-Object Interaction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physics-based-methods">Physics-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#learning-based-approaches">Learning-Based Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#hybrid-systems">Hybrid Systems</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#applications">8. Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#entertainment-and-media">Entertainment and Media</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#healthcare-and-biomechanics">Healthcare and Biomechanics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#engineering-and-design">Engineering and Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#human-computer-interaction">Human-Computer Interaction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#scientific-research">Scientific Research</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#challenges-and-future-directions">9. Challenges and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#scalability-and-generalization">Scalability and Generalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#higher-fidelity-dynamics">Higher-Fidelity Dynamics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#data-and-labeling-constraints">Data and Labeling Constraints</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physics-and-learning-integration">Physics and Learning Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#semantic-and-cognitive-aspects">Semantic and Cognitive Aspects</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#realism-vs-controllability">Realism vs. Controllability</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Lecture 02.1 – Image Formation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#historical-developments-in-image-formation">1. Historical Developments in Image Formation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#ancient-and-medieval-optics-camera-obscura">Ancient and Medieval Optics – Camera Obscura</a></li>
<li class="toctree-l3"><a class="reference internal" href="#renaissance-perspective-and-geometry">Renaissance Perspective and Geometry</a></li>
<li class="toctree-l3"><a class="reference internal" href="#early-cameras-and-photographic-imaging">Early Cameras and Photographic Imaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="#modern-developments">Modern Developments</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#the-pinhole-camera-model">2. The Pinhole Camera Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#coordinate-setup">Coordinate Setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="#proof-by-similar-triangles">Proof by Similar Triangles</a></li>
<li class="toctree-l3"><a class="reference internal" href="#numerical-example">Numerical Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="#inadequacy-of-a-simple-pinhole">Inadequacy of a Simple Pinhole</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#camera-intrinsics-and-the-projection-matrix">3. Camera Intrinsics and the Projection Matrix</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#extrinsic-parameters">Extrinsic Parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="#full-projection-example">Full Projection Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#image-distortions-correction">4. Image Distortions &amp; Correction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#properties-of-perspective-projection">5. Properties of Perspective Projection</a></li>
<li class="toctree-l2"><a class="reference internal" href="#advanced-theoretical-extensions">6. Advanced Theoretical Extensions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#light-field-imaging-and-plenoptic-cameras">Light Field Imaging and Plenoptic Cameras</a></li>
<li class="toctree-l3"><a class="reference internal" href="#non-conventional-imaging-techniques">Non-Conventional Imaging Techniques</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#applications-in-modern-vision-and-graphics">7. Applications in Modern Vision and Graphics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#computer-vision-and-3d-reconstruction">Computer Vision and 3D Reconstruction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#medical-imaging">Medical Imaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="#photorealistic-rendering-in-computer-graphics">Photorealistic Rendering in Computer Graphics</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#python-example-simulating-image-formation">8. Python Example: Simulating Image Formation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html">Lecture 02.2 – Rotations and Kinematic Chains</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#representations-of-3d-rotations">1. Representations of 3D Rotations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#a-rotation-matrices">A) Rotation Matrices</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#b-euler-angles">B) Euler Angles</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#c-quaternions">C) Quaternions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#lie-algebra-so-3-and-exponential-map">2. Lie Algebra <span class="math notranslate nohighlight">\(so(3)\)</span> and Exponential Map</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#rodrigues-rotation-formula">3. Rodrigues’ Rotation Formula</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#kinematic-chains-forward-inverse-kinematics">4. Kinematic Chains: Forward &amp; Inverse Kinematics</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#comparison-of-rotation-representations">Comparison of Rotation Representations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_03_1_surface_representations.html">Lecture 03.1 – Surface Representations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#mathematical-foundations-of-surface-representations">1. Mathematical Foundations of Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-parametric-surfaces">A) Parametric Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-implicit-surfaces">B) Implicit Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-explicit-surfaces">C) Explicit Surfaces</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#surface-differential-properties">2. Surface Differential Properties</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-surface-normals">A) Surface Normals</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-fundamental-forms-and-curvature">B) Fundamental Forms and Curvature</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-geodesics">C) Geodesics</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#discrete-surface-representations">3. Discrete Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-polygon-meshes">A) Polygon Meshes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-point-clouds">B) Point Clouds</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-signed-distance-fields-sdf">C) Signed Distance Fields (SDF)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#advanced-surface-representations">4. Advanced Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-bezier-curves-and-surfaces">A) Bézier Curves and Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-b-splines-and-nurbs">B) B-Splines and NURBS</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-subdivision-surfaces">C) Subdivision Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-level-sets">D) Level Sets</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#e-neural-implicit-representations">E) Neural Implicit Representations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#comparative-analysis-and-applications">5. Comparative Analysis and Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-computational-efficiency-and-storage">A) Computational Efficiency and Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-practical-applications">B) Practical Applications</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-operations-complexity">C) Operations Complexity</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#implementation-examples">6. Implementation Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-basic-mesh-processing-python">A) Basic Mesh Processing (Python)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-implicit-surface-utilities">B) Implicit Surface Utilities</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-bezier-curve-implementation">C) Bézier Curve Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-curvature-estimation-on-meshes">D) Curvature Estimation on Meshes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#advanced-topics-and-future-directions">7. Advanced Topics and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-multi-resolution-representations">A) Multi-Resolution Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-machine-learning-for-geometry">B) Machine Learning for Geometry</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-dynamic-surfaces">C) Dynamic Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-non-manifold-geometries">D) Non-Manifold Geometries</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html">Lecture 03.2 – Procrustes Alignment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#goal-learning-a-model-of-pose-and-shape">Goal: Learning a Model of Pose and Shape</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#the-challenge-of-registration">The Challenge of Registration</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#surface-representation-mesh">Surface Representation: Mesh</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#the-procrustes-alignment-problem-mathematical-formulation">The Procrustes Alignment Problem: Mathematical Formulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#rigid-transformations">Rigid Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#procrustes-alignment-solution">Procrustes Alignment Solution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#decoupling-translation-by-centroid-alignment">Decoupling Translation by Centroid Alignment</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#optimal-rotation-via-svd">Optimal Rotation via SVD</a><ul>
<li class="toctree-l4"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#reflection-adjustment">Reflection Adjustment</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#optimal-scale-optional">Optimal Scale (Optional)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#complete-mathematical-derivation">Complete Mathematical Derivation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#translation-derivation">Translation Derivation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#rotation-derivation">Rotation Derivation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#scale-derivation">Scale Derivation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#summary-of-procrustes-alignment-algorithm">Summary of Procrustes Alignment Algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#python-implementation-example">Python Implementation Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#practical-applications">Practical Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#interactive-visualization-ideas">Interactive Visualization Ideas</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_04_1_icp.html">Lecture 4.1: Iterative Closest Point</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#introduction-to-shape-alignment-and-registration">Introduction to Shape Alignment and Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#the-registration-problem">The Registration Problem</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#review-procrustes-analysis">Review: Procrustes Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#problem-unknown-correspondences">Problem: Unknown Correspondences</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#the-iterative-closest-point-icp-algorithm">The Iterative Closest Point (ICP) Algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#basic-icp-algorithm">Basic ICP Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#computational-considerations">Computational Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="lecture_04_1_icp.html#closest-point-computation">Closest Point Computation</a></li>
<li class="toctree-l4"><a class="reference internal" href="lecture_04_1_icp.html#convergence-and-local-minima">Convergence and Local Minima</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#point-to-point-vs-point-to-plane-icp">Point-to-Point vs. Point-to-Plane ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#point-to-point-icp">Point-to-Point ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#point-to-plane-icp">Point-to-Plane ICP</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#gradient-based-icp-for-non-rigid-registration">Gradient-based ICP for Non-Rigid Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#gradient-based-icp-algorithm">Gradient-based ICP Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#advantages-of-gradient-based-icp">Advantages of Gradient-based ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#computing-gradients">Computing Gradients</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#improving-icp-s-robustness">Improving ICP’s Robustness</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#data-association-direction">Data Association Direction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#robust-cost-functions">Robust Cost Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#trimmed-icp">Trimmed ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#ransac-based-approaches">RANSAC-based Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#additional-information">Additional Information</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#point-to-surface-distance">Point-to-Surface Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#icp-variants-and-extensions">ICP Variants and Extensions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#generalized-icp-gicp">Generalized ICP (GICP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#em-icp-and-probabilistic-approaches">EM-ICP and Probabilistic Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#coherent-point-drift-cpd">Coherent Point Drift (CPD)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#multi-scale-approaches">Multi-Scale Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#applications-of-icp">Applications of ICP</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#implementing-icp">Implementing ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#efficient-python-implementation">Efficient Python Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#practical-tips">Practical Tips</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_04_2_body_models.html">Lecture 04.2 - Body Models: Vertex-Based Models and SMPL</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#body-models-as-parameterized-functions">1. Body Models as Parameterized Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#rotations-articulation-and-pose-representation">2. Rotations, Articulation, and Pose Representation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#rotation-representation">2.1 Rotation Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#kinematic-chain">2.2 Kinematic Chain</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#linear-blend-skinning-and-its-limitations">3. Linear Blend Skinning and its Limitations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#linear-blend-skinning">3.1 Linear Blend Skinning</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#problems-with-standard-lbs">3.2 Problems with Standard LBS</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#blend-shapes-for-correcting-lbs-artifacts">3.3 Blend Shapes for Correcting LBS Artifacts</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#the-smpl-body-model">4. The SMPL Body Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#smpl-philosophy">4.1 SMPL Philosophy</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#smpl-model-architecture">4.2 SMPL Model Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#shape-blend-shapes">4.2.1 Shape Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#pose-blend-shapes">4.2.2 Pose Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#joint-regression">4.2.3 Joint Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#model-training">4.3 Model Training</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#comparison-with-scape">5. Comparison with SCAPE</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#the-scape-model">5.1 The SCAPE Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#different-approaches-to-deformation">5.2 Different Approaches to Deformation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#performance-comparison">5.3 Performance Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#other-advantages">5.4 Other Advantages</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#alignment-techniques-procrustes-analysis-and-icp">6. Alignment Techniques: Procrustes Analysis and ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#procrustes-analysis">6.1 Procrustes Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#iterative-closest-point-icp">6.2 Iterative Closest Point (ICP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#fitting-smpl-to-scans">6.3 Fitting SMPL to Scans</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#image-formation-and-the-pinhole-camera-model">7. Image Formation and the Pinhole Camera Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#the-pinhole-camera-model">7.1 The Pinhole Camera Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#lens-distortion">7.2 Lens Distortion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#extensions-and-advanced-applications">8. Extensions and Advanced Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#dynamic-soft-tissue-modeling">8.1 Dynamic Soft Tissue Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#specialized-extensions">8.2 Specialized Extensions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#deep-learning-for-model-fitting">8.3 Deep Learning for Model Fitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#probabilistic-approaches">8.4 Probabilistic Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#hybrid-models">8.5 Hybrid Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_05_1_body_model_training.html">Lecture 5.1 - Training a Body Model and Fitting SMPL to Scans</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#body-models-based-on-triangle-deformations">Body Models Based on Triangle Deformations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#scape-and-blendscape-models">SCAPE and BlendSCAPE Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#triangle-deformation-process">Triangle Deformation Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#comparison-smpl-vs-scape-blendscape">Comparison: SMPL vs. SCAPE/BlendSCAPE</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#training-a-body-model-from-registrations">Training a Body Model from Registrations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#the-challenge-of-raw-scan-data">The Challenge of Raw Scan Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#training-from-registrations">Training from Registrations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#obtaining-registrations-fitting-smpl-to-scans">Obtaining Registrations: Fitting SMPL to Scans</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#non-rigid-registration-process">Non-Rigid Registration Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#iterative-closest-point-icp-review">Iterative Closest Point (ICP) Review</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#registration-objective-formulation">Registration Objective Formulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#point-to-surface-distance">Point-to-Surface Distance</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#multi-stage-optimization-strategy">Multi-Stage Optimization Strategy</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#joint-registration-and-model-training">Joint Registration and Model Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#co-registration-approach">Co-Registration Approach</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a><ul>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-01-1-historical-body-models">Lecture 01.1 (Historical Body Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-01-2-introduction-to-human-models">Lecture 01.2 (Introduction to Human Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-01-3-introduction-to-human-models-continued">Lecture 01.3 (Introduction to Human Models Continued)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-02-1-image-formation">Lecture 02.1 (Image Formation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-02-2-rotations-kinematic-chains">Lecture 02.2 (Rotations &amp; Kinematic Chains)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-03-1-surface-representations">Lecture 03.1 (Surface Representations)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-03-2-procrustes-alignment">Lecture 03.2 (Procrustes Alignment)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-04-1-iterative-closest-points">Lecture 04.1 (Iterative Closest Points)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-04-2-body-models">Lecture 04.2 (Body Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-05-1-body-model-training">Lecture 05.1 (Body Model Training)</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Virtual Humans Lecture</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Lecture 02.1 – Image Formation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/lecture_02_1_image_formation.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="lecture-02-1-image-formation">
<span id="id1"></span><h1>Lecture 02.1 – Image Formation<a class="headerlink" href="#lecture-02-1-image-formation" title="Link to this heading"></a></h1>
<iframe width="600" height="400" src="https://www.youtube.com/embed/7-dJkA4Kg1g"
title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write;
encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><p><a class="reference external" href="https://virtualhumans.mpi-inf.mpg.de/VH23/slides/pdf/Lecture_02_1_Image_Formation.pdf">Lecture Slides: Image Formation</a></p>
<p>Image formation is the process by which a three-dimensional (3D) scene is transformed into a
two-dimensional (2D) image. Understanding this process is fundamental for both interpreting
images (as in computer vision) and synthesizing images (as in computer graphics). At its core,
image formation involves <strong>projection</strong> – the mapping of 3D points in the world to 2D points on an image
plane. This projection is governed by the physics of light and the geometry of the imaging system.</p>
<section id="historical-developments-in-image-formation">
<h2>1. Historical Developments in Image Formation<a class="headerlink" href="#historical-developments-in-image-formation" title="Link to this heading"></a></h2>
<p>Understanding how images are formed has been a pursuit spanning centuries, intertwining physics,
art, and mathematics.</p>
<section id="ancient-and-medieval-optics-camera-obscura">
<h3>Ancient and Medieval Optics – Camera Obscura<a class="headerlink" href="#ancient-and-medieval-optics-camera-obscura" title="Link to this heading"></a></h3>
<p>The basic principle of image formation by a pinhole (camera obscura) was known in antiquity.
Early records suggest that around the 5th century B.C., the Chinese philosopher Mozi and later
Aristotle were aware of the camera obscura effect, where light from a scene passing through a small
hole projects an inverted image on a surface. However, it was the medieval Arab scientist <strong>Ibn al-Haytham</strong>
(Alhazen, 965–1040 AD) who provided the first detailed description. In his <em>Book of Optics</em> (~1021 AD),
Ibn al-Haytham used the camera obscura to study light and vision, introducing the concept that vision
results from light entering the eye rather than emanating from it.</p>
</section>
<section id="renaissance-perspective-and-geometry">
<h3>Renaissance Perspective and Geometry<a class="headerlink" href="#renaissance-perspective-and-geometry" title="Link to this heading"></a></h3>
<p>A major leap occurred in the early 15th century with the formalization of <strong>linear perspective</strong>.
Artists and architects sought a systematic way to represent 3D scenes on 2D canvases realistically.
<strong>Filippo Brunelleschi</strong> is credited with demonstrating perspective around 1413. By observing a scene
through a small hole in a painted panel and reflecting the view in a mirror, he proved that parallel
lines in the world converge to a vanishing point on the horizon line in the painting.</p>
<p>Shortly thereafter, <strong>Leon Battista Alberti</strong> published <em>On Painting</em> (1435), providing a more formal
framework. This framework used the concept of a visual pyramid and an intersection of rays on a picture
plane. Artists like <strong>Masaccio</strong> and <strong>Leonardo da Vinci</strong> refined these techniques, and projective geometry
later gave perspective a rigorous mathematical underpinning.</p>
</section>
<section id="early-cameras-and-photographic-imaging">
<h3>Early Cameras and Photographic Imaging<a class="headerlink" href="#early-cameras-and-photographic-imaging" title="Link to this heading"></a></h3>
<p>The camera obscura remained a tool for artists into the Renaissance, often enhanced with lenses for brightness.
The leap to <strong>permanent image capture</strong> came in 1826 when <strong>Joseph Nicéphore Niépce</strong> produced the first known
photograph on a pewter plate (heliography). Although exposures were extremely long, it proved that images formed
by a camera obscura could be preserved chemically.</p>
<p>Subsequent improvements by <strong>Louis Daguerre</strong> (daguerreotype, 1839) and <strong>William Henry Fox Talbot</strong> (calotype)
reduced exposure times and introduced practical photographic processes. Throughout the 19th century, optical
scientists refined <strong>lens design</strong> to reduce aberrations, enabling sharper images.</p>
</section>
<section id="modern-developments">
<h3>Modern Developments<a class="headerlink" href="#modern-developments" title="Link to this heading"></a></h3>
<p>In the 20th century, <strong>photogrammetry</strong> and later <strong>computer vision</strong> formalized the pinhole camera as a
standard mathematical model of image formation. Early work in the 1960s introduced matrix methods and
homogeneous coordinates to handle perspective projection in computers.</p>
<p>From the late 20th century onward, camera sensors shifted from <strong>film</strong> to <strong>digital</strong> (CCD, CMOS).
With images as digital data, <strong>computational photography</strong> emerged. These developments reflect the
continued evolution of image formation from a purely geometric/optical process to one in which
significant software post-processing complements the hardware.</p>
</section>
</section>
<section id="the-pinhole-camera-model">
<h2>2. The Pinhole Camera Model<a class="headerlink" href="#the-pinhole-camera-model" title="Link to this heading"></a></h2>
<p>A <strong>pinhole camera</strong> is essentially a light-tight box with a small aperture (pinhole) through which
light from a scene projects onto the opposite side. This simple setup captures the fundamental
mapping from 3D to 2D. Real cameras with lenses behave similarly (lenses just gather more light),
so the pinhole model remains valid as the base geometric framework.</p>
<section id="coordinate-setup">
<h3>Coordinate Setup<a class="headerlink" href="#coordinate-setup" title="Link to this heading"></a></h3>
<p>We place the pinhole (center of projection) at the origin of a 3D coordinate system. The optical
axis aligns with the <span class="math notranslate nohighlight">\(Z\)</span>-axis, and the <strong>image plane</strong> is at <span class="math notranslate nohighlight">\(Z = f &gt; 0\)</span>, perpendicular
to the axis. A point <span class="math notranslate nohighlight">\(P = (X, Y, Z)\)</span> in 3D is projected to <span class="math notranslate nohighlight">\((x, y)\)</span> on the image plane by
drawing the ray from <span class="math notranslate nohighlight">\((0,0,0)\)</span> through <span class="math notranslate nohighlight">\((X,Y,Z)\)</span>. Where that ray meets the plane <span class="math notranslate nohighlight">\(Z=f\)</span>,
we get the image coordinates. By similar triangles:</p>
<div class="math notranslate nohighlight">
\[x = f \frac{X}{Z}, \quad y = f \frac{Y}{Z}.\]</div>
<p>Thus, <strong>perspective projection</strong> is defined by dividing by the depth <span class="math notranslate nohighlight">\(Z\)</span>.</p>
</section>
<section id="proof-by-similar-triangles">
<h3>Proof by Similar Triangles<a class="headerlink" href="#proof-by-similar-triangles" title="Link to this heading"></a></h3>
<p>Consider the line from <span class="math notranslate nohighlight">\((0,0,0)\)</span> to <span class="math notranslate nohighlight">\((X, Y, Z)\)</span>. In parametric form, it is:</p>
<div class="math notranslate nohighlight">
\[L(t) = t (X, Y, Z).\]</div>
<p>We want the intersection with the plane <span class="math notranslate nohighlight">\(Z = f\)</span>. Imposing
<span class="math notranslate nohighlight">\(t Z = f \Rightarrow t = f/Z\)</span>. Hence:</p>
<div class="math notranslate nohighlight">
\[\bigl(x, y, f\bigr) = \bigl(t X,\; t Y,\; t Z\bigr)
                    = \Bigl(\frac{fX}{Z}, \frac{fY}{Z}, f\Bigr).\]</div>
<p>Ignoring the <span class="math notranslate nohighlight">\(z = f\)</span> component, we obtain <span class="math notranslate nohighlight">\(x = fX/Z\)</span> and
<span class="math notranslate nohighlight">\(y = fY/Z\)</span>.</p>
</section>
<section id="numerical-example">
<h3>Numerical Example<a class="headerlink" href="#numerical-example" title="Link to this heading"></a></h3>
<p>Let <span class="math notranslate nohighlight">\(f = 1\,\mathrm{(unit)}\)</span>, and a 3D point <span class="math notranslate nohighlight">\((X,Y,Z)=(2,3,10)\)</span>. Then:</p>
<div class="math notranslate nohighlight">
\[x = 1 \times \frac{2}{10} = 0.2, \quad
y = 1 \times \frac{3}{10} = 0.3.\]</div>
<p>If we change <span class="math notranslate nohighlight">\(Z\)</span> to <span class="math notranslate nohighlight">\(5\)</span>, the image point doubles to <span class="math notranslate nohighlight">\((0.4, 0.6)\)</span>,
illustrating that <em>objects closer to the pinhole appear larger</em>.</p>
</section>
<section id="inadequacy-of-a-simple-pinhole">
<h3>Inadequacy of a Simple Pinhole<a class="headerlink" href="#inadequacy-of-a-simple-pinhole" title="Link to this heading"></a></h3>
<p>The pinhole camera model in geometry assumes an infinitesimally small hole to get a perfectly
sharp image projection. In practice, a very tiny pinhole lets through very little light, yielding
a dim image. If one enlarges the pinhole to admit more light, the image blurs because rays from
different points start overlapping at the image plane.</p>
<p>This inherent trade-off (sharpness vs. brightness) is why real cameras use lenses rather than
tiny pinholes. A lens of finite aperture can gather more light while still focusing rays from a
given scene point to one image point.</p>
</section>
</section>
<section id="camera-intrinsics-and-the-projection-matrix">
<h2>3. Camera Intrinsics and the Projection Matrix<a class="headerlink" href="#camera-intrinsics-and-the-projection-matrix" title="Link to this heading"></a></h2>
<p>So far, we have <span class="math notranslate nohighlight">\((x, y)\)</span> in a continuous coordinate system. Real cameras record pixels,
with an origin usually at the image’s corner and a certain pixel size. Let <span class="math notranslate nohighlight">\((u,v)\)</span> be
pixel coordinates. We define:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(s_x, s_y\)</span>: number of pixels per physical unit (e.g., mm) along <span class="math notranslate nohighlight">\(x,y\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\((c_x, c_y)\)</span>: location in pixel coordinates where the optical axis intersects the image plane (principal point).</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span>: skew parameter (usually <span class="math notranslate nohighlight">\(0\)</span> for standard cameras).</p></li>
</ul>
<p>Thus:</p>
<div class="math notranslate nohighlight">
\[u = s_x\, x + c_x, \quad
v = s_y\, y + c_y,\]</div>
<p>with optional skew term. These parameters form the <strong>intrinsic matrix</strong> <span class="math notranslate nohighlight">\(K\)</span>
(often <span class="math notranslate nohighlight">\(3 \times 3\)</span>). For example (zero skew):</p>
<div class="math notranslate nohighlight">
\[\begin{split}K = \begin{pmatrix}
f_x &amp; 0   &amp; c_x \\
0   &amp; f_y &amp; c_y \\
0   &amp; 0   &amp; 1
\end{pmatrix},\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(f_x = s_x\,f\)</span> and <span class="math notranslate nohighlight">\(f_y = s_y\,f\)</span>. In homogeneous coordinates,
the 2D point <span class="math notranslate nohighlight">\((u,v,1)^T\)</span> is <span class="math notranslate nohighlight">\(K\, (X,Y,Z)^T / Z\)</span>.</p>
<section id="extrinsic-parameters">
<h3>Extrinsic Parameters<a class="headerlink" href="#extrinsic-parameters" title="Link to this heading"></a></h3>
<p>A camera also has a <strong>position</strong> (translation <span class="math notranslate nohighlight">\(\mathbf{t}\)</span>) and
<strong>orientation</strong> (rotation <span class="math notranslate nohighlight">\(R\)</span>) relative to a <strong>world coordinate system</strong>.
Hence, a point in world coordinates <span class="math notranslate nohighlight">\((X_w, Y_w, Z_w)\)</span> first transforms to
camera coordinates:</p>
<div class="math notranslate nohighlight">
\[(X_c, Y_c, Z_c) = R\, (X_w, Y_w, Z_w) + \mathbf{t},\]</div>
<p>and then projects via:</p>
<div class="math notranslate nohighlight">
\[(x,y) = (fX_c/Z_c,\; fY_c/Z_c).\]</div>
<p>Combining with the intrinsic matrix, the <strong>camera projection matrix</strong> <span class="math notranslate nohighlight">\(P\)</span> is:</p>
<div class="math notranslate nohighlight">
\[P = K \,[R \mid \mathbf{t}],
\quad \text{size is } 3\times4.\]</div>
<p>Thus, for a homogeneous world coordinate <span class="math notranslate nohighlight">\(X_w\)</span>, its image in homogeneous
pixel coordinates is <span class="math notranslate nohighlight">\(x_{image} = P \, X_w\)</span>.</p>
</section>
<section id="full-projection-example">
<h3>Full Projection Example<a class="headerlink" href="#full-projection-example" title="Link to this heading"></a></h3>
<p>Consider a simple scenario where the world coordinate system coincides with the camera
coordinate system (so <span class="math notranslate nohighlight">\(R=I\)</span>, <span class="math notranslate nohighlight">\(t=0\)</span>), the focal length is <span class="math notranslate nohighlight">\(f=1\)</span> (normalized units),
and the image sensor has 1000 pixels per unit with principal point at the center.</p>
<p>Then the intrinsic matrix is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}K = \begin{pmatrix}
1000 &amp; 0 &amp; 0 \\
0 &amp; 1000 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{pmatrix}.\end{split}\]</div>
<p>For our 3D point <span class="math notranslate nohighlight">\((X, Y, Z) = (2, 3, 10)\)</span>, the projection in normalized sensor
units was <span class="math notranslate nohighlight">\((0.2, 0.3)\)</span>. Converting to pixel coordinates:</p>
<div class="math notranslate nohighlight">
\[\begin{split}u = 1000 \times 0.2 + 0 = 200 \text{ px},\\
v = 1000 \times 0.3 + 0 = 300 \text{ px}.\end{split}\]</div>
</section>
</section>
<section id="image-distortions-correction">
<h2>4. Image Distortions &amp; Correction<a class="headerlink" href="#image-distortions-correction" title="Link to this heading"></a></h2>
<p>Real lenses introduce <em>radial</em> distortion (barrel/pincushion) and possibly <em>tangential</em>
distortion. A common radial model is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}x_d = x \,(1 + k_1\,r^2 + k_2\,r^4 + k_3\,r^6),\\
y_d = y \,(1 + k_1\,r^2 + k_2\,r^4 + k_3\,r^6),\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(r^2 = x^2 + y^2\)</span>. Tangential distortion involves additional terms.</p>
<p>These distortions result in two main visible effects:</p>
<ul class="simple">
<li><p><strong>Barrel distortion</strong>: Negative <span class="math notranslate nohighlight">\(k_1\)</span> causes the image to “bulge” outward (straight lines bow outward)</p></li>
<li><p><strong>Pincushion distortion</strong>: Positive <span class="math notranslate nohighlight">\(k_1\)</span> makes the image “pinch” inward (straight lines bow inward)</p></li>
</ul>
<p>Calibration software (e.g., OpenCV) solves for these coefficients, so we can
undistort images. This is critical for precision vision applications where accurate
geometry is required.</p>
</section>
<section id="properties-of-perspective-projection">
<h2>5. Properties of Perspective Projection<a class="headerlink" href="#properties-of-perspective-projection" title="Link to this heading"></a></h2>
<p>Perspective projection has several important geometric properties:</p>
<ol class="arabic">
<li><p><strong>Straight lines project to straight lines</strong>: Except for lines passing through
the camera center (which project to a single point, a vanishing point),
projective geometry ensures that the image of a line is still a line.</p>
<p><strong>Proof sketch</strong>: A 3D line can be parameterized as <span class="math notranslate nohighlight">\(P(t) = P_0 + t\mathbf{d}\)</span>,
where <span class="math notranslate nohighlight">\(P_0=(X_0,Y_0,Z_0)\)</span> is a point on the line and <span class="math notranslate nohighlight">\(\mathbf{d}=(a,b,c)\)</span>
is its direction vector. The projection of this line yields an equation that
simplifies to a linear equation in the image plane.</p>
</li>
<li><p><strong>Vanishing points</strong>: Parallel lines in 3D that are not parallel to the image
plane appear to converge at a vanishing point. This point is where a set of
parallel lines appears to meet at infinity.</p>
<p>Any set of parallel lines within the scene will converge to a point on
the horizon line in the image. The horizon line itself is the vanishing line
of the ground plane.</p>
</li>
<li><p><strong>Planes to homographies</strong>: Points on a plane in 3D project via a 2D
projective transformation (homography). Thus, a planar surface can be unwarped
from its image if the plane’s equation and camera parameters are known.</p></li>
<li><p><strong>Field of View (FOV)</strong>: The focal length <span class="math notranslate nohighlight">\(f\)</span> and sensor size determine
the camera’s field of view. A small <span class="math notranslate nohighlight">\(f\)</span> yields a wide FOV (wide angle),
and a large <span class="math notranslate nohighlight">\(f\)</span> yields a narrow FOV (telephoto):</p>
<div class="math notranslate nohighlight">
\[\mathrm{FOV} \approx 2\,\arctan\Big(\frac{\text{sensor width}}{2f}\Big).\]</div>
<p>For example, a full-frame camera sensor (~36 mm wide) with a 35 mm focal length lens has
<span class="math notranslate nohighlight">\(\mathrm{FOV}_h \approx 2 \arctan(18/35) \approx 53^\circ\)</span>. If we increase <span class="math notranslate nohighlight">\(f\)</span>
to 70 mm, <span class="math notranslate nohighlight">\(\mathrm{FOV}_h \approx 28^\circ\)</span> – a much tighter view.</p>
</li>
<li><p><strong>Depth ambiguity</strong>: Single-view projection collapses the depth dimension;
many points along the same ray in 3D have the same 2D image. Therefore,
recovering 3D from one image is ill-posed without additional priors or
multiple views.</p></li>
</ol>
</section>
<section id="advanced-theoretical-extensions">
<h2>6. Advanced Theoretical Extensions<a class="headerlink" href="#advanced-theoretical-extensions" title="Link to this heading"></a></h2>
<p>While the pinhole model is a cornerstone, many modern imaging systems go beyond it.</p>
<section id="light-field-imaging-and-plenoptic-cameras">
<h3>Light Field Imaging and Plenoptic Cameras<a class="headerlink" href="#light-field-imaging-and-plenoptic-cameras" title="Link to this heading"></a></h3>
<p>A <strong>light field</strong> is a 4D function encoding the intensity of rays in free space,
parameterized by position and direction. The concept stems from the <strong>plenoptic
function</strong> of Adelson and Bergen, which includes spatial, angular, spectral,
and temporal dimensions of light.</p>
<p><strong>Plenoptic Cameras</strong>: By placing a micro-lens array near the sensor (behind a
main lens), each micro-lens captures different incident angles locally. Thus,
the recorded data is a 4D light field: two coordinates for the micro-lens array
position and two for sub-pixel direction. This enables <strong>post-capture refocus</strong>,
slight viewpoint shifts, and other computational possibilities.</p>
</section>
<section id="non-conventional-imaging-techniques">
<h3>Non-Conventional Imaging Techniques<a class="headerlink" href="#non-conventional-imaging-techniques" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Compressive Imaging</strong>: A single-pixel camera uses a spatial light modulator
(DMD) to sample the scene with known patterns, measuring the integrated intensity
on one detector. This is useful in wavelengths (like infrared) where large pixel
arrays are difficult or expensive.</p></li>
<li><p><strong>Non-Line-of-Sight (NLOS) Imaging</strong>: By emitting laser pulses and measuring
time-of-flight echoes with ultrafast detectors, one can reconstruct 3D shapes
hidden around corners. The wall acts as a diffuse relay.</p></li>
<li><p><strong>Coded Apertures, Wavefront Engineering</strong>: Various mask or aperture-coding
schemes modulate the incoming light so the captured data is more <em>invertible</em> for
tasks like deblurring, depth extraction, or spectral imaging.</p></li>
</ul>
</section>
</section>
<section id="applications-in-modern-vision-and-graphics">
<h2>7. Applications in Modern Vision and Graphics<a class="headerlink" href="#applications-in-modern-vision-and-graphics" title="Link to this heading"></a></h2>
<section id="computer-vision-and-3d-reconstruction">
<h3>Computer Vision and 3D Reconstruction<a class="headerlink" href="#computer-vision-and-3d-reconstruction" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Camera Calibration</strong>: One estimates the intrinsic matrix <span class="math notranslate nohighlight">\(K\)</span> and
extrinsic parameters by observing known 3D points and their 2D projections.
Many toolboxes implement this (e.g., OpenCV), typically solving for the
projection matrix <span class="math notranslate nohighlight">\(P = K[R|t]\)</span>.</p></li>
<li><p><strong>Stereo / Multi-view</strong>: With multiple images from different viewpoints, one
triangulates 3D points by intersecting rays. Epipolar geometry (fundamental or
essential matrix) arises directly from the pinhole model.</p></li>
<li><p><strong>Pose Estimation</strong>: Given correspondences between known 3D points and 2D
detections, we solve the PnP (Perspective-n-Point) problem. This is central to
augmented reality and robotics.</p></li>
<li><p><strong>Structure from Motion (SfM)</strong>: With multiple images of unknown camera poses,
both camera parameters and 3D structure can be iteratively estimated by
minimizing reprojection error (bundle adjustment).</p></li>
</ul>
</section>
<section id="medical-imaging">
<h3>Medical Imaging<a class="headerlink" href="#medical-imaging" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>X-ray and CT</strong>: X-ray radiographs are essentially projection images from a
point source (like a pinhole). CT collects multiple such projections to invert
the line integrals of X-ray attenuation, reconstructing cross-sectional
volumes.</p></li>
<li><p><strong>MRI, Ultrasound</strong>: Although their physics differ (spin resonance, acoustic
echoes), each has a forward image formation model, inverted computationally
to yield a final image.</p></li>
</ul>
</section>
<section id="photorealistic-rendering-in-computer-graphics">
<h3>Photorealistic Rendering in Computer Graphics<a class="headerlink" href="#photorealistic-rendering-in-computer-graphics" title="Link to this heading"></a></h3>
<p>Rendering is the <em>inverse</em> of vision: given a 3D scene, we compute which rays
reach each pixel. Methods like <strong>ray tracing</strong> sample rays from a virtual pinhole
or thin lens through each pixel into the scene, recursively simulating bounces
and lighting (using the <em>rendering equation</em>):</p>
<div class="math notranslate nohighlight">
\[L_o(x,\omega) = L_e(x,\omega) + \int_{\text{Hemi}} f_r(x,\omega',\omega) L_i(x,\omega') (\omega' \cdot n) d\omega'\]</div>
<p>By accurately modeling geometry, materials, and camera parameters (lens aperture,
focal length, etc.), we can recreate physically plausible images. Modern “physically
based rendering” aims for realism by faithfully implementing optical laws.</p>
</section>
</section>
<section id="python-example-simulating-image-formation">
<h2>8. Python Example: Simulating Image Formation<a class="headerlink" href="#python-example-simulating-image-formation" title="Link to this heading"></a></h2>
<p>Below is a minimal snippet that demonstrates a pinhole camera pipeline:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Intrinsic parameters</span>
<span class="n">f</span> <span class="o">=</span> <span class="mi">800</span>  <span class="c1"># focal length in pixels</span>
<span class="n">cx</span><span class="p">,</span> <span class="n">cy</span> <span class="o">=</span> <span class="mi">320</span><span class="p">,</span> <span class="mi">240</span>  <span class="c1"># principal point (image center)</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">f</span><span class="p">,</span>   <span class="mi">0</span><span class="p">,</span> <span class="n">cx</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span>   <span class="n">f</span><span class="p">,</span> <span class="n">cy</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span>   <span class="mi">0</span><span class="p">,</span>  <span class="mi">1</span><span class="p">]])</span>

<span class="c1"># Extrinsics: identity rotation &amp; zero translation</span>
<span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># Some 3D points (corners of a square at Z=5)</span>
<span class="n">points_3d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">1.0</span><span class="p">,</span>  <span class="mf">1.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span>  <span class="mf">1.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">]</span>
<span class="p">])</span>

<span class="c1"># Transform into camera coords</span>
<span class="n">points_cam</span> <span class="o">=</span> <span class="p">(</span><span class="n">points_3d</span> <span class="o">@</span> <span class="n">R</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="n">t</span>

<span class="c1"># Project to image (homogeneous)</span>
<span class="n">points_img_h</span> <span class="o">=</span> <span class="n">points_cam</span> <span class="o">@</span> <span class="n">K</span><span class="o">.</span><span class="n">T</span>
<span class="n">points_img</span> <span class="o">=</span> <span class="n">points_img_h</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">/</span> <span class="n">points_img_h</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">points_img</span><span class="p">)</span>
</pre></div>
</div>
<p>Running this code yields 2D pixel coordinates of the projected 3D square.
Try adjusting <span class="math notranslate nohighlight">\(Z\)</span> or <span class="math notranslate nohighlight">\(f\)</span> to see perspective effects.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="lecture_01_3_introduction_to_human_models_continued.html" class="btn btn-neutral float-left" title="Lecture 01.3 – Introduction to Human Models (Overview)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="lecture_02_2_rotations_kinematic_chains.html" class="btn btn-neutral float-right" title="Lecture 02.2 – Rotations and Kinematic Chains" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>