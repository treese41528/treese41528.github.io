

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Lecture 09.1: Neural Implicit and Point-Based Representations for Clothed Human Modeling &mdash; Fitting SMPL to IMU Optimization</title>
      <link rel="stylesheet" type="text/css" href="/VirtualHumans/_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="/VirtualHumans/_static/css/theme.css?v=e59714d7" />

  
    <link rel="canonical" href="https://treese41528.github.io/VirtualHumans/lecture_09_1_neual_implicit_and_point_based_clothing_models.html" />
      <script src="/VirtualHumans/_static/jquery.js?v=5d32c60e"></script>
      <script src="/VirtualHumans/_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="/VirtualHumans/_static/documentation_options.js?v=f2a433a1"></script>
      <script src="/VirtualHumans/_static/doctools.js?v=9bcbadda"></script>
      <script src="/VirtualHumans/_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="/VirtualHumans/_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Neural Radiance Fields: A Historical and Theoretical Overview" href="extended_materials_neural_radiance_fields.html" />
    <link rel="prev" title="Lecture 08.1: Vertex-Based Clothing Modeling for Virtual Humans" href="lecture_08_1_vertex_based_clothing_modeling.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Virtual Humans Lecture
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="lecture_01_1_historical_body_models.html">Lecture 01.1 – Historical Body Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#early-origins-simplified-primitives-and-kinematic-skeletons-1970s1980s">Early Origins: Simplified Primitives and Kinematic Skeletons (1970s–1980s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#advances-in-the-1990s-superquadrics-differentiable-fitting-and-physical-models">Advances in the 1990s: Superquadrics, Differentiable Fitting, and Physical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#the-impact-of-3d-scanning-and-data-from-anthropometry-to-statistical-models-1990s2000s">The Impact of 3D Scanning and Data: From Anthropometry to Statistical Models (1990s–2000s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#scape-and-the-emergence-of-pose-aware-models-mid-2000s">SCAPE and the Emergence of Pose-Aware Models (Mid-2000s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#consolidation-in-the-2010s-smpl-and-integration-with-learning-based-methods">Consolidation in the 2010s: SMPL and Integration with Learning-Based Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#deep-learning-and-neural-implicit-models-late-2010spresent">Deep Learning and Neural Implicit Models (Late 2010s–Present)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#timeline-summary-of-milestones">Timeline Summary of Milestones</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html">Lecture 01.2 – Introduction to Human Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#historical-context-of-human-body-modeling">1. Historical Context of Human Body Modeling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#early-developments">Early Developments</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#mid-20th-century-approaches">Mid-20th Century Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#data-driven-revolution-1990s-2000s">Data-Driven Revolution (1990s-2000s)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#mathematical-foundations-of-human-body-models">2. Mathematical Foundations of Human Body Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#the-smpl-model">The SMPL Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#pca-based-statistical-shape-modeling">PCA-Based Statistical Shape Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#kinematic-modeling">Kinematic Modeling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#applications-of-human-body-models">3. Applications of Human Body Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computer-animation-and-visual-effects">Computer Animation and Visual Effects</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#virtual-humans-and-avatars">Virtual Humans and Avatars</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#biomechanics-and-ergonomics">Biomechanics and Ergonomics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#human-computer-interaction-hci">Human-Computer Interaction (HCI)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computer-vision-and-ai">Computer Vision and AI</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#education-and-training">Education and Training</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#challenges-and-future-directions">4. Challenges and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computational-efficiency">Computational Efficiency</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#accuracy-and-detail">Accuracy and Detail</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#generalization">Generalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#clothing-and-accessories">Clothing and Accessories</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#emerging-approaches">Emerging Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html">Lecture 01.3 – Introduction to Human Models (Overview)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#historical-context">1. Historical Context</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#early-scientific-studies">Early Scientific Studies</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#mid-20th-century-to-digital-era">Mid-20th Century to Digital Era</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#st-century-advances">21st Century Advances</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#mathematical-foundations">2. Mathematical Foundations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#parametric-body-models">Parametric Body Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-surface-representations">Implicit Surface Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#kinematic-modeling">Kinematic Modeling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#image-formation-and-rendering">3. Image Formation and Rendering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#camera-models">Camera Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#shading-and-visibility">Shading and Visibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#differentiable-rendering">Differentiable Rendering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#surface-representation-methods">4. Surface Representation Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#explicit-mesh-models">Explicit Mesh Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-function-models">Implicit Function Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#motion-capture-and-behavior-synthesis">5. Motion Capture and Behavior Synthesis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#capturing-human-motion">Capturing Human Motion</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#behavior-synthesis">Behavior Synthesis</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#clothing-modeling">6. Clothing Modeling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physically-based-simulation">Physically-Based Simulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#data-driven-approaches">Data-Driven Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-clothing-models">Implicit Clothing Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#human-object-interaction">7. Human-Object Interaction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physics-based-methods">Physics-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#learning-based-approaches">Learning-Based Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#hybrid-systems">Hybrid Systems</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#applications">8. Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#entertainment-and-media">Entertainment and Media</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#healthcare-and-biomechanics">Healthcare and Biomechanics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#engineering-and-design">Engineering and Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#human-computer-interaction">Human-Computer Interaction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#scientific-research">Scientific Research</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#challenges-and-future-directions">9. Challenges and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#scalability-and-generalization">Scalability and Generalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#higher-fidelity-dynamics">Higher-Fidelity Dynamics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#data-and-labeling-constraints">Data and Labeling Constraints</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physics-and-learning-integration">Physics and Learning Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#semantic-and-cognitive-aspects">Semantic and Cognitive Aspects</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#realism-vs-controllability">Realism vs. Controllability</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_02_1_image_formation.html">Lecture 02.1 – Image Formation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#historical-developments-in-image-formation">1. Historical Developments in Image Formation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#ancient-and-medieval-optics-camera-obscura">Ancient and Medieval Optics – Camera Obscura</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#renaissance-perspective-and-geometry">Renaissance Perspective and Geometry</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#early-cameras-and-photographic-imaging">Early Cameras and Photographic Imaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#modern-developments">Modern Developments</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#the-pinhole-camera-model">2. The Pinhole Camera Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#coordinate-setup">Coordinate Setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#proof-by-similar-triangles">Proof by Similar Triangles</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#numerical-example">Numerical Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#inadequacy-of-a-simple-pinhole">Inadequacy of a Simple Pinhole</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#camera-intrinsics-and-the-projection-matrix">3. Camera Intrinsics and the Projection Matrix</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#extrinsic-parameters">Extrinsic Parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#full-projection-example">Full Projection Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#image-distortions-correction">4. Image Distortions &amp; Correction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#properties-of-perspective-projection">5. Properties of Perspective Projection</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#advanced-theoretical-extensions">6. Advanced Theoretical Extensions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#light-field-imaging-and-plenoptic-cameras">Light Field Imaging and Plenoptic Cameras</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#non-conventional-imaging-techniques">Non-Conventional Imaging Techniques</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#applications-in-modern-vision-and-graphics">7. Applications in Modern Vision and Graphics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#computer-vision-and-3d-reconstruction">Computer Vision and 3D Reconstruction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#medical-imaging">Medical Imaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#photorealistic-rendering-in-computer-graphics">Photorealistic Rendering in Computer Graphics</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#python-example-simulating-image-formation">8. Python Example: Simulating Image Formation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html">Lecture 02.2 – Rotations and Kinematic Chains</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#representations-of-3d-rotations">1. Representations of 3D Rotations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#a-rotation-matrices">A) Rotation Matrices</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#b-euler-angles">B) Euler Angles</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#c-quaternions">C) Quaternions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#lie-algebra-so-3-and-exponential-map">2. Lie Algebra <span class="math notranslate nohighlight">\(so(3)\)</span> and Exponential Map</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#rodrigues-rotation-formula">3. Rodrigues’ Rotation Formula</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#kinematic-chains-forward-inverse-kinematics">4. Kinematic Chains: Forward &amp; Inverse Kinematics</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#comparison-of-rotation-representations">Comparison of Rotation Representations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_03_1_surface_representations.html">Lecture 03.1 – Surface Representations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#mathematical-foundations-of-surface-representations">1. Mathematical Foundations of Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-parametric-surfaces">A) Parametric Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-implicit-surfaces">B) Implicit Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-explicit-surfaces">C) Explicit Surfaces</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#surface-differential-properties">2. Surface Differential Properties</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-surface-normals">A) Surface Normals</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-fundamental-forms-and-curvature">B) Fundamental Forms and Curvature</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-geodesics">C) Geodesics</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#discrete-surface-representations">3. Discrete Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-polygon-meshes">A) Polygon Meshes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-point-clouds">B) Point Clouds</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-signed-distance-fields-sdf">C) Signed Distance Fields (SDF)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#advanced-surface-representations">4. Advanced Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-bezier-curves-and-surfaces">A) Bézier Curves and Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-b-splines-and-nurbs">B) B-Splines and NURBS</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-subdivision-surfaces">C) Subdivision Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-level-sets">D) Level Sets</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#e-neural-implicit-representations">E) Neural Implicit Representations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#comparative-analysis-and-applications">5. Comparative Analysis and Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-computational-efficiency-and-storage">A) Computational Efficiency and Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-practical-applications">B) Practical Applications</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-operations-complexity">C) Operations Complexity</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#implementation-examples">6. Implementation Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-basic-mesh-processing-python">A) Basic Mesh Processing (Python)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-implicit-surface-utilities">B) Implicit Surface Utilities</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-bezier-curve-implementation">C) Bézier Curve Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-curvature-estimation-on-meshes">D) Curvature Estimation on Meshes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#advanced-topics-and-future-directions">7. Advanced Topics and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-multi-resolution-representations">A) Multi-Resolution Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-machine-learning-for-geometry">B) Machine Learning for Geometry</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-dynamic-surfaces">C) Dynamic Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-non-manifold-geometries">D) Non-Manifold Geometries</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html">Lecture 03.2 – Procrustes Alignment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#goal-learning-a-model-of-pose-and-shape">Goal: Learning a Model of Pose and Shape</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#the-challenge-of-registration">The Challenge of Registration</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#surface-representation-mesh">Surface Representation: Mesh</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#the-procrustes-alignment-problem-mathematical-formulation">The Procrustes Alignment Problem: Mathematical Formulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#rigid-transformations">Rigid Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#procrustes-alignment-solution">Procrustes Alignment Solution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#decoupling-translation-by-centroid-alignment">Decoupling Translation by Centroid Alignment</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#optimal-rotation-via-svd">Optimal Rotation via SVD</a><ul>
<li class="toctree-l4"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#reflection-adjustment">Reflection Adjustment</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#optimal-scale-optional">Optimal Scale (Optional)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#complete-mathematical-derivation">Complete Mathematical Derivation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#translation-derivation">Translation Derivation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#rotation-derivation">Rotation Derivation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#scale-derivation">Scale Derivation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#summary-of-procrustes-alignment-algorithm">Summary of Procrustes Alignment Algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#python-implementation-example">Python Implementation Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#practical-applications">Practical Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#interactive-visualization-ideas">Interactive Visualization Ideas</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_04_1_icp.html">Lecture 4.1: Iterative Closest Point</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#introduction-to-shape-alignment-and-registration">Introduction to Shape Alignment and Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#the-registration-problem">The Registration Problem</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#review-procrustes-analysis">Review: Procrustes Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#problem-unknown-correspondences">Problem: Unknown Correspondences</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#the-iterative-closest-point-icp-algorithm">The Iterative Closest Point (ICP) Algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#basic-icp-algorithm">Basic ICP Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#computational-considerations">Computational Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="lecture_04_1_icp.html#closest-point-computation">Closest Point Computation</a></li>
<li class="toctree-l4"><a class="reference internal" href="lecture_04_1_icp.html#convergence-and-local-minima">Convergence and Local Minima</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#point-to-point-vs-point-to-plane-icp">Point-to-Point vs. Point-to-Plane ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#point-to-point-icp">Point-to-Point ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#point-to-plane-icp">Point-to-Plane ICP</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#gradient-based-icp-for-non-rigid-registration">Gradient-based ICP for Non-Rigid Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#gradient-based-icp-algorithm">Gradient-based ICP Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#advantages-of-gradient-based-icp">Advantages of Gradient-based ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#computing-gradients">Computing Gradients</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#improving-icp-s-robustness">Improving ICP’s Robustness</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#data-association-direction">Data Association Direction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#robust-cost-functions">Robust Cost Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#trimmed-icp">Trimmed ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#ransac-based-approaches">RANSAC-based Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#additional-information">Additional Information</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#point-to-surface-distance">Point-to-Surface Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#icp-variants-and-extensions">ICP Variants and Extensions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#generalized-icp-gicp">Generalized ICP (GICP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#em-icp-and-probabilistic-approaches">EM-ICP and Probabilistic Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#coherent-point-drift-cpd">Coherent Point Drift (CPD)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#multi-scale-approaches">Multi-Scale Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#applications-of-icp">Applications of ICP</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#implementing-icp">Implementing ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#efficient-python-implementation">Efficient Python Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#practical-tips">Practical Tips</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_04_2_body_models.html">Lecture 04.2 - Body Models: Vertex-Based Models and SMPL</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#body-models-as-parameterized-functions">1. Body Models as Parameterized Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#rotations-articulation-and-pose-representation">2. Rotations, Articulation, and Pose Representation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#rotation-representation">2.1 Rotation Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#kinematic-chain">2.2 Kinematic Chain</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#linear-blend-skinning-and-its-limitations">3. Linear Blend Skinning and its Limitations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#linear-blend-skinning">3.1 Linear Blend Skinning</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#problems-with-standard-lbs">3.2 Problems with Standard LBS</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#blend-shapes-for-correcting-lbs-artifacts">3.3 Blend Shapes for Correcting LBS Artifacts</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#the-smpl-body-model">4. The SMPL Body Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#smpl-philosophy">4.1 SMPL Philosophy</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#smpl-model-architecture">4.2 SMPL Model Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#shape-blend-shapes">4.2.1 Shape Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#pose-blend-shapes">4.2.2 Pose Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#joint-regression">4.2.3 Joint Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#model-training">4.3 Model Training</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#comparison-with-scape">5. Comparison with SCAPE</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#the-scape-model">5.1 The SCAPE Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#different-approaches-to-deformation">5.2 Different Approaches to Deformation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#performance-comparison">5.3 Performance Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#other-advantages">5.4 Other Advantages</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#alignment-techniques-procrustes-analysis-and-icp">6. Alignment Techniques: Procrustes Analysis and ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#procrustes-analysis">6.1 Procrustes Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#iterative-closest-point-icp">6.2 Iterative Closest Point (ICP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#fitting-smpl-to-scans">6.3 Fitting SMPL to Scans</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#image-formation-and-the-pinhole-camera-model">7. Image Formation and the Pinhole Camera Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#the-pinhole-camera-model">7.1 The Pinhole Camera Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#lens-distortion">7.2 Lens Distortion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#extensions-and-advanced-applications">8. Extensions and Advanced Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#dynamic-soft-tissue-modeling">8.1 Dynamic Soft Tissue Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#specialized-extensions">8.2 Specialized Extensions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#deep-learning-for-model-fitting">8.3 Deep Learning for Model Fitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#probabilistic-approaches">8.4 Probabilistic Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#hybrid-models">8.5 Hybrid Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_05_1_body_model_training.html">Lecture 5.1 - Training a Body Model and Fitting SMPL to Scans</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#body-models-based-on-triangle-deformations">Body Models Based on Triangle Deformations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#scape-and-blendscape-models">SCAPE and BlendSCAPE Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#triangle-deformation-process">Triangle Deformation Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#comparison-smpl-vs-scape-blendscape">Comparison: SMPL vs. SCAPE/BlendSCAPE</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#training-a-body-model-from-registrations">Training a Body Model from Registrations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#the-challenge-of-raw-scan-data">The Challenge of Raw Scan Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#training-from-registrations">Training from Registrations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#obtaining-registrations-fitting-smpl-to-scans">Obtaining Registrations: Fitting SMPL to Scans</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#non-rigid-registration-process">Non-Rigid Registration Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#iterative-closest-point-icp-review">Iterative Closest Point (ICP) Review</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#registration-objective-formulation">Registration Objective Formulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#point-to-surface-distance">Point-to-Surface Distance</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#multi-stage-optimization-strategy">Multi-Stage Optimization Strategy</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#joint-registration-and-model-training">Joint Registration and Model Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#co-registration-approach">Co-Registration Approach</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_05_2_3d_registration.html">Lecture 05.2 - 3D Registration: From Classical ICP to Modern Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#rigid-registration-and-the-icp-algorithm">1. Rigid Registration and the ICP Algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#the-iterative-closest-point-icp-algorithm">The Iterative Closest Point (ICP) Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#convergence-analysis-and-failure-modes">Convergence Analysis and Failure Modes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#classical-non-rigid-registration">2. Classical Non-Rigid Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#thin-plate-spline-robust-point-matching-tps-rpm">Thin Plate Spline Robust Point Matching (TPS-RPM)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#coherent-point-drift-cpd">Coherent Point Drift (CPD)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#other-non-rigid-methods">Other Non-Rigid Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#parametric-models-and-the-smpl-body-model">3. Parametric Models and the SMPL Body Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#model-structure">Model Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#shape-blend-shapes-identity-variation">Shape Blend Shapes (Identity Variation)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#pose-blend-shapes-pose-dependent-deformation">Pose Blend Shapes (Pose-Dependent Deformation)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#linear-blend-skinning-lbs-for-articulation">Linear Blend Skinning (LBS) for Articulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#learning-smpl">Learning SMPL</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#using-smpl-for-registration">Using SMPL for Registration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#modeling-clothing-and-fine-details-smpl-d">4. Modeling Clothing and Fine Details: SMPL+D</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#why-smpl-d">Why SMPL+D?</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#how-displacements-are-applied">How Displacements Are Applied</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#limitations">Limitations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#applications">Applications</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#survey-of-3d-registration-methods-from-icp-to-deep-learning">5. Survey of 3D Registration Methods: From ICP to Deep Learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#early-pioneering-works-1990s-foundational-rigid-registration">5.1 Early Pioneering Works (1990s) – Foundational Rigid Registration</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#the-2000s-robust-and-non-rigid-registration-emerges">5.2 The 2000s – Robust and Non-Rigid Registration Emerges</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#s-template-based-and-parametric-model-registration">5.3 2010s – Template-based and Parametric Model Registration</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#s-learning-based-parametric-registration-and-hybrid-approaches">5.4 2020s – Learning-Based Parametric Registration and Hybrid Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html">Lecture 06.1 - Fitting the SMPL Model to Images via Optimization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#mathematical-background-pinhole-camera-and-projections">Mathematical Background: Pinhole Camera and Projections</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#perspective-projection">Perspective Projection</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#weak-perspective-projection">Weak-Perspective Projection</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#camera-extrinsics-vs-model-pose">Camera Extrinsics vs. Model Pose</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#d-keypoints-and-projection">2D Keypoints and Projection</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#the-smpl-model-as-a-differentiable-function-of-shape-and-pose">The SMPL Model as a Differentiable Function of Shape and Pose</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#shape-blend-shapes">Shape Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#pose-blend-shapes">Pose Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#joint-positions">Joint Positions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#linear-blend-skinning">Linear Blend Skinning</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#differentiability-of-smpl">Differentiability of SMPL</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#fitting-smpl-to-images-via-optimization-smplify">Fitting SMPL to Images via Optimization (SMPLify)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#objective-function">Objective Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#combined-objective">Combined Objective</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#optimization-strategy">Optimization Strategy</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#optimization-algorithms">Optimization Algorithms</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#automatic-differentiation-and-jacobians">Automatic Differentiation and Jacobians</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#result-of-smplify">Result of SMPLify</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#historical-progression-and-method-comparisons">Historical Progression and Method Comparisons</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#smplify-bogo-et-al-2016">SMPLify (Bogo et al. 2016)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#smplify-x-2019">SMPLify-X (2019)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html">Lecture 06.2 - Learning-Based Fitting of the SMPL Model to Images</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#foundations-of-learning-based-smpl-estimation">Foundations of Learning-Based SMPL Estimation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#integrating-smpl-into-neural-networks">Integrating SMPL into Neural Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#projection-functions">Projection Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#loss-functions">Loss Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#statistical-priors-and-adversarial-losses">Statistical Priors and Adversarial Losses</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#early-regression-approaches-hmr-and-nbf">Early Regression Approaches: HMR and NBF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#human-mesh-recovery-hmr">Human Mesh Recovery (HMR)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#neural-body-fitting-nbf">Neural Body Fitting (NBF)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#evolving-architectures-hybrid-and-improved-regression-methods">Evolving Architectures: Hybrid and Improved Regression Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#spin-optimization-in-the-training-loop">SPIN: Optimization in the Training Loop</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#pymaf-pyramidal-mesh-alignment-feedback">PyMAF: Pyramidal Mesh Alignment Feedback</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#cliff-using-full-frame-context-for-camera-orientation">CLIFF: Using Full-Frame Context for Camera Orientation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#pixie-whole-body-regression-with-part-experts">PIXIE: Whole-Body Regression with Part Experts</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#temporal-methods-from-single-images-to-video-sequences">Temporal Methods: From Single Images to Video Sequences</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#vibe-adversarial-motion-prior-with-grus">VIBE: Adversarial Motion Prior with GRUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#tcmr-temporally-consistent-mesh-recovery">TCMR: Temporally Consistent Mesh Recovery</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#motionbert-transformer-based-motion-representations">MotionBERT: Transformer-Based Motion Representations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#comparison-of-learning-based-methods">Comparison of Learning-Based Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#supervision-and-data">Supervision and Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#network-architecture">Network Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#objective-functions">Objective Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#pose-and-shape-priors">Pose and Shape Priors</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#performance">Performance</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#runtime">Runtime</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#strengths-and-weaknesses">Strengths and Weaknesses</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html">Lecture 07.1: Fitting SMPL to IMU Data Using Optimization-Based Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#classical-imu-based-pose-estimation-a-historical-perspective">Classical IMU-Based Pose Estimation: A Historical Perspective</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#attitude-and-heading-reference-systems">Attitude and Heading Reference Systems</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#kalman-filter-approaches">Kalman Filter Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#early-sparse-sensor-approaches">Early Sparse-Sensor Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#model-based-optimization-methods">Model-Based Optimization Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#learning-based-methods">Learning-Based Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#inertial-sensor-fundamentals-and-orientation-representations">Inertial Sensor Fundamentals and Orientation Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#gravity-alignment-and-drift-correction">Gravity Alignment and Drift Correction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#orientation-representations">Orientation Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#sensor-calibration">Sensor Calibration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#optimization-based-smpl-fitting-with-imu-data">Optimization-Based SMPL Fitting with IMU Data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#kinematic-model-and-sensor-prediction">Kinematic Model and Sensor Prediction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#regularization-and-prior-terms">Regularization and Prior Terms</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#gradient-and-jacobian-computation">Gradient and Jacobian Computation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#pseudocode-smpl-pose-estimation-from-imu-sequence">Pseudocode: SMPL Pose Estimation from IMU Sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#imu-based-human-pose-datasets-and-resources">IMU-Based Human Pose Datasets and Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#dip-imu-dataset-2018">DIP-IMU Dataset (2018)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#totalcapture-2017">TotalCapture (2017)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#amass-2019">AMASS (2019)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#other-datasets-and-resources">Other Datasets and Resources</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html">Lecture 07.2: Fitting SMPL to IMU Data Using Learning-Based Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#optimization-based-vs-learning-based-approaches">Optimization-Based vs. Learning-Based Approaches</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#learning-based-imu-to-pose-estimation-historical-overview-of-key-models">Learning-Based IMU-to-Pose Estimation: Historical Overview of Key Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#deep-inertial-poser-dip-2018">Deep Inertial Poser (DIP, 2018)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#transpose-2021">TransPose (2021)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#transformer-inertial-poser-tip-2022">Transformer Inertial Poser (TIP, 2022)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#physics-physical-inertial-poser-pip-2022">Physics/Physical Inertial Poser (PIP, 2022)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#other-notable-models-and-developments">Other Notable Models and Developments</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#problem-formulation-and-learning-task-definition">Problem Formulation and Learning Task Definition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#input-and-output-representations">Input and Output Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#learning-objective-and-loss-functions">Learning Objective and Loss Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#temporal-modeling-approaches">Temporal Modeling Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#supervised-vs-semi-supervised-training-synthetic-data">Supervised vs. Semi-Supervised Training; Synthetic Data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#model-architectures-and-design-considerations">Model Architectures and Design Considerations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#encoding-imu-measurements">Encoding IMU Measurements</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#network-structures">Network Structures</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#training-pipeline-and-pseudocode">Training Pipeline and Pseudocode</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#datasets-benchmarks-and-resources">Datasets, Benchmarks, and Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#dip-imu-dataset-2018">DIP-IMU Dataset (2018)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#totalcapture-2017">TotalCapture (2017)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#amass-2019">AMASS (2019)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#other-datasets-and-resources">Other Datasets and Resources</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#challenges-and-outlook">Challenges and Outlook</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html">Lecture 08.1: Vertex-Based Clothing Modeling for Virtual Humans</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#clothing-representation-as-vertex-displacements">Clothing Representation as Vertex Displacements</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#registration-of-clothed-human-scans">Registration of Clothed Human Scans</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#estimating-body-shape-under-clothing-the-buff-method">Estimating Body Shape Under Clothing: The BUFF Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#multi-layer-clothing-capture-the-clothcap-method">Multi-Layer Clothing Capture: The ClothCap Method</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#data-term">Data Term</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#boundary-term">Boundary Term</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#boundary-smoothness">Boundary Smoothness</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#laplacian-smoothness">Laplacian Smoothness</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#applications-of-multi-layer-registration">Applications of Multi-Layer Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#advantages-and-limitations">Advantages and Limitations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Lecture 09.1: Neural Implicit and Point-Based Representations for Clothed Human Modeling</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#background-explicit-vs-implicit-vs-point-based-representations">Background: Explicit vs. Implicit vs. Point-Based Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#mesh-based-models">Mesh-Based Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="#point-based-models">Point-Based Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="#neural-implicit-representations">Neural Implicit Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#neural-radiance-fields-nerfs-for-humans">Neural Radiance Fields (NeRFs) for Humans</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hybrid-approaches">Hybrid Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#expressiveness-and-topology">Expressiveness and Topology</a></li>
<li class="toctree-l2"><a class="reference internal" href="#differentiability-and-learning">Differentiability and Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#data-efficiency-and-performance">Data Efficiency and Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="#neural-implicit-function-foundations">Neural Implicit Function Foundations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#signed-distance-field-sdf">Signed Distance Field (SDF)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#occupancy-field-indicator-function">Occupancy Field (Indicator Function)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#volume-radiance-field">Volume Radiance Field</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#articulated-deformation-fields-for-implicit-models">Articulated Deformation Fields for Implicit Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#backward-warping-inverse-skinning-field">Backward Warping (Inverse Skinning Field)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#forward-warping-forward-skinning-field">Forward Warping (Forward Skinning Field)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pose-dependent-deformation-secondary-motion">Pose-Dependent Deformation (Secondary Motion)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#generative-implicit-models-for-clothed-bodies">Generative Implicit Models for Clothed Bodies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#smplicit-topology-aware-clothed-human-model">SMPLicit: Topology-Aware Clothed Human Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#imghum-implicit-generative-human-model">imGHUM: Implicit Generative Human Model</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#pose-dependent-implicit-models-and-animatable-avatars">Pose-Dependent Implicit Models and Animatable Avatars</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#nasa-neural-articulated-shape-approximation-eccv-2020">NASA: Neural Articulated Shape Approximation (ECCV 2020)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#scanimate-weakly-supervised-skinned-avatar-networks-cvpr-2021">SCANimate: Weakly-Supervised Skinned Avatar Networks (CVPR 2021)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#neural-gif-neural-generalized-implicit-functions-iccv-2021">Neural-GIF: Neural Generalized Implicit Functions (ICCV 2021)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#snarf-skinned-neural-articulated-implicit-shapes-iccv-2021">SNARF: Skinned Neural Articulated Implicit Shapes (ICCV 2021)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pop-the-power-of-points-iccv-2021-point-based-modeling">POP: The Power of Points (ICCV 2021) – Point-Based Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="#other-noteworthy-methods">Other Noteworthy Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#blueprint-algorithms-for-key-methods">Blueprint Algorithms for Key Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#snarf-training-procedure-differentiable-forward-skinning-for-implicit-surfaces">SNARF (Training Procedure): Differentiable Forward Skinning for Implicit Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pop-training-fitting-pipeline-point-based-model-for-pose-dependent-clothing">POP (Training &amp; Fitting Pipeline): Point-Based Model for Pose-Dependent Clothing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#smplicit-inference-fitting-workflow-generative-implicit-garment-model-conditioned-on-smpl">SMPLicit (Inference/Fitting Workflow): Generative Implicit Garment Model conditioned on SMPL</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#historical-perspective-and-future-outlook">Historical Perspective and Future Outlook</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#challenges-and-future-directions">Challenges and Future Directions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="extended_materials_neural_radiance_fields.html">Neural Radiance Fields: A Historical and Theoretical Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#foundations-3d-scene-representation-and-reconstruction-techniques">Foundations: 3D Scene Representation and Reconstruction Techniques</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#voxel-grids">Voxel Grids</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#point-clouds">Point Clouds</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#mesh-based-surfaces">Mesh-Based Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#light-fields-and-volumetric-rendering">Light Fields and Volumetric Rendering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#emergence-of-neural-radiance-fields-nerf">Emergence of Neural Radiance Fields (NeRF)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#core-idea">Core Idea</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#training-procedure">Training Procedure</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#nerf-architecture">NeRF Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#hierarchical-sampling">Hierarchical Sampling</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#original-results">Original Results</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#theoretical-and-mathematical-analysis-of-nerf">Theoretical and Mathematical Analysis of NeRF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#volume-rendering-formulation-in-nerf">Volume Rendering Formulation in NeRF</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#positional-encoding-and-neural-network-architecture">Positional Encoding and Neural Network Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#loss-function-and-optimization">Loss Function and Optimization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#major-advancements-and-extensions-of-nerf">Major Advancements and Extensions of NeRF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#anti-aliasing-and-unbounded-scenes-mip-nerf-and-nerf">Anti-Aliasing and Unbounded Scenes: mip-NeRF and NeRF++</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#efficiency-improvements-instant-nerf-and-plenoctrees">Efficiency Improvements: Instant NeRF and PlenOctrees</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#dynamic-and-deformable-nerfs-d-nerf-nerfies-nsff-etc">Dynamic and Deformable NeRFs (D-NeRF, Nerfies, NSFF, etc.)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#neural-radiance-fields-for-human-modeling-with-smpl-and-body-models">Neural Radiance Fields for Human Modeling (with SMPL and Body Models)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#other-notable-extensions">Other Notable Extensions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#comparison-with-other-3d-representations">Comparison with Other 3D Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-polygonal-meshes">Vs. Polygonal Meshes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-point-clouds-3d-splatting">Vs. Point Clouds / 3D Splatting</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-voxel-grids-and-volumetric-methods">Vs. Voxel Grids and Volumetric Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-multi-plane-images-mpis-light-fields">Vs. Multi-Plane Images (MPIs) / Light Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#accuracy-and-fidelity">Accuracy and Fidelity</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#applicability">Applicability</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#datasets-for-nerf-training-and-evaluation">Datasets for NeRF Training and Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#blender-synthetic-nerf-dataset">Blender Synthetic NeRF Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#local-light-field-fusion-llff-real-forward-facing-dataset">Local Light Field Fusion (LLFF) Real Forward-Facing Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#tanks-and-temples">Tanks and Temples</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#dtu-dataset">DTU Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#human3-6m">Human3.6M</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#zju-mocap-dataset">ZJU-MoCap Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#people-snapshot">People-Snapshot</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#synthetic-dynamic-scenes">Synthetic dynamic scenes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#other-datasets">Other datasets</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html">3D Gaussian Splatting: A Basic Introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#foundations">Foundations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#what-is-a-3d-scene">What is a 3D Scene?</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-scene-representation">3D Scene Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#computer-graphics-fundamentals">Computer Graphics Fundamentals</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#rasterization-and-ray-tracing">Rasterization and Ray Tracing</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#alpha-blending-and-compositing">Alpha Blending and Compositing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#the-evolution-of-novel-view-synthesis">The Evolution of Novel View Synthesis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#image-based-rendering">Image-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#structure-from-motion-and-multi-view-stereo">Structure-from-Motion and Multi-View Stereo</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#point-based-rendering">Point-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#neural-rendering">Neural Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#accelerated-neural-fields">Accelerated Neural Fields</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussian-splatting-a-convergence-of-approaches">3D Gaussian Splatting: A Convergence of Approaches</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#id1">Point-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#point-clouds-and-their-challenges">Point Clouds and Their Challenges</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#the-concept-of-splatting">The Concept of Splatting</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#elliptical-weighted-average-ewa-filtering">Elliptical Weighted Average (EWA) Filtering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-point-based-rendering">Differentiable Point-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussian-splatting-core-principles">3D Gaussian Splatting: Core Principles</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#key-insight-unifying-points-and-volumes">Key Insight: Unifying Points and Volumes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussians-as-scene-primitives">3D Gaussians as Scene Primitives</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#the-volumetric-rendering-equation">The Volumetric Rendering Equation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#alpha-compositing-with-gaussians">Alpha Compositing with Gaussians</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#mathematical-formulation-of-3d-gaussian-splatting">Mathematical Formulation of 3D Gaussian Splatting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#projecting-3d-gaussians-to-2d">Projecting 3D Gaussians to 2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#parameterization-of-3d-gaussians">Parameterization of 3D Gaussians</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#view-dependent-appearance">View-Dependent Appearance</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-rendering-equations">Differentiable Rendering Equations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#training-and-optimization">Training and Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#photometric-loss">Photometric Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#initial-point-cloud">Initial Point Cloud</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#optimization-process">Optimization Process</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-splatting-pipeline">Differentiable Splatting Pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#adaptive-density-control">Adaptive Density Control</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#implementation-and-real-time-rendering">Implementation and Real-Time Rendering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#tile-based-rendering">Tile-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#fast-sorting-strategies">Fast Sorting Strategies</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#gpu-accelerated-rasterization">GPU-Accelerated Rasterization</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#memory-considerations">Memory Considerations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#comparison-with-other-methods">Comparison with Other Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#nerf-vs-3d-gaussian-splatting">NeRF vs. 3D Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#voxel-based-representations-vs-gaussians">Voxel-Based Representations vs. Gaussians</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#traditional-point-based-rendering-vs-gaussian-splatting">Traditional Point-Based Rendering vs. Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#applications-and-extensions">Applications and Extensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#static-scene-reconstruction">Static Scene Reconstruction</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#dynamic-scene-capture">Dynamic Scene Capture</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#avatar-creation-and-animation">Avatar Creation and Animation</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#integration-with-neural-rendering">Integration with Neural Rendering</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#large-scale-scene-rendering">Large-Scale Scene Rendering</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#bezier-gaussian-triangles-bg-triangle-for-sharper-rendering">Bézier Gaussian Triangles (BG-Triangle) for Sharper Rendering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#representation">Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#performance">Performance</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#human-reconstruction-with-gaussian-splatting-and-priors">Human Reconstruction with Gaussian Splatting and Priors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#eg-humannerf-efficient-generalizable-human-nerf">EG-HumanNeRF: Efficient Generalizable Human NeRF</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#gps-gaussian-pixel-wise-gaussian-splatting-for-humans">GPS-Gaussian: Pixel-Wise Gaussian Splatting for Humans</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#generalizable-human-gaussians-ghg-with-smpl">Generalizable Human Gaussians (GHG) with SMPL</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#dynamic-scene-reconstruction-with-4d-gaussian-splatting">Dynamic Scene Reconstruction with 4D Gaussian Splatting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussian-splatting-4dgs">4D Gaussian Splatting (4DGS)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#speed-and-memory-enhancements-4dgs-1k-and-mega">Speed and Memory Enhancements (4DGS-1K and MEGA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#applications-to-mocap-and-4d-human-rendering">Applications to MoCap and 4D Human Rendering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#implementation-details-and-real-time-performance">Implementation Details and Real-Time Performance</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#data-structures">Data Structures</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#rasterization-shaders">Rasterization &amp; Shaders</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#gpu-memory-and-throughput">GPU Memory and Throughput</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-rendering-implementation">Differentiable Rendering Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#training-vs-inference-compute">Training vs. Inference Compute</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#accuracy-vs-speed-trade-offs">Accuracy vs. Speed trade-offs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#benchmarks-and-comparative-evaluation">Benchmarks and Comparative Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#static-scene-comparison">Static Scene Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#human-novel-view-comparison">Human Novel-View Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#dynamic-scene-comparison">Dynamic Scene Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#comparative-summary-table">Comparative Summary Table</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#datasets-and-resources">Datasets and Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#synthetic-nerf-dataset-blender-scenes">Synthetic NeRF Dataset (Blender Scenes)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#llff-local-light-field-fusion">LLFF (Local Light Field Fusion)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#tanks-and-temples">Tanks and Temples</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#multi-object-360-co3d-common-objects-in-3d">Multi-Object 360 (CO3D - Common Objects in 3D)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#amass-archive-of-motion-capture-as-surface-shapes">AMASS (Archive of Motion Capture as Surface Shapes)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#cape-clothed-auto-person-encoding">CAPE (Clothed Auto Person Encoding)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#thuman-thuman2-0">THuman / THuman2.0</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#renderpeople">RenderPeople</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#open-source-implementations">Open-Source Implementations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#future-directions">Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#improved-compression-techniques-for-memory-efficiency">Improved Compression Techniques for Memory Efficiency</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#handling-dynamic-and-deformable-scenes">Handling Dynamic and Deformable Scenes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#advanced-material-modeling-for-realistic-rendering">Advanced Material Modeling for Realistic Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#hybrid-approaches-integrating-neural-fields-and-explicit-representations">Hybrid Approaches Integrating Neural Fields and Explicit Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#scalability-for-large-scale-scenes-city-level-and-beyond">Scalability for Large-Scale Scenes (City-Level and Beyond)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#real-time-applications-in-ar-vr-and-gaming">Real-Time Applications in AR/VR and Gaming</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#integration-with-existing-graphics-pipelines">Integration with Existing Graphics Pipelines</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#learning-from-limited-data">Learning from Limited Data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#conclusion">Conclusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#glossary">Glossary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a><ul>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-01-1-historical-body-models">Lecture 01.1 (Historical Body Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-01-2-introduction-to-human-models">Lecture 01.2 (Introduction to Human Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-01-3-introduction-to-human-models-continued">Lecture 01.3 (Introduction to Human Models Continued)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-02-1-image-formation">Lecture 02.1 (Image Formation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-02-2-rotations-kinematic-chains">Lecture 02.2 (Rotations &amp; Kinematic Chains)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-03-1-surface-representations">Lecture 03.1 (Surface Representations)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-03-2-procrustes-alignment">Lecture 03.2 (Procrustes Alignment)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-04-1-iterative-closest-points">Lecture 04.1 (Iterative Closest Points)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-04-2-body-models">Lecture 04.2 (Body Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-05-1-body-model-training">Lecture 05.1 (Body Model Training)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-05-2-3d-registration">Lecture 05.2 (3D Registration)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-06-1-fitting-smpl-to-images">Lecture 06.1 (Fitting SMPL to Images)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-06-1-optimization-based-fitting-of-smpl-to-images">Lecture 06.1 (Optimization-Based Fitting of SMPL to Images)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-06-2-learning-based-fitting-of-smpl-to-images">Lecture 06.2 (Learning-Based Fitting of SMPL to Images)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-07-1-fitting-smpl-to-imu-optimization">Lecture 07.1 (Fitting SMPL to IMU Optimization)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-07-2-fitting-smpl-to-imu-learning">Lecture 07.2 (Fitting SMPL to IMU Learning)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="references.html#classic-and-optimization-based-methods">Classic and Optimization-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="references.html#learning-based-methods">Learning-Based Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="references.html#datasets-and-resources">Datasets and Resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#relevant-software-and-libraries">Relevant Software and Libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-08-1-references-for-vertex-based-clothing-modeling-for-virtual-humans">Lecture 08.1: References for Vertex-Based Clothing Modeling for Virtual Humans</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#body-models-and-shape-estimation">Body Models and Shape Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#alternative-representations">Alternative Representations</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#datasets">Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#software-and-libraries">Software and Libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-09-1-references-for-neural-implicit-and-point-based-representations-for-clothed-human-modeling">Lecture 09.1: References for Neural Implicit and Point-Based Representations for Clothed Human Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#point-based-models">Point-Based Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#id50">Datasets and Resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#related-software-and-libraries">Related Software and Libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#review-papers-and-tutorials">Review Papers and Tutorials</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#neural-radiance-fields-nerf">Neural Radiance Fields (NERF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#gaussian-splatting">Gaussian Splatting</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Virtual Humans Lecture</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Lecture 09.1: Neural Implicit and Point-Based Representations for Clothed Human Modeling</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/lecture_09_1_neual_implicit_and_point_based_clothing_models.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="lecture-09-1-neural-implicit-and-point-based-representations-for-clothed-human-modeling">
<span id="lecture-implicit-point-clothing"></span><h1>Lecture 09.1: Neural Implicit and Point-Based Representations for Clothed Human Modeling<a class="headerlink" href="#lecture-09-1-neural-implicit-and-point-based-representations-for-clothed-human-modeling" title="Link to this heading"></a></h1>
<iframe width="600" height="400" src="https://www.youtube.com/embed/n0TraTefJo4"
title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write;
encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><p><a class="reference external" href="https://virtualhumans.mpi-inf.mpg.de/VH23/slides/pdf/Lecture_09_1_Neural_Implicits_PointBased.pdf">Lecture Slides: Neural Implicit and Point-Based Representations for Clothed Human Modeling</a></p>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h2>
<p>Modeling clothed human bodies in 3D is a longstanding challenge in computer vision and graphics. Traditional approaches often rely on mesh-based models like SMPL (Skinned Multi-Person Linear model), which represent the human body with a fixed-topology triangulated mesh and blend skinning for animation. While mesh models are efficient and interpretable, they struggle to capture the complex geometry and varying topology of clothing (e.g., loose garments, layers) without significant manual effort or template modifications.</p>
<p>Recent advances have introduced alternative representations – notably neural implicit functions and point-based representations – which promise greater flexibility in representing high-detail, varying-topology surfaces like clothing. This lecture explores these emerging representations for clothed human modeling, including signed distance fields (SDFs), occupancy fields, Neural Radiance Fields (NeRFs) adapted to humans, and point-based methods. We compare these approaches to traditional mesh models in terms of expressiveness, differentiability, data efficiency, and animation performance, examining both their theoretical foundations and practical applications.</p>
</section>
<section id="background-explicit-vs-implicit-vs-point-based-representations">
<h2>Background: Explicit vs. Implicit vs. Point-Based Representations<a class="headerlink" href="#background-explicit-vs-implicit-vs-point-based-representations" title="Link to this heading"></a></h2>
<p>To understand the landscape of 3D human modeling approaches, we first need to differentiate between the main categories of representations.</p>
<section id="mesh-based-models">
<h3>Mesh-Based Models<a class="headerlink" href="#mesh-based-models" title="Link to this heading"></a></h3>
<p>Meshes represent surfaces by a set of vertices connected in a fixed topology. For human bodies, parametric mesh models (e.g., SMPL) provide low-dimensional controls for shape and pose, and clothing can be added via displacements on the base body mesh.</p>
<p><strong>Advantages:</strong>
- Intuitive and compatible with existing graphics pipelines
- Explicit surface definition makes collision handling and rendering straightforward
- Efficient skinning methods (e.g., Linear Blend Skinning) enable fast animation</p>
<p><strong>Limitations:</strong>
- Fixed genus/topology makes loose garments difficult to model without cutting/stitching mesh parts
- Fixed resolution (number of vertices) limits detail without increasing memory requirements
- Handling complex cloth deformations or topological changes is cumbersome
- Self-intersections become more likely with higher vertex counts</p>
<p>Despite these limitations, mesh models remain widely used due to their compatibility with existing tools and efficient animation properties.</p>
</section>
<section id="point-based-models">
<h3>Point-Based Models<a class="headerlink" href="#point-based-models" title="Link to this heading"></a></h3>
<p>Point clouds represent surfaces as an unstructured set of points in 3D (often with normals or colors). They can be seen as an intermediate between explicit and implicit representations.</p>
<p><strong>Advantages:</strong>
- Can capture arbitrary topology and fine detail (each point samples the surface geometry)
- No connectivity requirements allow representation of complex clothing geometries (holes, layers)
- Compatible with neural networks (e.g., set or convolutional architectures)
- Points can store local features encoding shape or deformation properties</p>
<p><strong>Limitations:</strong>
- Lack of connectivity means surfaces are “hollow” collections of points
- Special techniques (e.g., splatting or meshing) needed for rendering or collision detection
- Resolution is finite, though one can sample many points for high detail</p>
<p>Point-based representations have gained interest as they can achieve high resolution and topological flexibility like implicit functions, but with easier integration into standard graphics pipelines.</p>
</section>
<section id="neural-implicit-representations">
<h3>Neural Implicit Representations<a class="headerlink" href="#neural-implicit-representations" title="Link to this heading"></a></h3>
<p>Implicit representations define 3D surfaces as the level sets of a continuous function (typically parameterized by a neural network) defined over <span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span>. Two common implicit functions are signed distance fields (SDFs) and occupancy fields.</p>
<p><strong>Advantages:</strong>
- Represent arbitrary topologies since level sets can split or merge as needed
- Continuous representation not tied to a particular resolution
- Differentiable with respect to geometry, enabling optimization
- Can capture fine details with sufficient network capacity</p>
<p><strong>Limitations:</strong>
- Computationally heavy to train and evaluate
- Extracting a mesh requires expensive operations (e.g., Marching Cubes)
- Collision detection or physical simulation is non-trivial without conversion to explicit form</p>
<p>Neural implicit models represent surfaces implicitly as the zero-level set of a neural function <span class="math notranslate nohighlight">\(f_\theta(\mathbf{x})\)</span>. For example, an occupancy network outputs a probability of point <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> being inside the object; the surface is the decision boundary. An SDF network outputs the signed distance to the surface, with the surface at <span class="math notranslate nohighlight">\(f_\theta(\mathbf{x})=0\)</span>.</p>
</section>
<section id="neural-radiance-fields-nerfs-for-humans">
<h3>Neural Radiance Fields (NeRFs) for Humans<a class="headerlink" href="#neural-radiance-fields-nerfs-for-humans" title="Link to this heading"></a></h3>
<p>NeRF is a special implicit volumetric representation originally for view synthesis – a network <span class="math notranslate nohighlight">\(F_\theta(\mathbf{x},\mathbf{d})\)</span> outputs the color and density at any 3D point <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> (for a ray in direction <span class="math notranslate nohighlight">\(\mathbf{d}\)</span>), and rendering is done by volumetric integration.</p>
<p>For human modeling, NeRFs have been extended to handle dynamic articulation by conditioning on pose or learning deformation fields. While NeRFs primarily target appearance, the learned density essentially represents the shape implicitly. NeRF-based models can capture realistic details of clothing (texture, fine wrinkles) and allow novel view rendering. However, extracting geometry from a NeRF is an extra step (e.g., by thresholding density).</p>
<p>This topic is covered in more detail in the next section as well as the supplementary materials <a class="reference internal" href="extended_materials_neural_radiance_fields.html"><span class="doc">Extended Materials: Neural Radiance Fields</span></a>.</p>
</section>
<section id="hybrid-approaches">
<h3>Hybrid Approaches<a class="headerlink" href="#hybrid-approaches" title="Link to this heading"></a></h3>
<p>Some state-of-the-art methods combine representations. For example, SCARF (SIGGRAPH Asia 2022) is a hybrid of an explicit body mesh (for the human body) and a neural radiance field for the clothing layer. By integrating a deformable mesh for the body into the NeRF optimization, SCARF achieves a representation where the body pose and shape are controlled by the mesh, and the clothing (which can have complex geometry) is learned implicitly.</p>
<p>Such hybrid models can be optimized from monocular video directly (using differentiable rendering) and can even transfer clothing between subjects. This illustrates that the lines between representation types are often blurred in practice – e.g., one can use an implicit model for cloth on top of an explicit body.</p>
</section>
</section>
<section id="expressiveness-and-topology">
<h2>Expressiveness and Topology<a class="headerlink" href="#expressiveness-and-topology" title="Link to this heading"></a></h2>
<p>A key criterion is how well the representation captures the geometry complexity of clothed people. Meshes with fixed topology struggle to represent clothes that are not snug to the body – e.g., pants and shirts require different mesh connectivities. Traditional approaches created separate garment templates (e.g., a shirt template, a pants template) and learned deformation on each, making it hard to handle new garment types or multiple layers simultaneously.</p>
<p>Neural implicit functions, by contrast, do not require a template mesh and can seamlessly represent different topologies within one model. The SMPLicit model by Corona et al. demonstrates this: it jointly represents a wide variety of garment types (T-shirts, hoodies, jackets, skirts) with a single implicit function, whereas earlier methods needed one model per garment type.</p>
<p>Point-based models can also capture varying topology in a single representation because points are not connected – POP, for example, is a “cross-outfit” model learned from many outfits and can animate an outfit of arbitrary style after fitting its point cloud.</p>
<p>Thus, in terms of expressiveness and topology: implicit and point-based representations offer greater flexibility than single-template meshes, enabling one model to cover diverse clothing geometries.</p>
</section>
<section id="differentiability-and-learning">
<h2>Differentiability and Learning<a class="headerlink" href="#differentiability-and-learning" title="Link to this heading"></a></h2>
<p>Mesh models are explicit and often low-dimensional (e.g., SMPL has ~10 shape parameters, 69 pose parameters), so they are very data-efficient for minimal-clothed bodies and easy to fit with optimization. However, to capture clothing, mesh approaches either treat cloth as separate high-dimensional geometry (hard to learn without templates) or as displacements on the body (limited to tight clothing).</p>
<p>Neural implicit models, being high-dimensional function representations, typically require more data (e.g., many 3D scans or images) to train. But once trained, they are fully differentiable with respect to shape/pose parameters or even the input geometry itself. For instance, SMPLicit learns a latent space for clothing shape that is semantically interpretable (controlling garment size, length, etc.), and the whole pipeline is differentiable, allowing fitting to scans or images via gradient descent.</p>
<p>Point-based models like POP also require training on many scans to learn general clothing deformation behavior. A big advantage of neural representations is that they can generalize or interpolate; e.g., Neural-GIF and SNARF can generalize to novel poses not seen in training, something a simple blendshapes rig might not handle without explicit pose corrective data.</p>
<p>In terms of differentiability:
- The function <span class="math notranslate nohighlight">\(f_\theta(\mathbf{x})\)</span> in implicit models is differentiable w.r.t <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\theta\)</span>, enabling gradient-based algorithms for inverse problems
- Meshes have discrete vertices, so gradients exist w.r.t vertex positions but the topology is not editable
- Point clouds are somewhat differentiable – one can move points by gradients – but lack a continuous surface definition</p>
<p>Many modern methods combine neural implicits with gradient-based fitting. For example, when SMPLicit is fit to a new scan, the algorithm optimizes the clothing latent code and the implicit surface such that the unsigned distance field matches the scan points. This is possible because the model is end-to-end differentiable. In contrast, fitting a cloth template mesh might require non-differentiable operations like re-meshing if topology differs.</p>
</section>
<section id="data-efficiency-and-performance">
<h2>Data Efficiency and Performance<a class="headerlink" href="#data-efficiency-and-performance" title="Link to this heading"></a></h2>
<p>Mesh parametric models are extremely data-efficient for the shapes they represent (e.g., SMPL was learned from ~1700 body scans but generalizes well to new people’s body shapes within that distribution). However, to extend them to clothing (which has high variability), collecting a similar parametric basis is difficult.</p>
<p>Neural implicit models can absorb much more data. Many recent works train on tens of thousands of scans or synthetic data (e.g., SMPLicit leverages the CAPE dataset and others). Training these models is computationally intensive (often requiring days on GPUs). Additionally, querying an implicit surface can also be slower than a mesh: e.g., to render or simulate, one might need to evaluate the network many times.</p>
<p>Point-based models like POP can be faster at inference: since the surface is represented by a fixed set of points, rendering as surfels or point clouds is relatively quick, and animation involves moving points which can be done in parallel.</p>
<p>For animation and dynamic performance:
- Meshes with LBS are extremely fast to pose – just apply bone transforms to each vertex (linear complexity in number of vertices)
- Neural implicit avatars require either solving for correspondences (SNARF’s root-finding for each query point) or warping a grid (Neural-GIF’s deformation field) which is slower
- Point-based models can also be animated efficiently if each point stores skinning weights associated to the body</p>
<p>Some methods precompute structured latent grids or use spatial data structures to speed up querying implicit surfaces (e.g., octrees or multi-resolution feature grids in convolutional methods).</p>
<p>In summary, neural implicit models offer unparalleled flexibility and detail at the cost of training complexity and runtime, whereas point-based models offer a middle ground with high flexibility and easier integration, and mesh models are the fastest and most convenient for certain tasks but lack the capacity to represent complex clothing geometry.</p>
</section>
<section id="neural-implicit-function-foundations">
<h2>Neural Implicit Function Foundations<a class="headerlink" href="#neural-implicit-function-foundations" title="Link to this heading"></a></h2>
<p>Neural implicit functions represent surfaces by a continuous field <span class="math notranslate nohighlight">\(f(\mathbf{x})\)</span> defined for any 3D point <span class="math notranslate nohighlight">\(\mathbf{x}=(x,y,z)\in\mathbb{R}^3\)</span>. The surface is a particular level-set of <span class="math notranslate nohighlight">\(f\)</span>. We focus on three boundary types:</p>
<section id="signed-distance-field-sdf">
<h3>Signed Distance Field (SDF)<a class="headerlink" href="#signed-distance-field-sdf" title="Link to this heading"></a></h3>
<p><span class="math notranslate nohighlight">\(f(\mathbf{x})\)</span> outputs the signed distance to the surface. By convention:
- <span class="math notranslate nohighlight">\(f(\mathbf{x})&lt;0\)</span> if <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is inside the object
- <span class="math notranslate nohighlight">\(f(\mathbf{x})&gt;0\)</span> if outside
- <span class="math notranslate nohighlight">\(f(\mathbf{x})=0\)</span> exactly on the surface</p>
<p>Thus <span class="math notranslate nohighlight">\(S = \{\mathbf{x} \mid f(\mathbf{x})=0\}\)</span>. The classic SDF has the property <span class="math notranslate nohighlight">\(|\nabla f|=1\)</span> almost everywhere (the gradient’s magnitude equals 1 in continuous space), which is the <strong>Eikonal equation</strong> satisfied by distance functions.</p>
<p>DeepSDF (Park et al., 2019) introduced using a neural network to represent such a continuous SDF for a class of shapes. In DeepSDF, a decoder network <span class="math notranslate nohighlight">\(f_\theta(\mathbf{x}, \mathbf{z})\)</span> takes spatial point <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and a latent code <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> (representing a specific shape instance) and outputs an SDF value. This enables learning a family of shapes. In a human context, <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> could represent different identities or clothing styles.</p>
<p>The surface can be extracted via root-finding (find <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> such that <span class="math notranslate nohighlight">\(f_\theta(\mathbf{x},\mathbf{z})=0\)</span>) or by sampling a grid and applying Marching Cubes. Importantly, SDFs allow easy normal computation: at any surface point, the gradient <span class="math notranslate nohighlight">\(\nabla_\mathbf{x} f\)</span> is the outward normal.</p>
<p>Some methods use unsigned distance fields (UDFs) instead, where <span class="math notranslate nohighlight">\(u(\mathbf{x}) \ge 0\)</span> is the distance to the surface without an inside/outside distinction. UDFs are helpful when the concept of “inside” vs “outside” is ambiguous (e.g., clothing layers).</p>
</section>
<section id="occupancy-field-indicator-function">
<h3>Occupancy Field (Indicator Function)<a class="headerlink" href="#occupancy-field-indicator-function" title="Link to this heading"></a></h3>
<p><span class="math notranslate nohighlight">\(f(\mathbf{x})\)</span> outputs <span class="math notranslate nohighlight">\(1\)</span> if <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is inside the volume of the object, and <span class="math notranslate nohighlight">\(0\)</span> if outside (for a binary occupancy field). In practice, most use a continuous occupancy probability or logit in [0,1]. The surface is implicitly where the occupancy probability drops from inside to outside.</p>
<p>Occupancy Networks (Mescheder et al., 2019) popularized learning a neural implicit surface this way. They treat the network as a classifier: <span class="math notranslate nohighlight">\(f_\theta(\mathbf{x})\)</span> = probability <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is inside the object. During training, they sample points from mesh data and label them 1 (inside) or 0 (outside) and use a binary cross-entropy loss.</p>
<p>One big advantage is that you don’t need ground-truth distance values (just an inside/outside label, which is easier to get from a mesh or scan). However, occupancy networks are not as straightforward to convert to a distance or to get normals (though one can approximate normals by the gradient of the logit field).</p>
</section>
<section id="volume-radiance-field">
<h3>Volume Radiance Field<a class="headerlink" href="#volume-radiance-field" title="Link to this heading"></a></h3>
<p>Although NeRFs primarily target novel view synthesis, from a modeling perspective a NeRF learns a function <span class="math notranslate nohighlight">\(F(\mathbf{x},\mathbf{d}) = (\sigma, C)\)</span> giving a volume density <span class="math notranslate nohighlight">\(\sigma\)</span> (how much volume/matter is at point <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>) and color <span class="math notranslate nohighlight">\(C\)</span> for a ray hitting <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> from direction <span class="math notranslate nohighlight">\(\mathbf{d}\)</span>.</p>
<p>The object’s shape is represented by the density field <span class="math notranslate nohighlight">\(\sigma(\mathbf{x})\)</span>: surfaces manifest as regions where <span class="math notranslate nohighlight">\(\sigma\)</span> changes (ideally, a surface would be an infinitesimally thin shell of high density). Rendering is done by integrating along camera rays and using the optical model:</p>
<div class="math notranslate nohighlight">
\[I(\mathbf{r})=\int_0^{\infty} T(t)\sigma(\mathbf{r}(t))C(\mathbf{r}(t),\mathbf{d})dt\]</div>
<p>where <span class="math notranslate nohighlight">\(T(t)=\exp(-\int_0^t \sigma(\mathbf{r}(s))ds)\)</span> is transmittance.</p>
<p>For human modeling, one approach is to incorporate pose into <span class="math notranslate nohighlight">\(\sigma\)</span>’s definition. A common technique (as in HumanNeRF and related works) is to define the density in a canonical pose, and learn a deformation field that maps each query point in deformed (posed) space back to the canonical space before evaluating <span class="math notranslate nohighlight">\(F\)</span>.</p>
<p>The result is the ability to generate novel views for any frame (pose) of the video. Geometrically, a NeRF can be seen as an implicit surface at any desired threshold of density (some methods extract meshes from NeRFs by picking a density threshold and running Marching Cubes on the learned <span class="math notranslate nohighlight">\(\sigma(\mathbf{x})\)</span>).</p>
</section>
</section>
<section id="articulated-deformation-fields-for-implicit-models">
<h2>Articulated Deformation Fields for Implicit Models<a class="headerlink" href="#articulated-deformation-fields-for-implicit-models" title="Link to this heading"></a></h2>
<p>A central challenge in applying implicit representations to humans is handling articulation – the human body moves with many degrees of freedom, and clothing deforms with those movements. In a mesh model like SMPL, articulation is handled by Linear Blend Skinning (LBS): each vertex is attached to the skeleton with precomputed skinning weights, and given a pose (joint angles), one computes the transformation of each bone and blends them to find the new vertex position.</p>
<p>Two general strategies have emerged for implicit models:</p>
<section id="backward-warping-inverse-skinning-field">
<h3>Backward Warping (Inverse Skinning Field)<a class="headerlink" href="#backward-warping-inverse-skinning-field" title="Link to this heading"></a></h3>
<p>This approach defines a function <span class="math notranslate nohighlight">\(W^{-1}_\theta: \text{deformed space} \to \text{canonical space}\)</span> that maps any point <span class="math notranslate nohighlight">\(\mathbf{p}'\)</span> in the posed space back to a point <span class="math notranslate nohighlight">\(\mathbf{p}\)</span> in the canonical space (e.g., T-pose) given pose <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>To query if <span class="math notranslate nohighlight">\(\mathbf{p}'\)</span> is on the surface in pose <span class="math notranslate nohighlight">\(\theta\)</span>, we map it back to canonical space, get <span class="math notranslate nohighlight">\(\mathbf{p}=W^{-1}_\theta(\mathbf{p}')\)</span>, and then evaluate the implicit function <span class="math notranslate nohighlight">\(f_\text{canonical}(\mathbf{p})\)</span>. The equation for the surface in posed space would be:</p>
<div class="math notranslate nohighlight">
\[\{\mathbf{p}' \mid f_\text{canonical}(W^{-1}_\theta(\mathbf{p}')) = 0\}\]</div>
<p>Many early works (like NASA or the first implicit clothed models) opted to directly learn <span class="math notranslate nohighlight">\(W^{-1}_\theta\)</span>. That is, they train a network that given <span class="math notranslate nohighlight">\((\mathbf{p}', \theta)\)</span> predicts <span class="math notranslate nohighlight">\(\mathbf{p} = W^{-1}_\theta(\mathbf{p}')\)</span>. This network effectively learns the occupancy or SDF field in canonical space and how that field moves with pose.</p>
<p>For example, Neural-GIF uses an inverse map: it maps every query point in posed space to canonical, applies a learned deformation (for non-rigid cloth deformations), then evaluates the SDF. In Neural-GIF, the mapping is factorized: first an inverse LBS using predicted skinning weights to get an initial <span class="math notranslate nohighlight">\(\mathbf{p}\)</span>, then a learned displacement <span class="math notranslate nohighlight">\(\delta \mathbf{p}(\mathbf{p}, \theta)\)</span> in canonical space to account for pose-dependent offsets (like cloth wrinkles or body bulging).</p>
<p>Backward warping is intuitive and directly aligns with data: given a point on a posed scan, you can try to map it to canonical and enforce the occupancy to be inside. Its weakness is that <span class="math notranslate nohighlight">\(W^{-1}_\theta\)</span> itself is pose-dependent (a different function for each pose), making it hard to generalize to poses not in the training set.</p>
</section>
<section id="forward-warping-forward-skinning-field">
<h3>Forward Warping (Forward Skinning Field)<a class="headerlink" href="#forward-warping-forward-skinning-field" title="Link to this heading"></a></h3>
<p>Instead of mapping <span class="math notranslate nohighlight">\(\mathbf{p}' \to \mathbf{p}\)</span>, we define a field in canonical space that tells us where each canonical point goes when posed. Formally, <span class="math notranslate nohighlight">\(W_\theta: \text{canonical space} \to \text{deformed space}\)</span>, e.g., given a canonical point <span class="math notranslate nohighlight">\(\mathbf{p}\)</span> (in T-pose coordinates), <span class="math notranslate nohighlight">\(W_\theta(\mathbf{p}) = \mathbf{p}'\)</span> is its location under pose <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>With a forward map, the surface in posed space can be obtained by warping the entire canonical surface:</p>
<div class="math notranslate nohighlight">
\[S' = \{W_\theta(\mathbf{p}) \mid \mathbf{p} \in S_{\text{canonical}}\}\]</div>
<p>But to query whether a given <span class="math notranslate nohighlight">\(\mathbf{p}'\)</span> is on the surface, we need to find if there exists some <span class="math notranslate nohighlight">\(\mathbf{p}\)</span> such that <span class="math notranslate nohighlight">\(\mathbf{p}' = W_\theta(\mathbf{p})\)</span> and <span class="math notranslate nohighlight">\(f_\text{canonical}(\mathbf{p})=0\)</span>. That requires solving <span class="math notranslate nohighlight">\(\mathbf{p} = W_\theta^{-1}(\mathbf{p}')\)</span> anyway.</p>
<p>The key is to make <span class="math notranslate nohighlight">\(W_\theta\)</span> simpler (pose-independent) so that its inverse can be solved numerically in a stable way. SNARF (Chen et al., ICCV 2021) is the landmark method that does this. SNARF defines a forward skinning field in canonical space: basically, a set of skinning weights <span class="math notranslate nohighlight">\(w_b(\mathbf{p})\)</span> for each canonical coordinate <span class="math notranslate nohighlight">\(\mathbf{p}\)</span>, similar to SMPL’s weights but continuous. These weights are pose-independent (a function of <span class="math notranslate nohighlight">\(\mathbf{p}\)</span> only).</p>
<p>Given a pose (bone transforms <span class="math notranslate nohighlight">\(G_b(\theta)\)</span>), the forward warp is:</p>
<div class="math notranslate nohighlight">
\[W_\theta(\mathbf{p}) = \sum_b w_b(\mathbf{p}) (G_b(\theta)\cdot [\mathbf{p};1])_{1:3}\]</div>
<p>This looks like LBS, but <span class="math notranslate nohighlight">\(w_b\)</span> are given by a neural network (conditioned on <span class="math notranslate nohighlight">\(\mathbf{p}\)</span> coordinates) instead of pre-defined by a template. The benefit is that <span class="math notranslate nohighlight">\(w_b(\mathbf{p})\)</span> does not change with pose, so the network just learns one skinning weight field for the whole space. This field can generalize to new poses because it doesn’t directly encode pose; pose only comes into play in the known linear blend formula.</p>
<p>The challenge is that to train it, one must solve the inverse problem: given a point <span class="math notranslate nohighlight">\(\mathbf{p}'\)</span> in posed space (from a training mesh), find all <span class="math notranslate nohighlight">\(\mathbf{p}\)</span> in canonical such that <span class="math notranslate nohighlight">\(W_\theta(\mathbf{p})=\mathbf{p}'\)</span>. SNARF tackles this by iterative root finding with implicit differentiation to propagate gradients.</p>
<p>Essentially, for each query point <span class="math notranslate nohighlight">\(\mathbf{p}'\)</span>, they run a small iterative solver (like Newton or bisection in each bone’s local coordinate) to find a canonical <span class="math notranslate nohighlight">\(\mathbf{p}\)</span> that could produce it.</p>
</section>
<section id="pose-dependent-deformation-secondary-motion">
<h3>Pose-Dependent Deformation (Secondary Motion)<a class="headerlink" href="#pose-dependent-deformation-secondary-motion" title="Link to this heading"></a></h3>
<p>Whether using forward or backward skinning for the bulk articulation, real clothing also has non-rigid deformations dependent on pose (think of how a skirt flares when legs move, or wrinkles form at the elbow when arm bends). To model this, most frameworks include an additional deformation field or correctives.</p>
<p>In SMPL (meshes) this is handled by pose-blendshapes – vertex offsets as a function of joint angles. In implicit models, we incorporate a learned function <span class="math notranslate nohighlight">\(\delta \mathbf{p}(\mathbf{p}, \theta)\)</span> that displaces points in canonical space before evaluating the SDF/occupancy.</p>
<p>Neural-GIF explicitly does this: after inverse warping a point to canonical, they add a learned offset <span class="math notranslate nohighlight">\(\delta \mathbf{p}\)</span> (the “non-rigid deformation”) then evaluate the signed distance. This effectively allows the canonical shape to morph depending on pose, producing folds or muscle bulges.</p>
<p>Similarly, SNARF conditions its implicit shape on pose in a way that captures local changes (they achieve this by inputting local pose features to the occupancy MLP so the implicit surface itself is pose-conditioned, even though the skinning weights are static).</p>
<p>SCANimate introduced locally pose-aware implicit functions: instead of giving the network a global pose code, they feed per-bone pose features to different parts of the space, which reduced spurious correlations and improved generalization.</p>
</section>
</section>
<section id="generative-implicit-models-for-clothed-bodies">
<h2>Generative Implicit Models for Clothed Bodies<a class="headerlink" href="#generative-implicit-models-for-clothed-bodies" title="Link to this heading"></a></h2>
<p>Early neural implicit works on humans focused on static shape representation – learning a space of plausible human shapes (with or without clothing) that can be sampled or fit to data. These models are generative in nature, capturing the distribution of human geometry. They often build upon or replace parametric models like SMPL.</p>
<section id="smplicit-topology-aware-clothed-human-model">
<h3>SMPLicit: Topology-Aware Clothed Human Model<a class="headerlink" href="#smplicit-topology-aware-clothed-human-model" title="Link to this heading"></a></h3>
<p>SMPLicit (Corona et al., CVPR 2021) is a generative model that combines the SMPL parametric body with a neural implicit surface to represent clothing of various types. The motivation is to have a single learned model that can dress the SMPL body in anything from tight T-shirts to flowing skirts or coats, without needing separate templates for each garment topology. It is “topology-aware” in that it can handle different clothing topologies (even multiple layers) in one framework.</p>
<p><strong>Representation:</strong>
SMPLicit uses an unsigned distance field (UDF) to represent the clothed surface. The implicit function <span class="math notranslate nohighlight">\(C_\Theta(\mathbf{p})\)</span> takes as input a 3D point <span class="math notranslate nohighlight">\(\mathbf{p}\)</span> and outputs the distance to the clothing surface. A distance of zero means <span class="math notranslate nohighlight">\(\mathbf{p}\)</span> lies on the garment surface.</p>
<p>The UDF is defined in the local coordinates of the SMPL body – specifically, they augment <span class="math notranslate nohighlight">\(\mathbf{p}\)</span>’s representation with information about the underlying body. During training, <span class="math notranslate nohighlight">\(\mathbf{p}\)</span> is expressed relative to the nearest point on the SMPL body or in the local coordinate of a body part.</p>
<p>Additionally, SMPLicit has a latent code that represents the clothing shape/style. This latent is divided into “cut” (geometry) and “style” components in the architecture. The cut latent encodes which garment and its general shape (e.g., long-sleeve vs short-sleeve), and style latent encodes finer details or looseness.</p>
<p><strong>Architecture:</strong>
The SMPLicit network is a feed-forward decoder that outputs UDF value. It is trained in two modes simultaneously:
1. Given a ground-truth garment mesh on a SMPL body, they render a form of “occlusion map” and encode that to predict an initial latent (“cut”).
2. They also use an auto-decoder that directly optimizes a latent code per training example (this latent captures additional style aspects).</p>
<p>So the final latent fed to the network is a combination of the predicted cut-code from the occlusion map encoder and the learned style code.</p>
<p><strong>Training data:</strong>
SMPLicit was trained on a large set of synthetic and real scans of people in clothing (T-shirts, jackets, pants, etc., including multiple layers). They likely used datasets like CAPE (which provides registered clothed meshes) and others, possibly augmented with synthetic garments.</p>
<p>For each training example, they have the SMPL parameters (body shape and pose) and a clothed surface mesh. They sample points around the surface (both near-surface and in free space). The ground truth for a sample point can be the unsigned distance to the mesh. They train <span class="math notranslate nohighlight">\(C_\Theta\)</span> to minimize the error in predicted distance.</p>
<p><strong>Skinning and Output:</strong>
Once trained, SMPLicit can generate a clothed shape by querying <span class="math notranslate nohighlight">\(C_\Theta\)</span> on a 3D grid around the body and extracting the surface via Marching Cubes. This gives a garment mesh in canonical pose.</p>
<p>To animate the output garment on the SMPL skeleton, they do a simple post-process: for each vertex of the extracted garment mesh, find the nearest SMPL body vertex and copy its skinning weights. Then use SMPL’s LBS to skin the garment. This means SMPLicit’s garments will follow the body’s movements without secondary motion (like a parented object).</p>
<p><strong>Uses and performance:</strong>
SMPLicit is fully differentiable, which is a major advantage. The authors show two applications:
1. Fitting to 3D scans (take a raw scan of a person in clothes, optimize the latent code and possibly body shape so that SMPLicit’s surface matches the scan)
2. 3D reconstruction from images (starting from an image, use the encoder to get a cut latent from silhouettes and optimize further to match the image evidence)</p>
<p>In both cases, because the model is generative, it can produce plausible clothes even in unseen configurations. It also allowed interactive editing: since the latent has semantic directions (tightening a garment corresponds to moving latent in some direction), one can change the garment size or length in the latent space and regenerate the mesh.</p>
</section>
<section id="imghum-implicit-generative-human-model">
<h3>imGHUM: Implicit Generative Human Model<a class="headerlink" href="#imghum-implicit-generative-human-model" title="Link to this heading"></a></h3>
<p>Around the same time, imGHUM (Implicit Generative HUMan model) was proposed by Google AI (Xu Chen et al., 2021). While SMPLicit focuses on clothed bodies, imGHUM aims to be a holistic implicit model of the human body (minimally clothed, with a separate implicit head and hands model).</p>
<p>imGHUM represents the entire body (including detailed face and fingers) as a signed distance function. It is “holistic” in that it doesn’t treat clothing separately from body; it’s more akin to a replacement for SMPL that is continuous and high-resolution. imGHUM introduced a continuous SDF model with articulated joints that can be sampled to extremely high detail and is fully differentiable. They also built a generative prior (allowing sampling new identities) using a normalizing flow on the latent space.</p>
<p>For our cloth-focused discussion, imGHUM is mainly notable as an implicit body model; it doesn’t explicitly handle arbitrary outfits (it was trained on scans of minimally-clothed subjects, plus some with tight clothing).</p>
<p>Key difference from SMPLicit: SMPLicit keeps SMPL in the loop (for pose, shape, skinning after), and focuses on clothing geometry. imGHUM tries to model the human (body shape and pose) entirely in the implicit function, relying less on an external parametric model (though they still condition on pose like joint angles).</p>
</section>
</section>
<section id="pose-dependent-implicit-models-and-animatable-avatars">
<h2>Pose-Dependent Implicit Models and Animatable Avatars<a class="headerlink" href="#pose-dependent-implicit-models-and-animatable-avatars" title="Link to this heading"></a></h2>
<p>Animating a clothed human implicit model means making it respond correctly to new pose inputs. Several influential methods have made progress on this front.</p>
<section id="nasa-neural-articulated-shape-approximation-eccv-2020">
<h3>NASA: Neural Articulated Shape Approximation (ECCV 2020)<a class="headerlink" href="#nasa-neural-articulated-shape-approximation-eccv-2020" title="Link to this heading"></a></h3>
<p>NASA is one of the earliest to bring neural implicits to articulated objects (especially humans). It uses a part-based occupancy representation. The idea is to approximate a complex articulated shape by a union of simple implicit parts attached to a skeleton.</p>
<p>For each bone of the skeleton, NASA learns a local implicit shape (an occupancy field) in that bone’s local coordinate system. At pose time, it transforms sample points into each part’s coordinate frame (using the known skeletal transforms) and queries the occupancy. If a point is inside any part’s implicit shape, it’s considered inside the whole shape.</p>
<p>NASA’s network outputs an occupancy probability for each part given a point and the pose; effectively a set <span class="math notranslate nohighlight">\(\{o_b(\mathbf{x})\}\)</span> for <span class="math notranslate nohighlight">\(b=1...B\)</span> parts. These are combined (via max or sum) to yield the final occupancy.</p>
<p>NASA was trained on minimally clothed human scans with known SMPL fits, so they could get ground-truth part labels (each vertex belongs to a SMPL part). A limitation is that clothing which bridges parts (like a skirt spanning both legs) cannot be represented unless assigned to a single part or split.</p>
<p>NASA’s significance is showing that differentiable occupancy as a function of pose is possible by conditioning on joint angles. It uses a relatively small amount of data because each part’s shape is simpler. However, it inherits the drawback of backward mapping: each part’s occupancy is defined in that part’s canonical space, but the blend at inference time is somewhat heuristic.</p>
</section>
<section id="scanimate-weakly-supervised-skinned-avatar-networks-cvpr-2021">
<h3>SCANimate: Weakly-Supervised Skinned Avatar Networks (CVPR 2021)<a class="headerlink" href="#scanimate-weakly-supervised-skinned-avatar-networks-cvpr-2021" title="Link to this heading"></a></h3>
<p>SCANimate (Saito et al., 2021) is a milestone because it showed that one can learn an animatable clothed human directly from raw 3D scans without explicit correspondences or a template mesh. It is “weakly-supervised” in that it doesn’t require ground-truth canonical pose alignments of the scans or part labels – it only needs the scans and the ability to fit a parametric body to each scan (which gives a rough pose).</p>
<p>SCANimate’s pipeline is as follows:</p>
<ol class="arabic simple">
<li><p>They have a set of raw scans of a person in various poses, each scan is just a point cloud or mesh with no point-to-point correspondence across poses. They first fit a SMPL body to each scan (aligning pose, maybe shape). This gives them a common skeleton and a rough alignment (but the scan’s surface isn’t registered to a template).</p></li>
<li><p>They leverage two key observations:
- Fitting SMPL to a clothed scan is tractable (the body can be aligned under the clothes), but doing a full surface registration of a template to the clothed scan is hard (clothing breaks correspondence).
- Articulated transformations are invertible – meaning if you have an implicit model, you can move from posed to canonical and back, allowing a cycle consistency.</p></li>
<li><p>SCANimate essentially learns a backward warp (inverse skinning) to canonical, similar to Neural-GIF’s idea, but they enforce cycle consistency: if you warp a scan to canonical and then forward warp it back, you should recover the original scan. They do this by training a neural network that extends LBS to 3D space. This network learns the inverse warp <span class="math notranslate nohighlight">\(W^{-1}_\theta\)</span> such that when applied to all points of a posed scan, the points come into a coherent canonical pose alignment.</p></li>
<li><p>They introduce a locally pose-aware implicit function for the geometry. Instead of a single global latent or global pose vector, they condition the implicit surface on pose in a local manner. Concretely, for each query point, they incorporate features like which bones influence it and how. This reduces artifacts where an arm’s pose might erroneously affect a far part of the clothing.</p></li>
<li><p>They supervise the training weakly: use the scans (point clouds) and try to reconstruct them in canonical space and in posed space. Missing regions in scans are completed by the network. Through training, the network learns a canonical shape (with the clothing) and the deformation field to map it to any pose, without ever having a template registration.</p></li>
</ol>
<p>The result is an animatable implicit avatar that can be driven by pose parameters to produce new poses with realistic clothing deformation. SCANimate also can optionally model appearance (they mention it can be extended to textured avatar), but geometry is the main focus.</p>
<p>One notable thing: SCANimate still ultimately uses a SMPL model as a helper. It uses SMPL’s pose to condition the network and presumably uses SMPL’s skinning as an initial guess for warping. But it does not require the clothing topology to be pre-defined. It even allows different amounts of training data – they showed it works with as few as ~10 scans or as many as 100+, and more data yields better generality.</p>
</section>
<section id="neural-gif-neural-generalized-implicit-functions-iccv-2021">
<h3>Neural-GIF: Neural Generalized Implicit Functions (ICCV 2021)<a class="headerlink" href="#neural-gif-neural-generalized-implicit-functions-iccv-2021" title="Link to this heading"></a></h3>
<p>Neural-GIF (Tiwari et al., 2021) builds heavily on the idea of separating pose-dependent deformations from the base shape in an implicit framework. It is conceptually similar to SCANimate but with a more explicit formulation of the deformation field and without requiring scan alignment via cycle consistency. The method was described earlier in our discussion of backward vs forward warp: it maps each query point to canonical space, applies a learned deformation, then evaluates an SDF.</p>
<p>Neural-GIF assumes we have a set of poses of a single subject (like scans or even synthetic data), similar to SCANimate but perhaps they also support multi-subject in an extended version.</p>
<p>They factorize motion by articulation vs non-rigid:
- They use the SMPL model’s known skinning to handle large motions, and learn a non-rigid deformation field for pose-dependent effects.
- Specifically, given a point <span class="math notranslate nohighlight">\(\mathbf{p}'\)</span> in posed space and the pose <span class="math notranslate nohighlight">\(\theta\)</span>, they first find the corresponding canonical point via inverse LBS (they do this by predicting skinning weights <span class="math notranslate nohighlight">\(w\)</span> for <span class="math notranslate nohighlight">\(\mathbf{p}'\)</span> and then applying <span class="math notranslate nohighlight">\(W^{-1}(\mathbf{p}',w,\theta)\)</span>). This gives an initial <span class="math notranslate nohighlight">\(\mathbf{p}\)</span> in canonical space.
- Then they evaluate a displacement field <span class="math notranslate nohighlight">\(\delta \mathbf{p} = \delta(\mathbf{p},\theta)\)</span> which tells how <span class="math notranslate nohighlight">\(\mathbf{p}\)</span> should move in canonical space due to the pose.
- They add this, obtaining a deformed canonical point <span class="math notranslate nohighlight">\(\mathbf{p}^* = \mathbf{p} + \delta(\mathbf{p},\theta)\)</span>.
- Finally, they evaluate the signed distance field <span class="math notranslate nohighlight">\(f(\mathbf{p}^*)\)</span>. If <span class="math notranslate nohighlight">\(f(\mathbf{p}^*) \le 0\)</span> (inside or on surface), then the original point <span class="math notranslate nohighlight">\(\mathbf{p}'\)</span> is inside/on the posed surface.</p>
<p>The networks involved:
- One network predicts skinning weights <span class="math notranslate nohighlight">\(w(\mathbf{p}',\theta)\)</span> for the query (this is effectively a backward skinning field, pose-conditioned).
- Another network represents the displacement field <span class="math notranslate nohighlight">\(\delta(\mathbf{p},\theta)\)</span>.
- And another represents the canonical SDF.</p>
<p>They likely train them jointly. The loss comes from known ground truth occupancies of posed scans: they sample points and know whether they are inside or outside the clothed surface in posed state (from scan data).</p>
<p>Neural-GIF results show realistic pose-dependent wrinkles and dynamics for clothing. One benefit of their approach is not needing a pre-registered template; they learn from raw scans (like SCANimate).</p>
<p>In essence, Neural-GIF formalizes the canonical shape + learned pose deformation concept in a neural implicit way. It validates that even without forward skinning, if you have enough data of one person, a backward method can learn pose generalization by focusing on the deltas (since the main change between poses is captured by <span class="math notranslate nohighlight">\(\delta(\mathbf{p},\theta)\)</span>, which can be learned as a smooth function).</p>
</section>
<section id="snarf-skinned-neural-articulated-implicit-shapes-iccv-2021">
<h3>SNARF: Skinned Neural Articulated Implicit Shapes (ICCV 2021)<a class="headerlink" href="#snarf-skinned-neural-articulated-implicit-shapes-iccv-2021" title="Link to this heading"></a></h3>
<p>We have already covered SNARF’s methodology in depth. To recap salient points:</p>
<p>SNARF proposes a pose-independent canonical skinning field and finds correspondences via root finding. It demonstrated that such forward-skinning implicit models can outperform backward ones in pose generalization.</p>
<p>SNARF’s training was done on synthetic data: they used a dataset of clothed humans (likely generated via CLOTH3D or by dressing SMPL with different outfits and posing them). They had meshes for each training sample (with consistent vertex ordering per sequence since they come from a simulation, presumably). Using these, they could sample points and know occupancy.</p>
<p>They do not require knowing which canonical points correspond – SNARF finds them during training by solving <span class="math notranslate nohighlight">\(W_\theta(\mathbf{p})=\mathbf{p}'\)</span> as described. The analytical gradients mean they can backpropagate without having to differentiate through the iterative steps explicitly (they derive a formula via implicit differentiation).</p>
<p>Results: SNARF presented qualitative results of clothed humans (loose shirts, skirts, etc.) doing various poses. It maintained coherent geometry even in challenging poses like one arm touching the opposite side, where backward methods might create broken surfaces.</p>
<p>Limitations: SNARF in its original form still assumes one model per subject (like training on one person’s scans or one outfit’s variations). It doesn’t natively handle multiple garment types in one network (though one could extend it by adding a latent code for clothing type as in SMPLicit). Also, SNARF does not model texture – it’s geometry only.</p>
<p>Follow-up - FastSNARF: This improved algorithmic efficiency by formulating the root finding differently, perhaps using better initialization or analytical solutions in linearized cases. It made it feasible to train SNARF-like models much faster.</p>
</section>
<section id="pop-the-power-of-points-iccv-2021-point-based-modeling">
<h3>POP: The Power of Points (ICCV 2021) – Point-Based Modeling<a class="headerlink" href="#pop-the-power-of-points-iccv-2021-point-based-modeling" title="Link to this heading"></a></h3>
<p>Shifting from pure implicit, POP by Ma et al. (ICCV 2021) demonstrated a powerful alternative for animatable avatars.</p>
<p>POP represents a clothed human as an articulated point cloud with local features, rather than a mesh or continuous field. This point cloud is dense (they use ~10k points) and is associated with a particular subject+outfit. However, unlike a raw scan’s point cloud, POP’s points have additional learned parameters that enable generalization:</p>
<ul class="simple">
<li><p>Each point has a learned feature vector encoding local geometry details. This feature could be thought of analogous to a latent code per point for the shape of the nearby surface patch.</p></li>
<li><p>Each point also has a fixed position in a canonical pose and is attached to the body (they likely use the nearest SMPL vertex or bone weighting for each point to move it with the skeleton rigidly).</p></li>
<li><p>A neural network (likely an MLP) takes as input the pose (and possibly global shape info) and outputs a deformation for each point or an update to its feature. The paper mentions a “novel local clothing geometric feature” used to represent shape, and that the network learns to model pose-dependent clothing deformations.</p></li>
</ul>
<p>The training of POP is done on many subjects and outfits in many poses. They want a single model that works for multiple outfits. During training, they have numerous 3D scans of people in different clothing and poses. They must somehow align these to a common representation:</p>
<ul class="simple">
<li><p>They likely perform a nonrigid registration of a point cloud template to each scan. Or since they have SMPL for each scan (the data includes SMPL fits under clothing from CAPE dataset), they might seed points on SMPL surface and offset them to scan surface to initialize correspondences.</p></li>
<li><p>They learn a shared point set that can fit all outfits via different point features. Possibly they anchor points to SMPL body coordinates (like UV map positions on SMPL surface) so that each point corresponds to a particular body location. The variation in clothing is then handled by points moving off the body and feature differences.</p></li>
</ul>
<p>At inference, POP can fit a new scan (unseen outfit) by optimizing the point features to reconstruct that scan. They call this optimizing the “geometry feature”. This is analogous to finding the latent code in SMPLicit that fits a new scan. After fitting, the new outfit is represented by a point cloud with learned features, which POP’s network can then animate to novel poses.</p>
<p>Compatibility and speed: POP results show it outperforms state-of-art in multi-outfit modeling and unseen outfit animation. It also naturally produces surfaces (the points can be rendered as surfels, or triangulated using e.g. proximity). Since the representation is explicit points, it’s easier to integrate with graphics engines (as noted: implicit methods lack compatibility with standard tools, whereas point clouds can be handled by particle systems or meshing algorithms).</p>
<p>One can think of POP as doing for points what SMPL did for vertices: SMPL had a fixed mesh and learned deformations (blendshapes) for new shapes/poses. POP has a fixed set of points and learns how their positions shift for new outfits and poses.</p>
<p>Comparison to implicit: POP’s advantages include no need for heavy root-finding or neural query per-frame – animating is just linear blend plus a small MLP per point. This makes it fast. It’s also easier to enforce no penetrations (they could, for example, ensure points representing cloth stay outside the body by construction).</p>
<p>A disadvantage is that resolution is limited by number of points; extremely fine details or topology changes beyond what points can sample might be missed (though 10k points is quite dense). Also, if clothing has very thin structures (like belts, straps), implicit fields might represent those better as a continuous surface, whereas a sparse point cloud might need many points to capture a thin strip.</p>
</section>
<section id="other-noteworthy-methods">
<h3>Other Noteworthy Methods<a class="headerlink" href="#other-noteworthy-methods" title="Link to this heading"></a></h3>
<p>To be comprehensive, let’s briefly mention a few other relevant methods and how they fit:</p>
<ul class="simple">
<li><p><strong>PIFu / PIFuHD (CVPR 2019 &amp; 2020)</strong>: These are image-to-implicit models that infer a person’s shape from a single image. They use pixel-aligned features to query an occupancy function. While not directly an avatar model (they’re more for 3D reconstruction), PIFu’s occupancy network can be considered a static implicit model for clothed bodies (with no articulation component).</p></li>
<li><p><strong>ARCH, ARCH++ (CVPR 2020, 2021)</strong>: These models from Chen et al. learned an implicit occupancy with explicit correspondences to a template. They predict an implicit surface and a UV mapping to a template, thereby combining a parametric model with implicit details.</p></li>
<li><p><strong>SCARF (SIGAsia 2022)</strong>: Already discussed, it’s a hybrid radiance field model where the body uses SMPL (explicit) and clothing is an implicit volumetric representation. It is optimized from video, showcasing application of NeRF for clothed avatars with layered representation (body vs clothes separated).</p></li>
<li><p><strong>UNIF (ECCV 2022)</strong>: This improves NASA’s part-based approach by removing the need for part labels, learning to partition the space based on motion consistency. UNIF still defines one implicit SDF per bone, but it learns to assign points to bones via a bone-centered initialization + losses rather than ground-truth segmentation.</p></li>
</ul>
</section>
</section>
<section id="blueprint-algorithms-for-key-methods">
<h2>Blueprint Algorithms for Key Methods<a class="headerlink" href="#blueprint-algorithms-for-key-methods" title="Link to this heading"></a></h2>
<p>To further clarify how these methods work in practice, we provide high-level pseudocode for the training or usage of three representative methods: SNARF, POP, and SMPLicit. These are not exact code, but outline the main steps and network operations in each case.</p>
<section id="snarf-training-procedure-differentiable-forward-skinning-for-implicit-surfaces">
<h3>SNARF (Training Procedure): Differentiable Forward Skinning for Implicit Surfaces<a class="headerlink" href="#snarf-training-procedure-differentiable-forward-skinning-for-implicit-surfaces" title="Link to this heading"></a></h3>
<p>SNARF learns a canonical implicit shape <span class="math notranslate nohighlight">\(f(\mathbf{p})\)</span> and a skinning weight field <span class="math notranslate nohighlight">\(w_b(\mathbf{p})\)</span> for <span class="math notranslate nohighlight">\(B\)</span> bones. Training data are posed meshes (or occupancy data) with known skeleton poses.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span># Initialize neural networks:
<span class="linenos"> 2</span>F_theta(p)  # implicit occupancy/SDF in canonical space (MLP)
<span class="linenos"> 3</span>W_phi(p)    # skinning weight predictor (MLP that outputs w_1...w_B for point p)
<span class="linenos"> 4</span>
<span class="linenos"> 5</span>for each training iteration:
<span class="linenos"> 6</span>    # Sample a training mesh with pose θ (bone transforms G_b for b=1..B)
<span class="linenos"> 7</span>    # Sample N points {x_i&#39;} in the posed space of that mesh (some near surface, some inside/outside)
<span class="linenos"> 8</span>    For each point x_i&#39;:
<span class="linenos"> 9</span>        # Find canonical correspondences via forward skinning
<span class="linenos">10</span>        Compute skin weights via network: w = W_phi(x_i&#39;)
<span class="linenos">11</span>        Define function g_i(p) = \sum_b w_b * (G_b(θ) * [p;1])_{1:3} - x_i&#39;
<span class="linenos">12</span>          # This gives difference between forward-skinned canonical point p and target posed point x_i&#39;
<span class="linenos">13</span>        Use root-finding (e.g. iterative nonlinear solve) to find all solutions p_i such that g_i(p_i) ≈ 0
<span class="linenos">14</span>        For each solution p_i (there can be multiple or none):
<span class="linenos">15</span>            Evaluate occupancy = F_theta(p_i)  # implicit surface in canonical
<span class="linenos">16</span>            Collect occupancy probabilities o_i (taking max if multiple p_i solutions)
<span class="linenos">17</span>    Determine ground-truth occupancy label y_i for x_i&#39; (e.g. 1=inside if x_i&#39; is inside training mesh, else 0)
<span class="linenos">18</span>    Compute loss = \frac{1}{N}\sum_i BCE(o_i, y_i)  # binary cross-entropy for occupancy
<span class="linenos">19</span>    Backpropagate gradients:
<span class="linenos">20</span>        # Gradients flow through F_theta and W_phi, and through the root-finding step via implicit differentiation
<span class="linenos">21</span>    Update networks F_theta and W_phi (e.g. Adam optimizer)
</pre></div>
</div>
<p>After training, to use SNARF for animation: one would take the learned <span class="math notranslate nohighlight">\(F_\theta\)</span> and <span class="math notranslate nohighlight">\(W_\phi\)</span>. Given a new pose <span class="math notranslate nohighlight">\(\theta\)</span> and canonical surface <span class="math notranslate nohighlight">\(F_\theta(p)=0\)</span>, one can forward-skin each canonical point <span class="math notranslate nohighlight">\(p\)</span> via <span class="math notranslate nohighlight">\(W_\phi\)</span> weights and <span class="math notranslate nohighlight">\(G_b(\theta)\)</span> to get the deformed surface. For query-based rendering, for any posed-space point <span class="math notranslate nohighlight">\(x'\)</span>, find <span class="math notranslate nohighlight">\(p = W_\phi(x')^{-1}\)</span> via root-finding and evaluate <span class="math notranslate nohighlight">\(F_\theta(p)\)</span>. This implicit surface can be rendered or meshed.</p>
</section>
<section id="pop-training-fitting-pipeline-point-based-model-for-pose-dependent-clothing">
<h3>POP (Training &amp; Fitting Pipeline): Point-Based Model for Pose-Dependent Clothing<a class="headerlink" href="#pop-training-fitting-pipeline-point-based-model-for-pose-dependent-clothing" title="Link to this heading"></a></h3>
<p>POP learns a single model that can represent many outfits. It has a fixed set of <span class="math notranslate nohighlight">\(N\)</span> points (with positions attached to a reference body) and learns two networks: one to predict how points move with pose, and one to encode outfit geometry into point features.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span># Initialize:
<span class="linenos"> 2</span>{P_i (i=1..N)}  # template point positions on a canonical body (e.g., sampled on SMPL surface)
<span class="linenos"> 3</span>{v_i}           # feature vectors for each point (learned, initially random)
<span class="linenos"> 4</span>M_psi(v_i, θ)   # Network that predicts an offset ΔP_i for point i given its feature v_i and pose θ
<span class="linenos"> 5</span>(Optional second network E_phi)  # encodes geometry of a new scan into point features
<span class="linenos"> 6</span>
<span class="linenos"> 7</span># Training:
<span class="linenos"> 8</span>for each iteration:
<span class="linenos"> 9</span>    # Sample a training subject j with outfit j, and a pose θ_k from that subject&#39;s dataset
<span class="linenos">10</span>    # Retrieve ground-truth surface points X (or mesh) for subject j in pose θ_k
<span class="linenos">11</span>    # (Assume we have correspondence of X to our template points P via nearest neighbor or known mapping)
<span class="linenos">12</span>    For each point i:
<span class="linenos">13</span>        Compute deformed position: P_i&#39; = W_SMPL(θ_k, P_i)  # apply skeleton rigid transform to base point
<span class="linenos">14</span>        Compute neural offset: ΔP_i = M_psi(v_i_j, θ_k)
<span class="linenos">15</span>        Predicted position = \hat{X}_i = P_i&#39; + ΔP_i
<span class="linenos">16</span>    Compute loss = ∑_i || \hat{X}_i - X_i||^2   # point-to-point distance between predicted and actual surface
<span class="linenos">17</span>    Backpropagate to update M_psi and point features v_i_j for that outfit
<span class="linenos">18</span>
<span class="linenos">19</span># (Optional) Fitting new outfit:
<span class="linenos">20</span># Given new scan (subject m, outfit m), with multiple poses scans:
<span class="linenos">21</span>Initialize point features {v_i_m} (e.g. from body shape or average)
<span class="linenos">22</span>For each scan of subject m:
<span class="linenos">23</span>    optimize {v_i_m} to minimize ∑_i ||P_i&#39; + M_psi(v_i_m, θ_scan) - X_i^{scan}||^2
<span class="linenos">24</span># (This fits the point cloud model to all poses of new outfit)
</pre></div>
</div>
<p>After training, POP’s network <span class="math notranslate nohighlight">\(M_\psi\)</span> is fixed. To animate a new outfit, one obtains its point features (via the fitting process or an encoder). Then for any new pose <span class="math notranslate nohighlight">\(\theta\)</span>, each point’s motion is given by <span class="math notranslate nohighlight">\(P_i' = W_{\text{SMPL}}(\theta, P_i)\)</span> plus the learned corrective <span class="math notranslate nohighlight">\(\Delta P_i = M_\psi(v_i, \theta)\)</span>. The result is a new point cloud for the outfit in that pose, which can be rendered (e.g., splatting or surfels) or triangulated for a mesh.</p>
</section>
<section id="smplicit-inference-fitting-workflow-generative-implicit-garment-model-conditioned-on-smpl">
<h3>SMPLicit (Inference/Fitting Workflow): Generative Implicit Garment Model conditioned on SMPL<a class="headerlink" href="#smplicit-inference-fitting-workflow-generative-implicit-garment-model-conditioned-on-smpl" title="Link to this heading"></a></h3>
<p>SMPLicit is trained with an image encoder and auto-decoder for latent, and supervising UDF values. Here we outline how one would fit SMPLicit to a new observation (e.g., a depth scan or image of a person in clothing) and then generate/animate the output:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span># Inputs: SMPL body shape β and pose θ (if known or estimable), and target observation (e.g., 3D point cloud of clothed person)
<span class="linenos"> 2</span>Initialize latent code z (garment style latent) - either from encoder (if using image) or zero
<span class="linenos"> 3</span>
<span class="linenos"> 4</span>for iter in 1..T:
<span class="linenos"> 5</span>    for each query point q in space (sampled around body or on depth):
<span class="linenos"> 6</span>         Compute body-relative coords: (e.g., find nearest body surface point and encode q as (u,v,dist) on SMPL)
<span class="linenos"> 7</span>         Predict unsigned distance d = C_Theta(q, β, θ, z)   # Network inference
<span class="linenos"> 8</span>         Compute loss from target:
<span class="linenos"> 9</span>            If q is on observed surface, enforce d ≈ 0
<span class="linenos">10</span>            If q is outside (in free space), enforce d &gt; 0 (e.g. ≥ margin)
<span class="linenos">11</span>            If q is inside observed volume, enforce d ≈ 0 as well (for inside points, since unsigned)
<span class="linenos">12</span>    Backpropagate loss; update latent code z (and body β if optimizing shape)
<span class="linenos">13</span>Optimize until convergence
<span class="linenos">14</span>
<span class="linenos">15</span># Output:
<span class="linenos">16</span># Latent code z* that best explains the observation, and possibly refined body shape β*
<span class="linenos">17</span># Generate dense sampling: evaluate C_Theta on a 3D grid around body with z*, get unsigned distance field
<span class="linenos">18</span># Run Marching Cubes on the field d(x)=0 to extract garment mesh
<span class="linenos">19</span># Attach garment mesh to SMPL: for each garment vertex, find nearest SMPL vertex and copy skin weights
</pre></div>
</div>
<p>Now the fitted SMPLicit model can be animated: apply a new pose <span class="math notranslate nohighlight">\(\theta_{\text{new}}\)</span> to SMPL body, then skin the garment mesh with the copied weights. Because SMPLicit’s garments are generated in canonical pose and then just follow the body, extreme poses might cause visual artifacts (since the model itself didn’t encode pose-dependent cloth deformation). But as shown by Corona et al., it was sufficient for many scenarios.</p>
<p>This pipeline leverages the differentiability of the implicit model to do inverse fitting. Additionally, one could sample the latent space <span class="math notranslate nohighlight">\(z\)</span> to generate new random outfits on the same body (for example, varying tightness or length as the latent has semantic directions).</p>
</section>
</section>
<section id="historical-perspective-and-future-outlook">
<h2>Historical Perspective and Future Outlook<a class="headerlink" href="#historical-perspective-and-future-outlook" title="Link to this heading"></a></h2>
<p>In little over five years, the field has progressed from parametric mesh models of minimally-clothed bodies to highly expressive neural models of clothed humans. Early works like SMPL (2015) and its extensions (SMPL+DH for clothing, etc.) set the stage by establishing a common body parameterization. Around 2018-2019, deep learning on 3D surfaces took off: Occupancy Networks and DeepSDF showed that implicit fields can represent shapes with unprecedented detail and continuity.</p>
<p>The human modeling community quickly adopted these for clothed people: PIFu (2019) applied occupancy nets to single-view reconstruction of clothed humans; NASA (2019) introduced part-wise implicit models for articulated objects; CAPE (2020) demonstrated learning clothing deformations as offsets on SMPL with a large dataset.</p>
<p>By 2020-2021, we see a proliferation of neural implicit avatar models:
- SCANimate (CVPR 2021) and Neural-GIF (ICCV 2021) showed how to learn from raw scans, bringing these methods closer to real data application.
- SNARF (ICCV 2021) solved a key technical hurdle for inverse skinning and set a new standard for pose generalization.
- SMPLicit (CVPR 2021) bridged parametric and implicit models, enabling generative topology-varying clothes.
- POP (ICCV 2021) re-imagined explicit point representations and achieved similar goals with arguably simpler inference.</p>
<p>In 2022, research built on these foundations: imGHUM (2021/22) provided a full-body implicit model (adopted in some image-fitting pipelines like ICON), UNIF (ECCV 2022) improved part-based approaches, SCARF (2022) and related NeRF-human works integrated photorealistic rendering with geometric models (implicitly starting to unify shape and appearance).</p>
<p>We also see the rise of diffusion models and latent generative models for humans (e.g., DreamAvatar 2023 uses imGHUM as a differentiable prior in a diffusion pipeline to generate avatars). These indicate a future where one can generate a realistic clothed avatar from high-level descriptors and animate it – essentially achieving the decades-old vision of fully data-driven virtual human creation.</p>
<section id="challenges-and-future-directions">
<h3>Challenges and Future Directions<a class="headerlink" href="#challenges-and-future-directions" title="Link to this heading"></a></h3>
<p>Despite progress, challenges remain:</p>
<ul class="simple">
<li><p><strong>Data Efficiency</strong>: Capturing real clothed humans with sufficient quality for training is hard; methods are moving toward using commodity data (monocular videos, multi-view but sparse rigs) thanks to advances in differentiable rendering and neural fields.</p></li>
<li><p><strong>Dynamic Clothing</strong>: Secondary motion like cloth simulation is not fully solved by these quasi-static models; combining physical simulation or learned physics priors with neural avatars is an open area.</p></li>
<li><p><strong>Interactivity and Editing</strong>: Models like SMPLicit allow latent editing of garments, but a user-friendly and interpretable control (e.g., “shorten the sleeves”) is still being developed, possibly via semantic latent spaces or cross-modal (text2avatar) models.</p></li>
<li><p><strong>Performance</strong>: Real-time applications would benefit from lighter representations – techniques like POP’s point-based or optimized octree implicit models are promising to deploy avatars in real-time VR/AR.</p></li>
</ul>
<p>Another trend is standardizing evaluation – as many methods are quite complex, the community is starting to establish benchmarks for clothed avatar reconstruction and animation (datasets like CAPE, Cloth3D, RenderPeople, etc., are used widely, and metrics like IoU, chamfer distance for surfaces, are used to compare methods). We anticipate a convergence where the best ideas from each line – e.g., the topological flexibility of implicit surfaces, the speed of point-based methods, the leveraging of body priors from parametric models, and the realism of radiance fields – will combine. Hybrid models (like SCARF) already hint at this convergence.</p>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading"></a></h2>
<p>Neural implicit and point-based representations have revolutionized clothing and human body modeling. They overcome many limitations of meshes, enabling high-fidelity, differentiable avatars that can be learned from data. The comparative analysis suggests no one representation is strictly superior – each has trade-offs.</p>
<p>Mesh-based models are still useful for certain applications (especially where integration with existing pipelines and real-time are needed), but implicit models unlock unlimited detail and learning-friendly frameworks, while point-based models offer a compelling middle ground. Together, these developments bring us closer to lifelike virtual humans that can be automatically created and animated – a technology with wide-ranging applications in graphics, vision, VR/AR, and the metaverse.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="lecture_08_1_vertex_based_clothing_modeling.html" class="btn btn-neutral float-left" title="Lecture 08.1: Vertex-Based Clothing Modeling for Virtual Humans" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="extended_materials_neural_radiance_fields.html" class="btn btn-neutral float-right" title="Neural Radiance Fields: A Historical and Theoretical Overview" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>