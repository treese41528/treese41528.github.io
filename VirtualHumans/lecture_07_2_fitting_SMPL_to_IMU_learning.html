

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Lecture 07.2: Fitting SMPL to IMU Data Using Learning-Based Methods &mdash; Fitting SMPL to IMU Optimization</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
    <link rel="canonical" href="https://treese41528.github.io/VirtualHumans/lecture_07_2_fitting_SMPL_to_IMU_learning.html" />
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=f2a433a1"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Neural Radiance Fields: A Historical and Theoretical Overview" href="extended_materials_neural_radiance_fields.html" />
    <link rel="prev" title="Lecture 07.1: Fitting SMPL to IMU Data Using Optimization-Based Methods" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Virtual Humans Lecture
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="lecture_01_1_historical_body_models.html">Lecture 01.1 – Historical Body Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#early-origins-simplified-primitives-and-kinematic-skeletons-1970s1980s">Early Origins: Simplified Primitives and Kinematic Skeletons (1970s–1980s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#advances-in-the-1990s-superquadrics-differentiable-fitting-and-physical-models">Advances in the 1990s: Superquadrics, Differentiable Fitting, and Physical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#the-impact-of-3d-scanning-and-data-from-anthropometry-to-statistical-models-1990s2000s">The Impact of 3D Scanning and Data: From Anthropometry to Statistical Models (1990s–2000s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#scape-and-the-emergence-of-pose-aware-models-mid-2000s">SCAPE and the Emergence of Pose-Aware Models (Mid-2000s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#consolidation-in-the-2010s-smpl-and-integration-with-learning-based-methods">Consolidation in the 2010s: SMPL and Integration with Learning-Based Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#deep-learning-and-neural-implicit-models-late-2010spresent">Deep Learning and Neural Implicit Models (Late 2010s–Present)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#timeline-summary-of-milestones">Timeline Summary of Milestones</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html">Lecture 01.2 – Introduction to Human Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#historical-context-of-human-body-modeling">1. Historical Context of Human Body Modeling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#early-developments">Early Developments</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#mid-20th-century-approaches">Mid-20th Century Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#data-driven-revolution-1990s-2000s">Data-Driven Revolution (1990s-2000s)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#mathematical-foundations-of-human-body-models">2. Mathematical Foundations of Human Body Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#the-smpl-model">The SMPL Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#pca-based-statistical-shape-modeling">PCA-Based Statistical Shape Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#kinematic-modeling">Kinematic Modeling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#applications-of-human-body-models">3. Applications of Human Body Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computer-animation-and-visual-effects">Computer Animation and Visual Effects</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#virtual-humans-and-avatars">Virtual Humans and Avatars</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#biomechanics-and-ergonomics">Biomechanics and Ergonomics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#human-computer-interaction-hci">Human-Computer Interaction (HCI)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computer-vision-and-ai">Computer Vision and AI</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#education-and-training">Education and Training</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#challenges-and-future-directions">4. Challenges and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computational-efficiency">Computational Efficiency</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#accuracy-and-detail">Accuracy and Detail</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#generalization">Generalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#clothing-and-accessories">Clothing and Accessories</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#emerging-approaches">Emerging Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html">Lecture 01.3 – Introduction to Human Models (Overview)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#historical-context">1. Historical Context</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#early-scientific-studies">Early Scientific Studies</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#mid-20th-century-to-digital-era">Mid-20th Century to Digital Era</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#st-century-advances">21st Century Advances</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#mathematical-foundations">2. Mathematical Foundations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#parametric-body-models">Parametric Body Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-surface-representations">Implicit Surface Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#kinematic-modeling">Kinematic Modeling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#image-formation-and-rendering">3. Image Formation and Rendering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#camera-models">Camera Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#shading-and-visibility">Shading and Visibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#differentiable-rendering">Differentiable Rendering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#surface-representation-methods">4. Surface Representation Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#explicit-mesh-models">Explicit Mesh Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-function-models">Implicit Function Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#motion-capture-and-behavior-synthesis">5. Motion Capture and Behavior Synthesis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#capturing-human-motion">Capturing Human Motion</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#behavior-synthesis">Behavior Synthesis</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#clothing-modeling">6. Clothing Modeling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physically-based-simulation">Physically-Based Simulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#data-driven-approaches">Data-Driven Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-clothing-models">Implicit Clothing Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#human-object-interaction">7. Human-Object Interaction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physics-based-methods">Physics-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#learning-based-approaches">Learning-Based Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#hybrid-systems">Hybrid Systems</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#applications">8. Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#entertainment-and-media">Entertainment and Media</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#healthcare-and-biomechanics">Healthcare and Biomechanics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#engineering-and-design">Engineering and Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#human-computer-interaction">Human-Computer Interaction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#scientific-research">Scientific Research</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#challenges-and-future-directions">9. Challenges and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#scalability-and-generalization">Scalability and Generalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#higher-fidelity-dynamics">Higher-Fidelity Dynamics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#data-and-labeling-constraints">Data and Labeling Constraints</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physics-and-learning-integration">Physics and Learning Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#semantic-and-cognitive-aspects">Semantic and Cognitive Aspects</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#realism-vs-controllability">Realism vs. Controllability</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_02_1_image_formation.html">Lecture 02.1 – Image Formation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#historical-developments-in-image-formation">1. Historical Developments in Image Formation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#ancient-and-medieval-optics-camera-obscura">Ancient and Medieval Optics – Camera Obscura</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#renaissance-perspective-and-geometry">Renaissance Perspective and Geometry</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#early-cameras-and-photographic-imaging">Early Cameras and Photographic Imaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#modern-developments">Modern Developments</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#the-pinhole-camera-model">2. The Pinhole Camera Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#coordinate-setup">Coordinate Setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#proof-by-similar-triangles">Proof by Similar Triangles</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#numerical-example">Numerical Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#inadequacy-of-a-simple-pinhole">Inadequacy of a Simple Pinhole</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#camera-intrinsics-and-the-projection-matrix">3. Camera Intrinsics and the Projection Matrix</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#extrinsic-parameters">Extrinsic Parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#full-projection-example">Full Projection Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#image-distortions-correction">4. Image Distortions &amp; Correction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#properties-of-perspective-projection">5. Properties of Perspective Projection</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#advanced-theoretical-extensions">6. Advanced Theoretical Extensions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#light-field-imaging-and-plenoptic-cameras">Light Field Imaging and Plenoptic Cameras</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#non-conventional-imaging-techniques">Non-Conventional Imaging Techniques</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#applications-in-modern-vision-and-graphics">7. Applications in Modern Vision and Graphics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#computer-vision-and-3d-reconstruction">Computer Vision and 3D Reconstruction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#medical-imaging">Medical Imaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#photorealistic-rendering-in-computer-graphics">Photorealistic Rendering in Computer Graphics</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#python-example-simulating-image-formation">8. Python Example: Simulating Image Formation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html">Lecture 02.2 – Rotations and Kinematic Chains</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#representations-of-3d-rotations">1. Representations of 3D Rotations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#a-rotation-matrices">A) Rotation Matrices</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#b-euler-angles">B) Euler Angles</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#c-quaternions">C) Quaternions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#lie-algebra-so-3-and-exponential-map">2. Lie Algebra <span class="math notranslate nohighlight">\(so(3)\)</span> and Exponential Map</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#rodrigues-rotation-formula">3. Rodrigues’ Rotation Formula</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#kinematic-chains-forward-inverse-kinematics">4. Kinematic Chains: Forward &amp; Inverse Kinematics</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#comparison-of-rotation-representations">Comparison of Rotation Representations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_03_1_surface_representations.html">Lecture 03.1 – Surface Representations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#mathematical-foundations-of-surface-representations">1. Mathematical Foundations of Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-parametric-surfaces">A) Parametric Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-implicit-surfaces">B) Implicit Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-explicit-surfaces">C) Explicit Surfaces</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#surface-differential-properties">2. Surface Differential Properties</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-surface-normals">A) Surface Normals</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-fundamental-forms-and-curvature">B) Fundamental Forms and Curvature</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-geodesics">C) Geodesics</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#discrete-surface-representations">3. Discrete Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-polygon-meshes">A) Polygon Meshes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-point-clouds">B) Point Clouds</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-signed-distance-fields-sdf">C) Signed Distance Fields (SDF)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#advanced-surface-representations">4. Advanced Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-bezier-curves-and-surfaces">A) Bézier Curves and Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-b-splines-and-nurbs">B) B-Splines and NURBS</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-subdivision-surfaces">C) Subdivision Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-level-sets">D) Level Sets</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#e-neural-implicit-representations">E) Neural Implicit Representations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#comparative-analysis-and-applications">5. Comparative Analysis and Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-computational-efficiency-and-storage">A) Computational Efficiency and Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-practical-applications">B) Practical Applications</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-operations-complexity">C) Operations Complexity</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#implementation-examples">6. Implementation Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-basic-mesh-processing-python">A) Basic Mesh Processing (Python)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-implicit-surface-utilities">B) Implicit Surface Utilities</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-bezier-curve-implementation">C) Bézier Curve Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-curvature-estimation-on-meshes">D) Curvature Estimation on Meshes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#advanced-topics-and-future-directions">7. Advanced Topics and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-multi-resolution-representations">A) Multi-Resolution Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-machine-learning-for-geometry">B) Machine Learning for Geometry</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-dynamic-surfaces">C) Dynamic Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-non-manifold-geometries">D) Non-Manifold Geometries</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html">Lecture 03.2 – Procrustes Alignment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#goal-learning-a-model-of-pose-and-shape">Goal: Learning a Model of Pose and Shape</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#the-challenge-of-registration">The Challenge of Registration</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#surface-representation-mesh">Surface Representation: Mesh</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#the-procrustes-alignment-problem-mathematical-formulation">The Procrustes Alignment Problem: Mathematical Formulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#rigid-transformations">Rigid Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#procrustes-alignment-solution">Procrustes Alignment Solution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#decoupling-translation-by-centroid-alignment">Decoupling Translation by Centroid Alignment</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#optimal-rotation-via-svd">Optimal Rotation via SVD</a><ul>
<li class="toctree-l4"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#reflection-adjustment">Reflection Adjustment</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#optimal-scale-optional">Optimal Scale (Optional)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#complete-mathematical-derivation">Complete Mathematical Derivation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#translation-derivation">Translation Derivation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#rotation-derivation">Rotation Derivation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#scale-derivation">Scale Derivation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#summary-of-procrustes-alignment-algorithm">Summary of Procrustes Alignment Algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#python-implementation-example">Python Implementation Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#practical-applications">Practical Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#interactive-visualization-ideas">Interactive Visualization Ideas</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_04_1_icp.html">Lecture 4.1: Iterative Closest Point</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#introduction-to-shape-alignment-and-registration">Introduction to Shape Alignment and Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#the-registration-problem">The Registration Problem</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#review-procrustes-analysis">Review: Procrustes Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#problem-unknown-correspondences">Problem: Unknown Correspondences</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#the-iterative-closest-point-icp-algorithm">The Iterative Closest Point (ICP) Algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#basic-icp-algorithm">Basic ICP Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#computational-considerations">Computational Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="lecture_04_1_icp.html#closest-point-computation">Closest Point Computation</a></li>
<li class="toctree-l4"><a class="reference internal" href="lecture_04_1_icp.html#convergence-and-local-minima">Convergence and Local Minima</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#point-to-point-vs-point-to-plane-icp">Point-to-Point vs. Point-to-Plane ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#point-to-point-icp">Point-to-Point ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#point-to-plane-icp">Point-to-Plane ICP</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#gradient-based-icp-for-non-rigid-registration">Gradient-based ICP for Non-Rigid Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#gradient-based-icp-algorithm">Gradient-based ICP Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#advantages-of-gradient-based-icp">Advantages of Gradient-based ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#computing-gradients">Computing Gradients</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#improving-icp-s-robustness">Improving ICP’s Robustness</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#data-association-direction">Data Association Direction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#robust-cost-functions">Robust Cost Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#trimmed-icp">Trimmed ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#ransac-based-approaches">RANSAC-based Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#additional-information">Additional Information</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#point-to-surface-distance">Point-to-Surface Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#icp-variants-and-extensions">ICP Variants and Extensions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#generalized-icp-gicp">Generalized ICP (GICP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#em-icp-and-probabilistic-approaches">EM-ICP and Probabilistic Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#coherent-point-drift-cpd">Coherent Point Drift (CPD)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#multi-scale-approaches">Multi-Scale Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#applications-of-icp">Applications of ICP</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#implementing-icp">Implementing ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#efficient-python-implementation">Efficient Python Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#practical-tips">Practical Tips</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_04_2_body_models.html">Lecture 04.2 - Body Models: Vertex-Based Models and SMPL</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#body-models-as-parameterized-functions">1. Body Models as Parameterized Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#rotations-articulation-and-pose-representation">2. Rotations, Articulation, and Pose Representation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#rotation-representation">2.1 Rotation Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#kinematic-chain">2.2 Kinematic Chain</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#linear-blend-skinning-and-its-limitations">3. Linear Blend Skinning and its Limitations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#linear-blend-skinning">3.1 Linear Blend Skinning</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#problems-with-standard-lbs">3.2 Problems with Standard LBS</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#blend-shapes-for-correcting-lbs-artifacts">3.3 Blend Shapes for Correcting LBS Artifacts</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#the-smpl-body-model">4. The SMPL Body Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#smpl-philosophy">4.1 SMPL Philosophy</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#smpl-model-architecture">4.2 SMPL Model Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#shape-blend-shapes">4.2.1 Shape Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#pose-blend-shapes">4.2.2 Pose Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#joint-regression">4.2.3 Joint Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#model-training">4.3 Model Training</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#comparison-with-scape">5. Comparison with SCAPE</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#the-scape-model">5.1 The SCAPE Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#different-approaches-to-deformation">5.2 Different Approaches to Deformation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#performance-comparison">5.3 Performance Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#other-advantages">5.4 Other Advantages</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#alignment-techniques-procrustes-analysis-and-icp">6. Alignment Techniques: Procrustes Analysis and ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#procrustes-analysis">6.1 Procrustes Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#iterative-closest-point-icp">6.2 Iterative Closest Point (ICP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#fitting-smpl-to-scans">6.3 Fitting SMPL to Scans</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#image-formation-and-the-pinhole-camera-model">7. Image Formation and the Pinhole Camera Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#the-pinhole-camera-model">7.1 The Pinhole Camera Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#lens-distortion">7.2 Lens Distortion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#extensions-and-advanced-applications">8. Extensions and Advanced Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#dynamic-soft-tissue-modeling">8.1 Dynamic Soft Tissue Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#specialized-extensions">8.2 Specialized Extensions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#deep-learning-for-model-fitting">8.3 Deep Learning for Model Fitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#probabilistic-approaches">8.4 Probabilistic Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#hybrid-models">8.5 Hybrid Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_05_1_body_model_training.html">Lecture 5.1 - Training a Body Model and Fitting SMPL to Scans</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#body-models-based-on-triangle-deformations">Body Models Based on Triangle Deformations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#scape-and-blendscape-models">SCAPE and BlendSCAPE Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#triangle-deformation-process">Triangle Deformation Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#comparison-smpl-vs-scape-blendscape">Comparison: SMPL vs. SCAPE/BlendSCAPE</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#training-a-body-model-from-registrations">Training a Body Model from Registrations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#the-challenge-of-raw-scan-data">The Challenge of Raw Scan Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#training-from-registrations">Training from Registrations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#obtaining-registrations-fitting-smpl-to-scans">Obtaining Registrations: Fitting SMPL to Scans</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#non-rigid-registration-process">Non-Rigid Registration Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#iterative-closest-point-icp-review">Iterative Closest Point (ICP) Review</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#registration-objective-formulation">Registration Objective Formulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#point-to-surface-distance">Point-to-Surface Distance</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#multi-stage-optimization-strategy">Multi-Stage Optimization Strategy</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#joint-registration-and-model-training">Joint Registration and Model Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#co-registration-approach">Co-Registration Approach</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_05_2_3d_registration.html">Lecture 05.2 - 3D Registration: From Classical ICP to Modern Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#rigid-registration-and-the-icp-algorithm">1. Rigid Registration and the ICP Algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#the-iterative-closest-point-icp-algorithm">The Iterative Closest Point (ICP) Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#convergence-analysis-and-failure-modes">Convergence Analysis and Failure Modes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#classical-non-rigid-registration">2. Classical Non-Rigid Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#thin-plate-spline-robust-point-matching-tps-rpm">Thin Plate Spline Robust Point Matching (TPS-RPM)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#coherent-point-drift-cpd">Coherent Point Drift (CPD)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#other-non-rigid-methods">Other Non-Rigid Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#parametric-models-and-the-smpl-body-model">3. Parametric Models and the SMPL Body Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#model-structure">Model Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#shape-blend-shapes-identity-variation">Shape Blend Shapes (Identity Variation)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#pose-blend-shapes-pose-dependent-deformation">Pose Blend Shapes (Pose-Dependent Deformation)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#linear-blend-skinning-lbs-for-articulation">Linear Blend Skinning (LBS) for Articulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#learning-smpl">Learning SMPL</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#using-smpl-for-registration">Using SMPL for Registration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#modeling-clothing-and-fine-details-smpl-d">4. Modeling Clothing and Fine Details: SMPL+D</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#why-smpl-d">Why SMPL+D?</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#how-displacements-are-applied">How Displacements Are Applied</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#limitations">Limitations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#applications">Applications</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#survey-of-3d-registration-methods-from-icp-to-deep-learning">5. Survey of 3D Registration Methods: From ICP to Deep Learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#early-pioneering-works-1990s-foundational-rigid-registration">5.1 Early Pioneering Works (1990s) – Foundational Rigid Registration</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#the-2000s-robust-and-non-rigid-registration-emerges">5.2 The 2000s – Robust and Non-Rigid Registration Emerges</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#s-template-based-and-parametric-model-registration">5.3 2010s – Template-based and Parametric Model Registration</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#s-learning-based-parametric-registration-and-hybrid-approaches">5.4 2020s – Learning-Based Parametric Registration and Hybrid Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html">Lecture 06.1 - Fitting the SMPL Model to Images via Optimization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#mathematical-background-pinhole-camera-and-projections">Mathematical Background: Pinhole Camera and Projections</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#perspective-projection">Perspective Projection</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#weak-perspective-projection">Weak-Perspective Projection</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#camera-extrinsics-vs-model-pose">Camera Extrinsics vs. Model Pose</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#d-keypoints-and-projection">2D Keypoints and Projection</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#the-smpl-model-as-a-differentiable-function-of-shape-and-pose">The SMPL Model as a Differentiable Function of Shape and Pose</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#shape-blend-shapes">Shape Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#pose-blend-shapes">Pose Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#joint-positions">Joint Positions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#linear-blend-skinning">Linear Blend Skinning</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#differentiability-of-smpl">Differentiability of SMPL</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#fitting-smpl-to-images-via-optimization-smplify">Fitting SMPL to Images via Optimization (SMPLify)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#objective-function">Objective Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#combined-objective">Combined Objective</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#optimization-strategy">Optimization Strategy</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#optimization-algorithms">Optimization Algorithms</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#automatic-differentiation-and-jacobians">Automatic Differentiation and Jacobians</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#result-of-smplify">Result of SMPLify</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#historical-progression-and-method-comparisons">Historical Progression and Method Comparisons</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#smplify-bogo-et-al-2016">SMPLify (Bogo et al. 2016)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#smplify-x-2019">SMPLify-X (2019)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html">Lecture 06.2 - Learning-Based Fitting of the SMPL Model to Images</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#foundations-of-learning-based-smpl-estimation">Foundations of Learning-Based SMPL Estimation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#integrating-smpl-into-neural-networks">Integrating SMPL into Neural Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#projection-functions">Projection Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#loss-functions">Loss Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#statistical-priors-and-adversarial-losses">Statistical Priors and Adversarial Losses</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#early-regression-approaches-hmr-and-nbf">Early Regression Approaches: HMR and NBF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#human-mesh-recovery-hmr">Human Mesh Recovery (HMR)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#neural-body-fitting-nbf">Neural Body Fitting (NBF)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#evolving-architectures-hybrid-and-improved-regression-methods">Evolving Architectures: Hybrid and Improved Regression Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#spin-optimization-in-the-training-loop">SPIN: Optimization in the Training Loop</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#pymaf-pyramidal-mesh-alignment-feedback">PyMAF: Pyramidal Mesh Alignment Feedback</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#cliff-using-full-frame-context-for-camera-orientation">CLIFF: Using Full-Frame Context for Camera Orientation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#pixie-whole-body-regression-with-part-experts">PIXIE: Whole-Body Regression with Part Experts</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#temporal-methods-from-single-images-to-video-sequences">Temporal Methods: From Single Images to Video Sequences</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#vibe-adversarial-motion-prior-with-grus">VIBE: Adversarial Motion Prior with GRUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#tcmr-temporally-consistent-mesh-recovery">TCMR: Temporally Consistent Mesh Recovery</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#motionbert-transformer-based-motion-representations">MotionBERT: Transformer-Based Motion Representations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#comparison-of-learning-based-methods">Comparison of Learning-Based Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#supervision-and-data">Supervision and Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#network-architecture">Network Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#objective-functions">Objective Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#pose-and-shape-priors">Pose and Shape Priors</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#performance">Performance</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#runtime">Runtime</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#strengths-and-weaknesses">Strengths and Weaknesses</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html">Lecture 07.1: Fitting SMPL to IMU Data Using Optimization-Based Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#classical-imu-based-pose-estimation-a-historical-perspective">Classical IMU-Based Pose Estimation: A Historical Perspective</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#attitude-and-heading-reference-systems">Attitude and Heading Reference Systems</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#kalman-filter-approaches">Kalman Filter Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#early-sparse-sensor-approaches">Early Sparse-Sensor Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#model-based-optimization-methods">Model-Based Optimization Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#learning-based-methods">Learning-Based Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#inertial-sensor-fundamentals-and-orientation-representations">Inertial Sensor Fundamentals and Orientation Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#gravity-alignment-and-drift-correction">Gravity Alignment and Drift Correction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#orientation-representations">Orientation Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#sensor-calibration">Sensor Calibration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#optimization-based-smpl-fitting-with-imu-data">Optimization-Based SMPL Fitting with IMU Data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#kinematic-model-and-sensor-prediction">Kinematic Model and Sensor Prediction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#regularization-and-prior-terms">Regularization and Prior Terms</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#gradient-and-jacobian-computation">Gradient and Jacobian Computation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#pseudocode-smpl-pose-estimation-from-imu-sequence">Pseudocode: SMPL Pose Estimation from IMU Sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#imu-based-human-pose-datasets-and-resources">IMU-Based Human Pose Datasets and Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#dip-imu-dataset-2018">DIP-IMU Dataset (2018)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#totalcapture-2017">TotalCapture (2017)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#amass-2019">AMASS (2019)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#other-datasets-and-resources">Other Datasets and Resources</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Lecture 07.2: Fitting SMPL to IMU Data Using Learning-Based Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#optimization-based-vs-learning-based-approaches">Optimization-Based vs. Learning-Based Approaches</a></li>
<li class="toctree-l2"><a class="reference internal" href="#learning-based-imu-to-pose-estimation-historical-overview-of-key-models">Learning-Based IMU-to-Pose Estimation: Historical Overview of Key Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#deep-inertial-poser-dip-2018">Deep Inertial Poser (DIP, 2018)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#transpose-2021">TransPose (2021)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#transformer-inertial-poser-tip-2022">Transformer Inertial Poser (TIP, 2022)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#physics-physical-inertial-poser-pip-2022">Physics/Physical Inertial Poser (PIP, 2022)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#other-notable-models-and-developments">Other Notable Models and Developments</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#problem-formulation-and-learning-task-definition">Problem Formulation and Learning Task Definition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#input-and-output-representations">Input and Output Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#learning-objective-and-loss-functions">Learning Objective and Loss Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#temporal-modeling-approaches">Temporal Modeling Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="#supervised-vs-semi-supervised-training-synthetic-data">Supervised vs. Semi-Supervised Training; Synthetic Data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#model-architectures-and-design-considerations">Model Architectures and Design Considerations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#encoding-imu-measurements">Encoding IMU Measurements</a></li>
<li class="toctree-l3"><a class="reference internal" href="#network-structures">Network Structures</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#training-pipeline-and-pseudocode">Training Pipeline and Pseudocode</a></li>
<li class="toctree-l2"><a class="reference internal" href="#datasets-benchmarks-and-resources">Datasets, Benchmarks, and Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#dip-imu-dataset-2018">DIP-IMU Dataset (2018)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#totalcapture-2017">TotalCapture (2017)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#amass-2019">AMASS (2019)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#other-datasets-and-resources">Other Datasets and Resources</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#challenges-and-outlook">Challenges and Outlook</a></li>
<li class="toctree-l2"><a class="reference internal" href="#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="extended_materials_neural_radiance_fields.html">Neural Radiance Fields: A Historical and Theoretical Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#foundations-3d-scene-representation-and-reconstruction-techniques">Foundations: 3D Scene Representation and Reconstruction Techniques</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#voxel-grids">Voxel Grids</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#point-clouds">Point Clouds</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#mesh-based-surfaces">Mesh-Based Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#light-fields-and-volumetric-rendering">Light Fields and Volumetric Rendering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#emergence-of-neural-radiance-fields-nerf">Emergence of Neural Radiance Fields (NeRF)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#core-idea">Core Idea</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#training-procedure">Training Procedure</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#nerf-architecture">NeRF Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#hierarchical-sampling">Hierarchical Sampling</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#original-results">Original Results</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#theoretical-and-mathematical-analysis-of-nerf">Theoretical and Mathematical Analysis of NeRF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#volume-rendering-formulation-in-nerf">Volume Rendering Formulation in NeRF</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#positional-encoding-and-neural-network-architecture">Positional Encoding and Neural Network Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#loss-function-and-optimization">Loss Function and Optimization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#major-advancements-and-extensions-of-nerf">Major Advancements and Extensions of NeRF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#anti-aliasing-and-unbounded-scenes-mip-nerf-and-nerf">Anti-Aliasing and Unbounded Scenes: mip-NeRF and NeRF++</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#efficiency-improvements-instant-nerf-and-plenoctrees">Efficiency Improvements: Instant NeRF and PlenOctrees</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#dynamic-and-deformable-nerfs-d-nerf-nerfies-nsff-etc">Dynamic and Deformable NeRFs (D-NeRF, Nerfies, NSFF, etc.)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#neural-radiance-fields-for-human-modeling-with-smpl-and-body-models">Neural Radiance Fields for Human Modeling (with SMPL and Body Models)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#other-notable-extensions">Other Notable Extensions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#comparison-with-other-3d-representations">Comparison with Other 3D Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-polygonal-meshes">Vs. Polygonal Meshes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-point-clouds-3d-splatting">Vs. Point Clouds / 3D Splatting</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-voxel-grids-and-volumetric-methods">Vs. Voxel Grids and Volumetric Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-multi-plane-images-mpis-light-fields">Vs. Multi-Plane Images (MPIs) / Light Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#accuracy-and-fidelity">Accuracy and Fidelity</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#applicability">Applicability</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#datasets-for-nerf-training-and-evaluation">Datasets for NeRF Training and Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#blender-synthetic-nerf-dataset">Blender Synthetic NeRF Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#local-light-field-fusion-llff-real-forward-facing-dataset">Local Light Field Fusion (LLFF) Real Forward-Facing Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#tanks-and-temples">Tanks and Temples</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#dtu-dataset">DTU Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#human3-6m">Human3.6M</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#zju-mocap-dataset">ZJU-MoCap Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#people-snapshot">People-Snapshot</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#synthetic-dynamic-scenes">Synthetic dynamic scenes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#other-datasets">Other datasets</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html">3D Gaussian Splatting: A Basic Introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#foundations">Foundations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#what-is-a-3d-scene">What is a 3D Scene?</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-scene-representation">3D Scene Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#computer-graphics-fundamentals">Computer Graphics Fundamentals</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#rasterization-and-ray-tracing">Rasterization and Ray Tracing</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#alpha-blending-and-compositing">Alpha Blending and Compositing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#the-evolution-of-novel-view-synthesis">The Evolution of Novel View Synthesis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#image-based-rendering">Image-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#structure-from-motion-and-multi-view-stereo">Structure-from-Motion and Multi-View Stereo</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#point-based-rendering">Point-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#neural-rendering">Neural Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#accelerated-neural-fields">Accelerated Neural Fields</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussian-splatting-a-convergence-of-approaches">3D Gaussian Splatting: A Convergence of Approaches</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#id1">Point-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#point-clouds-and-their-challenges">Point Clouds and Their Challenges</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#the-concept-of-splatting">The Concept of Splatting</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#elliptical-weighted-average-ewa-filtering">Elliptical Weighted Average (EWA) Filtering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-point-based-rendering">Differentiable Point-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussian-splatting-core-principles">3D Gaussian Splatting: Core Principles</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#key-insight-unifying-points-and-volumes">Key Insight: Unifying Points and Volumes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussians-as-scene-primitives">3D Gaussians as Scene Primitives</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#the-volumetric-rendering-equation">The Volumetric Rendering Equation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#alpha-compositing-with-gaussians">Alpha Compositing with Gaussians</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#mathematical-formulation-of-3d-gaussian-splatting">Mathematical Formulation of 3D Gaussian Splatting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#projecting-3d-gaussians-to-2d">Projecting 3D Gaussians to 2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#parameterization-of-3d-gaussians">Parameterization of 3D Gaussians</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#view-dependent-appearance">View-Dependent Appearance</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-rendering-equations">Differentiable Rendering Equations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#training-and-optimization">Training and Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#photometric-loss">Photometric Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#initial-point-cloud">Initial Point Cloud</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#optimization-process">Optimization Process</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-splatting-pipeline">Differentiable Splatting Pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#adaptive-density-control">Adaptive Density Control</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#implementation-and-real-time-rendering">Implementation and Real-Time Rendering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#tile-based-rendering">Tile-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#fast-sorting-strategies">Fast Sorting Strategies</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#gpu-accelerated-rasterization">GPU-Accelerated Rasterization</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#memory-considerations">Memory Considerations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#comparison-with-other-methods">Comparison with Other Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#nerf-vs-3d-gaussian-splatting">NeRF vs. 3D Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#voxel-based-representations-vs-gaussians">Voxel-Based Representations vs. Gaussians</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#traditional-point-based-rendering-vs-gaussian-splatting">Traditional Point-Based Rendering vs. Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#applications-and-extensions">Applications and Extensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#static-scene-reconstruction">Static Scene Reconstruction</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#dynamic-scene-capture">Dynamic Scene Capture</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#avatar-creation-and-animation">Avatar Creation and Animation</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#integration-with-neural-rendering">Integration with Neural Rendering</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#large-scale-scene-rendering">Large-Scale Scene Rendering</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#bezier-gaussian-triangles-bg-triangle-for-sharper-rendering">Bézier Gaussian Triangles (BG-Triangle) for Sharper Rendering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#representation">Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#performance">Performance</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#human-reconstruction-with-gaussian-splatting-and-priors">Human Reconstruction with Gaussian Splatting and Priors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#eg-humannerf-efficient-generalizable-human-nerf">EG-HumanNeRF: Efficient Generalizable Human NeRF</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#gps-gaussian-pixel-wise-gaussian-splatting-for-humans">GPS-Gaussian: Pixel-Wise Gaussian Splatting for Humans</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#generalizable-human-gaussians-ghg-with-smpl">Generalizable Human Gaussians (GHG) with SMPL</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#dynamic-scene-reconstruction-with-4d-gaussian-splatting">Dynamic Scene Reconstruction with 4D Gaussian Splatting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussian-splatting-4dgs">4D Gaussian Splatting (4DGS)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#speed-and-memory-enhancements-4dgs-1k-and-mega">Speed and Memory Enhancements (4DGS-1K and MEGA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#applications-to-mocap-and-4d-human-rendering">Applications to MoCap and 4D Human Rendering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#implementation-details-and-real-time-performance">Implementation Details and Real-Time Performance</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#data-structures">Data Structures</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#rasterization-shaders">Rasterization &amp; Shaders</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#gpu-memory-and-throughput">GPU Memory and Throughput</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-rendering-implementation">Differentiable Rendering Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#training-vs-inference-compute">Training vs. Inference Compute</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#accuracy-vs-speed-trade-offs">Accuracy vs. Speed trade-offs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#benchmarks-and-comparative-evaluation">Benchmarks and Comparative Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#static-scene-comparison">Static Scene Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#human-novel-view-comparison">Human Novel-View Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#dynamic-scene-comparison">Dynamic Scene Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#comparative-summary-table">Comparative Summary Table</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#datasets-and-resources">Datasets and Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#synthetic-nerf-dataset-blender-scenes">Synthetic NeRF Dataset (Blender Scenes)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#llff-local-light-field-fusion">LLFF (Local Light Field Fusion)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#tanks-and-temples">Tanks and Temples</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#multi-object-360-co3d-common-objects-in-3d">Multi-Object 360 (CO3D - Common Objects in 3D)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#amass-archive-of-motion-capture-as-surface-shapes">AMASS (Archive of Motion Capture as Surface Shapes)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#cape-clothed-auto-person-encoding">CAPE (Clothed Auto Person Encoding)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#thuman-thuman2-0">THuman / THuman2.0</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#renderpeople">RenderPeople</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#open-source-implementations">Open-Source Implementations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#future-directions">Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#improved-compression-techniques-for-memory-efficiency">Improved Compression Techniques for Memory Efficiency</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#handling-dynamic-and-deformable-scenes">Handling Dynamic and Deformable Scenes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#advanced-material-modeling-for-realistic-rendering">Advanced Material Modeling for Realistic Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#hybrid-approaches-integrating-neural-fields-and-explicit-representations">Hybrid Approaches Integrating Neural Fields and Explicit Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#scalability-for-large-scale-scenes-city-level-and-beyond">Scalability for Large-Scale Scenes (City-Level and Beyond)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#real-time-applications-in-ar-vr-and-gaming">Real-Time Applications in AR/VR and Gaming</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#integration-with-existing-graphics-pipelines">Integration with Existing Graphics Pipelines</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#learning-from-limited-data">Learning from Limited Data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#conclusion">Conclusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#glossary">Glossary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a><ul>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-01-1-historical-body-models">Lecture 01.1 (Historical Body Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-01-2-introduction-to-human-models">Lecture 01.2 (Introduction to Human Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-01-3-introduction-to-human-models-continued">Lecture 01.3 (Introduction to Human Models Continued)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-02-1-image-formation">Lecture 02.1 (Image Formation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-02-2-rotations-kinematic-chains">Lecture 02.2 (Rotations &amp; Kinematic Chains)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-03-1-surface-representations">Lecture 03.1 (Surface Representations)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-03-2-procrustes-alignment">Lecture 03.2 (Procrustes Alignment)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-04-1-iterative-closest-points">Lecture 04.1 (Iterative Closest Points)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-04-2-body-models">Lecture 04.2 (Body Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-05-1-body-model-training">Lecture 05.1 (Body Model Training)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-05-2-3d-registration">Lecture 05.2 (3D Registration)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-06-1-fitting-smpl-to-images">Lecture 06.1 (Fitting SMPL to Images)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-06-1-optimization-based-fitting-of-smpl-to-images">Lecture 06.1 (Optimization-Based Fitting of SMPL to Images)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-06-2-learning-based-fitting-of-smpl-to-images">Lecture 06.2 (Learning-Based Fitting of SMPL to Images)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-07-1-fitting-smpl-to-imu-optimization">Lecture 07.1 (Fitting SMPL to IMU Optimization)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-07-2-fitting-smpl-to-imu-learning">Lecture 07.2 (Fitting SMPL to IMU Learning)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="references.html#classic-and-optimization-based-methods">Classic and Optimization-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="references.html#learning-based-methods">Learning-Based Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="references.html#datasets-and-resources">Datasets and Resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#relevant-software-and-libraries">Relevant Software and Libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#neural-radiance-fields-nerf">Neural Radiance Fields (NERF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#gaussian-splatting">Gaussian Splatting</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Virtual Humans Lecture</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Lecture 07.2: Fitting SMPL to IMU Data Using Learning-Based Methods</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/lecture_07_2_fitting_SMPL_to_IMU_learning.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="lecture-07-2-fitting-smpl-to-imu-data-using-learning-based-methods">
<span id="lecture-fitting-smpl-to-imu-learning"></span><h1>Lecture 07.2: Fitting SMPL to IMU Data Using Learning-Based Methods<a class="headerlink" href="#lecture-07-2-fitting-smpl-to-imu-data-using-learning-based-methods" title="Link to this heading"></a></h1>
<iframe width="600" height="400" src="https://www.youtube.com/embed/HLbuRR3RIdk"
title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write;
encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><p><a class="reference external" href="https://virtualhumans.mpi-inf.mpg.de/VH23/slides/pdf/Lecture_07_2_Fitting_SMPL_to_IMU_Learning.pdf">Lecture Slides: Fitting SMPL to IMU Data Using Learning-Based Methods</a></p>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h2>
<p>Estimating human body pose from inertial sensor data lies at the intersection of graphics, vision, and machine learning. Wearable IMUs (Inertial Measurement Units) provide orientation and acceleration readings for the body parts they’re attached to, but unlike camera-based motion capture, they don’t directly provide global position information. Our task is to fit a parametric human model (SMPL) to a sparse set of IMU signals – typically only 6 sensors on the body – and recover the full 3D pose.</p>
<p>This problem is severely under-constrained: many different body poses can produce the same set of IMU readings. Traditional solutions used optimization to enforce physical consistency, but these tend to be slow and operate offline. Recent learning-based approaches leverage data-driven priors to solve IMU-to-pose estimation in real time.</p>
<p>This chapter provides a comprehensive overview of learning-based methods for IMU-driven pose reconstruction, focusing on techniques that output SMPL pose parameters from sparse IMUs.</p>
</section>
<section id="optimization-based-vs-learning-based-approaches">
<h2>Optimization-Based vs. Learning-Based Approaches<a class="headerlink" href="#optimization-based-vs-learning-based-approaches" title="Link to this heading"></a></h2>
<p>Early approaches to IMU-based pose estimation treated it as an optimization problem: given IMU sensor measurements, find the body pose parameters that best reproduce those measurements using a human model.</p>
<p><strong>Optimization-Based Approaches</strong></p>
<p>A seminal example is Sparse Inertial Poser (SIP) by von Marcard et al. (2017). SIP attaches 6 IMUs (on the wrists, lower legs, head, and back) to a subject and fits the SMPL model’s pose parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> (and shape <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>) such that the model’s virtual sensors have orientations and accelerations matching the measured IMUs over time.</p>
<p>SIP uses a realistic statistical body model (SMPL) to impose anthropometric constraints and performs a joint optimization over multiple frames. This yields physically plausible pose sequences from only sparse sensors, even for challenging motions. However, SIP is an offline method – it requires minutes of computation for a motion sequence – and cannot run in real time. Moreover, solving the non-linear least-squares fitting can be sensitive to initialization and might still drift without strong priors.</p>
<p><strong>Learning-Based Approaches</strong></p>
<p>Learning-based approaches replace the iterative optimization with a trained predictive model (typically a deep neural network). The idea is to have a model learn the mapping from IMU sensor signals to human pose, based on examples. This was first demonstrated by Deep Inertial Poser (DIP) in 2018.</p>
<p>Learning-based methods bring several advantages:
- Once trained, they are extremely fast at runtime (achieving real-time inference, e.g., 60–90 FPS)
- They can implicitly learn human motion priors from data rather than relying on hand-crafted regularizers
- They naturally handle temporal dependencies by training on motion sequences</p>
<p>On the downside, learning methods require large amounts of training data (with ground-truth poses), which can be hard to obtain from IMU setups. They may also struggle with generalization to motions or subjects not well-represented in training data and can produce implausible results if asked to extrapolate beyond their learned priors.</p>
<p><strong>Hybrid Approaches</strong></p>
<p>A hybrid approach attempts to combine the strengths of both. For example, the Physics Informed/Physical Inertial Poser (PIP) method (2022) uses a neural network to produce an initial pose estimate quickly (the “kinematics” stage) and then runs a lightweight physics-based optimization to refine the pose so that it obeys physical constraints (ground contact, dynamics consistency).</p>
<p>This two-stage strategy still runs in real time (PIP achieves 60 FPS with 16 ms latency) and improves accuracy and physical realism over pure learning methods. Hybrid approaches highlight an important trend: purely data-driven models can be augmented with domain knowledge (physics, kinematics) to curb their failure modes (like foot skating or gradual drift).</p>
<p>In summary:
- Optimization-based methods (like SIP) excel in accuracy and physical consistency but are offline and require careful tuning
- Learning-based methods (like DIP and successors) enable real-time motion capture from IMUs at the cost of needing extensive training data
- Hybrid methods strive for the best of both: using learning for speed and optimization/physics for accuracy and plausibility</p>
</section>
<section id="learning-based-imu-to-pose-estimation-historical-overview-of-key-models">
<h2>Learning-Based IMU-to-Pose Estimation: Historical Overview of Key Models<a class="headerlink" href="#learning-based-imu-to-pose-estimation-historical-overview-of-key-models" title="Link to this heading"></a></h2>
<section id="deep-inertial-poser-dip-2018">
<h3>Deep Inertial Poser (DIP, 2018)<a class="headerlink" href="#deep-inertial-poser-dip-2018" title="Link to this heading"></a></h3>
<p>Deep Inertial Poser (DIP) was the first deep learning method to estimate full-body 3D pose from a sparse set of wearable IMUs. Published at SIGGRAPH Asia 2018 (Huang et al.), DIP demonstrated that a neural network can replace the expensive optimization used in SIP and still recover accurate poses in real time. DIP uses only 6 IMUs (on the lower legs, wrists, head, and pelvis/back) and outputs the 3D pose in the format of SMPL’s joint rotation parameters.</p>
<p><strong>Data and Training:</strong></p>
<p>One challenge DIP faced was the lack of large paired IMU–pose datasets. To train the network, DIP generated synthetic IMU data from existing motion capture datasets. The authors leveraged the newly introduced AMASS dataset (a large collection of mocap sequences retargeted to SMPL format) to obtain diverse human motions.</p>
<p>Using the SMPL model’s kinematics, they simulated IMU readings (sensor orientations and accelerations) for each motion by placing virtual sensors on the SMPL body and computing their orientation and linear acceleration over time. This provided effectively unlimited training data in a unified format.</p>
<p>DIP also recorded a smaller real dataset called DIP-IMU for validation and fine-tuning. The DIP-IMU dataset consists of 10 subjects wearing 17 Xsens IMUs, with ground-truth SMPL poses obtained via a full-suit optimization; it contains 64 sequences (about 330k frames) and is one of the largest IMU-motion datasets made public.</p>
<p><strong>Network Architecture:</strong></p>
<p>DIP employs a recurrent neural network (RNN) to model the temporal sequence of poses. In particular, DIP uses a stacked bi-directional RNN with gated recurrent units (GRUs) or LSTM units that process the IMU signals over time.</p>
<p>At each time step, the input to the network is the collection of sensor measurements from all 6 IMUs (each providing an orientation, represented e.g. as a quaternion or rotation matrix, and a linear acceleration vector). DIP normalizes these measurements carefully – for example, accounting for different coordinate frames (sensor local frame vs. global frame vs. SMPL body frame) so that the network learns in a consistent space.</p>
<p>The bi-directional RNN means that during training the network has access to both past and future IMU readings when estimating the pose at a given frame, which helps accuracy by smoothing over ambiguous frames. At test time, future data isn’t available, so DIP is deployed in a sliding window or forward-only manner to maintain real-time performance.</p>
<p><strong>Output and Loss:</strong></p>
<p>The network outputs the pose in terms of SMPL joint rotations for each time step. DIP represented rotations in axis-angle form (3 parameters per joint, 72-D output for 24 joints) or a similar rotation representation.</p>
<p>To account for the high uncertainty in this problem – certain IMU configurations don’t uniquely determine a pose – DIP’s loss function was formulated as a negative log-likelihood of a Gaussian output distribution. In practice, the network predicts both a mean pose and a diagonal covariance (per output dimension) at each time, effectively learning a per-joint uncertainty.</p>
<p>The training then maximizes the likelihood of the ground-truth under that Gaussian (equivalent to a weighted L2 loss where the weights are learned variances). This heteroscedastic regression allows the model to express less confidence (higher predicted variance) in ambiguous degrees of freedom.</p>
<p><strong>Performance:</strong></p>
<p>DIP demonstrated, for the first time, real-time 3D pose reconstruction from 6 IMUs. It achieves over 120 fps on a GPU, far exceeding real-time needs, and qualitatively the reconstructed motions are smooth and closely match the ground-truth motion capture.</p>
<p>On standard benchmarks (like TotalCapture and DIP-IMU test data), DIP showed accuracy improvements over the optimization baselines and prior methods, especially in dynamic motions, while running orders of magnitude faster.</p>
</section>
<section id="transpose-2021">
<h3>TransPose (2021)<a class="headerlink" href="#transpose-2021" title="Link to this heading"></a></h3>
<p>After DIP, one limitation remained: DIP (and similar models) estimated joint rotations but did not explicitly compute global translations of the body. In other words, a subject walking forward would be reconstructed by DIP as walking in place (feet moving, but root position fixed) because IMUs alone do not provide absolute position.</p>
<p>TransPose (Xinyu Yi et al., SIGGRAPH 2021) tackled this by producing both the body pose and the global trajectory from IMUs. TransPose is a DNN-based full motion capture system using 6 IMUs, achieving over 90 fps in real time.</p>
<p><strong>Pose Estimation Innovations:</strong></p>
<p>For the body pose, TransPose introduced a multi-stage neural network that estimates joint positions in a hierarchical manner (from extremities inward) before resolving the final joint rotations.</p>
<p>Specifically, the network first predicts the 3D positions of “leaf” joints (e.g., hands, feet, head) relative to the root, then uses these as intermediate constraints to predict the positions of more central joints, and so on, ultimately inferring the full skeletal pose.</p>
<p>By breaking the problem down into leaf-to-root predictions, the network can more easily satisfy kinematic constraints (like where the feet and hands should be) before determining the internal posture. This design improved both accuracy and computational efficiency of pose estimation.</p>
<p><strong>Global Translation Estimation:</strong></p>
<p>A key contribution of TransPose is a solution for global body translation (the position of the root in the world). With no direct positional sensors, TransPose uses two strategies:
1. A supporting-foot heuristic
2. An RNN-based predictor</p>
<p>These are fused by a confidence measure. The supporting-foot method leverages the fact that when one foot is on the ground and stationary, the body’s horizontal displacement can be inferred by assuming that foot is static on the floor.</p>
<p>The second method is an RNN that directly learns to predict the velocity or displacement of the body from the IMU sequence. TransPose combines these two estimates with a confidence-based fusion: when the supporting-foot detector is confident (e.g., foot contact is certain), that solution is trusted more; otherwise the learned translation is used.</p>
<p>This yields robust global position tracking – for example, a person can walk around a large area and TransPose’s virtual avatar will accurately follow the trajectory, something not possible with DIP alone.</p>
<p><strong>Performance and Impact:</strong></p>
<p>TransPose reported substantially higher accuracy than prior state-of-the-art (including DIP and SIP) on benchmark datasets. By capturing global motion, it enabled full 3D motion capture from IMUs alone, previously achievable only with additional external references like cameras.</p>
<p>TransPose remained efficient, running at 90+ fps. It was also evaluated on live scenarios (the authors demonstrated ping-pong playing, umbrella walking, etc. with only IMUs) and showed reliable results.</p>
</section>
<section id="transformer-inertial-poser-tip-2022">
<h3>Transformer Inertial Poser (TIP, 2022)<a class="headerlink" href="#transformer-inertial-poser-tip-2022" title="Link to this heading"></a></h3>
<p>While RNNs were effective for DIP and TransPose, the field soon explored Transformers for modeling temporal sequences of IMU data. Transformer Inertial Poser (TIP) (Jiang et al., SIGGRAPH Asia 2022) is an attention-based model that estimates full-body motion from 6 IMUs and even generates the 3D terrain profile that the person walked on.</p>
<p>TIP addresses some key challenges: maintaining long-term temporal consistency, minimizing drift in global/joint motion, and handling a variety of motions on different terrains.</p>
<p><strong>Transformer Architecture:</strong></p>
<p>TIP uses a conditional Transformer decoder architecture. Rather than processing the IMU sequence purely with recurrence, TIP’s Transformer can attend over a history window of inputs and past predictions.</p>
<p>It explicitly feeds back the previous pose outputs as input context for the next prediction, thereby “reasoning about the prediction history”. This design helps produce consistent predictions and avoid jitter, as the model can learn to not deviate drastically from recent poses and to enforce smooth kinematics.</p>
<p>The self-attention mechanism in the Transformer is well-suited to capturing both short- and long-range dependencies (e.g., patterns in gait or periodic motions) that RNNs might struggle with if not well-trained.</p>
<p><strong>Stationary Body Points (SBP):</strong></p>
<p>One of TIP’s novel concepts is the introduction of Stationary Body Points (SBPs) as a learning target. SBPs are points on the body that are momentarily stationary with respect to the world (for example, a foot during stance phase, or perhaps the hips when sitting on a chair).</p>
<p>TIP’s network is trained to predict which body points are stationary at each time and to output their global positions. These SBPs can be identified robustly by the Transformer. TIP then uses analytical routines to enforce consistency at those points.</p>
<p>For instance, if the network predicts that the left foot is an SBP at frame t (meaning the left foot should be on the ground and not moving), TIP will correct the raw pose prediction by adjusting the left foot’s pose to exactly maintain its previous global position (preventing any slight drift or jitter in that foot).</p>
<p>SBP predictions thus act as self-discovered “contact constraints” that the system applies to improve physical realism. This is a form of post-processing using network outputs: the network doesn’t directly output a final pose, but rather intermediate signals (SBPs) that allow a deterministic fixing of drift.</p>
<p><strong>Terrain Generation:</strong></p>
<p>Because TIP can identify when and where feet are stationary, it can also infer properties of the ground. TIP includes an algorithm to generate a terrain height map from the pattern of SBPs.</p>
<p>Essentially, as the person walks, TIP accumulates the lowest positions of stationary feet and assumes those lie on the ground surface, constructing a coarse height field of the walked terrain. This is useful for visualization (showing the ground the person likely walked on) and more importantly, TIP feeds the terrain information back to correct global motion.</p>
<p>If the terrain is uneven, the global root motion can be adjusted to respect the height map (ensuring the feet meet the ground height). By integrating environment context, TIP goes beyond predicting pose in isolation, making the result more plausible.</p>
<p><strong>Performance:</strong></p>
<p>TIP was evaluated on both synthetic data and real IMU recordings, demonstrating improved accuracy over strong baselines like DIP and TransPose. It showed excellent temporal stability, with much reduced drift in joint positions thanks to SBP corrections.</p>
<p>In live demos, TIP could reconstruct motions on various imagined terrains in real time.</p>
</section>
<section id="physics-physical-inertial-poser-pip-2022">
<h3>Physics/Physical Inertial Poser (PIP, 2022)<a class="headerlink" href="#physics-physical-inertial-poser-pip-2022" title="Link to this heading"></a></h3>
<p>While the above learning-based methods implicitly learn kinematic priors, they do not guarantee physical correctness (e.g., a jump might not obey gravity, or predicted joint torques could be implausible).</p>
<p>Physical Inertial Poser (PIP) (Xinyu Yi et al., CVPR 2022) takes a hybrid approach by integrating a physics simulation layer with the neural network. PIP is physics-aware real-time motion capture from 6 IMUs, and notably it estimates not only pose but also joint torques and ground reaction forces – a full dynamics solution.</p>
<p><strong>Two-Module Approach:</strong></p>
<p>PIP consists of a neural kinematics module followed by a physics-based optimizer. The first stage is similar to TransPose/DIP: a neural network (in PIP’s case, a relatively lightweight one to keep latency low) regresses an initial pose sequence from the IMU data.</p>
<p>This kinematic output on its own might have minor foot skating or may drift over long sequences (since the network isn’t perfect). The second stage then takes that initial motion as a reference and runs a short physics simulation/optimization to adjust the motion so that it satisfies Newtonian physics and environmental contacts.</p>
<p>This involves a dynamic model of the human (often using an articulated rigid body model matching SMPL’s skeleton, with masses, inertia, etc.) and possibly a ground plane. The optimizer in PIP corrects any physical violations – e.g., if a foot was predicted slightly below the ground or moving when it should be static, the physics module will adjust forces/torques to keep it planted.</p>
<p>If the initial motion implied non-zero acceleration of center-of-mass without sufficient ground reaction force, the physics correction will tweak the motion (and output forces) to obey momentum conservation. In essence, the second stage finds the closest physically valid motion to the network’s output.</p>
<p>Because the network’s output is already close to correct, the physics optimization can converge very fast (PIP manages 60 Hz total). The result is a pose sequence that not only fits the IMU data but also could be produced by a plausible set of forces and torques – i.e., it’s physically simulatable.</p>
<p>PIP outputs the estimated joint torques and ground reaction forces as well, which are by-products of making the motion physical. This is valuable for biomechanics or VR applications requiring feedback forces.</p>
<p><strong>Performance:</strong></p>
<p>PIP achieved state-of-the-art accuracy on standard evaluations, with notable improvements in temporal stability and physical realism. For example, PIP’s output for very long motions does not drift over time, unlike some purely learned models that might accumulate small errors.</p>
<p>Qualitatively, motions like running and jumping are handled more faithfully – the jumps have proper apex and landing dynamics, and running has realistic foot pushes. The authors reported that previous methods struggled with such long or dynamic motions, producing artifacts due to lack of physics.</p>
<p>PIP was even recognized as a Best Paper Finalist at CVPR 2022, indicating the significance of combining learning with physics.</p>
</section>
<section id="other-notable-models-and-developments">
<h3>Other Notable Models and Developments<a class="headerlink" href="#other-notable-models-and-developments" title="Link to this heading"></a></h3>
<p>In addition to the above primary methods, there have been other significant contributions in the IMU-to-pose literature:</p>
<p><strong>Fusion Methods:</strong> Some works combine IMUs with other sensors (e.g., cameras). For instance, the TotalCapture project (Trumble et al. 2017) itself presented a method fusing multi-view video with IMUs to improve pose accuracy. More recent “fusion poser” approaches use deep networks to blend sparse IMUs with a smartphone camera feed or ambient sensors to enhance global positioning.</p>
<p><strong>High-Frequency and Low-Latency Models:</strong> One work proposed a Fast Inertial Poser (2024) that simplifies the network to run on mobile devices in real time, fusing raw IMU streams with efficient temporal convolution (targeting &gt;100 Hz on a phone).</p>
<p><strong>Handling Loose or Varied Sensor Placement:</strong> A challenge is that IMUs might not always be tightly attached at known body landmarks. The Loose Inertial Poser (LIP) introduced methods to handle the misalignment and slippage of sensors on a loose garment (a jacket). By learning a calibration and compensation for sensor movement relative to the body, it achieves accurate pose with a more user-friendly setup.</p>
<p><strong>Commercial Systems and Extended Datasets:</strong> Companies like Xsens (Roetenberg et al., 2007) have full-body IMU suits (with 17 sensors) that use proprietary sensor fusion algorithms for pose. These are not purely learning-based (they use extended Kalman filters and models), but provide ground truth data and set benchmarks. Datasets like TotalCapture and DIP-IMU have been used to evaluate new algorithms.</p>
</section>
</section>
<section id="problem-formulation-and-learning-task-definition">
<h2>Problem Formulation and Learning Task Definition<a class="headerlink" href="#problem-formulation-and-learning-task-definition" title="Link to this heading"></a></h2>
<section id="input-and-output-representations">
<h3>Input and Output Representations<a class="headerlink" href="#input-and-output-representations" title="Link to this heading"></a></h3>
<p>We define the human body model and sensor measurements formally. The SMPL model parameterizes a human body pose by a vector <span class="math notranslate nohighlight">\(\boldsymbol{\theta} \in \mathbb{R}^{3K}\)</span>, where typically <span class="math notranslate nohighlight">\(K=24\)</span> joints and each joint’s rotation is given in 3-parameter axis-angle form. We can denote the pose as <span class="math notranslate nohighlight">\(\boldsymbol{\theta} = [\boldsymbol{\theta}_1, \boldsymbol{\theta}_2,\dots,\boldsymbol{\theta}_K]\)</span> where each <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_j \in \mathbb{R}^3\)</span> is the axis-angle rotation of joint <span class="math notranslate nohighlight">\(j\)</span> in the kinematic chain (including the root orientation).</p>
<p>The body shape is given by parameters <span class="math notranslate nohighlight">\(\boldsymbol{\beta} \in \mathbb{R}^{N}\)</span> (often <span class="math notranslate nohighlight">\(N=10\)</span> principal shape coefficients) which can be fixed or predicted, though most learning-based methods assume an average shape or calibrate <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> beforehand. The full 3D mesh or joint coordinates <span class="math notranslate nohighlight">\(\mathbf{J}(\boldsymbol{\theta}, \boldsymbol{\beta})\)</span> can be obtained via SMPL’s forward kinematics function.</p>
<p>An IMU sensor attached to a body segment provides two main readings:
1. The orientation of the sensor (often as a quaternion <span class="math notranslate nohighlight">\(q \in \mathbb{H}\)</span> or rotation matrix <span class="math notranslate nohighlight">\(R \in SO(3)\)</span>) with respect to the world frame
2. The linear acceleration <span class="math notranslate nohighlight">\(\mathbf{a} \in \mathbb{R}^3\)</span> of the sensor in the world frame (often including gravity)</p>
<p>Formally, let there be <span class="math notranslate nohighlight">\(M\)</span> IMUs on the body, each rigidly attached at a known location (e.g., left wrist, right wrist, etc.). At time <span class="math notranslate nohighlight">\(t\)</span>, IMU <span class="math notranslate nohighlight">\(i\)</span> provides an orientation <span class="math notranslate nohighlight">\(R_{i}^{(t)}\)</span> (which transforms a vector from the sensor’s local frame to the global inertial frame) and an acceleration vector <span class="math notranslate nohighlight">\(\mathbf{a}_{i}^{(t)}\)</span> (usually measured in the sensor’s local coordinates or converted to global coordinates).</p>
<p>The set of all IMU readings at time <span class="math notranslate nohighlight">\(t\)</span> can be denoted as:</p>
<div class="math notranslate nohighlight">
\[x(t) = \{R_1^{(t)}, a_1^{(t)}, R_2^{(t)}, a_2^{(t)}, \ldots, R_M^{(t)}, a_M^{(t)}\}\]</div>
<p>which is the input to our pose estimation system at time <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>In practice, one might represent each orientation <span class="math notranslate nohighlight">\(R_i\)</span> as a 4D quaternion or a 6D/9D rotation representation to feed into a network. Each acceleration <span class="math notranslate nohighlight">\(\mathbf{a}_i\)</span> is a 3D vector. So the input <span class="math notranslate nohighlight">\(\mathbf{x}^{(t)}\)</span> is a fixed-size vector concatenating all IMU data (e.g., for <span class="math notranslate nohighlight">\(M=6\)</span> IMUs, if using quaternions, <span class="math notranslate nohighlight">\(\mathbf{x}^{(t)} \in \mathbb{R}^{6*7}\)</span> since each IMU gives 4 (orientation) + 3 (accel) values).</p>
<p>The output of the learning-based model at time <span class="math notranslate nohighlight">\(t\)</span> is an estimate of the body’s pose (and possibly global position). This can be represented in various ways: the most direct is the SMPL pose parameter vector <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\theta}}^{(t)} \in \mathbb{R}^{72}\)</span>.</p>
<p>Some methods output the global translation <span class="math notranslate nohighlight">\(\hat{\mathbf{p}}_{\text{root}}^{(t)} \in \mathbb{R}^3\)</span> for the root (pelvis) as well, if tackling global motion (TransPose, TIP do this). Others might output joint positions <span class="math notranslate nohighlight">\(\hat{\mathbf{J}}^{(t)} \in \mathbb{R}^{3K}\)</span> directly in 3D space and then fit rotations to those (TransPose’s intermediate stage).</p>
<p>For clarity, we consider the output to ultimately be the full pose and global position: <span class="math notranslate nohighlight">\(\hat{\mathbf{y}}^{(t)} = [\hat{\boldsymbol{\theta}}^{(t)}; \hat{\mathbf{p}}_{\text{root}}^{(t)}]\)</span>. Some systems do not predict <span class="math notranslate nohighlight">\(\mathbf{p}_{root}\)</span> (DIP left it undefined, effectively setting it to zero or initial position), but newer ones do.</p>
<p>Crucially, IMU-to-pose is inherently a sequence problem. A single time step’s sensor readings <span class="math notranslate nohighlight">\(\mathbf{x}^{(t)}\)</span> is often not sufficient to determine <span class="math notranslate nohighlight">\(\mathbf{y}^{(t)}\)</span>; the context of previous and future frames helps disambiguate motion and gravity direction, etc. Therefore, the learning task is often defined over a time window or whole sequence.</p>
<p>We denote the input sequence <span class="math notranslate nohighlight">\(\mathbf{X}_{1:T} = (\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \dots, \mathbf{x}^{(T)})\)</span> and the corresponding pose sequence <span class="math notranslate nohighlight">\(\mathbf{Y}_{1:T} = (\mathbf{y}^{(1)}, \dots, \mathbf{y}^{(T)})\)</span>. The learning-based model can be seen as learning a function <span class="math notranslate nohighlight">\(f_\Theta\)</span> (with parameters <span class="math notranslate nohighlight">\(\Theta\)</span>) such that:</p>
<div class="math notranslate nohighlight">
\[\hat{\mathbf{Y}}_{1:T} = f_\Theta(\mathbf{X}_{1:T})\]</div>
<p>Often this factorizes in time (e.g., producing one frame at a time with an RNN), but the key is it uses the sequence as input to produce a sequence of outputs.</p>
</section>
<section id="learning-objective-and-loss-functions">
<h3>Learning Objective and Loss Functions<a class="headerlink" href="#learning-objective-and-loss-functions" title="Link to this heading"></a></h3>
<p>During training, we have ground-truth pose sequences (obtained from a high-quality system or simulation) for the training sequences of IMU data. Let <span class="math notranslate nohighlight">\(\mathbf{Y}_{1:T}^{*}\)</span> be the ground-truth poses. We define a loss function <span class="math notranslate nohighlight">\(L(\hat{\mathbf{Y}}, \mathbf{Y}^*)\)</span> that measures the error of our predicted sequence. Common loss terms include:</p>
<p><strong>Pose (rotation) loss:</strong> This term directly penalizes errors in joint rotations. For example, one can convert each predicted joint rotation and ground-truth rotation to axis-angle and compute the angle difference. If <span class="math notranslate nohighlight">\(\mathbf{R}_j^{(t)}(\hat{\boldsymbol{\theta}})\)</span> is the rotation matrix for joint <span class="math notranslate nohighlight">\(j\)</span> from predicted pose and <span class="math notranslate nohighlight">\(\mathbf{R}_j^{(t)}\)</span> is ground truth, an orientation loss can be:</p>
<div class="math notranslate nohighlight">
\[L_{\text{orient}} = \frac{1}{KT}\sum_{t=1}^T \sum_{j=1}^K \angle(\mathbf{R}_j^{(t)} \mathbf{R}_j^{(t)T})\]</div>
<p>where <span class="math notranslate nohighlight">\(\angle(R)\)</span> denotes the angle of the rotation <span class="math notranslate nohighlight">\(R\)</span>. This essentially sums the per-joint angular error in degrees or radians.</p>
<p>Alternatively, one can use quaternion distance or even easier, an L2 loss on axis-angle components (though care is needed around <span class="math notranslate nohighlight">\(2\pi\)</span> wrap-around). Some methods simplify and just do an L2 loss on the pose parameter vector:</p>
<div class="math notranslate nohighlight">
\[L_{\text{pose}} = \frac{1}{T}\sum_t |\hat{\boldsymbol{\theta}}^{(t)} - \boldsymbol{\theta}^{*(t)}|^2\]</div>
<p><strong>Joint position loss:</strong> Because ultimately we care about where body parts are, a popular loss is on 3D joint coordinates. If the model <span class="math notranslate nohighlight">\(M(\boldsymbol{\theta}, \boldsymbol{\beta})\)</span> produces joint positions, we can penalize:</p>
<div class="math notranslate nohighlight">
\[|M(\hat{\boldsymbol{\theta}}^{(t)}, \hat{\boldsymbol{\beta}}) - M(\boldsymbol{\theta}^{*(t)}, \boldsymbol{\beta}^*)|^2\]</div>
<p>summed over joints. This helps when small orientation errors of a proximal joint lead to big position errors at the extremities (the loss will directly account for that). DIP and others often included a position loss in addition to orientation loss to better guide the network.</p>
<p><strong>Global translation/position loss:</strong> If the method predicts root translation, one can include an L2 loss on the root’s position:</p>
<div class="math notranslate nohighlight">
\[L_{\text{trans}} = \frac{1}{T}\sum_t |\hat{\mathbf{p}}_{\text{root}}^{(t)} - \mathbf{p}_{\text{root}}^{*(t)}|^2\]</div>
<p>This was relevant for TransPose (they likely trained on some data with known trajectories, possibly synthetic or from optical motion capture like TotalCapture which provides global positions).</p>
<p><strong>Velocity and acceleration loss (smoothness):</strong> To encourage temporal consistency, loss terms can penalize differences between successive frames. For example, a velocity loss:</p>
<div class="math notranslate nohighlight">
\[L_{\text{vel}} = \frac{1}{T-1}\sum_t |\hat{\mathbf{J}}^{(t+1)} - \hat{\mathbf{J}}^{(t)} - (\mathbf{J}^{*(t+1)} - \mathbf{J}^{*(t)})|^2\]</div>
<p>which ensures the predicted motion increments match the ground truth increments.</p>
<p>If ground truth velocities are not known, one might add a smoothness regularizer:</p>
<div class="math notranslate nohighlight">
\[\sum_t |\hat{\mathbf{J}}^{(t+1)} - 2\hat{\mathbf{J}}^{(t)} + \hat{\mathbf{J}}^{(t-1)}|^2\]</div>
<p>to minimize excessive jitter (this is a second derivative (acceleration) penalty).</p>
<p><strong>Contact or consistency constraints:</strong> Recent learning approaches incorporate specific constraints via loss. For instance, one can detect foot contact in ground truth (or define it via velocity threshold) and then add a foot velocity loss for those frames: if foot should be planted, penalize any predicted foot movement.</p>
<p>TIP’s notion of Stationary Body Points (SBP) was actually integrated as a target; one could train a network to output a binary contact flag and use a loss against known contacts, or encourage predicted contacts to have zero velocity by a soft constraint.</p>
<p>Another consistency constraint is sensor reconstruction loss: since we know the input IMU orientation, one could require that the predicted pose, when fed into the forward model, reproduces the sensor readings.</p>
<p>Typically, the overall loss is a weighted sum of such terms:</p>
<div class="math notranslate nohighlight">
\[L = w_{pose}L_{pose} + w_{pos}L_{pos} + w_{vel}L_{vel} + w_{cons}L_{consistency} + \ldots\]</div>
<p>The weights are tuned to balance units and importance (for instance, degrees vs meters, etc.).</p>
<p>DIP’s probabilistic approach can be seen as having the network output an uncertainty per joint angle <span class="math notranslate nohighlight">\(\sigma_j^2\)</span>, and then the loss is:</p>
<div class="math notranslate nohighlight">
\[L = \frac{1}{T}\sum_{t,j} \frac{|\hat{\theta}_j^{(t)} - \theta_j^{*(t)}|^2}{\sigma_j^2(t)} + \log \sigma_j^2(t)\]</div>
<p>which implicitly adjusts weights <span class="math notranslate nohighlight">\(w\)</span> during training. This is an advanced strategy to handle multi-modal outputs.</p>
</section>
<section id="temporal-modeling-approaches">
<h3>Temporal Modeling Approaches<a class="headerlink" href="#temporal-modeling-approaches" title="Link to this heading"></a></h3>
<p>The mapping from a sequence of IMU readings to a sequence of poses is highly temporal in nature. Three main approaches have been used to model time in learning-based solutions:</p>
<p><strong>Recurrent Neural Networks (RNNs):</strong> This includes LSTM and GRU based models. An RNN processes one frame at a time, carrying forward a hidden state <span class="math notranslate nohighlight">\(\mathbf{h}^{(t)}\)</span> that encodes past information.</p>
<p>A typical setup for IMU pose: at each time <span class="math notranslate nohighlight">\(t\)</span>, feed <span class="math notranslate nohighlight">\(\mathbf{x}^{(t)}\)</span> (IMU features) and the previous hidden state <span class="math notranslate nohighlight">\(\mathbf{h}^{(t-1)}\)</span> into an LSTM, output a new hidden state and the pose estimate <span class="math notranslate nohighlight">\(\hat{\mathbf{y}}^{(t)}\)</span>. RNNs naturally handle arbitrary sequence lengths and can be run online.</p>
<p>DIP used a bi-directional RNN during training, which means it had one RNN reading forward (1→T) and one backward (T→1), and combined their hidden states for the output. This gives the network future context (improving accuracy) at the cost of only working offline.</p>
<p>DIP’s solution was to train with bi-RNN (for maximum learning of temporal patterns), but deploy with a forward-only RNN for real-time. TransPose used a multi-layer RNN in its translation module (to fuse foot heuristic with learned motion). RNNs are good with continuous streaming data and have relatively low computational cost per frame.</p>
<p><strong>Sliding windows / temporal convolution:</strong> Another approach is to consider a fixed-size window of <span class="math notranslate nohighlight">\(W\)</span> frames around time <span class="math notranslate nohighlight">\(t\)</span> and use a feed-forward network (or 1-D convolution) to map that window to the pose at the center.</p>
<p>For example, one could take frames <span class="math notranslate nohighlight">\(t-K\)</span> to <span class="math notranslate nohighlight">\(t+K\)</span> as input (concatenated or as a sequence into a small CNN) and output <span class="math notranslate nohighlight">\(\hat{\mathbf{y}}^{(t)}\)</span>. This was not explicitly used in DIP (they opted for RNN), but some works (e.g., a baseline DIP tried and found less effective) might have attempted a fully connected network on a window.</p>
<p>Temporal convolutional networks (TCN) could also be applied: convolving on the time axis to produce a smoothed, latency-controlled output. The advantage of window methods is parallel processing (process all frames in a sequence in parallel if using convolution), and the ability to incorporate some future context (depending on window). The downside is fixed latency (you need future frames for center). DIP’s sliding window deployment with an overlap is a variant of this: they might run the bi-RNN on a window of, say, 1 second, and output the first half of that window, then slide.</p>
<p><strong>Transformers:</strong> As exemplified by TIP, Transformers use self-attention to model long-range dependencies in a sequence of sensor data. A Transformer’s encoder could take the entire sequence of IMU readings (or a large chunk) and globally attend to any time step when predicting the pose at a certain time.</p>
<p>TIP specifically used a Transformer decoder framework where the model iteratively produces poses auto-regressively, attending to past outputs and inputs. This allows flexible context length – potentially the model can consider very long histories (hundreds of frames) if needed, which might capture slow drift trends or repetitive cycles.</p>
<p>Transformers often require more data to train (due to their many parameters and lack of built-in inductive bias for smooth temporal progression like RNNs have) and careful positional encoding (to know the order of frames). TIP overcame data scarcity by training on both synthetic and real sequences and incorporating the history via the decoder mechanism.</p>
<p>The benefit of Transformers is the rich modeling of complex time relationships (they might learn, for example, that a certain IMU pattern 10 seconds ago combined with the current pattern implies something now – an RNN with limited memory might forget). The cost is computational: self-attention is <span class="math notranslate nohighlight">\(O(T^2)\)</span> for sequence length <span class="math notranslate nohighlight">\(T\)</span>, which can be an issue for long sequences or high-frequency data unless windowed attention is used. TIP managed real-time, suggesting they either constrained <span class="math notranslate nohighlight">\(T\)</span> or optimized the model well.</p>
<p>In summary, RNNs (especially bi-directional or stacked) have been the workhorse for temporal modeling in early works (DIP, TransPose), whereas Transformers are emerging for their strength in capturing long-range constraints (TIP). Some methods might also combine approaches; e.g., TransPose uses an RNN for translation estimation but a feed-forward for pose after a hierarchical pass.</p>
</section>
<section id="supervised-vs-semi-supervised-training-synthetic-data">
<h3>Supervised vs. Semi-Supervised Training; Synthetic Data<a class="headerlink" href="#supervised-vs-semi-supervised-training-synthetic-data" title="Link to this heading"></a></h3>
<p>Training a learning-based model for this task can be fully supervised if we have ground-truth pose sequences for our IMU inputs. As discussed, obtaining ground-truth usually means using an expensive system (like an optical mocap with markers or a dense IMU suit or both) to record a few subjects, or using simulated data.</p>
<p>DIP and successors have leaned heavily on synthetic training data generated from mocap databases like AMASS. The process involves taking poses from these databases and computing what a set of IMUs would measure if the person was doing that pose sequence. Since the pose sequence is known (from mocap), this automatically provides the supervision: the algorithm is trained to map the synthetic IMU readings back to the original known pose.</p>
<p>This approach vastly increases training sample size and motion diversity – e.g., AMASS aggregates 18 motion datasets, over 40 hours of motion, including complex actions. The challenge is domain mismatch: simulated IMU readings may differ from real IMUs. Simulated data assumes a perfect sensor (no noise, no bias drift, exact alignment to body). Real IMUs have sensor noise, calibration offsets, magnetic disturbances, etc.</p>
<p>To bridge this gap, methods do a few things:
1. Add realistic noise to synthetic IMU signals during training (e.g., Gaussian noise on orientations, drift perturbations) to teach the network to be robust
2. Use domain adaptation or fine-tuning on a smaller set of real data</p>
<p>DIP, for example, fine-tuned their model on the DIP-IMU real dataset for a few epochs to adapt to real sensor characteristics. This significantly improved accuracy on real test motions compared to using the pure synthetic-trained model.</p>
<p>Semi-supervised or self-supervised training becomes useful if we have a lot of real IMU sequences without ground-truth poses. One can incorporate those by using losses that do not require ground truth pose. For instance, one can train a model with a mix of supervised loss (on synthetic or the few labeled real) and unsupervised consistency loss on unlabeled real.</p>
<p>A common unsupervised loss is the reconstruction loss of sensor signals: we mentioned above, ensure the predicted pose re-generates the IMU readings. If a network is good, feeding its output pose back into the forward model should produce the same orientation/accel that was input. By enforcing this on real data, the network learns from real sensor patterns even without knowing the exact pose – essentially it’s learning to solve the inverse problem by trying to be a consistent inverse of the forward physics.</p>
<p>Physics-based constraints can also help self-supervise: for example, an unlabeled IMU sequence might have obvious periods of rest – the network could be trained to recognize and enforce zero-velocity on predicted motion during those periods (since if accelerometers read ~9.81 m/s² with little variation, likely the person is static; the network should output a static pose).</p>
<p>Some recent research explores Physics-Informed Neural Networks (PINNs) for IMU pose, where the loss includes physical equations (like equations of motion) on unlabeled sequences.</p>
<p>Another approach is synthetic fine-tuning: using simulated but in-the-loop refinement. For instance, one could train a network on synthetic data, then deploy it on some real data and use the agreement between forward-simulated sensors and actual sensors as a cue to update the model (a form of self-calibration).</p>
<p>In practice, DIP, TransPose, TIP all use predominantly supervised learning with synthetic data plus limited real data fine-tune. PIP’s first stage (kinematics network) was trained on synthetic data (likely similar to DIP).</p>
<p>What made these works succeed is the realization that AMASS + simulated IMUs provides effectively infinite training data of diverse actions. The AMASS dataset (released 2019) was a boon: DIP (2018) had to collect smaller mocap sets themselves; later works could directly draw from AMASS’s 343 motion subjects and huge action variety.</p>
<p>Of course, careful alignment of coordinate frames between simulation and device is needed (e.g., aligning the virtual world frame’s gravity to match IMU convention that z-axis points opposite gravity).</p>
<p>In summary, supervised learning on synthetic data is the current standard approach, with fine-tuning on real data to handle domain shift. Semi-supervised ideas hold promise to further leverage unannotated real IMU data by imposing that the model’s outputs obey physical measurement laws and consistency. This could reduce reliance on expensive motion capture labeling in the future.</p>
</section>
</section>
<section id="model-architectures-and-design-considerations">
<h2>Model Architectures and Design Considerations<a class="headerlink" href="#model-architectures-and-design-considerations" title="Link to this heading"></a></h2>
<section id="encoding-imu-measurements">
<h3>Encoding IMU Measurements<a class="headerlink" href="#encoding-imu-measurements" title="Link to this heading"></a></h3>
<p>The first step in the model is to convert raw IMU streams into a suitable ML input. Each IMU’s orientation can be represented in several ways:</p>
<p><strong>Quaternions (4D):</strong> A straightforward encoding of orientation is a quaternion <span class="math notranslate nohighlight">\((w, x, y, z)\)</span> representing the rotation from some reference frame to the sensor frame. Quaternions need to be normalized (unit length). Networks can handle this, but sometimes learning to maintain normalization is tricky. Often quaternions are fed as is, and one might enforce normalization either by an explicit layer or by normalizing the network’s output if it ever predicts orientations.</p>
<p><strong>Rotation matrices (9D):</strong> We can also feed the 3×3 rotation matrix elements (9 numbers). This is redundant (only 3 DoF are true degrees of freedom) and also has orthonormal constraints. But in practice some have used 9D with a normalization step.</p>
<p><strong>6D continuous representation:</strong> Zhou et al. (2019) proposed representing rotation by two 3D vectors, corresponding to the first two columns of the rotation matrix, which are then normalized and the third is their cross product. This 6D representation has no singularities and networks can output any 6 numbers which will map to a valid rotation. It’s common in vision tasks and could be applied here too. Some recent IMU works likely use it internally for output angles; for input, since IMU orientation is known exactly, one might not bother converting it – quaternion is fine.</p>
<p><strong>Euler angles (3D):</strong> If one picks a consistent e.g., ZXY Euler angle convention, they could use 3 numbers from the IMU orientation (pitch, roll, yaw). However, Euler angles have discontinuities (gimbal lock, angle wrapping). It’s generally avoided as a direct learning input to a neural net in favor of quaternions or 6D.</p>
<p>For accelerations, typically the linear acceleration vector (with gravity subtracted or not) in the global frame is used. One subtlety: IMUs often give acceleration in the sensor’s local frame. To express it in a fixed world frame (like SMPL’s global coordinates), one multiplies by the orientation.</p>
<p>Many methods transform accelerations to a body-centric frame instead for learning. For example, DIP normalized the inputs by rotating all sensor orientations and accelerations into the root IMU’s frame or a reference frame tied to the body. This removes global orientation as a factor – the network doesn’t have to learn invariance to the person facing north vs east; it only cares about pose relative to whatever direction the root is facing.</p>
<p>In practice, one can take the pelvis IMU orientation <span class="math notranslate nohighlight">\(R_{\text{pelvis}}\)</span> and use it to invert-transform all other orientations: <span class="math notranslate nohighlight">\(\tilde{R}_i = R_{\text{pelvis}}^{-1} R_i\)</span>, so that pelvis becomes identity orientation. Similarly transform accelerations to pelvis frame. This way the network mostly sees relative orientations of limbs to pelvis (which correlates directly to joint angles) and the pelvis orientation itself (which indicates global heading).</p>
<p>DIP mentions careful treatment of coordinate systems as a crucial step. Failing to do this can cause the network to struggle – e.g., it would have to learn that an IMU orientation of (0.7, 0, 0.7, 0) for the pelvis might correspond to the same pose as (1,0,0,0) just rotated in yaw.</p>
<p>Another encoding is to break the acceleration into gravity + linear. An IMU actually measures <span class="math notranslate nohighlight">\(acc_{\text{measured}} = -g \hat{z}_{\text{world to IMU}} + acc_{\text{linear}}\)</span>. Some networks feed the acceleration as two parts: the gravity vector (which is basically the orientation’s down-axis) and the short-term linear acceleration (due to motion). But usually the orientation already contains gravity direction information, so it may be redundant.</p>
<p>Additionally, one can include angular velocity (gyroscope) readings if available. In principle, IMUs give orientation (from an internal filter) and also raw gyro which indicates rotation speed. Most methods focus on orientation+acc only, since orientation is essentially integrated gyro and is less noisy. But including angular velocity might add high-frequency motion clues.</p>
<p>Few papers explicitly mention using gyro or magnetometer; DIP did not use magnetometer (which provides compass heading) explicitly, though the orientation likely came from a fusion that uses it.</p>
<p>Finally, each time step’s multi-IMU data is often flattened and concatenated to feed into an RNN or transformer. For an RNN, one can also feed them in a structured way (like processing each IMU with a sub-network then merging).</p>
<p>DIP’s network design included an initial per-sensor fully-connected layer to embed each sensor’s data to a feature vector, then concatenating those. This is analogous to treating each IMU as a “channel.” Some architectures could also use convolution across sensors (if sensors were ordered in a kinematic order). However, typically a simple concatenation suffices since the number of sensors is small and fixed.</p>
</section>
<section id="network-structures">
<h3>Network Structures<a class="headerlink" href="#network-structures" title="Link to this heading"></a></h3>
<p><strong>Feed-forward vs. recurrent vs. attention:</strong> We discussed temporal modeling; here we consider the overall network topology. DIP’s architecture was a stacked bi-directional GRU network. It had multiple layers of GRU cells – the output of one GRU goes into the next GRU (this allows higher-level features to be extracted as you go up layers).</p>
<p>The bi-directional nature means one GRU layer processes from frame1→frameT, another processes from frameT→frame1, and their outputs are concatenated. On top of the last GRU, a final fully-connected layer maps to the pose output.</p>
<p>The DIP network was not extremely deep (maybe 2 layers of GRU with a few hundred hidden units each), but it was enough to capture the temporal patterns.</p>
<p>TransPose’s pose network was multi-stage but each stage might be smaller (perhaps a couple of dense layers). Its translation network was an RNN (probably an LSTM) that integrated velocity. TIP’s network is a Transformer: presumably an encoder that takes the IMU sequence and a decoder that outputs pose sequence auto-regressively. That means the architecture had the typical transformer blocks (multi-head self-attention, feed-forward sublayers). The query of the decoder at each time could be the previous pose plus positional info, and the decoder cross-attends to the IMU encoder output. This is a bit complex architecture but conceptually it replaces the recurrence with self-attention.</p>
<p>A common pattern is the use of dense intermediate layers to expand or reduce feature size. For example, DIP likely had a dense embedding of each time step’s sensor data to, say, 128D, then the GRU’s hidden state maybe 256D, etc., and a final output layer. These fully connected layers can be seen as the network learning an appropriate representation of the sensor signals (for instance, computing relative orientations or angles).</p>
<p>One interesting component in TransPose is the leaf joint prediction. This was implemented as an intermediate supervised output of joint positions. We can interpret it as a form of hierarchical decoding: the network might first output 3D positions of hands, feet, head. Then in a next layer (or next stage of the network), it uses those and the IMU data to predict the next set (like elbows, knees, etc.), ending with the root.</p>
<p>This hierarchical approach ensures that end-effectors (which are directly influenced by IMU on that segment) are correct first, then internal joints (which are more ambiguous from IMUs alone) can be inferred given where the limbs ended up. Implementing this requires either a multi-output architecture or sequential modules.</p>
<p>The TransPose authors describe it as a “multi-stage network”, which implies they had perhaps one sub-network focusing on leaves, then fed its results into another sub-network for the rest. They likely supervised the intermediate predictions with the ground-truth joint positions (a kind of deep supervision to guide each stage).</p>
<p>Most networks output the pose for each frame independently given the context. However, one can include feedback loops: TIP feeds back previous outputs explicitly in the decoder, and even DIP’s RNN effectively feeds back its hidden state which contains info about previous outputs.</p>
<p><strong>Output decoding:</strong> Finally, how does the network output map to a valid pose? If the network outputs <span class="math notranslate nohighlight">\(3K\)</span> numbers as joint rotations in axis-angle, that is taken as is (the network must learn to output reasonable values, typically it will because it’s trained to minimize rotation error).</p>
<p>Sometimes a network might output a rotation in a form that isn’t guaranteed valid (say it outputs 3 numbers intended as axis-angle but those can represent an angle beyond <span class="math notranslate nohighlight">\(\pi\)</span> which might be interpreted differently). Usually this is handled by the loss rather than by constraints on output. If a network outputs quaternions for joints, one would normalize them. If it outputs a rotation matrix (like some frameworks do via 9D rep), one would use a differentiable orthonormalization.</p>
<p>TransPose’s approach of outputting joint positions means they needed an extra step to retrieve joint angles. Possibly they solved an inverse kinematics (IK) problem: given predicted positions of hands, feet, etc., find joint angles that place those correctly. They might do this via a least-squares solve or incorporate a differentiable IK layer. But since they said it’s multi-stage network (which is within the learning model), they might have implicitly learned to produce rotations that match those positions by design. It’s a bit unclear, but likely they had differentiable kinematics in the loop.</p>
<p><strong>Global translation output:</strong> If predicting root translation, one simple method is to predict the root’s velocity at each frame (in the horizontal plane and vertical). TransPose did a combination of foot heuristic and RNN for this. TIP predicted stationary points which indirectly gives translation once you align those stationary points over time.</p>
<p>A network could also output the root position directly, but it might be better to predict velocity to avoid unbounded error (since velocity can be integrated and networks are good at local increments). In training, however, having ground-truth positions allows direct supervision.</p>
<p><strong>Network size considerations:</strong> DIP’s network was small enough to run on a CPU in real-time (though GPU was used for training). TIP’s transformer was likely larger, but they may have optimized sequence length (real-time demo indicates it was feasible). PIP’s network purposely was kept small to keep the 16ms latency – because after the network, they still do a physics solve.</p>
<p>PIP’s network might be a single LSTM layer or even a feed-forward that looks at a short window of IMU data to output the current pose (since the physics will correct any long-term issues). Indeed, PIP’s paper mentions the kinematics module regresses “motion state” – possibly meaning pose plus velocities – which a simple network can do in one shot.</p>
<p>In general, designing the network architecture involves deciding: how much temporal history to use, how to encode the input features (taking care of coordinate frames), whether to incorporate any physics priors inside (some have tried to hardcode gravity subtraction or known limb lengths, etc. in the network itself), and how to ensure outputs are valid (often by choosing appropriate representation or by adding losses that keep them in check).</p>
</section>
</section>
<section id="training-pipeline-and-pseudocode">
<h2>Training Pipeline and Pseudocode<a class="headerlink" href="#training-pipeline-and-pseudocode" title="Link to this heading"></a></h2>
<p>Bringing all the pieces together, we outline a basic pipeline for training a learning-based IMU-to-pose model:</p>
<p><strong>Data Preparation:</strong></p>
<ol class="arabic simple">
<li><p>Collect or generate a set of training sequences. For example, use AMASS to get many pose sequences <span class="math notranslate nohighlight">\({\boldsymbol{\theta}}_{1:T}\)</span>.</p></li>
<li><p>For each sequence, simulate IMU readings. For each time frame, compute sensor orientations and accelerations from the pose. Add noise if desired. Store pairs of <span class="math notranslate nohighlight">\((\mathbf{X}_{1:T}, \mathbf{Y}_{1:T})\)</span> where <span class="math notranslate nohighlight">\(\mathbf{Y}_{1:T}\)</span> are the ground-truth SMPL poses (and optionally root translations).</p></li>
<li><p>Split into training, validation sets. Also prepare any real sequences for fine-tuning or testing (e.g., DIP-IMU, TotalCapture sequences with ground truth).</p></li>
</ol>
<p><strong>Model Initialization:</strong></p>
<ol class="arabic simple">
<li><p>Design the neural network model <span class="math notranslate nohighlight">\(f_\Theta\)</span>. Choose RNN/Transformer etc., set input size = <span class="math notranslate nohighlight">\(M\)</span> sensors * (orientation+accel dim), output size = pose (72) + maybe root vel/pos (3).</p></li>
<li><p>Initialize weights <span class="math notranslate nohighlight">\(\Theta\)</span> (random or Xavier initialization, etc.). If using a two-stage (like PIP), initialize both the kinematics network and prepare the physics module parameters (which might not have trainable weights if it’s analytic).</p></li>
</ol>
<p><strong>Training Loop (Supervised):</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">each</span> <span class="n">batch</span> <span class="n">of</span> <span class="n">sequences</span> <span class="ow">in</span> <span class="n">training_data</span><span class="p">:</span>
        <span class="c1"># Prepare batch data</span>
        <span class="n">X_batch</span><span class="p">,</span> <span class="n">Y_batch</span> <span class="o">=</span> <span class="n">batch_sequences</span><span class="p">()</span>  <span class="c1"># shape [batch, T, features] and [batch, T, output_dim]</span>

        <span class="c1"># Forward pass:</span>
        <span class="n">Y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_batch</span><span class="p">)</span>  <span class="c1"># run the network on each sequence</span>

        <span class="c1"># Compute loss:</span>
        <span class="n">loss_pose</span> <span class="o">=</span> <span class="n">MSE</span><span class="p">(</span><span class="n">Y_pred</span><span class="p">[</span><span class="s1">&#39;pose&#39;</span><span class="p">],</span> <span class="n">Y_batch</span><span class="p">[</span><span class="s1">&#39;pose&#39;</span><span class="p">])</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_pose</span>
        <span class="k">if</span> <span class="n">predict_root</span><span class="p">:</span>
            <span class="n">loss_trans</span> <span class="o">=</span> <span class="n">MSE</span><span class="p">(</span><span class="n">Y_pred</span><span class="p">[</span><span class="s1">&#39;root_pos&#39;</span><span class="p">],</span> <span class="n">Y_batch</span><span class="p">[</span><span class="s1">&#39;root_pos&#39;</span><span class="p">])</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">loss_trans</span>
        <span class="c1"># (Add other losses if defined, e.g., velocity, contact)</span>

        <span class="c1"># Backpropagation:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="c1"># (Optional) Validate on val set, adjust learning rate, etc.</span>
</pre></div>
</div>
<p>This pseudocode depicts a simple supervised training. In practice, sequence batches can be handled by unrolling RNNs or by padding sequences to a common length. The loss here is just MSE (mean squared error) as a placeholder for potentially the sum of all relevant terms (orientation differences, etc.). alpha is a weight for translation loss.</p>
<p><strong>Fine-tuning / Domain Adaptation (if applicable):</strong> After initial training on synthetic data, one can fine-tune on a smaller real dataset:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">few_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">each</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">real_data</span><span class="p">:</span>
        <span class="n">X_real</span><span class="p">,</span> <span class="n">Y_real</span> <span class="o">=</span> <span class="n">batch_real</span><span class="p">()</span>
        <span class="n">Y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_real</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">MSE</span><span class="p">(</span><span class="n">Y_pred</span><span class="p">,</span> <span class="n">Y_real</span><span class="p">)</span>  <span class="c1"># real ground truth from DIP-IMU or TotalCapture</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>This step uses a smaller learning rate to not forget the general learned motion prior, but adapt to real sensor noise and biases.</p>
<p><strong>Inference (Online Deployment):</strong> Once trained, the model can be used on new IMU data in real time:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize hidden state for RNN if needed</span>
<span class="n">h</span> <span class="o">=</span> <span class="kc">None</span>
<span class="k">for</span> <span class="n">each</span> <span class="n">new</span> <span class="n">frame</span> <span class="n">t</span><span class="p">:</span>
    <span class="n">x_t</span> <span class="o">=</span> <span class="n">read_IMUs</span><span class="p">()</span>          <span class="c1"># get current IMU orientations &amp; accels</span>
    <span class="n">x_t</span> <span class="o">=</span> <span class="n">normalize_frame</span><span class="p">(</span><span class="n">x_t</span><span class="p">)</span> <span class="c1"># apply coordinate transforms, normalization</span>
    <span class="n">y_t</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>  <span class="c1"># RNN: provide previous hidden state</span>
    <span class="c1"># y_t contains pose (and possibly root translation)</span>
    <span class="n">visualize_pose</span><span class="p">(</span><span class="n">y_t</span><span class="p">)</span>        <span class="c1"># or send to application</span>
</pre></div>
</div>
<p>In a transformer model deployed auto-regressively, the prediction might be done one step at a time as well, feeding back the last few poses as context. In an RNN, the hidden state h carries the needed history compactly. The normalize_frame would, for example, rotate the input so that the pelvis IMU orientation is identity (using an initial calibration stance as reference perhaps).</p>
<p><strong>(Optional) Physics Correction at Inference:</strong> If using a hybrid like PIP, after obtaining a window of poses (say last <span class="math notranslate nohighlight">\(N\)</span> frames from the network), one would run a physics solver:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pose_sequence</span> <span class="o">=</span> <span class="n">get_last_N_predicted_poses</span><span class="p">()</span>
<span class="n">pose_sequence_corrected</span> <span class="o">=</span> <span class="n">physics_optimize</span><span class="p">(</span><span class="n">pose_sequence</span><span class="p">,</span> <span class="n">IMU_measurements_last_N</span><span class="p">)</span>
<span class="n">output</span><span class="p">(</span><span class="n">pose_sequence_corrected</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<p>This optimization typically adjusts the sequence to better match the recent measurements and physical laws, and returns the corrected current pose.</p>
<p>During training, monitoring metrics like orientation error (in degrees) per joint or position error (in mm) for key joints can guide development. A common metric in literature is the Mean Per Joint Position Error (MPJPE) in millimeters, sometimes after aligning the root.</p>
<p>On DIP-IMU and TotalCapture, for instance, methods report MPJPE to compare accuracy. The pseudocode above glosses over details like batching variable-length sequences (often handled by packing in PyTorch LSTMs) and assumes fully supervised training. In a semi-supervised scenario, one might have an inner loop where for unlabeled data you only use reconstruction loss: e.g., train to minimize difference between input IMU orientation and orientation derived from predicted pose.</p>
</section>
<section id="datasets-benchmarks-and-resources">
<h2>Datasets, Benchmarks, and Resources<a class="headerlink" href="#datasets-benchmarks-and-resources" title="Link to this heading"></a></h2>
<p>To develop and evaluate IMU-to-pose methods, several datasets and benchmarks are used:</p>
<section id="dip-imu-dataset-2018">
<h3>DIP-IMU Dataset (2018)<a class="headerlink" href="#dip-imu-dataset-2018" title="Link to this heading"></a></h3>
<p>Introduced with Deep Inertial Poser, DIP-IMU is one of the first large-scale datasets for sparse IMU human motion capture. It contains 10 subjects (9 male, 1 female) each performing a variety of motions in 5 categories (walking, running, sports, etc.), recorded with a full Xsens suit of 17 IMUs at 60 Hz.</p>
<p>In total it has 64 sequences comprising about 330,000 time instants of data. Each frame has the 3D orientation (quaternion) of each IMU (from Xsens’s on-board EKF) and the raw accelerometer readings.</p>
<p>Ground-truth poses (3D joint angles) were obtained via a high-end optical motion capture system simultaneously recorded, and then post-processed to fit the SMPL model. DIP-IMU is publicly available for research – the project page provides a download link after registration.</p>
<p>This dataset has become a standard benchmark: SIP’s optimization was tested on it, and DIP used it for validation. It enables researchers to test their algorithms on common motions and compare error metrics (e.g., mean joint error in degrees or centimeters).</p>
<p>The DIP-IMU dataset’s size (over 300k frames) also made it suitable for training deep networks, and indeed DIP and subsequent methods (TransPose, etc.) train on a mix of DIP-IMU and synthetic data.</p>
<p>(URL: <a class="reference external" href="http://dip.is.tue.mpg.de">http://dip.is.tue.mpg.de</a>)</p>
</section>
<section id="totalcapture-2017">
<h3>TotalCapture (2017)<a class="headerlink" href="#totalcapture-2017" title="Link to this heading"></a></h3>
<p>The TotalCapture dataset, released by Trumble et al., is a multimodal motion capture dataset that includes synchronized video, IMU, and Vicon marker data for human motions. It features 5 subjects (4 male, 1 female) each performing four distinct sets of actions (ROM exercises, walking, acting, and freestyle) with each sequence repeated 3 times.</p>
<p>The dataset provides data at 60 Hz from 8 calibrated cameras and 120 Hz data from a set of 13 IMUs (the subjects likely wore a Motion Analysis or Xsens set covering most body segments). With nearly 1.9 million frames of synchronized data, TotalCapture was the first dataset to offer IMU data aligned with ground-truth 3D poses (from a Vicon optical system) on such a large scale.</p>
<p>Researchers have used TotalCapture to evaluate IMU-only pose estimation as well as fusion of video and IMUs. For instance, the SIP paper compared their 6-IMU method against a baseline on TotalCapture sequences. The DIP authors also utilized TotalCapture by fitting SMPL to the Vicon data to create reference poses for testing their network.</p>
<p>The dataset can be obtained by request from the University of Surrey’s website (it requires a signup due to its size and to agree to usage terms). TotalCapture provides diverse indoor motions and challenging freeform activities (like acting out scenarios) which test an algorithm’s generalization.</p>
<p>It is particularly useful for methods that combine visual and inertial data, but also for pure-inertial methods that can use the IMU streams and the “ground truth” SMPL pose fits provided by DIP authors for quantitative evaluation.</p>
<p>(URL: <a class="reference external" href="https://cvssp.org/data/totalcapture/">https://cvssp.org/data/totalcapture/</a>)</p>
</section>
<section id="amass-2019">
<h3>AMASS (2019)<a class="headerlink" href="#amass-2019" title="Link to this heading"></a></h3>
<p>The Archive of Motion Capture as Surface Shapes (AMASS) is a large collection of mocap datasets unified into a common format of human model parameters. AMASS gathers 15 different mocap datasets (recorded with marker-based systems) and fits the SMPL (and SMPL+H for hands) model to all of them using the MoSh++ algorithm.</p>
<p>The result is a dataset of 11,000+ motions, over 40 hours of data from more than 300 subjects, all represented consistently as sequences of SMPL pose and shape parameters.</p>
<p>AMASS does not contain IMU data per se, but it has been hugely beneficial for IMU research because one can simulate IMU measurements from the AMASS motions. For example, given a sequence of SMPL poses from AMASS, one can compute the orientations and accelerations of virtual IMUs placed on the SMPL body (this is exactly what DIP and others do to generate training data).</p>
<p>The TransPose repository’s preprocessing script demonstrates this: it takes AMASS sequences, assumes 6 IMU placement as in SIP, and computes “synthetic” IMU sensor data (orientation and acceleration) for each sequence.</p>
<p>This synthetic data can be used to train networks so that they don’t overfit to the specific motions of DIP-IMU or TotalCapture, and it covers a far broader range of movements (since AMASS includes data from CMU MoCap, Human3.6M, gait datasets, etc.).</p>
<p>AMASS is accessible for research; users must register on the AMASS website and can then download the parameter files for the various sub-datasets. By using AMASS, one can also compute pose priors – many optimization methods (including SIP) leverage the fact that AMASS provides a distribution of typical human poses and shapes.</p>
<p>In summary, while AMASS is not an IMU dataset in itself, it is an invaluable resource to generate data for algorithm development and to serve as a prior on human motion.</p>
<p>(URL: <a class="reference external" href="https://amass.is.tue.mpg.de">https://amass.is.tue.mpg.de</a>)</p>
</section>
<section id="other-datasets-and-resources">
<h3>Other Datasets and Resources<a class="headerlink" href="#other-datasets-and-resources" title="Link to this heading"></a></h3>
<p>A number of other datasets and tools are worth mentioning:</p>
<ul class="simple">
<li><p>The TNT15 Dataset referenced in the SIP paper refers to a motion capture dataset from Tübingen (likely containing various motions) that was used as a baseline; it may be available through MPI.</p></li>
<li><p>More recently, researchers have begun collecting IMU data in outdoor or clinical settings: for example, IMUPoser (CHI 2023) used smartphone IMUs to capture daily activities, and MM-fit dataset provides IMUs for workout motions.</p></li>
<li><p>The KIT Motion Dataset and MPI Limitations Dataset have also been used for evaluating how methods handle extreme or unusual motions.</p></li>
</ul>
<p>On the software side, many open-source implementations for IMU pose estimation exist:
- The original SIP code (in C++) was available as a research prototype, and there are re-implementations like the one by Yan et al. in the Fusion Pose project.
- The DIP authors released their PyTorch code and pretrained model on the DIP project page, allowing researchers to directly use the DIP network for comparison.
- The TransPose project (2021) provides code that includes not only a neural network but also an example of using a solver (Ceres) to refine global pose with IMUs, giving a practical example of combining learning with optimization.</p>
<p>When developing an optimization-based method, one can use these datasets to test: for instance, start by fitting a single frame’s pose to one frame of DIP-IMU orientation data (which should be similar to solving a PSO problem per frame), then extend to a window of frames to incorporate acceleration.</p>
<p>By leveraging these datasets, the community has established evaluation protocols – common metrics include MPJPE (Mean Per Joint Position Error) in millimeters between the estimated pose and ground-truth pose, as well as orientation errors in degrees for each joint.</p>
<p>For example, on DIP-IMU, DIP (the neural method) reports an average joint position error around 60~80mm, whereas SIP (optimization) achieves around 100mm on certain motions, illustrating the trade-off between real-time inference and global accuracy.</p>
<p>Datasets like TotalCapture allow comparisons against vision-based methods: e.g., combining IMUs with video can reduce errors compared to video-only, especially for occluded limbs. Overall, the availability of data and code has greatly accelerated research, enabling more advanced techniques like hybrid model-learning methods and domain adaptation for different sensor configurations.</p>
</section>
</section>
<section id="challenges-and-outlook">
<h2>Challenges and Outlook<a class="headerlink" href="#challenges-and-outlook" title="Link to this heading"></a></h2>
<p>Despite significant progress, several challenges remain in fitting SMPL to IMU data, and ongoing research is exploring solutions:</p>
<p><strong>Generalization to Unseen Motions:</strong> A model trained on certain activities might struggle with very different ones. For instance, a network trained mostly on locomotion (walking, running) might have difficulty with acrobatic moves or crawling, which involve unusual sensor readings.</p>
<p>Ensuring motion diversity in training data is key (hence using AMASS). The model also needs capacity to represent very different poses (from standing upright to upside-down poses). Future work could involve adaptive models or mixture-of-experts that handle different motion regimes, or continued pre-training on enormous motion datasets to be more universal.</p>
<p><strong>Subject Generalization and Shape:</strong> Most learning approaches assume a generic or average body shape. If a person is very tall or short or has different limb lengths, the same IMU orientation might yield different joint angles (because limb length differences alter the relation of sensor orientation to posture).</p>
<p>Optimization approaches inherently handle individual anthropometry (by calibrating the model’s shape <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>). For learning methods, one could feed body shape as an additional input to the network (if one could estimate it from, say, the distance between some sensors).</p>
<p>Alternatively, one can calibrate shape by a short sequence: e.g., have the person do a T-pose or some known motion, and optimize <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> so that the network’s output fits the sensor data of that known pose.</p>
<p>Research in joint shape-pose estimation from IMUs is nascent – some works use foot-to-hip distance observed during walking to infer leg length. Future systems might include a calibration phase where the network quickly estimates the user’s body shape or even fine-tunes to the user’s data (which crosses into personalizing the model).</p>
<p><strong>Drift and Cumulative Error:</strong> Without external references, global position estimates will always have some drift. TransPose’s foot locking and TIP’s stationary point approach greatly mitigate drift, but over very long periods (minutes of continuous movement), small errors can still accumulate.</p>
<p>One outlook is to integrate occasional zero-velocity updates or similar to reset drift – in pedestrian tracking, it’s known that if a person stops occasionally, one can reset velocity to zero. IMUs also typically have magnetometers for compass heading, which could be used to prevent heading drift (most methods didn’t explicitly use magnetometer, but it’s a piece of info that could stabilize global orientation over long times).</p>
<p>If environmental feedback can be obtained (e.g., recognizing the person returned to their start point), that could correct drift. In absence of any external signal, physics-based methods like PIP ensure physical consistency but cannot know global location drift if the entire motion floats in one direction slightly off – they would need some assumption (like flat ground and eventually you must come to rest).</p>
<p><strong>Real-Time and Low-Power Implementation:</strong> For truly wearable systems (say an app on a smartphone reading IMUs from wearable sensors), computational efficiency is crucial. Models like DIP and TransPose are light enough for a modern smartphone GPU. Transformers might be heavier; one can use quantization or simpler architecture for deployment.</p>
<p>Some research (as mentioned with Fast Inertial Poser) is looking at pruning models to run on microcontrollers in the IMU units themselves. Balancing model complexity with latency and battery life will be important for practical systems (e.g., VR suits, sports analytics devices).</p>
<p><strong>Robustness to Sensor Errors:</strong> IMUs can produce faulty data: magnetic distortion can throw off orientation, accelerometers saturate on impact, sensors may disconnect briefly. A robust pose estimator should handle missing data or outliers.</p>
<p>This can be addressed by filtering the input (e.g., smoothing the IMU signals, or using sensor fusion outputs that are already filtered). Neural nets could also learn to infer a missing sensor’s information from others (e.g., if one sensor fails, perhaps the network can still guess pose from the remaining).</p>
<p>Designing redundancy or using an odd number of sensors (say 7th sensor as a redundant one that the network uses only if needed) could be explored.</p>
<p><strong>Multi-person and Interaction:</strong> All methods discussed assume one person’s IMUs, independent of others. In scenarios with multiple people each wearing IMUs, there’s the potential to confuse signals if not properly labeled.</p>
<p>Also, physical interaction between people (like two people hugging or carrying each other) poses new challenges – the IMUs don’t directly tell if an external force/contact from another person is happening, which could lead to impossible poses if solved individually.</p>
<p>This suggests future directions in joint pose estimation from multiple subjects’ IMUs, possibly using a combined physics model (like simulating two bodies that can exert forces on each other and fitting that to IMUs).</p>
<p><strong>Combining Learning with Physics (Outlook):</strong> PIP has shown one way to combine them (serially). Another outlook is to integrate physics within the learning process. For example, one could have a differentiable physics engine (there are libraries for that) and train the neural network’s outputs to not just match pose ground truth but also to minimize physical violations.</p>
<p>This would inject physics knowledge into the network during training itself, potentially resulting in a model that at runtime naturally outputs physically plausible motions without needing a second stage.</p>
<p>One could also learn the physics parameters (like ground friction, or personalized mass distribution) in a parallel stream. We might see Physics-informed Neural Networks (PINNs) for human motion, where the loss function includes terms for Euler-Lagrange equations or momentum conservation.</p>
<p>Already, a self-supervised PINN was proposed for IMU pose that estimates dynamics without ground truth forces.</p>
<p><strong>Integration with Other Sensors:</strong> While pure IMU is appealing for its independence, in practice there’s growing interest in hybrid systems. For example, a system might use a smartwatch IMU plus occasional camera images from a phone to reduce drift, or use ultra-wideband (UWB) radio beacons for positional references while IMUs do the pose.</p>
<p>Such combinations could be handled by learning (e.g., an RNN that takes both IMU and UWB ranges to output global position more accurately). From an outlook perspective, IMU-based pose is likely to be a component in larger VR/AR systems where additional cues (like foot pressure insoles, etc.) are available. Each additional modality can be fused via learning.</p>
<p><strong>Better Losses and Uncertainty:</strong> The heteroscedastic uncertainty approach DIP used could be extended: models could output full probability distributions over ambiguous joint angles. For instance, imagine an IMU on the back cannot tell if the person’s arms are raised or down (if arms had no sensors) – a model might output two modes.</p>
<p>Currently, networks typically output a single best guess. In the future, probabilistic pose outputs (like a Gaussian Mixture Model over pose space) could be used, which a downstream application or optimizer might refine with additional clues.</p>
<p>This connects to the idea of Bayesian deep learning for motion capture, providing not just an estimate but a confidence (which could be crucial for, say, alerting if the system is unsure of the pose).</p>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading"></a></h2>
<p>Fitting the SMPL model to IMU data via learning-based methods has evolved rapidly from early feasibility (DIP) to sophisticated systems (TransPose, TIP, PIP) that address many limitations. Learning-based methods excel at real-time performance and leveraging motion data to fill in the gaps left by sparse sensors.</p>
<p>By integrating ideas from sequential modeling (RNNs, Transformers) and combining with physical reasoning (contact constraints, dynamics), current systems achieve impressive accuracy – often within a few centimeters error for key joints.</p>
<p>The remaining issues of drift and unusual motions are being tackled with creative hybrids and larger training sets.</p>
<p>The outlook is that wearable motion capture will become increasingly accurate and practical, with perhaps just a handful of IMUs needed to drive high-fidelity human avatars in real time.</p>
<p>The synergy of learning and physics is a promising avenue: we can expect future research to focus on end-to-end differentiable physics-informed networks that learn from both data and physical laws to achieve robust IMU-based human tracking in any environment.</p>
<p>The ultimate goal is a system that you can strap on a few sensors and forget about – it will just reliably translate your movements into a virtual body, whether you’re walking, dancing, or climbing a wall, all without cameras.</p>
<p>The progress reviewed in this chapter suggests that goal is on the horizon, making IMU-based pose estimation a thrilling area of ongoing research for students and experts alike.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html" class="btn btn-neutral float-left" title="Lecture 07.1: Fitting SMPL to IMU Data Using Optimization-Based Methods" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="extended_materials_neural_radiance_fields.html" class="btn btn-neutral float-right" title="Neural Radiance Fields: A Historical and Theoretical Overview" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>