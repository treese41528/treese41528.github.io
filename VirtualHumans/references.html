

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>References &mdash; Fitting SMPL to IMU Optimization</title>
      <link rel="stylesheet" type="text/css" href="/VirtualHumans/_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="/VirtualHumans/_static/css/theme.css?v=e59714d7" />

  
    <link rel="canonical" href="https://treese41528.github.io/VirtualHumans/references.html" />
      <script src="/VirtualHumans/_static/jquery.js?v=5d32c60e"></script>
      <script src="/VirtualHumans/_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="/VirtualHumans/_static/documentation_options.js?v=f2a433a1"></script>
      <script src="/VirtualHumans/_static/doctools.js?v=9bcbadda"></script>
      <script src="/VirtualHumans/_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="/VirtualHumans/_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="3D Gaussian Splatting: A Basic Introduction" href="extendeed_materials_gaussian_splatting.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Virtual Humans Lecture
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="lecture_01_1_historical_body_models.html">Lecture 01.1 – Historical Body Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#early-origins-simplified-primitives-and-kinematic-skeletons-1970s1980s">Early Origins: Simplified Primitives and Kinematic Skeletons (1970s–1980s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#advances-in-the-1990s-superquadrics-differentiable-fitting-and-physical-models">Advances in the 1990s: Superquadrics, Differentiable Fitting, and Physical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#the-impact-of-3d-scanning-and-data-from-anthropometry-to-statistical-models-1990s2000s">The Impact of 3D Scanning and Data: From Anthropometry to Statistical Models (1990s–2000s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#scape-and-the-emergence-of-pose-aware-models-mid-2000s">SCAPE and the Emergence of Pose-Aware Models (Mid-2000s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#consolidation-in-the-2010s-smpl-and-integration-with-learning-based-methods">Consolidation in the 2010s: SMPL and Integration with Learning-Based Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#deep-learning-and-neural-implicit-models-late-2010spresent">Deep Learning and Neural Implicit Models (Late 2010s–Present)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#timeline-summary-of-milestones">Timeline Summary of Milestones</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html">Lecture 01.2 – Introduction to Human Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#historical-context-of-human-body-modeling">1. Historical Context of Human Body Modeling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#early-developments">Early Developments</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#mid-20th-century-approaches">Mid-20th Century Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#data-driven-revolution-1990s-2000s">Data-Driven Revolution (1990s-2000s)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#mathematical-foundations-of-human-body-models">2. Mathematical Foundations of Human Body Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#the-smpl-model">The SMPL Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#pca-based-statistical-shape-modeling">PCA-Based Statistical Shape Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#kinematic-modeling">Kinematic Modeling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#applications-of-human-body-models">3. Applications of Human Body Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computer-animation-and-visual-effects">Computer Animation and Visual Effects</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#virtual-humans-and-avatars">Virtual Humans and Avatars</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#biomechanics-and-ergonomics">Biomechanics and Ergonomics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#human-computer-interaction-hci">Human-Computer Interaction (HCI)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computer-vision-and-ai">Computer Vision and AI</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#education-and-training">Education and Training</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#challenges-and-future-directions">4. Challenges and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computational-efficiency">Computational Efficiency</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#accuracy-and-detail">Accuracy and Detail</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#generalization">Generalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#clothing-and-accessories">Clothing and Accessories</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#emerging-approaches">Emerging Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html">Lecture 01.3 – Introduction to Human Models (Overview)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#historical-context">1. Historical Context</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#early-scientific-studies">Early Scientific Studies</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#mid-20th-century-to-digital-era">Mid-20th Century to Digital Era</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#st-century-advances">21st Century Advances</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#mathematical-foundations">2. Mathematical Foundations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#parametric-body-models">Parametric Body Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-surface-representations">Implicit Surface Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#kinematic-modeling">Kinematic Modeling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#image-formation-and-rendering">3. Image Formation and Rendering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#camera-models">Camera Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#shading-and-visibility">Shading and Visibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#differentiable-rendering">Differentiable Rendering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#surface-representation-methods">4. Surface Representation Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#explicit-mesh-models">Explicit Mesh Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-function-models">Implicit Function Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#motion-capture-and-behavior-synthesis">5. Motion Capture and Behavior Synthesis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#capturing-human-motion">Capturing Human Motion</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#behavior-synthesis">Behavior Synthesis</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#clothing-modeling">6. Clothing Modeling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physically-based-simulation">Physically-Based Simulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#data-driven-approaches">Data-Driven Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-clothing-models">Implicit Clothing Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#human-object-interaction">7. Human-Object Interaction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physics-based-methods">Physics-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#learning-based-approaches">Learning-Based Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#hybrid-systems">Hybrid Systems</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#applications">8. Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#entertainment-and-media">Entertainment and Media</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#healthcare-and-biomechanics">Healthcare and Biomechanics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#engineering-and-design">Engineering and Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#human-computer-interaction">Human-Computer Interaction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#scientific-research">Scientific Research</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#challenges-and-future-directions">9. Challenges and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#scalability-and-generalization">Scalability and Generalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#higher-fidelity-dynamics">Higher-Fidelity Dynamics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#data-and-labeling-constraints">Data and Labeling Constraints</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physics-and-learning-integration">Physics and Learning Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#semantic-and-cognitive-aspects">Semantic and Cognitive Aspects</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#realism-vs-controllability">Realism vs. Controllability</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_02_1_image_formation.html">Lecture 02.1 – Image Formation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#historical-developments-in-image-formation">1. Historical Developments in Image Formation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#ancient-and-medieval-optics-camera-obscura">Ancient and Medieval Optics – Camera Obscura</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#renaissance-perspective-and-geometry">Renaissance Perspective and Geometry</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#early-cameras-and-photographic-imaging">Early Cameras and Photographic Imaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#modern-developments">Modern Developments</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#the-pinhole-camera-model">2. The Pinhole Camera Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#coordinate-setup">Coordinate Setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#proof-by-similar-triangles">Proof by Similar Triangles</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#numerical-example">Numerical Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#inadequacy-of-a-simple-pinhole">Inadequacy of a Simple Pinhole</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#camera-intrinsics-and-the-projection-matrix">3. Camera Intrinsics and the Projection Matrix</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#extrinsic-parameters">Extrinsic Parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#full-projection-example">Full Projection Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#image-distortions-correction">4. Image Distortions &amp; Correction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#properties-of-perspective-projection">5. Properties of Perspective Projection</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#advanced-theoretical-extensions">6. Advanced Theoretical Extensions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#light-field-imaging-and-plenoptic-cameras">Light Field Imaging and Plenoptic Cameras</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#non-conventional-imaging-techniques">Non-Conventional Imaging Techniques</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#applications-in-modern-vision-and-graphics">7. Applications in Modern Vision and Graphics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#computer-vision-and-3d-reconstruction">Computer Vision and 3D Reconstruction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#medical-imaging">Medical Imaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#photorealistic-rendering-in-computer-graphics">Photorealistic Rendering in Computer Graphics</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#python-example-simulating-image-formation">8. Python Example: Simulating Image Formation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html">Lecture 02.2 – Rotations and Kinematic Chains</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#representations-of-3d-rotations">1. Representations of 3D Rotations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#a-rotation-matrices">A) Rotation Matrices</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#b-euler-angles">B) Euler Angles</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#c-quaternions">C) Quaternions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#lie-algebra-so-3-and-exponential-map">2. Lie Algebra <span class="math notranslate nohighlight">\(so(3)\)</span> and Exponential Map</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#rodrigues-rotation-formula">3. Rodrigues’ Rotation Formula</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#kinematic-chains-forward-inverse-kinematics">4. Kinematic Chains: Forward &amp; Inverse Kinematics</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#comparison-of-rotation-representations">Comparison of Rotation Representations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_03_1_surface_representations.html">Lecture 03.1 – Surface Representations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#mathematical-foundations-of-surface-representations">1. Mathematical Foundations of Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-parametric-surfaces">A) Parametric Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-implicit-surfaces">B) Implicit Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-explicit-surfaces">C) Explicit Surfaces</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#surface-differential-properties">2. Surface Differential Properties</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-surface-normals">A) Surface Normals</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-fundamental-forms-and-curvature">B) Fundamental Forms and Curvature</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-geodesics">C) Geodesics</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#discrete-surface-representations">3. Discrete Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-polygon-meshes">A) Polygon Meshes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-point-clouds">B) Point Clouds</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-signed-distance-fields-sdf">C) Signed Distance Fields (SDF)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#advanced-surface-representations">4. Advanced Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-bezier-curves-and-surfaces">A) Bézier Curves and Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-b-splines-and-nurbs">B) B-Splines and NURBS</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-subdivision-surfaces">C) Subdivision Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-level-sets">D) Level Sets</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#e-neural-implicit-representations">E) Neural Implicit Representations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#comparative-analysis-and-applications">5. Comparative Analysis and Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-computational-efficiency-and-storage">A) Computational Efficiency and Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-practical-applications">B) Practical Applications</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-operations-complexity">C) Operations Complexity</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#implementation-examples">6. Implementation Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-basic-mesh-processing-python">A) Basic Mesh Processing (Python)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-implicit-surface-utilities">B) Implicit Surface Utilities</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-bezier-curve-implementation">C) Bézier Curve Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-curvature-estimation-on-meshes">D) Curvature Estimation on Meshes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#advanced-topics-and-future-directions">7. Advanced Topics and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-multi-resolution-representations">A) Multi-Resolution Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-machine-learning-for-geometry">B) Machine Learning for Geometry</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-dynamic-surfaces">C) Dynamic Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-non-manifold-geometries">D) Non-Manifold Geometries</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html">Lecture 03.2 – Procrustes Alignment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#goal-learning-a-model-of-pose-and-shape">Goal: Learning a Model of Pose and Shape</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#the-challenge-of-registration">The Challenge of Registration</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#surface-representation-mesh">Surface Representation: Mesh</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#the-procrustes-alignment-problem-mathematical-formulation">The Procrustes Alignment Problem: Mathematical Formulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#rigid-transformations">Rigid Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#procrustes-alignment-solution">Procrustes Alignment Solution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#decoupling-translation-by-centroid-alignment">Decoupling Translation by Centroid Alignment</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#optimal-rotation-via-svd">Optimal Rotation via SVD</a><ul>
<li class="toctree-l4"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#reflection-adjustment">Reflection Adjustment</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#optimal-scale-optional">Optimal Scale (Optional)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#complete-mathematical-derivation">Complete Mathematical Derivation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#translation-derivation">Translation Derivation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#rotation-derivation">Rotation Derivation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#scale-derivation">Scale Derivation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#summary-of-procrustes-alignment-algorithm">Summary of Procrustes Alignment Algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#python-implementation-example">Python Implementation Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#practical-applications">Practical Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#interactive-visualization-ideas">Interactive Visualization Ideas</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_04_1_icp.html">Lecture 4.1: Iterative Closest Point</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#introduction-to-shape-alignment-and-registration">Introduction to Shape Alignment and Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#the-registration-problem">The Registration Problem</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#review-procrustes-analysis">Review: Procrustes Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#problem-unknown-correspondences">Problem: Unknown Correspondences</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#the-iterative-closest-point-icp-algorithm">The Iterative Closest Point (ICP) Algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#basic-icp-algorithm">Basic ICP Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#computational-considerations">Computational Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="lecture_04_1_icp.html#closest-point-computation">Closest Point Computation</a></li>
<li class="toctree-l4"><a class="reference internal" href="lecture_04_1_icp.html#convergence-and-local-minima">Convergence and Local Minima</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#point-to-point-vs-point-to-plane-icp">Point-to-Point vs. Point-to-Plane ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#point-to-point-icp">Point-to-Point ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#point-to-plane-icp">Point-to-Plane ICP</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#gradient-based-icp-for-non-rigid-registration">Gradient-based ICP for Non-Rigid Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#gradient-based-icp-algorithm">Gradient-based ICP Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#advantages-of-gradient-based-icp">Advantages of Gradient-based ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#computing-gradients">Computing Gradients</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#improving-icp-s-robustness">Improving ICP’s Robustness</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#data-association-direction">Data Association Direction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#robust-cost-functions">Robust Cost Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#trimmed-icp">Trimmed ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#ransac-based-approaches">RANSAC-based Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#additional-information">Additional Information</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#point-to-surface-distance">Point-to-Surface Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#icp-variants-and-extensions">ICP Variants and Extensions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#generalized-icp-gicp">Generalized ICP (GICP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#em-icp-and-probabilistic-approaches">EM-ICP and Probabilistic Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#coherent-point-drift-cpd">Coherent Point Drift (CPD)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#multi-scale-approaches">Multi-Scale Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#applications-of-icp">Applications of ICP</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#implementing-icp">Implementing ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#efficient-python-implementation">Efficient Python Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#practical-tips">Practical Tips</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_04_2_body_models.html">Lecture 04.2 - Body Models: Vertex-Based Models and SMPL</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#body-models-as-parameterized-functions">1. Body Models as Parameterized Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#rotations-articulation-and-pose-representation">2. Rotations, Articulation, and Pose Representation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#rotation-representation">2.1 Rotation Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#kinematic-chain">2.2 Kinematic Chain</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#linear-blend-skinning-and-its-limitations">3. Linear Blend Skinning and its Limitations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#linear-blend-skinning">3.1 Linear Blend Skinning</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#problems-with-standard-lbs">3.2 Problems with Standard LBS</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#blend-shapes-for-correcting-lbs-artifacts">3.3 Blend Shapes for Correcting LBS Artifacts</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#the-smpl-body-model">4. The SMPL Body Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#smpl-philosophy">4.1 SMPL Philosophy</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#smpl-model-architecture">4.2 SMPL Model Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#shape-blend-shapes">4.2.1 Shape Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#pose-blend-shapes">4.2.2 Pose Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#joint-regression">4.2.3 Joint Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#model-training">4.3 Model Training</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#comparison-with-scape">5. Comparison with SCAPE</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#the-scape-model">5.1 The SCAPE Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#different-approaches-to-deformation">5.2 Different Approaches to Deformation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#performance-comparison">5.3 Performance Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#other-advantages">5.4 Other Advantages</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#alignment-techniques-procrustes-analysis-and-icp">6. Alignment Techniques: Procrustes Analysis and ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#procrustes-analysis">6.1 Procrustes Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#iterative-closest-point-icp">6.2 Iterative Closest Point (ICP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#fitting-smpl-to-scans">6.3 Fitting SMPL to Scans</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#image-formation-and-the-pinhole-camera-model">7. Image Formation and the Pinhole Camera Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#the-pinhole-camera-model">7.1 The Pinhole Camera Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#lens-distortion">7.2 Lens Distortion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#extensions-and-advanced-applications">8. Extensions and Advanced Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#dynamic-soft-tissue-modeling">8.1 Dynamic Soft Tissue Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#specialized-extensions">8.2 Specialized Extensions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#deep-learning-for-model-fitting">8.3 Deep Learning for Model Fitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#probabilistic-approaches">8.4 Probabilistic Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#hybrid-models">8.5 Hybrid Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_05_1_body_model_training.html">Lecture 5.1 - Training a Body Model and Fitting SMPL to Scans</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#body-models-based-on-triangle-deformations">Body Models Based on Triangle Deformations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#scape-and-blendscape-models">SCAPE and BlendSCAPE Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#triangle-deformation-process">Triangle Deformation Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#comparison-smpl-vs-scape-blendscape">Comparison: SMPL vs. SCAPE/BlendSCAPE</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#training-a-body-model-from-registrations">Training a Body Model from Registrations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#the-challenge-of-raw-scan-data">The Challenge of Raw Scan Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#training-from-registrations">Training from Registrations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#obtaining-registrations-fitting-smpl-to-scans">Obtaining Registrations: Fitting SMPL to Scans</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#non-rigid-registration-process">Non-Rigid Registration Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#iterative-closest-point-icp-review">Iterative Closest Point (ICP) Review</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#registration-objective-formulation">Registration Objective Formulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#point-to-surface-distance">Point-to-Surface Distance</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#multi-stage-optimization-strategy">Multi-Stage Optimization Strategy</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#joint-registration-and-model-training">Joint Registration and Model Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#co-registration-approach">Co-Registration Approach</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_05_2_3d_registration.html">Lecture 05.2 - 3D Registration: From Classical ICP to Modern Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#rigid-registration-and-the-icp-algorithm">1. Rigid Registration and the ICP Algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#the-iterative-closest-point-icp-algorithm">The Iterative Closest Point (ICP) Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#convergence-analysis-and-failure-modes">Convergence Analysis and Failure Modes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#classical-non-rigid-registration">2. Classical Non-Rigid Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#thin-plate-spline-robust-point-matching-tps-rpm">Thin Plate Spline Robust Point Matching (TPS-RPM)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#coherent-point-drift-cpd">Coherent Point Drift (CPD)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#other-non-rigid-methods">Other Non-Rigid Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#parametric-models-and-the-smpl-body-model">3. Parametric Models and the SMPL Body Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#model-structure">Model Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#shape-blend-shapes-identity-variation">Shape Blend Shapes (Identity Variation)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#pose-blend-shapes-pose-dependent-deformation">Pose Blend Shapes (Pose-Dependent Deformation)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#linear-blend-skinning-lbs-for-articulation">Linear Blend Skinning (LBS) for Articulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#learning-smpl">Learning SMPL</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#using-smpl-for-registration">Using SMPL for Registration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#modeling-clothing-and-fine-details-smpl-d">4. Modeling Clothing and Fine Details: SMPL+D</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#why-smpl-d">Why SMPL+D?</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#how-displacements-are-applied">How Displacements Are Applied</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#limitations">Limitations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#applications">Applications</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#survey-of-3d-registration-methods-from-icp-to-deep-learning">5. Survey of 3D Registration Methods: From ICP to Deep Learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#early-pioneering-works-1990s-foundational-rigid-registration">5.1 Early Pioneering Works (1990s) – Foundational Rigid Registration</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#the-2000s-robust-and-non-rigid-registration-emerges">5.2 The 2000s – Robust and Non-Rigid Registration Emerges</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#s-template-based-and-parametric-model-registration">5.3 2010s – Template-based and Parametric Model Registration</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#s-learning-based-parametric-registration-and-hybrid-approaches">5.4 2020s – Learning-Based Parametric Registration and Hybrid Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html">Lecture 06.1 - Fitting the SMPL Model to Images via Optimization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#mathematical-background-pinhole-camera-and-projections">Mathematical Background: Pinhole Camera and Projections</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#perspective-projection">Perspective Projection</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#weak-perspective-projection">Weak-Perspective Projection</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#camera-extrinsics-vs-model-pose">Camera Extrinsics vs. Model Pose</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#d-keypoints-and-projection">2D Keypoints and Projection</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#the-smpl-model-as-a-differentiable-function-of-shape-and-pose">The SMPL Model as a Differentiable Function of Shape and Pose</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#shape-blend-shapes">Shape Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#pose-blend-shapes">Pose Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#joint-positions">Joint Positions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#linear-blend-skinning">Linear Blend Skinning</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#differentiability-of-smpl">Differentiability of SMPL</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#fitting-smpl-to-images-via-optimization-smplify">Fitting SMPL to Images via Optimization (SMPLify)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#objective-function">Objective Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#combined-objective">Combined Objective</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#optimization-strategy">Optimization Strategy</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#optimization-algorithms">Optimization Algorithms</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#automatic-differentiation-and-jacobians">Automatic Differentiation and Jacobians</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#result-of-smplify">Result of SMPLify</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#historical-progression-and-method-comparisons">Historical Progression and Method Comparisons</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#smplify-bogo-et-al-2016">SMPLify (Bogo et al. 2016)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#smplify-x-2019">SMPLify-X (2019)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html">Lecture 06.2 - Learning-Based Fitting of the SMPL Model to Images</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#foundations-of-learning-based-smpl-estimation">Foundations of Learning-Based SMPL Estimation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#integrating-smpl-into-neural-networks">Integrating SMPL into Neural Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#projection-functions">Projection Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#loss-functions">Loss Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#statistical-priors-and-adversarial-losses">Statistical Priors and Adversarial Losses</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#early-regression-approaches-hmr-and-nbf">Early Regression Approaches: HMR and NBF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#human-mesh-recovery-hmr">Human Mesh Recovery (HMR)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#neural-body-fitting-nbf">Neural Body Fitting (NBF)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#evolving-architectures-hybrid-and-improved-regression-methods">Evolving Architectures: Hybrid and Improved Regression Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#spin-optimization-in-the-training-loop">SPIN: Optimization in the Training Loop</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#pymaf-pyramidal-mesh-alignment-feedback">PyMAF: Pyramidal Mesh Alignment Feedback</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#cliff-using-full-frame-context-for-camera-orientation">CLIFF: Using Full-Frame Context for Camera Orientation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#pixie-whole-body-regression-with-part-experts">PIXIE: Whole-Body Regression with Part Experts</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#temporal-methods-from-single-images-to-video-sequences">Temporal Methods: From Single Images to Video Sequences</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#vibe-adversarial-motion-prior-with-grus">VIBE: Adversarial Motion Prior with GRUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#tcmr-temporally-consistent-mesh-recovery">TCMR: Temporally Consistent Mesh Recovery</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#motionbert-transformer-based-motion-representations">MotionBERT: Transformer-Based Motion Representations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#comparison-of-learning-based-methods">Comparison of Learning-Based Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#supervision-and-data">Supervision and Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#network-architecture">Network Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#objective-functions">Objective Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#pose-and-shape-priors">Pose and Shape Priors</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#performance">Performance</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#runtime">Runtime</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#strengths-and-weaknesses">Strengths and Weaknesses</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html">Lecture 07.1: Fitting SMPL to IMU Data Using Optimization-Based Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#classical-imu-based-pose-estimation-a-historical-perspective">Classical IMU-Based Pose Estimation: A Historical Perspective</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#attitude-and-heading-reference-systems">Attitude and Heading Reference Systems</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#kalman-filter-approaches">Kalman Filter Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#early-sparse-sensor-approaches">Early Sparse-Sensor Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#model-based-optimization-methods">Model-Based Optimization Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#learning-based-methods">Learning-Based Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#inertial-sensor-fundamentals-and-orientation-representations">Inertial Sensor Fundamentals and Orientation Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#gravity-alignment-and-drift-correction">Gravity Alignment and Drift Correction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#orientation-representations">Orientation Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#sensor-calibration">Sensor Calibration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#optimization-based-smpl-fitting-with-imu-data">Optimization-Based SMPL Fitting with IMU Data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#kinematic-model-and-sensor-prediction">Kinematic Model and Sensor Prediction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#regularization-and-prior-terms">Regularization and Prior Terms</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#gradient-and-jacobian-computation">Gradient and Jacobian Computation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#pseudocode-smpl-pose-estimation-from-imu-sequence">Pseudocode: SMPL Pose Estimation from IMU Sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#imu-based-human-pose-datasets-and-resources">IMU-Based Human Pose Datasets and Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#dip-imu-dataset-2018">DIP-IMU Dataset (2018)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#totalcapture-2017">TotalCapture (2017)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#amass-2019">AMASS (2019)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#other-datasets-and-resources">Other Datasets and Resources</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html">Lecture 07.2: Fitting SMPL to IMU Data Using Learning-Based Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#optimization-based-vs-learning-based-approaches">Optimization-Based vs. Learning-Based Approaches</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#learning-based-imu-to-pose-estimation-historical-overview-of-key-models">Learning-Based IMU-to-Pose Estimation: Historical Overview of Key Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#deep-inertial-poser-dip-2018">Deep Inertial Poser (DIP, 2018)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#transpose-2021">TransPose (2021)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#transformer-inertial-poser-tip-2022">Transformer Inertial Poser (TIP, 2022)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#physics-physical-inertial-poser-pip-2022">Physics/Physical Inertial Poser (PIP, 2022)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#other-notable-models-and-developments">Other Notable Models and Developments</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#problem-formulation-and-learning-task-definition">Problem Formulation and Learning Task Definition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#input-and-output-representations">Input and Output Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#learning-objective-and-loss-functions">Learning Objective and Loss Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#temporal-modeling-approaches">Temporal Modeling Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#supervised-vs-semi-supervised-training-synthetic-data">Supervised vs. Semi-Supervised Training; Synthetic Data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#model-architectures-and-design-considerations">Model Architectures and Design Considerations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#encoding-imu-measurements">Encoding IMU Measurements</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#network-structures">Network Structures</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#training-pipeline-and-pseudocode">Training Pipeline and Pseudocode</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#datasets-benchmarks-and-resources">Datasets, Benchmarks, and Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#dip-imu-dataset-2018">DIP-IMU Dataset (2018)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#totalcapture-2017">TotalCapture (2017)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#amass-2019">AMASS (2019)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#other-datasets-and-resources">Other Datasets and Resources</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#challenges-and-outlook">Challenges and Outlook</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="extended_materials_neural_radiance_fields.html">Neural Radiance Fields: A Historical and Theoretical Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#foundations-3d-scene-representation-and-reconstruction-techniques">Foundations: 3D Scene Representation and Reconstruction Techniques</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#voxel-grids">Voxel Grids</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#point-clouds">Point Clouds</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#mesh-based-surfaces">Mesh-Based Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#light-fields-and-volumetric-rendering">Light Fields and Volumetric Rendering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#emergence-of-neural-radiance-fields-nerf">Emergence of Neural Radiance Fields (NeRF)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#core-idea">Core Idea</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#training-procedure">Training Procedure</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#nerf-architecture">NeRF Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#hierarchical-sampling">Hierarchical Sampling</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#original-results">Original Results</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#theoretical-and-mathematical-analysis-of-nerf">Theoretical and Mathematical Analysis of NeRF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#volume-rendering-formulation-in-nerf">Volume Rendering Formulation in NeRF</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#positional-encoding-and-neural-network-architecture">Positional Encoding and Neural Network Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#loss-function-and-optimization">Loss Function and Optimization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#major-advancements-and-extensions-of-nerf">Major Advancements and Extensions of NeRF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#anti-aliasing-and-unbounded-scenes-mip-nerf-and-nerf">Anti-Aliasing and Unbounded Scenes: mip-NeRF and NeRF++</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#efficiency-improvements-instant-nerf-and-plenoctrees">Efficiency Improvements: Instant NeRF and PlenOctrees</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#dynamic-and-deformable-nerfs-d-nerf-nerfies-nsff-etc">Dynamic and Deformable NeRFs (D-NeRF, Nerfies, NSFF, etc.)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#neural-radiance-fields-for-human-modeling-with-smpl-and-body-models">Neural Radiance Fields for Human Modeling (with SMPL and Body Models)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#other-notable-extensions">Other Notable Extensions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#comparison-with-other-3d-representations">Comparison with Other 3D Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-polygonal-meshes">Vs. Polygonal Meshes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-point-clouds-3d-splatting">Vs. Point Clouds / 3D Splatting</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-voxel-grids-and-volumetric-methods">Vs. Voxel Grids and Volumetric Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-multi-plane-images-mpis-light-fields">Vs. Multi-Plane Images (MPIs) / Light Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#accuracy-and-fidelity">Accuracy and Fidelity</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#applicability">Applicability</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#datasets-for-nerf-training-and-evaluation">Datasets for NeRF Training and Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#blender-synthetic-nerf-dataset">Blender Synthetic NeRF Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#local-light-field-fusion-llff-real-forward-facing-dataset">Local Light Field Fusion (LLFF) Real Forward-Facing Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#tanks-and-temples">Tanks and Temples</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#dtu-dataset">DTU Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#human3-6m">Human3.6M</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#zju-mocap-dataset">ZJU-MoCap Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#people-snapshot">People-Snapshot</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#synthetic-dynamic-scenes">Synthetic dynamic scenes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#other-datasets">Other datasets</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html">3D Gaussian Splatting: A Basic Introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#foundations">Foundations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#what-is-a-3d-scene">What is a 3D Scene?</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-scene-representation">3D Scene Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#computer-graphics-fundamentals">Computer Graphics Fundamentals</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#rasterization-and-ray-tracing">Rasterization and Ray Tracing</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#alpha-blending-and-compositing">Alpha Blending and Compositing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#the-evolution-of-novel-view-synthesis">The Evolution of Novel View Synthesis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#image-based-rendering">Image-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#structure-from-motion-and-multi-view-stereo">Structure-from-Motion and Multi-View Stereo</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#point-based-rendering">Point-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#neural-rendering">Neural Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#accelerated-neural-fields">Accelerated Neural Fields</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussian-splatting-a-convergence-of-approaches">3D Gaussian Splatting: A Convergence of Approaches</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#id1">Point-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#point-clouds-and-their-challenges">Point Clouds and Their Challenges</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#the-concept-of-splatting">The Concept of Splatting</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#elliptical-weighted-average-ewa-filtering">Elliptical Weighted Average (EWA) Filtering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-point-based-rendering">Differentiable Point-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussian-splatting-core-principles">3D Gaussian Splatting: Core Principles</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#key-insight-unifying-points-and-volumes">Key Insight: Unifying Points and Volumes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussians-as-scene-primitives">3D Gaussians as Scene Primitives</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#the-volumetric-rendering-equation">The Volumetric Rendering Equation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#alpha-compositing-with-gaussians">Alpha Compositing with Gaussians</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#mathematical-formulation-of-3d-gaussian-splatting">Mathematical Formulation of 3D Gaussian Splatting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#projecting-3d-gaussians-to-2d">Projecting 3D Gaussians to 2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#parameterization-of-3d-gaussians">Parameterization of 3D Gaussians</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#view-dependent-appearance">View-Dependent Appearance</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-rendering-equations">Differentiable Rendering Equations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#training-and-optimization">Training and Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#photometric-loss">Photometric Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#initial-point-cloud">Initial Point Cloud</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#optimization-process">Optimization Process</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-splatting-pipeline">Differentiable Splatting Pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#adaptive-density-control">Adaptive Density Control</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#implementation-and-real-time-rendering">Implementation and Real-Time Rendering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#tile-based-rendering">Tile-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#fast-sorting-strategies">Fast Sorting Strategies</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#gpu-accelerated-rasterization">GPU-Accelerated Rasterization</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#memory-considerations">Memory Considerations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#comparison-with-other-methods">Comparison with Other Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#nerf-vs-3d-gaussian-splatting">NeRF vs. 3D Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#voxel-based-representations-vs-gaussians">Voxel-Based Representations vs. Gaussians</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#traditional-point-based-rendering-vs-gaussian-splatting">Traditional Point-Based Rendering vs. Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#applications-and-extensions">Applications and Extensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#static-scene-reconstruction">Static Scene Reconstruction</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#dynamic-scene-capture">Dynamic Scene Capture</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#avatar-creation-and-animation">Avatar Creation and Animation</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#integration-with-neural-rendering">Integration with Neural Rendering</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#large-scale-scene-rendering">Large-Scale Scene Rendering</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#bezier-gaussian-triangles-bg-triangle-for-sharper-rendering">Bézier Gaussian Triangles (BG-Triangle) for Sharper Rendering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#representation">Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#performance">Performance</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#human-reconstruction-with-gaussian-splatting-and-priors">Human Reconstruction with Gaussian Splatting and Priors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#eg-humannerf-efficient-generalizable-human-nerf">EG-HumanNeRF: Efficient Generalizable Human NeRF</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#gps-gaussian-pixel-wise-gaussian-splatting-for-humans">GPS-Gaussian: Pixel-Wise Gaussian Splatting for Humans</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#generalizable-human-gaussians-ghg-with-smpl">Generalizable Human Gaussians (GHG) with SMPL</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#dynamic-scene-reconstruction-with-4d-gaussian-splatting">Dynamic Scene Reconstruction with 4D Gaussian Splatting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussian-splatting-4dgs">4D Gaussian Splatting (4DGS)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#speed-and-memory-enhancements-4dgs-1k-and-mega">Speed and Memory Enhancements (4DGS-1K and MEGA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#applications-to-mocap-and-4d-human-rendering">Applications to MoCap and 4D Human Rendering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#implementation-details-and-real-time-performance">Implementation Details and Real-Time Performance</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#data-structures">Data Structures</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#rasterization-shaders">Rasterization &amp; Shaders</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#gpu-memory-and-throughput">GPU Memory and Throughput</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-rendering-implementation">Differentiable Rendering Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#training-vs-inference-compute">Training vs. Inference Compute</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#accuracy-vs-speed-trade-offs">Accuracy vs. Speed trade-offs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#benchmarks-and-comparative-evaluation">Benchmarks and Comparative Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#static-scene-comparison">Static Scene Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#human-novel-view-comparison">Human Novel-View Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#dynamic-scene-comparison">Dynamic Scene Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#comparative-summary-table">Comparative Summary Table</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#datasets-and-resources">Datasets and Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#synthetic-nerf-dataset-blender-scenes">Synthetic NeRF Dataset (Blender Scenes)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#llff-local-light-field-fusion">LLFF (Local Light Field Fusion)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#tanks-and-temples">Tanks and Temples</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#multi-object-360-co3d-common-objects-in-3d">Multi-Object 360 (CO3D - Common Objects in 3D)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#amass-archive-of-motion-capture-as-surface-shapes">AMASS (Archive of Motion Capture as Surface Shapes)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#cape-clothed-auto-person-encoding">CAPE (Clothed Auto Person Encoding)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#thuman-thuman2-0">THuman / THuman2.0</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#renderpeople">RenderPeople</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#open-source-implementations">Open-Source Implementations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#future-directions">Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#improved-compression-techniques-for-memory-efficiency">Improved Compression Techniques for Memory Efficiency</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#handling-dynamic-and-deformable-scenes">Handling Dynamic and Deformable Scenes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#advanced-material-modeling-for-realistic-rendering">Advanced Material Modeling for Realistic Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#hybrid-approaches-integrating-neural-fields-and-explicit-representations">Hybrid Approaches Integrating Neural Fields and Explicit Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#scalability-for-large-scale-scenes-city-level-and-beyond">Scalability for Large-Scale Scenes (City-Level and Beyond)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#real-time-applications-in-ar-vr-and-gaming">Real-Time Applications in AR/VR and Gaming</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#integration-with-existing-graphics-pipelines">Integration with Existing Graphics Pipelines</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#learning-from-limited-data">Learning from Limited Data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#conclusion">Conclusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#glossary">Glossary</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">References</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#lecture-01-1-historical-body-models">Lecture 01.1 (Historical Body Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lecture-01-2-introduction-to-human-models">Lecture 01.2 (Introduction to Human Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lecture-01-3-introduction-to-human-models-continued">Lecture 01.3 (Introduction to Human Models Continued)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lecture-02-1-image-formation">Lecture 02.1 (Image Formation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lecture-02-2-rotations-kinematic-chains">Lecture 02.2 (Rotations &amp; Kinematic Chains)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lecture-03-1-surface-representations">Lecture 03.1 (Surface Representations)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lecture-03-2-procrustes-alignment">Lecture 03.2 (Procrustes Alignment)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lecture-04-1-iterative-closest-points">Lecture 04.1 (Iterative Closest Points)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lecture-04-2-body-models">Lecture 04.2 (Body Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lecture-05-1-body-model-training">Lecture 05.1 (Body Model Training)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lecture-05-2-3d-registration">Lecture 05.2 (3D Registration)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lecture-06-1-fitting-smpl-to-images">Lecture 06.1 (Fitting SMPL to Images)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lecture-06-1-optimization-based-fitting-of-smpl-to-images">Lecture 06.1 (Optimization-Based Fitting of SMPL to Images)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lecture-06-2-learning-based-fitting-of-smpl-to-images">Lecture 06.2 (Learning-Based Fitting of SMPL to Images)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lecture-07-1-fitting-smpl-to-imu-optimization">Lecture 07.1 (Fitting SMPL to IMU Optimization)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lecture-07-2-fitting-smpl-to-imu-learning">Lecture 07.2 (Fitting SMPL to IMU Learning)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#classic-and-optimization-based-methods">Classic and Optimization-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="#learning-based-methods">Learning-Based Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#datasets-and-resources">Datasets and Resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="#relevant-software-and-libraries">Relevant Software and Libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="#neural-radiance-fields-nerf">Neural Radiance Fields (NERF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gaussian-splatting">Gaussian Splatting</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Virtual Humans Lecture</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">References</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/references.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="references">
<span id="ref-section"></span><h1>References<a class="headerlink" href="#references" title="Link to this heading"></a></h1>
<section id="lecture-01-1-historical-body-models">
<h2>Lecture 01.1 (Historical Body Models)<a class="headerlink" href="#lecture-01-1-historical-body-models" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p><strong>Johansson, G.</strong> (1973). <em>Visual perception of biological motion and a model for its analysis</em>. Perception &amp; Psychophysics, 14(2), 201–211.</p></li>
<li><p><strong>Marr, D., &amp; Nishihara, H.</strong> (1978). <em>Representation and recognition of the spatial organization of three-dimensional shapes</em>. Proc. R. Soc. Lond. B, 200(1140), 269–294.</p></li>
<li><p><strong>Nevatia, R., &amp; Binford, T.</strong> (1977). <em>Description and recognition of curved objects</em>. Artificial Intelligence, 8, 77–98.</p></li>
<li><p><strong>O’Rourke, J., &amp; Badler, N.</strong> (1980). <em>Model-based image analysis of human motion using constraint propagation</em>. IEEE Trans. on Pattern Analysis and Machine Intelligence, 2(6), 522–532.</p></li>
<li><p><strong>Hogg, D.</strong> (1983). <em>Model-based vision: A program to see a walking person</em>. Image and Vision Computing, 1(1), 5–20.</p></li>
<li><p><strong>Metaxas, D., &amp; Terzopoulos, D.</strong> (1993). <em>Shape and nonrigid motion estimation through physics-based synthesis</em>. IEEE Trans. on Pattern Analysis and Machine Intelligence, 15(6), 580–591.</p></li>
<li><p><strong>Gavrila, D., &amp; Davis, L.</strong> (1996). <em>3-D model-based tracking of humans in action: A multi-view approach</em>. Proc. CVPR, 73–80.</p></li>
<li><p><strong>Bregler, C., &amp; Malik, J.</strong> (1998). <em>Tracking people with twists and exponential maps</em>. Proc. CVPR, 8–15.</p></li>
<li><p><strong>Blanz, V., &amp; Vetter, T.</strong> (1999). <em>A morphable model for the synthesis of 3D faces</em>. Proc. SIGGRAPH ‘99, 187–194.</p></li>
<li><p><strong>CAESAR Project Report</strong> (1999). 3D body scans of ~4,000 individuals.</p></li>
<li><p><strong>Allen, B., Curless, B., &amp; Popović, Z.</strong> (2003). <em>The space of human body shapes: Reconstruction and parameterization from range scans</em>. ACM SIGGRAPH, 587–594.</p></li>
<li><p><strong>Anguelov, D., Srinivasan, P., Koller, D., Thrun, S., Rodgers, J., &amp; Davis, J.</strong> (2005). <em>SCAPE: Shape Completion and Animation of People</em>. ACM SIGGRAPH, 408–416.</p></li>
<li><p><strong>Hasler, N., Stoll, C., Sunkel, M., Rosenhahn, B., &amp; Seidel, H.-P.</strong> (2009). <em>A statistical model of human pose and body shape</em>. Eurographics 2009.</p></li>
<li><p><strong>Sigal, L., Balan, A., &amp; Black, M.</strong> (2010). <em>Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</em>. IJCV, 87(1), 4–27.</p></li>
<li><p><strong>Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., &amp; Black, M. J.</strong> (2015). <em>SMPL: A Skinned Multi-Person Linear Model</em>. ACM Transactions on Graphics, 34(6), 248:1–16.</p></li>
<li><p><strong>Pons-Moll, G., Pujades, S., Hu, S., &amp; Black, M. J.</strong> (2015). <em>Dyna: A model of dynamic human shape in motion</em>. ACM Transactions on Graphics, 34(4), 120:1–14.</p></li>
<li><p><strong>Bogo, F., et al.</strong> (2016). <em>Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</em>. ECCV 2016.</p></li>
<li><p><strong>Kanazawa, A., Black, M. J., Jacobs, D., &amp; Malik, J.</strong> (2018). <em>End-to-end recovery of human shape and pose</em>. CVPR 2018.</p></li>
<li><p><strong>Kato, H., Ushiku, Y., &amp; Harada, T.</strong> (2018). <em>Neural 3D Mesh Renderer</em>. CVPR 2018.</p></li>
<li><p><strong>Saito, S., Huang, Z., Natsume, R., Morishima, S., Kanazawa, A., &amp; Li, H.</strong> (2019). <em>PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization</em>. ICCV 2019.</p></li>
<li><p><strong>Shysheya, A., Zakharov, E., et al.</strong> (2019). <em>Textured Neural Avatars</em>. CVPR 2019.</p></li>
<li><p><strong>Deng, B., et al.</strong> (2020). <em>NASA: Neural Articulated Shape Approximation</em>. ECCV 2020.</p></li>
<li><p><strong>Recent Works:</strong> Various papers (2020–2022) on neural implicit representations, NeRF-based human modeling, and neural avatars.</p></li>
</ol>
</section>
<section id="lecture-01-2-introduction-to-human-models">
<h2>Lecture 01.2 (Introduction to Human Models)<a class="headerlink" href="#lecture-01-2-introduction-to-human-models" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p><strong>Allen, B., Curless, B., &amp; Popović, Z.</strong> (2003). <em>The space of human body shapes: Reconstruction and parameterization from range scans</em>. ACM SIGGRAPH, 587–594.</p></li>
<li><p><strong>Anguelov, D., Srinivasan, P., Koller, D., Thrun, S., Rodgers, J., &amp; Davis, J.</strong> (2005). <em>SCAPE: Shape Completion and Animation of People</em>. ACM SIGGRAPH, 408–416.</p></li>
<li><p><strong>Hirshberg, D., Loper, M., Rachlin, E., &amp; Black, M.</strong> (2012). <em>Coregistration: Simultaneous alignment and modeling of articulated 3D shape</em>. ECCV, 242–255.</p></li>
<li><p><strong>Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., &amp; Black, M. J.</strong> (2015). <em>SMPL: A Skinned Multi-Person Linear Model</em>. ACM Transactions on Graphics, 34(6), Article 248.</p></li>
<li><p><strong>Pons-Moll, G., Pujades, S., Hu, S., &amp; Black, M. J.</strong> (2017). <em>ClothCap: Seamless 4D clothing capture and retargeting</em>. ACM Transactions on Graphics, 36(4), Article 73.</p></li>
<li><p><strong>Pons-Moll, G., Taylor, J., &amp; Romero, J.</strong> (2015). <em>Dyna: A Model of Dynamic Human Shape in Motion</em>. ACM Transactions on Graphics, 34(4), 120:1–14.</p></li>
<li><p><strong>Allen, B., Curless, B., Popović, Z., &amp; Hertzmann, A.</strong> (2006). <em>Learning a correlated model of identity and pose-dependent body shape variation for real-time synthesis</em>. Proc. SCA, 147–156.</p></li>
<li><p><strong>Chen, Y., Liu, Z., &amp; Zhang, Z.</strong> (2013). <em>Tensor-based human body modeling</em>. CVPR, 105–112.</p></li>
<li><p><strong>Hasler, N., Stoll, C., Sunkel, M., Rosenhahn, B., &amp; Seidel, H.-P.</strong> (2009). <em>A statistical model of human pose and body shape</em>. Eurographics.</p></li>
<li><p><strong>Bogo, F., Kanazawa, A., Lassner, C., Gehler, P., Romero, J., &amp; Black, M.</strong> (2016). <em>Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</em>. ECCV, 561–578.</p></li>
<li><p><strong>Kanazawa, A., Black, M.J., Jacobs, D., &amp; Malik, J.</strong> (2018). <em>End-to-End Recovery of Human Shape and Pose</em>. CVPR, 7122–7131.</p></li>
<li><p><strong>Deng, B., Liu, L., Dong, Y., Chang, M., &amp; Cai, J.</strong> (2020). <em>NASA: Neural Articulated Shape Approximation</em>. ECCV 2020.</p></li>
<li><p><strong>Hanavan, E.P.</strong> (1964). <em>A Mathematical Model of the Human Body</em>. Technical Report, Air Force Aerospace Medical Research Lab.</p></li>
<li><p><strong>Kuipers, J.B.</strong> (2002). <em>Quaternions and Rotation Sequences: A Primer with Applications to Orbits, Aerospace and Virtual Reality</em>. Princeton University Press.</p></li>
<li><p><strong>Park, S.I., &amp; Hodgins, J.K.</strong> (2008). <em>Data-driven modeling of skin and muscle deformation</em>. ACM Transactions on Graphics, 27(3), Article 96.</p></li>
</ol>
</section>
<section id="lecture-01-3-introduction-to-human-models-continued">
<h2>Lecture 01.3 (Introduction to Human Models Continued)<a class="headerlink" href="#lecture-01-3-introduction-to-human-models-continued" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p><strong>Weber Brothers</strong> (1836). Early gait analysis (historical references).</p></li>
<li><p><strong>Baraff, D. &amp; Witkin, A.</strong> (1998). <em>Large steps in cloth simulation.</em> SIGGRAPH.</p></li>
<li><p><strong>Bogo, F., Kanazawa, A., Lassner, C., Gehler, P., Romero, J., &amp; Black, M.</strong> (2016). <em>Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image.</em> ECCV.</p></li>
<li><p><strong>Cao, Z., Simon, T., Wei, S. E., &amp; Sheikh, Y.</strong> (2017). <em>Realtime multi-person 2D pose estimation using part affinity fields.</em> CVPR.</p></li>
<li><p><strong>Pons-Moll, G., Pujades, S., Hu, S., &amp; Black, M. J.</strong> (2017). <em>ClothCap: Seamless 4D clothing capture and retargeting.</em> ACM TOG (SIGGRAPH).</p></li>
<li><p><strong>Park, J. J., Florence, P., Straub, J., Newcombe, R., &amp; Lovegrove, S.</strong> (2019). <em>DeepSDF: Learning continuous signed distance functions for shape representation.</em> CVPR.</p></li>
<li><p><strong>Delp, S. L., et al.</strong> <em>OpenSim: Open-Source Software to Create and Analyze Dynamic Simulations of Movement.</em></p></li>
<li><p><strong>Deng, B., et al.</strong> (2020). <em>NASA: Neural Articulated Shape Approximation.</em> ECCV.</p></li>
<li><p><strong>Güler, R. A., Neverova, N., &amp; Kokkinos, I.</strong> (2018). <em>DensePose: Dense human pose estimation in the wild.</em> CVPR.</p></li>
<li><p><strong>Hassan, M., et al.</strong> (2019). <em>Resolving 3D Human Pose Ambiguities with 3D Scene Constraints.</em> 3DV.</p></li>
<li><p><strong>Hodgins, J., Wooten, W., Brogan, D., &amp; O’Brien, J.</strong> (1995). <em>Animating human athletics.</em> SIGGRAPH.</p></li>
<li><p><strong>Kato, H., Ushiku, Y., &amp; Harada, T.</strong> (2018). <em>Neural 3D mesh renderer.</em> CVPR.</p></li>
<li><p><strong>Kanazawa, A., Black, M. J., Jacobs, D. W., &amp; Malik, J.</strong> (2018). <em>End-to-end recovery of human shape and pose.</em> CVPR.</p></li>
<li><p><strong>Kocabas, M., et al.</strong> (2020). <em>VIBE: Video inference for human body pose and shape estimation.</em> CVPR.</p></li>
<li><p><strong>Marey, E.-J., Muybridge, E.</strong> (1880s). Chronophotography and motion studies.</p></li>
<li><p><strong>Mordatch, I., et al.</strong> (2012). <em>Discovery of complex behaviors through contact-invariant optimization.</em> SIGGRAPH.</p></li>
<li><p><strong>Peng, X. B. &amp; van de Panne, M.</strong> (2018). <em>DeepMimic: Example-guided deep reinforcement learning of physics-based character skills.</em> SIGGRAPH.</p></li>
<li><p><strong>Pons-Moll, G. et al.</strong> (2015). <em>Dyna: A model of dynamic human shape in motion.</em> ACM TOG (SIGGRAPH).</p></li>
<li><p><strong>Anguelov, D., et al.</strong> (2005). <em>SCAPE: Shape completion and animation of people.</em> SIGGRAPH.</p></li>
<li><p><strong>Shoemake, K.</strong> (1985). <em>Animating rotation with quaternion curves.</em> SIGGRAPH.</p></li>
<li><p><strong>SMPL references: Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., &amp; Black, M. J.</strong> (2015). <em>SMPL: A Skinned Multi-Person Linear Model.</em> ACM TOG (SIGGRAPH Asia).</p></li>
<li><p><strong>TailorNet references: Patel, M.</strong>, et al. (2020). <em>TailorNet: Predicting clothing in 3D as a function of human pose, shape and garment style.</em> CVPR.</p></li>
<li><p><strong>VIBE references: Kocabas, M.</strong> (2020). <em>VIBE: Video Inference for Human Body Pose and Shape Estimation.</em> CVPR.</p></li>
<li><p><strong>Wang, N., et al.</strong> (2021). Various references on neural implicit representations for clothing.</p></li>
<li><p><strong>Xie, F., et al.</strong> (2021). <em>Physics-based motion correction.</em> (arXiv / conference).</p></li>
</ol>
</section>
<section id="lecture-02-1-image-formation">
<h2>Lecture 02.1 (Image Formation)<a class="headerlink" href="#lecture-02-1-image-formation" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p><strong>Hartley, R., &amp; Zisserman, A.</strong> (2004). <em>Multiple View Geometry in Computer Vision</em>, 2nd ed. Cambridge University Press.</p></li>
<li><p><strong>Zhang, Z.</strong> (2000). <em>A Flexible New Technique for Camera Calibration</em>. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(11), 1330-1334.</p></li>
<li><p><strong>OpenCV Documentation</strong> — Lens Distortion and Calibration.</p></li>
<li><p><strong>Levoy, M.</strong> (Stanford). Digital Photography course notes/lectures.</p></li>
<li><p><strong>Collins, R.</strong> <em>Camera Models in Computer Vision</em>, lecture/course slides.</p></li>
<li><p><strong>Alhazen (Ibn al-Haytham)</strong> (~1021). <em>Book of Optics</em>. English translation and commentary by A.I. Sabra, 1989.</p></li>
<li><p><strong>Kemp, M.</strong> (1990). <em>The Science of Art: Optical Themes in Western Art from Brunelleschi to Seurat</em>. Yale University Press.</p></li>
<li><p><strong>Niépce, J. N.</strong> (1826). <em>Heliography</em>, Earliest surviving photograph: <em>View from the Window at Le Gras</em>.</p></li>
<li><p><strong>Adelson, E. H., &amp; Bergen, J. R.</strong> (1991). <em>The Plenoptic Function and the Elements of Early Vision</em>. <em>Computational Models of Visual Processing</em>, MIT Press.</p></li>
<li><p><strong>Ng, R., et al.</strong> (2005). <em>Light Field Photography with a Hand-Held Plenoptic Camera</em>. Computer Science Technical Report, Stanford.</p></li>
<li><p><strong>Duarte, M. F., et al.</strong> (2008). <em>Single-Pixel Imaging via Compressive Sampling</em>. IEEE Signal Processing Magazine, 25(2), 83–91.</p></li>
<li><p><strong>Velten, A., et al.</strong> (2012). <em>Recovering Three-Dimensional Shape around a Corner using Ultrafast Time-of-Flight Imaging</em>. Nature Communications, 3:745.</p></li>
<li><p><strong>Herman, G. H.</strong> (1980). <em>Image Reconstruction from Projections</em>. Academic Press.</p></li>
<li><p><strong>Kajiya, J. T.</strong> (1986). <em>The Rendering Equation</em>. Proc. SIGGRAPH.</p></li>
<li><p><strong>Forsyth, D. A., &amp; Ponce, J.</strong> (2012). <em>Computer Vision: A Modern Approach</em>, 2nd Edition. Prentice Hall.</p></li>
<li><p><strong>Szeliski, R.</strong> (2010). <em>Computer Vision: Algorithms and Applications</em>. Springer.</p></li>
<li><p><strong>Trucco, E., &amp; Verri, A.</strong> (1998). <em>Introductory Techniques for 3-D Computer Vision</em>. Prentice Hall.</p></li>
<li><p><strong>Raskar, R., &amp; Tumblin, J.</strong> (2009). <em>Computational Photography: Mastering New Techniques for Lenses, Lighting, and Sensors</em>. A K Peters.</p></li>
<li><p><strong>Levoy, M., &amp; Hanrahan, P.</strong> (1996). <em>Light Field Rendering</em>. Proc. SIGGRAPH.</p></li>
<li><p><strong>Ihrke, I., Kutulakos, K., Lensch, H., Magnor, M., &amp; Heidrich, W.</strong> (2010). <em>Transparent and Specular Object Reconstruction</em>. Computer Graphics Forum, 29(8), 2400-2426.</p></li>
</ol>
</section>
<section id="lecture-02-2-rotations-kinematic-chains">
<h2>Lecture 02.2 (Rotations &amp; Kinematic Chains)<a class="headerlink" href="#lecture-02-2-rotations-kinematic-chains" title="Link to this heading"></a></h2>
<p><strong>Rotations &amp; so(3)</strong></p>
<ol class="arabic simple">
<li><p><strong>Kuipers, J.B.</strong> (2002). <em>Quaternions and Rotation Sequences</em>. Princeton University Press.</p></li>
<li><p><strong>Craig, J.J.</strong> (2005). <em>Introduction to Robotics: Mechanics and Control</em>. Pearson.</p></li>
<li><p><strong>Shoemake, K.</strong> (1985). <em>Animating Rotation with Quaternion Curves</em>. SIGGRAPH.</p></li>
<li><p><strong>NASA Technical Notes</strong> (1968). <em>On gimbal lock</em> (Apollo Missions).</p></li>
<li><p><strong>Rodrigues’ Rotation Formula, Exponential Map for so(3)</strong> (n.d.).
<cite>https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula</cite></p></li>
</ol>
<p><strong>Kinematic Chains</strong></p>
<ol class="arabic simple">
<li><p><strong>Bregler, C.</strong> (1998). <em>Articulated Body Tracking</em>. ICCV.</p></li>
<li><p><strong>Modern Robotics website</strong> (n.d.).
<cite>http://modernrobotics.northwestern.edu</cite></p></li>
<li><p><strong>Siciliano, B., &amp; Khatib, O.</strong> (eds) (2016). <em>Handbook of Robotics</em>. Springer.</p></li>
</ol>
<p><strong>Axis-Angle &amp; Non-Unit Axis</strong></p>
<ol class="arabic simple">
<li><p><strong>Hartley, R., &amp; Zisserman, A.</strong> (2004). <em>Multiple View Geometry</em>, 2nd ed. Cambridge University Press.</p></li>
<li><p><strong>Grassia, F.S.</strong> (1998). <em>Practical parameterization of rotations using the exponential map</em>. JGT.</p></li>
</ol>
</section>
<section id="lecture-03-1-surface-representations">
<h2>Lecture 03.1 (Surface Representations)<a class="headerlink" href="#lecture-03-1-surface-representations" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p><strong>Angel, E.</strong> (2008). <em>Interactive Computer Graphics</em>. Addison-Wesley.</p></li>
<li><p><strong>Botsch, M., et al.</strong> (2010). <em>Polygon Mesh Processing</em>. A K Peters.</p></li>
<li><p><strong>Curless, B., &amp; Levoy, M.</strong> (1996). <em>A Volumetric Method for Building Complex Models from Range Images</em>. SIGGRAPH.</p></li>
<li><p><strong>do Carmo, M.</strong> (1976). <em>Differential Geometry of Curves and Surfaces</em>. Prentice-Hall.</p></li>
<li><p><strong>Rusinkiewicz, S., &amp; Levoy, M.</strong> (2001). <em>Efficient Variants of the ICP Algorithm</em>. 3DIM.</p></li>
<li><p><strong>Piegl, L., &amp; Tiller, W.</strong> (1997). <em>The NURBS Book</em>. Springer.</p></li>
<li><p><strong>Park, J., et al.</strong> (2019). <em>DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation</em>. CVPR.</p></li>
<li><p><strong>Mildenhall, B., et al.</strong> (2020). <em>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</em>. ECCV.</p></li>
<li><p><strong>Osher, S., &amp; Fedkiw, R.</strong> (2003). <em>Level Set Methods and Dynamic Implicit Surfaces</em>. Springer.</p></li>
<li><p><strong>Kobbelt, L., &amp; Botsch, M.</strong> (2004). <em>A Survey of Point-Based Techniques in Computer Graphics</em>. Computers &amp; Graphics.</p></li>
<li><p><strong>Crane, K., de Goes, F., Desbrun, M., &amp; Schröder, P.</strong> (2013). <em>Digital Geometry Processing with Discrete Exterior Calculus</em>. ACM SIGGRAPH Courses.</p></li>
<li><p><strong>Barr, A.</strong> (1981). <em>Superquadrics and Angle-Preserving Transformations</em>. IEEE Computer Graphics and Applications.</p></li>
</ol>
</section>
<section id="lecture-03-2-procrustes-alignment">
<h2>Lecture 03.2 (Procrustes Alignment)<a class="headerlink" href="#lecture-03-2-procrustes-alignment" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p><strong>Dryden, I.L., &amp; Mardia, K.V.</strong> (2016). <em>Statistical Shape Analysis</em>. Wiley.</p></li>
<li><p><strong>Cootes, T.F.</strong> (1992). <em>Active Shape Models</em>. ECCV.</p></li>
<li><p><strong>Gower, J.C.</strong> (1975). <em>Generalized Procrustes Analysis</em>. Psychometrika.</p></li>
<li><p><strong>Loper, M., et al.</strong> (2015). <em>SMPL: A Skinned Multi-Person Linear Model</em>. ACM TOG (SIGGRAPH Asia).</p></li>
<li><p><strong>Hartley, R., &amp; Zisserman, A.</strong> (2004). <em>Multiple View Geometry in Computer Vision</em>. Cambridge University Press.</p></li>
</ol>
</section>
<section id="lecture-04-1-iterative-closest-points">
<h2>Lecture 04.1 (Iterative Closest Points)<a class="headerlink" href="#lecture-04-1-iterative-closest-points" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p><strong>Besl, P.J., &amp; McKay, N.D.</strong> (1992). <em>A Method for Registration of 3-D Shapes</em>. IEEE TPAMI.</p></li>
<li><p><strong>Chen, Y., &amp; Medioni, G.</strong> (1992). <em>Object Modeling by Registration of Multiple Range Images</em>. SIGGRAPH.</p></li>
<li><p><strong>Arun, K.S., Huang, T.S., &amp; Blostein, S.D.</strong> (1987). <em>Least-Squares Fitting of Two 3-D Point Sets</em>. IEEE TPAMI.</p></li>
<li><p><strong>Horn, B.K.P.</strong> (1987). <em>Closed-Form Solution of Absolute Orientation Using Unit Quaternions</em>. JOSA.</p></li>
<li><p><strong>Rusinkiewicz, S., &amp; Levoy, M.</strong> (2001). <em>Efficient Variants of the ICP Algorithm</em>. 3DIM.</p></li>
<li><p><strong>Segal, A., Haehnel, D., &amp; Thrun, S.</strong> (2009). <em>Generalized-ICP</em>. Robotics: Science and Systems.</p></li>
<li><p><strong>Amberg, B., Romdhani, S., &amp; Vetter, T.</strong> (2007). <em>Optimal Step Nonrigid ICP Algorithm</em>. CVPR.</p></li>
<li><p><strong>Myronenko, A., &amp; Song, X.</strong> (2010). <em>Point Set Registration: Coherent Point Drift</em>. IEEE TPAMI.</p></li>
<li><p><strong>Open3D Documentation</strong> (n.d.). <cite>http://www.open3d.org</cite>.</p></li>
<li><p><strong>SciPy Spatial Module Documentation</strong> (n.d.). <cite>https://docs.scipy.org/doc/scipy/reference/spatial.html</cite>.</p></li>
<li><p><strong>Newcombe, R.A., et al.</strong> (2011). <em>KinectFusion: Real-time 3D Reconstruction and Interaction Using a Moving Depth Camera</em>. UIST.</p></li>
<li><p><strong>Chetverikov, D., et al.</strong> (2002). <em>The Trimmed Iterative Closest Point Algorithm</em>. ICPR.</p></li>
<li><p><strong>Granger, S., &amp; Pennec, X.</strong> (2002). <em>Multi-scale EM-ICP: A Fast and Robust Approach for Surface Registration</em>. ECCV.</p></li>
<li><p><strong>Rangarajan, A., et al.</strong> (1997). <em>Softassign Procrustes Matching Algorithm</em>. IPMI.</p></li>
</ol>
</section>
<section id="lecture-04-2-body-models">
<h2>Lecture 04.2 (Body Models)<a class="headerlink" href="#lecture-04-2-body-models" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p><strong>Anguelov, D., Srinivasan, P., Koller, D., Thrun, S., Rodgers, J., &amp; Davis, J.</strong> (2005). <em>SCAPE: Shape Completion and Animation of People</em>. ACM SIGGRAPH.</p></li>
<li><p><strong>Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., &amp; Black, M. J.</strong> (2015). <em>SMPL: A Skinned Multi-Person Linear Model</em>. ACM Transactions on Graphics.</p></li>
<li><p><strong>Hartley, R., &amp; Zisserman, A.</strong> (2004). <em>Multiple View Geometry in Computer Vision</em>. 2nd Edition, Cambridge University Press.</p></li>
<li><p><strong>Besl, P.J., &amp; McKay, N.D.</strong> (1992). <em>A Method for Registration of 3-D Shapes</em>. IEEE TPAMI.</p></li>
<li><p><strong>Chen, Y., &amp; Medioni, G.</strong> (1992). <em>Object Modeling by Registration of Multiple Range Images</em>. SIGGRAPH.</p></li>
<li><p><strong>Arun, K.S., Huang, T.S., &amp; Blostein, S.D.</strong> (1987). <em>Least-Squares Fitting of Two 3-D Point Sets</em>. IEEE TPAMI.</p></li>
<li><p><strong>Kuipers, J.B.</strong> (1999). <em>Quaternions and Rotation Sequences</em>. Princeton University Press.</p></li>
<li><p><strong>Spong, M.W., Hutchinson, S., &amp; Vidyasagar, M.</strong> (2006). <em>Robot Modeling and Control</em>. Wiley.</p></li>
<li><p><strong>Myronenko, A., &amp; Song, X.</strong> (2009). <em>Point Set Registration: Coherent Point Drift</em>. NIPS.</p></li>
<li><p><strong>Pons-Moll, G., et al.</strong> (2023). <em>Training a Body Model and Fitting SMPL to Scans</em>. Virtual Humans (Lecture 5.1).</p></li>
<li><p><strong>Kanazawa, A., Black, M.J., Jacobs, D., &amp; Malik, J.</strong> (2018). <em>End-to-End Recovery of Human Shape and Pose</em>. CVPR.</p></li>
<li><p><strong>Szeliski, R.</strong> (2010). <em>Computer Vision: Algorithms and Applications</em>. Springer.</p></li>
</ol>
</section>
<section id="lecture-05-1-body-model-training">
<h2>Lecture 05.1 (Body Model Training)<a class="headerlink" href="#lecture-05-1-body-model-training" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p><strong>Anguelov, D., Srinivasan, P., Koller, D., Thrun, S., Rodgers, J., &amp; Davis, J.</strong> (2005). <em>SCAPE: Shape Completion and Animation of People</em>. ACM Transactions on Graphics, 24(3), 408-416.</p></li>
<li><p><strong>Hirshberg, D.A., Loper, M., Rachlin, E., &amp; Black, M.J.</strong> (2012). <em>Coregistration: Simultaneous Alignment and Modeling of Articulated 3D Shape</em>. European Conference on Computer Vision (ECCV), 242-255.</p></li>
<li><p><strong>Besl, P.J., &amp; McKay, N.D.</strong> (1992). <em>A Method for Registration of 3-D Shapes</em>. IEEE Transactions on Pattern Analysis and Machine Intelligence, 14(2), 239-256.</p></li>
<li><p><strong>Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., &amp; Black, M.J.</strong> (2015). <em>SMPL: A Skinned Multi-Person Linear Model</em>. ACM Transactions on Graphics, 34(6), 248:1-248:16.</p></li>
<li><p><strong>Hirshberg, D.A., Loper, M., Rachlin, E., &amp; Black, M.J.</strong> (2012). <em>Coregistration: Simultaneous Alignment and Modeling of Articulated 3D Shape</em>. European Conference on Computer Vision (ECCV), 242-255.</p></li>
<li><p><strong>Geman, S., &amp; McClure, D.E.</strong> (1987). <em>Statistical Methods for Tomographic Image Reconstruction</em>. Bulletin of the International Statistical Institute, 52(4), 5-21.</p></li>
<li><p><strong>Allen, B., Curless, B., &amp; Popović, Z.</strong> (2003). <em>The Space of Human Body Shapes: Reconstruction and Parameterization from Range Scans</em>. ACM Transactions on Graphics, 22(3), 587-594.</p></li>
<li><p><strong>Sorkine, O., &amp; Alexa, M.</strong> (2007). <em>As-Rigid-As-Possible Surface Modeling</em>. Symposium on Geometry Processing, 109-116.</p></li>
<li><p><strong>Amberg, B., Romdhani, S., &amp; Vetter, T.</strong> (2007). <em>Optimal Step Nonrigid ICP Algorithms for Surface Registration</em>. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 1-8.</p></li>
<li><p><strong>Myronenko, A., &amp; Song, X.</strong> (2010). <em>Point Set Registration: Coherent Point Drift</em>. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(12), 2262-2275.</p></li>
<li><p><strong>Pons-Moll, G., et al.</strong> (2015). <em>Dyna: A Model of Dynamic Human Shape in Motion</em>. ACM Transactions on Graphics, 34(4), 120:1-120:14.</p></li>
<li><p><strong>Bogo, F., Kanazawa, A., Lassner, C., Gehler, P., Romero, J., &amp; Black, M.J.</strong> (2016). <em>Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image</em>. European Conference on Computer Vision (ECCV), 561-578.</p></li>
<li><p><strong>Feng, A., Casas, D., &amp; Shapiro, A.</strong> (2015). <em>Avatar Reshaping and Automatic Rigging Using a Deformable Model</em>. Proceedings of the 8th ACM SIGGRAPH Conference on Motion in Games, 57-64.</p></li>
<li><p><strong>Joo, H., Simon, T., &amp; Sheikh, Y.</strong> (2018). <em>Total Capture: A 3D Deformation Model for Tracking Faces, Hands, and Bodies</em>. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 8320-8329.</p></li>
</ol>
</section>
<section id="lecture-05-2-3d-registration">
<span id="lecture-05-2-references"></span><h2>Lecture 05.2 (3D Registration)<a class="headerlink" href="#lecture-05-2-3d-registration" title="Link to this heading"></a></h2>
<p><strong>Rigid Registration and ICP</strong></p>
<ol class="arabic simple">
<li><p><strong>Horn, B.K.P.</strong> (1987). <em>Closed-Form Solution of Absolute Orientation Using Unit Quaternions</em>. Journal of the Optical Society of America A, 4(4), 629-642.</p></li>
<li><p><strong>Arun, K.S., Huang, T.S., &amp; Blostein, S.D.</strong> (1987). <em>Least-Squares Fitting of Two 3-D Point Sets</em>. IEEE Transactions on Pattern Analysis and Machine Intelligence, 9(5), 698-700.</p></li>
<li><p><strong>Umeyama, S.</strong> (1991). <em>Least-Squares Estimation of Transformation Parameters Between Two Point Patterns</em>. IEEE Transactions on Pattern Analysis and Machine Intelligence, 13(4), 376-380.</p></li>
<li><p><strong>Besl, P.J., &amp; McKay, N.D.</strong> (1992). <em>A Method for Registration of 3-D Shapes</em>. IEEE Transactions on Pattern Analysis and Machine Intelligence, 14(2), 239-256.</p></li>
<li><p><strong>Chen, Y., &amp; Medioni, G.</strong> (1991). <em>Object Modeling by Registration of Multiple Range Images</em>. Proceedings of IEEE International Conference on Robotics and Automation, 2724-2729.</p></li>
<li><p><strong>Zhang, Z.</strong> (1994). <em>Iterative Point Matching for Registration of Free-Form Curves and Surfaces</em>. International Journal of Computer Vision, 13(2), 119-152.</p></li>
<li><p><strong>Rusinkiewicz, S., &amp; Levoy, M.</strong> (2001). <em>Efficient Variants of the ICP Algorithm</em>. Proceedings of the Third International Conference on 3D Digital Imaging and Modeling, 145-152.</p></li>
<li><p><strong>Yang, J., Li, H., Campbell, D., &amp; Jia, Y.</strong> (2016). <em>Go-ICP: A Globally Optimal Solution to 3D ICP Point-Set Registration</em>. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(11), 2241-2254.</p></li>
</ol>
<p><strong>Non-Rigid Registration</strong></p>
<ol class="arabic simple" start="9">
<li><p><strong>Gold, S., Rangarajan, A., Lu, C.P., Pappu, S., &amp; Mjolsness, E.</strong> (1998). <em>New Algorithms for 2D and 3D Point Matching: Pose Estimation and Correspondence</em>. Pattern Recognition, 31(8), 1019-1031.</p></li>
<li><p><strong>Chui, H., &amp; Rangarajan, A.</strong> (2003). <em>A New Point Matching Algorithm for Non-Rigid Registration</em>. Computer Vision and Image Understanding, 89(2-3), 114-141.</p></li>
<li><p><strong>Myronenko, A., &amp; Song, X.</strong> (2010). <em>Point Set Registration: Coherent Point Drift</em>. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(12), 2262-2275.</p></li>
<li><p><strong>Amberg, B., Romdhani, S., &amp; Vetter, T.</strong> (2007). <em>Optimal Step Nonrigid ICP Algorithms for Surface Registration</em>. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 1-8.</p></li>
<li><p><strong>Sumner, R.W., Schmid, J., &amp; Pauly, M.</strong> (2007). <em>Embedded Deformation for Shape Manipulation</em>. ACM Transactions on Graphics (SIGGRAPH), 26(3), Article 80.</p></li>
<li><p><strong>Li, H., Adams, B., Guibas, L.J., &amp; Pauly, M.</strong> (2009). <em>Robust Single-View Geometry and Motion Reconstruction</em>. ACM Transactions on Graphics (SIGGRAPH Asia), 28(5), Article 175.</p></li>
<li><p><strong>Feldmar, J., &amp; Ayache, N.</strong> (1996). <em>Rigid, Affine and Locally Affine Registration of Free-Form Surfaces</em>. International Journal of Computer Vision, 18(2), 99-119.</p></li>
<li><p><strong>Bogo, F., Romero, J., Loper, M., &amp; Black, M.J.</strong> (2014). <em>FAUST: Dataset and Evaluation for 3D Mesh Registration</em>. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 3794-3801.</p></li>
</ol>
<p><strong>Parametric Models and SMPL</strong></p>
<ol class="arabic simple" start="17">
<li><p><strong>Anguelov, D., Srinivasan, P., Koller, D., Thrun, S., Rodgers, J., &amp; Davis, J.</strong> (2005). <em>SCAPE: Shape Completion and Animation of People</em>. ACM Transactions on Graphics (SIGGRAPH), 24(3), 408-416.</p></li>
<li><p><strong>Allen, B., Curless, B., &amp; Popović, Z.</strong> (2003). <em>The Space of Human Body Shapes: Reconstruction and Parameterization from Range Scans</em>. ACM Transactions on Graphics (SIGGRAPH), 22(3), 587-594.</p></li>
<li><p><strong>Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., &amp; Black, M.J.</strong> (2015). <em>SMPL: A Skinned Multi-Person Linear Model</em>. ACM Transactions on Graphics (SIGGRAPH Asia), 34(6), Article 248.</p></li>
<li><p><strong>Bogo, F., Kanazawa, A., Lassner, C., Gehler, P., Romero, J., &amp; Black, M.J.</strong> (2016). <em>Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image</em>. Proceedings of European Conference on Computer Vision (ECCV), 561-578.</p></li>
<li><p><strong>Hirshberg, D.A., Loper, M., Rachlin, E., &amp; Black, M.J.</strong> (2012). <em>Coregistration: Simultaneous Alignment and Modeling of Articulated 3D Shape</em>. Proceedings of European Conference on Computer Vision (ECCV), 242-255.</p></li>
<li><p><strong>Hasler, N., Stoll, C., Sunkel, M., Rosenhahn, B., &amp; Seidel, H.P.</strong> (2009). <em>A Statistical Model of Human Pose and Body Shape</em>. Computer Graphics Forum (Eurographics), 28(2), 337-346.</p></li>
</ol>
<p><strong>Clothing and SMPL+D</strong></p>
<ol class="arabic simple" start="23">
<li><p><strong>Alldieck, T., Magnor, M., Xu, W., Theobalt, C., &amp; Pons-Moll, G.</strong> (2019). <em>Learning to Reconstruct People in Clothing from a Single RGB Camera</em>. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 1175-1186.</p></li>
<li><p><strong>Ma, Q., Yang, J., Ranjan, A., Pujades, S., Pons-Moll, G., Tang, S., &amp; Black, M.J.</strong> (2020). <em>Learning to Dress 3D People in Generative Clothing</em>. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 6469-6478.</p></li>
<li><p><strong>Pons-Moll, G., Pujades, S., Hu, S., &amp; Black, M.J.</strong> (2017). <em>ClothCap: Seamless 4D Clothing Capture and Retargeting</em>. ACM Transactions on Graphics (SIGGRAPH), 36(4), Article 73.</p></li>
<li><p><strong>Yang, J., Franco, J.S., Hétroy-Wheeler, F., &amp; Wuhrer, S.</strong> (2018). <em>Analyzing Clothing Layer Deformation Statistics of 3D Human Motions</em>. Proceedings of European Conference on Computer Vision (ECCV), 237-253.</p></li>
</ol>
<p><strong>Modern Learning-Based Methods</strong></p>
<ol class="arabic simple" start="27">
<li><p><strong>Groueix, T., Fisher, M., Kim, V.G., Russell, B.C., &amp; Aubry, M.</strong> (2018). <em>3D-CODED: 3D Correspondences by Deep Deformation</em>. Proceedings of European Conference on Computer Vision (ECCV), 230-246.</p></li>
<li><p><strong>Bhatnagar, B.L., Tiwari, G., Theobalt, C., &amp; Pons-Moll, G.</strong> (2020). <em>IPNet: Combining Implicit Function Learning and Parametric Models for 3D Human Reconstruction</em>. Proceedings of European Conference on Computer Vision (ECCV), 311-329.</p></li>
<li><p><strong>Bhatnagar, B.L., Xie, X., Petrov, I., Sminchisescu, C., Theobalt, C., &amp; Pons-Moll, G.</strong> (2020). <em>LoopReg: Self-supervised Learning of Implicit Surface Correspondences, Pose and Shape for 3D Human Mesh Registration</em>. Advances in Neural Information Processing Systems (NeurIPS), 33.</p></li>
<li><p><strong>Chen, X., Zheng, Y., Black, M.J., Hilliges, O., &amp; Geiger, A.</strong> (2021). <em>SNARF: Differentiable Forward Skinning for Animating Non-Rigid Neural Implicit Shapes</em>. Proceedings of International Conference on Computer Vision (ICCV), 11594-11604.</p></li>
<li><p><strong>Deng, B., Lewis, J.P., Jeruzalski, T., Pons-Moll, G., Hinton, G., Norouzi, M., &amp; Tagliasacchi, A.</strong> (2020). <em>NASA Neural Articulated Shape Approximation</em>. Proceedings of European Conference on Computer Vision (ECCV), 612-628.</p></li>
<li><p><strong>Corona, E., Pumarola, A., Alenyà, G., Pons-Moll, G., &amp; Moreno-Noguer, F.</strong> (2022). <em>Learned Vertex Descent: A New Direction for 3D Human Model Fitting</em>. Proceedings of European Conference on Computer Vision (ECCV), 716-734.</p></li>
<li><p><strong>Wang, N., Zhang, Y., Li, Z., Fu, Y., Liu, W., &amp; Xiang, Y.</strong> (2020). <em>Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images</em>. Proceedings of European Conference on Computer Vision (ECCV), 55-71.</p></li>
<li><p><strong>Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., &amp; Ng, R.</strong> (2020). <em>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</em>. Proceedings of European Conference on Computer Vision (ECCV), 405-421.</p></li>
<li><p><strong>Saito, S., Huang, Z., Natsume, R., Morishima, S., Kanazawa, A., &amp; Li, H.</strong> (2019). <em>PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization</em>. Proceedings of International Conference on Computer Vision (ICCV), 2304-2314.</p></li>
</ol>
<p><strong>Datasets</strong></p>
<ol class="arabic simple" start="36">
<li><p><strong>Bogo, F., Romero, J., Loper, M., &amp; Black, M.J.</strong> (2014). <em>FAUST: Dataset and Evaluation for 3D Mesh Registration</em>. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 3794-3801.</p></li>
<li><p><strong>Ma, Q., Yang, J., Ranjan, A., Pujades, S., Pons-Moll, G., Tang, S., &amp; Black, M.J.</strong> (2020). <em>Learning to Dress 3D People in Generative Clothing</em>. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 6469-6478.</p></li>
<li><p><strong>Turk, G., &amp; Levoy, M.</strong> (1994). <em>The Stanford 3D Scanning Repository</em>. Stanford University Computer Graphics Laboratory.</p></li>
<li><p><strong>Zhou, Q.Y., Park, J., &amp; Koltun, V.</strong> (2016). <em>Fast Global Registration</em>. Proceedings of European Conference on Computer Vision (ECCV), 766-782.</p></li>
<li><p><strong>Newcombe, R.A., Fox, D., &amp; Seitz, S.M.</strong> (2015). <em>DynamicFusion: Reconstruction and Tracking of Non-rigid Scenes in Real-Time</em>. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 343-352.</p></li>
</ol>
</section>
<section id="lecture-06-1-fitting-smpl-to-images">
<h2>Lecture 06.1 (Fitting SMPL to Images)<a class="headerlink" href="#lecture-06-1-fitting-smpl-to-images" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p><strong>Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., &amp; Black, M. J.</strong> (2015). <em>SMPL: A Skinned Multi-Person Linear Model</em>. ACM Transactions on Graphics, 34(6), 248:1–16.</p></li>
<li><p><strong>Bogo, F., Kanazawa, A., Lassner, C., Gehler, P., Romero, J., &amp; Black, M.</strong> (2016). <em>Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image</em>. ECCV 2016.</p></li>
<li><p><strong>Hartley, R., &amp; Zisserman, A.</strong> (2004). <em>Multiple View Geometry in Computer Vision</em>, 2nd ed. Cambridge University Press.</p></li>
<li><p><strong>Pavlakos, G., Choutas, V., Ghorbani, N., Bolkart, T., Osman, A. A., Tzionas, D., &amp; Black, M. J.</strong> (2019). <em>Expressive Body Capture: 3D Hands, Face, and Body from a Single Image</em>. CVPR 2019.</p></li>
<li><p><strong>Kanazawa, A., Black, M. J., Jacobs, D., &amp; Malik, J.</strong> (2018). <em>End-to-end Recovery of Human Shape and Pose</em>. CVPR 2018.</p></li>
<li><p><strong>Kolotouros, N., Pavlakos, G., Black, M. J., &amp; Daniilidis, K.</strong> (2019). <em>Learning to Reconstruct 3D Human Pose and Shape via Model-fitting in the Loop</em>. ICCV 2019.</p></li>
<li><p><strong>Zhang, Y., Chen, X., Li, T., Tian, S., Wang, M., &amp; Tang, S.</strong> (2021). <em>PyMAF: 3D Human Pose and Shape Regression with Pyramidal Mesh Alignment Feedback Loop</em>. ICCV 2021.</p></li>
<li><p><strong>Feng, Y., Feng, M., Black, M. J., &amp; Bolkart, T.</strong> (2021). <em>Collaborative Regression of Expressive Bodies using Moderation</em>. ICCV 2021.</p></li>
<li><p><strong>Li, Z., Wu, T., Dellandrea, E., Wang, Y., &amp; Chen, L.</strong> (2022). <em>CLIFF: Carrying Location Information in Full Frames into Human Pose and Shape Estimation</em>. ECCV 2022.</p></li>
<li><p><strong>Kocabas, M., Athanasiou, N., &amp; Black, M. J.</strong> (2020). <em>VIBE: Video Inference for Human Body Pose and Shape Estimation</em>. CVPR 2020.</p></li>
<li><p><strong>Ionescu, C., Papava, D., Olaru, V., &amp; Sminchisescu, C.</strong> (2014). <em>Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments</em>. IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(7), 1325-1339.</p></li>
<li><p><strong>von Marcard, T., Henschel, R., Black, M., Rosenhahn, B., &amp; Pons-Moll, G.</strong> (2018). <em>Recovering Accurate 3D Human Pose in the Wild Using IMUs and a Moving Camera</em>. ECCV 2018.</p></li>
<li><p><strong>Lassner, C., Romero, J., Kiefel, M., Bogo, F., Black, M. J., &amp; Gehler, P. V.</strong> (2017). <em>Unite the People: Closing the Loop Between 3D and 2D Human Representations</em>. CVPR 2017.</p></li>
<li><p><strong>Patel, P., Huang, C.-H., Tesch, J., Hoffmann, D., Tripathi, S., &amp; Black, M. J.</strong> (2021). <em>AGORA: Avatars in Geography Optimized for Regression Analysis</em>. CVPR 2021.</p></li>
<li><p><strong>Zhou, Y., Barnes, C., Lu, J., Yang, J., &amp; Li, H.</strong> (2019). <em>On the Continuity of Rotation Representations in Neural Networks</em>. CVPR 2019.</p></li>
<li><p><strong>Geman, S., &amp; McClure, D.</strong> (1987). <em>Statistical Methods for Tomographic Image Reconstruction</em>. Bulletin of the International Statistical Institute, 52(4), 5-21.</p></li>
<li><p><strong>Nocedal, J., &amp; Wright, S. J.</strong> (2006). <em>Numerical Optimization</em>. Springer.</p></li>
<li><p><strong>Anguelov, D., Srinivasan, P., Koller, D., Thrun, S., Rodgers, J., &amp; Davis, J.</strong> (2005). <em>SCAPE: Shape Completion and Animation of People</em>. ACM Transactions on Graphics (SIGGRAPH), 24(3), 408-416.</p></li>
<li><p><strong>Johnson, S., &amp; Everingham, M.</strong> (2010). <em>Clustered Pose and Nonlinear Appearance Models for Human Pose Estimation</em>. BMVC 2010.</p></li>
<li><p><strong>Alldieck, T., Magnor, M., Xu, W., Theobalt, C., &amp; Pons-Moll, G.</strong> (2019). <em>Learning to Reconstruct People in Clothing from a Single RGB Camera</em>. CVPR 2019.</p></li>
<li><p><strong>Cao, Z., Hidalgo, G., Simon, T., Wei, S.-E., &amp; Sheikh, Y.</strong> (2021). <em>OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields</em>. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(1), 172-186.</p></li>
<li><p><strong>Kato, H., Ushiku, Y., &amp; Harada, T.</strong> (2018). <em>Neural 3D Mesh Renderer</em>. CVPR 2018.</p></li>
</ol>
</section>
<section id="lecture-06-1-optimization-based-fitting-of-smpl-to-images">
<h2>Lecture 06.1 (Optimization-Based Fitting of SMPL to Images)<a class="headerlink" href="#lecture-06-1-optimization-based-fitting-of-smpl-to-images" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p><strong>Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., &amp; Black, M. J.</strong> (2015). <em>SMPL: A Skinned Multi-Person Linear Model</em>. ACM Transactions on Graphics, 34(6), 248:1–16.</p></li>
<li><p><strong>Bogo, F., Kanazawa, A., Lassner, C., Gehler, P., Romero, J., &amp; Black, M.</strong> (2016). <em>Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image</em>. ECCV 2016.</p></li>
<li><p><strong>Hartley, R., &amp; Zisserman, A.</strong> (2004). <em>Multiple View Geometry in Computer Vision</em>, 2nd ed. Cambridge University Press.</p></li>
<li><p><strong>Pavlakos, G., Choutas, V., Ghorbani, N., Bolkart, T., Osman, A. A., Tzionas, D., &amp; Black, M. J.</strong> (2019). <em>Expressive Body Capture: 3D Hands, Face, and Body from a Single Image</em>. CVPR 2019.</p></li>
<li><p><strong>Kanazawa, A., Black, M. J., Jacobs, D., &amp; Malik, J.</strong> (2018). <em>End-to-end Recovery of Human Shape and Pose</em>. CVPR 2018.</p></li>
<li><p><strong>Kolotouros, N., Pavlakos, G., Black, M. J., &amp; Daniilidis, K.</strong> (2019). <em>Learning to Reconstruct 3D Human Pose and Shape via Model-fitting in the Loop</em>. ICCV 2019.</p></li>
<li><p><strong>Zhang, H., Tian, Y., Zhou, X., Ouyang, W., Liu, Y., Wang, L., &amp; Sun, Z.</strong> (2021). <em>PyMAF: 3D Human Pose and Shape Regression with Pyramidal Mesh Alignment Feedback Loop</em>. ICCV 2021.</p></li>
<li><p><strong>Feng, Y., Feng, M., Black, M. J., &amp; Bolkart, T.</strong> (2021). <em>Collaborative Regression of Expressive Bodies using Moderation</em>. ICCV 2021.</p></li>
<li><p><strong>Li, Z., Wu, T., Dellandrea, E., Wang, Y., &amp; Chen, L.</strong> (2022). <em>CLIFF: Carrying Location Information in Full Frames into Human Pose and Shape Estimation</em>. ECCV 2022.</p></li>
<li><p><strong>Kocabas, M., Athanasiou, N., &amp; Black, M. J.</strong> (2020). <em>VIBE: Video Inference for Human Body Pose and Shape Estimation</em>. CVPR 2020.</p></li>
<li><p><strong>Ionescu, C., Papava, D., Olaru, V., &amp; Sminchisescu, C.</strong> (2014). <em>Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments</em>. IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(7), 1325-1339.</p></li>
<li><p><strong>von Marcard, T., Henschel, R., Black, M., Rosenhahn, B., &amp; Pons-Moll, G.</strong> (2018). <em>Recovering Accurate 3D Human Pose in the Wild Using IMUs and a Moving Camera</em>. ECCV 2018.</p></li>
<li><p><strong>Lassner, C., Romero, J., Kiefel, M., Bogo, F., Black, M. J., &amp; Gehler, P. V.</strong> (2017). <em>Unite the People: Closing the Loop Between 3D and 2D Human Representations</em>. CVPR 2017.</p></li>
<li><p><strong>Patel, P., Huang, C.-H., Tesch, J., Hoffmann, D., Tripathi, S., &amp; Black, M. J.</strong> (2021). <em>AGORA: Avatars in Geography Optimized for Regression Analysis</em>. CVPR 2021.</p></li>
<li><p><strong>Zhou, Y., Barnes, C., Lu, J., Yang, J., &amp; Li, H.</strong> (2019). <em>On the Continuity of Rotation Representations in Neural Networks</em>. CVPR 2019.</p></li>
<li><p><strong>Geman, S., &amp; McClure, D.</strong> (1987). <em>Statistical Methods for Tomographic Image Reconstruction</em>. Bulletin of the International Statistical Institute, 52(4), 5-21.</p></li>
<li><p><strong>Nocedal, J., &amp; Wright, S. J.</strong> (2006). <em>Numerical Optimization</em>. Springer.</p></li>
<li><p><strong>Anguelov, D., Srinivasan, P., Koller, D., Thrun, S., Rodgers, J., &amp; Davis, J.</strong> (2005). <em>SCAPE: Shape Completion and Animation of People</em>. ACM Transactions on Graphics (SIGGRAPH), 24(3), 408-416.</p></li>
<li><p><strong>Johnson, S., &amp; Everingham, M.</strong> (2010). <em>Clustered Pose and Nonlinear Appearance Models for Human Pose Estimation</em>. BMVC 2010.</p></li>
<li><p><strong>Alldieck, T., Magnor, M., Xu, W., Theobalt, C., &amp; Pons-Moll, G.</strong> (2019). <em>Learning to Reconstruct People in Clothing from a Single RGB Camera</em>. CVPR 2019.</p></li>
<li><p><strong>Cao, Z., Hidalgo, G., Simon, T., Wei, S.-E., &amp; Sheikh, Y.</strong> (2021). <em>OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields</em>. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(1), 172-186.</p></li>
<li><p><strong>Kato, H., Ushiku, Y., &amp; Harada, T.</strong> (2018). <em>Neural 3D Mesh Renderer</em>. CVPR 2018.</p></li>
</ol>
</section>
<section id="lecture-06-2-learning-based-fitting-of-smpl-to-images">
<h2>Lecture 06.2 (Learning-Based Fitting of SMPL to Images)<a class="headerlink" href="#lecture-06-2-learning-based-fitting-of-smpl-to-images" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p><strong>Kanazawa, A., Black, M. J., Jacobs, D. W., &amp; Malik, J.</strong> (2018). <em>End-to-End Recovery of Human Shape and Pose</em>. CVPR 2018.</p></li>
<li><p><strong>Omran, M., Lassner, C., Pons-Moll, G., Gehler, P., &amp; Schiele, B.</strong> (2018). <em>Neural Body Fitting: Unifying Deep Learning and Model Based Human Pose and Shape Estimation</em>. 3DV 2018.</p></li>
<li><p><strong>Kolotouros, N., Pavlakos, G., Black, M. J., &amp; Daniilidis, K.</strong> (2019). <em>Learning to Reconstruct 3D Human Pose and Shape via Model-fitting in the Loop</em>. ICCV 2019.</p></li>
<li><p><strong>Zhang, H., Tian, Y., Zhou, X., Ouyang, W., Liu, Y., Wang, L., &amp; Sun, Z.</strong> (2021). <em>PyMAF: 3D Human Pose and Shape Regression with Pyramidal Mesh Alignment Feedback Loop</em>. ICCV 2021.</p></li>
<li><p><strong>Li, Z., Wu, T., Dellandrea, E., Wang, Y., &amp; Chen, L.</strong> (2022). <em>CLIFF: Carrying Location Information in Full Frames into Human Pose and Shape Estimation</em>. ECCV 2022.</p></li>
<li><p><strong>Feng, Y., Feng, M., Black, M. J., &amp; Bolkart, T.</strong> (2021). <em>Collaborative Regression of Expressive Bodies using Moderation</em>. ICCV 2021.</p></li>
<li><p><strong>Kocabas, M., Athanasiou, N., &amp; Black, M. J.</strong> (2020). <em>VIBE: Video Inference for Human Body Pose and Shape Estimation</em>. CVPR 2020.</p></li>
<li><p><strong>Choi, H., Moon, G., Chang, J. Y., &amp; Lee, K. M.</strong> (2021). <em>Beyond Static Features for Temporally Consistent 3D Human Pose and Shape from a Video</em>. CVPR 2021.</p></li>
<li><p><strong>Zhu, W., Ma, X., Wang, Y., Li, H., &amp; Kong, W.</strong> (2023). <em>MotionBERT: Unified Pretraining for Human Motion Analysis</em>. ICCV 2023.</p></li>
<li><p><strong>Pavlakos, G., Zhu, L., Zhou, X., &amp; Daniilidis, K.</strong> (2018). <em>Learning to Estimate 3D Human Pose and Shape from a Single Color Image</em>. CVPR 2018.</p></li>
<li><p><strong>Joo, H., Simon, T., &amp; Sheikh, Y.</strong> (2018). <em>Total Capture: A 3D Deformation Model for Tracking Faces, Hands, and Bodies</em>. CVPR 2018.</p></li>
<li><p><strong>Zheng, Z., Yu, T., Wei, Y., Dai, Q., &amp; Liu, Y.</strong> (2019). <em>DeepHuman: 3D Human Reconstruction from a Single Image</em>. ICCV 2019.</p></li>
<li><p><strong>Goel, S., Katan, A., Kanazawa, A., &amp; Malik, J.</strong> (2022). <em>Human Mesh Recovery from Multiple Shots</em>. CVPR 2022.</p></li>
<li><p><strong>Andriluka, M., Pishchulin, L., Gehler, P., &amp; Schiele, B.</strong> (2014). <em>2D Human Pose Estimation: New Benchmark and State of the Art Analysis</em>. CVPR 2014.</p></li>
<li><p><strong>Kocabas, M., Huang, C.-H. P., Hilliges, O., &amp; Black, M. J.</strong> (2021). <em>PARE: Part Attention Regressor for 3D Human Body Estimation</em>. ICCV 2021.</p></li>
<li><p><strong>Güler, R. A., Neverova, N., &amp; Kokkinos, I.</strong> (2018). <em>DensePose: Dense Human Pose Estimation in the Wild</em>. CVPR 2018.</p></li>
<li><p><strong>Zhou, Y., Barnes, C., Lu, J., Yang, J., &amp; Li, H.</strong> (2019). <em>On the Continuity of Rotation Representations in Neural Networks</em>. CVPR 2019.</p></li>
<li><p><strong>Mehta, D., Sridhar, S., Sotnychenko, O., Rhodin, H., Shafiei, M., Seidel, H.-P., Xu, W., Casas, D., &amp; Theobalt, C.</strong> (2017). <em>VNect: Real-time 3D Human Pose Estimation with a Single RGB Camera</em>. ACM Transactions on Graphics, 36(4), 1-14.</p></li>
<li><p><strong>Mahmood, N., Ghorbani, N., Troje, N. F., Pons-Moll, G., &amp; Black, M. J.</strong> (2019). <em>AMASS: Archive of Motion Capture as Surface Shapes</em>. ICCV 2019.</p></li>
<li><p><strong>Sun, Y., Bao, Q., Liu, W., Fu, Y., Black, M. J., &amp; Mei, T.</strong> (2021). <em>Monocular, One-stage, Regression of Multiple 3D People</em>. ICCV 2021.</p></li>
<li><p><strong>Huang, Z., Zhu, Y., Bogo, F., Lassner, C., Mehta, D., Sotnychenko, O., Romero, J., &amp; Black, M. J.</strong> (2022). <em>SMPLer-X: Scaling Up Expressive Human Shape and Pose Modeling</em>. arXiv preprint arXiv:2207.02628.</p></li>
<li><p><strong>Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., &amp; Black, M. J.</strong> (2015). <em>SMPL: A Skinned Multi-Person Linear Model</em>. ACM Transactions on Graphics, 34(6), 248:1–16.</p></li>
</ol>
</section>
<section id="lecture-07-1-fitting-smpl-to-imu-optimization">
<h2>Lecture 07.1 (Fitting SMPL to IMU Optimization)<a class="headerlink" href="#lecture-07-1-fitting-smpl-to-imu-optimization" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p>Mahony, R., Hamel, T., &amp; Pflimlin, J.-M. (2008). <em>Nonlinear Complementary Filters on the Special Orthogonal Group</em>. IEEE Transactions on Automatic Control, 53(5), 1203-1218.
DOI: <a class="reference external" href="https://doi.org/10.1109/TAC.2008.923738">https://doi.org/10.1109/TAC.2008.923738</a>
URL: <a class="reference external" href="https://hal.archives-ouvertes.fr/hal-00488376/document">https://hal.archives-ouvertes.fr/hal-00488376/document</a></p></li>
<li><p>Madgwick, S. O. H. (2010). <em>An Efficient Orientation Filter for Inertial and Inertial/Magnetic Sensor Arrays</em>. Report x-io Technologies.
URL: <a class="reference external" href="https://x-io.co.uk/open-source-imu-and-ahrs-algorithms/">https://x-io.co.uk/open-source-imu-and-ahrs-algorithms/</a>
GitHub: <a class="reference external" href="https://github.com/xioTechnologies/Fusion">https://github.com/xioTechnologies/Fusion</a></p></li>
<li><p>Roetenberg, D., Luinge, H., &amp; Slycke, P. (2007). <em>Xsens MVN: Full 6DOF Human Motion Tracking Using Miniature Inertial Sensors</em>. Xsens Technologies White Paper.
URL: <a class="reference external" href="https://www.xsens.com/hubfs/Downloads/usermanual/MVN_user_manual.pdf">https://www.xsens.com/hubfs/Downloads/usermanual/MVN_user_manual.pdf</a></p></li>
<li><p>Slyper, R., &amp; Hodgins, J. K. (2008). <em>Action Capture with Accelerometers</em>. ACM Symposium on Computer Animation (SCA).
DOI: <a class="reference external" href="https://doi.org/10.1145/1632592.1632604">https://doi.org/10.1145/1632592.1632604</a></p></li>
<li><p>Tautges, J., Zinke, A., Krüger, B., Weber, A., Baumann, J., &amp; Helten, T. (2011). <em>Motion Reconstruction Using Sparse Accelerometer Data</em>. ACM Transactions on Graphics (TOG), 30(3), Article No. 18.
DOI: <a class="reference external" href="https://doi.org/10.1145/1966394.1966397">https://doi.org/10.1145/1966394.1966397</a></p></li>
<li><p>Riaz, Q., Tao, G., Krüger, B., &amp; Weber, A. (2015). <em>Motion reconstruction using very few accelerometers and ground contacts</em>. Graphical Models, 79, 23-38.
DOI: <a class="reference external" href="https://doi.org/10.1016/j.gmod.2015.04.001">https://doi.org/10.1016/j.gmod.2015.04.001</a></p></li>
<li><p>von Marcard, T., Pons-Moll, G., &amp; Rosenhahn, B. (2017). <em>Sparse Inertial Poser: Automatic 3D Human Pose Estimation from Sparse IMUs</em>. Computer Graphics Forum (Eurographics 2017).
DOI: <a class="reference external" href="https://doi.org/10.1111/cgf.13125">https://doi.org/10.1111/cgf.13125</a>
Project Page: <a class="reference external" href="https://virtualhumans.mpi-inf.mpg.de/sip/">https://virtualhumans.mpi-inf.mpg.de/sip/</a>
GitHub: <a class="reference external" href="https://github.com/wangsen1312/Sparse-Inertial-Poser">https://github.com/wangsen1312/Sparse-Inertial-Poser</a> (unofficial)</p></li>
<li><p>Huang, Y., Kaufmann, M., Aksan, E., Black, M. J., &amp; Hilliges, O. (2018). <em>Deep Inertial Poser: Learning to Reconstruct Human Pose from Sparse Inertial Measurements in Real Time</em>. ACM Transactions on Graphics (SIGGRAPH Asia 2018), 37(6), Article No. 185.
DOI: <a class="reference external" href="https://doi.org/10.1145/3272127.3275108">https://doi.org/10.1145/3272127.3275108</a>
arXiv: <a class="reference external" href="https://arxiv.org/abs/1809.07116">https://arxiv.org/abs/1809.07116</a>
Project Page &amp; Dataset: <a class="reference external" href="http://dip.is.tue.mpg.de/">http://dip.is.tue.mpg.de/</a>
GitHub: <a class="reference external" href="https://github.com/eth-ait/dip18">https://github.com/eth-ait/dip18</a></p></li>
<li><p>Yi, X., Zhou, Y., Xu, F., Yan, W., &amp; Tan, J. (2021). <em>TransPose: Real-time 3D Human Translation and Pose Estimation with Six Inertial Sensors</em>. ACM Transactions on Graphics (SIGGRAPH Asia), 40(4).
DOI: <a class="reference external" href="https://doi.org/10.1145/3450626.3459786">https://doi.org/10.1145/3450626.3459786</a>
arXiv: <a class="reference external" href="https://arxiv.org/abs/2105.11796">https://arxiv.org/abs/2105.11796</a>
GitHub: <a class="reference external" href="https://github.com/Xinyu-Yi/TransPose">https://github.com/Xinyu-Yi/TransPose</a></p></li>
<li><p>von Marcard, T., Henschel, R., Black, M. J., Rosenhahn, B., &amp; Pons-Moll, G. (2018). <em>Recovering Accurate 3D Human Pose in The Wild Using IMUs and a Moving Camera</em>. European Conference on Computer Vision (ECCV), 614-631.
DOI: <a class="reference external" href="https://doi.org/10.1007/978-3-030-01249-6_37">https://doi.org/10.1007/978-3-030-01249-6_37</a>
URL: <a class="reference external" href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Timo_von_Marcard_Recovering_Accurate_3D_ECCV_2018_paper.pdf">https://openaccess.thecvf.com/content_ECCV_2018/papers/Timo_von_Marcard_Recovering_Accurate_3D_ECCV_2018_paper.pdf</a></p></li>
<li><p>Trumble, M., Gilbert, A., Malleson, C., Hilton, A., &amp; Collomosse, J. (2017). <em>Total Capture: 3D Human Pose Estimation Fusing Video and Inertial Sensors</em>. British Machine Vision Conference (BMVC), 1-13.
DOI: <a class="reference external" href="https://doi.org/10.5244/C.31.14">https://doi.org/10.5244/C.31.14</a>
Dataset: <a class="reference external" href="https://cvssp.org/data/totalcapture/">https://cvssp.org/data/totalcapture/</a></p></li>
<li><p>Mahmood, N., Ghorbani, N., Troje, N. F., Pons-Moll, G., &amp; Black, M. J. (2019). <em>AMASS: Archive of Motion Capture as Surface Shapes</em>. ICCV 2019.
DOI: <a class="reference external" href="https://doi.org/10.1109/ICCV.2019.00520">https://doi.org/10.1109/ICCV.2019.00520</a>
Project Page: <a class="reference external" href="http://amass.is.tue.mpg.de">http://amass.is.tue.mpg.de</a>
GitHub: <a class="reference external" href="https://github.com/nghorbani/amass">https://github.com/nghorbani/amass</a></p></li>
<li><p>Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., &amp; Black, M. J. (2015). <em>SMPL: A Skinned Multi-Person Linear Model</em>. SIGGRAPH Asia 2015.
DOI: <a class="reference external" href="https://doi.org/10.1145/2816795.2818013">https://doi.org/10.1145/2816795.2818013</a>
Project Page: <a class="reference external" href="https://smpl.is.tue.mpg.de/">https://smpl.is.tue.mpg.de/</a>
GitHub: <a class="reference external" href="https://github.com/vchoutas/smplx">https://github.com/vchoutas/smplx</a></p></li>
<li><p>Kim, J., Bae, S.-H., &amp; Woo, W. (2023). <em>IMUPoser: Full-Body Pose Estimation using IMUs in Phones</em>. CHI Conference on Human Factors in Computing Systems, 1-14.
DOI: <a class="reference external" href="https://doi.org/10.1145/3544548.3580991">https://doi.org/10.1145/3544548.3580991</a>
Project Page: <a class="reference external" href="https://rikky0611.github.io/IMUPoser/">https://rikky0611.github.io/IMUPoser/</a></p></li>
<li><p>Xu, F., Xu, H., Yin, X., Yi, X., &amp; Tan, J. (2023). <em>PIP: Physics-informed Human Motion Pose Estimation from Sparse Inertial Sensors</em>. IEEE Transactions on Visualization and Computer Graphics.
DOI: <a class="reference external" href="https://doi.org/10.1109/TVCG.2023.3276484">https://doi.org/10.1109/TVCG.2023.3276484</a>
arXiv: <a class="reference external" href="https://arxiv.org/abs/2303.02585">https://arxiv.org/abs/2303.02585</a></p></li>
</ol>
</section>
<section id="lecture-07-2-fitting-smpl-to-imu-learning">
<h2>Lecture 07.2 (Fitting SMPL to IMU Learning)<a class="headerlink" href="#lecture-07-2-fitting-smpl-to-imu-learning" title="Link to this heading"></a></h2>
<section id="classic-and-optimization-based-methods">
<h3>Classic and Optimization-Based Methods<a class="headerlink" href="#classic-and-optimization-based-methods" title="Link to this heading"></a></h3>
<ol class="arabic simple">
<li><p>Mahony, R., Hamel, T., &amp; Pflimlin, J.-M. (2008). <em>Nonlinear Complementary Filters on the Special Orthogonal Group</em>. IEEE Transactions on Automatic Control, 53(5), 1203-1218.
DOI: <a class="reference external" href="https://doi.org/10.1109/TAC.2008.923738">https://doi.org/10.1109/TAC.2008.923738</a>
URL: <a class="reference external" href="https://hal.archives-ouvertes.fr/hal-00488376/document">https://hal.archives-ouvertes.fr/hal-00488376/document</a></p></li>
<li><p>Madgwick, S. O. H. (2010). <em>An Efficient Orientation Filter for Inertial and Inertial/Magnetic Sensor Arrays</em>. Report x-io Technologies.
URL: <a class="reference external" href="https://x-io.co.uk/open-source-imu-and-ahrs-algorithms/">https://x-io.co.uk/open-source-imu-and-ahrs-algorithms/</a>
GitHub: <a class="reference external" href="https://github.com/xioTechnologies/Fusion">https://github.com/xioTechnologies/Fusion</a></p></li>
<li><p>Roetenberg, D., Luinge, H., &amp; Slycke, P. (2007). <em>Xsens MVN: Full 6DOF Human Motion Tracking Using Miniature Inertial Sensors</em>. Xsens Technologies White Paper.
URL: <a class="reference external" href="https://www.xsens.com/hubfs/Downloads/usermanual/MVN_user_manual.pdf">https://www.xsens.com/hubfs/Downloads/usermanual/MVN_user_manual.pdf</a></p></li>
<li><p>Slyper, R., &amp; Hodgins, J. K. (2008). <em>Action Capture with Accelerometers</em>. ACM Symposium on Computer Animation (SCA).
DOI: <a class="reference external" href="https://doi.org/10.1145/1632592.1632604">https://doi.org/10.1145/1632592.1632604</a></p></li>
<li><p>Tautges, J., Zinke, A., Krüger, B., Weber, A., Baumann, J., &amp; Helten, T. (2011). <em>Motion Reconstruction Using Sparse Accelerometer Data</em>. ACM Transactions on Graphics (TOG), 30(3), Article No. 18.
DOI: <a class="reference external" href="https://doi.org/10.1145/1966394.1966397">https://doi.org/10.1145/1966394.1966397</a></p></li>
<li><p>Riaz, Q., Tao, G., Krüger, B., &amp; Weber, A. (2015). <em>Motion reconstruction using very few accelerometers and ground contacts</em>. Graphical Models, 79, 23-38.
DOI: <a class="reference external" href="https://doi.org/10.1016/j.gmod.2015.04.001">https://doi.org/10.1016/j.gmod.2015.04.001</a></p></li>
<li><p>von Marcard, T., Pons-Moll, G., &amp; Rosenhahn, B. (2017). <em>Sparse Inertial Poser: Automatic 3D Human Pose Estimation from Sparse IMUs</em>. Computer Graphics Forum (Eurographics 2017).
DOI: <a class="reference external" href="https://doi.org/10.1111/cgf.13125">https://doi.org/10.1111/cgf.13125</a>
Project Page: <a class="reference external" href="https://virtualhumans.mpi-inf.mpg.de/sip/">https://virtualhumans.mpi-inf.mpg.de/sip/</a>
GitHub: <a class="reference external" href="https://github.com/wangsen1312/Sparse-Inertial-Poser">https://github.com/wangsen1312/Sparse-Inertial-Poser</a> (unofficial)</p></li>
</ol>
</section>
<section id="learning-based-methods">
<h3>Learning-Based Methods<a class="headerlink" href="#learning-based-methods" title="Link to this heading"></a></h3>
<ol class="arabic simple" start="8">
<li><p>Huang, Y., Kaufmann, M., Aksan, E., Black, M. J., &amp; Hilliges, O. (2018). <em>Deep Inertial Poser: Learning to Reconstruct Human Pose from Sparse Inertial Measurements in Real Time</em>. ACM Transactions on Graphics (SIGGRAPH Asia 2018), 37(6), Article No. 185.
DOI: <a class="reference external" href="https://doi.org/10.1145/3272127.3275108">https://doi.org/10.1145/3272127.3275108</a>
arXiv: <a class="reference external" href="https://arxiv.org/abs/1809.07116">https://arxiv.org/abs/1809.07116</a>
Project Page &amp; Dataset: <a class="reference external" href="http://dip.is.tue.mpg.de/">http://dip.is.tue.mpg.de/</a>
GitHub: <a class="reference external" href="https://github.com/eth-ait/dip18">https://github.com/eth-ait/dip18</a></p></li>
<li><p>Yi, X., Zhou, Y., Xu, F., Yan, W., &amp; Tan, J. (2021). <em>TransPose: Real-time 3D Human Translation and Pose Estimation with Six Inertial Sensors</em>. ACM Transactions on Graphics (SIGGRAPH Asia), 40(4).
DOI: <a class="reference external" href="https://doi.org/10.1145/3450626.3459786">https://doi.org/10.1145/3450626.3459786</a>
arXiv: <a class="reference external" href="https://arxiv.org/abs/2105.11796">https://arxiv.org/abs/2105.11796</a>
GitHub: <a class="reference external" href="https://github.com/Xinyu-Yi/TransPose">https://github.com/Xinyu-Yi/TransPose</a></p></li>
<li><p>Jiang, J., Larsson, P., &amp; Black, M. J. (2022). <em>TIP: Task-Informed Motion Priors for 3D Human Body Tracking</em>. ACM Transactions on Graphics (SIGGRAPH Asia).
arXiv: <a class="reference external" href="https://arxiv.org/abs/2209.04318">https://arxiv.org/abs/2209.04318</a>
Project Page: <a class="reference external" href="https://github.com/jyf588/transformer-inertial-poser">https://github.com/jyf588/transformer-inertial-poser</a></p></li>
<li><p>Yi, X., Zhou, Y., Xu, F., &amp; Tan, J. (2022). <em>PIP: Physics-informed Human Motion Pose Estimation from Sparse Inertial Sensors</em>. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).
DOI: <a class="reference external" href="https://doi.org/10.1109/CVPR52688.2022.01322">https://doi.org/10.1109/CVPR52688.2022.01322</a>
Project Page: <a class="reference external" href="https://xinyu-yi.github.io/PIP/">https://xinyu-yi.github.io/PIP/</a>
GitHub: <a class="reference external" href="https://github.com/Xinyu-Yi/PIP">https://github.com/Xinyu-Yi/PIP</a></p></li>
<li><p>von Marcard, T., Henschel, R., Black, M. J., Rosenhahn, B., &amp; Pons-Moll, G. (2018). <em>Recovering Accurate 3D Human Pose in The Wild Using IMUs and a Moving Camera</em>. European Conference on Computer Vision (ECCV), 614-631.
DOI: <a class="reference external" href="https://doi.org/10.1007/978-3-030-01249-6_37">https://doi.org/10.1007/978-3-030-01249-6_37</a>
URL: <a class="reference external" href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Timo_von_Marcard_Recovering_Accurate_3D_ECCV_2018_paper.pdf">https://openaccess.thecvf.com/content_ECCV_2018/papers/Timo_von_Marcard_Recovering_Accurate_3D_ECCV_2018_paper.pdf</a></p></li>
</ol>
</section>
</section>
<section id="datasets-and-resources">
<h2>Datasets and Resources<a class="headerlink" href="#datasets-and-resources" title="Link to this heading"></a></h2>
<ol class="arabic simple" start="13">
<li><p>Trumble, M., Gilbert, A., Malleson, C., Hilton, A., &amp; Collomosse, J. (2017). <em>Total Capture: 3D Human Pose Estimation Fusing Video and Inertial Sensors</em>. British Machine Vision Conference (BMVC), 1-13.
DOI: <a class="reference external" href="https://doi.org/10.5244/C.31.14">https://doi.org/10.5244/C.31.14</a>
Dataset: <a class="reference external" href="https://cvssp.org/data/totalcapture/">https://cvssp.org/data/totalcapture/</a></p></li>
<li><p>Mahmood, N., Ghorbani, N., Troje, N. F., Pons-Moll, G., &amp; Black, M. J. (2019). <em>AMASS: Archive of Motion Capture as Surface Shapes</em>. ICCV 2019.
DOI: <a class="reference external" href="https://doi.org/10.1109/ICCV.2019.00520">https://doi.org/10.1109/ICCV.2019.00520</a>
Project Page: <a class="reference external" href="http://amass.is.tue.mpg.de">http://amass.is.tue.mpg.de</a>
GitHub: <a class="reference external" href="https://github.com/nghorbani/amass">https://github.com/nghorbani/amass</a></p></li>
<li><p>Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., &amp; Black, M. J. (2015). <em>SMPL: A Skinned Multi-Person Linear Model</em>. SIGGRAPH Asia 2015.
DOI: <a class="reference external" href="https://doi.org/10.1145/2816795.2818013">https://doi.org/10.1145/2816795.2818013</a>
Project Page: <a class="reference external" href="https://smpl.is.tue.mpg.de/">https://smpl.is.tue.mpg.de/</a>
GitHub: <a class="reference external" href="https://github.com/vchoutas/smplx">https://github.com/vchoutas/smplx</a></p></li>
<li><p>Kim, J., Bae, S.-H., &amp; Woo, W. (2023). <em>IMUPoser: Full-Body Pose Estimation using IMUs in Phones</em>. CHI Conference on Human Factors in Computing Systems, 1-14.
DOI: <a class="reference external" href="https://doi.org/10.1145/3544548.3580991">https://doi.org/10.1145/3544548.3580991</a>
Project Page: <a class="reference external" href="https://rikky0611.github.io/IMUPoser/">https://rikky0611.github.io/IMUPoser/</a></p></li>
<li><p>Xu, F., Xu, H., Yin, X., Yi, X., &amp; Tan, J. (2023). <em>PIP: Physics-informed Human Motion Pose Estimation from Sparse Inertial Sensors</em>. IEEE Transactions on Visualization and Computer Graphics.
DOI: <a class="reference external" href="https://doi.org/10.1109/TVCG.2023.3276484">https://doi.org/10.1109/TVCG.2023.3276484</a>
arXiv: <a class="reference external" href="https://arxiv.org/abs/2303.02585">https://arxiv.org/abs/2303.02585</a></p></li>
</ol>
</section>
<section id="relevant-software-and-libraries">
<h2>Relevant Software and Libraries<a class="headerlink" href="#relevant-software-and-libraries" title="Link to this heading"></a></h2>
<ol class="arabic simple" start="18">
<li><p>PyTorch (Deep Learning Framework)
URL: <a class="reference external" href="https://pytorch.org">https://pytorch.org</a></p></li>
<li><p>TensorFlow (Deep Learning Framework)
URL: <a class="reference external" href="https://www.tensorflow.org">https://www.tensorflow.org</a></p></li>
<li><p>PyTorch3D (3D Computer Vision Library)
GitHub: <a class="reference external" href="https://github.com/facebookresearch/pytorch3d">https://github.com/facebookresearch/pytorch3d</a></p></li>
<li><p>Pinocchio (Rigid Body Dynamics Library)
GitHub: <a class="reference external" href="https://github.com/stack-of-tasks/pinocchio">https://github.com/stack-of-tasks/pinocchio</a></p></li>
<li><p>SMPL-X (Official SMPL Model Implementation)
GitHub: <a class="reference external" href="https://github.com/vchoutas/smplx">https://github.com/vchoutas/smplx</a></p></li>
<li><p>SMPLify (Fitting SMPL to Data)
GitHub: <a class="reference external" href="https://github.com/classner/up/tree/master/up_tools/camera">https://github.com/classner/up/tree/master/up_tools/camera</a></p></li>
</ol>
</section>
<section id="neural-radiance-fields-nerf">
<h2>Neural Radiance Fields (NERF)<a class="headerlink" href="#neural-radiance-fields-nerf" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p><strong>Adelson, E. H., &amp; Bergen, J. R.</strong> (1991). <em>The plenoptic function and the elements of early vision</em>. In Computational Models of Visual Processing, 3–20. Cambridge, MA: MIT Press.</p></li>
<li><p><strong>Allen, B., Curless, B., &amp; Popović, Z.</strong> (2003). <em>The space of human body shapes: Reconstruction and parameterization from range scans</em>. ACM Transactions on Graphics, 22(3), 587–594.</p></li>
<li><p><strong>Alldieck, T., Magnor, M., Xu, W., Theobalt, C., &amp; Pons-Moll, G.</strong> (2018). <em>Video based reconstruction of 3D people models</em>. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 8387–8397.</p></li>
<li><p><strong>Anguelov, D., Srinivasan, P., Koller, D., Thrun, S., Rodgers, J., &amp; Davis, J.</strong> (2005). <em>SCAPE: Shape Completion and Animation of People</em>. ACM Transactions on Graphics (SIGGRAPH), 24(3), 408–416.</p></li>
<li><p><strong>Barr, A.</strong> (1981). <em>Superquadrics and Angle-Preserving Transformations</em>. IEEE Computer Graphics and Applications, 1(1), 11–23.</p></li>
<li><p><strong>Barron, J. T., Mildenhall, B., Tancik, M., Hedman, P., Martin-Brualla, R., &amp; Srinivasan, P. P.</strong> (2021). <em>Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields</em>. IEEE/CVF International Conference on Computer Vision (ICCV), 5855–5864.</p></li>
<li><p><strong>Barron, J. T., Mildenhall, B., Verbin, D., Srinivasan, P. P., &amp; Hedman, P.</strong> (2022). <em>Mip-NeRF 360: Unbounded anti-aliased neural radiance fields</em>. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 5470–5479.</p></li>
<li><p><strong>Besl, P.J., &amp; McKay, N.D.</strong> (1992). <em>A Method for Registration of 3-D Shapes</em>. IEEE Transactions on Pattern Analysis and Machine Intelligence, 14(2), 239–256.</p></li>
<li><p><strong>Bhatnagar, B.L., Tiwari, G., Theobalt, C., &amp; Pons-Moll, G.</strong> (2020). <em>IPNet: Combining Implicit Function Learning and Parametric Models for 3D Human Reconstruction</em>. European Conference on Computer Vision (ECCV), 311–329.</p></li>
<li><p><strong>Blanz, V., &amp; Vetter, T.</strong> (1999). <em>A morphable model for the synthesis of 3D faces</em>. Proceedings of SIGGRAPH ‘99, 187–194.</p></li>
<li><p><strong>Botsch, M., Kobbelt, L., Pauly, M., Alliez, P., &amp; Lévy, B.</strong> (2010). <em>Polygon Mesh Processing</em>. A K Peters/CRC Press.</p></li>
<li><p><strong>Chen, Y., Liu, Z., &amp; Zhang, Z.</strong> (2013). <em>Tensor-based human body modeling</em>. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 105–112.</p></li>
<li><p><strong>Chen, Y., &amp; Medioni, G.</strong> (1992). <em>Object Modeling by Registration of Multiple Range Images</em>. Image and Vision Computing, 10(3), 145–155.</p></li>
<li><p><strong>Choy, C. B., Xu, D., Gwak, J., Chen, K., &amp; Savarese, S.</strong> (2016). <em>3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction</em>. European Conference on Computer Vision (ECCV), 628–644.</p></li>
<li><p><strong>Corona, E., Pumarola, A., Alenyà, G., Pons-Moll, G., &amp; Moreno-Noguer, F.</strong> (2022). <em>Learned Vertex Descent: A New Direction for 3D Human Model Fitting</em>. European Conference on Computer Vision (ECCV), 716–734.</p></li>
<li><p><strong>Curless, B., &amp; Levoy, M.</strong> (1996). <em>A Volumetric Method for Building Complex Models from Range Images</em>. Proceedings of SIGGRAPH ‘96, 303–312.</p></li>
<li><p><strong>Deng, B., Lewis, J.P., Jeruzalski, T., Pons-Moll, G., Hinton, G., Norouzi, M., &amp; Tagliasacchi, A.</strong> (2020). <em>NASA Neural Articulated Shape Approximation</em>. European Conference on Computer Vision (ECCV), 612–628.</p></li>
<li><p><strong>Feng, Y., Feng, M., Black, M. J., &amp; Bolkart, T.</strong> (2021). <em>Learning an Animatable Detailed 3D Face Model from In-The-Wild Images</em>. ACM Transactions on Graphics (SIGGRAPH), 40(4), Article 88.</p></li>
<li><p><strong>Fridovich-Keil, S., Yu, A., Tancik, M., Chen, Q., Recht, B., &amp; Kanazawa, A.</strong> (2022). <em>Plenoxels: Radiance Fields without Neural Networks</em>. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 5501–5510.</p></li>
<li><p><strong>Gortler, S. J., Grzeszczuk, R., Szeliski, R., &amp; Cohen, M. F.</strong> (1996). <em>The Lumigraph</em>. Proceedings of SIGGRAPH ‘96, 43–54.</p></li>
<li><p><strong>Grossman, J. P., &amp; Dally, W. J.</strong> (1998). <em>Point Sample Rendering</em>. Rendering Techniques ‘98 (Proceedings of the Eurographics Workshop on Rendering), 181–192.</p></li>
<li><p><strong>Hasler, N., Stoll, C., Sunkel, M., Rosenhahn, B., &amp; Seidel, H.-P.</strong> (2009). <em>A statistical model of human pose and body shape</em>. Computer Graphics Forum (Eurographics), 28(2), 337–346.</p></li>
<li><p><strong>Ionescu, C., Papava, D., Olaru, V., &amp; Sminchisescu, C.</strong> (2014). <em>Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments</em>. IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(7), 1325–1339.</p></li>
<li><p><strong>Kajiya, J. T., &amp; Von Herzen, B. P.</strong> (1984). <em>Ray tracing volume densities</em>. Proceedings of SIGGRAPH ‘84, 165–174.</p></li>
<li><p><strong>Kanazawa, A., Black, M. J., Jacobs, D., &amp; Malik, J.</strong> (2018). <em>End-to-end Recovery of Human Shape and Pose</em>. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 7122–7131.</p></li>
<li><p><strong>Kazhdan, M., Bolitho, M., &amp; Hoppe, H.</strong> (2006). <em>Poisson Surface Reconstruction</em>. Proceedings of the Fourth Eurographics Symposium on Geometry Processing, 61–70.</p></li>
<li><p><strong>Kerbl, B., Kopanas, G., Leimkühler, T., &amp; Drettakis, G.</strong> (2023). <em>3D Gaussian Splatting for Real-Time Radiance Field Rendering</em>. ACM Transactions on Graphics (SIGGRAPH), 42(4), Article 142.</p></li>
<li><p><strong>Knapitsch, A., Park, J., Zhou, Q.-Y., &amp; Koltun, V.</strong> (2017). <em>Tanks and Temples: Benchmarking Large-Scale Scene Reconstruction</em>. ACM Transactions on Graphics, 36(4), Article 78.</p></li>
<li><p><strong>Kocabas, M., Athanasiou, N., &amp; Black, M. J.</strong> (2020). <em>VIBE: Video Inference for Human Body Pose and Shape Estimation</em>. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 5253–5263.</p></li>
<li><p><strong>Kutulakos, K. N., &amp; Seitz, S. M.</strong> (2000). <em>A Theory of Shape by Space Carving</em>. International Journal of Computer Vision, 38(3), 199–218.</p></li>
<li><p><strong>Laurentini, A.</strong> (1994). <em>The Visual Hull Concept for Silhouette-Based Image Understanding</em>. IEEE Transactions on Pattern Analysis and Machine Intelligence, 16(2), 150–162.</p></li>
<li><p><strong>Levoy, M., &amp; Hanrahan, P.</strong> (1996). <em>Light Field Rendering</em>. Proceedings of SIGGRAPH ‘96, 31–42.</p></li>
<li><p><strong>Li, Z., Niklaus, S., Snavely, N., &amp; Wang, O.</strong> (2021). <em>Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes</em>. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 6498–6508.</p></li>
<li><p><strong>Liu, L., Gu, J., Lin, K. Z., Chua, T. S., &amp; Theobalt, C.</strong> (2020). <em>Neural Sparse Voxel Fields</em>. Advances in Neural Information Processing Systems (NeurIPS), 33, 15651–15663.</p></li>
<li><p><strong>Lombardi, S., Simon, T., Saragih, J., Schwartz, G., Lehrmann, A., &amp; Sheikh, Y.</strong> (2019). <em>Neural Volumes: Learning Dynamic Renderable Volumes from Images</em>. ACM Transactions on Graphics (SIGGRAPH), 38(4), Article 65.</p></li>
<li><p><strong>Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., &amp; Black, M. J.</strong> (2015). <em>SMPL: A Skinned Multi-Person Linear Model</em>. ACM Transactions on Graphics (SIGGRAPH Asia), 34(6), Article 248.</p></li>
<li><p><strong>Martin-Brualla, R., Radwan, N., Sajjadi, M. S. M., Barron, J. T., Dosovitskiy, A., &amp; Duckworth, D.</strong> (2021). <em>NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections</em>. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 7210–7219.</p></li>
<li><p><strong>Maturana, D., &amp; Scherer, S.</strong> (2015). <em>VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition</em>. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 922–928.</p></li>
<li><p><strong>Max, N.</strong> (1995). <em>Optical Models for Direct Volume Rendering</em>. IEEE Transactions on Visualization and Computer Graphics, 1(2), 99–108.</p></li>
<li><p><strong>Mildenhall, B., Srinivasan, P. P., Ortiz-Cayon, R., Kalantari, N. K., Ramamoorthi, R., Ng, R., &amp; Kar, A.</strong> (2019). <em>Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines</em>. ACM Transactions on Graphics (SIGGRAPH), 38(4), Article 29.</p></li>
<li><p><strong>Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., &amp; Ng, R.</strong> (2020). <em>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</em>. European Conference on Computer Vision (ECCV), 405–421.</p></li>
<li><p><strong>Müller, T., Evans, A., Schied, C., &amp; Keller, A.</strong> (2022). <em>Instant Neural Graphics Primitives with a Multiresolution Hash Encoding</em>. ACM Transactions on Graphics (SIGGRAPH), 41(4), Article 102.</p></li>
<li><p><strong>Ng, R., Levoy, M., Brédif, M., Duval, G., Horowitz, M., &amp; Hanrahan, P.</strong> (2005). <em>Light Field Photography with a Hand-Held Plenoptic Camera</em>. Stanford University Computer Science Technical Report, CSTR 2005-02.</p></li>
<li><p><strong>Osher, S., &amp; Fedkiw, R.</strong> (2003). <em>Level Set Methods and Dynamic Implicit Surfaces</em>. Springer.</p></li>
<li><p><strong>Park, J. J., Florence, P., Straub, J., Newcombe, R., &amp; Lovegrove, S.</strong> (2019). <em>DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation</em>. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 165–174.</p></li>
<li><p><strong>Park, K., Sinha, U., Barron, J. T., Bouaziz, S., Goldman, D. B., Seitz, S. M., &amp; Martin-Brualla, R.</strong> (2021). <em>Nerfies: Deformable Neural Radiance Fields</em>. IEEE/CVF International Conference on Computer Vision (ICCV), 5865–5874.</p></li>
<li><p><strong>Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., &amp; Bao, H.</strong> (2021). <em>Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies</em>. IEEE/CVF International Conference on Computer Vision (ICCV), 14314–14323.</p></li>
<li><p><strong>Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., &amp; Zhou, X.</strong> (2021). <em>Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans</em>. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 9054–9063.</p></li>
<li><p><strong>Pumarola, A., Corona, E., Pons-Moll, G., &amp; Moreno-Noguer, F.</strong> (2021). <em>D-NeRF: Neural Radiance Fields for Dynamic Scenes</em>. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 10318–10327.</p></li>
<li><p><strong>Qi, C. R., Su, H., Mo, K., &amp; Guibas, L. J.</strong> (2017). <em>PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</em>. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 652–660.</p></li>
<li><p><strong>Rahaman, N., Baratin, A., Arpit, D., Draxler, F., Lin, M., Hamprecht, F., Bengio, Y., &amp; Courville, A.</strong> (2019). <em>On the Spectral Bias of Neural Networks</em>. Proceedings of the 36th International Conference on Machine Learning (ICML), 5301–5310.</p></li>
<li><p><strong>Rusinkiewicz, S., &amp; Levoy, M.</strong> (2000). <em>QSplat: A Multiresolution Point Rendering System for Large Meshes</em>. Proceedings of SIGGRAPH 2000, 343–352.</p></li>
<li><p><strong>Saito, S., Huang, Z., Natsume, R., Morishima, S., Kanazawa, A., &amp; Li, H.</strong> (2019). <em>PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization</em>. IEEE/CVF International Conference on Computer Vision (ICCV), 2304–2314.</p></li>
<li><p><strong>Schönberger, J. L., &amp; Frahm, J.-M.</strong> (2016). <em>Structure-from-Motion Revisited</em>. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 4104–4113.</p></li>
<li><p><strong>Su, S.-Y., Yu, F., Zollhöfer, M., &amp; Rhodin, H.</strong> (2021). <em>A-NeRF: Articulated Neural Radiance Fields for Learning Human Shape, Appearance, and Pose</em>. Advances in Neural Information Processing Systems (NeurIPS), 34, 12278–12291.</p></li>
<li><p><strong>Szeliski, R.</strong> (2010). <em>Computer Vision: Algorithms and Applications</em>. Springer.</p></li>
<li><p><strong>Tancik, M., Srinivasan, P. P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Singhal, U., Ramamoorthi, R., Barron, J. T., &amp; Ng, R.</strong> (2020). <em>Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains</em>. Advances in Neural Information Processing Systems (NeurIPS), 33, 7537–7547.</p></li>
<li><p><strong>Tretschk, E., Tewari, A., Golyanik, V., Zollhöfer, M., Stoll, C., &amp; Theobalt, C.</strong> (2021). <em>Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Dynamic Scene From Monocular Video</em>. IEEE/CVF International Conference on Computer Vision (ICCV), 12959–12970.</p></li>
<li><p><strong>Wang, Z., Wu, S., Xie, W., Chen, M., &amp; Prisacariu, V. A.</strong> (2021). <em>NeRF–: Neural Radiance Fields Without Known Camera Parameters</em>. arXiv preprint arXiv:2102.07064.</p></li>
<li><p><strong>Weng, C.-Y., Curless, B., &amp; Kemelmacher-Shlizerman, I.</strong> (2022). <em>HumanNeRF: Free-Viewpoint Rendering of Moving People from Monocular Video</em>. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 16210–16220.</p></li>
<li><p><strong>Xu, P., Zhang, W., Chen, Y., Bao, L., Yang, J., &amp; Cui, Z.</strong> (2022). <em>Surface-Aligned Neural Radiance Fields for Controllable 3D Human Synthesis</em>. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 15750–15760.</p></li>
<li><p><strong>Yu, A., Fridovich-Keil, S., Tancik, M., Chen, Q., Recht, B., &amp; Kanazawa, A.</strong> (2021). <em>Plenoctrees for real-time rendering of neural radiance fields</em>. IEEE/CVF International Conference on Computer Vision (ICCV), 5752–5761.</p></li>
<li><p><strong>Yu, A., Li, R., Tancik, M., Li, H., Ng, R., &amp; Kanazawa, A.</strong> (2021). <em>PlenOctrees for Real-time Rendering of Neural Radiance Fields</em>. IEEE/CVF International Conference on Computer Vision (ICCV), 5752–5761.</p></li>
<li><p><strong>Zhang, K., Riegler, G., Snavely, N., &amp; Koltun, V.</strong> (2020). <em>NeRF++: Analyzing and Improving Neural Radiance Fields</em>. arXiv preprint arXiv:2010.07492.</p></li>
<li><p><strong>Zhu, W., Ma, X., Wang, Y., Li, H., &amp; Kong, W.</strong> (2023). <em>MotionBERT: Unified Pretraining for Human Motion Analysis</em>. IEEE/CVF International Conference on Computer Vision (ICCV).</p></li>
</ol>
</section>
<section id="gaussian-splatting">
<h2>Gaussian Splatting<a class="headerlink" href="#gaussian-splatting" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p>Kerbl, B., Kopanas, G., Leimkühler, T., &amp; Drettakis, G. (2023). <em>3D Gaussian Splatting for Real-Time Radiance Field Rendering</em>. ACM Transactions on Graphics (SIGGRAPH 2023), 42(4).
DOI: <a class="reference external" href="https://doi.org/10.1145/3592433">https://doi.org/10.1145/3592433</a>
GitHub: <a class="reference external" href="https://github.com/graphdeco-inria/gaussian-splatting">https://github.com/graphdeco-inria/gaussian-splatting</a></p></li>
<li><p>Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., &amp; Ng, R. (2020). <em>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</em>. ECCV 2020.
DOI: <a class="reference external" href="https://doi.org/10.48550/arXiv.2003.08934">https://doi.org/10.48550/arXiv.2003.08934</a>
GitHub: <a class="reference external" href="https://github.com/bmild/nerf">https://github.com/bmild/nerf</a></p></li>
<li><p>Barron, J. T., Mildenhall, B., Tancik, M., Hedman, P., Martin-Brualla, R., &amp; Srinivasan, P. P. (2021). <em>Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields</em>. ICCV 2021.
DOI: <a class="reference external" href="https://doi.org/10.48550/arXiv.2103.13415">https://doi.org/10.48550/arXiv.2103.13415</a>
GitHub: <a class="reference external" href="https://github.com/google-research/multinerf">https://github.com/google-research/multinerf</a></p></li>
<li><p>Barron, J. T., Mildenhall, B., Verbin, D., Srinivasan, P. P., &amp; Hedman, P. (2022). <em>Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields</em>. CVPR 2022.
DOI: <a class="reference external" href="https://doi.org/10.48550/arXiv.2111.12077">https://doi.org/10.48550/arXiv.2111.12077</a>
GitHub: <a class="reference external" href="https://github.com/google-research/multinerf">https://github.com/google-research/multinerf</a></p></li>
<li><p>Müller, T., Evans, A., Schied, C., &amp; Keller, A. (2022). <em>Instant Neural Graphics Primitives with a Multiresolution Hash Encoding</em>. ACM Transactions on Graphics, 41(4).
DOI: <a class="reference external" href="https://doi.org/10.1145/3528223.3530127">https://doi.org/10.1145/3528223.3530127</a>
GitHub: <a class="reference external" href="https://github.com/NVlabs/instant-ngp">https://github.com/NVlabs/instant-ngp</a></p></li>
<li><p>Fridovich-Keil, S., Yu, A., Tancik, M., Chen, Q., Recht, B., &amp; Kanazawa, A. (2022). <em>Plenoxels: Radiance Fields without Neural Networks</em>. CVPR 2022.
DOI: <a class="reference external" href="https://doi.org/10.48550/arXiv.2112.05131">https://doi.org/10.48550/arXiv.2112.05131</a>
GitHub: <a class="reference external" href="https://github.com/sxyu/svox2">https://github.com/sxyu/svox2</a></p></li>
<li><p>Pfister, H., Zwicker, M., van Baar, J., &amp; Gross, M. (2000). <em>Surfels: Surface Elements as Rendering Primitives</em>. SIGGRAPH 2000.
DOI: <a class="reference external" href="https://doi.org/10.1145/344779.344936">https://doi.org/10.1145/344779.344936</a>
URL: <a class="reference external" href="https://www.cs.umd.edu/~zwicker/publications/Surfels-SIG00.pdf">https://www.cs.umd.edu/~zwicker/publications/Surfels-SIG00.pdf</a></p></li>
<li><p>Zwicker, M., Pfister, H., van Baar, J., &amp; Gross, M. (2001). <em>Surface Splatting</em>. SIGGRAPH 2001.
DOI: <a class="reference external" href="https://doi.org/10.1145/383259.383300">https://doi.org/10.1145/383259.383300</a>
URL: <a class="reference external" href="https://dl.acm.org/doi/10.1145/383259.383300">https://dl.acm.org/doi/10.1145/383259.383300</a></p></li>
<li><p>Wang, Y., Liu, D., Cao, Y., Mu, Z., &amp; Zhang, H. (2019). <em>Differentiable Surface Splatting for Point-based Geometry Processing</em>. ACM Transactions on Graphics, 38(6).
DOI: <a class="reference external" href="https://doi.org/10.1145/3355089.3356513">https://doi.org/10.1145/3355089.3356513</a>
GitHub: <a class="reference external" href="https://github.com/yifita/DSS">https://github.com/yifita/DSS</a></p></li>
<li><p>Aliev, K. A., Sevastopolsky, A., Kolos, M., Ulyanov, D., &amp; Lempitsky, V. (2020). <em>Neural Point-Based Graphics</em>. ECCV 2020.
DOI: <a class="reference external" href="https://doi.org/10.1007/978-3-030-58542-6_42">https://doi.org/10.1007/978-3-030-58542-6_42</a>
GitHub: <a class="reference external" href="https://github.com/alievk/npbg">https://github.com/alievk/npbg</a></p></li>
<li><p>Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., &amp; Black, M. J. (2015). <em>SMPL: A Skinned Multi-Person Linear Model</em>. SIGGRAPH Asia 2015.
DOI: <a class="reference external" href="https://doi.org/10.1145/2816795.2818013">https://doi.org/10.1145/2816795.2818013</a>
Project Page: <a class="reference external" href="https://smpl.is.tue.mpg.de/">https://smpl.is.tue.mpg.de/</a></p></li>
<li><p>Kopanas, G., Philip, J., Leimkühler, T., &amp; Drettakis, G. (2021). <em>Point-Based Neural Rendering with Per-View Optimization</em>. Computer Graphics Forum (Eurographics 2021).
DOI: <a class="reference external" href="https://doi.org/10.1111/cgf.14339">https://doi.org/10.1111/cgf.14339</a>
GitHub: <a class="reference external" href="https://repo-sam.inria.fr/fungraph/differentiable-multi-view/">https://repo-sam.inria.fr/fungraph/differentiable-multi-view/</a></p></li>
<li><p>Rückert, D., Franke, L., &amp; Stamminger, M. (2022). <em>ADOP: Approximate Differentiable One-Pixel Point Rendering</em>. ACM Transactions on Graphics, 41(4).
DOI: <a class="reference external" href="https://doi.org/10.1145/3528223.3530122">https://doi.org/10.1145/3528223.3530122</a>
GitHub: <a class="reference external" href="https://github.com/darglein/ADOP">https://github.com/darglein/ADOP</a></p></li>
<li><p>Lassner, C., &amp; Zollhöfer, M. (2021). <em>Pulsar: Efficient Sphere-based Neural Rendering</em>. CVPR 2021.
DOI: <a class="reference external" href="https://doi.org/10.1109/CVPR46437.2021.01086">https://doi.org/10.1109/CVPR46437.2021.01086</a>
GitHub: <a class="reference external" href="https://github.com/facebookresearch/pytorch3d/blob/main/pytorch3d/renderer/points/pulsar.py">https://github.com/facebookresearch/pytorch3d/blob/main/pytorch3d/renderer/points/pulsar.py</a></p></li>
<li><p>Saito, S., Huang, Z., Natsume, R., Morishima, S., Kanazawa, A., &amp; Li, H. (2019). <em>PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization</em>. ICCV 2019.
DOI: <a class="reference external" href="https://doi.org/10.1109/ICCV.2019.00257">https://doi.org/10.1109/ICCV.2019.00257</a>
GitHub: <a class="reference external" href="https://github.com/shunsukesaito/PIFu">https://github.com/shunsukesaito/PIFu</a></p></li>
<li><p>Saito, S., Simon, T., Saragih, J., &amp; Joo, H. (2020). <em>PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization</em>. CVPR 2020.
DOI: <a class="reference external" href="https://doi.org/10.1109/CVPR42600.2020.00207">https://doi.org/10.1109/CVPR42600.2020.00207</a>
GitHub: <a class="reference external" href="https://github.com/facebookresearch/pifuhd">https://github.com/facebookresearch/pifuhd</a></p></li>
<li><p>Xiu, Y., Yang, J., Tzionas, D., &amp; Black, M. J. (2022). <em>ICON: Implicit Clothed humans Obtained from Normals</em>. CVPR 2022.
DOI: <a class="reference external" href="https://doi.org/10.1109/CVPR52688.2022.00401">https://doi.org/10.1109/CVPR52688.2022.00401</a>
GitHub: <a class="reference external" href="https://github.com/YuliangXiu/ICON">https://github.com/YuliangXiu/ICON</a></p></li>
<li><p>Snavely, N., Seitz, S. M., &amp; Szeliski, R. (2006). <em>Photo Tourism: Exploring Photo Collections in 3D</em>. SIGGRAPH 2006.
DOI: <a class="reference external" href="https://doi.org/10.1145/1141911.1141964">https://doi.org/10.1145/1141911.1141964</a>
Project Page: <a class="reference external" href="https://phototour.cs.washington.edu/">https://phototour.cs.washington.edu/</a></p></li>
<li><p>Mahmood, N., Ghorbani, N., Troje, N. F., Pons-Moll, G., &amp; Black, M. J. (2019). <em>AMASS: Archive of Motion Capture as Surface Shapes</em>. ICCV 2019.
DOI: <a class="reference external" href="https://doi.org/10.1109/ICCV.2019.00520">https://doi.org/10.1109/ICCV.2019.00520</a>
Project Page: <a class="reference external" href="http://amass.is.tue.mpg.de">http://amass.is.tue.mpg.de</a></p></li>
<li><p>Wu, G., Yi, T., Fang, J., Xie, L., Zhang, X., Wei, W., Liu, W., Tian, Q., &amp; Wang, X. (2024). <em>4D Gaussian Splatting for Real-Time Dynamic Scene Rendering</em>. CVPR 2024.
DOI: <a class="reference external" href="https://doi.org/10.48550/arXiv.2310.08579">https://doi.org/10.48550/arXiv.2310.08579</a>
GitHub: <a class="reference external" href="https://github.com/hustvl/4DGaussians">https://github.com/hustvl/4DGaussians</a></p></li>
<li><p>Levoy, M., &amp; Hanrahan, P. (1996). <em>Light Field Rendering</em>. SIGGRAPH 1996.
DOI: <a class="reference external" href="https://doi.org/10.1145/237170.237199">https://doi.org/10.1145/237170.237199</a>
URL: <a class="reference external" href="https://graphics.stanford.edu/papers/light/">https://graphics.stanford.edu/papers/light/</a></p></li>
<li><p>Gortler, S. J., Grzeszczuk, R., Szeliski, R., &amp; Cohen, M. F. (1996). <em>The Lumigraph</em>. SIGGRAPH 1996.
DOI: <a class="reference external" href="https://doi.org/10.1145/237170.237200">https://doi.org/10.1145/237170.237200</a>
URL: <a class="reference external" href="https://www.microsoft.com/en-us/research/publication/the-lumigraph/">https://www.microsoft.com/en-us/research/publication/the-lumigraph/</a></p></li>
<li><p>Bhatnagar, B. L., Tiwari, G., Theobalt, C., &amp; Pons-Moll, G. (2019). <em>Multi-Garment Net: Learning to Dress 3D People from Images</em>. ICCV 2019.
DOI: <a class="reference external" href="https://doi.org/10.1109/ICCV.2019.00543">https://doi.org/10.1109/ICCV.2019.00543</a>
GitHub: <a class="reference external" href="https://github.com/bharat-b7/MultiGarmentNetwork">https://github.com/bharat-b7/MultiGarmentNetwork</a></p></li>
<li><p>Yao, K., Wu, M., Dai, H., Tuytelaars, T., &amp; Yu, J. (2025). <em>BG-Triangle: Bézier Gaussian Triangle for 3D Vectorization and Rendering</em>. arXiv:2503.13961.</p></li>
<li><p>Wang, Z., Kanamori, Y., &amp; Endo, Y. (2024). <em>EG-HumanNeRF: Efficient Generalizable Human NeRF Utilizing Human Prior for Sparse View</em>. arXiv:2410.12242.</p></li>
<li><p>Zheng, S., Zhou, B., Shao, R., Liu, B., Zhang, S., Nie, L., &amp; Liu, Y. (2024). <em>GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for Real-time Human Novel View Synthesis</em>. CVPR 2024 (Highlight).</p></li>
<li><p>Kwon, Y., Fang, B., Lu, Y., Dong, H., Zhang, C., Carrasco, F. V., Mosella-Montoro, A., Xu, J., Takagi, S., Kim, D., Prakash, A., &amp; De la Torre, F. (2024). <em>Generalizable Human Gaussians for Sparse View Synthesis</em>. arXiv:2407.12777.</p></li>
<li><p>Yuan, Y., Shen, Q., Yang, X., &amp; Wang, X. (2025). <em>1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering</em>. arXiv:2503.16422.</p></li>
<li><p>Zhang, X., Liu, Z., Ge, X., He, D., Xu, T., Lin, Z., Yan, S., &amp; Zhang, J. (2024). <em>MEGA: Memory-Efficient 4D Gaussian Splatting for Dynamic Scenes</em>. arXiv:2309.17367 (ICLR 2025 submission).</p></li>
<li><p>Huang, B., Yu, Z., Chen, A., Geiger, A., &amp; Gao, S. (2024). <em>2D Gaussian Splatting for Geometrically Accurate Radiance Fields</em>. ACM SIGGRAPH 2024.</p></li>
<li><p>Li, Z., Zheng, Z., Wang, L., &amp; Liu, Y. (2024). <em>Animatable Gaussians: Learning Pose-Dependent Gaussian Maps for High-Fidelity Human Avatar Modeling</em>. CVPR 2024.</p></li>
<li><p>RenderPeople –
URL: <a class="reference external" href="https://www.renderpeople.com">https://www.renderpeople.com</a></p></li>
<li><p>Knapitsch, A., Park, J., Zhou, Q.-Y., &amp; Koltun, V. (2017). <em>Tanks and Temples: Benchmarking Large-Scale Scene Reconstruction</em>. ACM Transactions on Graphics, 36(4), 1–13.
DOI: <a class="reference external" href="https://doi.org/10.1145/3072959.3073599">https://doi.org/10.1145/3072959.3073599</a>
Dataset: <a class="reference external" href="https://tanksandtemples.org/">https://tanksandtemples.org/</a></p></li>
</ol>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="extendeed_materials_gaussian_splatting.html" class="btn btn-neutral float-left" title="3D Gaussian Splatting: A Basic Introduction" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>