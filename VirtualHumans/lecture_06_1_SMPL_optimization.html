

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Lecture 06.1 - Fitting the SMPL Model to Images via Optimization &mdash; Fitting SMPL to IMU Optimization</title>
      <link rel="stylesheet" type="text/css" href="/VirtualHumans/_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="/VirtualHumans/_static/css/theme.css?v=e59714d7" />

  
    <link rel="canonical" href="https://treese41528.github.io/VirtualHumans/lecture_06_1_SMPL_optimization.html" />
      <script src="/VirtualHumans/_static/jquery.js?v=5d32c60e"></script>
      <script src="/VirtualHumans/_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="/VirtualHumans/_static/documentation_options.js?v=f2a433a1"></script>
      <script src="/VirtualHumans/_static/doctools.js?v=9bcbadda"></script>
      <script src="/VirtualHumans/_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="/VirtualHumans/_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Lecture 06.2 - Learning-Based Fitting of the SMPL Model to Images" href="lecture_06_2SMPL_model_fitting.html" />
    <link rel="prev" title="Lecture 05.2 - 3D Registration: From Classical ICP to Modern Methods" href="lecture_05_2_3d_registration.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Virtual Humans Lecture
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="lecture_01_1_historical_body_models.html">Lecture 01.1 – Historical Body Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#early-origins-simplified-primitives-and-kinematic-skeletons-1970s1980s">Early Origins: Simplified Primitives and Kinematic Skeletons (1970s–1980s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#advances-in-the-1990s-superquadrics-differentiable-fitting-and-physical-models">Advances in the 1990s: Superquadrics, Differentiable Fitting, and Physical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#the-impact-of-3d-scanning-and-data-from-anthropometry-to-statistical-models-1990s2000s">The Impact of 3D Scanning and Data: From Anthropometry to Statistical Models (1990s–2000s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#scape-and-the-emergence-of-pose-aware-models-mid-2000s">SCAPE and the Emergence of Pose-Aware Models (Mid-2000s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#consolidation-in-the-2010s-smpl-and-integration-with-learning-based-methods">Consolidation in the 2010s: SMPL and Integration with Learning-Based Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#deep-learning-and-neural-implicit-models-late-2010spresent">Deep Learning and Neural Implicit Models (Late 2010s–Present)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#timeline-summary-of-milestones">Timeline Summary of Milestones</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html">Lecture 01.2 – Introduction to Human Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#historical-context-of-human-body-modeling">1. Historical Context of Human Body Modeling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#early-developments">Early Developments</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#mid-20th-century-approaches">Mid-20th Century Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#data-driven-revolution-1990s-2000s">Data-Driven Revolution (1990s-2000s)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#mathematical-foundations-of-human-body-models">2. Mathematical Foundations of Human Body Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#the-smpl-model">The SMPL Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#pca-based-statistical-shape-modeling">PCA-Based Statistical Shape Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#kinematic-modeling">Kinematic Modeling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#applications-of-human-body-models">3. Applications of Human Body Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computer-animation-and-visual-effects">Computer Animation and Visual Effects</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#virtual-humans-and-avatars">Virtual Humans and Avatars</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#biomechanics-and-ergonomics">Biomechanics and Ergonomics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#human-computer-interaction-hci">Human-Computer Interaction (HCI)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computer-vision-and-ai">Computer Vision and AI</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#education-and-training">Education and Training</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#challenges-and-future-directions">4. Challenges and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computational-efficiency">Computational Efficiency</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#accuracy-and-detail">Accuracy and Detail</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#generalization">Generalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#clothing-and-accessories">Clothing and Accessories</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#emerging-approaches">Emerging Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html">Lecture 01.3 – Introduction to Human Models (Overview)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#historical-context">1. Historical Context</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#early-scientific-studies">Early Scientific Studies</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#mid-20th-century-to-digital-era">Mid-20th Century to Digital Era</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#st-century-advances">21st Century Advances</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#mathematical-foundations">2. Mathematical Foundations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#parametric-body-models">Parametric Body Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-surface-representations">Implicit Surface Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#kinematic-modeling">Kinematic Modeling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#image-formation-and-rendering">3. Image Formation and Rendering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#camera-models">Camera Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#shading-and-visibility">Shading and Visibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#differentiable-rendering">Differentiable Rendering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#surface-representation-methods">4. Surface Representation Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#explicit-mesh-models">Explicit Mesh Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-function-models">Implicit Function Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#motion-capture-and-behavior-synthesis">5. Motion Capture and Behavior Synthesis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#capturing-human-motion">Capturing Human Motion</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#behavior-synthesis">Behavior Synthesis</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#clothing-modeling">6. Clothing Modeling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physically-based-simulation">Physically-Based Simulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#data-driven-approaches">Data-Driven Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-clothing-models">Implicit Clothing Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#human-object-interaction">7. Human-Object Interaction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physics-based-methods">Physics-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#learning-based-approaches">Learning-Based Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#hybrid-systems">Hybrid Systems</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#applications">8. Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#entertainment-and-media">Entertainment and Media</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#healthcare-and-biomechanics">Healthcare and Biomechanics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#engineering-and-design">Engineering and Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#human-computer-interaction">Human-Computer Interaction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#scientific-research">Scientific Research</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#challenges-and-future-directions">9. Challenges and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#scalability-and-generalization">Scalability and Generalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#higher-fidelity-dynamics">Higher-Fidelity Dynamics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#data-and-labeling-constraints">Data and Labeling Constraints</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physics-and-learning-integration">Physics and Learning Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#semantic-and-cognitive-aspects">Semantic and Cognitive Aspects</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#realism-vs-controllability">Realism vs. Controllability</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_02_1_image_formation.html">Lecture 02.1 – Image Formation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#historical-developments-in-image-formation">1. Historical Developments in Image Formation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#ancient-and-medieval-optics-camera-obscura">Ancient and Medieval Optics – Camera Obscura</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#renaissance-perspective-and-geometry">Renaissance Perspective and Geometry</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#early-cameras-and-photographic-imaging">Early Cameras and Photographic Imaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#modern-developments">Modern Developments</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#the-pinhole-camera-model">2. The Pinhole Camera Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#coordinate-setup">Coordinate Setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#proof-by-similar-triangles">Proof by Similar Triangles</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#numerical-example">Numerical Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#inadequacy-of-a-simple-pinhole">Inadequacy of a Simple Pinhole</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#camera-intrinsics-and-the-projection-matrix">3. Camera Intrinsics and the Projection Matrix</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#extrinsic-parameters">Extrinsic Parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#full-projection-example">Full Projection Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#image-distortions-correction">4. Image Distortions &amp; Correction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#properties-of-perspective-projection">5. Properties of Perspective Projection</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#advanced-theoretical-extensions">6. Advanced Theoretical Extensions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#light-field-imaging-and-plenoptic-cameras">Light Field Imaging and Plenoptic Cameras</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#non-conventional-imaging-techniques">Non-Conventional Imaging Techniques</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#applications-in-modern-vision-and-graphics">7. Applications in Modern Vision and Graphics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#computer-vision-and-3d-reconstruction">Computer Vision and 3D Reconstruction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#medical-imaging">Medical Imaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#photorealistic-rendering-in-computer-graphics">Photorealistic Rendering in Computer Graphics</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#python-example-simulating-image-formation">8. Python Example: Simulating Image Formation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html">Lecture 02.2 – Rotations and Kinematic Chains</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#representations-of-3d-rotations">1. Representations of 3D Rotations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#a-rotation-matrices">A) Rotation Matrices</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#b-euler-angles">B) Euler Angles</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#c-quaternions">C) Quaternions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#lie-algebra-so-3-and-exponential-map">2. Lie Algebra <span class="math notranslate nohighlight">\(so(3)\)</span> and Exponential Map</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#rodrigues-rotation-formula">3. Rodrigues’ Rotation Formula</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#kinematic-chains-forward-inverse-kinematics">4. Kinematic Chains: Forward &amp; Inverse Kinematics</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#comparison-of-rotation-representations">Comparison of Rotation Representations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_03_1_surface_representations.html">Lecture 03.1 – Surface Representations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#mathematical-foundations-of-surface-representations">1. Mathematical Foundations of Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-parametric-surfaces">A) Parametric Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-implicit-surfaces">B) Implicit Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-explicit-surfaces">C) Explicit Surfaces</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#surface-differential-properties">2. Surface Differential Properties</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-surface-normals">A) Surface Normals</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-fundamental-forms-and-curvature">B) Fundamental Forms and Curvature</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-geodesics">C) Geodesics</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#discrete-surface-representations">3. Discrete Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-polygon-meshes">A) Polygon Meshes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-point-clouds">B) Point Clouds</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-signed-distance-fields-sdf">C) Signed Distance Fields (SDF)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#advanced-surface-representations">4. Advanced Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-bezier-curves-and-surfaces">A) Bézier Curves and Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-b-splines-and-nurbs">B) B-Splines and NURBS</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-subdivision-surfaces">C) Subdivision Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-level-sets">D) Level Sets</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#e-neural-implicit-representations">E) Neural Implicit Representations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#comparative-analysis-and-applications">5. Comparative Analysis and Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-computational-efficiency-and-storage">A) Computational Efficiency and Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-practical-applications">B) Practical Applications</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-operations-complexity">C) Operations Complexity</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#implementation-examples">6. Implementation Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-basic-mesh-processing-python">A) Basic Mesh Processing (Python)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-implicit-surface-utilities">B) Implicit Surface Utilities</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-bezier-curve-implementation">C) Bézier Curve Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-curvature-estimation-on-meshes">D) Curvature Estimation on Meshes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#advanced-topics-and-future-directions">7. Advanced Topics and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-multi-resolution-representations">A) Multi-Resolution Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-machine-learning-for-geometry">B) Machine Learning for Geometry</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-dynamic-surfaces">C) Dynamic Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-non-manifold-geometries">D) Non-Manifold Geometries</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html">Lecture 03.2 – Procrustes Alignment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#goal-learning-a-model-of-pose-and-shape">Goal: Learning a Model of Pose and Shape</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#the-challenge-of-registration">The Challenge of Registration</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#surface-representation-mesh">Surface Representation: Mesh</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#the-procrustes-alignment-problem-mathematical-formulation">The Procrustes Alignment Problem: Mathematical Formulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#rigid-transformations">Rigid Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#procrustes-alignment-solution">Procrustes Alignment Solution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#decoupling-translation-by-centroid-alignment">Decoupling Translation by Centroid Alignment</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#optimal-rotation-via-svd">Optimal Rotation via SVD</a><ul>
<li class="toctree-l4"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#reflection-adjustment">Reflection Adjustment</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#optimal-scale-optional">Optimal Scale (Optional)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#complete-mathematical-derivation">Complete Mathematical Derivation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#translation-derivation">Translation Derivation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#rotation-derivation">Rotation Derivation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#scale-derivation">Scale Derivation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#summary-of-procrustes-alignment-algorithm">Summary of Procrustes Alignment Algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#python-implementation-example">Python Implementation Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#practical-applications">Practical Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#interactive-visualization-ideas">Interactive Visualization Ideas</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_04_1_icp.html">Lecture 4.1: Iterative Closest Point</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#introduction-to-shape-alignment-and-registration">Introduction to Shape Alignment and Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#the-registration-problem">The Registration Problem</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#review-procrustes-analysis">Review: Procrustes Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#problem-unknown-correspondences">Problem: Unknown Correspondences</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#the-iterative-closest-point-icp-algorithm">The Iterative Closest Point (ICP) Algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#basic-icp-algorithm">Basic ICP Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#computational-considerations">Computational Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="lecture_04_1_icp.html#closest-point-computation">Closest Point Computation</a></li>
<li class="toctree-l4"><a class="reference internal" href="lecture_04_1_icp.html#convergence-and-local-minima">Convergence and Local Minima</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#point-to-point-vs-point-to-plane-icp">Point-to-Point vs. Point-to-Plane ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#point-to-point-icp">Point-to-Point ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#point-to-plane-icp">Point-to-Plane ICP</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#gradient-based-icp-for-non-rigid-registration">Gradient-based ICP for Non-Rigid Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#gradient-based-icp-algorithm">Gradient-based ICP Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#advantages-of-gradient-based-icp">Advantages of Gradient-based ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#computing-gradients">Computing Gradients</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#improving-icp-s-robustness">Improving ICP’s Robustness</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#data-association-direction">Data Association Direction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#robust-cost-functions">Robust Cost Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#trimmed-icp">Trimmed ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#ransac-based-approaches">RANSAC-based Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#additional-information">Additional Information</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#point-to-surface-distance">Point-to-Surface Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#icp-variants-and-extensions">ICP Variants and Extensions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#generalized-icp-gicp">Generalized ICP (GICP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#em-icp-and-probabilistic-approaches">EM-ICP and Probabilistic Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#coherent-point-drift-cpd">Coherent Point Drift (CPD)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#multi-scale-approaches">Multi-Scale Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#applications-of-icp">Applications of ICP</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#implementing-icp">Implementing ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#efficient-python-implementation">Efficient Python Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#practical-tips">Practical Tips</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_04_2_body_models.html">Lecture 04.2 - Body Models: Vertex-Based Models and SMPL</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#body-models-as-parameterized-functions">1. Body Models as Parameterized Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#rotations-articulation-and-pose-representation">2. Rotations, Articulation, and Pose Representation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#rotation-representation">2.1 Rotation Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#kinematic-chain">2.2 Kinematic Chain</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#linear-blend-skinning-and-its-limitations">3. Linear Blend Skinning and its Limitations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#linear-blend-skinning">3.1 Linear Blend Skinning</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#problems-with-standard-lbs">3.2 Problems with Standard LBS</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#blend-shapes-for-correcting-lbs-artifacts">3.3 Blend Shapes for Correcting LBS Artifacts</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#the-smpl-body-model">4. The SMPL Body Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#smpl-philosophy">4.1 SMPL Philosophy</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#smpl-model-architecture">4.2 SMPL Model Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#shape-blend-shapes">4.2.1 Shape Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#pose-blend-shapes">4.2.2 Pose Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#joint-regression">4.2.3 Joint Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#model-training">4.3 Model Training</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#comparison-with-scape">5. Comparison with SCAPE</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#the-scape-model">5.1 The SCAPE Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#different-approaches-to-deformation">5.2 Different Approaches to Deformation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#performance-comparison">5.3 Performance Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#other-advantages">5.4 Other Advantages</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#alignment-techniques-procrustes-analysis-and-icp">6. Alignment Techniques: Procrustes Analysis and ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#procrustes-analysis">6.1 Procrustes Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#iterative-closest-point-icp">6.2 Iterative Closest Point (ICP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#fitting-smpl-to-scans">6.3 Fitting SMPL to Scans</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#image-formation-and-the-pinhole-camera-model">7. Image Formation and the Pinhole Camera Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#the-pinhole-camera-model">7.1 The Pinhole Camera Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#lens-distortion">7.2 Lens Distortion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#extensions-and-advanced-applications">8. Extensions and Advanced Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#dynamic-soft-tissue-modeling">8.1 Dynamic Soft Tissue Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#specialized-extensions">8.2 Specialized Extensions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#deep-learning-for-model-fitting">8.3 Deep Learning for Model Fitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#probabilistic-approaches">8.4 Probabilistic Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#hybrid-models">8.5 Hybrid Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_05_1_body_model_training.html">Lecture 5.1 - Training a Body Model and Fitting SMPL to Scans</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#body-models-based-on-triangle-deformations">Body Models Based on Triangle Deformations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#scape-and-blendscape-models">SCAPE and BlendSCAPE Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#triangle-deformation-process">Triangle Deformation Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#comparison-smpl-vs-scape-blendscape">Comparison: SMPL vs. SCAPE/BlendSCAPE</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#training-a-body-model-from-registrations">Training a Body Model from Registrations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#the-challenge-of-raw-scan-data">The Challenge of Raw Scan Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#training-from-registrations">Training from Registrations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#obtaining-registrations-fitting-smpl-to-scans">Obtaining Registrations: Fitting SMPL to Scans</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#non-rigid-registration-process">Non-Rigid Registration Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#iterative-closest-point-icp-review">Iterative Closest Point (ICP) Review</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#registration-objective-formulation">Registration Objective Formulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#point-to-surface-distance">Point-to-Surface Distance</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#multi-stage-optimization-strategy">Multi-Stage Optimization Strategy</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#joint-registration-and-model-training">Joint Registration and Model Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#co-registration-approach">Co-Registration Approach</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_05_2_3d_registration.html">Lecture 05.2 - 3D Registration: From Classical ICP to Modern Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#rigid-registration-and-the-icp-algorithm">1. Rigid Registration and the ICP Algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#the-iterative-closest-point-icp-algorithm">The Iterative Closest Point (ICP) Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#convergence-analysis-and-failure-modes">Convergence Analysis and Failure Modes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#classical-non-rigid-registration">2. Classical Non-Rigid Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#thin-plate-spline-robust-point-matching-tps-rpm">Thin Plate Spline Robust Point Matching (TPS-RPM)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#coherent-point-drift-cpd">Coherent Point Drift (CPD)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#other-non-rigid-methods">Other Non-Rigid Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#parametric-models-and-the-smpl-body-model">3. Parametric Models and the SMPL Body Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#model-structure">Model Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#shape-blend-shapes-identity-variation">Shape Blend Shapes (Identity Variation)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#pose-blend-shapes-pose-dependent-deformation">Pose Blend Shapes (Pose-Dependent Deformation)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#linear-blend-skinning-lbs-for-articulation">Linear Blend Skinning (LBS) for Articulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#learning-smpl">Learning SMPL</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#using-smpl-for-registration">Using SMPL for Registration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#modeling-clothing-and-fine-details-smpl-d">4. Modeling Clothing and Fine Details: SMPL+D</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#why-smpl-d">Why SMPL+D?</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#how-displacements-are-applied">How Displacements Are Applied</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#limitations">Limitations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#applications">Applications</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#survey-of-3d-registration-methods-from-icp-to-deep-learning">5. Survey of 3D Registration Methods: From ICP to Deep Learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#early-pioneering-works-1990s-foundational-rigid-registration">5.1 Early Pioneering Works (1990s) – Foundational Rigid Registration</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#the-2000s-robust-and-non-rigid-registration-emerges">5.2 The 2000s – Robust and Non-Rigid Registration Emerges</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#s-template-based-and-parametric-model-registration">5.3 2010s – Template-based and Parametric Model Registration</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#s-learning-based-parametric-registration-and-hybrid-approaches">5.4 2020s – Learning-Based Parametric Registration and Hybrid Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Lecture 06.1 - Fitting the SMPL Model to Images via Optimization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#mathematical-background-pinhole-camera-and-projections">Mathematical Background: Pinhole Camera and Projections</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#perspective-projection">Perspective Projection</a></li>
<li class="toctree-l3"><a class="reference internal" href="#weak-perspective-projection">Weak-Perspective Projection</a></li>
<li class="toctree-l3"><a class="reference internal" href="#camera-extrinsics-vs-model-pose">Camera Extrinsics vs. Model Pose</a></li>
<li class="toctree-l3"><a class="reference internal" href="#d-keypoints-and-projection">2D Keypoints and Projection</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#the-smpl-model-as-a-differentiable-function-of-shape-and-pose">The SMPL Model as a Differentiable Function of Shape and Pose</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#shape-blend-shapes">Shape Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pose-blend-shapes">Pose Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#joint-positions">Joint Positions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#linear-blend-skinning">Linear Blend Skinning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#differentiability-of-smpl">Differentiability of SMPL</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#fitting-smpl-to-images-via-optimization-smplify">Fitting SMPL to Images via Optimization (SMPLify)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#objective-function">Objective Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="#combined-objective">Combined Objective</a></li>
<li class="toctree-l3"><a class="reference internal" href="#optimization-strategy">Optimization Strategy</a></li>
<li class="toctree-l3"><a class="reference internal" href="#optimization-algorithms">Optimization Algorithms</a></li>
<li class="toctree-l3"><a class="reference internal" href="#automatic-differentiation-and-jacobians">Automatic Differentiation and Jacobians</a></li>
<li class="toctree-l3"><a class="reference internal" href="#result-of-smplify">Result of SMPLify</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#historical-progression-and-method-comparisons">Historical Progression and Method Comparisons</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#smplify-bogo-et-al-2016">SMPLify (Bogo et al. 2016)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#smplify-x-2019">SMPLify-X (2019)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html">Lecture 06.2 - Learning-Based Fitting of the SMPL Model to Images</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#foundations-of-learning-based-smpl-estimation">Foundations of Learning-Based SMPL Estimation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#integrating-smpl-into-neural-networks">Integrating SMPL into Neural Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#projection-functions">Projection Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#loss-functions">Loss Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#statistical-priors-and-adversarial-losses">Statistical Priors and Adversarial Losses</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#early-regression-approaches-hmr-and-nbf">Early Regression Approaches: HMR and NBF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#human-mesh-recovery-hmr">Human Mesh Recovery (HMR)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#neural-body-fitting-nbf">Neural Body Fitting (NBF)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#evolving-architectures-hybrid-and-improved-regression-methods">Evolving Architectures: Hybrid and Improved Regression Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#spin-optimization-in-the-training-loop">SPIN: Optimization in the Training Loop</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#pymaf-pyramidal-mesh-alignment-feedback">PyMAF: Pyramidal Mesh Alignment Feedback</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#cliff-using-full-frame-context-for-camera-orientation">CLIFF: Using Full-Frame Context for Camera Orientation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#pixie-whole-body-regression-with-part-experts">PIXIE: Whole-Body Regression with Part Experts</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#temporal-methods-from-single-images-to-video-sequences">Temporal Methods: From Single Images to Video Sequences</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#vibe-adversarial-motion-prior-with-grus">VIBE: Adversarial Motion Prior with GRUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#tcmr-temporally-consistent-mesh-recovery">TCMR: Temporally Consistent Mesh Recovery</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#motionbert-transformer-based-motion-representations">MotionBERT: Transformer-Based Motion Representations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#comparison-of-learning-based-methods">Comparison of Learning-Based Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#supervision-and-data">Supervision and Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#network-architecture">Network Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#objective-functions">Objective Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#pose-and-shape-priors">Pose and Shape Priors</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#performance">Performance</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#runtime">Runtime</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#strengths-and-weaknesses">Strengths and Weaknesses</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html">Lecture 07.1: Fitting SMPL to IMU Data Using Optimization-Based Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#classical-imu-based-pose-estimation-a-historical-perspective">Classical IMU-Based Pose Estimation: A Historical Perspective</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#attitude-and-heading-reference-systems">Attitude and Heading Reference Systems</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#kalman-filter-approaches">Kalman Filter Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#early-sparse-sensor-approaches">Early Sparse-Sensor Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#model-based-optimization-methods">Model-Based Optimization Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#learning-based-methods">Learning-Based Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#inertial-sensor-fundamentals-and-orientation-representations">Inertial Sensor Fundamentals and Orientation Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#gravity-alignment-and-drift-correction">Gravity Alignment and Drift Correction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#orientation-representations">Orientation Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#sensor-calibration">Sensor Calibration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#optimization-based-smpl-fitting-with-imu-data">Optimization-Based SMPL Fitting with IMU Data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#kinematic-model-and-sensor-prediction">Kinematic Model and Sensor Prediction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#regularization-and-prior-terms">Regularization and Prior Terms</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#gradient-and-jacobian-computation">Gradient and Jacobian Computation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#pseudocode-smpl-pose-estimation-from-imu-sequence">Pseudocode: SMPL Pose Estimation from IMU Sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#imu-based-human-pose-datasets-and-resources">IMU-Based Human Pose Datasets and Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#dip-imu-dataset-2018">DIP-IMU Dataset (2018)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#totalcapture-2017">TotalCapture (2017)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#amass-2019">AMASS (2019)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#other-datasets-and-resources">Other Datasets and Resources</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html">Lecture 07.2: Fitting SMPL to IMU Data Using Learning-Based Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#optimization-based-vs-learning-based-approaches">Optimization-Based vs. Learning-Based Approaches</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#learning-based-imu-to-pose-estimation-historical-overview-of-key-models">Learning-Based IMU-to-Pose Estimation: Historical Overview of Key Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#deep-inertial-poser-dip-2018">Deep Inertial Poser (DIP, 2018)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#transpose-2021">TransPose (2021)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#transformer-inertial-poser-tip-2022">Transformer Inertial Poser (TIP, 2022)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#physics-physical-inertial-poser-pip-2022">Physics/Physical Inertial Poser (PIP, 2022)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#other-notable-models-and-developments">Other Notable Models and Developments</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#problem-formulation-and-learning-task-definition">Problem Formulation and Learning Task Definition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#input-and-output-representations">Input and Output Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#learning-objective-and-loss-functions">Learning Objective and Loss Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#temporal-modeling-approaches">Temporal Modeling Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#supervised-vs-semi-supervised-training-synthetic-data">Supervised vs. Semi-Supervised Training; Synthetic Data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#model-architectures-and-design-considerations">Model Architectures and Design Considerations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#encoding-imu-measurements">Encoding IMU Measurements</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#network-structures">Network Structures</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#training-pipeline-and-pseudocode">Training Pipeline and Pseudocode</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#datasets-benchmarks-and-resources">Datasets, Benchmarks, and Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#dip-imu-dataset-2018">DIP-IMU Dataset (2018)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#totalcapture-2017">TotalCapture (2017)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#amass-2019">AMASS (2019)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#other-datasets-and-resources">Other Datasets and Resources</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#challenges-and-outlook">Challenges and Outlook</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html">Lecture 08.1: Vertex-Based Clothing Modeling for Virtual Humans</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#clothing-representation-as-vertex-displacements">Clothing Representation as Vertex Displacements</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#registration-of-clothed-human-scans">Registration of Clothed Human Scans</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#estimating-body-shape-under-clothing-the-buff-method">Estimating Body Shape Under Clothing: The BUFF Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#multi-layer-clothing-capture-the-clothcap-method">Multi-Layer Clothing Capture: The ClothCap Method</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#data-term">Data Term</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#boundary-term">Boundary Term</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#boundary-smoothness">Boundary Smoothness</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#laplacian-smoothness">Laplacian Smoothness</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#applications-of-multi-layer-registration">Applications of Multi-Layer Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#advantages-and-limitations">Advantages and Limitations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html">Lecture 09.1: Neural Implicit and Point-Based Representations for Clothed Human Modeling</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#background-explicit-vs-implicit-vs-point-based-representations">Background: Explicit vs. Implicit vs. Point-Based Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#mesh-based-models">Mesh-Based Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#point-based-models">Point-Based Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#neural-implicit-representations">Neural Implicit Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#neural-radiance-fields-nerfs-for-humans">Neural Radiance Fields (NeRFs) for Humans</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#hybrid-approaches">Hybrid Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#expressiveness-and-topology">Expressiveness and Topology</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#differentiability-and-learning">Differentiability and Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#data-efficiency-and-performance">Data Efficiency and Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#neural-implicit-function-foundations">Neural Implicit Function Foundations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#signed-distance-field-sdf">Signed Distance Field (SDF)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#occupancy-field-indicator-function">Occupancy Field (Indicator Function)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#volume-radiance-field">Volume Radiance Field</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#articulated-deformation-fields-for-implicit-models">Articulated Deformation Fields for Implicit Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#backward-warping-inverse-skinning-field">Backward Warping (Inverse Skinning Field)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#forward-warping-forward-skinning-field">Forward Warping (Forward Skinning Field)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#pose-dependent-deformation-secondary-motion">Pose-Dependent Deformation (Secondary Motion)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#generative-implicit-models-for-clothed-bodies">Generative Implicit Models for Clothed Bodies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#smplicit-topology-aware-clothed-human-model">SMPLicit: Topology-Aware Clothed Human Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#imghum-implicit-generative-human-model">imGHUM: Implicit Generative Human Model</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#pose-dependent-implicit-models-and-animatable-avatars">Pose-Dependent Implicit Models and Animatable Avatars</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#nasa-neural-articulated-shape-approximation-eccv-2020">NASA: Neural Articulated Shape Approximation (ECCV 2020)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#scanimate-weakly-supervised-skinned-avatar-networks-cvpr-2021">SCANimate: Weakly-Supervised Skinned Avatar Networks (CVPR 2021)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#neural-gif-neural-generalized-implicit-functions-iccv-2021">Neural-GIF: Neural Generalized Implicit Functions (ICCV 2021)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#snarf-skinned-neural-articulated-implicit-shapes-iccv-2021">SNARF: Skinned Neural Articulated Implicit Shapes (ICCV 2021)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#pop-the-power-of-points-iccv-2021-point-based-modeling">POP: The Power of Points (ICCV 2021) – Point-Based Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#other-noteworthy-methods">Other Noteworthy Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#blueprint-algorithms-for-key-methods">Blueprint Algorithms for Key Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#snarf-training-procedure-differentiable-forward-skinning-for-implicit-surfaces">SNARF (Training Procedure): Differentiable Forward Skinning for Implicit Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#pop-training-fitting-pipeline-point-based-model-for-pose-dependent-clothing">POP (Training &amp; Fitting Pipeline): Point-Based Model for Pose-Dependent Clothing</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#smplicit-inference-fitting-workflow-generative-implicit-garment-model-conditioned-on-smpl">SMPLicit (Inference/Fitting Workflow): Generative Implicit Garment Model conditioned on SMPL</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#historical-perspective-and-future-outlook">Historical Perspective and Future Outlook</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#challenges-and-future-directions">Challenges and Future Directions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="extended_materials_neural_radiance_fields.html">Neural Radiance Fields: A Historical and Theoretical Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#foundations-3d-scene-representation-and-reconstruction-techniques">Foundations: 3D Scene Representation and Reconstruction Techniques</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#voxel-grids">Voxel Grids</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#point-clouds">Point Clouds</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#mesh-based-surfaces">Mesh-Based Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#light-fields-and-volumetric-rendering">Light Fields and Volumetric Rendering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#emergence-of-neural-radiance-fields-nerf">Emergence of Neural Radiance Fields (NeRF)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#core-idea">Core Idea</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#training-procedure">Training Procedure</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#nerf-architecture">NeRF Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#hierarchical-sampling">Hierarchical Sampling</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#original-results">Original Results</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#theoretical-and-mathematical-analysis-of-nerf">Theoretical and Mathematical Analysis of NeRF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#volume-rendering-formulation-in-nerf">Volume Rendering Formulation in NeRF</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#positional-encoding-and-neural-network-architecture">Positional Encoding and Neural Network Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#loss-function-and-optimization">Loss Function and Optimization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#major-advancements-and-extensions-of-nerf">Major Advancements and Extensions of NeRF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#anti-aliasing-and-unbounded-scenes-mip-nerf-and-nerf">Anti-Aliasing and Unbounded Scenes: mip-NeRF and NeRF++</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#efficiency-improvements-instant-nerf-and-plenoctrees">Efficiency Improvements: Instant NeRF and PlenOctrees</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#dynamic-and-deformable-nerfs-d-nerf-nerfies-nsff-etc">Dynamic and Deformable NeRFs (D-NeRF, Nerfies, NSFF, etc.)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#neural-radiance-fields-for-human-modeling-with-smpl-and-body-models">Neural Radiance Fields for Human Modeling (with SMPL and Body Models)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#other-notable-extensions">Other Notable Extensions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#comparison-with-other-3d-representations">Comparison with Other 3D Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-polygonal-meshes">Vs. Polygonal Meshes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-point-clouds-3d-splatting">Vs. Point Clouds / 3D Splatting</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-voxel-grids-and-volumetric-methods">Vs. Voxel Grids and Volumetric Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-multi-plane-images-mpis-light-fields">Vs. Multi-Plane Images (MPIs) / Light Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#accuracy-and-fidelity">Accuracy and Fidelity</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#applicability">Applicability</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#datasets-for-nerf-training-and-evaluation">Datasets for NeRF Training and Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#blender-synthetic-nerf-dataset">Blender Synthetic NeRF Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#local-light-field-fusion-llff-real-forward-facing-dataset">Local Light Field Fusion (LLFF) Real Forward-Facing Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#tanks-and-temples">Tanks and Temples</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#dtu-dataset">DTU Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#human3-6m">Human3.6M</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#zju-mocap-dataset">ZJU-MoCap Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#people-snapshot">People-Snapshot</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#synthetic-dynamic-scenes">Synthetic dynamic scenes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#other-datasets">Other datasets</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html">3D Gaussian Splatting: A Basic Introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#foundations">Foundations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#what-is-a-3d-scene">What is a 3D Scene?</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-scene-representation">3D Scene Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#computer-graphics-fundamentals">Computer Graphics Fundamentals</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#rasterization-and-ray-tracing">Rasterization and Ray Tracing</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#alpha-blending-and-compositing">Alpha Blending and Compositing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#the-evolution-of-novel-view-synthesis">The Evolution of Novel View Synthesis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#image-based-rendering">Image-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#structure-from-motion-and-multi-view-stereo">Structure-from-Motion and Multi-View Stereo</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#point-based-rendering">Point-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#neural-rendering">Neural Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#accelerated-neural-fields">Accelerated Neural Fields</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussian-splatting-a-convergence-of-approaches">3D Gaussian Splatting: A Convergence of Approaches</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#id1">Point-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#point-clouds-and-their-challenges">Point Clouds and Their Challenges</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#the-concept-of-splatting">The Concept of Splatting</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#elliptical-weighted-average-ewa-filtering">Elliptical Weighted Average (EWA) Filtering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-point-based-rendering">Differentiable Point-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussian-splatting-core-principles">3D Gaussian Splatting: Core Principles</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#key-insight-unifying-points-and-volumes">Key Insight: Unifying Points and Volumes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussians-as-scene-primitives">3D Gaussians as Scene Primitives</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#the-volumetric-rendering-equation">The Volumetric Rendering Equation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#alpha-compositing-with-gaussians">Alpha Compositing with Gaussians</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#mathematical-formulation-of-3d-gaussian-splatting">Mathematical Formulation of 3D Gaussian Splatting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#projecting-3d-gaussians-to-2d">Projecting 3D Gaussians to 2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#parameterization-of-3d-gaussians">Parameterization of 3D Gaussians</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#view-dependent-appearance">View-Dependent Appearance</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-rendering-equations">Differentiable Rendering Equations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#training-and-optimization">Training and Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#photometric-loss">Photometric Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#initial-point-cloud">Initial Point Cloud</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#optimization-process">Optimization Process</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-splatting-pipeline">Differentiable Splatting Pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#adaptive-density-control">Adaptive Density Control</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#implementation-and-real-time-rendering">Implementation and Real-Time Rendering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#tile-based-rendering">Tile-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#fast-sorting-strategies">Fast Sorting Strategies</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#gpu-accelerated-rasterization">GPU-Accelerated Rasterization</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#memory-considerations">Memory Considerations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#comparison-with-other-methods">Comparison with Other Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#nerf-vs-3d-gaussian-splatting">NeRF vs. 3D Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#voxel-based-representations-vs-gaussians">Voxel-Based Representations vs. Gaussians</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#traditional-point-based-rendering-vs-gaussian-splatting">Traditional Point-Based Rendering vs. Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#applications-and-extensions">Applications and Extensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#static-scene-reconstruction">Static Scene Reconstruction</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#dynamic-scene-capture">Dynamic Scene Capture</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#avatar-creation-and-animation">Avatar Creation and Animation</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#integration-with-neural-rendering">Integration with Neural Rendering</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#large-scale-scene-rendering">Large-Scale Scene Rendering</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#bezier-gaussian-triangles-bg-triangle-for-sharper-rendering">Bézier Gaussian Triangles (BG-Triangle) for Sharper Rendering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#representation">Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#performance">Performance</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#human-reconstruction-with-gaussian-splatting-and-priors">Human Reconstruction with Gaussian Splatting and Priors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#eg-humannerf-efficient-generalizable-human-nerf">EG-HumanNeRF: Efficient Generalizable Human NeRF</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#gps-gaussian-pixel-wise-gaussian-splatting-for-humans">GPS-Gaussian: Pixel-Wise Gaussian Splatting for Humans</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#generalizable-human-gaussians-ghg-with-smpl">Generalizable Human Gaussians (GHG) with SMPL</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#dynamic-scene-reconstruction-with-4d-gaussian-splatting">Dynamic Scene Reconstruction with 4D Gaussian Splatting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussian-splatting-4dgs">4D Gaussian Splatting (4DGS)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#speed-and-memory-enhancements-4dgs-1k-and-mega">Speed and Memory Enhancements (4DGS-1K and MEGA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#applications-to-mocap-and-4d-human-rendering">Applications to MoCap and 4D Human Rendering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#implementation-details-and-real-time-performance">Implementation Details and Real-Time Performance</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#data-structures">Data Structures</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#rasterization-shaders">Rasterization &amp; Shaders</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#gpu-memory-and-throughput">GPU Memory and Throughput</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-rendering-implementation">Differentiable Rendering Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#training-vs-inference-compute">Training vs. Inference Compute</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#accuracy-vs-speed-trade-offs">Accuracy vs. Speed trade-offs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#benchmarks-and-comparative-evaluation">Benchmarks and Comparative Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#static-scene-comparison">Static Scene Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#human-novel-view-comparison">Human Novel-View Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#dynamic-scene-comparison">Dynamic Scene Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#comparative-summary-table">Comparative Summary Table</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#datasets-and-resources">Datasets and Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#synthetic-nerf-dataset-blender-scenes">Synthetic NeRF Dataset (Blender Scenes)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#llff-local-light-field-fusion">LLFF (Local Light Field Fusion)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#tanks-and-temples">Tanks and Temples</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#multi-object-360-co3d-common-objects-in-3d">Multi-Object 360 (CO3D - Common Objects in 3D)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#amass-archive-of-motion-capture-as-surface-shapes">AMASS (Archive of Motion Capture as Surface Shapes)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#cape-clothed-auto-person-encoding">CAPE (Clothed Auto Person Encoding)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#thuman-thuman2-0">THuman / THuman2.0</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#renderpeople">RenderPeople</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#open-source-implementations">Open-Source Implementations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#future-directions">Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#improved-compression-techniques-for-memory-efficiency">Improved Compression Techniques for Memory Efficiency</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#handling-dynamic-and-deformable-scenes">Handling Dynamic and Deformable Scenes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#advanced-material-modeling-for-realistic-rendering">Advanced Material Modeling for Realistic Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#hybrid-approaches-integrating-neural-fields-and-explicit-representations">Hybrid Approaches Integrating Neural Fields and Explicit Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#scalability-for-large-scale-scenes-city-level-and-beyond">Scalability for Large-Scale Scenes (City-Level and Beyond)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#real-time-applications-in-ar-vr-and-gaming">Real-Time Applications in AR/VR and Gaming</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#integration-with-existing-graphics-pipelines">Integration with Existing Graphics Pipelines</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#learning-from-limited-data">Learning from Limited Data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#conclusion">Conclusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#glossary">Glossary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a><ul>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-01-1-historical-body-models">Lecture 01.1 (Historical Body Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-01-2-introduction-to-human-models">Lecture 01.2 (Introduction to Human Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-01-3-introduction-to-human-models-continued">Lecture 01.3 (Introduction to Human Models Continued)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-02-1-image-formation">Lecture 02.1 (Image Formation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-02-2-rotations-kinematic-chains">Lecture 02.2 (Rotations &amp; Kinematic Chains)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-03-1-surface-representations">Lecture 03.1 (Surface Representations)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-03-2-procrustes-alignment">Lecture 03.2 (Procrustes Alignment)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-04-1-iterative-closest-points">Lecture 04.1 (Iterative Closest Points)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-04-2-body-models">Lecture 04.2 (Body Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-05-1-body-model-training">Lecture 05.1 (Body Model Training)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-05-2-3d-registration">Lecture 05.2 (3D Registration)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-06-1-fitting-smpl-to-images">Lecture 06.1 (Fitting SMPL to Images)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-06-1-optimization-based-fitting-of-smpl-to-images">Lecture 06.1 (Optimization-Based Fitting of SMPL to Images)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-06-2-learning-based-fitting-of-smpl-to-images">Lecture 06.2 (Learning-Based Fitting of SMPL to Images)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-07-1-fitting-smpl-to-imu-optimization">Lecture 07.1 (Fitting SMPL to IMU Optimization)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-07-2-fitting-smpl-to-imu-learning">Lecture 07.2 (Fitting SMPL to IMU Learning)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="references.html#classic-and-optimization-based-methods">Classic and Optimization-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="references.html#learning-based-methods">Learning-Based Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="references.html#datasets-and-resources">Datasets and Resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#relevant-software-and-libraries">Relevant Software and Libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-08-1-references-for-vertex-based-clothing-modeling-for-virtual-humans">Lecture 08.1: References for Vertex-Based Clothing Modeling for Virtual Humans</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#body-models-and-shape-estimation">Body Models and Shape Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#alternative-representations">Alternative Representations</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#datasets">Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#software-and-libraries">Software and Libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-09-1-references-for-neural-implicit-and-point-based-representations-for-clothed-human-modeling">Lecture 09.1: References for Neural Implicit and Point-Based Representations for Clothed Human Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#point-based-models">Point-Based Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#id50">Datasets and Resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#related-software-and-libraries">Related Software and Libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#review-papers-and-tutorials">Review Papers and Tutorials</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#neural-radiance-fields-nerf">Neural Radiance Fields (NERF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#gaussian-splatting">Gaussian Splatting</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Virtual Humans Lecture</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Lecture 06.1 - Fitting the SMPL Model to Images via Optimization</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/lecture_06_1_SMPL_optimization.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="lecture-06-1-fitting-the-smpl-model-to-images-via-optimization">
<span id="lecture-06-1-smpl-fitting"></span><h1>Lecture 06.1 - Fitting the SMPL Model to Images via Optimization<a class="headerlink" href="#lecture-06-1-fitting-the-smpl-model-to-images-via-optimization" title="Link to this heading"></a></h1>
<iframe width="600" height="400" src="https://www.youtube.com/embed/t44EmLGK9sI"
title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write;
encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><p><a class="reference external" href="https://virtualhumans.mpi-inf.mpg.de/VH23/slides/pdf/Lecture_06_1_Fitting_SMPL_to_Images_Optimization.pdf">Lecture Slides: Fitting the SMPL Model to Images via Optimization</a></p>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h2>
<p>In this lecture, we explore the process of fitting a 3D parametric human body model (specifically SMPL) to 2D images through optimization techniques. This process enables us to recover 3D human pose and shape from a single image—a fundamentally ill-posed problem that requires sophisticated mathematical formulations and careful optimization strategies.</p>
</section>
<section id="mathematical-background-pinhole-camera-and-projections">
<h2>Mathematical Background: Pinhole Camera and Projections<a class="headerlink" href="#mathematical-background-pinhole-camera-and-projections" title="Link to this heading"></a></h2>
<p>Before fitting a 3D model to 2D images, we must understand how 3D points are projected onto the 2D image plane. The pinhole camera model provides a mathematical abstraction of this imaging process.</p>
<section id="perspective-projection">
<h3>Perspective Projection<a class="headerlink" href="#perspective-projection" title="Link to this heading"></a></h3>
<p>In a pinhole camera, 3D points in the world are projected through an optical center onto a flat image plane. The projection can be described with intrinsic and extrinsic camera parameters:</p>
<ul class="simple">
<li><p><strong>Intrinsic parameters</strong> (matrix <span class="math notranslate nohighlight">\(K\)</span>): focal length (<span class="math notranslate nohighlight">\(f\)</span>) and principal point offsets (<span class="math notranslate nohighlight">\(c_x, c_y\)</span>), which define how coordinates in the camera coordinate system map to pixel coordinates.</p></li>
<li><p><strong>Extrinsic parameters</strong> (<span class="math notranslate nohighlight">\(R, t\)</span>): rotation and translation that transform world coordinates to the camera’s coordinate frame (i.e., the camera pose in the world).</p></li>
</ul>
<p>Using homogeneous coordinates, the full perspective projection can be written as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix} u \\ v \\ 1 \end{bmatrix} = \mathbf{K}\,[\,\mathbf{R}\mid \mathbf{t}\,]\, \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\((X, Y, Z)\)</span> is a point in 3D (world coordinates), <span class="math notranslate nohighlight">\((u,v)\)</span> is its resulting pixel coordinate on the image.</p>
<p>Expanded into scalar form, this perspective projection implies:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(u = f_x \frac{X_c}{Z_c} + c_x\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(v = f_y \frac{Y_c}{Z_c} + c_y\)</span></p></li>
</ul>
<p>where <span class="math notranslate nohighlight">\((X_c, Y_c, Z_c) = R (X,Y,Z)^T + t\)</span> are the 3D coordinates of the point in the camera coordinate system, and <span class="math notranslate nohighlight">\(f_x, f_y\)</span> are the focal lengths (in pixels) in the <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> directions (often <span class="math notranslate nohighlight">\(f_x=f_y=f\)</span> for square pixels).</p>
<p>The parameters <span class="math notranslate nohighlight">\(c_x, c_y\)</span> denote the principal point (the projection of the camera center onto the image plane, often near the center of the image).</p>
<p>This formulation uses homogeneous coordinates to succinctly express the projection as a matrix multiplication. The <span class="math notranslate nohighlight">\(3\times 4\)</span> camera matrix <span class="math notranslate nohighlight">\(P = K [R|t]\)</span> combines all intrinsic and extrinsic parameters and maps homogeneous world coordinates to homogeneous image coordinates.</p>
</section>
<section id="weak-perspective-projection">
<h3>Weak-Perspective Projection<a class="headerlink" href="#weak-perspective-projection" title="Link to this heading"></a></h3>
<p>A special case of the pinhole model, often used for 3D human fitting, is the weak-perspective projection (also called scaled orthographic projection). This model assumes the depth variation of the subject is small relative to the distance from the camera, so all points are roughly at an average depth <span class="math notranslate nohighlight">\(Z_0\)</span>.</p>
<p>One can then approximate <span class="math notranslate nohighlight">\(s = f/Z_c \approx f/Z_0\)</span> as a constant scale factor. The projection simplifies to:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(u \approx s \, X_c + c_x\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(v \approx s \, Y_c + c_y\)</span></p></li>
</ul>
<p>where <span class="math notranslate nohighlight">\(s = f/Z_0\)</span> is an overall isotropic scale, and translations <span class="math notranslate nohighlight">\(t_x = c_x, t_y = c_y\)</span> account for 2D positioning.</p>
<p>In other words, a weak-perspective camera performs an orthographic projection (parallel projection) followed by a uniform scaling. This is less accurate than full perspective but avoids nonlinearity in <span class="math notranslate nohighlight">\(Z\)</span> and can be convenient when the subject’s distance is unknown.</p>
<p>Many 3D human fitting methods initially assume a weak-perspective model to simplify optimization. For instance, the person’s scale and 2D image location can be estimated from the bounding box before refining with a full perspective model.</p>
</section>
<section id="camera-extrinsics-vs-model-pose">
<h3>Camera Extrinsics vs. Model Pose<a class="headerlink" href="#camera-extrinsics-vs-model-pose" title="Link to this heading"></a></h3>
<p>In a generative fitting context, one can either optimize the camera extrinsic parameters <span class="math notranslate nohighlight">\((R,t)\)</span> or, equivalently, the global orientation and position of the 3D subject. In practice, fitting a human model to an image often involves introducing a global rotation <span class="math notranslate nohighlight">\(\theta_{\text{global}}\)</span> and translation <span class="math notranslate nohighlight">\(t\)</span> for the model. These effectively play the same role as the camera’s <span class="math notranslate nohighlight">\(R\)</span> and <span class="math notranslate nohighlight">\(t\)</span> (with the camera assumed fixed).</p>
<p>We can treat the root joint of the human model as the world origin and adjust its pose and location to align with the camera. The goal of fitting will be to find the model’s pose parameters and the camera parameters such that the model’s projected 2D joints match the observed 2D keypoints in the image.</p>
</section>
<section id="d-keypoints-and-projection">
<h3>2D Keypoints and Projection<a class="headerlink" href="#d-keypoints-and-projection" title="Link to this heading"></a></h3>
<p>Modern pose estimation methods (e.g., OpenPose or DeepLabCut) can detect 2D positions of human joints in an image. Let <span class="math notranslate nohighlight">\(\{\mathbf{p}_i^{\text{obs}}\}\)</span> be the set of observed 2D joint coordinates (e.g., for <span class="math notranslate nohighlight">\(i=1,\dots, J\)</span> joints). Let <span class="math notranslate nohighlight">\(\mathbf{J}(\beta,\theta)\)</span> be the 3D joint positions of our parametric model for shape <span class="math notranslate nohighlight">\(\beta\)</span> and pose <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>To compare <span class="math notranslate nohighlight">\(\mathbf{J}\)</span> with image evidence, we project these model joints into the image with the camera model <span class="math notranslate nohighlight">\(\Pi\)</span> (which may be perspective or weak-perspective). The projected point is <span class="math notranslate nohighlight">\(\mathbf{p}_i^{\text{pred}} = \Pi(\mathbf{J}_i(\beta,\theta))\)</span>.</p>
<p>For a perspective model, <span class="math notranslate nohighlight">\(\Pi(X,Y,Z) = \big(f_x X/Z + c_x, f_y Y/Z + c_y\big)\)</span> as described above.</p>
<p>The fitting procedure will try to minimize the reprojection error between the predicted 2D joints and the detected 2D joints:</p>
<div class="math notranslate nohighlight">
\[E_{\text{joint}}(\beta,\theta,R,t) = \sum_{i=1}^{J} w_i \,\|\,\Pi(R\,\mathbf{J}_i(\beta,\theta) + t)\;-\;\mathbf{p}_i^{\text{obs}}\|^2\]</div>
<p>where <span class="math notranslate nohighlight">\(w_i\)</span> are weights reflecting detection confidence for each joint.</p>
<p>In practice, a robust penalty <span class="math notranslate nohighlight">\(\rho\)</span> is often used instead of a simple square, to reduce the influence of outliers (e.g., a misdetected keypoint). A common choice is the Geman-McClure or Huber robust function, which behaves like least-squares for small errors but less aggressively penalizes large errors, thus improving robustness to mis-detections or occlusions.</p>
<p>By formulating the objective in terms of 2D reprojection error, we leverage well-established techniques from camera calibration and multiple-view geometry: effectively, the algorithm is solving the inverse problem of finding 3D pose from 2D projections, which is inherently ill-posed due to depth ambiguity. This is why additional prior terms and constraints (discussed next) are crucial for a successful fit.</p>
</section>
</section>
<section id="the-smpl-model-as-a-differentiable-function-of-shape-and-pose">
<h2>The SMPL Model as a Differentiable Function of Shape and Pose<a class="headerlink" href="#the-smpl-model-as-a-differentiable-function-of-shape-and-pose" title="Link to this heading"></a></h2>
<p>SMPL (Skinned Multi-Person Linear model) is a parametric 3D human body model that provides a mapping from shape parameters <span class="math notranslate nohighlight">\(\beta\)</span> and pose parameters <span class="math notranslate nohighlight">\(\theta\)</span> to a complete triangulated mesh of the human body.</p>
<p>In SMPL, the shape is controlled by a low-dimensional vector <span class="math notranslate nohighlight">\(\beta\)</span> (usually <span class="math notranslate nohighlight">\(\mathbb{R}^{10}\)</span>) which coefficients a set of principal components of body shape, and the pose is controlled by a set of joint rotation parameters <span class="math notranslate nohighlight">\(\theta\)</span> (usually <span class="math notranslate nohighlight">\(\mathbb{R}^{72}\)</span>, i.e., 24 joints each with 3 DOF axis-angle rotation).</p>
<p>Formally, SMPL defines a differentiable function:</p>
<div class="math notranslate nohighlight">
\[M(\beta, \theta) = W\!\big( T(\beta,\theta), J(\beta), \theta, \mathcal{W} \big)\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(T(\beta,\theta)\)</span> is the template mesh after applying shape and pose deformations</p></li>
<li><p><span class="math notranslate nohighlight">\(J(\beta)\)</span> is the set of joint locations for the given shape</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{W}\)</span> are fixed blend skinning weights</p></li>
<li><p><span class="math notranslate nohighlight">\(W(\cdot)\)</span> is the linear blend skinning function that applies the pose articulations to the mesh vertices</p></li>
</ul>
<p>The model can be understood in stages:</p>
<section id="shape-blend-shapes">
<h3>Shape Blend Shapes<a class="headerlink" href="#shape-blend-shapes" title="Link to this heading"></a></h3>
<p>First, a base template mesh <span class="math notranslate nohighlight">\(T_0\)</span> (an average human shape in a reference pose, e.g., standing T-pose) is deformed according to <span class="math notranslate nohighlight">\(\beta\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbf{V}_{\text{shape}}(\beta) = T_0 + B_S\,\beta\]</div>
<p>where <span class="math notranslate nohighlight">\(B_S\)</span> is a matrix of shape blend shapes. This yields a person-specific mesh in the rest pose.</p>
<p>Intuitively, <span class="math notranslate nohighlight">\(\beta\)</span> might encode variations like height, weight, limb lengths, etc., learned from a dataset of body scans.</p>
</section>
<section id="pose-blend-shapes">
<h3>Pose Blend Shapes<a class="headerlink" href="#pose-blend-shapes" title="Link to this heading"></a></h3>
<p>Even for a fixed person shape, certain poses cause non-rigid deformations of the body (e.g., muscle bulging, twisting of limbs flattening the flesh). SMPL captures typical pose-dependent deformations with another linear model:</p>
<div class="math notranslate nohighlight">
\[\mathbf{V}_{\text{posed}}(\beta,\theta) = \mathbf{V}_{\text{shape}}(\beta) + B_P(\theta)\]</div>
<p>The pose blend shapes <span class="math notranslate nohighlight">\(B_P\)</span> are a set of corrective shapes that account for how bending a joint changes the mesh geometry.</p>
<p>In practice, <span class="math notranslate nohighlight">\(B_P(\theta)\)</span> is implemented by first converting each relative joint rotation <span class="math notranslate nohighlight">\(\theta_j\)</span> (usually represented in axis-angle or quaternion form) into its rotation matrix <span class="math notranslate nohighlight">\(R_j\)</span>. For each joint, the deviation from the rest pose is <span class="math notranslate nohighlight">\(R_j - I\)</span> (a <span class="math notranslate nohighlight">\(3\times 3\)</span> matrix, flattened to 9-dimensional vector).</p>
<p>The pose blend shape model multiplies each of these by a learned coefficient matrix and sums the contributions for all joints. This produces a <span class="math notranslate nohighlight">\(3N\)</span>-vector of vertex displacements that make the limbs bend more naturally.</p>
</section>
<section id="joint-positions">
<h3>Joint Positions<a class="headerlink" href="#joint-positions" title="Link to this heading"></a></h3>
<p>We also compute the joint positions <span class="math notranslate nohighlight">\(J(\beta)\)</span> for this mesh. SMPL defines <span class="math notranslate nohighlight">\(J(\beta)\)</span> by a linear regression from the vertex positions.</p>
<p>The regression matrix <span class="math notranslate nohighlight">\(J_{\text{reg}}\)</span> (dimension <span class="math notranslate nohighlight">\(K\times N\)</span> for <span class="math notranslate nohighlight">\(K\)</span> joints and <span class="math notranslate nohighlight">\(N\)</span> vertices) was learned from training scans so that <span class="math notranslate nohighlight">\(J(\beta) = J_{\text{reg}}\cdot \mathbf{V}_{\text{shape}}(\beta)\)</span> gives the 3D coordinates of each joint (in the rest pose) as a linear combination of nearby vertices.</p>
<p>This means taller or larger bodies (different <span class="math notranslate nohighlight">\(\beta\)</span>) will have joints that are farther apart, reflecting the shape change.</p>
</section>
<section id="linear-blend-skinning">
<h3>Linear Blend Skinning<a class="headerlink" href="#linear-blend-skinning" title="Link to this heading"></a></h3>
<p>Finally, the mesh is articulated by rotating and translating each part according to the pose. SMPL uses standard linear blend skinning (LBS), which is a weighted sum of rigid transformations.</p>
<p>Each vertex <span class="math notranslate nohighlight">\(i\)</span> of the mesh is associated with a fixed set of weights <span class="math notranslate nohighlight">\(\{w_{i1}, \dots, w_{iK}\}\)</span> (one weight for each joint, where <span class="math notranslate nohighlight">\(K=24\)</span> for SMPL) that sum to 1. These weights define how much the vertex moves with each bone.</p>
<p>Given the pose <span class="math notranslate nohighlight">\(\theta\)</span>, we can compute a transformation <span class="math notranslate nohighlight">\(G_j(\theta,\beta)\)</span> for each joint <span class="math notranslate nohighlight">\(j\)</span> that takes the vertex from the rest pose to its posed location for that joint’s movement (this is essentially forward kinematics).</p>
<p>Applying LBS, the final vertex position <span class="math notranslate nohighlight">\(v_i\)</span> is:</p>
<div class="math notranslate nohighlight">
\[v_i(\beta,\theta) = \sum_{j=1}^{K} w_{ij} G_j(\theta,\beta) \tilde{v}_{i}(\beta,\theta)\]</div>
<p>where <span class="math notranslate nohighlight">\(\tilde{v}_{i}(\beta,\theta)\)</span> is the homogeneous coordinate of the vertex <span class="math notranslate nohighlight">\(i\)</span> in the rest pose (after the blend shapes), and <span class="math notranslate nohighlight">\(G_j(\theta,\beta)\)</span> is the <span class="math notranslate nohighlight">\(4\times 4\)</span> homogeneous transformation matrix of joint <span class="math notranslate nohighlight">\(j\)</span>.</p>
<p>In simpler terms, we rotate and translate the entire mesh according to each joint’s motion, weighted by the skinning weights. The result <span class="math notranslate nohighlight">\(M(\beta,\theta)\)</span> is a set of 3D vertices representing the fully posed mesh.</p>
</section>
<section id="differentiability-of-smpl">
<h3>Differentiability of SMPL<a class="headerlink" href="#differentiability-of-smpl" title="Link to this heading"></a></h3>
<p>SMPL’s formulation is fully differentiable. It consists of linear operations (addition, multiplication) and smooth nonlinearities (mainly the sines/cosines inside the rotation matrices).</p>
<p>This means we can compute analytical gradients <span class="math notranslate nohighlight">\(\partial M / \partial \beta\)</span> and <span class="math notranslate nohighlight">\(\partial M / \partial \theta\)</span> if needed, or rely on automatic differentiation. Modern implementations of SMPL use packages like PyTorch or TensorFlow to get these derivatives, enabling gradient-based optimization or integration into neural networks.</p>
<p>In summary, SMPL provides a function that takes <span class="math notranslate nohighlight">\((\beta,\theta)\)</span> to joint locations <span class="math notranslate nohighlight">\(\mathbf{J}(\beta,\theta)\)</span> and vertex coordinates <span class="math notranslate nohighlight">\(M(\beta,\theta)\)</span> in 3D. The joints can be projected into the image for 2D alignment, and the vertices can be used for silhouette alignment or collision tests. Because <span class="math notranslate nohighlight">\(M\)</span> is differentiable, one can fit SMPL to data by optimizing <span class="math notranslate nohighlight">\(\beta\)</span> and <span class="math notranslate nohighlight">\(\theta\)</span> via gradient-based methods.</p>
</section>
</section>
<section id="fitting-smpl-to-images-via-optimization-smplify">
<h2>Fitting SMPL to Images via Optimization (SMPLify)<a class="headerlink" href="#fitting-smpl-to-images-via-optimization-smplify" title="Link to this heading"></a></h2>
<p>Given an input image (RGB) with detected 2D keypoints, the goal is to recover the underlying 3D body shape <span class="math notranslate nohighlight">\(\beta\)</span> and pose <span class="math notranslate nohighlight">\(\theta\)</span> (and often the camera parameters).</p>
<p>In a generative optimization approach, we define an objective function that measures how well a hypothesized model explains the image evidence, and then we adjust the model parameters to minimize this objective. This approach was popularized by the SMPLify algorithm (Bogo et al., 2016), which provided the first automatic system to fit SMPL to single-image 2D joint detections.</p>
<section id="objective-function">
<h3>Objective Function<a class="headerlink" href="#objective-function" title="Link to this heading"></a></h3>
<p>The objective function <span class="math notranslate nohighlight">\(\mathcal{E}\)</span> typically contains multiple terms designed to enforce both data fit (reprojection error) and prior knowledge (to resolve ambiguities and ensure plausible humans):</p>
<ol class="arabic">
<li><p><strong>Keypoint Reprojection Term</strong> (<span class="math notranslate nohighlight">\(E_J\)</span>):</p>
<p>This is the data term we formulated above. It penalizes differences between observed 2D joint locations and the projected 3D model joints:</p>
<div class="math notranslate nohighlight">
\[E_J(\beta,\theta, R, t; \,J^{\text{obs}}) = \sum_{i=1}^{J} w_i\, \rho\!\Big(\| \Pi(R\,J_i(\beta,\theta) + t) - u_i^{\text{obs}}\|^2\Big)\]</div>
<p>where <span class="math notranslate nohighlight">\(u_i^{\text{obs}}\)</span> is the detected 2D position of joint <span class="math notranslate nohighlight">\(i\)</span> in the image. The function <span class="math notranslate nohighlight">\(\rho\)</span> is a robust penalty (SMPLify uses a Geman-McClure penalty), and <span class="math notranslate nohighlight">\(w_i\)</span> is the detection confidence for joint <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p>This term drives the optimization to explain the 2D keypoints by projecting the 3D model’s joints. If the model’s projected knees are, say, too far to the left of the image knee points, this term’s gradient will pull the model’s knees inwards in 3D.</p>
</li>
<li><p><strong>Pose Prior</strong> (<span class="math notranslate nohighlight">\(E_\theta\)</span>):</p>
<p>Reprojection error alone is insufficient because many 3D poses can explain the same 2D joints due to depth ambiguity. To regularize the solution, we introduce a prior on human pose. The pose prior penalizes implausible or extreme joint angles.</p>
<p>In SMPLify, a mixture of Gaussians prior over the pose joint angles is used. This prior was learned from a large motion-capture dataset (e.g., CMU Mocap) and encodes typical human poses. The cost can be written as:</p>
<div class="math notranslate nohighlight">
\[E_\theta(\theta) = -\log P_{\text{pose}}(\theta)\]</div>
<p>where <span class="math notranslate nohighlight">\(P_{\text{pose}}\)</span> is the probability of pose <span class="math notranslate nohighlight">\(\theta\)</span> under the Gaussian mixture model.</p>
<p>In practice, this results in a sum of quadratic penalties for certain joint angle combinations, with higher weight on rarely observed configurations. For example, if <span class="math notranslate nohighlight">\(\theta\)</span> encodes the knee angle, the prior will strongly penalize hyperextension beyond the normal range.</p>
<p>In the SMPLify implementation, there was also a specific term <span class="math notranslate nohighlight">\(E_a(\theta)\)</span> to prevent unnatural bending of elbows and knees. This ensures that even if the 2D keypoints might be fit by an inverted limb, the optimizer will favor the normal bending direction.</p>
</li>
<li><p><strong>Shape Prior</strong> (<span class="math notranslate nohighlight">\(E_\beta\)</span>):</p>
<p>The shape parameters <span class="math notranslate nohighlight">\(\beta\)</span> need regularization as well. With only a single view, body shape is difficult to infer – for instance, a taller but farther person can project similarly to a shorter but closer person.</p>
<p>SMPLify uses a simple Gaussian prior on shape, assuming that <span class="math notranslate nohighlight">\(\beta\)</span> is drawn from a zero-mean distribution (since the SMPL shape space is learned such that <span class="math notranslate nohighlight">\(\beta=0\)</span> corresponds to an average physique).</p>
<p>The shape prior can be:</p>
<div class="math notranslate nohighlight">
\[E_\beta(\beta) = \lambda_\beta \|\beta\|^2\]</div>
<p>a weighted L2 norm on the shape coefficients. This keeps the solution near the mean body shape unless the data strongly suggests otherwise. It helps prevent the optimizer from producing extreme body shapes just to marginally improve the keypoint alignment.</p>
</li>
<li><p><strong>Interpenetration Penalty</strong> (<span class="math notranslate nohighlight">\(E_{\text{coll}}\)</span>):</p>
<p>A common problem in model fitting is self-intersection – the model might fold or twist into an implausible pose where limbs interpenetrate each other (e.g., an arm going through the torso, or legs interlocking).</p>
<p>These collisions can satisfy 2D joint alignment (since 2D joints don’t “see” the collision), but are physically impossible. To address this, SMPLify introduced an interpenetration term that penalizes intersections of mesh geometry.</p>
<p>They approximate each body limb as a capsule (a cylinder with round caps) and further approximate capsules by sets of spheres for computational efficiency. If two body parts that shouldn’t normally touch have overlapping spheres, a cost is added proportional to the volume of intersection.</p>
<p>This term <span class="math notranslate nohighlight">\(E_{\text{coll}}(\beta,\theta)\)</span> is zero for collision-free poses, and grows when, say, the forearm pokes into the torso. By making this differentiable (using smooth approximations of overlap), the optimizer can compute gradients that push intersecting parts apart.</p>
<p>The effect is to preserve body realism: the algorithm will prefer a slightly worse 2D fit if it avoids implausible self-intersection.</p>
</li>
<li><p><strong>(Optional) Silhouette Term</strong> (<span class="math notranslate nohighlight">\(E_{\text{silh}}\)</span>):</p>
<p>If a segmentation of the person in the image is available (i.e., the 2D silhouette), an additional term can be included to improve the alignment of the 3D model’s outline with the image.</p>
<p>Silhouette alignment is not used in the original SMPLify, but later works and variations have explored it. One way to define this term is to project the 3D mesh vertices (or a set of sampled points on the mesh) onto the image and enforce that they fall inside the detected silhouette, and vice versa.</p>
<p>A common implementation is to use the silhouette distance transform: for each projected vertex, add a cost equal to its distance to the nearest silhouette pixel (this drives vertices into the silhouette); and for each silhouette pixel, measure distance to the nearest projected vertex or edge of the mesh (driving the model to explain the silhouette).</p>
<p>In practice, <span class="math notranslate nohighlight">\(E_{\text{silh}}\)</span> helps refine shape and some pose details (like limb thickness or slight rotation) that 2D joints alone cannot constrain. However, it requires a decent image segmentation and increases computational cost. It’s an optional term that can be toggled on when segmentation masks are available.</p>
</li>
</ol>
</section>
<section id="combined-objective">
<h3>Combined Objective<a class="headerlink" href="#combined-objective" title="Link to this heading"></a></h3>
<p>Combining these terms, the full objective for single-image fitting can be written as:</p>
<div class="math notranslate nohighlight">
\[\mathcal{E}(\beta,\theta,R,t) = E_J(\beta,\theta,R,t) + \lambda_\theta E_\theta(\theta) + \lambda_a E_a(\theta) + \lambda_\beta E_\beta(\beta) + \lambda_{\text{coll}} E_{\text{coll}}(\beta,\theta) + \lambda_{\text{silh}} E_{\text{silh}}(\beta,\theta,R,t)\]</div>
<p>Here we explicitly list separate weights <span class="math notranslate nohighlight">\(\lambda\)</span> for each term (pose prior, angle-limit prior, shape prior, collision, silhouette). In the original SMPLify, <span class="math notranslate nohighlight">\(\lambda_{\text{silh}}=0\)</span> (no silhouette term used) and the other weights are set empirically.</p>
<p>The optimization problem is to find the parameters <span class="math notranslate nohighlight">\((\hat\beta,\hat\theta,\hat{R},\hat{t}) = \arg\min \mathcal{E}\)</span>. This is a non-linear least squares problem with many variables (72 pose params + 10 shape params + possibly 3 orientation + 3 translation = 88 unknowns, if camera intrinsics like focal length are known a priori).</p>
<p>It is solved with iterative numerical optimization.</p>
</section>
<section id="optimization-strategy">
<h3>Optimization Strategy<a class="headerlink" href="#optimization-strategy" title="Link to this heading"></a></h3>
<p>A naive attempt to optimize all parameters jointly from random initialization will likely get stuck in a poor local minimum – the problem is high-dimensional and multimodal. SMPLify addresses this by breaking the optimization into stages:</p>
<p><strong>Stage I – Initialization:</strong></p>
<p>The detected 2D joints are used to initialize global position and scale. SMPLify assumes the person is upright and facing the camera initially. They estimate an initial scaling/translation by using the torso keypoints: for example, the distance between shoulders in the image suggests a depth (given an assumed real shoulder width).</p>
<p>In practice, Bogo et al. fixed the camera focal length (using a typical value or known camera intrinsics) and initialized the depth of the person by assuming the person’s height is about 1.7m (or using a rough heuristic). The global rotation is initialized so the model faces the camera and is aligned vertically. This yields initial <span class="math notranslate nohighlight">\(R,t\)</span>.</p>
<p>The pose <span class="math notranslate nohighlight">\(\theta\)</span> can be initialized to a mean pose (standing pose), or some simplistic initialization like all joint angles zero. The shape <span class="math notranslate nohighlight">\(\beta\)</span> is initialized to zero (mean shape) unless there is prior information about the body.</p>
<p><strong>Stage II – Camera/Global Pose Fit:</strong></p>
<p>First, keep pose <span class="math notranslate nohighlight">\(\theta\)</span> and shape <span class="math notranslate nohighlight">\(\beta\)</span> at their initial values except the global orientation (which is part of <span class="math notranslate nohighlight">\(\theta\)</span> for the root joint) and translation. Optimize <span class="math notranslate nohighlight">\(E_J\)</span> with respect to the global rotation and translation only, to roughly align the model to the image.</p>
<p>In this stage, the limbs are not posed yet – they might remain in a default pose. Essentially, we fit the root position of the model so that the hip, shoulder center, etc., match the image.</p>
<p>Since <span class="math notranslate nohighlight">\(\beta\)</span> is fixed, the model has average body size, which might not perfectly match the person, but it’s sufficient for initial alignment. Because of perspective, depth (translation in Z) and overall scale are coupled; with a known focal length, moving the model closer or farther changes the apparent size. The optimizer adjusts these to minimize reprojection error.</p>
<p>One constraint used is to assume the person’s depth such that the model not only aligns in 2D but also has reasonable depth (for example, the model’s feet are on the ground plane if known). After this, we have an initial <span class="math notranslate nohighlight">\((R,t)\)</span>.</p>
<p><strong>Stage III – Pose Fit:</strong></p>
<p>Next, the joint angles <span class="math notranslate nohighlight">\(\theta\)</span> are optimized to match the 2D joints, while the priors <span class="math notranslate nohighlight">\(E_\theta\)</span> and <span class="math notranslate nohighlight">\(E_a\)</span> are applied to keep the pose plausible.</p>
<p>This is done in a multi-step or multi-scale way: SMPLify starts with a high weight on the pose prior, ensuring the pose stays close to natural, then gradually decreases that weight to allow the model to better fit the data. This technique (called graduated optimization) helps avoid unnatural jumps early on.</p>
<p>For example, initially the optimizer might not bend the knee much because the prior is strong, but as the weight reduces, it will bend the knee to match an observed crouch if the data clearly shows it.</p>
<p>The fitting might also be done limb by limb or with different parts sequentially, though the SMPLify paper primarily did all joints together with a robust penalty.</p>
<p>The angle limit term <span class="math notranslate nohighlight">\(E_a\)</span> prevents the optimizer from flipping the limbs the wrong way. At the end of this stage, <span class="math notranslate nohighlight">\(\theta\)</span> is roughly consistent with the 2D joints.</p>
<p><strong>Stage IV – Shape Fit:</strong></p>
<p>Once the pose is aligned, the body shape <span class="math notranslate nohighlight">\(\beta\)</span> is refined. Shape changes can account for residual systematic offsets in joints – for instance, if all joints of the model are slightly lower than the image points, the optimizer can achieve a better fit by increasing the leg length in <span class="math notranslate nohighlight">\(\beta\)</span> rather than moving every joint individually (which pose might not allow if the person is consistently taller).</p>
<p>The shape prior <span class="math notranslate nohighlight">\(E_\beta\)</span> is important here to prevent absurd solutions. In SMPLify, the shape was updated after pose: the algorithm observed that body shape does subtly affect 2D joint alignment (especially at shoulders and hips).</p>
<p>By fitting shape, SMPLify demonstrated that even from just 2D joints one can infer whether the person is, say, very lanky or more stout. This was an intriguing result, as it showed 2D pose alone carries some shape information. Typically, <span class="math notranslate nohighlight">\(\beta\)</span> is solved with pose fixed, or alternated a couple of times with pose to fine-tune both.</p>
<p><strong>Stage V – Full Refinement:</strong></p>
<p>Optionally, one can then let all parameters <span class="math notranslate nohighlight">\((\beta,\theta,R,t)\)</span> vary together for a few final iterations to fine-tune the fit. At this point, good initialization from previous stages means the solution is in the basin of a correct minimum, so joint optimization should converge to a fine result.</p>
<p>Interpenetration penalty <span class="math notranslate nohighlight">\(E_{\text{coll}}\)</span> is activated to push limbs out if any collision is happening. This sometimes requires a trade-off: the optimizer might slightly sacrifice keypoint reprojection accuracy to remove a collision (because <span class="math notranslate nohighlight">\(E_J\)</span> and <span class="math notranslate nohighlight">\(E_{\text{coll}}\)</span> compete).</p>
<p>The weights <span class="math notranslate nohighlight">\(\lambda_{\text{coll}}\)</span> are tuned so that avoiding impossible interpenetrations is more important than shaving off the last pixel of joint error.</p>
</section>
<section id="optimization-algorithms">
<h3>Optimization Algorithms<a class="headerlink" href="#optimization-algorithms" title="Link to this heading"></a></h3>
<p>Throughout these stages, one must choose an appropriate optimization algorithm. SMPLify was implemented using a quasi-Newton optimizer (LBFGS) and also a specialized Dogleg trust-region method for non-linear least squares (from the Chumpy library).</p>
<p>In general, algorithms like Levenberg-Marquardt (which blends Gauss-Newton with gradient descent) are well-suited for bundle-adjustment-type problems like this. They require computing the Jacobian of all residuals (which we get via autodiff).</p>
<p>Alternatively, one can use Powell’s method (a derivative-free direction set method) or CMA-ES (an evolutionary strategy) for the pose, but these tend to be slower.</p>
<p>SMPLify’s success came from leveraging the analytic gradients – by having a differentiable model, they could use efficient gradient-based optimization to fit dozens of parameters in under a minute.</p>
<p>Each term contributes to the gradient: for instance, the pose prior gives a gradient pushing <span class="math notranslate nohighlight">\(\theta\)</span> towards the mean pose, the collision term gives a gradient pushing intersecting parts away from each other, etc.</p>
<p>The use of robust loss (Geman-McClure) means the influence of a single bad keypoint detection on the gradient is capped, improving stability.</p>
</section>
<section id="automatic-differentiation-and-jacobians">
<h3>Automatic Differentiation and Jacobians<a class="headerlink" href="#automatic-differentiation-and-jacobians" title="Link to this heading"></a></h3>
<p>As mentioned, the SMPL function is differentiable. Frameworks like Chumpy (used in the original SMPLify) or modern autograd libraries can provide <span class="math notranslate nohighlight">\(\partial \Pi/\partial \theta\)</span> and <span class="math notranslate nohighlight">\(\partial \Pi/\partial \beta\)</span>, etc., automatically.</p>
<p>However, it’s instructive to consider the complexity: the Jacobian of the full model w.r.t. all parameters is a large matrix (e.g., projecting 24 joints gives 48 residuals (2D coords for each) and we have ~82 unknowns, so Jacobian is 48×82).</p>
<p>Fortunately, many entries are zero due to the sparse influence of parameters (e.g., finger rotations don’t affect leg joints). Exploiting sparsity is key in speeding up Gauss-Newton steps.</p>
<p>In practice, one might not form the full Jacobian explicitly but rather use iterative gradient methods (like LBFGS) that only require Jacobian-vector products, which autograd can compute efficiently.</p>
<p>Autodiff also makes it easy to experiment with new terms: e.g., if we add a silhouette IOU term, we can implement its computation and rely on autodiff for gradients, rather than deriving them by hand.</p>
<p>By the end of the optimization, we obtain <span class="math notranslate nohighlight">\(\beta\)</span> and <span class="math notranslate nohighlight">\(\theta\)</span> that (hopefully) explain the image: the model’s projected joints align with the image joints, the pose looks natural (thanks to priors), and limbs are not interpenetrating. The output is a full 3D mesh (the SMPL model) in a pose and shape that matches the person in the image.</p>
<p>This can be used for downstream tasks like generating novel views, estimating metrics (heights, limb lengths), or serving as initialization for further refinement with image-based losses.</p>
</section>
<section id="result-of-smplify">
<h3>Result of SMPLify<a class="headerlink" href="#result-of-smplify" title="Link to this heading"></a></h3>
<p>It’s worth noting that SMPLify was a breakthrough in 2016, demonstrating that even without any manual intervention, one could get a reasonable 3D humanoid mesh from just a single photo’s 2D keypoints.</p>
<p>It showed superior 3D pose accuracy on benchmarks like Human3.6M and HumanEva compared to earlier 3D pose estimation methods at the time. However, it is not without limitations: it can be slow (on the order of 30-60 seconds per image), sometimes gets stuck in local minima (especially if the 2D keypoints are noisy or occluded), and the quality of results depends heavily on the quality of the 2D joint detector.</p>
</section>
</section>
<section id="historical-progression-and-method-comparisons">
<h2>Historical Progression and Method Comparisons<a class="headerlink" href="#historical-progression-and-method-comparisons" title="Link to this heading"></a></h2>
<p>The field of single-image 3D human pose and shape estimation has rapidly evolved since SMPLify (2016). Early approaches like SMPLify are optimization-based: they rely on minimizing a carefully designed objective at test time for each image.</p>
<p>Subsequent approaches explored regression-based models: using deep learning to directly predict <span class="math notranslate nohighlight">\(\beta,\theta\)</span> from the image in a single forward pass. Each paradigm has strengths and weaknesses, and recent works often combine ideas from both.</p>
<p>Here we survey key developments:</p>
<section id="smplify-bogo-et-al-2016">
<h3>SMPLify (Bogo et al. 2016)<a class="headerlink" href="#smplify-bogo-et-al-2016" title="Link to this heading"></a></h3>
<p>As detailed above, this was a seminal work using an optimization approach. It established the feasibility of fitting a full-body parametric model to 2D joints. The method achieved state-of-the-art accuracy on 3D pose benchmarks of the time (e.g., Human3.6M) by virtue of the strong body prior and accurate 2D keypoint detectors.</p>
<p>Its outputs are interpretable (every term in the objective has semantic meaning) and relatively free of “training bias” (it can be applied to any person image, even outside the distribution of some training set, as long as 2D joints can be detected).</p>
<p>However, SMPLify was slow and sometimes required manual cleanup (for extreme poses or when the detector failed). It also did not model hands and face – it used the basic SMPL (body only). Extensions soon followed.</p>
</section>
<section id="smplify-x-2019">
<h3>SMPLify-X (2019)<a class="headerlink" href="#smplify-x-2019" title="Link to this heading"></a></h3>
<p>SMPLify-X represents a significant extension of SMPLify, addressing key limitations of the original approach. While SMPLify could only recover body pose and shape, SMPLify-X enables comprehensive whole-body capture by jointly optimizing for face, hands, and body in a single framework.
It works with SMPL-X (eXpressive SMPL), a more expressive parametric model that extends SMPL with 54 parameters for the body, detailed hand articulation with 15 joints per hand (30 additional degrees of freedom), and a facial blendshape model with 10 expression parameters. The optimization objective in SMPLify-X incorporates several new components.
It uses specialized detectors for facial landmarks and hand keypoints, adding facial landmark reprojection error and hand keypoint reprojection error terms to the objective function. It also introduces learned hand and face pose priors (derived from a new expressive whole-body dataset) to ensure anatomically plausible configurations of fingers and facial expressions.
These priors are critical since finger motion and facial expressions are highly constrained by anatomy, yet exhibit significant variety. A key technical contribution was an improved self-interpenetration avoidance model. The original SMPLify used approximate capsules for collision detection, but this became impractical with the detailed finger and face geometry.
SMPLify-X introduces a collision model with a mixture of 3D Gaussians attached to the body surface for efficient detection of self-penetrations involving fingers, face, and other body parts, with analytical gradients for optimization.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="lecture_05_2_3d_registration.html" class="btn btn-neutral float-left" title="Lecture 05.2 - 3D Registration: From Classical ICP to Modern Methods" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="lecture_06_2SMPL_model_fitting.html" class="btn btn-neutral float-right" title="Lecture 06.2 - Learning-Based Fitting of the SMPL Model to Images" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>