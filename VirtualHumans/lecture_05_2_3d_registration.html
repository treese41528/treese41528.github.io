

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Lecture 05.2 - 3D Registration: From Classical ICP to Modern Methods &mdash; Fitting SMPL to IMU Optimization</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
    <link rel="canonical" href="https://treese41528.github.io/VirtualHumans/lecture_05_2_3d_registration.html" />
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=f2a433a1"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Lecture 06.1 - Fitting the SMPL Model to Images via Optimization" href="lecture_06_1_SMPL_optimization.html" />
    <link rel="prev" title="Lecture 5.1 - Training a Body Model and Fitting SMPL to Scans" href="lecture_05_1_body_model_training.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Virtual Humans Lecture
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="lecture_01_1_historical_body_models.html">Lecture 01.1 – Historical Body Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#early-origins-simplified-primitives-and-kinematic-skeletons-1970s1980s">Early Origins: Simplified Primitives and Kinematic Skeletons (1970s–1980s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#advances-in-the-1990s-superquadrics-differentiable-fitting-and-physical-models">Advances in the 1990s: Superquadrics, Differentiable Fitting, and Physical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#the-impact-of-3d-scanning-and-data-from-anthropometry-to-statistical-models-1990s2000s">The Impact of 3D Scanning and Data: From Anthropometry to Statistical Models (1990s–2000s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#scape-and-the-emergence-of-pose-aware-models-mid-2000s">SCAPE and the Emergence of Pose-Aware Models (Mid-2000s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#consolidation-in-the-2010s-smpl-and-integration-with-learning-based-methods">Consolidation in the 2010s: SMPL and Integration with Learning-Based Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#deep-learning-and-neural-implicit-models-late-2010spresent">Deep Learning and Neural Implicit Models (Late 2010s–Present)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#timeline-summary-of-milestones">Timeline Summary of Milestones</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html">Lecture 01.2 – Introduction to Human Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#historical-context-of-human-body-modeling">1. Historical Context of Human Body Modeling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#early-developments">Early Developments</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#mid-20th-century-approaches">Mid-20th Century Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#data-driven-revolution-1990s-2000s">Data-Driven Revolution (1990s-2000s)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#mathematical-foundations-of-human-body-models">2. Mathematical Foundations of Human Body Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#the-smpl-model">The SMPL Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#pca-based-statistical-shape-modeling">PCA-Based Statistical Shape Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#kinematic-modeling">Kinematic Modeling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#applications-of-human-body-models">3. Applications of Human Body Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computer-animation-and-visual-effects">Computer Animation and Visual Effects</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#virtual-humans-and-avatars">Virtual Humans and Avatars</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#biomechanics-and-ergonomics">Biomechanics and Ergonomics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#human-computer-interaction-hci">Human-Computer Interaction (HCI)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computer-vision-and-ai">Computer Vision and AI</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#education-and-training">Education and Training</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#challenges-and-future-directions">4. Challenges and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computational-efficiency">Computational Efficiency</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#accuracy-and-detail">Accuracy and Detail</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#generalization">Generalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#clothing-and-accessories">Clothing and Accessories</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#emerging-approaches">Emerging Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html">Lecture 01.3 – Introduction to Human Models (Overview)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#historical-context">1. Historical Context</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#early-scientific-studies">Early Scientific Studies</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#mid-20th-century-to-digital-era">Mid-20th Century to Digital Era</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#st-century-advances">21st Century Advances</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#mathematical-foundations">2. Mathematical Foundations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#parametric-body-models">Parametric Body Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-surface-representations">Implicit Surface Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#kinematic-modeling">Kinematic Modeling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#image-formation-and-rendering">3. Image Formation and Rendering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#camera-models">Camera Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#shading-and-visibility">Shading and Visibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#differentiable-rendering">Differentiable Rendering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#surface-representation-methods">4. Surface Representation Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#explicit-mesh-models">Explicit Mesh Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-function-models">Implicit Function Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#motion-capture-and-behavior-synthesis">5. Motion Capture and Behavior Synthesis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#capturing-human-motion">Capturing Human Motion</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#behavior-synthesis">Behavior Synthesis</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#clothing-modeling">6. Clothing Modeling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physically-based-simulation">Physically-Based Simulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#data-driven-approaches">Data-Driven Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-clothing-models">Implicit Clothing Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#human-object-interaction">7. Human-Object Interaction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physics-based-methods">Physics-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#learning-based-approaches">Learning-Based Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#hybrid-systems">Hybrid Systems</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#applications">8. Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#entertainment-and-media">Entertainment and Media</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#healthcare-and-biomechanics">Healthcare and Biomechanics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#engineering-and-design">Engineering and Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#human-computer-interaction">Human-Computer Interaction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#scientific-research">Scientific Research</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#challenges-and-future-directions">9. Challenges and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#scalability-and-generalization">Scalability and Generalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#higher-fidelity-dynamics">Higher-Fidelity Dynamics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#data-and-labeling-constraints">Data and Labeling Constraints</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physics-and-learning-integration">Physics and Learning Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#semantic-and-cognitive-aspects">Semantic and Cognitive Aspects</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#realism-vs-controllability">Realism vs. Controllability</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_02_1_image_formation.html">Lecture 02.1 – Image Formation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#historical-developments-in-image-formation">1. Historical Developments in Image Formation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#ancient-and-medieval-optics-camera-obscura">Ancient and Medieval Optics – Camera Obscura</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#renaissance-perspective-and-geometry">Renaissance Perspective and Geometry</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#early-cameras-and-photographic-imaging">Early Cameras and Photographic Imaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#modern-developments">Modern Developments</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#the-pinhole-camera-model">2. The Pinhole Camera Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#coordinate-setup">Coordinate Setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#proof-by-similar-triangles">Proof by Similar Triangles</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#numerical-example">Numerical Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#inadequacy-of-a-simple-pinhole">Inadequacy of a Simple Pinhole</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#camera-intrinsics-and-the-projection-matrix">3. Camera Intrinsics and the Projection Matrix</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#extrinsic-parameters">Extrinsic Parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#full-projection-example">Full Projection Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#image-distortions-correction">4. Image Distortions &amp; Correction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#properties-of-perspective-projection">5. Properties of Perspective Projection</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#advanced-theoretical-extensions">6. Advanced Theoretical Extensions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#light-field-imaging-and-plenoptic-cameras">Light Field Imaging and Plenoptic Cameras</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#non-conventional-imaging-techniques">Non-Conventional Imaging Techniques</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#applications-in-modern-vision-and-graphics">7. Applications in Modern Vision and Graphics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#computer-vision-and-3d-reconstruction">Computer Vision and 3D Reconstruction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#medical-imaging">Medical Imaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#photorealistic-rendering-in-computer-graphics">Photorealistic Rendering in Computer Graphics</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#python-example-simulating-image-formation">8. Python Example: Simulating Image Formation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html">Lecture 02.2 – Rotations and Kinematic Chains</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#representations-of-3d-rotations">1. Representations of 3D Rotations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#a-rotation-matrices">A) Rotation Matrices</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#b-euler-angles">B) Euler Angles</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#c-quaternions">C) Quaternions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#lie-algebra-so-3-and-exponential-map">2. Lie Algebra <span class="math notranslate nohighlight">\(so(3)\)</span> and Exponential Map</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#rodrigues-rotation-formula">3. Rodrigues’ Rotation Formula</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#kinematic-chains-forward-inverse-kinematics">4. Kinematic Chains: Forward &amp; Inverse Kinematics</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#comparison-of-rotation-representations">Comparison of Rotation Representations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_03_1_surface_representations.html">Lecture 03.1 – Surface Representations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#mathematical-foundations-of-surface-representations">1. Mathematical Foundations of Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-parametric-surfaces">A) Parametric Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-implicit-surfaces">B) Implicit Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-explicit-surfaces">C) Explicit Surfaces</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#surface-differential-properties">2. Surface Differential Properties</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-surface-normals">A) Surface Normals</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-fundamental-forms-and-curvature">B) Fundamental Forms and Curvature</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-geodesics">C) Geodesics</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#discrete-surface-representations">3. Discrete Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-polygon-meshes">A) Polygon Meshes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-point-clouds">B) Point Clouds</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-signed-distance-fields-sdf">C) Signed Distance Fields (SDF)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#advanced-surface-representations">4. Advanced Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-bezier-curves-and-surfaces">A) Bézier Curves and Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-b-splines-and-nurbs">B) B-Splines and NURBS</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-subdivision-surfaces">C) Subdivision Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-level-sets">D) Level Sets</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#e-neural-implicit-representations">E) Neural Implicit Representations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#comparative-analysis-and-applications">5. Comparative Analysis and Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-computational-efficiency-and-storage">A) Computational Efficiency and Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-practical-applications">B) Practical Applications</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-operations-complexity">C) Operations Complexity</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#implementation-examples">6. Implementation Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-basic-mesh-processing-python">A) Basic Mesh Processing (Python)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-implicit-surface-utilities">B) Implicit Surface Utilities</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-bezier-curve-implementation">C) Bézier Curve Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-curvature-estimation-on-meshes">D) Curvature Estimation on Meshes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#advanced-topics-and-future-directions">7. Advanced Topics and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-multi-resolution-representations">A) Multi-Resolution Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-machine-learning-for-geometry">B) Machine Learning for Geometry</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-dynamic-surfaces">C) Dynamic Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-non-manifold-geometries">D) Non-Manifold Geometries</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html">Lecture 03.2 – Procrustes Alignment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#goal-learning-a-model-of-pose-and-shape">Goal: Learning a Model of Pose and Shape</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#the-challenge-of-registration">The Challenge of Registration</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#surface-representation-mesh">Surface Representation: Mesh</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#the-procrustes-alignment-problem-mathematical-formulation">The Procrustes Alignment Problem: Mathematical Formulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#rigid-transformations">Rigid Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#procrustes-alignment-solution">Procrustes Alignment Solution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#decoupling-translation-by-centroid-alignment">Decoupling Translation by Centroid Alignment</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#optimal-rotation-via-svd">Optimal Rotation via SVD</a><ul>
<li class="toctree-l4"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#reflection-adjustment">Reflection Adjustment</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#optimal-scale-optional">Optimal Scale (Optional)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#complete-mathematical-derivation">Complete Mathematical Derivation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#translation-derivation">Translation Derivation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#rotation-derivation">Rotation Derivation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#scale-derivation">Scale Derivation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#summary-of-procrustes-alignment-algorithm">Summary of Procrustes Alignment Algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#python-implementation-example">Python Implementation Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#practical-applications">Practical Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#interactive-visualization-ideas">Interactive Visualization Ideas</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_04_1_icp.html">Lecture 4.1: Iterative Closest Point</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#introduction-to-shape-alignment-and-registration">Introduction to Shape Alignment and Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#the-registration-problem">The Registration Problem</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#review-procrustes-analysis">Review: Procrustes Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#problem-unknown-correspondences">Problem: Unknown Correspondences</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#the-iterative-closest-point-icp-algorithm">The Iterative Closest Point (ICP) Algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#basic-icp-algorithm">Basic ICP Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#computational-considerations">Computational Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="lecture_04_1_icp.html#closest-point-computation">Closest Point Computation</a></li>
<li class="toctree-l4"><a class="reference internal" href="lecture_04_1_icp.html#convergence-and-local-minima">Convergence and Local Minima</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#point-to-point-vs-point-to-plane-icp">Point-to-Point vs. Point-to-Plane ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#point-to-point-icp">Point-to-Point ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#point-to-plane-icp">Point-to-Plane ICP</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#gradient-based-icp-for-non-rigid-registration">Gradient-based ICP for Non-Rigid Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#gradient-based-icp-algorithm">Gradient-based ICP Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#advantages-of-gradient-based-icp">Advantages of Gradient-based ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#computing-gradients">Computing Gradients</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#improving-icp-s-robustness">Improving ICP’s Robustness</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#data-association-direction">Data Association Direction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#robust-cost-functions">Robust Cost Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#trimmed-icp">Trimmed ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#ransac-based-approaches">RANSAC-based Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#additional-information">Additional Information</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#point-to-surface-distance">Point-to-Surface Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#icp-variants-and-extensions">ICP Variants and Extensions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#generalized-icp-gicp">Generalized ICP (GICP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#em-icp-and-probabilistic-approaches">EM-ICP and Probabilistic Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#coherent-point-drift-cpd">Coherent Point Drift (CPD)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#multi-scale-approaches">Multi-Scale Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#applications-of-icp">Applications of ICP</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#implementing-icp">Implementing ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#efficient-python-implementation">Efficient Python Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#practical-tips">Practical Tips</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_04_2_body_models.html">Lecture 04.2 - Body Models: Vertex-Based Models and SMPL</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#body-models-as-parameterized-functions">1. Body Models as Parameterized Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#rotations-articulation-and-pose-representation">2. Rotations, Articulation, and Pose Representation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#rotation-representation">2.1 Rotation Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#kinematic-chain">2.2 Kinematic Chain</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#linear-blend-skinning-and-its-limitations">3. Linear Blend Skinning and its Limitations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#linear-blend-skinning">3.1 Linear Blend Skinning</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#problems-with-standard-lbs">3.2 Problems with Standard LBS</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#blend-shapes-for-correcting-lbs-artifacts">3.3 Blend Shapes for Correcting LBS Artifacts</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#the-smpl-body-model">4. The SMPL Body Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#smpl-philosophy">4.1 SMPL Philosophy</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#smpl-model-architecture">4.2 SMPL Model Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#shape-blend-shapes">4.2.1 Shape Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#pose-blend-shapes">4.2.2 Pose Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#joint-regression">4.2.3 Joint Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#model-training">4.3 Model Training</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#comparison-with-scape">5. Comparison with SCAPE</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#the-scape-model">5.1 The SCAPE Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#different-approaches-to-deformation">5.2 Different Approaches to Deformation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#performance-comparison">5.3 Performance Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#other-advantages">5.4 Other Advantages</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#alignment-techniques-procrustes-analysis-and-icp">6. Alignment Techniques: Procrustes Analysis and ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#procrustes-analysis">6.1 Procrustes Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#iterative-closest-point-icp">6.2 Iterative Closest Point (ICP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#fitting-smpl-to-scans">6.3 Fitting SMPL to Scans</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#image-formation-and-the-pinhole-camera-model">7. Image Formation and the Pinhole Camera Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#the-pinhole-camera-model">7.1 The Pinhole Camera Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#lens-distortion">7.2 Lens Distortion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#extensions-and-advanced-applications">8. Extensions and Advanced Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#dynamic-soft-tissue-modeling">8.1 Dynamic Soft Tissue Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#specialized-extensions">8.2 Specialized Extensions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#deep-learning-for-model-fitting">8.3 Deep Learning for Model Fitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#probabilistic-approaches">8.4 Probabilistic Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#hybrid-models">8.5 Hybrid Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_05_1_body_model_training.html">Lecture 5.1 - Training a Body Model and Fitting SMPL to Scans</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#body-models-based-on-triangle-deformations">Body Models Based on Triangle Deformations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#scape-and-blendscape-models">SCAPE and BlendSCAPE Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#triangle-deformation-process">Triangle Deformation Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#comparison-smpl-vs-scape-blendscape">Comparison: SMPL vs. SCAPE/BlendSCAPE</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#training-a-body-model-from-registrations">Training a Body Model from Registrations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#the-challenge-of-raw-scan-data">The Challenge of Raw Scan Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#training-from-registrations">Training from Registrations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#obtaining-registrations-fitting-smpl-to-scans">Obtaining Registrations: Fitting SMPL to Scans</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#non-rigid-registration-process">Non-Rigid Registration Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#iterative-closest-point-icp-review">Iterative Closest Point (ICP) Review</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#registration-objective-formulation">Registration Objective Formulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#point-to-surface-distance">Point-to-Surface Distance</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#multi-stage-optimization-strategy">Multi-Stage Optimization Strategy</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#joint-registration-and-model-training">Joint Registration and Model Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#co-registration-approach">Co-Registration Approach</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Lecture 05.2 - 3D Registration: From Classical ICP to Modern Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#rigid-registration-and-the-icp-algorithm">1. Rigid Registration and the ICP Algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#the-iterative-closest-point-icp-algorithm">The Iterative Closest Point (ICP) Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="#convergence-analysis-and-failure-modes">Convergence Analysis and Failure Modes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#classical-non-rigid-registration">2. Classical Non-Rigid Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#thin-plate-spline-robust-point-matching-tps-rpm">Thin Plate Spline Robust Point Matching (TPS-RPM)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#coherent-point-drift-cpd">Coherent Point Drift (CPD)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#other-non-rigid-methods">Other Non-Rigid Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#parametric-models-and-the-smpl-body-model">3. Parametric Models and the SMPL Body Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#model-structure">Model Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="#shape-blend-shapes-identity-variation">Shape Blend Shapes (Identity Variation)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pose-blend-shapes-pose-dependent-deformation">Pose Blend Shapes (Pose-Dependent Deformation)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#linear-blend-skinning-lbs-for-articulation">Linear Blend Skinning (LBS) for Articulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#learning-smpl">Learning SMPL</a></li>
<li class="toctree-l3"><a class="reference internal" href="#using-smpl-for-registration">Using SMPL for Registration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#modeling-clothing-and-fine-details-smpl-d">4. Modeling Clothing and Fine Details: SMPL+D</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#why-smpl-d">Why SMPL+D?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#how-displacements-are-applied">How Displacements Are Applied</a></li>
<li class="toctree-l3"><a class="reference internal" href="#limitations">Limitations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#applications">Applications</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#survey-of-3d-registration-methods-from-icp-to-deep-learning">5. Survey of 3D Registration Methods: From ICP to Deep Learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#early-pioneering-works-1990s-foundational-rigid-registration">5.1 Early Pioneering Works (1990s) – Foundational Rigid Registration</a></li>
<li class="toctree-l3"><a class="reference internal" href="#the-2000s-robust-and-non-rigid-registration-emerges">5.2 The 2000s – Robust and Non-Rigid Registration Emerges</a></li>
<li class="toctree-l3"><a class="reference internal" href="#s-template-based-and-parametric-model-registration">5.3 2010s – Template-based and Parametric Model Registration</a></li>
<li class="toctree-l3"><a class="reference internal" href="#s-learning-based-parametric-registration-and-hybrid-approaches">5.4 2020s – Learning-Based Parametric Registration and Hybrid Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html">Lecture 06.1 - Fitting the SMPL Model to Images via Optimization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#mathematical-background-pinhole-camera-and-projections">Mathematical Background: Pinhole Camera and Projections</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#perspective-projection">Perspective Projection</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#weak-perspective-projection">Weak-Perspective Projection</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#camera-extrinsics-vs-model-pose">Camera Extrinsics vs. Model Pose</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#d-keypoints-and-projection">2D Keypoints and Projection</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#the-smpl-model-as-a-differentiable-function-of-shape-and-pose">The SMPL Model as a Differentiable Function of Shape and Pose</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#shape-blend-shapes">Shape Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#pose-blend-shapes">Pose Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#joint-positions">Joint Positions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#linear-blend-skinning">Linear Blend Skinning</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#differentiability-of-smpl">Differentiability of SMPL</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#fitting-smpl-to-images-via-optimization-smplify">Fitting SMPL to Images via Optimization (SMPLify)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#objective-function">Objective Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#combined-objective">Combined Objective</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#optimization-strategy">Optimization Strategy</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#optimization-algorithms">Optimization Algorithms</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#automatic-differentiation-and-jacobians">Automatic Differentiation and Jacobians</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#result-of-smplify">Result of SMPLify</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#historical-progression-and-method-comparisons">Historical Progression and Method Comparisons</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#smplify-bogo-et-al-2016">SMPLify (Bogo et al. 2016)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#smplify-x-2019">SMPLify-X (2019)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html">Lecture 06.2 - Learning-Based Fitting of the SMPL Model to Images</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#foundations-of-learning-based-smpl-estimation">Foundations of Learning-Based SMPL Estimation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#integrating-smpl-into-neural-networks">Integrating SMPL into Neural Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#projection-functions">Projection Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#loss-functions">Loss Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#statistical-priors-and-adversarial-losses">Statistical Priors and Adversarial Losses</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#early-regression-approaches-hmr-and-nbf">Early Regression Approaches: HMR and NBF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#human-mesh-recovery-hmr">Human Mesh Recovery (HMR)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#neural-body-fitting-nbf">Neural Body Fitting (NBF)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#evolving-architectures-hybrid-and-improved-regression-methods">Evolving Architectures: Hybrid and Improved Regression Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#spin-optimization-in-the-training-loop">SPIN: Optimization in the Training Loop</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#pymaf-pyramidal-mesh-alignment-feedback">PyMAF: Pyramidal Mesh Alignment Feedback</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#cliff-using-full-frame-context-for-camera-orientation">CLIFF: Using Full-Frame Context for Camera Orientation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#pixie-whole-body-regression-with-part-experts">PIXIE: Whole-Body Regression with Part Experts</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#temporal-methods-from-single-images-to-video-sequences">Temporal Methods: From Single Images to Video Sequences</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#vibe-adversarial-motion-prior-with-grus">VIBE: Adversarial Motion Prior with GRUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#tcmr-temporally-consistent-mesh-recovery">TCMR: Temporally Consistent Mesh Recovery</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#motionbert-transformer-based-motion-representations">MotionBERT: Transformer-Based Motion Representations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#comparison-of-learning-based-methods">Comparison of Learning-Based Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#supervision-and-data">Supervision and Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#network-architecture">Network Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#objective-functions">Objective Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#pose-and-shape-priors">Pose and Shape Priors</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#performance">Performance</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#runtime">Runtime</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#strengths-and-weaknesses">Strengths and Weaknesses</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html">Lecture 07.1: Fitting SMPL to IMU Data Using Optimization-Based Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#classical-imu-based-pose-estimation-a-historical-perspective">Classical IMU-Based Pose Estimation: A Historical Perspective</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#attitude-and-heading-reference-systems">Attitude and Heading Reference Systems</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#kalman-filter-approaches">Kalman Filter Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#early-sparse-sensor-approaches">Early Sparse-Sensor Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#model-based-optimization-methods">Model-Based Optimization Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#learning-based-methods">Learning-Based Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#inertial-sensor-fundamentals-and-orientation-representations">Inertial Sensor Fundamentals and Orientation Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#gravity-alignment-and-drift-correction">Gravity Alignment and Drift Correction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#orientation-representations">Orientation Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#sensor-calibration">Sensor Calibration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#optimization-based-smpl-fitting-with-imu-data">Optimization-Based SMPL Fitting with IMU Data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#kinematic-model-and-sensor-prediction">Kinematic Model and Sensor Prediction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#regularization-and-prior-terms">Regularization and Prior Terms</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#gradient-and-jacobian-computation">Gradient and Jacobian Computation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#pseudocode-smpl-pose-estimation-from-imu-sequence">Pseudocode: SMPL Pose Estimation from IMU Sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#imu-based-human-pose-datasets-and-resources">IMU-Based Human Pose Datasets and Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#dip-imu-dataset-2018">DIP-IMU Dataset (2018)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#totalcapture-2017">TotalCapture (2017)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#amass-2019">AMASS (2019)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#other-datasets-and-resources">Other Datasets and Resources</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html">Lecture 07.2: Fitting SMPL to IMU Data Using Learning-Based Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#optimization-based-vs-learning-based-approaches">Optimization-Based vs. Learning-Based Approaches</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#learning-based-imu-to-pose-estimation-historical-overview-of-key-models">Learning-Based IMU-to-Pose Estimation: Historical Overview of Key Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#deep-inertial-poser-dip-2018">Deep Inertial Poser (DIP, 2018)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#transpose-2021">TransPose (2021)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#transformer-inertial-poser-tip-2022">Transformer Inertial Poser (TIP, 2022)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#physics-physical-inertial-poser-pip-2022">Physics/Physical Inertial Poser (PIP, 2022)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#other-notable-models-and-developments">Other Notable Models and Developments</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#problem-formulation-and-learning-task-definition">Problem Formulation and Learning Task Definition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#input-and-output-representations">Input and Output Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#learning-objective-and-loss-functions">Learning Objective and Loss Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#temporal-modeling-approaches">Temporal Modeling Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#supervised-vs-semi-supervised-training-synthetic-data">Supervised vs. Semi-Supervised Training; Synthetic Data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#model-architectures-and-design-considerations">Model Architectures and Design Considerations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#encoding-imu-measurements">Encoding IMU Measurements</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#network-structures">Network Structures</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#training-pipeline-and-pseudocode">Training Pipeline and Pseudocode</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#datasets-benchmarks-and-resources">Datasets, Benchmarks, and Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#dip-imu-dataset-2018">DIP-IMU Dataset (2018)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#totalcapture-2017">TotalCapture (2017)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#amass-2019">AMASS (2019)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#other-datasets-and-resources">Other Datasets and Resources</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#challenges-and-outlook">Challenges and Outlook</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html">Lecture 08.1: Vertex-Based Clothing Modeling for Virtual Humans</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#clothing-representation-as-vertex-displacements">Clothing Representation as Vertex Displacements</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#registration-of-clothed-human-scans">Registration of Clothed Human Scans</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#estimating-body-shape-under-clothing-the-buff-method">Estimating Body Shape Under Clothing: The BUFF Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#multi-layer-clothing-capture-the-clothcap-method">Multi-Layer Clothing Capture: The ClothCap Method</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#data-term">Data Term</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#boundary-term">Boundary Term</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#boundary-smoothness">Boundary Smoothness</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#laplacian-smoothness">Laplacian Smoothness</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#applications-of-multi-layer-registration">Applications of Multi-Layer Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#advantages-and-limitations">Advantages and Limitations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html">Lecture 09.1: Neural Implicit and Point-Based Representations for Clothed Human Modeling</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#background-explicit-vs-implicit-vs-point-based-representations">Background: Explicit vs. Implicit vs. Point-Based Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#mesh-based-models">Mesh-Based Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#point-based-models">Point-Based Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#neural-implicit-representations">Neural Implicit Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#neural-radiance-fields-nerfs-for-humans">Neural Radiance Fields (NeRFs) for Humans</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#hybrid-approaches">Hybrid Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#expressiveness-and-topology">Expressiveness and Topology</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#differentiability-and-learning">Differentiability and Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#data-efficiency-and-performance">Data Efficiency and Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#neural-implicit-function-foundations">Neural Implicit Function Foundations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#signed-distance-field-sdf">Signed Distance Field (SDF)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#occupancy-field-indicator-function">Occupancy Field (Indicator Function)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#volume-radiance-field">Volume Radiance Field</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#articulated-deformation-fields-for-implicit-models">Articulated Deformation Fields for Implicit Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#backward-warping-inverse-skinning-field">Backward Warping (Inverse Skinning Field)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#forward-warping-forward-skinning-field">Forward Warping (Forward Skinning Field)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#pose-dependent-deformation-secondary-motion">Pose-Dependent Deformation (Secondary Motion)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#generative-implicit-models-for-clothed-bodies">Generative Implicit Models for Clothed Bodies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#smplicit-topology-aware-clothed-human-model">SMPLicit: Topology-Aware Clothed Human Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#imghum-implicit-generative-human-model">imGHUM: Implicit Generative Human Model</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#pose-dependent-implicit-models-and-animatable-avatars">Pose-Dependent Implicit Models and Animatable Avatars</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#nasa-neural-articulated-shape-approximation-eccv-2020">NASA: Neural Articulated Shape Approximation (ECCV 2020)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#scanimate-weakly-supervised-skinned-avatar-networks-cvpr-2021">SCANimate: Weakly-Supervised Skinned Avatar Networks (CVPR 2021)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#neural-gif-neural-generalized-implicit-functions-iccv-2021">Neural-GIF: Neural Generalized Implicit Functions (ICCV 2021)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#snarf-skinned-neural-articulated-implicit-shapes-iccv-2021">SNARF: Skinned Neural Articulated Implicit Shapes (ICCV 2021)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#pop-the-power-of-points-iccv-2021-point-based-modeling">POP: The Power of Points (ICCV 2021) – Point-Based Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#other-noteworthy-methods">Other Noteworthy Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#blueprint-algorithms-for-key-methods">Blueprint Algorithms for Key Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#snarf-training-procedure-differentiable-forward-skinning-for-implicit-surfaces">SNARF (Training Procedure): Differentiable Forward Skinning for Implicit Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#pop-training-fitting-pipeline-point-based-model-for-pose-dependent-clothing">POP (Training &amp; Fitting Pipeline): Point-Based Model for Pose-Dependent Clothing</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#smplicit-inference-fitting-workflow-generative-implicit-garment-model-conditioned-on-smpl">SMPLicit (Inference/Fitting Workflow): Generative Implicit Garment Model conditioned on SMPL</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#historical-perspective-and-future-outlook">Historical Perspective and Future Outlook</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#challenges-and-future-directions">Challenges and Future Directions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="extended_materials_neural_radiance_fields.html">Neural Radiance Fields: A Historical and Theoretical Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#foundations-3d-scene-representation-and-reconstruction-techniques">Foundations: 3D Scene Representation and Reconstruction Techniques</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#voxel-grids">Voxel Grids</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#point-clouds">Point Clouds</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#mesh-based-surfaces">Mesh-Based Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#light-fields-and-volumetric-rendering">Light Fields and Volumetric Rendering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#emergence-of-neural-radiance-fields-nerf">Emergence of Neural Radiance Fields (NeRF)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#core-idea">Core Idea</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#training-procedure">Training Procedure</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#nerf-architecture">NeRF Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#hierarchical-sampling">Hierarchical Sampling</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#original-results">Original Results</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#theoretical-and-mathematical-analysis-of-nerf">Theoretical and Mathematical Analysis of NeRF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#volume-rendering-formulation-in-nerf">Volume Rendering Formulation in NeRF</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#positional-encoding-and-neural-network-architecture">Positional Encoding and Neural Network Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#loss-function-and-optimization">Loss Function and Optimization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#major-advancements-and-extensions-of-nerf">Major Advancements and Extensions of NeRF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#anti-aliasing-and-unbounded-scenes-mip-nerf-and-nerf">Anti-Aliasing and Unbounded Scenes: mip-NeRF and NeRF++</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#efficiency-improvements-instant-nerf-and-plenoctrees">Efficiency Improvements: Instant NeRF and PlenOctrees</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#dynamic-and-deformable-nerfs-d-nerf-nerfies-nsff-etc">Dynamic and Deformable NeRFs (D-NeRF, Nerfies, NSFF, etc.)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#neural-radiance-fields-for-human-modeling-with-smpl-and-body-models">Neural Radiance Fields for Human Modeling (with SMPL and Body Models)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#other-notable-extensions">Other Notable Extensions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#comparison-with-other-3d-representations">Comparison with Other 3D Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-polygonal-meshes">Vs. Polygonal Meshes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-point-clouds-3d-splatting">Vs. Point Clouds / 3D Splatting</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-voxel-grids-and-volumetric-methods">Vs. Voxel Grids and Volumetric Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-multi-plane-images-mpis-light-fields">Vs. Multi-Plane Images (MPIs) / Light Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#accuracy-and-fidelity">Accuracy and Fidelity</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#applicability">Applicability</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#datasets-for-nerf-training-and-evaluation">Datasets for NeRF Training and Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#blender-synthetic-nerf-dataset">Blender Synthetic NeRF Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#local-light-field-fusion-llff-real-forward-facing-dataset">Local Light Field Fusion (LLFF) Real Forward-Facing Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#tanks-and-temples">Tanks and Temples</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#dtu-dataset">DTU Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#human3-6m">Human3.6M</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#zju-mocap-dataset">ZJU-MoCap Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#people-snapshot">People-Snapshot</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#synthetic-dynamic-scenes">Synthetic dynamic scenes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#other-datasets">Other datasets</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html">3D Gaussian Splatting: A Basic Introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#foundations">Foundations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#what-is-a-3d-scene">What is a 3D Scene?</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-scene-representation">3D Scene Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#computer-graphics-fundamentals">Computer Graphics Fundamentals</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#rasterization-and-ray-tracing">Rasterization and Ray Tracing</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#alpha-blending-and-compositing">Alpha Blending and Compositing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#the-evolution-of-novel-view-synthesis">The Evolution of Novel View Synthesis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#image-based-rendering">Image-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#structure-from-motion-and-multi-view-stereo">Structure-from-Motion and Multi-View Stereo</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#point-based-rendering">Point-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#neural-rendering">Neural Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#accelerated-neural-fields">Accelerated Neural Fields</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussian-splatting-a-convergence-of-approaches">3D Gaussian Splatting: A Convergence of Approaches</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#id1">Point-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#point-clouds-and-their-challenges">Point Clouds and Their Challenges</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#the-concept-of-splatting">The Concept of Splatting</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#elliptical-weighted-average-ewa-filtering">Elliptical Weighted Average (EWA) Filtering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-point-based-rendering">Differentiable Point-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussian-splatting-core-principles">3D Gaussian Splatting: Core Principles</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#key-insight-unifying-points-and-volumes">Key Insight: Unifying Points and Volumes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussians-as-scene-primitives">3D Gaussians as Scene Primitives</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#the-volumetric-rendering-equation">The Volumetric Rendering Equation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#alpha-compositing-with-gaussians">Alpha Compositing with Gaussians</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#mathematical-formulation-of-3d-gaussian-splatting">Mathematical Formulation of 3D Gaussian Splatting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#projecting-3d-gaussians-to-2d">Projecting 3D Gaussians to 2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#parameterization-of-3d-gaussians">Parameterization of 3D Gaussians</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#view-dependent-appearance">View-Dependent Appearance</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-rendering-equations">Differentiable Rendering Equations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#training-and-optimization">Training and Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#photometric-loss">Photometric Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#initial-point-cloud">Initial Point Cloud</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#optimization-process">Optimization Process</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-splatting-pipeline">Differentiable Splatting Pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#adaptive-density-control">Adaptive Density Control</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#implementation-and-real-time-rendering">Implementation and Real-Time Rendering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#tile-based-rendering">Tile-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#fast-sorting-strategies">Fast Sorting Strategies</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#gpu-accelerated-rasterization">GPU-Accelerated Rasterization</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#memory-considerations">Memory Considerations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#comparison-with-other-methods">Comparison with Other Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#nerf-vs-3d-gaussian-splatting">NeRF vs. 3D Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#voxel-based-representations-vs-gaussians">Voxel-Based Representations vs. Gaussians</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#traditional-point-based-rendering-vs-gaussian-splatting">Traditional Point-Based Rendering vs. Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#applications-and-extensions">Applications and Extensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#static-scene-reconstruction">Static Scene Reconstruction</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#dynamic-scene-capture">Dynamic Scene Capture</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#avatar-creation-and-animation">Avatar Creation and Animation</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#integration-with-neural-rendering">Integration with Neural Rendering</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#large-scale-scene-rendering">Large-Scale Scene Rendering</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#bezier-gaussian-triangles-bg-triangle-for-sharper-rendering">Bézier Gaussian Triangles (BG-Triangle) for Sharper Rendering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#representation">Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#performance">Performance</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#human-reconstruction-with-gaussian-splatting-and-priors">Human Reconstruction with Gaussian Splatting and Priors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#eg-humannerf-efficient-generalizable-human-nerf">EG-HumanNeRF: Efficient Generalizable Human NeRF</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#gps-gaussian-pixel-wise-gaussian-splatting-for-humans">GPS-Gaussian: Pixel-Wise Gaussian Splatting for Humans</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#generalizable-human-gaussians-ghg-with-smpl">Generalizable Human Gaussians (GHG) with SMPL</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#dynamic-scene-reconstruction-with-4d-gaussian-splatting">Dynamic Scene Reconstruction with 4D Gaussian Splatting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussian-splatting-4dgs">4D Gaussian Splatting (4DGS)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#speed-and-memory-enhancements-4dgs-1k-and-mega">Speed and Memory Enhancements (4DGS-1K and MEGA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#applications-to-mocap-and-4d-human-rendering">Applications to MoCap and 4D Human Rendering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#implementation-details-and-real-time-performance">Implementation Details and Real-Time Performance</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#data-structures">Data Structures</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#rasterization-shaders">Rasterization &amp; Shaders</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#gpu-memory-and-throughput">GPU Memory and Throughput</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-rendering-implementation">Differentiable Rendering Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#training-vs-inference-compute">Training vs. Inference Compute</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#accuracy-vs-speed-trade-offs">Accuracy vs. Speed trade-offs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#benchmarks-and-comparative-evaluation">Benchmarks and Comparative Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#static-scene-comparison">Static Scene Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#human-novel-view-comparison">Human Novel-View Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#dynamic-scene-comparison">Dynamic Scene Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#comparative-summary-table">Comparative Summary Table</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#datasets-and-resources">Datasets and Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#synthetic-nerf-dataset-blender-scenes">Synthetic NeRF Dataset (Blender Scenes)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#llff-local-light-field-fusion">LLFF (Local Light Field Fusion)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#tanks-and-temples">Tanks and Temples</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#multi-object-360-co3d-common-objects-in-3d">Multi-Object 360 (CO3D - Common Objects in 3D)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#amass-archive-of-motion-capture-as-surface-shapes">AMASS (Archive of Motion Capture as Surface Shapes)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#cape-clothed-auto-person-encoding">CAPE (Clothed Auto Person Encoding)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#thuman-thuman2-0">THuman / THuman2.0</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#renderpeople">RenderPeople</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#open-source-implementations">Open-Source Implementations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#future-directions">Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#improved-compression-techniques-for-memory-efficiency">Improved Compression Techniques for Memory Efficiency</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#handling-dynamic-and-deformable-scenes">Handling Dynamic and Deformable Scenes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#advanced-material-modeling-for-realistic-rendering">Advanced Material Modeling for Realistic Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#hybrid-approaches-integrating-neural-fields-and-explicit-representations">Hybrid Approaches Integrating Neural Fields and Explicit Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#scalability-for-large-scale-scenes-city-level-and-beyond">Scalability for Large-Scale Scenes (City-Level and Beyond)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#real-time-applications-in-ar-vr-and-gaming">Real-Time Applications in AR/VR and Gaming</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#integration-with-existing-graphics-pipelines">Integration with Existing Graphics Pipelines</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#learning-from-limited-data">Learning from Limited Data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#conclusion">Conclusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#glossary">Glossary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a><ul>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-01-1-historical-body-models">Lecture 01.1 (Historical Body Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-01-2-introduction-to-human-models">Lecture 01.2 (Introduction to Human Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-01-3-introduction-to-human-models-continued">Lecture 01.3 (Introduction to Human Models Continued)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-02-1-image-formation">Lecture 02.1 (Image Formation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-02-2-rotations-kinematic-chains">Lecture 02.2 (Rotations &amp; Kinematic Chains)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-03-1-surface-representations">Lecture 03.1 (Surface Representations)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-03-2-procrustes-alignment">Lecture 03.2 (Procrustes Alignment)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-04-1-iterative-closest-points">Lecture 04.1 (Iterative Closest Points)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-04-2-body-models">Lecture 04.2 (Body Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-05-1-body-model-training">Lecture 05.1 (Body Model Training)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-05-2-3d-registration">Lecture 05.2 (3D Registration)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-06-1-fitting-smpl-to-images">Lecture 06.1 (Fitting SMPL to Images)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-06-1-optimization-based-fitting-of-smpl-to-images">Lecture 06.1 (Optimization-Based Fitting of SMPL to Images)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-06-2-learning-based-fitting-of-smpl-to-images">Lecture 06.2 (Learning-Based Fitting of SMPL to Images)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-07-1-fitting-smpl-to-imu-optimization">Lecture 07.1 (Fitting SMPL to IMU Optimization)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-07-2-fitting-smpl-to-imu-learning">Lecture 07.2 (Fitting SMPL to IMU Learning)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="references.html#classic-and-optimization-based-methods">Classic and Optimization-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="references.html#learning-based-methods">Learning-Based Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="references.html#datasets-and-resources">Datasets and Resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#relevant-software-and-libraries">Relevant Software and Libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-08-1-references-for-vertex-based-clothing-modeling-for-virtual-humans">Lecture 08.1: References for Vertex-Based Clothing Modeling for Virtual Humans</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#body-models-and-shape-estimation">Body Models and Shape Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#alternative-representations">Alternative Representations</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#datasets">Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#software-and-libraries">Software and Libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-09-1-references-for-neural-implicit-and-point-based-representations-for-clothed-human-modeling">Lecture 09.1: References for Neural Implicit and Point-Based Representations for Clothed Human Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#point-based-models">Point-Based Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#id50">Datasets and Resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#related-software-and-libraries">Related Software and Libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#review-papers-and-tutorials">Review Papers and Tutorials</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#neural-radiance-fields-nerf">Neural Radiance Fields (NERF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#gaussian-splatting">Gaussian Splatting</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Virtual Humans Lecture</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Lecture 05.2 - 3D Registration: From Classical ICP to Modern Methods</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/lecture_05_2_3d_registration.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="lecture-05-2-3d-registration-from-classical-icp-to-modern-methods">
<span id="lecture-05-2-3d-registration"></span><h1>Lecture 05.2 - 3D Registration: From Classical ICP to Modern Methods<a class="headerlink" href="#lecture-05-2-3d-registration-from-classical-icp-to-modern-methods" title="Link to this heading"></a></h1>
<iframe width="600" height="400" src="https://www.youtube.com/embed/NAIyqnAQULE"
title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write;
encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><p><a class="reference external" href="https://virtualhumans.mpi-inf.mpg.de/VH23/slides/pdf/Lecture_05_1_TrainingBodyModel_FittingSMPLtoScans.pdf">Lecture Slides: 3D Registration 5.2.1</a></p>
<iframe width="600" height="400" src="https://www.youtube.com/embed/TvsjvmclAVM"
title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write;
encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><p><a class="reference external" href="https://virtualhumans.mpi-inf.mpg.de/VH23/slides/pdf/Lecture_05_2_Learning_Based_Registration.pdf">Lecture Slides: 3D Registration 5.2.2</a></p>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h2>
<p>3D registration is the process of aligning two or more 3D datasets (such as point clouds or meshes) into a common coordinate frame. Given a “source” shape and a “target” shape, registration finds the spatial transformation that best superimposes the source onto the target. This is a fundamental problem in computer vision and graphics, with applications ranging from scanning and mapping to medical imaging and character animation.</p>
<p>Registration methods can be broadly categorized as rigid (assuming the object is solid and only rotated/translated) or non-rigid (allowing deformation of the object). Over the past few decades, researchers have developed a spectrum of techniques for 3D registration, from classical geometry-based algorithms to sophisticated learned models.</p>
<p>This lecture provides a comprehensive treatment of 3D registration, covering:</p>
<ol class="arabic simple">
<li><p>Rigid Registration and the ICP Algorithm</p></li>
<li><p>Classical Non-Rigid Registration</p></li>
<li><p>Parametric Models and the SMPL Body Model</p></li>
<li><p>Modeling Clothing and Fine Details: SMPL+D</p></li>
<li><p>Survey of 3D Registration Methods: From ICP to Deep Learning</p></li>
</ol>
</section>
<section id="rigid-registration-and-the-icp-algorithm">
<h2>1. Rigid Registration and the ICP Algorithm<a class="headerlink" href="#rigid-registration-and-the-icp-algorithm" title="Link to this heading"></a></h2>
<p>Rigid registration assumes the shapes differ only by a 3D rigid transform (rotation <span class="math notranslate nohighlight">\(\mathbf{R}\in SO(3)\)</span> and translation <span class="math notranslate nohighlight">\(\mathbf{t}\in\mathbb{R}^3\)</span>). The goal is to find <span class="math notranslate nohighlight">\(\mathbf{R},\mathbf{t}\)</span> minimizing a distance between the source and target point sets.</p>
<p>If we have <span class="math notranslate nohighlight">\(N\)</span> points <span class="math notranslate nohighlight">\({\mathbf{x}_i}\)</span> in the source and corresponding points <span class="math notranslate nohighlight">\({\mathbf{y}_i}\)</span> in the target, a common objective is the sum of squared distances:</p>
<div class="math notranslate nohighlight">
\[F(\mathbf{R},\mathbf{t}) = \sum_{i=1}^N\|\mathbf{R}\mathbf{x}_i+\mathbf{t}-\mathbf{y}_i\|^2\]</div>
<p>To minimize <span class="math notranslate nohighlight">\(F\)</span>, we can use the method of Procrustes alignment. The optimal translation is:</p>
<div class="math notranslate nohighlight">
\[\mathbf{t}^* = \bar{\mathbf{y}} - \mathbf{R}\bar{\mathbf{x}}\]</div>
<p>This aligns the centroids <span class="math notranslate nohighlight">\(\bar{\mathbf{x}}, \bar{\mathbf{y}}\)</span>. The optimal rotation <span class="math notranslate nohighlight">\(\mathbf{R}^*\)</span> is found by centering the points and solving a Wahba’s problem. One solution is via singular value decomposition (SVD):</p>
<ol class="arabic simple">
<li><p>Form the cross-covariance matrix <span class="math notranslate nohighlight">\(\mathbf{C} = \sum_i (\mathbf{x}_i - \bar{\mathbf{x}})(\mathbf{y}_i - \bar{\mathbf{y}})^T\)</span></p></li>
<li><p>Compute <span class="math notranslate nohighlight">\(\mathbf{R}^* = \mathbf{U}\cdot\mathrm{diag}(1,\dots,1,\det(\mathbf{U}\mathbf{V}^T))\cdot\mathbf{V}^T\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{U}\mathbf{\Sigma}\mathbf{V}^T = \mathbf{C}\)</span> is the SVD</p></li>
</ol>
<p>This yields the least-squares rigid transform (essentially the algorithm of Umeyama (1991)). If correspondences are known and outlier-free, this solution is global and exact.</p>
<section id="the-iterative-closest-point-icp-algorithm">
<h3>The Iterative Closest Point (ICP) Algorithm<a class="headerlink" href="#the-iterative-closest-point-icp-algorithm" title="Link to this heading"></a></h3>
<p>In practice, correspondences between source and target points are usually unknown. The Iterative Closest Point (ICP) algorithm addresses this by alternating between finding correspondences and updating the transform. Introduced simultaneously by Chen &amp; Medioni (1991) and Besl &amp; McKay (1992), ICP has become “the dominant method for aligning three-dimensional models based purely on geometry.”</p>
<p>The algorithm assumes a reasonably good initial pose and then iteratively:</p>
<ol class="arabic simple">
<li><p>Finds the closest point on the target for each source point (choosing a “corresponding” point pair)</p></li>
<li><p>Updates the transform <span class="math notranslate nohighlight">\((\mathbf{R},\mathbf{t})\)</span> by solving the Procrustes problem for these pairs</p></li>
</ol>
<p>Each iteration non-increasingly improves the alignment error (it is a coordinate-descent on <span class="math notranslate nohighlight">\(F\)</span>), and ICP converges to a local minimum.</p>
<p>In pseudocode form:</p>
<div class="highlight-rst notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span># ICP Algorithm
<span class="linenos">2</span>Initialize R ← R₀, t ← t₀ (initial guess)
<span class="linenos">3</span>
<span class="linenos">4</span>Repeat until convergence:
<span class="linenos">5</span>    For each source point x_i, find its closest point y_j on the target (e.g. using a k-d tree)
<span class="linenos">6</span>    Solve for (R,t) = arg min ∑_i |R·x_i + t - y_{j(i)}|² (closed-form via SVD as above)
<span class="linenos">7</span>
<span class="linenos">8</span>Output final R,t and aligned shapes
</pre></div>
</div>
<p>ICP is simple and empirically effective when the shapes have good initial alignment and sufficient overlap. It is widely used for registering multiple scans from 3D scanners into a single model. Numerous variants improve its robustness and speed:</p>
<ul class="simple">
<li><p><strong>Point-to-plane ICP</strong>: Replaces point-to-point distance with the distance of source points to tangent planes on the target, leading to faster convergence</p></li>
<li><p><strong>Outlier rejection</strong>: Discarding pairs with distance beyond a threshold</p></li>
<li><p><strong>Weighting schemes</strong>: Downweight uncertain correspondences</p></li>
<li><p><strong>Sampling strategies</strong>: Different approaches for selecting subsets of points to match</p></li>
</ul>
<p>Modern implementations (such as in PCL or Open3D libraries) can align point clouds in real time by combining these techniques.</p>
</section>
<section id="convergence-analysis-and-failure-modes">
<h3>Convergence Analysis and Failure Modes<a class="headerlink" href="#convergence-analysis-and-failure-modes" title="Link to this heading"></a></h3>
<p>Under certain conditions (e.g., the shapes are roughly convex and the initial alignment is close), ICP converges linearly to the nearest local optimum. Each iteration’s correspondence step is non-expansive in terms of the error metric, and the alignment step finds the optimal transform for those correspondences, so error decreases or stays the same.</p>
<p>Despite its popularity, ICP has important failure modes:</p>
<ul class="simple">
<li><p>The algorithm guarantees convergence only to a local minimum, so if the initial guess is far from the correct alignment or the shapes have symmetric/ambiguous geometry, ICP may converge to a wrong pose</p></li>
<li><p>It is a greedy approach: the closest-point matching can jump into incorrect correspondences if there are repetitive structures or if one shape is a partial view of the other</p></li>
</ul>
<p>Global methods have been developed to mitigate this, such as Go-ICP (which uses branch-and-bound to find the global optimum), or by using feature-based initial alignment (e.g., matching 3D descriptors like FPFH or spin images before ICP).</p>
<p>In practice, a good heuristic is to run ICP multiple times from random initial perturbed poses (or use a global optimizer for initialization) to increase the chance of finding the global alignment. When successful, ICP produces an accurate rigid transformation (often within sensor noise levels, e.g., millimeter accuracy for object scans).</p>
</section>
</section>
<section id="classical-non-rigid-registration">
<h2>2. Classical Non-Rigid Registration<a class="headerlink" href="#classical-non-rigid-registration" title="Link to this heading"></a></h2>
<p>Rigid alignment is insufficient when the source shape is a deformed version of the target (e.g., different poses of a person, or an object that bends). Non-rigid registration allows more general transformations such as anisotropic scaling or free-form deformations.</p>
<p>Mathematically, we seek a mapping <span class="math notranslate nohighlight">\(T: \mathbb{R}^3 \to \mathbb{R}^3\)</span> (not just rigid) that aligns the shapes. This is an ill-posed problem without regularization – many mappings can align a sparse set of points perfectly, including physically implausible ones. Thus, methods introduce smoothness constraints or parametric deformation models to regularize <span class="math notranslate nohighlight">\(T\)</span>.</p>
<section id="thin-plate-spline-robust-point-matching-tps-rpm">
<h3>Thin Plate Spline Robust Point Matching (TPS-RPM)<a class="headerlink" href="#thin-plate-spline-robust-point-matching-tps-rpm" title="Link to this heading"></a></h3>
<p>TPS-RPM, introduced by Chui and Rangarajan (2003), integrates the Robust Point Matching strategy with a Thin-Plate Spline deformation model. The thin-plate spline is a smooth interpolating function defined by a set of control points; it minimizes bending energy, making it well-suited to model plausible deformations.</p>
<p>TPS-RPM treats correspondences in a soft manner: rather than a binary assignment of each source point to a single target point (like ICP), it introduces a fuzzy correspondence matrix <span class="math notranslate nohighlight">\(P\)</span> where <span class="math notranslate nohighlight">\(P_{ij}\in[0,1]\)</span> indicates the probability or weight of <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> mapping to <span class="math notranslate nohighlight">\(\mathbf{y}_j\)</span>.</p>
<p>An annealing (graduated optimization) scheme is used:</p>
<ol class="arabic simple">
<li><p>Start with very smooth (high regularization, correspondences nearly uniform)</p></li>
<li><p>Gradually allow more flexibility and crisper correspondences</p></li>
<li><p>At each stage, alternate between:
- Solving for the correspondence weights <span class="math notranslate nohighlight">\(P\)</span> (analogous to an E-step, often via softmin of distances)
- Solving for the TPS warp parameters (analogous to an M-step, fitting a smooth spline to the weighted correspondences)</p></li>
</ol>
<p>As the “temperature” lowers, <span class="math notranslate nohighlight">\(P\)</span> tends toward a permutation matrix (hard assignment) and the spline converges to a final mapping.</p>
<p>TPS-RPM was one of the first algorithms to robustly handle non-rigid 2D and 3D point-set alignment without known correspondences. It remains conceptually important as it framed non-rigid registration as a joint optimization of correspondence and shape deformation.</p>
</section>
<section id="coherent-point-drift-cpd">
<h3>Coherent Point Drift (CPD)<a class="headerlink" href="#coherent-point-drift-cpd" title="Link to this heading"></a></h3>
<p>The CPD algorithm, introduced by Myronenko and Song (2010), takes a probabilistic perspective. It assumes one point set (the source) represents the centroids of a Gaussian Mixture Model (GMM), and the other (target) points are observed data points. The alignment is treated as maximizing the likelihood of the target given a transformed source.</p>
<p>A key insight of CPD was to enforce motion coherence: all the Gaussian centroids (source points) move as a group via a smooth deformation field, rather than independently.</p>
<p>In the rigid CPD, this means the centroids move rigidly (the GMM centroid positions are parameterized by a global <span class="math notranslate nohighlight">\(\mathbf{R},\mathbf{t}\)</span>), leading to essentially a probabilistic ICP with an EM algorithm:</p>
<ul class="simple">
<li><p>E-step: Soft assignments of points to GMM centroids</p></li>
<li><p>M-step: Update <span class="math notranslate nohighlight">\(\mathbf{R},\mathbf{t}\)</span> by closed form</p></li>
</ul>
<p>In the non-rigid CPD, the motion of centroids is parameterized by a smooth radial basis function (like Thin-Plate or Gaussian kernel), with a regularization term that penalizes large deviations of neighboring centroids.</p>
<p>The EM formulation naturally yields soft correspondences (via posterior probabilities of Gaussian components) and handles outliers by including a uniform “noise” component in the mixture. CPD tends to preserve the topological structure of the point cloud during deformation (hence “coherent drift”).</p>
</section>
<section id="other-non-rigid-methods">
<h3>Other Non-Rigid Methods<a class="headerlink" href="#other-non-rigid-methods" title="Link to this heading"></a></h3>
<p>Numerous other techniques exist. Non-Rigid ICP (NR-ICP) algorithms extend ICP by incrementally deforming a template with regularization. For example, Amberg et al. (2007) introduced an “optimal step” NR-ICP that represents the source shape as a mesh and at each iteration solves a linear system to update vertex positions under an as-rigid-as-possible (ARAP) penalty.</p>
<p>Another approach is the embedded deformation technique (Sumner et al. 2007), which uses a deformation graph (sparse set of nodes in the volume) that undergo affine transformations, blending those to deform the entire shape. This reduces degrees of freedom and ensures local rigidity.</p>
<p>In summary, classical non-rigid registration methods turned the ill-posed problem into a (mostly) solvable one by combining soft correspondence estimation with smooth deformation models. They paved the way for many applications, such as building datasets like MPI FAUST (2014), which was created by registering 300 raw 3D scans of people into alignment and establishing point-to-point correspondences across scans.</p>
</section>
</section>
<section id="parametric-models-and-the-smpl-body-model">
<h2>3. Parametric Models and the SMPL Body Model<a class="headerlink" href="#parametric-models-and-the-smpl-body-model" title="Link to this heading"></a></h2>
<p>While generic non-rigid algorithms make minimal assumptions about object shape, in some domains we have strong prior models of how shape can vary. Human bodies are a prime example: a human scan has <span class="math notranslate nohighlight">\(N\approx 10^4\)</span> points (if meshed, 6890 vertices in the commonly used template), which is a huge number of degrees of freedom for arbitrary deformation, but the true variability of human bodies lies in a much lower-dimensional space (body shape and pose).</p>
<p>Parametric human models exploit this by encoding body shape and pose in a compact set of parameters. The most widely used model today is SMPL (Skinned Multi-Person Linear model) by Loper et al. 2015. SMPL provides a differentiable function <span class="math notranslate nohighlight">\(M(\boldsymbol{\beta}, \boldsymbol{\theta})\)</span> that outputs a full 3D mesh of a human given shape parameters <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> and pose parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>.</p>
<section id="model-structure">
<h3>Model Structure<a class="headerlink" href="#model-structure" title="Link to this heading"></a></h3>
<p>SMPL’s output is a triangulated mesh with <span class="math notranslate nohighlight">\(N=6890\)</span> vertices (a standard template topology). The model has <span class="math notranslate nohighlight">\(|\boldsymbol{\beta}|=10\)</span> shape parameters and <span class="math notranslate nohighlight">\(|\boldsymbol{\theta}|=24\times 3=72\)</span> pose parameters (3 for each joint’s rotation in axis-angle). Internally, SMPL has a rig (skeleton) of <span class="math notranslate nohighlight">\(K=24\)</span> joints in a kinematic tree (including the root).</p>
<p>There are three main components in the SMPL function:</p>
</section>
<section id="shape-blend-shapes-identity-variation">
<h3>Shape Blend Shapes (Identity Variation)<a class="headerlink" href="#shape-blend-shapes-identity-variation" title="Link to this heading"></a></h3>
<p>The model learns a low-dimensional shape space (capturing different body proportions and details across individuals). Mathematically, this is a linear blend shape model:</p>
<div class="math notranslate nohighlight">
\[\mathbf{T}(\boldsymbol{\beta}) = \bar{\mathbf{T}} + \mathbf{B}_S \boldsymbol{\beta}\]</div>
<p>where:
- <span class="math notranslate nohighlight">\(\bar{\mathbf{T}}\in\mathbb{R}^{3N}\)</span> is the mean shape (a reference mesh in a neutral pose, e.g., T-pose)
- <span class="math notranslate nohighlight">\(\mathbf{B}_S=[\mathbf{S}_1,\mathbf{S}_2,\ldots,\mathbf{S}_{|\beta|}]\in\mathbb{R}^{3N\times|\beta|}\)</span> is a matrix of principal shape directions
- Each <span class="math notranslate nohighlight">\(\mathbf{S}_n\)</span> is a shape blend shape (essentially a vertex displacement vector corresponding to one principal component of human shape variation)</p>
<p>Given a coefficient vector <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>, the shaped template <span class="math notranslate nohighlight">\(\mathbf{T}(\boldsymbol{\beta})\)</span> is a 3N vector stacking all vertices of the mesh in the zero-pose, but morphed to the body shape defined by <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>.</p>
<p>As a byproduct, SMPL computes the locations of the skeleton joints for this body using a learned joint regressor <span class="math notranslate nohighlight">\(J_{\text{reg}}\)</span> (a fixed <span class="math notranslate nohighlight">\(K\times N\)</span> matrix) that picks a linear combination of vertex positions to define each joint coordinate:</p>
<div class="math notranslate nohighlight">
\[J(\boldsymbol{\beta}) = J_{\text{reg}} \cdot \mathbf{T}(\boldsymbol{\beta})\]</div>
<p>This yields <span class="math notranslate nohighlight">\(K=24\)</span> joint locations in the posed space.</p>
</section>
<section id="pose-blend-shapes-pose-dependent-deformation">
<h3>Pose Blend Shapes (Pose-Dependent Deformation)<a class="headerlink" href="#pose-blend-shapes-pose-dependent-deformation" title="Link to this heading"></a></h3>
<p>If we were to articulate the shaped mesh <span class="math notranslate nohighlight">\(\mathbf{T}(\beta)\)</span> purely by rotating limbs (via skinning), we would see artifacts. SMPL addresses this by learning pose-dependent deformations.</p>
<p>These are also linear blend shapes, but their “weights” depend on the pose <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>. SMPL represents the pose by relative rotation matrices <span class="math notranslate nohighlight">\(R_1,\dots,R_{K}\)</span> for each joint. A pose feature <span class="math notranslate nohighlight">\(\mathbf{f}(\boldsymbol{\theta})\)</span> is constructed by concatenating the rotation matrices (excluding the root) after subtracting the identity:</p>
<div class="math notranslate nohighlight">
\[\mathbf{f}(\boldsymbol{\theta})=[\text{vec}(R_1-I), \text{vec}(R_2-I), \ldots, \text{vec}(R_{K-1}-I)] \in\mathbb{R}^{(K-1)\times 9}\]</div>
<p>This vector (of length 207 when <span class="math notranslate nohighlight">\(K=24\)</span>) is zero in the zero-pose and captures how each joint’s rotation deviates from rest.</p>
<p>SMPL then has a pose blend shape matrix <span class="math notranslate nohighlight">\(\mathbf{B}_P \in \mathbb{R}^{3N\times 207}\)</span> that maps this pose feature to a corrective displacement on the vertices:</p>
<div class="math notranslate nohighlight">
\[\mathbf{P}(\boldsymbol{\theta}) = \mathbf{B}_P \mathbf{f}(\boldsymbol{\theta}) \in\mathbb{R}^{3N}\]</div>
<p>After computing pose blend shapes, we add them to get the posed template in zero pose:</p>
<div class="math notranslate nohighlight">
\[\mathbf{T}_P(\boldsymbol{\beta},\boldsymbol{\theta}) = \mathbf{T}(\boldsymbol{\beta}) + \mathbf{P}(\boldsymbol{\theta}) \in\mathbb{R}^{3N}\]</div>
<p>or equivalently:</p>
<div class="math notranslate nohighlight">
\[\mathbf{T}_P = \bar{\mathbf{T}} + \mathbf{B}_S\boldsymbol{\beta} + \mathbf{B}_P(\boldsymbol{\theta})\]</div>
</section>
<section id="linear-blend-skinning-lbs-for-articulation">
<h3>Linear Blend Skinning (LBS) for Articulation<a class="headerlink" href="#linear-blend-skinning-lbs-for-articulation" title="Link to this heading"></a></h3>
<p>Now we have a mesh <span class="math notranslate nohighlight">\(\mathbf{T}_P(\boldsymbol{\beta},\boldsymbol{\theta})\)</span> in the canonical pose (T-pose) that reflects the person’s shape and pose-dependent deformations. The final step is to pose the model: apply the joint rotations <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> to the mesh to obtain the vertices in the target pose.</p>
<p>SMPL uses Linear Blend Skinning (also known as skeleton subspace deformation). Each vertex <span class="math notranslate nohighlight">\(i\)</span> is attached to the <span class="math notranslate nohighlight">\(K\)</span> skeleton joints with weights <span class="math notranslate nohighlight">\(w_{ik}\)</span> (these are given by a skinning weight matrix <span class="math notranslate nohighlight">\(W\in \mathbb{R}^{N\times K}\)</span>, where each row sums to 1).</p>
<p>For each joint <span class="math notranslate nohighlight">\(k\)</span>, we compute a <span class="math notranslate nohighlight">\(4\times 4\)</span> homogeneous global transform <span class="math notranslate nohighlight">\(G_k(\boldsymbol{\theta}, J(\boldsymbol{\beta}))\)</span> that takes a point in the rest pose coordinate system of joint <span class="math notranslate nohighlight">\(k\)</span> to the new posed location. This is done by forward kinematics along the kinematic tree.</p>
<p>The deformed position of vertex <span class="math notranslate nohighlight">\(i\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\mathbf{v}'_i = \sum_{k=1}^K w_{ik} (G_k(\boldsymbol{\theta},J(\boldsymbol{\beta})) [ \mathbf{v}_i, 1 ]^T)_{1..3}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{v}_i\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>-th vertex of <span class="math notranslate nohighlight">\(\mathbf{T}_P(\boldsymbol{\beta},\boldsymbol{\theta})\)</span>.</p>
<p>Putting it all together, the SMPL function can be written as:</p>
<div class="math notranslate nohighlight">
\[M(\boldsymbol{\beta},\boldsymbol{\theta}) = W(\bar{\mathbf{T}}+\mathbf{B}_S\boldsymbol{\beta}+\mathbf{B}_P(\boldsymbol{\theta}), J(\boldsymbol{\beta}), \boldsymbol{\theta}, W)\]</div>
<p>where <span class="math notranslate nohighlight">\(W(\cdot)\)</span> denotes the Linear Blend Skinning operation.</p>
</section>
<section id="learning-smpl">
<h3>Learning SMPL<a class="headerlink" href="#learning-smpl" title="Link to this heading"></a></h3>
<p>The parameters <span class="math notranslate nohighlight">\(\mathbf{B}_S\)</span>, <span class="math notranslate nohighlight">\(\mathbf{B}_P\)</span>, <span class="math notranslate nohighlight">\(J_{\text{reg}}\)</span>, and <span class="math notranslate nohighlight">\(W\)</span> were learned from a training set of thousands of registered 3D body scans. In training, the identity of each scan (which person) and the pose (from a motion capture) were known, so one could solve for the blend shapes by linear regression.</p>
<p>One important design choice: SMPL’s pose blend shapes are pose-dependent offsets that are independent of identity. This factorization makes the model simple and additive, but it cannot capture any idiosyncratic pose deformation (e.g., two people with different body fat might deform slightly differently in the same pose).</p>
</section>
<section id="using-smpl-for-registration">
<h3>Using SMPL for Registration<a class="headerlink" href="#using-smpl-for-registration" title="Link to this heading"></a></h3>
<p>A major advantage of SMPL is that it reduces a complex registration problem (deforming a high-dimensional surface) to a low-dimensional parameter estimation problem. To fit SMPL to a 3D scan, one needs to find <span class="math notranslate nohighlight">\(\boldsymbol{\beta},\boldsymbol{\theta}\)</span> (and possibly a global translation) that makes <span class="math notranslate nohighlight">\(M(\boldsymbol{\beta},\boldsymbol{\theta})\)</span> align the scan as closely as possible.</p>
<p>This is typically done by minimizing an objective that measures distance from the scan points to the SMPL surface (and vice versa) while regularizing the parameters (e.g., keeping <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> near the mean and <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> in plausible ranges). Because SMPL is differentiable, one can use gradient-based optimization for this fitting.</p>
<p>Classical approaches like SMPLify (Bogo et al., 2016) did exactly this: they fit SMPL to 2D keypoints or 3D scans by iterative closest point alignment with priors. When properly initialized, such model-based fitting is very robust and yields a complete, watertight mesh even from partial data.</p>
</section>
</section>
<section id="modeling-clothing-and-fine-details-smpl-d">
<h2>4. Modeling Clothing and Fine Details: SMPL+D<a class="headerlink" href="#modeling-clothing-and-fine-details-smpl-d" title="Link to this heading"></a></h2>
<p>SMPL+D refers to extending the SMPL model by adding a free-form displacement field on top of the regular SMPL surface. The “+D” stands for extra per-vertex displacements (often called offsets or D-vertices).</p>
<p>Intuitively, SMPL models a naked or minimally-clothed body. SMPL+D says: in addition, each vertex can shift by some small 3D vector to capture person-specific details not in SMPL (e.g., the shape of their clothing, hair, or subtle geometry like muscle tone).</p>
<p>Mathematically, if <span class="math notranslate nohighlight">\(M(\boldsymbol{\beta},\boldsymbol{\theta})\)</span> is the SMPL surface, then:</p>
<div class="math notranslate nohighlight">
\[M^+(\boldsymbol{\beta},\boldsymbol{\theta},D) = M(\boldsymbol{\beta},\boldsymbol{\theta}) + D\]</div>
<p>where <span class="math notranslate nohighlight">\(D\in \mathbb{R}^{3N}\)</span> is a vector of displacements for each vertex (in the same order as the SMPL mesh).</p>
<p>In practice, <span class="math notranslate nohighlight">\(D\)</span> is usually defined in the rest pose and then rotated with the body. That is, we actually add <span class="math notranslate nohighlight">\(D\)</span> to the template before skinning. Many works simply optimize <span class="math notranslate nohighlight">\(D\)</span> as 3D free parameters (with smoothing regularization) when fitting a scan, allowing the fitted model to exactly coincide with the scan even if it has wrinkles or garments not represented by SMPL.</p>
<section id="why-smpl-d">
<h3>Why SMPL+D?<a class="headerlink" href="#why-smpl-d" title="Link to this heading"></a></h3>
<p>SMPL, being linear and low-dimensional, cannot capture high-frequency details or cloth wrinkles. By adding <span class="math notranslate nohighlight">\(D\)</span>, we get a piecewise linear model: the base is linear in <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> and pose elements (SMPL), and then an extra term linear in <span class="math notranslate nohighlight">\(D\)</span> (trivial identity mapping).</p>
<p>Crucially, SMPL+D retains the parametric, controllable nature of SMPL for the big deformations, while <span class="math notranslate nohighlight">\(D\)</span> accommodates all the remaining detail. This concept was hinted at even in the original SMPL paper and has been explicitly used in works like “Learning to Reconstruct People in Clothing” by Alldieck et al. (CVPR 2019).</p>
<p>From a registration perspective, SMPL+D is extremely useful. A common approach to fit a clothed 3D scan is:</p>
<ol class="arabic simple">
<li><p>First fit SMPL (getting the underlying body shape and pose)</p></li>
<li><p>Then compute the residual difference between the scan and the SMPL surface as <span class="math notranslate nohighlight">\(D\)</span>, assigning each vertex a displacement to reach the scan</p></li>
</ol>
<p>This results in an exact fit to the scan (if vertex correspondences are established). The output is a mesh that has the same connectivity as SMPL but matches the scan closely – effectively a registered mesh.</p>
</section>
<section id="how-displacements-are-applied">
<h3>How Displacements Are Applied<a class="headerlink" href="#how-displacements-are-applied" title="Link to this heading"></a></h3>
<p>Typically, <span class="math notranslate nohighlight">\(D\)</span> is defined in the model’s rest pose. So the pipeline becomes:</p>
<ol class="arabic simple">
<li><p>Start with <span class="math notranslate nohighlight">\(\bar{T}+\mathbf{B}_S\boldsymbol{\beta}\)</span> (the naked body shape)</p></li>
<li><p>Add <span class="math notranslate nohighlight">\(D\)</span> (now it’s a clothed shape in T-pose)</p></li>
<li><p>Add <span class="math notranslate nohighlight">\(\mathbf{B}_P(\boldsymbol{\theta})\)</span> pose correctives</p></li>
<li><p>Skin the whole thing</p></li>
</ol>
<p>Since <span class="math notranslate nohighlight">\(D\)</span> moves some vertices outwards (like away from the body for a jacket), those vertices will still follow along with the nearest joints’ rotations thanks to skinning weights. Thus the clothing moves approximately correctly with the body.</p>
<p>This approach treats clothing as if it were “painted onto” the body with the same rig. It cannot capture secondary motion or sliding of loose clothing, but it is surprisingly effective for moderately tight apparel.</p>
</section>
<section id="limitations">
<h3>Limitations<a class="headerlink" href="#limitations" title="Link to this heading"></a></h3>
<p>One must be careful when adding <span class="math notranslate nohighlight">\(D\)</span>: large displacements can distort skinning if the clothing extends far from the body (e.g., a long skirt might clip through legs when animated because SMPL’s single skinning weight per vertex might not suffice for cloth).</p>
<p>SMPL+D also increases the dimensionality of the model tremendously (<span class="math notranslate nohighlight">\(3N \approx 20k\)</span> new parameters), so it’s usually used when one has actual scan data to fit (not as a generative model by itself, unless combined with neural nets that predict <span class="math notranslate nohighlight">\(D\)</span>).</p>
</section>
<section id="applications">
<h3>Applications<a class="headerlink" href="#applications" title="Link to this heading"></a></h3>
<p>Once you have a SMPL+D model of a person (i.e., <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> fit to their body, and <span class="math notranslate nohighlight">\(D\)</span> fit to their clothes/hair), you can animate it by changing <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>. The <span class="math notranslate nohighlight">\(D\)</span> stays attached to the same vertices, so as the body moves, the clothes move too.</p>
<p>This makes SMPL+D a straightforward solution for clothed avatar animation without needing physical simulation. It won’t exhibit cloth dynamics (no secondary motion like a skirt swaying), but for many purposes (AR/VR avatars, games) it provides a good trade-off of realism and simplicity.</p>
</section>
</section>
<section id="survey-of-3d-registration-methods-from-icp-to-deep-learning">
<h2>5. Survey of 3D Registration Methods: From ICP to Deep Learning<a class="headerlink" href="#survey-of-3d-registration-methods-from-icp-to-deep-learning" title="Link to this heading"></a></h2>
<p>Let’s now trace the historical progression of 3D registration methods, highlighting key representative works and showing how ideas evolved and how new technologies (like neural networks) have influenced the field.</p>
<section id="early-pioneering-works-1990s-foundational-rigid-registration">
<h3>5.1 Early Pioneering Works (1990s) – Foundational Rigid Registration<a class="headerlink" href="#early-pioneering-works-1990s-foundational-rigid-registration" title="Link to this heading"></a></h3>
<p>The 1990s established the foundations for rigid registration:</p>
<ul class="simple">
<li><p><strong>Horn (1987) and Arun &amp; Huang (1987)</strong>: Developed closed-form solutions for absolute orientation (the Procrustes problem) using quaternions or SVD</p></li>
<li><p><strong>Chen &amp; Medioni (1991) and Besl &amp; McKay (1992)</strong>: Introduced the ICP algorithm, establishing the first general solution to align 3D shapes with unknown correspondences</p></li>
<li><p><strong>Zhang (1994)</strong>: Improved ICP with a space partitioning technique for speed and discussed convergence properties</p></li>
<li><p><strong>Feldmar &amp; Ayache (1996)</strong>: Extended ICP to deformable medical image registration using splines</p></li>
<li><p><strong>Rusinkiewicz &amp; Levoy (2001)</strong>: “Efficient Variants of ICP”, a landmark paper that systematically reviewed many enhancements and made ICP fast enough for real-time use</p></li>
</ul>
<p>By 2000, rigid registration became largely a solved problem with ICP and its variants, as long as a decent initial guess is available. Challenges remained in global alignment (overcoming local minima) and speed, but the basic algorithmic toolbox was established.</p>
</section>
<section id="the-2000s-robust-and-non-rigid-registration-emerges">
<h3>5.2 The 2000s – Robust and Non-Rigid Registration Emerges<a class="headerlink" href="#the-2000s-robust-and-non-rigid-registration-emerges" title="Link to this heading"></a></h3>
<p>The 2000s saw significant advances in non-rigid registration:</p>
<ul class="simple">
<li><p><strong>Gold, Rangarajan et al. (1998)</strong>: Proposed Robust Point Matching (RPM) with softassign and deterministic annealing for 2D point sets</p></li>
<li><p><strong>Chui &amp; Rangarajan (2003)</strong>: TPS-RPM combined RPM with Thin Plate Splines for non-rigid alignment, a breakthrough in non-rigid registration</p></li>
<li><p><strong>Amberg, Romdhani, Vetter (2007)</strong>: Optimal Step Nonrigid ICP introduced a general framework for embedded deformation in ICP</p></li>
<li><p><strong>Myronenko &amp; Song (2009-2010)</strong>: Released CPD, providing a principled probabilistic approach that became extremely popular due to its ease of use and robustness</p></li>
</ul>
<p>By 2010, a typical pipeline might be: use CPD or TPS-RPM to get an initial non-rigid alignment, then project into a known template (like a human model) for fine-tuning.</p>
</section>
<section id="s-template-based-and-parametric-model-registration">
<h3>5.3 2010s – Template-based and Parametric Model Registration<a class="headerlink" href="#s-template-based-and-parametric-model-registration" title="Link to this heading"></a></h3>
<p>The 2010s saw the rise of template-based registration:</p>
<ul class="simple">
<li><p><strong>Anguelov et al. (2005)</strong>: SCAPE model – an earlier parametric human model (nonlinear blend shapes) used to fit meshes of people</p></li>
<li><p><strong>Bogo et al. (2014)</strong>: FAUST dataset – provided 300 meshes of humans in correspondence, and an evaluation for registration algorithms</p></li>
<li><p><strong>SMPL (2015)</strong>: As described, SMPL provided a ready-made template with a parametric function. Fitting SMPL to a scan (with or without D) became a common approach</p></li>
<li><p><strong>SMPLify (2016)</strong>: Bogo et al. showed fitting SMPL to 2D keypoints in images to recover 3D pose/shape</p></li>
<li><p><strong>3D-CODED (2018)</strong>: A deep learning approach by Groueix et al. that introduced the idea of a deformation-based autoencoder for shapes</p></li>
</ul>
</section>
<section id="s-learning-based-parametric-registration-and-hybrid-approaches">
<h3>5.4 2020s – Learning-Based Parametric Registration and Hybrid Approaches<a class="headerlink" href="#s-learning-based-parametric-registration-and-hybrid-approaches" title="Link to this heading"></a></h3>
<p>Around 2019-2020, we see an explosion of methods that leverage deep learning to improve 3D registration, especially for humans:</p>
<ul class="simple">
<li><p><strong>IP-Net (ECCV 2020)</strong>: Implicit Part Network by Bhatnagar et al. combines implicit surface reconstruction with model fitting</p></li>
<li><p><strong>LoopReg (NeurIPS 2020)</strong>: Tackled the problem of registering an entire corpus of 3D scans to a common model in a self-supervised way</p></li>
<li><p><strong>Neural Deformation Fields (2021+)</strong>: Recent approaches represent the deformation by neural networks. For instance, SNARF (Chen et al., ICCV 2021) learns a neural forward skinning field</p></li>
<li><p><strong>Learned Vertex Descent (LVD, ECCV 2022)</strong>: Takes a hybrid approach, training an ensemble of tiny networks, one per vertex of the template, that output a descent direction for that vertex towards the data</p></li>
</ul>
<p>These newer methods aim to make the registration pipeline partially neural so it can learn to avoid local minima and cope with data imperfections, while still maintaining the logical structure of ICP (correspondence &amp; update).</p>
</section>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading"></a></h2>
<p>The field of 3D registration has evolved from deterministic algorithms operating on point sets to highly learned systems that leverage data-driven priors:</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="lecture_05_1_body_model_training.html" class="btn btn-neutral float-left" title="Lecture 5.1 - Training a Body Model and Fitting SMPL to Scans" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="lecture_06_1_SMPL_optimization.html" class="btn btn-neutral float-right" title="Lecture 06.1 - Fitting the SMPL Model to Images via Optimization" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>