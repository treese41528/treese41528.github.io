

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Lecture 07.1: Fitting SMPL to IMU Data Using Optimization-Based Methods &mdash; Fitting SMPL to IMU Optimization</title>
      <link rel="stylesheet" type="text/css" href="/VirtualHumans/_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="/VirtualHumans/_static/css/theme.css?v=e59714d7" />

  
    <link rel="canonical" href="https://treese41528.github.io/VirtualHumans/lecture_07_1_fitting_SMPL_to_IMU_optimization.html" />
      <script src="/VirtualHumans/_static/jquery.js?v=5d32c60e"></script>
      <script src="/VirtualHumans/_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="/VirtualHumans/_static/documentation_options.js?v=f2a433a1"></script>
      <script src="/VirtualHumans/_static/doctools.js?v=9bcbadda"></script>
      <script src="/VirtualHumans/_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="/VirtualHumans/_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Lecture 07.2: Fitting SMPL to IMU Data Using Learning-Based Methods" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html" />
    <link rel="prev" title="Lecture 06.2 - Learning-Based Fitting of the SMPL Model to Images" href="lecture_06_2SMPL_model_fitting.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Virtual Humans Lecture
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="lecture_01_1_historical_body_models.html">Lecture 01.1 – Historical Body Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#early-origins-simplified-primitives-and-kinematic-skeletons-1970s1980s">Early Origins: Simplified Primitives and Kinematic Skeletons (1970s–1980s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#advances-in-the-1990s-superquadrics-differentiable-fitting-and-physical-models">Advances in the 1990s: Superquadrics, Differentiable Fitting, and Physical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#the-impact-of-3d-scanning-and-data-from-anthropometry-to-statistical-models-1990s2000s">The Impact of 3D Scanning and Data: From Anthropometry to Statistical Models (1990s–2000s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#scape-and-the-emergence-of-pose-aware-models-mid-2000s">SCAPE and the Emergence of Pose-Aware Models (Mid-2000s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#consolidation-in-the-2010s-smpl-and-integration-with-learning-based-methods">Consolidation in the 2010s: SMPL and Integration with Learning-Based Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#deep-learning-and-neural-implicit-models-late-2010spresent">Deep Learning and Neural Implicit Models (Late 2010s–Present)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#timeline-summary-of-milestones">Timeline Summary of Milestones</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html">Lecture 01.2 – Introduction to Human Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#historical-context-of-human-body-modeling">1. Historical Context of Human Body Modeling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#early-developments">Early Developments</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#mid-20th-century-approaches">Mid-20th Century Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#data-driven-revolution-1990s-2000s">Data-Driven Revolution (1990s-2000s)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#mathematical-foundations-of-human-body-models">2. Mathematical Foundations of Human Body Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#the-smpl-model">The SMPL Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#pca-based-statistical-shape-modeling">PCA-Based Statistical Shape Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#kinematic-modeling">Kinematic Modeling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#applications-of-human-body-models">3. Applications of Human Body Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computer-animation-and-visual-effects">Computer Animation and Visual Effects</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#virtual-humans-and-avatars">Virtual Humans and Avatars</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#biomechanics-and-ergonomics">Biomechanics and Ergonomics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#human-computer-interaction-hci">Human-Computer Interaction (HCI)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computer-vision-and-ai">Computer Vision and AI</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#education-and-training">Education and Training</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#challenges-and-future-directions">4. Challenges and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computational-efficiency">Computational Efficiency</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#accuracy-and-detail">Accuracy and Detail</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#generalization">Generalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#clothing-and-accessories">Clothing and Accessories</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#emerging-approaches">Emerging Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html">Lecture 01.3 – Introduction to Human Models (Overview)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#historical-context">1. Historical Context</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#early-scientific-studies">Early Scientific Studies</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#mid-20th-century-to-digital-era">Mid-20th Century to Digital Era</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#st-century-advances">21st Century Advances</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#mathematical-foundations">2. Mathematical Foundations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#parametric-body-models">Parametric Body Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-surface-representations">Implicit Surface Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#kinematic-modeling">Kinematic Modeling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#image-formation-and-rendering">3. Image Formation and Rendering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#camera-models">Camera Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#shading-and-visibility">Shading and Visibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#differentiable-rendering">Differentiable Rendering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#surface-representation-methods">4. Surface Representation Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#explicit-mesh-models">Explicit Mesh Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-function-models">Implicit Function Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#motion-capture-and-behavior-synthesis">5. Motion Capture and Behavior Synthesis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#capturing-human-motion">Capturing Human Motion</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#behavior-synthesis">Behavior Synthesis</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#clothing-modeling">6. Clothing Modeling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physically-based-simulation">Physically-Based Simulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#data-driven-approaches">Data-Driven Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-clothing-models">Implicit Clothing Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#human-object-interaction">7. Human-Object Interaction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physics-based-methods">Physics-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#learning-based-approaches">Learning-Based Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#hybrid-systems">Hybrid Systems</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#applications">8. Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#entertainment-and-media">Entertainment and Media</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#healthcare-and-biomechanics">Healthcare and Biomechanics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#engineering-and-design">Engineering and Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#human-computer-interaction">Human-Computer Interaction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#scientific-research">Scientific Research</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#challenges-and-future-directions">9. Challenges and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#scalability-and-generalization">Scalability and Generalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#higher-fidelity-dynamics">Higher-Fidelity Dynamics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#data-and-labeling-constraints">Data and Labeling Constraints</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physics-and-learning-integration">Physics and Learning Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#semantic-and-cognitive-aspects">Semantic and Cognitive Aspects</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#realism-vs-controllability">Realism vs. Controllability</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_02_1_image_formation.html">Lecture 02.1 – Image Formation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#historical-developments-in-image-formation">1. Historical Developments in Image Formation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#ancient-and-medieval-optics-camera-obscura">Ancient and Medieval Optics – Camera Obscura</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#renaissance-perspective-and-geometry">Renaissance Perspective and Geometry</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#early-cameras-and-photographic-imaging">Early Cameras and Photographic Imaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#modern-developments">Modern Developments</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#the-pinhole-camera-model">2. The Pinhole Camera Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#coordinate-setup">Coordinate Setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#proof-by-similar-triangles">Proof by Similar Triangles</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#numerical-example">Numerical Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#inadequacy-of-a-simple-pinhole">Inadequacy of a Simple Pinhole</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#camera-intrinsics-and-the-projection-matrix">3. Camera Intrinsics and the Projection Matrix</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#extrinsic-parameters">Extrinsic Parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#full-projection-example">Full Projection Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#image-distortions-correction">4. Image Distortions &amp; Correction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#properties-of-perspective-projection">5. Properties of Perspective Projection</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#advanced-theoretical-extensions">6. Advanced Theoretical Extensions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#light-field-imaging-and-plenoptic-cameras">Light Field Imaging and Plenoptic Cameras</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#non-conventional-imaging-techniques">Non-Conventional Imaging Techniques</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#applications-in-modern-vision-and-graphics">7. Applications in Modern Vision and Graphics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#computer-vision-and-3d-reconstruction">Computer Vision and 3D Reconstruction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#medical-imaging">Medical Imaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#photorealistic-rendering-in-computer-graphics">Photorealistic Rendering in Computer Graphics</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#python-example-simulating-image-formation">8. Python Example: Simulating Image Formation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html">Lecture 02.2 – Rotations and Kinematic Chains</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#representations-of-3d-rotations">1. Representations of 3D Rotations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#a-rotation-matrices">A) Rotation Matrices</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#b-euler-angles">B) Euler Angles</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#c-quaternions">C) Quaternions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#lie-algebra-so-3-and-exponential-map">2. Lie Algebra <span class="math notranslate nohighlight">\(so(3)\)</span> and Exponential Map</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#rodrigues-rotation-formula">3. Rodrigues’ Rotation Formula</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#kinematic-chains-forward-inverse-kinematics">4. Kinematic Chains: Forward &amp; Inverse Kinematics</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#comparison-of-rotation-representations">Comparison of Rotation Representations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_03_1_surface_representations.html">Lecture 03.1 – Surface Representations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#mathematical-foundations-of-surface-representations">1. Mathematical Foundations of Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-parametric-surfaces">A) Parametric Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-implicit-surfaces">B) Implicit Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-explicit-surfaces">C) Explicit Surfaces</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#surface-differential-properties">2. Surface Differential Properties</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-surface-normals">A) Surface Normals</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-fundamental-forms-and-curvature">B) Fundamental Forms and Curvature</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-geodesics">C) Geodesics</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#discrete-surface-representations">3. Discrete Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-polygon-meshes">A) Polygon Meshes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-point-clouds">B) Point Clouds</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-signed-distance-fields-sdf">C) Signed Distance Fields (SDF)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#advanced-surface-representations">4. Advanced Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-bezier-curves-and-surfaces">A) Bézier Curves and Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-b-splines-and-nurbs">B) B-Splines and NURBS</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-subdivision-surfaces">C) Subdivision Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-level-sets">D) Level Sets</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#e-neural-implicit-representations">E) Neural Implicit Representations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#comparative-analysis-and-applications">5. Comparative Analysis and Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-computational-efficiency-and-storage">A) Computational Efficiency and Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-practical-applications">B) Practical Applications</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-operations-complexity">C) Operations Complexity</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#implementation-examples">6. Implementation Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-basic-mesh-processing-python">A) Basic Mesh Processing (Python)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-implicit-surface-utilities">B) Implicit Surface Utilities</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-bezier-curve-implementation">C) Bézier Curve Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-curvature-estimation-on-meshes">D) Curvature Estimation on Meshes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#advanced-topics-and-future-directions">7. Advanced Topics and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-multi-resolution-representations">A) Multi-Resolution Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-machine-learning-for-geometry">B) Machine Learning for Geometry</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-dynamic-surfaces">C) Dynamic Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-non-manifold-geometries">D) Non-Manifold Geometries</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html">Lecture 03.2 – Procrustes Alignment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#goal-learning-a-model-of-pose-and-shape">Goal: Learning a Model of Pose and Shape</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#the-challenge-of-registration">The Challenge of Registration</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#surface-representation-mesh">Surface Representation: Mesh</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#the-procrustes-alignment-problem-mathematical-formulation">The Procrustes Alignment Problem: Mathematical Formulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#rigid-transformations">Rigid Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#procrustes-alignment-solution">Procrustes Alignment Solution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#decoupling-translation-by-centroid-alignment">Decoupling Translation by Centroid Alignment</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#optimal-rotation-via-svd">Optimal Rotation via SVD</a><ul>
<li class="toctree-l4"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#reflection-adjustment">Reflection Adjustment</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#optimal-scale-optional">Optimal Scale (Optional)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#complete-mathematical-derivation">Complete Mathematical Derivation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#translation-derivation">Translation Derivation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#rotation-derivation">Rotation Derivation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#scale-derivation">Scale Derivation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#summary-of-procrustes-alignment-algorithm">Summary of Procrustes Alignment Algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#python-implementation-example">Python Implementation Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#practical-applications">Practical Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#interactive-visualization-ideas">Interactive Visualization Ideas</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_04_1_icp.html">Lecture 4.1: Iterative Closest Point</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#introduction-to-shape-alignment-and-registration">Introduction to Shape Alignment and Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#the-registration-problem">The Registration Problem</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#review-procrustes-analysis">Review: Procrustes Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#problem-unknown-correspondences">Problem: Unknown Correspondences</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#the-iterative-closest-point-icp-algorithm">The Iterative Closest Point (ICP) Algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#basic-icp-algorithm">Basic ICP Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#computational-considerations">Computational Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="lecture_04_1_icp.html#closest-point-computation">Closest Point Computation</a></li>
<li class="toctree-l4"><a class="reference internal" href="lecture_04_1_icp.html#convergence-and-local-minima">Convergence and Local Minima</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#point-to-point-vs-point-to-plane-icp">Point-to-Point vs. Point-to-Plane ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#point-to-point-icp">Point-to-Point ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#point-to-plane-icp">Point-to-Plane ICP</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#gradient-based-icp-for-non-rigid-registration">Gradient-based ICP for Non-Rigid Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#gradient-based-icp-algorithm">Gradient-based ICP Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#advantages-of-gradient-based-icp">Advantages of Gradient-based ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#computing-gradients">Computing Gradients</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#improving-icp-s-robustness">Improving ICP’s Robustness</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#data-association-direction">Data Association Direction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#robust-cost-functions">Robust Cost Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#trimmed-icp">Trimmed ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#ransac-based-approaches">RANSAC-based Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#additional-information">Additional Information</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#point-to-surface-distance">Point-to-Surface Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#icp-variants-and-extensions">ICP Variants and Extensions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#generalized-icp-gicp">Generalized ICP (GICP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#em-icp-and-probabilistic-approaches">EM-ICP and Probabilistic Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#coherent-point-drift-cpd">Coherent Point Drift (CPD)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#multi-scale-approaches">Multi-Scale Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#applications-of-icp">Applications of ICP</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#implementing-icp">Implementing ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#efficient-python-implementation">Efficient Python Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#practical-tips">Practical Tips</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_04_2_body_models.html">Lecture 04.2 - Body Models: Vertex-Based Models and SMPL</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#body-models-as-parameterized-functions">1. Body Models as Parameterized Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#rotations-articulation-and-pose-representation">2. Rotations, Articulation, and Pose Representation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#rotation-representation">2.1 Rotation Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#kinematic-chain">2.2 Kinematic Chain</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#linear-blend-skinning-and-its-limitations">3. Linear Blend Skinning and its Limitations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#linear-blend-skinning">3.1 Linear Blend Skinning</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#problems-with-standard-lbs">3.2 Problems with Standard LBS</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#blend-shapes-for-correcting-lbs-artifacts">3.3 Blend Shapes for Correcting LBS Artifacts</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#the-smpl-body-model">4. The SMPL Body Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#smpl-philosophy">4.1 SMPL Philosophy</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#smpl-model-architecture">4.2 SMPL Model Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#shape-blend-shapes">4.2.1 Shape Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#pose-blend-shapes">4.2.2 Pose Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#joint-regression">4.2.3 Joint Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#model-training">4.3 Model Training</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#comparison-with-scape">5. Comparison with SCAPE</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#the-scape-model">5.1 The SCAPE Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#different-approaches-to-deformation">5.2 Different Approaches to Deformation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#performance-comparison">5.3 Performance Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#other-advantages">5.4 Other Advantages</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#alignment-techniques-procrustes-analysis-and-icp">6. Alignment Techniques: Procrustes Analysis and ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#procrustes-analysis">6.1 Procrustes Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#iterative-closest-point-icp">6.2 Iterative Closest Point (ICP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#fitting-smpl-to-scans">6.3 Fitting SMPL to Scans</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#image-formation-and-the-pinhole-camera-model">7. Image Formation and the Pinhole Camera Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#the-pinhole-camera-model">7.1 The Pinhole Camera Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#lens-distortion">7.2 Lens Distortion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#extensions-and-advanced-applications">8. Extensions and Advanced Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#dynamic-soft-tissue-modeling">8.1 Dynamic Soft Tissue Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#specialized-extensions">8.2 Specialized Extensions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#deep-learning-for-model-fitting">8.3 Deep Learning for Model Fitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#probabilistic-approaches">8.4 Probabilistic Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#hybrid-models">8.5 Hybrid Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_05_1_body_model_training.html">Lecture 5.1 - Training a Body Model and Fitting SMPL to Scans</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#body-models-based-on-triangle-deformations">Body Models Based on Triangle Deformations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#scape-and-blendscape-models">SCAPE and BlendSCAPE Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#triangle-deformation-process">Triangle Deformation Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#comparison-smpl-vs-scape-blendscape">Comparison: SMPL vs. SCAPE/BlendSCAPE</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#training-a-body-model-from-registrations">Training a Body Model from Registrations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#the-challenge-of-raw-scan-data">The Challenge of Raw Scan Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#training-from-registrations">Training from Registrations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#obtaining-registrations-fitting-smpl-to-scans">Obtaining Registrations: Fitting SMPL to Scans</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#non-rigid-registration-process">Non-Rigid Registration Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#iterative-closest-point-icp-review">Iterative Closest Point (ICP) Review</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#registration-objective-formulation">Registration Objective Formulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#point-to-surface-distance">Point-to-Surface Distance</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#multi-stage-optimization-strategy">Multi-Stage Optimization Strategy</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#joint-registration-and-model-training">Joint Registration and Model Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#co-registration-approach">Co-Registration Approach</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_05_2_3d_registration.html">Lecture 05.2 - 3D Registration: From Classical ICP to Modern Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#rigid-registration-and-the-icp-algorithm">1. Rigid Registration and the ICP Algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#the-iterative-closest-point-icp-algorithm">The Iterative Closest Point (ICP) Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#convergence-analysis-and-failure-modes">Convergence Analysis and Failure Modes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#classical-non-rigid-registration">2. Classical Non-Rigid Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#thin-plate-spline-robust-point-matching-tps-rpm">Thin Plate Spline Robust Point Matching (TPS-RPM)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#coherent-point-drift-cpd">Coherent Point Drift (CPD)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#other-non-rigid-methods">Other Non-Rigid Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#parametric-models-and-the-smpl-body-model">3. Parametric Models and the SMPL Body Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#model-structure">Model Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#shape-blend-shapes-identity-variation">Shape Blend Shapes (Identity Variation)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#pose-blend-shapes-pose-dependent-deformation">Pose Blend Shapes (Pose-Dependent Deformation)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#linear-blend-skinning-lbs-for-articulation">Linear Blend Skinning (LBS) for Articulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#learning-smpl">Learning SMPL</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#using-smpl-for-registration">Using SMPL for Registration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#modeling-clothing-and-fine-details-smpl-d">4. Modeling Clothing and Fine Details: SMPL+D</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#why-smpl-d">Why SMPL+D?</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#how-displacements-are-applied">How Displacements Are Applied</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#limitations">Limitations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#applications">Applications</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#survey-of-3d-registration-methods-from-icp-to-deep-learning">5. Survey of 3D Registration Methods: From ICP to Deep Learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#early-pioneering-works-1990s-foundational-rigid-registration">5.1 Early Pioneering Works (1990s) – Foundational Rigid Registration</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#the-2000s-robust-and-non-rigid-registration-emerges">5.2 The 2000s – Robust and Non-Rigid Registration Emerges</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#s-template-based-and-parametric-model-registration">5.3 2010s – Template-based and Parametric Model Registration</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#s-learning-based-parametric-registration-and-hybrid-approaches">5.4 2020s – Learning-Based Parametric Registration and Hybrid Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html">Lecture 06.1 - Fitting the SMPL Model to Images via Optimization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#mathematical-background-pinhole-camera-and-projections">Mathematical Background: Pinhole Camera and Projections</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#perspective-projection">Perspective Projection</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#weak-perspective-projection">Weak-Perspective Projection</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#camera-extrinsics-vs-model-pose">Camera Extrinsics vs. Model Pose</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#d-keypoints-and-projection">2D Keypoints and Projection</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#the-smpl-model-as-a-differentiable-function-of-shape-and-pose">The SMPL Model as a Differentiable Function of Shape and Pose</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#shape-blend-shapes">Shape Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#pose-blend-shapes">Pose Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#joint-positions">Joint Positions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#linear-blend-skinning">Linear Blend Skinning</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#differentiability-of-smpl">Differentiability of SMPL</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#fitting-smpl-to-images-via-optimization-smplify">Fitting SMPL to Images via Optimization (SMPLify)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#objective-function">Objective Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#combined-objective">Combined Objective</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#optimization-strategy">Optimization Strategy</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#optimization-algorithms">Optimization Algorithms</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#automatic-differentiation-and-jacobians">Automatic Differentiation and Jacobians</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#result-of-smplify">Result of SMPLify</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#historical-progression-and-method-comparisons">Historical Progression and Method Comparisons</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#smplify-bogo-et-al-2016">SMPLify (Bogo et al. 2016)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#smplify-x-2019">SMPLify-X (2019)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html">Lecture 06.2 - Learning-Based Fitting of the SMPL Model to Images</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#foundations-of-learning-based-smpl-estimation">Foundations of Learning-Based SMPL Estimation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#integrating-smpl-into-neural-networks">Integrating SMPL into Neural Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#projection-functions">Projection Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#loss-functions">Loss Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#statistical-priors-and-adversarial-losses">Statistical Priors and Adversarial Losses</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#early-regression-approaches-hmr-and-nbf">Early Regression Approaches: HMR and NBF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#human-mesh-recovery-hmr">Human Mesh Recovery (HMR)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#neural-body-fitting-nbf">Neural Body Fitting (NBF)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#evolving-architectures-hybrid-and-improved-regression-methods">Evolving Architectures: Hybrid and Improved Regression Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#spin-optimization-in-the-training-loop">SPIN: Optimization in the Training Loop</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#pymaf-pyramidal-mesh-alignment-feedback">PyMAF: Pyramidal Mesh Alignment Feedback</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#cliff-using-full-frame-context-for-camera-orientation">CLIFF: Using Full-Frame Context for Camera Orientation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#pixie-whole-body-regression-with-part-experts">PIXIE: Whole-Body Regression with Part Experts</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#temporal-methods-from-single-images-to-video-sequences">Temporal Methods: From Single Images to Video Sequences</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#vibe-adversarial-motion-prior-with-grus">VIBE: Adversarial Motion Prior with GRUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#tcmr-temporally-consistent-mesh-recovery">TCMR: Temporally Consistent Mesh Recovery</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#motionbert-transformer-based-motion-representations">MotionBERT: Transformer-Based Motion Representations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#comparison-of-learning-based-methods">Comparison of Learning-Based Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#supervision-and-data">Supervision and Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#network-architecture">Network Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#objective-functions">Objective Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#pose-and-shape-priors">Pose and Shape Priors</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#performance">Performance</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#runtime">Runtime</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#strengths-and-weaknesses">Strengths and Weaknesses</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Lecture 07.1: Fitting SMPL to IMU Data Using Optimization-Based Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#classical-imu-based-pose-estimation-a-historical-perspective">Classical IMU-Based Pose Estimation: A Historical Perspective</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#attitude-and-heading-reference-systems">Attitude and Heading Reference Systems</a></li>
<li class="toctree-l3"><a class="reference internal" href="#kalman-filter-approaches">Kalman Filter Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="#early-sparse-sensor-approaches">Early Sparse-Sensor Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-based-optimization-methods">Model-Based Optimization Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="#learning-based-methods">Learning-Based Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#inertial-sensor-fundamentals-and-orientation-representations">Inertial Sensor Fundamentals and Orientation Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#gravity-alignment-and-drift-correction">Gravity Alignment and Drift Correction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#orientation-representations">Orientation Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#sensor-calibration">Sensor Calibration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#optimization-based-smpl-fitting-with-imu-data">Optimization-Based SMPL Fitting with IMU Data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#kinematic-model-and-sensor-prediction">Kinematic Model and Sensor Prediction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#regularization-and-prior-terms">Regularization and Prior Terms</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gradient-and-jacobian-computation">Gradient and Jacobian Computation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#pseudocode-smpl-pose-estimation-from-imu-sequence">Pseudocode: SMPL Pose Estimation from IMU Sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="#imu-based-human-pose-datasets-and-resources">IMU-Based Human Pose Datasets and Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#dip-imu-dataset-2018">DIP-IMU Dataset (2018)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#totalcapture-2017">TotalCapture (2017)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#amass-2019">AMASS (2019)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#other-datasets-and-resources">Other Datasets and Resources</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html">Lecture 07.2: Fitting SMPL to IMU Data Using Learning-Based Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#optimization-based-vs-learning-based-approaches">Optimization-Based vs. Learning-Based Approaches</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#learning-based-imu-to-pose-estimation-historical-overview-of-key-models">Learning-Based IMU-to-Pose Estimation: Historical Overview of Key Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#deep-inertial-poser-dip-2018">Deep Inertial Poser (DIP, 2018)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#transpose-2021">TransPose (2021)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#transformer-inertial-poser-tip-2022">Transformer Inertial Poser (TIP, 2022)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#physics-physical-inertial-poser-pip-2022">Physics/Physical Inertial Poser (PIP, 2022)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#other-notable-models-and-developments">Other Notable Models and Developments</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#problem-formulation-and-learning-task-definition">Problem Formulation and Learning Task Definition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#input-and-output-representations">Input and Output Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#learning-objective-and-loss-functions">Learning Objective and Loss Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#temporal-modeling-approaches">Temporal Modeling Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#supervised-vs-semi-supervised-training-synthetic-data">Supervised vs. Semi-Supervised Training; Synthetic Data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#model-architectures-and-design-considerations">Model Architectures and Design Considerations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#encoding-imu-measurements">Encoding IMU Measurements</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#network-structures">Network Structures</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#training-pipeline-and-pseudocode">Training Pipeline and Pseudocode</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#datasets-benchmarks-and-resources">Datasets, Benchmarks, and Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#dip-imu-dataset-2018">DIP-IMU Dataset (2018)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#totalcapture-2017">TotalCapture (2017)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#amass-2019">AMASS (2019)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#other-datasets-and-resources">Other Datasets and Resources</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#challenges-and-outlook">Challenges and Outlook</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html">Lecture 08.1: Vertex-Based Clothing Modeling for Virtual Humans</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#clothing-representation-as-vertex-displacements">Clothing Representation as Vertex Displacements</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#registration-of-clothed-human-scans">Registration of Clothed Human Scans</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#estimating-body-shape-under-clothing-the-buff-method">Estimating Body Shape Under Clothing: The BUFF Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#multi-layer-clothing-capture-the-clothcap-method">Multi-Layer Clothing Capture: The ClothCap Method</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#data-term">Data Term</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#boundary-term">Boundary Term</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#boundary-smoothness">Boundary Smoothness</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#laplacian-smoothness">Laplacian Smoothness</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#applications-of-multi-layer-registration">Applications of Multi-Layer Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#advantages-and-limitations">Advantages and Limitations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html">Lecture 09.1: Neural Implicit and Point-Based Representations for Clothed Human Modeling</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#background-explicit-vs-implicit-vs-point-based-representations">Background: Explicit vs. Implicit vs. Point-Based Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#mesh-based-models">Mesh-Based Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#point-based-models">Point-Based Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#neural-implicit-representations">Neural Implicit Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#neural-radiance-fields-nerfs-for-humans">Neural Radiance Fields (NeRFs) for Humans</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#hybrid-approaches">Hybrid Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#expressiveness-and-topology">Expressiveness and Topology</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#differentiability-and-learning">Differentiability and Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#data-efficiency-and-performance">Data Efficiency and Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#neural-implicit-function-foundations">Neural Implicit Function Foundations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#signed-distance-field-sdf">Signed Distance Field (SDF)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#occupancy-field-indicator-function">Occupancy Field (Indicator Function)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#volume-radiance-field">Volume Radiance Field</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#articulated-deformation-fields-for-implicit-models">Articulated Deformation Fields for Implicit Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#backward-warping-inverse-skinning-field">Backward Warping (Inverse Skinning Field)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#forward-warping-forward-skinning-field">Forward Warping (Forward Skinning Field)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#pose-dependent-deformation-secondary-motion">Pose-Dependent Deformation (Secondary Motion)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#generative-implicit-models-for-clothed-bodies">Generative Implicit Models for Clothed Bodies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#smplicit-topology-aware-clothed-human-model">SMPLicit: Topology-Aware Clothed Human Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#imghum-implicit-generative-human-model">imGHUM: Implicit Generative Human Model</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#pose-dependent-implicit-models-and-animatable-avatars">Pose-Dependent Implicit Models and Animatable Avatars</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#nasa-neural-articulated-shape-approximation-eccv-2020">NASA: Neural Articulated Shape Approximation (ECCV 2020)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#scanimate-weakly-supervised-skinned-avatar-networks-cvpr-2021">SCANimate: Weakly-Supervised Skinned Avatar Networks (CVPR 2021)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#neural-gif-neural-generalized-implicit-functions-iccv-2021">Neural-GIF: Neural Generalized Implicit Functions (ICCV 2021)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#snarf-skinned-neural-articulated-implicit-shapes-iccv-2021">SNARF: Skinned Neural Articulated Implicit Shapes (ICCV 2021)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#pop-the-power-of-points-iccv-2021-point-based-modeling">POP: The Power of Points (ICCV 2021) – Point-Based Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#other-noteworthy-methods">Other Noteworthy Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#blueprint-algorithms-for-key-methods">Blueprint Algorithms for Key Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#snarf-training-procedure-differentiable-forward-skinning-for-implicit-surfaces">SNARF (Training Procedure): Differentiable Forward Skinning for Implicit Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#pop-training-fitting-pipeline-point-based-model-for-pose-dependent-clothing">POP (Training &amp; Fitting Pipeline): Point-Based Model for Pose-Dependent Clothing</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#smplicit-inference-fitting-workflow-generative-implicit-garment-model-conditioned-on-smpl">SMPLicit (Inference/Fitting Workflow): Generative Implicit Garment Model conditioned on SMPL</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#historical-perspective-and-future-outlook">Historical Perspective and Future Outlook</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#challenges-and-future-directions">Challenges and Future Directions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="extended_materials_neural_radiance_fields.html">Neural Radiance Fields: A Historical and Theoretical Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#foundations-3d-scene-representation-and-reconstruction-techniques">Foundations: 3D Scene Representation and Reconstruction Techniques</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#voxel-grids">Voxel Grids</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#point-clouds">Point Clouds</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#mesh-based-surfaces">Mesh-Based Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#light-fields-and-volumetric-rendering">Light Fields and Volumetric Rendering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#emergence-of-neural-radiance-fields-nerf">Emergence of Neural Radiance Fields (NeRF)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#core-idea">Core Idea</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#training-procedure">Training Procedure</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#nerf-architecture">NeRF Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#hierarchical-sampling">Hierarchical Sampling</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#original-results">Original Results</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#theoretical-and-mathematical-analysis-of-nerf">Theoretical and Mathematical Analysis of NeRF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#volume-rendering-formulation-in-nerf">Volume Rendering Formulation in NeRF</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#positional-encoding-and-neural-network-architecture">Positional Encoding and Neural Network Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#loss-function-and-optimization">Loss Function and Optimization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#major-advancements-and-extensions-of-nerf">Major Advancements and Extensions of NeRF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#anti-aliasing-and-unbounded-scenes-mip-nerf-and-nerf">Anti-Aliasing and Unbounded Scenes: mip-NeRF and NeRF++</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#efficiency-improvements-instant-nerf-and-plenoctrees">Efficiency Improvements: Instant NeRF and PlenOctrees</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#dynamic-and-deformable-nerfs-d-nerf-nerfies-nsff-etc">Dynamic and Deformable NeRFs (D-NeRF, Nerfies, NSFF, etc.)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#neural-radiance-fields-for-human-modeling-with-smpl-and-body-models">Neural Radiance Fields for Human Modeling (with SMPL and Body Models)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#other-notable-extensions">Other Notable Extensions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#comparison-with-other-3d-representations">Comparison with Other 3D Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-polygonal-meshes">Vs. Polygonal Meshes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-point-clouds-3d-splatting">Vs. Point Clouds / 3D Splatting</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-voxel-grids-and-volumetric-methods">Vs. Voxel Grids and Volumetric Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-multi-plane-images-mpis-light-fields">Vs. Multi-Plane Images (MPIs) / Light Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#accuracy-and-fidelity">Accuracy and Fidelity</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#applicability">Applicability</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#datasets-for-nerf-training-and-evaluation">Datasets for NeRF Training and Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#blender-synthetic-nerf-dataset">Blender Synthetic NeRF Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#local-light-field-fusion-llff-real-forward-facing-dataset">Local Light Field Fusion (LLFF) Real Forward-Facing Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#tanks-and-temples">Tanks and Temples</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#dtu-dataset">DTU Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#human3-6m">Human3.6M</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#zju-mocap-dataset">ZJU-MoCap Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#people-snapshot">People-Snapshot</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#synthetic-dynamic-scenes">Synthetic dynamic scenes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#other-datasets">Other datasets</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html">3D Gaussian Splatting: A Basic Introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#foundations">Foundations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#what-is-a-3d-scene">What is a 3D Scene?</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-scene-representation">3D Scene Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#computer-graphics-fundamentals">Computer Graphics Fundamentals</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#rasterization-and-ray-tracing">Rasterization and Ray Tracing</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#alpha-blending-and-compositing">Alpha Blending and Compositing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#the-evolution-of-novel-view-synthesis">The Evolution of Novel View Synthesis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#image-based-rendering">Image-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#structure-from-motion-and-multi-view-stereo">Structure-from-Motion and Multi-View Stereo</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#point-based-rendering">Point-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#neural-rendering">Neural Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#accelerated-neural-fields">Accelerated Neural Fields</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussian-splatting-a-convergence-of-approaches">3D Gaussian Splatting: A Convergence of Approaches</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#id1">Point-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#point-clouds-and-their-challenges">Point Clouds and Their Challenges</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#the-concept-of-splatting">The Concept of Splatting</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#elliptical-weighted-average-ewa-filtering">Elliptical Weighted Average (EWA) Filtering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-point-based-rendering">Differentiable Point-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussian-splatting-core-principles">3D Gaussian Splatting: Core Principles</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#key-insight-unifying-points-and-volumes">Key Insight: Unifying Points and Volumes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussians-as-scene-primitives">3D Gaussians as Scene Primitives</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#the-volumetric-rendering-equation">The Volumetric Rendering Equation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#alpha-compositing-with-gaussians">Alpha Compositing with Gaussians</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#mathematical-formulation-of-3d-gaussian-splatting">Mathematical Formulation of 3D Gaussian Splatting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#projecting-3d-gaussians-to-2d">Projecting 3D Gaussians to 2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#parameterization-of-3d-gaussians">Parameterization of 3D Gaussians</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#view-dependent-appearance">View-Dependent Appearance</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-rendering-equations">Differentiable Rendering Equations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#training-and-optimization">Training and Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#photometric-loss">Photometric Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#initial-point-cloud">Initial Point Cloud</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#optimization-process">Optimization Process</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-splatting-pipeline">Differentiable Splatting Pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#adaptive-density-control">Adaptive Density Control</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#implementation-and-real-time-rendering">Implementation and Real-Time Rendering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#tile-based-rendering">Tile-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#fast-sorting-strategies">Fast Sorting Strategies</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#gpu-accelerated-rasterization">GPU-Accelerated Rasterization</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#memory-considerations">Memory Considerations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#comparison-with-other-methods">Comparison with Other Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#nerf-vs-3d-gaussian-splatting">NeRF vs. 3D Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#voxel-based-representations-vs-gaussians">Voxel-Based Representations vs. Gaussians</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#traditional-point-based-rendering-vs-gaussian-splatting">Traditional Point-Based Rendering vs. Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#applications-and-extensions">Applications and Extensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#static-scene-reconstruction">Static Scene Reconstruction</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#dynamic-scene-capture">Dynamic Scene Capture</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#avatar-creation-and-animation">Avatar Creation and Animation</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#integration-with-neural-rendering">Integration with Neural Rendering</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#large-scale-scene-rendering">Large-Scale Scene Rendering</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#bezier-gaussian-triangles-bg-triangle-for-sharper-rendering">Bézier Gaussian Triangles (BG-Triangle) for Sharper Rendering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#representation">Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#performance">Performance</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#human-reconstruction-with-gaussian-splatting-and-priors">Human Reconstruction with Gaussian Splatting and Priors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#eg-humannerf-efficient-generalizable-human-nerf">EG-HumanNeRF: Efficient Generalizable Human NeRF</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#gps-gaussian-pixel-wise-gaussian-splatting-for-humans">GPS-Gaussian: Pixel-Wise Gaussian Splatting for Humans</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#generalizable-human-gaussians-ghg-with-smpl">Generalizable Human Gaussians (GHG) with SMPL</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#dynamic-scene-reconstruction-with-4d-gaussian-splatting">Dynamic Scene Reconstruction with 4D Gaussian Splatting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussian-splatting-4dgs">4D Gaussian Splatting (4DGS)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#speed-and-memory-enhancements-4dgs-1k-and-mega">Speed and Memory Enhancements (4DGS-1K and MEGA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#applications-to-mocap-and-4d-human-rendering">Applications to MoCap and 4D Human Rendering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#implementation-details-and-real-time-performance">Implementation Details and Real-Time Performance</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#data-structures">Data Structures</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#rasterization-shaders">Rasterization &amp; Shaders</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#gpu-memory-and-throughput">GPU Memory and Throughput</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-rendering-implementation">Differentiable Rendering Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#training-vs-inference-compute">Training vs. Inference Compute</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#accuracy-vs-speed-trade-offs">Accuracy vs. Speed trade-offs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#benchmarks-and-comparative-evaluation">Benchmarks and Comparative Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#static-scene-comparison">Static Scene Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#human-novel-view-comparison">Human Novel-View Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#dynamic-scene-comparison">Dynamic Scene Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#comparative-summary-table">Comparative Summary Table</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#datasets-and-resources">Datasets and Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#synthetic-nerf-dataset-blender-scenes">Synthetic NeRF Dataset (Blender Scenes)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#llff-local-light-field-fusion">LLFF (Local Light Field Fusion)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#tanks-and-temples">Tanks and Temples</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#multi-object-360-co3d-common-objects-in-3d">Multi-Object 360 (CO3D - Common Objects in 3D)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#amass-archive-of-motion-capture-as-surface-shapes">AMASS (Archive of Motion Capture as Surface Shapes)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#cape-clothed-auto-person-encoding">CAPE (Clothed Auto Person Encoding)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#thuman-thuman2-0">THuman / THuman2.0</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#renderpeople">RenderPeople</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#open-source-implementations">Open-Source Implementations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#future-directions">Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#improved-compression-techniques-for-memory-efficiency">Improved Compression Techniques for Memory Efficiency</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#handling-dynamic-and-deformable-scenes">Handling Dynamic and Deformable Scenes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#advanced-material-modeling-for-realistic-rendering">Advanced Material Modeling for Realistic Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#hybrid-approaches-integrating-neural-fields-and-explicit-representations">Hybrid Approaches Integrating Neural Fields and Explicit Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#scalability-for-large-scale-scenes-city-level-and-beyond">Scalability for Large-Scale Scenes (City-Level and Beyond)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#real-time-applications-in-ar-vr-and-gaming">Real-Time Applications in AR/VR and Gaming</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#integration-with-existing-graphics-pipelines">Integration with Existing Graphics Pipelines</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#learning-from-limited-data">Learning from Limited Data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#conclusion">Conclusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#glossary">Glossary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a><ul>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-01-1-historical-body-models">Lecture 01.1 (Historical Body Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-01-2-introduction-to-human-models">Lecture 01.2 (Introduction to Human Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-01-3-introduction-to-human-models-continued">Lecture 01.3 (Introduction to Human Models Continued)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-02-1-image-formation">Lecture 02.1 (Image Formation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-02-2-rotations-kinematic-chains">Lecture 02.2 (Rotations &amp; Kinematic Chains)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-03-1-surface-representations">Lecture 03.1 (Surface Representations)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-03-2-procrustes-alignment">Lecture 03.2 (Procrustes Alignment)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-04-1-iterative-closest-points">Lecture 04.1 (Iterative Closest Points)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-04-2-body-models">Lecture 04.2 (Body Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-05-1-body-model-training">Lecture 05.1 (Body Model Training)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-05-2-3d-registration">Lecture 05.2 (3D Registration)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-06-1-fitting-smpl-to-images">Lecture 06.1 (Fitting SMPL to Images)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-06-1-optimization-based-fitting-of-smpl-to-images">Lecture 06.1 (Optimization-Based Fitting of SMPL to Images)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-06-2-learning-based-fitting-of-smpl-to-images">Lecture 06.2 (Learning-Based Fitting of SMPL to Images)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-07-1-fitting-smpl-to-imu-optimization">Lecture 07.1 (Fitting SMPL to IMU Optimization)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-07-2-fitting-smpl-to-imu-learning">Lecture 07.2 (Fitting SMPL to IMU Learning)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="references.html#classic-and-optimization-based-methods">Classic and Optimization-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="references.html#learning-based-methods">Learning-Based Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="references.html#datasets-and-resources">Datasets and Resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#relevant-software-and-libraries">Relevant Software and Libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-08-1-references-for-vertex-based-clothing-modeling-for-virtual-humans">Lecture 08.1: References for Vertex-Based Clothing Modeling for Virtual Humans</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#body-models-and-shape-estimation">Body Models and Shape Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#alternative-representations">Alternative Representations</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#datasets">Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#software-and-libraries">Software and Libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-09-1-references-for-neural-implicit-and-point-based-representations-for-clothed-human-modeling">Lecture 09.1: References for Neural Implicit and Point-Based Representations for Clothed Human Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#point-based-models">Point-Based Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#id50">Datasets and Resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#related-software-and-libraries">Related Software and Libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#review-papers-and-tutorials">Review Papers and Tutorials</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#neural-radiance-fields-nerf">Neural Radiance Fields (NERF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#gaussian-splatting">Gaussian Splatting</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Virtual Humans Lecture</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Lecture 07.1: Fitting SMPL to IMU Data Using Optimization-Based Methods</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/lecture_07_1_fitting_SMPL_to_IMU_optimization.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="lecture-07-1-fitting-smpl-to-imu-data-using-optimization-based-methods">
<span id="lecture-fitting-smpl-to-imu-optimization"></span><h1>Lecture 07.1: Fitting SMPL to IMU Data Using Optimization-Based Methods<a class="headerlink" href="#lecture-07-1-fitting-smpl-to-imu-data-using-optimization-based-methods" title="Link to this heading"></a></h1>
<iframe width="600" height="400" src="https://www.youtube.com/embed/ISFp6gfP2V4"
title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write;
encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><p><a class="reference external" href="https://virtualhumans.mpi-inf.mpg.de/VH23/slides/pdf/Lecture_07_1_Fitting_SMPL_to_IMU_Optimization.pdf">Lecture Slides: Fitting SMPL to IMU Data Using Optimization-Based Methods</a></p>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h2>
<p>In this lecture, we explore how to fit a parametric human body model (SMPL) to inertial measurement unit (IMU) data using optimization techniques. We begin with a historical perspective on classical IMU-based pose estimation methods, highlighting how the field evolved from strapdown navigation and filtering approaches to modern model-based optimization. We then introduce the mathematical foundations of inertial sensors, covering how accelerometers and gyroscopes measure motion and orientation, as well as how we represent rotations (using matrices, quaternions, etc.) while accounting for gravity alignment, drift, and sensor biases. With these foundations, we formulate a rigorous optimization pipeline for fitting the SMPL model to IMU data. This includes calibrating IMUs to body segments, choosing a pose parameterization, defining objective functions based on orientation and acceleration consistency, adding anthropomorphic priors, deriving gradients, and solving the nonlinear least squares problem (e.g., via Levenberg–Marquardt or L-BFGS). Pseudocode is provided to outline the full pipeline for estimating a sequence of poses from IMU readings. Finally, we discuss key datasets for IMU-based human pose estimation – such as DIP-IMU, TotalCapture, and AMASS – including their scope and how they can be accessed.</p>
</section>
<section id="classical-imu-based-pose-estimation-a-historical-perspective">
<h2>Classical IMU-Based Pose Estimation: A Historical Perspective<a class="headerlink" href="#classical-imu-based-pose-estimation-a-historical-perspective" title="Link to this heading"></a></h2>
<p>Early approaches to human pose estimation with IMUs drew heavily from strapdown inertial navigation and classical state estimation techniques developed in aerospace and robotics. In a strapdown inertial navigation system, gyroscopes and accelerometers rigidly attached (“strapped down”) to a body are integrated to track the body’s orientation and position. The gyroscopes measure the body’s angular velocity in its local (body-fixed) frame, providing signals <span class="math notranslate nohighlight">\(\omega_i(t)\)</span> along each local axis. Integrating these angular rates over time (solving <span class="math notranslate nohighlight">\(\dot{R}=R[\Omega]_{\times}\)</span> for rotation matrix <span class="math notranslate nohighlight">\(R(t)\)</span>) yields the orientation trajectory. Likewise, accelerometers measure specific force – the difference between the body’s linear acceleration and gravity – in the body frame. Double integration of accelerometer readings (after adding back gravity in the global frame) can yield position, but with unbounded drift errors in practice. Traditional strapdown algorithms demonstrated that orientation can be maintained accurately over short periods by pure integration, but drift accumulates in yaw and position, requiring additional corrections. These methods were initially applied to vehicles and rigid bodies; applying them directly to human motion is challenging due to segments’ complex motion and the lack of direct position references on the body.</p>
<section id="attitude-and-heading-reference-systems">
<h3>Attitude and Heading Reference Systems<a class="headerlink" href="#attitude-and-heading-reference-systems" title="Link to this heading"></a></h3>
<p>To stabilize orientation estimates, researchers introduced attitude and heading reference systems (AHRS) and complementary filtering techniques. Notably, Mahony et al. (2008) proposed a nonlinear complementary filter on <span class="math notranslate nohighlight">\(SO(3)\)</span> that fuses gyroscope data with accelerometer (and optionally magnetometer) measurements to track orientation. The Mahony filter treats orientation estimation as a deterministic observer on the rotation group, using accelerometer readings to correct the tilt (roll/pitch) drift and magnetometer readings to correct heading, while also estimating gyroscope bias online. This approach is essentially a real-time feedback loop that blends high-frequency gyro integration with low-frequency gravity/magnetic corrections, achieving near-zero steady-state error on orientation. Around the same time, Madgwick’s filter (2010) introduced a gradient-descent based orientation estimator using quaternions. The Madgwick filter also fuses gyro and accelerometer (and magnetometer in a MARG sensor setup) by minimizing an orientation error function, and it demonstrated accuracy comparable to Kalman filtering while being computationally light. Both Mahony and Madgwick filters became popular in embedded IMU applications for orientation tracking, as they require minimal processing yet substantially reduce drift by using the accelerometer to “anchor” the gravity direction and magnetometer to fix yaw.</p>
</section>
<section id="kalman-filter-approaches">
<h3>Kalman Filter Approaches<a class="headerlink" href="#kalman-filter-approaches" title="Link to this heading"></a></h3>
<p>In parallel, the Kalman filter and its variants (Extended Kalman Filter, Unscented Kalman Filter) were applied to human motion tracking. In commercial inertial motion capture systems like Xsens MVN, an EKF is used to fuse the inertial sensor data (accelerometers, gyros, magnetometers on each body segment) and constrain them with a kinematic human model. The full body is modeled as a chain of segments, each with an IMU, and the EKF estimates the 3D orientation of each segment. Known joint constraints or contact events can be incorporated as additional measurements. These systems achieve real-time full-body pose capture with many sensors (Xsens uses 17 IMUs for 23 body segments), and serve as a ground truth reference for reduced-sensor approaches. Academic works also explored Kalman filtering for lower-limb or upper-body tracking with fewer IMUs by modeling the human as an articulated system and updating joint angles from IMU measurements. Kalman filters provide a probabilistic framework to handle sensor noise and can incorporate prior knowledge of motion dynamics, but designing the state-space model and filter for a high-dimensional human skeleton is complex and computationally heavy. Simpler complementary filters (like Mahony/Madgwick) became prevalent for just orientation estimation of each segment, due to ease of tuning and stability.</p>
</section>
<section id="early-sparse-sensor-approaches">
<h3>Early Sparse-Sensor Approaches<a class="headerlink" href="#early-sparse-sensor-approaches" title="Link to this heading"></a></h3>
<p>The desire to reduce the number of worn sensors (to make motion capture less obtrusive) led to data-driven pose estimation methods in the late 2000s and early 2010s. Slyper and Hodgins (2008) demonstrated an early system with only five accelerometers (no gyros, thus measuring only accelerations) placed on the upper body. They used a database of example motions and a nearest-neighbor search to map the sparse accelerometer signals to likely full-body poses. This “lazy learning” approach showed that even with very limited inertial data, one could retrieve plausible motions from a large motion library. Tautges et al. (2011) extended this idea by adding temporal scaling, allowing motions performed at different speeds to be matched via time-warping of the sensor signals. Riaz et al. (2015) further reduced the sensor count to three accelerometers (on the lower back and both wrists) by incorporating heuristic foot-ground contact detection to aid pose estimation. These early sparse-sensor methods demonstrated the possibility of “mocap in the wild” with minimal equipment, but they suffered from the fundamental limitations of using only accelerometers. Accelerometers provide information about changes in motion (specific force) but not the static orientation of limbs – as a result, these methods could capture dynamic movements but struggled with slow or static poses, and they relied heavily on prior motion data for plausibility. In summary, before 2015, sparse IMU pose estimation either required many sensors with filtering (as in Xsens) or used very few sensors with data-driven reconstructions, but no approach yet provided accurate, model-consistent 3D poses from a truly minimal IMU set without external infrastructure.</p>
</section>
<section id="model-based-optimization-methods">
<h3>Model-Based Optimization Methods<a class="headerlink" href="#model-based-optimization-methods" title="Link to this heading"></a></h3>
<p>A breakthrough came with the introduction of model-based optimization methods that explicitly fit a parametric human model to IMU data. The seminal work Sparse Inertial Poser (SIP) by von Marcard et al. (2017) was the first to recover full 3D human pose using only 6 IMUs (located on the wrists, ankles, pelvis, and head) by leveraging a statistical body model and temporal optimization. SIP made the problem tractable by combining two key ideas: (i) using a realistic skinned body model (SMPL) which enforces anthropometric consistency (i.e. bone lengths and shape are human-like) and (ii) formulating a joint optimization over multiple frames to fit the model’s pose to the IMUs’ orientation and acceleration readings. Instead of frame-by-frame pose estimation, they solved for an entire motion trajectory that best explains the sensor data in a batch optimization. By incorporating pose priors and physics (acceleration consistency), SIP overcame the ambiguity of sparse sensors. Their results showed accurate reconstruction of complex motions (walking, running, climbing, etc.) using a small set of inertial sensors without any cameras. This was a significant improvement over earlier methods and even outperformed a baseline that used the same six sensors plus a large motion database. SIP’s analysis-by-synthesis approach – simulate the IMU measurements from the model and adjust the model to match the measurements – became a foundation for subsequent research.</p>
</section>
<section id="learning-based-methods">
<h3>Learning-Based Methods<a class="headerlink" href="#learning-based-methods" title="Link to this heading"></a></h3>
<p>Following SIP, researchers explored learning-based methods to improve speed and generalization. Deep Inertial Poser (DIP) by Huang et al. (2018) introduced a neural network approach to map IMU data to human pose in real time. They noted that while optimization (as in SIP) effectively exploits temporal information and physical constraints, it is too slow for real-time use and can get stuck in poor local minima without a good initialization. DIP addressed these issues by training a bi-directional recurrent neural network on a large corpus of motions (including synthesized IMU data from motion capture). The network learns a temporal pose prior, effectively imitating the role of the optimizer but running orders of magnitude faster at inference. DIP’s introduction of a dedicated IMU-motion dataset (DIP-IMU, see Section on Datasets) and an open-source implementation made sparse IMU pose estimation more accessible. In the years since, further improvements have been made: for example, Transformer-based models have been used to better capture long-term dependencies in IMU sequences, and approaches like TransPose and PIP (Physics-informed Inertial Poser) integrate learned models with physical knowledge for drift correction. Researchers have also looked at combining IMUs with other sensors (e.g., cameras or ultrawide-band): von Marcard et al. (2018) showed that coupling 6 IMUs with a moving camera (e.g., a smartphone filming the person) allows full global pose estimation in outdoor scenes, leveraging both visual and inertial cues. Nonetheless, the focus of this lecture is on the optimization-based fitting paradigm, as it provides a clear framework to understand the problem’s constraints and serves as the basis upon which many learning approaches build or to which they are compared.</p>
</section>
</section>
<section id="inertial-sensor-fundamentals-and-orientation-representations">
<h2>Inertial Sensor Fundamentals and Orientation Representations<a class="headerlink" href="#inertial-sensor-fundamentals-and-orientation-representations" title="Link to this heading"></a></h2>
<p>To fit a body model to IMU data, we must understand what information IMUs provide and how to represent rotations and orientations mathematically. An inertial measurement unit (IMU) typically consists of a tri-axial accelerometer and tri-axial gyroscope (often also a magnetometer, though many methods consider 6-DoF IMUs without magnetometers). We denote an IMU’s local coordinate frame as the sensor frame, and assume it is rigidly attached to a body segment. The sensor provides two primary measurements at each time step:</p>
<ol class="arabic simple">
<li><p><strong>Angular velocity</strong> <span class="math notranslate nohighlight">\(\Omega_y\)</span> in the body (sensor) frame, measured by the gyroscope. The gyroscope reading (a 3D vector) relates to the true angular velocity <span class="math notranslate nohighlight">\(\Omega\)</span> of the sensor by <span class="math notranslate nohighlight">\(\Omega_y=\Omega+b+\mu\)</span>, where <span class="math notranslate nohighlight">\(b\)</span> is a (slowly-varying) bias and <span class="math notranslate nohighlight">\(\mu\)</span> is noise. In other words, the gyro outputs the actual instantaneous rotation rate of the sensor plus some constant offset and noise. Integrating these readings yields orientation change. However, even a small bias can integrate into a large drift in orientation over time, since the gyro has no absolute reference.</p></li>
<li><p><strong>Linear acceleration (specific force)</strong> minus gravity, measured by the accelerometer. Formally, an ideal accelerometer gives <span class="math notranslate nohighlight">\(a=R^T(\ddot{p}-g_0)\)</span>, where <span class="math notranslate nohighlight">\(\ddot{p}\)</span> is the linear acceleration of the sensor in world coordinates, <span class="math notranslate nohighlight">\(g_0\)</span> is the gravity vector in world frame (approximately <span class="math notranslate nohighlight">\((0,0,-9.81\text{ m/s}^2)\)</span>), and <span class="math notranslate nohighlight">\(R\)</span> is the rotation matrix from world frame to sensor frame. The accelerometer also has bias and noise (denoted <span class="math notranslate nohighlight">\(b_a\)</span> and <span class="math notranslate nohighlight">\(\mu_a\)</span>), but a key point is that when the sensor is static (no linear movement), <span class="math notranslate nohighlight">\(\ddot{p}=0\)</span> and thus the accelerometer’s reading is just the gravity vector expressed in the sensor’s frame (plus bias/noise). This fact allows accelerometers to act as an absolute inclination reference: by normalizing the measured acceleration <span class="math notranslate nohighlight">\(a\)</span> when motion is slow, one obtains a unit vector <span class="math notranslate nohighlight">\(v_a \approx -R^T e_3\)</span> that points opposite to gravity (here <span class="math notranslate nohighlight">\(e_3=(0,0,1)^T\)</span> is the upward unit vector). Thus, the accelerometer can tell us “which way is down,” i.e., the sensor’s tilt (roll and pitch angles) with respect to horizontal. It cannot, however, observe rotation about the gravity axis (yaw), since spinning about a vertical axis does not change the gravity projection. This is why a magnetometer (sensing the Earth’s magnetic field) or other reference is needed to fix absolute heading if gyro drift in yaw must be corrected.</p></li>
</ol>
<section id="gravity-alignment-and-drift-correction">
<h3>Gravity Alignment and Drift Correction<a class="headerlink" href="#gravity-alignment-and-drift-correction" title="Link to this heading"></a></h3>
<p>In practice, IMU orientation tracking algorithms (like Mahony/Madgwick filters or Kalman filters) use the accelerometer to correct gyro integration drift in the roll/pitch directions by comparing the expected gravity direction from the current orientation estimate to the measured acceleration direction. For example, if the IMU is estimated to be level but the accelerometer reads a significant horizontal component of gravity, the filter will apply a corrective rotation to align the estimated orientation with gravity over time. This forms a complementary behavior: gyros dominate the high-frequency motion tracking (since they respond instantaneously to rotation), while accelerometers provide a low-frequency reference (gravity). Magnetometers, when available, do the same for yaw, comparing the device’s heading to the Earth’s magnetic north and correcting drift about vertical. In an optimization context, we will similarly use accelerometer readings to constrain the orientation and even the translational acceleration of body segments, essentially anchoring the solution to gravity.</p>
</section>
<section id="orientation-representations">
<h3>Orientation Representations<a class="headerlink" href="#orientation-representations" title="Link to this heading"></a></h3>
<p>To perform computations on orientations, we need a mathematical representation. The orientation of a rigid body (or IMU) can be represented by a rotation matrix <span class="math notranslate nohighlight">\(R \in SO(3)\)</span>, which is an orthonormal <span class="math notranslate nohighlight">\(3 \times 3\)</span> matrix rotating vectors from the local frame to the global frame. Rotation matrices are unique and convenient for transforming vectors, but have 9 parameters with 6 constraints (not minimal). Another popular representation is the unit quaternion <span class="math notranslate nohighlight">\(q=(w,x,y,z)\)</span> (with <span class="math notranslate nohighlight">\(w^2+x^2+y^2+z^2=1\)</span>), which is a 4-parameter representation of the rotation. Unit quaternions can be converted to matrices and composed (multiplied) to combine rotations; they avoid the singularities (gimbal lock) that Euler angles have. We often use quaternions in IMU filtering because they are numerically stable for integration (the differential equation <span class="math notranslate nohighlight">\(\dot{q}=\frac{1}{2}q \otimes (0,\Omega)\)</span> integrates the gyro reading <span class="math notranslate nohighlight">\(\Omega\)</span> into the quaternion over time). In optimization, however, one may prefer the axis-angle (exponential map) representation for each joint’s rotation: this is a 3D vector <span class="math notranslate nohighlight">\(\theta=\theta u\)</span> encoding a rotation of <span class="math notranslate nohighlight">\(\theta\)</span> radians about axis <span class="math notranslate nohighlight">\(u\)</span>. The rotation matrix can be obtained as <span class="math notranslate nohighlight">\(R=\exp([{\theta}]_{\times})\)</span> using Rodrigues’ formula. The axis-angle vector is an efficient minimal representation (3 parameters for any 3D rotation), and is the format used for pose parameters in the SMPL model. We denote by <span class="math notranslate nohighlight">\(\log(R)\)</span> the inverse operation, giving an axis-angle vector in <span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span> from a rotation matrix. This is useful for measuring differences: for two orientations <span class="math notranslate nohighlight">\(R_1,R_2 \in SO(3)\)</span>, one measure of discrepancy is the rotation <span class="math notranslate nohighlight">\(R_{err}=R_1 R_2^T\)</span>; we can represent <span class="math notranslate nohighlight">\(R_{err}\)</span> as an axis-angle vector <span class="math notranslate nohighlight">\(\delta\)</span> and use its magnitude <span class="math notranslate nohighlight">\(\|\delta\|\)</span> (which equals the rotation angle) as the geodesic distance between orientations.</p>
</section>
<section id="sensor-calibration">
<h3>Sensor Calibration<a class="headerlink" href="#sensor-calibration" title="Link to this heading"></a></h3>
<p>Before fitting IMUs to a body model, a one-time calibration is usually performed. This involves determining the orientation offset between each IMU’s axis alignment and the body segment’s anatomical orientation. For example, an IMU strapped to a person’s thigh might not be perfectly aligned with the thigh’s coordinate frame as defined by the body model (SMPL’s joint frame). Calibration can be done by having the user assume a known pose (like T-pose or stand straight) and computing a constant rotation that aligns the IMU reading to the model’s expected orientation for that pose. This yields a static rotation <span class="math notranslate nohighlight">\(R_{cal,i}\)</span> for each sensor <span class="math notranslate nohighlight">\(i\)</span> that transforms from the sensor’s frame to the corresponding body segment frame. We will assume these calibration rotations are known, so we can directly compare sensor measurements with model predictions in a common coordinate system.</p>
</section>
</section>
<section id="optimization-based-smpl-fitting-with-imu-data">
<h2>Optimization-Based SMPL Fitting with IMU Data<a class="headerlink" href="#optimization-based-smpl-fitting-with-imu-data" title="Link to this heading"></a></h2>
<p>With sensor models and orientation math established, we now formulate the optimization problem of fitting the SMPL body model to a sequence of IMU measurements. SMPL (Skinned Multi-Person Linear model) is a parametric model that represents the 3D human body with pose and shape parameters. The shape parameters <span class="math notranslate nohighlight">\(\beta\)</span> (a low-dimensional vector, e.g. 10 coefficients) define a neutral body mesh and joint lengths, while the pose parameters <span class="math notranslate nohighlight">\(\Theta\)</span> (usually 24 joints × 3 axis-angle values = 72-D) define the rotation of each joint in the kinematic chain. SMPL provides a function <span class="math notranslate nohighlight">\(M(\beta,\Theta)\)</span> that outputs a full mesh and joint locations. For our purposes, we care about the global transforms of certain body segments where IMUs are attached. We assume we have <span class="math notranslate nohighlight">\(N\)</span> IMUs on the subject, attached to known segments (e.g., N=6 in SIP: IMUs on head, torso, two wrists, and two ankles). We are given a time sequence <span class="math notranslate nohighlight">\(t=1,\ldots,T\)</span> of IMU data: typically this includes an orientation estimate (often as a quaternion <span class="math notranslate nohighlight">\(Q_{i,t}^{meas}\)</span>) for each sensor <span class="math notranslate nohighlight">\(i\)</span>, and linear acceleration readings <span class="math notranslate nohighlight">\(a_{i,t}^{meas}\)</span> from each sensor’s accelerometer. Our goal is to find the sequence of SMPL pose parameters <span class="math notranslate nohighlight">\(\Theta_1,\Theta_2,\ldots,\Theta_T\)</span> (and possibly a fixed shape <span class="math notranslate nohighlight">\(\beta\)</span> for the subject) that best explain these IMU measurements.</p>
<section id="kinematic-model-and-sensor-prediction">
<h3>Kinematic Model and Sensor Prediction<a class="headerlink" href="#kinematic-model-and-sensor-prediction" title="Link to this heading"></a></h3>
<p>We first describe how, for a given pose <span class="math notranslate nohighlight">\(\Theta_t\)</span>, we can predict the measurements an IMU would read. Using forward kinematics on SMPL’s skeleton, we can compute the global rotation <span class="math notranslate nohighlight">\(R_{i,t}^{pred}\)</span> of each body segment <span class="math notranslate nohighlight">\(i\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> (this is obtained by composing the rotations from the root up to that segment in the kinematic tree). We then apply the calibration offset for that IMU, i.e. <span class="math notranslate nohighlight">\(R_{i,t}^{pred}=R_{i,t}^{segment} \cdot R_{cal,i}\)</span>, which gives the predicted global orientation of the IMU box. This predicted orientation can be compared to the IMU’s reported orientation <span class="math notranslate nohighlight">\(R_{i,t}^{meas}\)</span> (from the sensor’s internal filter or a localization algorithm). We quantify the orientation discrepancy by the rotation <span class="math notranslate nohighlight">\(R_{i,t}^{err}=R_{i,t}^{pred}(R_{i,t}^{meas})^{-1}\)</span>. Let <span class="math notranslate nohighlight">\(\delta\theta_{i,t}=\log(R_{i,t}^{err})\)</span> be the axis-angle vector of this small rotation. Ideally, <span class="math notranslate nohighlight">\(\delta\theta_{i,t}=0\)</span> if the model’s segment orientation matches the sensor. So one component of our cost function will be the orientation residual:</p>
<div class="math notranslate nohighlight">
\[E_{orient}=\sum_{t=1}^{T}\sum_{i=1}^{N} \| \delta\theta_{i,t} \|^2,\]</div>
<p>the sum of squared geodesic angle differences between predicted and measured orientations for all sensors over all frames. This term encourages the model’s pose to have each instrumented limb oriented correctly relative to the global frame.</p>
<p>Next, consider the accelerometer readings. The IMU provides <span class="math notranslate nohighlight">\(a_{i,t}^{meas}\)</span>, which (after subtracting any known bias) equals <span class="math notranslate nohighlight">\(\mathbf{R}_{i,t}^{\text{sensor}}\!^T (\ddot{\mathbf{p}}_{i,t} - \mathbf{g}_0)\)</span>, where <span class="math notranslate nohighlight">\(\ddot{p}_{i,t}\)</span> is the global acceleration of the sensor and <span class="math notranslate nohighlight">\(R_{i,t}^{sensor}\)</span> is the sensor’s orientation (global-to-local rotation). If we rotate the measured acceleration into the global frame using the measured orientation, we get:</p>
<div class="math notranslate nohighlight">
\[a_{i,t}^{global}=R_{i,t}^{meas} a_{i,t}^{meas} \approx \ddot{p}_{i,t}-g_0.\]</div>
<p>We can predict <span class="math notranslate nohighlight">\(\ddot{p}_{i,t}\)</span> from the model by taking second differences of the IMU’s position on the model. Let <span class="math notranslate nohighlight">\(p_{i,t}^{pred}\)</span> be the global 3D position of IMU <span class="math notranslate nohighlight">\(i\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> according to the SMPL model (this is obtained from the forward kinematics, using the joint angles to get the segment pose and then adding the offset from joint to the IMU’s location). A simple approximation of the second time derivative is <span class="math notranslate nohighlight">\(\ddot{p}_{i,t} \approx (p_{i,t+1}^{pred}-2p_{i,t}^{pred}+p_{i,t-1}^{pred})/\Delta t^2\)</span> for <span class="math notranslate nohighlight">\(2 \leq t \leq T-1\)</span>. Using this, our model-predicted specific force in global frame is <span class="math notranslate nohighlight">\(\ddot{p}_{i,t}^{pred}-g_0\)</span>. We then define the acceleration residual as the difference between predicted and measured specific force:</p>
<div class="math notranslate nohighlight">
\[r_{i,t}^{acc}=(\ddot{p}_{i,t}^{pred}-g_0)-a_{i,t}^{global},\]</div>
<p>and include its squared norm in the cost:</p>
<div class="math notranslate nohighlight">
\[E_{accel}=\sum_{t=2}^{T-1}\sum_{i=1}^{N} \| r_{i,t}^{acc} \|^2.\]</div>
<p>This term encourages the model’s motion to have the correct linear acceleration for each sensor. Intuitively, if the person kicks their leg, the leg’s IMU will sense a forward acceleration followed by a backward acceleration (when the leg decelerates); the optimizer will try to make the leg in the model move such that its accelerations match that pattern. Note that including acceleration couples the poses across time – this provides a form of temporal smoothing constraint, as physically plausible accelerations cannot be caused by jittery, frame-wise independent poses. It also helps correct drift in position/orientation: e.g. if the model starts to drift, the accelerometer residuals will grow, pushing the solution back in line. In practice, orientation measurements (from the IMU’s onboard filter) are much less noisy than acceleration, so algorithms often weight the orientation residual more heavily than the acceleration residual (e.g., <span class="math notranslate nohighlight">\(w_{acc} &lt; w_{orient}\)</span>). Some implementations even filter or integrate acceleration information over windows to reduce noise. Nonetheless, acceleration data is crucial for capturing fast dynamics and for observing the translational components of motion which a pure orientation-only approach would miss (orientation alone cannot tell if a person is stationary or jumping, for example).</p>
</section>
<section id="regularization-and-prior-terms">
<h3>Regularization and Prior Terms<a class="headerlink" href="#regularization-and-prior-terms" title="Link to this heading"></a></h3>
<p>In addition to these sensor consistency terms, we must include regularization and prior terms to ensure the estimated pose is physically and biologically plausible (and to prevent the solution from drifting in unobservable directions). One key prior is the anthropometric consistency provided by the SMPL model’s shape parameters. If the shape <span class="math notranslate nohighlight">\(\beta\)</span> is known (e.g., measured or estimated from the person’s height/weight), we can fix it; if not, it can be treated as an unknown and optimized as well, but with a strong prior to avoid overfitting the IMU data. A common approach is to include a penalty on deviations from a mean shape or known limb lengths. For example, if no shape estimate is given, one could add <span class="math notranslate nohighlight">\(E_{shape}=\|\beta\|^2/\sigma^2\)</span> assuming a zero-mean Gaussian prior on shape coefficients. In SIP, anthropometric consistency means the body model will not produce impossible limb lengths or proportions, which already constrains the pose search space heavily. Beyond shape, we often add a pose prior term <span class="math notranslate nohighlight">\(E_{pose}\)</span>. This can be a statistical prior learned from motion capture data (e.g., a Gaussian mixture model over joint angles as in SMPLify), or simply a quadratic penalty on joint angle deviations from a neutral pose. The prior discourages extreme or nonsensical poses (e.g., limbs bending in unlikely ways) especially under noisy measurements or in areas where the IMUs have no sensitivity. For instance, if the subject’s lower body has no sensors, the prior will keep the legs in a reasonable posture unless accelerometers force a change. Another regularizer can be a temporal smoothness prior, such as <span class="math notranslate nohighlight">\(E_{smooth}=\sum_t\|\Theta_t-\Theta_{t-1}\|^2\)</span>, to penalize abrupt changes in pose between frames (though the acceleration term already implicitly encourages smooth motion).</p>
<p>Combining all terms, the full objective function can be written as:</p>
<div class="math notranslate nohighlight">
\[E(\beta,\Theta_{1..T})=w_o E_{orient}+w_a E_{accel}+w_p E_{pose}+w_s E_{shape},\]</div>
<p>with weights <span class="math notranslate nohighlight">\(w_o,w_a,w_p,w_s\)</span> set according to the confidence in each component. The goal is to find the pose parameters <span class="math notranslate nohighlight">\(\Theta_{1..T}\)</span> (and possibly shape <span class="math notranslate nohighlight">\(\beta\)</span>) that minimize this energy. This is a nonlinear least-squares problem, since the residuals are nonlinear functions of the pose parameters.</p>
</section>
<section id="gradient-and-jacobian-computation">
<h3>Gradient and Jacobian Computation<a class="headerlink" href="#gradient-and-jacobian-computation" title="Link to this heading"></a></h3>
<p>To solve the optimization, we typically use gradient-based algorithms. The function is differentiable with respect to the pose parameters (assuming we use a smooth representation like axis-angles or quaternions for the orientation; quaternions would require adding a unit-norm constraint, so often axis-angles are easier to handle in unconstrained optimization). Let <span class="math notranslate nohighlight">\(x\)</span> denote the stacked unknowns (all poses and perhaps shape). We need the gradient <span class="math notranslate nohighlight">\(\nabla_x E\)</span> and ideally the Hessian or Jacobian structure. The objective is a sum of squared residuals, which makes it suitable for Gauss-Newton or Levenberg–Marquardt (LM) optimization. We can derive the Jacobian of each residual (orientation or acceleration) with respect to the pose parameters. This essentially involves the kinematic Jacobians of the body model. For example, the orientation of sensor <span class="math notranslate nohighlight">\(i\)</span> depends on all joint rotations from the root up to that segment. We can compute the partial derivative of the sensor’s orientation (in axis-angle form) with respect to each joint angle on that path. In practice, one can derive that a small change in a parent joint angle <span class="math notranslate nohighlight">\(\delta\theta_j\)</span> will induce a small rotation of the sensor frame equal to <span class="math notranslate nohighlight">\(\delta\theta_j\)</span> rotated into the sensor’s frame (this is related to the concept of screw axes in robotics). Similarly, the position <span class="math notranslate nohighlight">\(p_{i,t}\)</span> of a sensor has a well-known Jacobian in terms of the joint angles (this is the translational part of the spatial Jacobian in robotic kinematics). Analytically deriving all these Jacobians is complex but feasible; however, modern implementations often use automatic differentiation or the chain rule on the computational graph of the model. The SMPL model provides derivatives of joint positions with respect to pose and shape parameters, since it’s essentially a linear blend skinning model. For orientation residuals, one convenient strategy is to map the rotation difference to a 3D angle-axis error <span class="math notranslate nohighlight">\(\delta\theta\)</span> as we did, and use the fact that for a small orientation error, the gradient with respect to a parent joint is proportional to the cross product of the parent joint’s axis and the sensor’s orientation error vector. More concretely, if joint <span class="math notranslate nohighlight">\(j\)</span> affects sensor <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\(\frac{\partial\delta\theta_i}{\partial\theta_j} \approx R_j^{global}a_j\)</span>, where <span class="math notranslate nohighlight">\(a_j\)</span> is the joint’s rotation axis in its local frame (and <span class="math notranslate nohighlight">\(R_j^{global}\)</span> brings it to global coordinates). This yields a vector in the direction the sensor would rotate if joint <span class="math notranslate nohighlight">\(j\)</span> is rotated by a small amount, which can be compared to <span class="math notranslate nohighlight">\(\delta\theta_i\)</span>. The full derivation is beyond our scope, but essentially one builds a large sparse Jacobian matrix <span class="math notranslate nohighlight">\(J\)</span> that maps changes in all joint angles (and shape) to changes in all residuals.</p>
<p>Crucially, the Jacobian <span class="math notranslate nohighlight">\(J\)</span> is quite sparse: each residual involves only a subset of joints (for orientation, a sensor’s residual involves only the joints on the kinematic chain from the root to that sensor; for acceleration, it involves those joints across two adjacent time frames as well). The problem size for a sequence can be large (e.g., 6 IMUs, 100 frames, 72 pose params per frame = 7200 unknowns), but the sparsity can be exploited by sparse solvers. The original SIP implementation used a Gauss-Newton optimizer with the LM damping strategy and solved the normal equations <span class="math notranslate nohighlight">\(J^T J \Delta x = -J^T r\)</span> using a sparse Cholesky factorization. Each LM iteration refines the pose estimates, and it typically converges in 10–20 iterations for a given motion sequence. An alternative is to use a quasi-Newton method like L-BFGS (limited-memory BFGS), which requires only function and gradient evaluations (no explicit second-order step) and is memory-efficient for large problems. L-BFGS has been used in similar human pose optimization tasks (such as SMPLify for image-based fitting) and can be effective if the residuals have nice smooth behavior. However, the bundle-adjustment style structure of this problem (many frames, shared parameters) is well-suited to Gauss-Newton. Levenberg–Marquardt offers robustness by interpolating between Gauss-Newton (when close to minimum) and gradient descent (when far, via a damping factor). Each iteration will adjust the pose parameters to reduce the total error. The process may need a good initialization to succeed – typically, one could initialize all poses <span class="math notranslate nohighlight">\(\Theta_t\)</span> to some default (e.g., T-pose or the static pose indicated by initial IMU orientations) and velocities to zero. If a rough estimate of the subject’s orientation is known (say, from averaging IMU orientations over a second), that can initialize the global root orientation to reduce ambiguities (especially yaw, which accelerometers don’t provide).</p>
</section>
</section>
<section id="pseudocode-smpl-pose-estimation-from-imu-sequence">
<h2>Pseudocode: SMPL Pose Estimation from IMU Sequence<a class="headerlink" href="#pseudocode-smpl-pose-estimation-from-imu-sequence" title="Link to this heading"></a></h2>
<p>Below is a pseudocode outline of the entire pipeline for fitting SMPL to IMU data using optimization.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Inputs:
#   - SMPL model M(beta, Theta) with known shape beta (or initial beta with prior)
#   - IMU calibration rotations R_cal[i] for each sensor i (sensor-&gt;segment frame)
#   - Sensor measurements for t = 1...T:
#        Orientation quaternions Q_meas[i][t]  (global orientation of sensor i)
#        Acceleration vectors a_meas[i][t]    (local accelerometer readings of sensor i)
#   - Weights: w_o, w_a for orientation/accel terms; and pose/shape priors as needed
# Output:
#   - Estimated pose parameters Theta[1..T] (and possibly refined shape beta)

# Initialization:
for each frame t:
    Theta[t] := Theta_initial   (e.g., all joints to zero angles or a guessed pose)
beta := beta_initial (e.g., average shape or given measurements)
# Optionally, initialize global root orientation from IMU on torso to align model

# Optimization loop (Gauss-Newton or L-BFGS):
for iter = 1 to MaxIters:
    # Compute residuals and Jacobians
    total_error := 0
    J := zero_matrix()
    r := zero_vector()
    for t = 1 to T:
        # Forward kinematics for this frame:
        compute global rotations R_segment[j] for each joint j using Theta[t]
        for each sensor i:
            # Predicted orientation = segment rotation * calibration offset
            R_pred = R_segment[seg(i)] * R_cal[i]
            Q_pred = quaternion(R_pred)
            # Orientation residual (quaternion difference)
            Q_err = Q_pred * inv(Q_meas[i][t])
            delta_theta = axis_angle(Q_err)    # 3D angle-axis difference
            append delta_theta to residual vector r (orientation part)
            # Compute Jacobian rows for orientation residual wrt relevant Theta[t] joints
            for each joint j affecting sensor i:
                J_orient_part = d(delta_theta)/d(theta_j[t])   # use kinematic jacobian
                insert into J matrix
            # (If using quaternion directly, residual could be 4D with constraint)

            # Acceleration residual (if t is not first/last frame):
            if 2 &lt;= t &lt;= T-1:
                p_pred_prev = joint_position(seg(i), Theta[t-1], beta)
                p_pred_curr = joint_position(seg(i), Theta[t], beta)
                p_pred_next = joint_position(seg(i), Theta[t+1], beta)
                ddp_pred = (p_pred_next - 2*p_pred_curr + p_pred_prev) / (dt^2)
                a_global = quat_to_matrix(Q_meas[i][t]) * a_meas[i][t]
                acc_res = ddp_pred - g - a_global
                append acc_res to residual vector r (accel part)
                # Jacobians for accel residual:
                for each joint j affecting seg(i) at times t-1, t, t+1:
                    J_acc_part = d(acc_res)/d(theta_j[s])   # includes terms from pos jacobians
                    insert into J matrix
    # Add pose prior residuals &amp; Jacobians (e.g., (Theta[t] - Theta_prior)/sigma)
    # Add shape prior residuals &amp; Jacobians (if optimizing beta)

    # Check convergence criteria (e.g., small gradient or small error change)
    if ||r||^2 is below threshold:
        break

    # Solve for update step:
    if using Gauss-Newton/LM:
        # Normal equations: (J^T J + lambda I) dx = -J^T r
        solve for dx (e.g., via sparse Cholesky)
    if using L-BFGS:
        compute dx = -H_approx * (J^T r)    # uses accumulated info of gradients

    # Update parameters:
    apply dx to Theta and beta:
    for each frame t and joint j:
        theta_j[t] += dtheta_j[t]   (add the small rotation to the axis-angle, or compose quaternions)
    beta += dbeta  (update shape if applicable)

end for

Return Theta[1..T] (and beta)
</pre></div>
</div>
</section>
<section id="imu-based-human-pose-datasets-and-resources">
<h2>IMU-Based Human Pose Datasets and Resources<a class="headerlink" href="#imu-based-human-pose-datasets-and-resources" title="Link to this heading"></a></h2>
<p>Public datasets have been crucial for advancing IMU-based pose estimation, both for evaluating optimization methods and for training learning-based models. We highlight a few key datasets and resources, along with their URLs and characteristics:</p>
<section id="dip-imu-dataset-2018">
<h3>DIP-IMU Dataset (2018)<a class="headerlink" href="#dip-imu-dataset-2018" title="Link to this heading"></a></h3>
<p>Introduced by Huang et al. alongside Deep Inertial Poser, DIP-IMU is one of the first large-scale datasets for sparse IMU human motion capture. It contains 10 subjects (9 male, 1 female) each performing a variety of motions in 5 categories (walking, running, sports, etc.), recorded with a full Xsens suit of 17 IMUs at 60 Hz. In total it has 64 sequences comprising about 330,000 time instants of data. Each frame has the 3D orientation (quaternion) of each IMU (from Xsens’s on-board EKF) and the raw accelerometer readings. Ground-truth poses (3D joint angles) were obtained via a high-end optical motion capture system simultaneously recorded, and then post-processed to fit the SMPL model. DIP-IMU is publicly available for research – the project page provides a download link after registration. This dataset has become a standard benchmark: SIP’s optimization was tested on it, and DIP used it for validation. It enables researchers to test their algorithms on common motions and compare error metrics (e.g., mean joint error in degrees or centimeters). The DIP-IMU dataset’s size (over 300k frames) also made it suitable for training deep networks, and indeed DIP and subsequent methods (TransPose, etc.) train on a mix of DIP-IMU and synthetic data. (URL: <a class="reference external" href="http://dip.is.tue.mpg.de">http://dip.is.tue.mpg.de</a>)</p>
</section>
<section id="totalcapture-2017">
<h3>TotalCapture (2017)<a class="headerlink" href="#totalcapture-2017" title="Link to this heading"></a></h3>
<p>The TotalCapture dataset, released by Trumble et al., is a multimodal motion capture dataset that includes synchronized video, IMU, and Vicon marker data for human motions. It features 5 subjects (4 male, 1 female) each performing four distinct sets of actions (ROM exercises, walking, acting, and freestyle) with each sequence repeated 3 times. The dataset provides data at 60 Hz from 8 calibrated cameras and 120 Hz data from a set of 13 IMUs (the subjects likely wore a Motion Analysis or Xsens set covering most body segments). With nearly 1.9 million frames of synchronized data, TotalCapture was the first dataset to offer IMU data aligned with ground-truth 3D poses (from a Vicon optical system) on such a large scale. Researchers have used TotalCapture to evaluate IMU-only pose estimation as well as fusion of video and IMUs. For instance, the SIP paper compared their 6-IMU method against a baseline on TotalCapture sequences. The DIP authors also utilized TotalCapture by fitting SMPL to the Vicon data to create reference poses for testing their network. The dataset can be obtained by request from the University of Surrey’s website (it requires a signup due to its size and to agree to usage terms). TotalCapture provides diverse indoor motions and challenging freeform activities (like acting out scenarios) which test an algorithm’s generalization. It is particularly useful for methods that combine visual and inertial data, but also for pure-inertial methods that can use the IMU streams and the “ground truth” SMPL pose fits provided by DIP authors for quantitative evaluation. (URL: <a class="reference external" href="https://cvssp.org/data/totalcapture/">https://cvssp.org/data/totalcapture/</a>)</p>
</section>
<section id="amass-2019">
<h3>AMASS (2019)<a class="headerlink" href="#amass-2019" title="Link to this heading"></a></h3>
<p>The Archive of Motion Capture as Surface Shapes (AMASS) is a large collection of mocap datasets unified into a common format of human model parameters. AMASS gathers 15 different mocap datasets (recorded with marker-based systems) and fits the SMPL (and SMPL+H for hands) model to all of them using the MoSh++ algorithm. The result is a dataset of 11,000+ motions, over 40 hours of data from more than 300 subjects, all represented consistently as sequences of SMPL pose and shape parameters. AMASS does not contain IMU data per se, but it has been hugely beneficial for IMU research because one can simulate IMU measurements from the AMASS motions. For example, given a sequence of SMPL poses from AMASS, one can compute the orientations and accelerations of virtual IMUs placed on the SMPL body (this is exactly what DIP and others do to generate training data). The TransPose repository’s preprocessing script demonstrates this: it takes AMASS sequences, assumes 6 IMU placement as in SIP, and computes “synthetic” IMU sensor data (orientation and acceleration) for each sequence. This synthetic data can be used to train networks so that they don’t overfit to the specific motions of DIP-IMU or TotalCapture, and it covers a far broader range of movements (since AMASS includes data from CMU MoCap, Human3.6M, gait datasets, etc.). AMASS is accessible for research; users must register on the AMASS website and can then download the parameter files for the various sub-datasets. By using AMASS, one can also compute pose priors – many optimization methods (including SIP) leverage the fact that AMASS provides a distribution of typical human poses and shapes. In summary, while AMASS is not an IMU dataset in itself, it is an invaluable resource to generate data for algorithm development and to serve as a prior on human motion. (URL: <a class="reference external" href="https://amass.is.tue.mpg.de">https://amass.is.tue.mpg.de</a>)</p>
</section>
<section id="other-datasets-and-resources">
<h3>Other Datasets and Resources<a class="headerlink" href="#other-datasets-and-resources" title="Link to this heading"></a></h3>
<p>A number of other datasets and tools are worth mentioning. The TNT15 Dataset referenced in the SIP paper refers to a motion capture dataset from Tübingen (likely containing various motions) that was used as a baseline; it may be available through MPI. More recently, researchers have begun collecting IMU data in outdoor or clinical settings: for example, IMUPoser (CHI 2023) used smartphone IMUs to capture daily activities, and MM-fit dataset provides IMUs for workout motions. The KIT Motion Dataset and MPI Limitations Dataset have also been used for evaluating how methods handle extreme or unusual motions. On the software side, many open-source implementations for IMU pose estimation exist: the original SIP code (in C++) was available as a research prototype, and there are re-implementations like the one by Yan et al. in the Fusion Pose project. The DIP authors released their PyTorch code and pretrained model on the DIP project page, allowing researchers to directly use the DIP network for comparison. The TransPose project (2021) provides code that includes not only a neural network but also an example of using a solver (Ceres) to refine global pose with IMUs, giving a practical example of combining learning with optimization. When developing an optimization-based method, one can use these datasets to test: for instance, start by fitting a single frame’s pose to one frame of DIP-IMU orientation data (which should be similar to solving a PSO problem per frame), then extend to a window of frames to incorporate acceleration.</p>
<p>By leveraging these datasets, the community has established evaluation protocols – common metrics include MPJPE (Mean Per Joint Position Error) in millimeters between the estimated pose and ground-truth pose, as well as orientation errors in degrees for each joint. For example, on DIP-IMU, DIP (the neural method) reports an average joint position error around 60~80mm, whereas SIP (optimization) achieves around 100mm on certain motions, illustrating the trade-off between real-time inference and global accuracy. Datasets like TotalCapture allow comparisons against vision-based methods: e.g., combining IMUs with video can reduce errors compared to video-only, especially for occluded limbs. Overall, the availability of data and code has greatly accelerated research, enabling more advanced techniques like hybrid model-learning methods and domain adaptation for different sensor configurations.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="lecture_06_2SMPL_model_fitting.html" class="btn btn-neutral float-left" title="Lecture 06.2 - Learning-Based Fitting of the SMPL Model to Images" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="lecture_07_2_fitting_SMPL_to_IMU_learning.html" class="btn btn-neutral float-right" title="Lecture 07.2: Fitting SMPL to IMU Data Using Learning-Based Methods" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>