

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>3D Gaussian Splatting: A Basic Introduction &mdash; Fitting SMPL to IMU Optimization</title>
      <link rel="stylesheet" type="text/css" href="/VirtualHumans/_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="/VirtualHumans/_static/css/theme.css?v=e59714d7" />

  
    <link rel="canonical" href="https://treese41528.github.io/VirtualHumans/extendeed_materials_gaussian_splatting.html" />
      <script src="/VirtualHumans/_static/jquery.js?v=5d32c60e"></script>
      <script src="/VirtualHumans/_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="/VirtualHumans/_static/documentation_options.js?v=f2a433a1"></script>
      <script src="/VirtualHumans/_static/doctools.js?v=9bcbadda"></script>
      <script src="/VirtualHumans/_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="/VirtualHumans/_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="References" href="references.html" />
    <link rel="prev" title="Neural Radiance Fields: A Historical and Theoretical Overview" href="extended_materials_neural_radiance_fields.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Virtual Humans Lecture
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="lecture_01_1_historical_body_models.html">Lecture 01.1 – Historical Body Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#early-origins-simplified-primitives-and-kinematic-skeletons-1970s1980s">Early Origins: Simplified Primitives and Kinematic Skeletons (1970s–1980s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#advances-in-the-1990s-superquadrics-differentiable-fitting-and-physical-models">Advances in the 1990s: Superquadrics, Differentiable Fitting, and Physical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#the-impact-of-3d-scanning-and-data-from-anthropometry-to-statistical-models-1990s2000s">The Impact of 3D Scanning and Data: From Anthropometry to Statistical Models (1990s–2000s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#scape-and-the-emergence-of-pose-aware-models-mid-2000s">SCAPE and the Emergence of Pose-Aware Models (Mid-2000s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#consolidation-in-the-2010s-smpl-and-integration-with-learning-based-methods">Consolidation in the 2010s: SMPL and Integration with Learning-Based Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#deep-learning-and-neural-implicit-models-late-2010spresent">Deep Learning and Neural Implicit Models (Late 2010s–Present)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#timeline-summary-of-milestones">Timeline Summary of Milestones</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html">Lecture 01.2 – Introduction to Human Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#historical-context-of-human-body-modeling">1. Historical Context of Human Body Modeling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#early-developments">Early Developments</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#mid-20th-century-approaches">Mid-20th Century Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#data-driven-revolution-1990s-2000s">Data-Driven Revolution (1990s-2000s)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#mathematical-foundations-of-human-body-models">2. Mathematical Foundations of Human Body Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#the-smpl-model">The SMPL Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#pca-based-statistical-shape-modeling">PCA-Based Statistical Shape Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#kinematic-modeling">Kinematic Modeling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#applications-of-human-body-models">3. Applications of Human Body Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computer-animation-and-visual-effects">Computer Animation and Visual Effects</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#virtual-humans-and-avatars">Virtual Humans and Avatars</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#biomechanics-and-ergonomics">Biomechanics and Ergonomics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#human-computer-interaction-hci">Human-Computer Interaction (HCI)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computer-vision-and-ai">Computer Vision and AI</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#education-and-training">Education and Training</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#challenges-and-future-directions">4. Challenges and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computational-efficiency">Computational Efficiency</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#accuracy-and-detail">Accuracy and Detail</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#generalization">Generalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#clothing-and-accessories">Clothing and Accessories</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#emerging-approaches">Emerging Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html">Lecture 01.3 – Introduction to Human Models (Overview)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#historical-context">1. Historical Context</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#early-scientific-studies">Early Scientific Studies</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#mid-20th-century-to-digital-era">Mid-20th Century to Digital Era</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#st-century-advances">21st Century Advances</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#mathematical-foundations">2. Mathematical Foundations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#parametric-body-models">Parametric Body Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-surface-representations">Implicit Surface Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#kinematic-modeling">Kinematic Modeling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#image-formation-and-rendering">3. Image Formation and Rendering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#camera-models">Camera Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#shading-and-visibility">Shading and Visibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#differentiable-rendering">Differentiable Rendering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#surface-representation-methods">4. Surface Representation Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#explicit-mesh-models">Explicit Mesh Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-function-models">Implicit Function Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#motion-capture-and-behavior-synthesis">5. Motion Capture and Behavior Synthesis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#capturing-human-motion">Capturing Human Motion</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#behavior-synthesis">Behavior Synthesis</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#clothing-modeling">6. Clothing Modeling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physically-based-simulation">Physically-Based Simulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#data-driven-approaches">Data-Driven Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-clothing-models">Implicit Clothing Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#human-object-interaction">7. Human-Object Interaction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physics-based-methods">Physics-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#learning-based-approaches">Learning-Based Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#hybrid-systems">Hybrid Systems</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#applications">8. Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#entertainment-and-media">Entertainment and Media</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#healthcare-and-biomechanics">Healthcare and Biomechanics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#engineering-and-design">Engineering and Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#human-computer-interaction">Human-Computer Interaction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#scientific-research">Scientific Research</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#challenges-and-future-directions">9. Challenges and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#scalability-and-generalization">Scalability and Generalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#higher-fidelity-dynamics">Higher-Fidelity Dynamics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#data-and-labeling-constraints">Data and Labeling Constraints</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physics-and-learning-integration">Physics and Learning Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#semantic-and-cognitive-aspects">Semantic and Cognitive Aspects</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#realism-vs-controllability">Realism vs. Controllability</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_02_1_image_formation.html">Lecture 02.1 – Image Formation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#historical-developments-in-image-formation">1. Historical Developments in Image Formation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#ancient-and-medieval-optics-camera-obscura">Ancient and Medieval Optics – Camera Obscura</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#renaissance-perspective-and-geometry">Renaissance Perspective and Geometry</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#early-cameras-and-photographic-imaging">Early Cameras and Photographic Imaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#modern-developments">Modern Developments</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#the-pinhole-camera-model">2. The Pinhole Camera Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#coordinate-setup">Coordinate Setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#proof-by-similar-triangles">Proof by Similar Triangles</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#numerical-example">Numerical Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#inadequacy-of-a-simple-pinhole">Inadequacy of a Simple Pinhole</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#camera-intrinsics-and-the-projection-matrix">3. Camera Intrinsics and the Projection Matrix</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#extrinsic-parameters">Extrinsic Parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#full-projection-example">Full Projection Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#image-distortions-correction">4. Image Distortions &amp; Correction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#properties-of-perspective-projection">5. Properties of Perspective Projection</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#advanced-theoretical-extensions">6. Advanced Theoretical Extensions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#light-field-imaging-and-plenoptic-cameras">Light Field Imaging and Plenoptic Cameras</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#non-conventional-imaging-techniques">Non-Conventional Imaging Techniques</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#applications-in-modern-vision-and-graphics">7. Applications in Modern Vision and Graphics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#computer-vision-and-3d-reconstruction">Computer Vision and 3D Reconstruction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#medical-imaging">Medical Imaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#photorealistic-rendering-in-computer-graphics">Photorealistic Rendering in Computer Graphics</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#python-example-simulating-image-formation">8. Python Example: Simulating Image Formation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html">Lecture 02.2 – Rotations and Kinematic Chains</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#representations-of-3d-rotations">1. Representations of 3D Rotations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#a-rotation-matrices">A) Rotation Matrices</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#b-euler-angles">B) Euler Angles</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#c-quaternions">C) Quaternions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#lie-algebra-so-3-and-exponential-map">2. Lie Algebra <span class="math notranslate nohighlight">\(so(3)\)</span> and Exponential Map</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#rodrigues-rotation-formula">3. Rodrigues’ Rotation Formula</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#kinematic-chains-forward-inverse-kinematics">4. Kinematic Chains: Forward &amp; Inverse Kinematics</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#comparison-of-rotation-representations">Comparison of Rotation Representations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_03_1_surface_representations.html">Lecture 03.1 – Surface Representations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#mathematical-foundations-of-surface-representations">1. Mathematical Foundations of Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-parametric-surfaces">A) Parametric Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-implicit-surfaces">B) Implicit Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-explicit-surfaces">C) Explicit Surfaces</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#surface-differential-properties">2. Surface Differential Properties</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-surface-normals">A) Surface Normals</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-fundamental-forms-and-curvature">B) Fundamental Forms and Curvature</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-geodesics">C) Geodesics</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#discrete-surface-representations">3. Discrete Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-polygon-meshes">A) Polygon Meshes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-point-clouds">B) Point Clouds</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-signed-distance-fields-sdf">C) Signed Distance Fields (SDF)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#advanced-surface-representations">4. Advanced Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-bezier-curves-and-surfaces">A) Bézier Curves and Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-b-splines-and-nurbs">B) B-Splines and NURBS</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-subdivision-surfaces">C) Subdivision Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-level-sets">D) Level Sets</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#e-neural-implicit-representations">E) Neural Implicit Representations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#comparative-analysis-and-applications">5. Comparative Analysis and Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-computational-efficiency-and-storage">A) Computational Efficiency and Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-practical-applications">B) Practical Applications</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-operations-complexity">C) Operations Complexity</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#implementation-examples">6. Implementation Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-basic-mesh-processing-python">A) Basic Mesh Processing (Python)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-implicit-surface-utilities">B) Implicit Surface Utilities</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-bezier-curve-implementation">C) Bézier Curve Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-curvature-estimation-on-meshes">D) Curvature Estimation on Meshes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#advanced-topics-and-future-directions">7. Advanced Topics and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-multi-resolution-representations">A) Multi-Resolution Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-machine-learning-for-geometry">B) Machine Learning for Geometry</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-dynamic-surfaces">C) Dynamic Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-non-manifold-geometries">D) Non-Manifold Geometries</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html">Lecture 03.2 – Procrustes Alignment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#goal-learning-a-model-of-pose-and-shape">Goal: Learning a Model of Pose and Shape</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#the-challenge-of-registration">The Challenge of Registration</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#surface-representation-mesh">Surface Representation: Mesh</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#the-procrustes-alignment-problem-mathematical-formulation">The Procrustes Alignment Problem: Mathematical Formulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#rigid-transformations">Rigid Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#procrustes-alignment-solution">Procrustes Alignment Solution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#decoupling-translation-by-centroid-alignment">Decoupling Translation by Centroid Alignment</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#optimal-rotation-via-svd">Optimal Rotation via SVD</a><ul>
<li class="toctree-l4"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#reflection-adjustment">Reflection Adjustment</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#optimal-scale-optional">Optimal Scale (Optional)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#complete-mathematical-derivation">Complete Mathematical Derivation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#translation-derivation">Translation Derivation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#rotation-derivation">Rotation Derivation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#scale-derivation">Scale Derivation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#summary-of-procrustes-alignment-algorithm">Summary of Procrustes Alignment Algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#python-implementation-example">Python Implementation Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#practical-applications">Practical Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#interactive-visualization-ideas">Interactive Visualization Ideas</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_04_1_icp.html">Lecture 4.1: Iterative Closest Point</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#introduction-to-shape-alignment-and-registration">Introduction to Shape Alignment and Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#the-registration-problem">The Registration Problem</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#review-procrustes-analysis">Review: Procrustes Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#problem-unknown-correspondences">Problem: Unknown Correspondences</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#the-iterative-closest-point-icp-algorithm">The Iterative Closest Point (ICP) Algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#basic-icp-algorithm">Basic ICP Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#computational-considerations">Computational Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="lecture_04_1_icp.html#closest-point-computation">Closest Point Computation</a></li>
<li class="toctree-l4"><a class="reference internal" href="lecture_04_1_icp.html#convergence-and-local-minima">Convergence and Local Minima</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#point-to-point-vs-point-to-plane-icp">Point-to-Point vs. Point-to-Plane ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#point-to-point-icp">Point-to-Point ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#point-to-plane-icp">Point-to-Plane ICP</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#gradient-based-icp-for-non-rigid-registration">Gradient-based ICP for Non-Rigid Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#gradient-based-icp-algorithm">Gradient-based ICP Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#advantages-of-gradient-based-icp">Advantages of Gradient-based ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#computing-gradients">Computing Gradients</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#improving-icp-s-robustness">Improving ICP’s Robustness</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#data-association-direction">Data Association Direction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#robust-cost-functions">Robust Cost Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#trimmed-icp">Trimmed ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#ransac-based-approaches">RANSAC-based Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#additional-information">Additional Information</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#point-to-surface-distance">Point-to-Surface Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#icp-variants-and-extensions">ICP Variants and Extensions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#generalized-icp-gicp">Generalized ICP (GICP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#em-icp-and-probabilistic-approaches">EM-ICP and Probabilistic Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#coherent-point-drift-cpd">Coherent Point Drift (CPD)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#multi-scale-approaches">Multi-Scale Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#applications-of-icp">Applications of ICP</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#implementing-icp">Implementing ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#efficient-python-implementation">Efficient Python Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#practical-tips">Practical Tips</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_04_2_body_models.html">Lecture 04.2 - Body Models: Vertex-Based Models and SMPL</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#body-models-as-parameterized-functions">1. Body Models as Parameterized Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#rotations-articulation-and-pose-representation">2. Rotations, Articulation, and Pose Representation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#rotation-representation">2.1 Rotation Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#kinematic-chain">2.2 Kinematic Chain</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#linear-blend-skinning-and-its-limitations">3. Linear Blend Skinning and its Limitations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#linear-blend-skinning">3.1 Linear Blend Skinning</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#problems-with-standard-lbs">3.2 Problems with Standard LBS</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#blend-shapes-for-correcting-lbs-artifacts">3.3 Blend Shapes for Correcting LBS Artifacts</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#the-smpl-body-model">4. The SMPL Body Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#smpl-philosophy">4.1 SMPL Philosophy</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#smpl-model-architecture">4.2 SMPL Model Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#shape-blend-shapes">4.2.1 Shape Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#pose-blend-shapes">4.2.2 Pose Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#joint-regression">4.2.3 Joint Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#model-training">4.3 Model Training</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#comparison-with-scape">5. Comparison with SCAPE</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#the-scape-model">5.1 The SCAPE Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#different-approaches-to-deformation">5.2 Different Approaches to Deformation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#performance-comparison">5.3 Performance Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#other-advantages">5.4 Other Advantages</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#alignment-techniques-procrustes-analysis-and-icp">6. Alignment Techniques: Procrustes Analysis and ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#procrustes-analysis">6.1 Procrustes Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#iterative-closest-point-icp">6.2 Iterative Closest Point (ICP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#fitting-smpl-to-scans">6.3 Fitting SMPL to Scans</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#image-formation-and-the-pinhole-camera-model">7. Image Formation and the Pinhole Camera Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#the-pinhole-camera-model">7.1 The Pinhole Camera Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#lens-distortion">7.2 Lens Distortion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#extensions-and-advanced-applications">8. Extensions and Advanced Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#dynamic-soft-tissue-modeling">8.1 Dynamic Soft Tissue Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#specialized-extensions">8.2 Specialized Extensions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#deep-learning-for-model-fitting">8.3 Deep Learning for Model Fitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#probabilistic-approaches">8.4 Probabilistic Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#hybrid-models">8.5 Hybrid Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_05_1_body_model_training.html">Lecture 5.1 - Training a Body Model and Fitting SMPL to Scans</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#body-models-based-on-triangle-deformations">Body Models Based on Triangle Deformations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#scape-and-blendscape-models">SCAPE and BlendSCAPE Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#triangle-deformation-process">Triangle Deformation Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#comparison-smpl-vs-scape-blendscape">Comparison: SMPL vs. SCAPE/BlendSCAPE</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#training-a-body-model-from-registrations">Training a Body Model from Registrations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#the-challenge-of-raw-scan-data">The Challenge of Raw Scan Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#training-from-registrations">Training from Registrations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#obtaining-registrations-fitting-smpl-to-scans">Obtaining Registrations: Fitting SMPL to Scans</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#non-rigid-registration-process">Non-Rigid Registration Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#iterative-closest-point-icp-review">Iterative Closest Point (ICP) Review</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#registration-objective-formulation">Registration Objective Formulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#point-to-surface-distance">Point-to-Surface Distance</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#multi-stage-optimization-strategy">Multi-Stage Optimization Strategy</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#joint-registration-and-model-training">Joint Registration and Model Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#co-registration-approach">Co-Registration Approach</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_05_2_3d_registration.html">Lecture 05.2 - 3D Registration: From Classical ICP to Modern Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#rigid-registration-and-the-icp-algorithm">1. Rigid Registration and the ICP Algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#the-iterative-closest-point-icp-algorithm">The Iterative Closest Point (ICP) Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#convergence-analysis-and-failure-modes">Convergence Analysis and Failure Modes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#classical-non-rigid-registration">2. Classical Non-Rigid Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#thin-plate-spline-robust-point-matching-tps-rpm">Thin Plate Spline Robust Point Matching (TPS-RPM)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#coherent-point-drift-cpd">Coherent Point Drift (CPD)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#other-non-rigid-methods">Other Non-Rigid Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#parametric-models-and-the-smpl-body-model">3. Parametric Models and the SMPL Body Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#model-structure">Model Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#shape-blend-shapes-identity-variation">Shape Blend Shapes (Identity Variation)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#pose-blend-shapes-pose-dependent-deformation">Pose Blend Shapes (Pose-Dependent Deformation)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#linear-blend-skinning-lbs-for-articulation">Linear Blend Skinning (LBS) for Articulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#learning-smpl">Learning SMPL</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#using-smpl-for-registration">Using SMPL for Registration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#modeling-clothing-and-fine-details-smpl-d">4. Modeling Clothing and Fine Details: SMPL+D</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#why-smpl-d">Why SMPL+D?</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#how-displacements-are-applied">How Displacements Are Applied</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#limitations">Limitations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#applications">Applications</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#survey-of-3d-registration-methods-from-icp-to-deep-learning">5. Survey of 3D Registration Methods: From ICP to Deep Learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#early-pioneering-works-1990s-foundational-rigid-registration">5.1 Early Pioneering Works (1990s) – Foundational Rigid Registration</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#the-2000s-robust-and-non-rigid-registration-emerges">5.2 The 2000s – Robust and Non-Rigid Registration Emerges</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#s-template-based-and-parametric-model-registration">5.3 2010s – Template-based and Parametric Model Registration</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#s-learning-based-parametric-registration-and-hybrid-approaches">5.4 2020s – Learning-Based Parametric Registration and Hybrid Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html">Lecture 06.1 - Fitting the SMPL Model to Images via Optimization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#mathematical-background-pinhole-camera-and-projections">Mathematical Background: Pinhole Camera and Projections</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#perspective-projection">Perspective Projection</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#weak-perspective-projection">Weak-Perspective Projection</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#camera-extrinsics-vs-model-pose">Camera Extrinsics vs. Model Pose</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#d-keypoints-and-projection">2D Keypoints and Projection</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#the-smpl-model-as-a-differentiable-function-of-shape-and-pose">The SMPL Model as a Differentiable Function of Shape and Pose</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#shape-blend-shapes">Shape Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#pose-blend-shapes">Pose Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#joint-positions">Joint Positions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#linear-blend-skinning">Linear Blend Skinning</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#differentiability-of-smpl">Differentiability of SMPL</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#fitting-smpl-to-images-via-optimization-smplify">Fitting SMPL to Images via Optimization (SMPLify)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#objective-function">Objective Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#combined-objective">Combined Objective</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#optimization-strategy">Optimization Strategy</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#optimization-algorithms">Optimization Algorithms</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#automatic-differentiation-and-jacobians">Automatic Differentiation and Jacobians</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#result-of-smplify">Result of SMPLify</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#historical-progression-and-method-comparisons">Historical Progression and Method Comparisons</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#smplify-bogo-et-al-2016">SMPLify (Bogo et al. 2016)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#smplify-x-2019">SMPLify-X (2019)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html">Lecture 06.2 - Learning-Based Fitting of the SMPL Model to Images</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#foundations-of-learning-based-smpl-estimation">Foundations of Learning-Based SMPL Estimation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#integrating-smpl-into-neural-networks">Integrating SMPL into Neural Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#projection-functions">Projection Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#loss-functions">Loss Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#statistical-priors-and-adversarial-losses">Statistical Priors and Adversarial Losses</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#early-regression-approaches-hmr-and-nbf">Early Regression Approaches: HMR and NBF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#human-mesh-recovery-hmr">Human Mesh Recovery (HMR)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#neural-body-fitting-nbf">Neural Body Fitting (NBF)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#evolving-architectures-hybrid-and-improved-regression-methods">Evolving Architectures: Hybrid and Improved Regression Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#spin-optimization-in-the-training-loop">SPIN: Optimization in the Training Loop</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#pymaf-pyramidal-mesh-alignment-feedback">PyMAF: Pyramidal Mesh Alignment Feedback</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#cliff-using-full-frame-context-for-camera-orientation">CLIFF: Using Full-Frame Context for Camera Orientation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#pixie-whole-body-regression-with-part-experts">PIXIE: Whole-Body Regression with Part Experts</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#temporal-methods-from-single-images-to-video-sequences">Temporal Methods: From Single Images to Video Sequences</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#vibe-adversarial-motion-prior-with-grus">VIBE: Adversarial Motion Prior with GRUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#tcmr-temporally-consistent-mesh-recovery">TCMR: Temporally Consistent Mesh Recovery</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#motionbert-transformer-based-motion-representations">MotionBERT: Transformer-Based Motion Representations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#comparison-of-learning-based-methods">Comparison of Learning-Based Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#supervision-and-data">Supervision and Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#network-architecture">Network Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#objective-functions">Objective Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#pose-and-shape-priors">Pose and Shape Priors</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#performance">Performance</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#runtime">Runtime</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#strengths-and-weaknesses">Strengths and Weaknesses</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html">Lecture 07.1: Fitting SMPL to IMU Data Using Optimization-Based Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#classical-imu-based-pose-estimation-a-historical-perspective">Classical IMU-Based Pose Estimation: A Historical Perspective</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#attitude-and-heading-reference-systems">Attitude and Heading Reference Systems</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#kalman-filter-approaches">Kalman Filter Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#early-sparse-sensor-approaches">Early Sparse-Sensor Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#model-based-optimization-methods">Model-Based Optimization Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#learning-based-methods">Learning-Based Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#inertial-sensor-fundamentals-and-orientation-representations">Inertial Sensor Fundamentals and Orientation Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#gravity-alignment-and-drift-correction">Gravity Alignment and Drift Correction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#orientation-representations">Orientation Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#sensor-calibration">Sensor Calibration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#optimization-based-smpl-fitting-with-imu-data">Optimization-Based SMPL Fitting with IMU Data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#kinematic-model-and-sensor-prediction">Kinematic Model and Sensor Prediction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#regularization-and-prior-terms">Regularization and Prior Terms</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#gradient-and-jacobian-computation">Gradient and Jacobian Computation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#pseudocode-smpl-pose-estimation-from-imu-sequence">Pseudocode: SMPL Pose Estimation from IMU Sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#imu-based-human-pose-datasets-and-resources">IMU-Based Human Pose Datasets and Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#dip-imu-dataset-2018">DIP-IMU Dataset (2018)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#totalcapture-2017">TotalCapture (2017)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#amass-2019">AMASS (2019)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#other-datasets-and-resources">Other Datasets and Resources</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html">Lecture 07.2: Fitting SMPL to IMU Data Using Learning-Based Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#optimization-based-vs-learning-based-approaches">Optimization-Based vs. Learning-Based Approaches</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#learning-based-imu-to-pose-estimation-historical-overview-of-key-models">Learning-Based IMU-to-Pose Estimation: Historical Overview of Key Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#deep-inertial-poser-dip-2018">Deep Inertial Poser (DIP, 2018)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#transpose-2021">TransPose (2021)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#transformer-inertial-poser-tip-2022">Transformer Inertial Poser (TIP, 2022)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#physics-physical-inertial-poser-pip-2022">Physics/Physical Inertial Poser (PIP, 2022)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#other-notable-models-and-developments">Other Notable Models and Developments</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#problem-formulation-and-learning-task-definition">Problem Formulation and Learning Task Definition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#input-and-output-representations">Input and Output Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#learning-objective-and-loss-functions">Learning Objective and Loss Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#temporal-modeling-approaches">Temporal Modeling Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#supervised-vs-semi-supervised-training-synthetic-data">Supervised vs. Semi-Supervised Training; Synthetic Data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#model-architectures-and-design-considerations">Model Architectures and Design Considerations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#encoding-imu-measurements">Encoding IMU Measurements</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#network-structures">Network Structures</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#training-pipeline-and-pseudocode">Training Pipeline and Pseudocode</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#datasets-benchmarks-and-resources">Datasets, Benchmarks, and Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#dip-imu-dataset-2018">DIP-IMU Dataset (2018)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#totalcapture-2017">TotalCapture (2017)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#amass-2019">AMASS (2019)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#other-datasets-and-resources">Other Datasets and Resources</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#challenges-and-outlook">Challenges and Outlook</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html">Lecture 08.1: Vertex-Based Clothing Modeling for Virtual Humans</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#clothing-representation-as-vertex-displacements">Clothing Representation as Vertex Displacements</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#registration-of-clothed-human-scans">Registration of Clothed Human Scans</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#estimating-body-shape-under-clothing-the-buff-method">Estimating Body Shape Under Clothing: The BUFF Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#multi-layer-clothing-capture-the-clothcap-method">Multi-Layer Clothing Capture: The ClothCap Method</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#data-term">Data Term</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#boundary-term">Boundary Term</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#boundary-smoothness">Boundary Smoothness</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#laplacian-smoothness">Laplacian Smoothness</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#applications-of-multi-layer-registration">Applications of Multi-Layer Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#advantages-and-limitations">Advantages and Limitations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html">Lecture 09.1: Neural Implicit and Point-Based Representations for Clothed Human Modeling</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#background-explicit-vs-implicit-vs-point-based-representations">Background: Explicit vs. Implicit vs. Point-Based Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#mesh-based-models">Mesh-Based Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#point-based-models">Point-Based Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#neural-implicit-representations">Neural Implicit Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#neural-radiance-fields-nerfs-for-humans">Neural Radiance Fields (NeRFs) for Humans</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#hybrid-approaches">Hybrid Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#expressiveness-and-topology">Expressiveness and Topology</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#differentiability-and-learning">Differentiability and Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#data-efficiency-and-performance">Data Efficiency and Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#neural-implicit-function-foundations">Neural Implicit Function Foundations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#signed-distance-field-sdf">Signed Distance Field (SDF)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#occupancy-field-indicator-function">Occupancy Field (Indicator Function)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#volume-radiance-field">Volume Radiance Field</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#articulated-deformation-fields-for-implicit-models">Articulated Deformation Fields for Implicit Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#backward-warping-inverse-skinning-field">Backward Warping (Inverse Skinning Field)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#forward-warping-forward-skinning-field">Forward Warping (Forward Skinning Field)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#pose-dependent-deformation-secondary-motion">Pose-Dependent Deformation (Secondary Motion)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#generative-implicit-models-for-clothed-bodies">Generative Implicit Models for Clothed Bodies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#smplicit-topology-aware-clothed-human-model">SMPLicit: Topology-Aware Clothed Human Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#imghum-implicit-generative-human-model">imGHUM: Implicit Generative Human Model</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#pose-dependent-implicit-models-and-animatable-avatars">Pose-Dependent Implicit Models and Animatable Avatars</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#nasa-neural-articulated-shape-approximation-eccv-2020">NASA: Neural Articulated Shape Approximation (ECCV 2020)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#scanimate-weakly-supervised-skinned-avatar-networks-cvpr-2021">SCANimate: Weakly-Supervised Skinned Avatar Networks (CVPR 2021)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#neural-gif-neural-generalized-implicit-functions-iccv-2021">Neural-GIF: Neural Generalized Implicit Functions (ICCV 2021)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#snarf-skinned-neural-articulated-implicit-shapes-iccv-2021">SNARF: Skinned Neural Articulated Implicit Shapes (ICCV 2021)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#pop-the-power-of-points-iccv-2021-point-based-modeling">POP: The Power of Points (ICCV 2021) – Point-Based Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#other-noteworthy-methods">Other Noteworthy Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#blueprint-algorithms-for-key-methods">Blueprint Algorithms for Key Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#snarf-training-procedure-differentiable-forward-skinning-for-implicit-surfaces">SNARF (Training Procedure): Differentiable Forward Skinning for Implicit Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#pop-training-fitting-pipeline-point-based-model-for-pose-dependent-clothing">POP (Training &amp; Fitting Pipeline): Point-Based Model for Pose-Dependent Clothing</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#smplicit-inference-fitting-workflow-generative-implicit-garment-model-conditioned-on-smpl">SMPLicit (Inference/Fitting Workflow): Generative Implicit Garment Model conditioned on SMPL</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#historical-perspective-and-future-outlook">Historical Perspective and Future Outlook</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#challenges-and-future-directions">Challenges and Future Directions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="extended_materials_neural_radiance_fields.html">Neural Radiance Fields: A Historical and Theoretical Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#foundations-3d-scene-representation-and-reconstruction-techniques">Foundations: 3D Scene Representation and Reconstruction Techniques</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#voxel-grids">Voxel Grids</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#point-clouds">Point Clouds</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#mesh-based-surfaces">Mesh-Based Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#light-fields-and-volumetric-rendering">Light Fields and Volumetric Rendering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#emergence-of-neural-radiance-fields-nerf">Emergence of Neural Radiance Fields (NeRF)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#core-idea">Core Idea</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#training-procedure">Training Procedure</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#nerf-architecture">NeRF Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#hierarchical-sampling">Hierarchical Sampling</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#original-results">Original Results</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#theoretical-and-mathematical-analysis-of-nerf">Theoretical and Mathematical Analysis of NeRF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#volume-rendering-formulation-in-nerf">Volume Rendering Formulation in NeRF</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#positional-encoding-and-neural-network-architecture">Positional Encoding and Neural Network Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#loss-function-and-optimization">Loss Function and Optimization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#major-advancements-and-extensions-of-nerf">Major Advancements and Extensions of NeRF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#anti-aliasing-and-unbounded-scenes-mip-nerf-and-nerf">Anti-Aliasing and Unbounded Scenes: mip-NeRF and NeRF++</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#efficiency-improvements-instant-nerf-and-plenoctrees">Efficiency Improvements: Instant NeRF and PlenOctrees</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#dynamic-and-deformable-nerfs-d-nerf-nerfies-nsff-etc">Dynamic and Deformable NeRFs (D-NeRF, Nerfies, NSFF, etc.)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#neural-radiance-fields-for-human-modeling-with-smpl-and-body-models">Neural Radiance Fields for Human Modeling (with SMPL and Body Models)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#other-notable-extensions">Other Notable Extensions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#comparison-with-other-3d-representations">Comparison with Other 3D Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-polygonal-meshes">Vs. Polygonal Meshes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-point-clouds-3d-splatting">Vs. Point Clouds / 3D Splatting</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-voxel-grids-and-volumetric-methods">Vs. Voxel Grids and Volumetric Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-multi-plane-images-mpis-light-fields">Vs. Multi-Plane Images (MPIs) / Light Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#accuracy-and-fidelity">Accuracy and Fidelity</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#applicability">Applicability</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#datasets-for-nerf-training-and-evaluation">Datasets for NeRF Training and Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#blender-synthetic-nerf-dataset">Blender Synthetic NeRF Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#local-light-field-fusion-llff-real-forward-facing-dataset">Local Light Field Fusion (LLFF) Real Forward-Facing Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#tanks-and-temples">Tanks and Temples</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#dtu-dataset">DTU Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#human3-6m">Human3.6M</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#zju-mocap-dataset">ZJU-MoCap Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#people-snapshot">People-Snapshot</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#synthetic-dynamic-scenes">Synthetic dynamic scenes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#other-datasets">Other datasets</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">3D Gaussian Splatting: A Basic Introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#foundations">Foundations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#what-is-a-3d-scene">What is a 3D Scene?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#d-scene-representation">3D Scene Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#computer-graphics-fundamentals">Computer Graphics Fundamentals</a></li>
<li class="toctree-l3"><a class="reference internal" href="#rasterization-and-ray-tracing">Rasterization and Ray Tracing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#alpha-blending-and-compositing">Alpha Blending and Compositing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#the-evolution-of-novel-view-synthesis">The Evolution of Novel View Synthesis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#image-based-rendering">Image-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="#structure-from-motion-and-multi-view-stereo">Structure-from-Motion and Multi-View Stereo</a></li>
<li class="toctree-l3"><a class="reference internal" href="#point-based-rendering">Point-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="#neural-rendering">Neural Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="#accelerated-neural-fields">Accelerated Neural Fields</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#d-gaussian-splatting-a-convergence-of-approaches">3D Gaussian Splatting: A Convergence of Approaches</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">Point-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="#point-clouds-and-their-challenges">Point Clouds and Their Challenges</a></li>
<li class="toctree-l3"><a class="reference internal" href="#the-concept-of-splatting">The Concept of Splatting</a></li>
<li class="toctree-l3"><a class="reference internal" href="#elliptical-weighted-average-ewa-filtering">Elliptical Weighted Average (EWA) Filtering</a></li>
<li class="toctree-l3"><a class="reference internal" href="#differentiable-point-based-rendering">Differentiable Point-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="#d-gaussian-splatting-core-principles">3D Gaussian Splatting: Core Principles</a></li>
<li class="toctree-l3"><a class="reference internal" href="#key-insight-unifying-points-and-volumes">Key Insight: Unifying Points and Volumes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#d-gaussians-as-scene-primitives">3D Gaussians as Scene Primitives</a></li>
<li class="toctree-l3"><a class="reference internal" href="#the-volumetric-rendering-equation">The Volumetric Rendering Equation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#alpha-compositing-with-gaussians">Alpha Compositing with Gaussians</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#mathematical-formulation-of-3d-gaussian-splatting">Mathematical Formulation of 3D Gaussian Splatting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#projecting-3d-gaussians-to-2d">Projecting 3D Gaussians to 2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="#parameterization-of-3d-gaussians">Parameterization of 3D Gaussians</a></li>
<li class="toctree-l3"><a class="reference internal" href="#view-dependent-appearance">View-Dependent Appearance</a></li>
<li class="toctree-l3"><a class="reference internal" href="#differentiable-rendering-equations">Differentiable Rendering Equations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#training-and-optimization">Training and Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#photometric-loss">Photometric Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#initial-point-cloud">Initial Point Cloud</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#optimization-process">Optimization Process</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#differentiable-splatting-pipeline">Differentiable Splatting Pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptive-density-control">Adaptive Density Control</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#implementation-and-real-time-rendering">Implementation and Real-Time Rendering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#tile-based-rendering">Tile-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fast-sorting-strategies">Fast Sorting Strategies</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gpu-accelerated-rasterization">GPU-Accelerated Rasterization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#memory-considerations">Memory Considerations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#comparison-with-other-methods">Comparison with Other Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="#nerf-vs-3d-gaussian-splatting">NeRF vs. 3D Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="#voxel-based-representations-vs-gaussians">Voxel-Based Representations vs. Gaussians</a></li>
<li class="toctree-l2"><a class="reference internal" href="#traditional-point-based-rendering-vs-gaussian-splatting">Traditional Point-Based Rendering vs. Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="#applications-and-extensions">Applications and Extensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="#static-scene-reconstruction">Static Scene Reconstruction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dynamic-scene-capture">Dynamic Scene Capture</a></li>
<li class="toctree-l2"><a class="reference internal" href="#avatar-creation-and-animation">Avatar Creation and Animation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#integration-with-neural-rendering">Integration with Neural Rendering</a></li>
<li class="toctree-l2"><a class="reference internal" href="#large-scale-scene-rendering">Large-Scale Scene Rendering</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bezier-gaussian-triangles-bg-triangle-for-sharper-rendering">Bézier Gaussian Triangles (BG-Triangle) for Sharper Rendering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#representation">Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#performance">Performance</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#human-reconstruction-with-gaussian-splatting-and-priors">Human Reconstruction with Gaussian Splatting and Priors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#eg-humannerf-efficient-generalizable-human-nerf">EG-HumanNeRF: Efficient Generalizable Human NeRF</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gps-gaussian-pixel-wise-gaussian-splatting-for-humans">GPS-Gaussian: Pixel-Wise Gaussian Splatting for Humans</a></li>
<li class="toctree-l3"><a class="reference internal" href="#generalizable-human-gaussians-ghg-with-smpl">Generalizable Human Gaussians (GHG) with SMPL</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#dynamic-scene-reconstruction-with-4d-gaussian-splatting">Dynamic Scene Reconstruction with 4D Gaussian Splatting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#d-gaussian-splatting-4dgs">4D Gaussian Splatting (4DGS)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#speed-and-memory-enhancements-4dgs-1k-and-mega">Speed and Memory Enhancements (4DGS-1K and MEGA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#applications-to-mocap-and-4d-human-rendering">Applications to MoCap and 4D Human Rendering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#implementation-details-and-real-time-performance">Implementation Details and Real-Time Performance</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#data-structures">Data Structures</a></li>
<li class="toctree-l3"><a class="reference internal" href="#rasterization-shaders">Rasterization &amp; Shaders</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gpu-memory-and-throughput">GPU Memory and Throughput</a></li>
<li class="toctree-l3"><a class="reference internal" href="#differentiable-rendering-implementation">Differentiable Rendering Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training-vs-inference-compute">Training vs. Inference Compute</a></li>
<li class="toctree-l3"><a class="reference internal" href="#accuracy-vs-speed-trade-offs">Accuracy vs. Speed trade-offs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#benchmarks-and-comparative-evaluation">Benchmarks and Comparative Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#static-scene-comparison">Static Scene Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="#human-novel-view-comparison">Human Novel-View Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dynamic-scene-comparison">Dynamic Scene Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="#comparative-summary-table">Comparative Summary Table</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#datasets-and-resources">Datasets and Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#synthetic-nerf-dataset-blender-scenes">Synthetic NeRF Dataset (Blender Scenes)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#llff-local-light-field-fusion">LLFF (Local Light Field Fusion)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tanks-and-temples">Tanks and Temples</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multi-object-360-co3d-common-objects-in-3d">Multi-Object 360 (CO3D - Common Objects in 3D)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#amass-archive-of-motion-capture-as-surface-shapes">AMASS (Archive of Motion Capture as Surface Shapes)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cape-clothed-auto-person-encoding">CAPE (Clothed Auto Person Encoding)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#thuman-thuman2-0">THuman / THuman2.0</a></li>
<li class="toctree-l3"><a class="reference internal" href="#renderpeople">RenderPeople</a></li>
<li class="toctree-l3"><a class="reference internal" href="#open-source-implementations">Open-Source Implementations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#future-directions">Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#improved-compression-techniques-for-memory-efficiency">Improved Compression Techniques for Memory Efficiency</a></li>
<li class="toctree-l3"><a class="reference internal" href="#handling-dynamic-and-deformable-scenes">Handling Dynamic and Deformable Scenes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#advanced-material-modeling-for-realistic-rendering">Advanced Material Modeling for Realistic Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hybrid-approaches-integrating-neural-fields-and-explicit-representations">Hybrid Approaches Integrating Neural Fields and Explicit Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#scalability-for-large-scale-scenes-city-level-and-beyond">Scalability for Large-Scale Scenes (City-Level and Beyond)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#real-time-applications-in-ar-vr-and-gaming">Real-Time Applications in AR/VR and Gaming</a></li>
<li class="toctree-l3"><a class="reference internal" href="#integration-with-existing-graphics-pipelines">Integration with Existing Graphics Pipelines</a></li>
<li class="toctree-l3"><a class="reference internal" href="#learning-from-limited-data">Learning from Limited Data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#conclusion">Conclusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="#glossary">Glossary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a><ul>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-01-1-historical-body-models">Lecture 01.1 (Historical Body Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-01-2-introduction-to-human-models">Lecture 01.2 (Introduction to Human Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-01-3-introduction-to-human-models-continued">Lecture 01.3 (Introduction to Human Models Continued)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-02-1-image-formation">Lecture 02.1 (Image Formation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-02-2-rotations-kinematic-chains">Lecture 02.2 (Rotations &amp; Kinematic Chains)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-03-1-surface-representations">Lecture 03.1 (Surface Representations)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-03-2-procrustes-alignment">Lecture 03.2 (Procrustes Alignment)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-04-1-iterative-closest-points">Lecture 04.1 (Iterative Closest Points)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-04-2-body-models">Lecture 04.2 (Body Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-05-1-body-model-training">Lecture 05.1 (Body Model Training)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-05-2-3d-registration">Lecture 05.2 (3D Registration)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-06-1-fitting-smpl-to-images">Lecture 06.1 (Fitting SMPL to Images)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-06-1-optimization-based-fitting-of-smpl-to-images">Lecture 06.1 (Optimization-Based Fitting of SMPL to Images)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-06-2-learning-based-fitting-of-smpl-to-images">Lecture 06.2 (Learning-Based Fitting of SMPL to Images)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-07-1-fitting-smpl-to-imu-optimization">Lecture 07.1 (Fitting SMPL to IMU Optimization)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-07-2-fitting-smpl-to-imu-learning">Lecture 07.2 (Fitting SMPL to IMU Learning)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="references.html#classic-and-optimization-based-methods">Classic and Optimization-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="references.html#learning-based-methods">Learning-Based Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="references.html#datasets-and-resources">Datasets and Resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#relevant-software-and-libraries">Relevant Software and Libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-08-1-references-for-vertex-based-clothing-modeling-for-virtual-humans">Lecture 08.1: References for Vertex-Based Clothing Modeling for Virtual Humans</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#body-models-and-shape-estimation">Body Models and Shape Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#alternative-representations">Alternative Representations</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#datasets">Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#software-and-libraries">Software and Libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-09-1-references-for-neural-implicit-and-point-based-representations-for-clothed-human-modeling">Lecture 09.1: References for Neural Implicit and Point-Based Representations for Clothed Human Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#point-based-models">Point-Based Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#id50">Datasets and Resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#related-software-and-libraries">Related Software and Libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#review-papers-and-tutorials">Review Papers and Tutorials</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#neural-radiance-fields-nerf">Neural Radiance Fields (NERF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#gaussian-splatting">Gaussian Splatting</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Virtual Humans Lecture</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">3D Gaussian Splatting: A Basic Introduction</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/extendeed_materials_gaussian_splatting.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="d-gaussian-splatting-a-basic-introduction">
<span id="lecture-gaussian-splatting-overview"></span><h1>3D Gaussian Splatting: A Basic Introduction<a class="headerlink" href="#d-gaussian-splatting-a-basic-introduction" title="Link to this heading"></a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h2>
<p>3D Gaussian Splatting represents a breakthrough in novel view synthesis, offering real-time rendering of photorealistic scenes by combining the high quality of neural radiance fields with the efficiency of traditional rendering techniques. This approach has revolutionized the field by enabling photo-realistic rendering at interactive frame rates while maintaining the ability to represent complex geometry and appearance.
Unlike previous approaches to novel view synthesis that rely heavily on neural networks to implicitly encode scene information, 3D Gaussian Splatting uses an explicit representation with millions of 3D Gaussian primitives that can be efficiently rendered through a specialized pipeline. This representation strikes an optimal balance between quality and performance, making it suitable for applications ranging from virtual reality to digital human creation.
This guide provides a comprehensive overview of 3D Gaussian Splatting, from foundational concepts to practical implementation. We’ll explore the theoretical underpinnings, mathematical formulations, algorithmic details, and real-world applications of this powerful technique.</p>
</section>
<section id="foundations">
<h2>Foundations<a class="headerlink" href="#foundations" title="Link to this heading"></a></h2>
<section id="what-is-a-3d-scene">
<h3>What is a 3D Scene?<a class="headerlink" href="#what-is-a-3d-scene" title="Link to this heading"></a></h3>
<p>A 3D scene consists of objects, lights, and cameras arranged in three-dimensional space:</p>
<ul class="simple">
<li><p><strong>Geometry</strong>: The shapes and positions of objects in the scene</p></li>
<li><p><strong>Material Properties</strong>: How surfaces interact with light (color, reflectance, transparency)</p></li>
<li><p><strong>Lighting</strong>: The illumination of the scene from various light sources</p></li>
<li><p><strong>Camera</strong>: The viewpoint from which the scene is observed</p></li>
</ul>
<p>The goal of 3D scene reconstruction and rendering is to capture or create these elements and visualize them from arbitrary viewpoints.</p>
</section>
<section id="d-scene-representation">
<h3>3D Scene Representation<a class="headerlink" href="#d-scene-representation" title="Link to this heading"></a></h3>
<p>In computer graphics and vision, a 3D scene representation is a way to model the geometry and appearance of real or virtual environments. Common representations include:</p>
<ul class="simple">
<li><p><strong>Polygonal Meshes</strong>: Use vertices and faces to approximate surfaces; efficient for rendering on GPUs but can struggle with complex topology.</p></li>
<li><p><strong>Voxel Grids</strong>: Discretize space into tiny cubes (like 3D pixels) storing color or density, enabling volumetric effects but often requiring high memory for fine detail.</p></li>
<li><p><strong>Point Clouds</strong>: Represent scenes as unconnected points in space with attributes (color, normal, etc.), which are simple and flexible but typically produce sparse renderings with holes.</p></li>
<li><p><strong>Neural Implicit Representations</strong>: Encode scenes in the weights of neural networks or continuous functions, yielding smooth interpolation but requiring inference for rendering.</p></li>
</ul>
<p><strong>3D Gaussian Splatting</strong> falls between point clouds and volumetric fields: it represents the scene as a cloud of 3D Gaussian density distributions (ellipsoids) that are continuous and overlapping, providing a smooth coverage of space without a fixed grid structure.</p>
</section>
<section id="computer-graphics-fundamentals">
<h3>Computer Graphics Fundamentals<a class="headerlink" href="#computer-graphics-fundamentals" title="Link to this heading"></a></h3>
<p>To understand Gaussian Splatting, we need to review some fundamental concepts in computer graphics:</p>
<ol class="arabic simple">
<li><p><strong>3D Representation</strong>: Objects in 3D space can be represented in various ways:
* <strong>Meshes</strong>: Collections of vertices, edges, and faces (typically triangles)
* <strong>Point Clouds</strong>: Sets of points in 3D space
* <strong>Volumetric Representations</strong>: Voxel grids or continuous functions defining density and color</p></li>
<li><p><strong>Camera Models</strong>: A camera defines how 3D points are projected onto a 2D image:
* <strong>Intrinsics</strong>: Focal length, principal point, etc.
* <strong>Extrinsics</strong>: Position and orientation of the camera in 3D space</p></li>
<li><p><strong>Rendering</strong>: The process of generating a 2D image from a 3D scene description</p></li>
</ol>
<p>To appreciate Gaussian splatting, we must also understand coordinate transformations.
In a typical rendering pipeline, a virtual camera with an intrinsic model (focal length, image plane) and
extrinsic pose (position and orientation) captures the scene. Coordinates transform from <strong>world space</strong>
(the scene’s coordinate system) to <strong>camera/view space</strong>, and finally to <strong>screen space</strong> via a projection
(usually perspective projection).</p>
</section>
<section id="rasterization-and-ray-tracing">
<h3>Rasterization and Ray Tracing<a class="headerlink" href="#rasterization-and-ray-tracing" title="Link to this heading"></a></h3>
<p>Traditional rendering methods fall into two main categories:</p>
<dl>
<dt><strong>Rasterization</strong>:</dt><dd><p>The process of converting vector graphics (like triangles) into a raster image (pixels on a screen). This involves:</p>
<ol class="arabic simple">
<li><p>Transforming 3D vertices to screen space</p></li>
<li><p>Determining which pixels are covered by each primitive</p></li>
<li><p>Computing the color for each pixel based on material properties and lighting</p></li>
</ol>
<p>Rasterization is efficient and forms the basis of real-time graphics in games and interactive applications.</p>
</dd>
<dt><strong>Ray Tracing</strong>:</dt><dd><p>A technique that simulates the physical behavior of light by:</p>
<ol class="arabic simple">
<li><p>Casting rays from the camera through each pixel</p></li>
<li><p>Computing intersections with scene geometry</p></li>
<li><p>Recursively following reflection, refraction, and shadow rays</p></li>
</ol>
<p>Ray tracing produces more physically accurate images but has traditionally been more computationally expensive.</p>
</dd>
</dl>
<p><strong>3D Gaussian Splatting</strong> uses a hybrid approach. It treats each 3D Gaussian as a primitive that can be rasterized by
projecting it to an ellipse on the screen, but then it blends these contributions along each ray akin to ray tracing a volume.
In effect, it performs a sort of splatting rasterization where continuous Gaussian blobs are drawn with transparency and
combined in the correct back-to-front order. This approach leverages the speed of rasterization (by drawing many points quickly)
while achieving the quality of ray-traced volume integration (by alpha blending accumulative contributions).</p>
</section>
<section id="alpha-blending-and-compositing">
<h3>Alpha Blending and Compositing<a class="headerlink" href="#alpha-blending-and-compositing" title="Link to this heading"></a></h3>
<p>When rendering transparent or semi-transparent objects, alpha blending is used to combine colors:</p>
<div class="math notranslate nohighlight">
\[C_{final} = \alpha_{src} \cdot C_{src} + (1 - \alpha_{src}) \cdot C_{dst}\]</div>
<p>Where:
- <span class="math notranslate nohighlight">\(C_{src}\)</span> is the source color (new fragment)
- <span class="math notranslate nohighlight">\(C_{dst}\)</span> is the destination color (existing pixel)
- <span class="math notranslate nohighlight">\(\alpha_{src}\)</span> is the opacity of the source (0 = transparent, 1 = opaque)</p>
<p>This “over” operator is fundamental to compositing multiple transparent elements in the correct order. In the context of rendering a set of semi-transparent elements (like Gaussian splats or volume samples), the goal is to compute the pixel color as if light traveled through the elements, picking up color and attenuating as it goes.</p>
<p>The compositing equation for front-to-back rendering is:</p>
<div class="math notranslate nohighlight">
\[C_{\text{out}} = C_{0} + C_{1}(1-\alpha_{0}) + C_{2}(1-\alpha_{0})(1-\alpha_{1}) + \dots\]</div>
<p>Here each layer (0,1,2,…) has a color C and opacity α, and layers are ordered from nearest to farthest from the viewer. The nearest layer contributes its full color C₀ weighted by its opacity α₀. The next layer’s contribution C₁ is reduced by (1-α₀), meaning it only contributes in the portion not occluded by the first layer, and so on.</p>
<p>In a continuous form (for volume rendering), the color seen along a ray can be written as an integral of the attenuated radiance:</p>
<div class="math notranslate nohighlight">
\[I = \int_{0}^{L} T(t)\, \sigma(t)\, c(t)\, dt, \quad \text{with} \quad
T(t) = \exp\!\Big(-\int_{0}^{t} \sigma(s) ds\Big),\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma(t)\)</span> is the density (opacity per unit length) at distance <span class="math notranslate nohighlight">\(t\)</span> along the ray, <span class="math notranslate nohighlight">\(c(t)\)</span> is the
color or radiance at that point, and <span class="math notranslate nohighlight">\(T(t)\)</span> is the accumulated transparency (transmittance) up to that point.
This is the volumetric rendering equation used in radiance field methods like NeRF.
Gaussian splatting adheres to the same image formation model as NeRF, meaning it blends contributions using an
equivalent of this volume rendering integral but in a discretized manner by summing many Gaussian “particles” along the ray.</p>
</section>
</section>
<section id="the-evolution-of-novel-view-synthesis">
<h2>The Evolution of Novel View Synthesis<a class="headerlink" href="#the-evolution-of-novel-view-synthesis" title="Link to this heading"></a></h2>
<p>Novel view synthesis is the task of generating new views of a scene from limited input images. This field has evolved dramatically over the years.</p>
<section id="image-based-rendering">
<h3>Image-Based Rendering<a class="headerlink" href="#image-based-rendering" title="Link to this heading"></a></h3>
<p>Early approaches to novel view synthesis focused on interpolating between captured images:</p>
<ul class="simple">
<li><p><strong>Light Fields</strong> (Levoy &amp; Hanrahan, 1996): Represented a scene as a 4D function of light rays</p></li>
<li><p><strong>Lumigraphs</strong> (Gortler et al., 1996): Combined sparse geometric information with dense image sampling</p></li>
<li><p><strong>Unstructured Lumigraph</strong> (Buehler et al., 2001): An image-based rendering method that handles sparse view input by leveraging a mesh proxy</p></li>
</ul>
<p>These methods required dense sampling of viewpoints, limiting their practical applications. They stored many input views and interpolated between them.</p>
</section>
<section id="structure-from-motion-and-multi-view-stereo">
<h3>Structure-from-Motion and Multi-View Stereo<a class="headerlink" href="#structure-from-motion-and-multi-view-stereo" title="Link to this heading"></a></h3>
<p>To address the limitations of pure image-based methods, researchers developed techniques to recover 3D structure:</p>
<ul class="simple">
<li><p><strong>Structure-from-Motion (SfM)</strong>: Recovers camera poses and a sparse 3D point cloud from multiple images. As capturing geometry became easier, approaches produced point clouds or meshes of the scene, enabling geometry-based view synthesis.</p></li>
<li><p><strong>Multi-View Stereo (MVS)</strong>: Densifies the sparse point cloud to create more complete 3D models</p></li>
</ul>
<p>These classical computer vision approaches provided more geometric understanding but often struggled with complex materials and lighting effects.</p>
</section>
<section id="point-based-rendering">
<h3>Point-Based Rendering<a class="headerlink" href="#point-based-rendering" title="Link to this heading"></a></h3>
<p>Point-based rendering emerged as a simple way to render scenes captured as point clouds (e.g., from MVS or depth sensors) without mesh connectivity. However, naive point rendering shows gaps and aliasing. Research in the 2000s introduced splatting—rendering each point as a disk or Gaussian blob (also called surfel if it has a normal and texture) to cover holes and smooth the result.</p>
<p>Notable developments include:
- <strong>Surfels</strong> (Pfister et al., 2000): Introduced as a rendering primitive for scanned surfaces
- <strong>EWA Surface Splatting</strong> (Zwicker et al., 2001): Developed to properly filter and blend point contributions on screen</p>
<p>These laid the groundwork for representing scenes by clouds of particles instead of connected triangles.</p>
</section>
<section id="neural-rendering">
<h3>Neural Rendering<a class="headerlink" href="#neural-rendering" title="Link to this heading"></a></h3>
<p>The advent of deep learning spurred a new wave of novel view synthesis:</p>
<ul class="simple">
<li><p><strong>Neural Point-Based Graphics (NPBG)</strong> (Aliev et al., 2020): Combined point clouds with learned neural descriptors to improve fidelity</p></li>
<li><p><strong>Neural Radiance Fields (NeRF)</strong> (Mildenhall et al., 2020): Showed that optimizing an MLP to represent volume density and color yields photorealistic novel views</p></li>
</ul>
<p>NeRF marked a paradigm shift by representing a scene as a continuous function modeled by a neural network:</p>
<div class="math notranslate nohighlight">
\[F_\Theta(x, d) \mapsto (c, \sigma)\]</div>
<p>Where:
- <span class="math notranslate nohighlight">\(x \in \mathbb{R}^3\)</span> is a 3D position
- <span class="math notranslate nohighlight">\(d \in \mathbb{R}^2\)</span> is a viewing direction
- <span class="math notranslate nohighlight">\(c \in \mathbb{R}^3\)</span> is the emitted color (RGB)
- <span class="math notranslate nohighlight">\(\sigma \in \mathbb{R}\)</span> is the volume density</p>
<p>NeRF renders images using volumetric integration along camera rays, achieving unprecedented quality for novel view synthesis. However, it suffers from slow rendering times due to the need to evaluate the neural network for many points along each ray.</p>
</section>
<section id="accelerated-neural-fields">
<h3>Accelerated Neural Fields<a class="headerlink" href="#accelerated-neural-fields" title="Link to this heading"></a></h3>
<p>NeRF sparked a revolution: numerous variants improved quality and speed over the next few years:</p>
<ul class="simple">
<li><p><strong>Mip-NeRF</strong> and <strong>Mip-NeRF 360</strong> (Barron et al., 2021, 2022): Addressed aliasing and unbounded scenes</p></li>
<li><p><strong>Plenoxels</strong> (Fridovich-Keil et al., 2022): Removed the neural network, optimizing explicit grids of voxels with densities and colors</p></li>
<li><p><strong>Instant NGP</strong> (Müller et al., 2022): Used a multiresolution hash grid encoding to drastically speed up training and rendering</p></li>
</ul>
<p>These voxel-based radiance fields achieve fast training but still face memory-resolution trade-offs and need interpolation during ray marching.</p>
</section>
</section>
<section id="d-gaussian-splatting-a-convergence-of-approaches">
<h2>3D Gaussian Splatting: A Convergence of Approaches<a class="headerlink" href="#d-gaussian-splatting-a-convergence-of-approaches" title="Link to this heading"></a></h2>
<p><strong>3D Gaussian Splatting</strong> (Kerbl et al., 2023) can be seen as a convergence of these directions. It brings back an explicit point-based representation (like the classic surfel idea) but imbues it with volumetric rendering principles (like NeRF’s alpha blending). Crucially, it optimizes the scene in situ by gradient descent, similar to neural fields, making it a differentiable point-based representation.</p>
<p>The result is a method that can be trained rapidly (minutes) like the fastest radiance field methods, yet renders in real-time with quality comparable to slower neural networks. This bridges the gap between neural and point-based novel view synthesis: millions of tiny Gaussian primitives optimized to reproduce input images, achieving state-of-the-art novel view synthesis results with unprecedented speed.</p>
<section id="id1">
<h3>Point-Based Rendering<a class="headerlink" href="#id1" title="Link to this heading"></a></h3>
</section>
<section id="point-clouds-and-their-challenges">
<h3>Point Clouds and Their Challenges<a class="headerlink" href="#point-clouds-and-their-challenges" title="Link to this heading"></a></h3>
<p>Point clouds provide a simple and flexible representation of 3D geometry:</p>
<ul class="simple">
<li><p>Each point has a position in 3D space and optional attributes (color, normal, etc.)</p></li>
<li><p>No explicit connectivity information (unlike meshes)</p></li>
<li><p>Can be directly acquired from sensors like LiDAR or derived from MVS</p></li>
</ul>
<p>However, rendering raw point clouds presents challenges:
- Gaps between points can result in holes or artifacts
- Point size is not inherently defined
- Handling occlusion requires careful ordering</p>
</section>
<section id="the-concept-of-splatting">
<h3>The Concept of Splatting<a class="headerlink" href="#the-concept-of-splatting" title="Link to this heading"></a></h3>
<p>Splatting addresses these challenges by representing each point as a small surface element or “splat”:</p>
<ol class="arabic simple">
<li><p>Project each 3D point onto the image plane</p></li>
<li><p>Render each point as a small 2D footprint (disk or ellipse)</p></li>
<li><p>Blend these footprints together to form a continuous surface</p></li>
</ol>
<p>The size and shape of each splat can be adjusted based on local density, viewing angle, and surface properties.</p>
</section>
<section id="elliptical-weighted-average-ewa-filtering">
<h3>Elliptical Weighted Average (EWA) Filtering<a class="headerlink" href="#elliptical-weighted-average-ewa-filtering" title="Link to this heading"></a></h3>
<p>EWA filtering, introduced by Zwicker et al. (2001), improves the quality of point-based rendering:</p>
<ol class="arabic simple">
<li><p>Represent each point as a 3D Gaussian ellipsoid</p></li>
<li><p>Project this Gaussian to the image plane, resulting in a 2D Gaussian ellipse</p></li>
<li><p>Apply filtering to prevent aliasing</p></li>
<li><p>Blend the filtered footprints to create the final image</p></li>
</ol>
<p>This approach provides:
- A 3D Gaussian kernel in object space is projected to screen space
- The projected kernel is combined with a low-pass filter to prevent aliasing
- This combined filter function (the EWA filter) is used for each splat’s contribution</p>
</section>
<section id="differentiable-point-based-rendering">
<h3>Differentiable Point-Based Rendering<a class="headerlink" href="#differentiable-point-based-rendering" title="Link to this heading"></a></h3>
<p>Recent advances have made point-based rendering differentiable, enabling optimization of point attributes through backpropagation:</p>
<ul class="simple">
<li><p><strong>Differentiable Surface Splatting</strong> (Wang et al., 2019): Made the splatting pipeline differentiable, allowing gradients to flow back to point positions and attributes</p></li>
<li><p><strong>Neural Point-Based Graphics</strong> (Aliev et al., 2020): Combined traditional point rendering with neural networks</p></li>
<li><p><strong>ADOP</strong> (Rückert et al., 2022): Provided an approximate differentiable one-pixel point rendering approach</p></li>
<li><p><strong>Pulsar</strong> (Lassner &amp; Zollhöfer, 2021): Created an efficient sphere-based neural rendering method</p></li>
</ul>
<p>These methods lay the groundwork for learning-based approaches like 3D Gaussian Splatting by providing differentiable rendering that can compute gradients with respect to point attributes.</p>
</section>
<section id="d-gaussian-splatting-core-principles">
<h3>3D Gaussian Splatting: Core Principles<a class="headerlink" href="#d-gaussian-splatting-core-principles" title="Link to this heading"></a></h3>
<p>3D Gaussian Splatting builds upon both volumetric rendering concepts from NeRF and point-based rendering techniques.</p>
</section>
<section id="key-insight-unifying-points-and-volumes">
<h3>Key Insight: Unifying Points and Volumes<a class="headerlink" href="#key-insight-unifying-points-and-volumes" title="Link to this heading"></a></h3>
<p>The fundamental insight of 3D Gaussian Splatting is that:</p>
<ol class="arabic simple">
<li><p>Both volumetric rendering (as in NeRF) and alpha blending of point splats follow the same mathematical model</p></li>
<li><p>By representing a scene as a collection of 3D Gaussians instead of a neural network, rendering can be performed efficiently</p></li>
<li><p>The Gaussian representation is differentiable, allowing optimization through gradient descent</p></li>
</ol>
<p>This unified view enables high-quality rendering with real-time performance.</p>
</section>
<section id="d-gaussians-as-scene-primitives">
<h3>3D Gaussians as Scene Primitives<a class="headerlink" href="#d-gaussians-as-scene-primitives" title="Link to this heading"></a></h3>
<p>In 3D Gaussian Splatting, each scene element is represented as an anisotropic 3D Gaussian:</p>
<div class="math notranslate nohighlight">
\[G(x) = \exp\left(-\frac{1}{2}(x-\mu)^T \Sigma^{-1} (x-\mu)\right)\]</div>
<p>Where:
- <span class="math notranslate nohighlight">\(\mu \in \mathbb{R}^3\)</span> is the center (mean position)
- <span class="math notranslate nohighlight">\(\Sigma \in \mathbb{R}^{3 \times 3}\)</span> is the covariance matrix, defining the Gaussian’s shape and orientation
- For an isotropic Gaussian, <span class="math notranslate nohighlight">\(\Sigma = \sigma^2 I\)</span> (a sphere)
- For an anisotropic Gaussian, <span class="math notranslate nohighlight">\(\Sigma\)</span> can represent any ellipsoid</p>
<p>Each Gaussian also carries:
- An opacity parameter controlling its contribution to the final image
- Color information, either as a constant RGB value or view-dependent (via spherical harmonics)</p>
<p>A Gaussian in 3D is a function defined by a mean (center position) and a covariance matrix
(which describes its spatial extent in each direction). The covariance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span> is symmetric positive semi-definite.
We can interpret <span class="math notranslate nohighlight">\(\Sigma\)</span> as defining an ellipsoid surface where <span class="math notranslate nohighlight">\((\mathbf{x}-\mathbf{p})^T \Sigma^{-1} (\mathbf{x}-\mathbf{p}) = 1\)</span>. The eigenvalues of <span class="math notranslate nohighlight">\(\Sigma\)</span> give the squared lengths of the ellipsoid’s principal axes, and the eigenvectors give the orientation of those axes.</p>
</section>
<section id="the-volumetric-rendering-equation">
<h3>The Volumetric Rendering Equation<a class="headerlink" href="#the-volumetric-rendering-equation" title="Link to this heading"></a></h3>
<p>The volumetric rendering equation computes the color of a pixel by integrating contributions along a camera ray:</p>
<div class="math notranslate nohighlight">
\[C(r) = \int_{t_{min}}^{t_{max}} T(t) \cdot \sigma(r(t)) \cdot c(r(t), d) \, dt\]</div>
<p>Where:
- <span class="math notranslate nohighlight">\(r(t) = o + t \cdot d\)</span> is the position along the ray
- <span class="math notranslate nohighlight">\(T(t) = \exp\left(-\int_{t_{min}}^{t} \sigma(r(s)) \, ds\right)\)</span> is the transmittance
- <span class="math notranslate nohighlight">\(\sigma(r(t))\)</span> is the density at position <span class="math notranslate nohighlight">\(r(t)\)</span>
- <span class="math notranslate nohighlight">\(c(r(t), d)\)</span> is the color at position <span class="math notranslate nohighlight">\(r(t)\)</span> viewed from direction <span class="math notranslate nohighlight">\(d\)</span></p>
<p>In practice, this integral is approximated as a discrete sum:</p>
<div class="math notranslate nohighlight">
\[C \approx \sum_{i=1}^{N} T_i \cdot (1 - e^{-\sigma_i \delta_i}) \cdot c_i\]</div>
<p>Where:
- <span class="math notranslate nohighlight">\(T_i = \prod_{j=1}^{i-1} (1 - \alpha_j)\)</span> with <span class="math notranslate nohighlight">\(\alpha_j = 1 - e^{-\sigma_j \delta_j}\)</span>
- <span class="math notranslate nohighlight">\(\delta_i\)</span> is the distance between adjacent samples</p>
<p>An important insight is that Gaussian splatting uses the same optical model as NeRF and other radiance fields.
That is, it assumes the color seen by the camera is the integral of emitted radiance times transparency.
No shadowing or global illumination is considered; each Gaussian contributes independently to the rays that intersect it.</p>
</section>
<section id="alpha-compositing-with-gaussians">
<h3>Alpha Compositing with Gaussians<a class="headerlink" href="#alpha-compositing-with-gaussians" title="Link to this heading"></a></h3>
<p>In 3D Gaussian Splatting, the same rendering equation is implemented using alpha compositing of Gaussian splats:</p>
<div class="math notranslate nohighlight">
\[C = \sum_{i=1}^{N} \left(\prod_{j=1}^{i-1}(1-\alpha_j)\right) \alpha_i c_i\]</div>
<p>Where:
- Gaussians are sorted front-to-back
- <span class="math notranslate nohighlight">\(\alpha_i\)</span> is derived from the Gaussian’s opacity and its projected 2D footprint
- <span class="math notranslate nohighlight">\(c_i\)</span> is the Gaussian’s color (potentially view-dependent)
- <span class="math notranslate nohighlight">\(\prod_{j=1}^{i-1}(1-\alpha_j)\)</span> accounts for occlusion by closer Gaussians</p>
<p>Alpha compositing with many Gaussians follows the same rules as described in the alpha blending section. However, one challenge is that there may be millions of Gaussians, and they are not ordered in any simple way like layers. For proper rendering, we need to:</p>
<ol class="arabic simple">
<li><p>Project all Gaussians to 2D (obtaining their elliptical footprint and an effective per-pixel alpha mask)</p></li>
<li><p>Sort all these “splats” by depth (distance from camera)</p></li>
<li><p>Blend them back-to-front (furthest first) or front-to-back into the image</p></li>
</ol>
<p>For front-to-back compositing (which is numerically stable and easy to implement incrementally):</p>
<div class="math notranslate nohighlight">
\[\begin{split}C_{\text{accum}} \leftarrow C_{\text{accum}} + (1 - \alpha_{\text{accum}})\, \alpha_i\, C_i, \\
\alpha_{\text{accum}} \leftarrow \alpha_{\text{accum}} + (1 - \alpha_{\text{accum}})\, \alpha_i,\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(C_{\text{accum}}\)</span> and <span class="math notranslate nohighlight">\(\alpha_{\text{accum}}\)</span> are the running accumulated color and opacity, and <span class="math notranslate nohighlight">\(C_i,\alpha_i\)</span> are the next splat’s color and opacity.</p>
</section>
</section>
<section id="mathematical-formulation-of-3d-gaussian-splatting">
<h2>Mathematical Formulation of 3D Gaussian Splatting<a class="headerlink" href="#mathematical-formulation-of-3d-gaussian-splatting" title="Link to this heading"></a></h2>
<p>Let’s delve into the mathematical details of 3D Gaussian Splatting.</p>
<section id="projecting-3d-gaussians-to-2d">
<h3>Projecting 3D Gaussians to 2D<a class="headerlink" href="#projecting-3d-gaussians-to-2d" title="Link to this heading"></a></h3>
<p>When rendering, each 3D Gaussian is projected to a 2D elliptical splat on the image plane:</p>
<ol class="arabic">
<li><p>The center <span class="math notranslate nohighlight">\(\mu\)</span> is projected to the image using the camera projection matrix</p></li>
<li><p>The 3D covariance <span class="math notranslate nohighlight">\(\Sigma\)</span> is transformed to a 2D covariance <span class="math notranslate nohighlight">\(\Sigma_{img}\)</span> in image space:</p>
<div class="math notranslate nohighlight">
\[\Sigma_{img} = J \cdot W \cdot \Sigma \cdot W^T \cdot J^T\]</div>
<p>Where:
- <span class="math notranslate nohighlight">\(W\)</span> is the view transformation matrix
- <span class="math notranslate nohighlight">\(J\)</span> is the Jacobian of the projection at the Gaussian’s center</p>
</li>
<li><p>The resulting 2D Gaussian defines the shape and extent of the splat on the image</p></li>
</ol>
<p>Projecting a 3D Gaussian onto the image plane yields a 2D Gaussian (an ellipse) in screen space. However, because the camera uses a perspective projection (not a linear mapping), projecting a Gaussian exactly is non-trivial. A common approach is to approximate the perspective transform locally as an affine transform around the Gaussian’s center. This yields a linear mapping of the covariance.</p>
<p>Let’s derive the projection formula for a 3D Gaussian’s covariance to the screen (image) plane in a simplified scenario. Assume we have a pinhole camera. Let a 3D Gaussian have world covariance <span class="math notranslate nohighlight">\(\Sigma\)</span>. We want the covariance of the image projection.</p>
<p><strong>Step 1: Transform to camera coordinates.</strong> Apply the rigid transformation (rotation <span class="math notranslate nohighlight">\(R_c\)</span> and translation <span class="math notranslate nohighlight">\(t_c\)</span> of the camera pose) to the Gaussian. In linear approximation, the covariance in camera frame (before projection) is <span class="math notranslate nohighlight">\(\Sigma_c = R_c \Sigma R_c^T\)</span> (we ignore translation for covariance). This aligns the Gaussian with the camera’s view axes.</p>
<p><strong>Step 2: Project to image.</strong> For an orthographic camera (no perspective foreshortening), projection is just dropping the z-coordinate, and indeed the covariance on the image would be the top-left 2x2 block of <span class="math notranslate nohighlight">\(\Sigma_c\)</span>. For a perspective camera, we consider an object point at depth <span class="math notranslate nohighlight">\(Z\)</span> projects with scale <span class="math notranslate nohighlight">\(f/Z\)</span> (where <span class="math notranslate nohighlight">\(f\)</span> is focal length). If the Gaussian has a small extent, we linearize around depth <span class="math notranslate nohighlight">\(Z_0\)</span> (the depth of the Gaussian center).</p>
<p><strong>Step 3: Simplify for usage.</strong> The final result used is:</p>
<div class="math notranslate nohighlight">
\[\Sigma' = J W\, \Sigma\, W^T J^T,\]</div>
<p>with <span class="math notranslate nohighlight">\(\Sigma'\)</span> being a <span class="math notranslate nohighlight">\(3\times3\)</span> matrix in camera coords, and then using the upper-left <span class="math notranslate nohighlight">\(2\times2\)</span> block as the image covariance.</p>
<p>The key take-away is that an anisotropic 3D Gaussian remains anisotropic in the image – usually an ellipse. The formula provides a way to compute that ellipse’s shape from the 3D parameters. We can thus rasterize each Gaussian as an oriented ellipse with appropriate size.</p>
</section>
<section id="parameterization-of-3d-gaussians">
<h3>Parameterization of 3D Gaussians<a class="headerlink" href="#parameterization-of-3d-gaussians" title="Link to this heading"></a></h3>
<p>To ensure that the Gaussian representation remains valid during optimization, the covariance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span> is parameterized as:</p>
<div class="math notranslate nohighlight">
\[\Sigma = R \cdot S \cdot S^T \cdot R^T\]</div>
<p>Where:
- <span class="math notranslate nohighlight">\(R\)</span> is a rotation matrix (often represented as a quaternion)
- <span class="math notranslate nohighlight">\(S\)</span> is a diagonal scaling matrix with entries <span class="math notranslate nohighlight">\(s_x, s_y, s_z\)</span></p>
<p>This decomposition guarantees that <span class="math notranslate nohighlight">\(\Sigma\)</span> remains positive semidefinite throughout optimization.</p>
<p>Directly optimizing the <span class="math notranslate nohighlight">\(3\times3\)</span> covariance matrices <span class="math notranslate nohighlight">\(\Sigma\)</span> for potentially millions of Gaussians is tricky, because we must ensure each <span class="math notranslate nohighlight">\(\Sigma\)</span> stays valid (symmetric positive semi-definite). Gradient descent could easily nudge some matrix to become non-PSD, causing the Gaussian to become invalid (e.g., negative variance in some direction).</p>
<p>This parameterization ensures any combination of parameters yields a valid covariance (since <span class="math notranslate nohighlight">\(R\)</span> is orthonormal and <span class="math notranslate nohighlight">\(S S^T\)</span> is positive semidef.). During optimization, the quaternion representing <span class="math notranslate nohighlight">\(R\)</span> is normalized after each gradient update to remain a valid rotation.</p>
</section>
<section id="view-dependent-appearance">
<h3>View-Dependent Appearance<a class="headerlink" href="#view-dependent-appearance" title="Link to this heading"></a></h3>
<p>To model view-dependent effects like specular highlights, each Gaussian can use spherical harmonics to encode color as a function of viewing direction:</p>
<div class="math notranslate nohighlight">
\[c(d) = \sum_{l=0}^{L} \sum_{m=-l}^{l} c_{lm} Y_{lm}(d)\]</div>
<p>Where:
- <span class="math notranslate nohighlight">\(Y_{lm}(d)\)</span> are the spherical harmonic basis functions
- <span class="math notranslate nohighlight">\(c_{lm}\)</span> are the coefficients (typically using up to 2nd order, resulting in 9 coefficients per color channel)</p>
<p>This allows each Gaussian to exhibit different colors when viewed from different angles, similar to NeRF’s view-dependent modeling.</p>
</section>
<section id="differentiable-rendering-equations">
<h3>Differentiable Rendering Equations<a class="headerlink" href="#differentiable-rendering-equations" title="Link to this heading"></a></h3>
<p>A major strength of 3D Gaussian Splatting is that the rendering process is made differentiable so that gradient-based optimization can be used to adjust the Gaussians to fit input images. Differentiating the rendering with respect to Gaussian parameters means computing partial derivatives of the final pixel colors with respect to each Gaussian’s position, covariance, color, and opacity.</p>
<p>The rendering function can be denoted as <span class="math notranslate nohighlight">\(I = R(\{p_i, \Sigma_i, c_i, \alpha_i\})\)</span> producing an image <span class="math notranslate nohighlight">\(I\)</span>. We have a loss <span class="math notranslate nohighlight">\(L = \sum_{\text{pixels}} \|I - I_{\text{target}}\|^2\)</span> for example. The gradient <span class="math notranslate nohighlight">\(\frac{\partial L}{\partial p_i}\)</span> indicates how moving Gaussian <span class="math notranslate nohighlight">\(i\)</span> in space would affect the loss, and similarly for <span class="math notranslate nohighlight">\(\Sigma_i\)</span>, etc.</p>
<p>In simpler terms, because each Gaussian’s influence on the image is smooth and continuous (thanks to the Gaussian function), the rendering is differentiable w.rt. the Gaussian parameters. The gradients tell us how to adjust each Gaussian to reduce the reconstruction error.</p>
<p>The differentiable rendering equations for Gaussian splats ensure that one can start from an initial guess (say a sparse set of Gaussians) and converge to an accurate model of the scene by iterative optimization (gradient descent).</p>
</section>
</section>
<section id="training-and-optimization">
<h2>Training and Optimization<a class="headerlink" href="#training-and-optimization" title="Link to this heading"></a></h2>
<p>The 3D Gaussian Splatting pipeline is trained end-to-end by optimizing the parameters of all Gaussians to match a set of input photographs.</p>
<section id="photometric-loss">
<h3>Photometric Loss<a class="headerlink" href="#photometric-loss" title="Link to this heading"></a></h3>
<p>The training uses a photometric loss that compares rendered images to ground truth photos:</p>
<div class="math notranslate nohighlight">
\[L(\theta) = (1 - \lambda) \cdot L_1 + \lambda \cdot L_{D-SSIM}\]</div>
<p>Where:
- <span class="math notranslate nohighlight">\(L_1 = \|I_{render} - I_{gt}\|_1\)</span> (L1 distance between rendered and ground truth images)
- <span class="math notranslate nohighlight">\(L_{D-SSIM} = 1 - SSIM(I_{render}, I_{gt})\)</span> (structural similarity term)
- <span class="math notranslate nohighlight">\(\lambda\)</span> is a weighting factor (typically 0.2)
- <span class="math notranslate nohighlight">\(\theta\)</span> represents all Gaussian parameters</p>
</section>
<section id="initial-point-cloud">
<h3>Initial Point Cloud<a class="headerlink" href="#initial-point-cloud" title="Link to this heading"></a></h3>
<p>The training typically starts from a sparse point cloud generated by Structure-from-Motion (SfM):</p>
<ol class="arabic simple">
<li><p>Each point in the SfM reconstruction becomes an initial 3D Gaussian</p></li>
<li><p>Through optimization, these Gaussians are refined, and new ones are added as needed</p></li>
<li><p>The final model might contain millions of optimized Gaussians</p></li>
</ol>
<p>This approach enables reconstruction from a minimal set of images without requiring dense MVS reconstruction or additional inputs.</p>
</section>
</section>
<section id="optimization-process">
<h2>Optimization Process<a class="headerlink" href="#optimization-process" title="Link to this heading"></a></h2>
<p>The optimization process for 3D Gaussian Splatting involves adjusting millions of parameters (positions <span class="math notranslate nohighlight">\(p_i\)</span>, opacities <span class="math notranslate nohighlight">\(\alpha_i\)</span>, covariance <span class="math notranslate nohighlight">\(\Sigma_i\)</span>, and colors <span class="math notranslate nohighlight">\(c_i\)</span> of each Gaussian) so that rendered images match the input photographs. This is done by minimizing a reconstruction loss between the rendered image and ground truth, for each training view, summed over all views.</p>
<p><strong>Iterative Optimization:</strong> The method uses gradient descent (or a variant like Adam optimizer) to update the Gaussian parameters. Because each Gaussian’s effect on each pixel is differentiable, they can accumulate gradients by rendering the current estimate from each input view, comparing to the actual image, and backpropagating.</p>
<p>At each gradient step, for each Gaussian, they compute how the image error changes if that Gaussian’s parameters change – this is done efficiently using derived analytic gradients. For example, the gradient w.rt. position will consider the image gradient (difference) at the splat’s location, essentially pushing the Gaussian toward where the image says it should be.</p>
<p>The optimization proceeds with iterations of: render → compute loss → compute gradients → update Gaussians. This continues until the error converges. During this process, they also perform adaptive density control, which adds or removes Gaussians to improve the fit.</p>
<section id="differentiable-splatting-pipeline">
<h3>Differentiable Splatting Pipeline<a class="headerlink" href="#differentiable-splatting-pipeline" title="Link to this heading"></a></h3>
<p>The entire rendering pipeline is differentiable, allowing gradients to be computed with respect to all Gaussian parameters:</p>
<ol class="arabic simple">
<li><p>Position gradients: How changing a Gaussian’s position affects the image</p></li>
<li><p>Covariance gradients: How changing a Gaussian’s shape affects the image</p></li>
<li><p>Opacity gradients: How changing a Gaussian’s opacity affects the image</p></li>
<li><p>Color gradients: How changing a Gaussian’s color affects the image</p></li>
</ol>
<p>These gradients are used to update the parameters via gradient descent (typically Adam optimizer).</p>
</section>
<section id="adaptive-density-control">
<h3>Adaptive Density Control<a class="headerlink" href="#adaptive-density-control" title="Link to this heading"></a></h3>
<p>One important note: unlike some other methods that needed special handling for far-away geometry
(Mip-NeRF warps space, or Plenoxels require an octree to allocate distant voxels), Gaussian Splatting’s Gaussians
remain in Euclidean space with no additional warp. The adaptive control naturally covers the space with as many
Gaussians as needed. Distant parts of the scene might simply get more Gaussians if needed, rather than compressing them.</p>
<ol class="arabic simple">
<li><p><strong>Densification</strong>: Adding Gaussians where needed
- <strong>Splitting</strong>: Large Gaussians with high error are split into smaller ones
- <strong>Cloning</strong>: Small Gaussians with high error are cloned (duplicated and slightly offset)</p></li>
<li><p><strong>Pruning</strong>: Removing Gaussians with negligible contribution (very low opacity)</p></li>
</ol>
<p>This procedure allows the representation to adapt to the scene’s complexity, allocating more Gaussians to detailed regions and fewer to simple areas.</p>
<p>The overall optimization algorithm alternates between:
1. <strong>Parameter Update</strong>: Standard gradient descent to update existing Gaussian parameters
2. <strong>Density Control</strong>: Periodically adjusting the number and distribution of Gaussians</p>
</section>
</section>
<section id="implementation-and-real-time-rendering">
<h2>Implementation and Real-Time Rendering<a class="headerlink" href="#implementation-and-real-time-rendering" title="Link to this heading"></a></h2>
<p>Efficient implementation is crucial for achieving real-time performance with 3D Gaussian Splatting.</p>
<section id="tile-based-rendering">
<h3>Tile-Based Rendering<a class="headerlink" href="#tile-based-rendering" title="Link to this heading"></a></h3>
<p>To achieve real-time performance, 3D Gaussian Splatting uses a tile-based approach:</p>
<ol class="arabic simple">
<li><p>Divide the screen into tiles (e.g., 16×16 pixels)</p></li>
<li><p>For each tile:
- Determine which Gaussians overlap it
- Sort those Gaussians by depth
- Render them front-to-back</p></li>
</ol>
<p>This localizes sorting to small batches of Gaussians, dramatically reducing computational overhead. The naive approach of considering every Gaussian for every pixel would be far too slow. Tile-based rendering exploits spatial coherence in the scene and image.</p>
</section>
<section id="fast-sorting-strategies">
<h3>Fast Sorting Strategies<a class="headerlink" href="#fast-sorting-strategies" title="Link to this heading"></a></h3>
<p>Sorting is required because correct alpha blending needs a defined order (usually far-to-near or near-to-far). The implementation uses GPU sorting algorithms that are highly optimized (e.g., parallel radix sort or merge sort using CUDA). Once Gaussians are culled per tile, each tile’s list is sorted by depth (the Gaussian’s center depth, or perhaps the depth of the Gaussian’s frontmost point – but likely center is fine if Gaussians are small relative to depth variation).</p>
<p>Sorting thousands of items on GPU is quite fast (modern GPUs can sort millions of numbers in a few milliseconds). And since this can be done tile-parallel (each tile sorting in parallel), the overall complexity is manageable.</p>
<p>The renderer is visibility-aware, meaning it accounts for occlusion by sorting. It also avoids “hard limits” on number of splats that get gradients, which implies the sorting + compositing method can handle an arbitrary number of overlapping Gaussians without capping (unlike some prior methods).</p>
<p>After sorting per tile, the renderer then <strong>rasterizes</strong> the Gaussian splats into that tile’s pixels in depth order. Rasterization means drawing the 2D Gaussian (ellipse) shape. This can be implemented as drawing either a textured sprite (point splat) or drawing a screen-aligned quad and in the fragment shader evaluate the Gaussian equation to get the coverage and alpha.</p>
<p>To ensure efficiency, the implementation likely uses <strong>shader programs</strong> or CUDA kernels that process each Gaussian and output to pixels. A naive draw call per Gaussian would be too slow (millions of draw calls). Instead, they might use a single compute shader that processes all Gaussians in a tile and writes into a tile buffer with atomic operations or blending.</p>
</section>
<section id="gpu-accelerated-rasterization">
<h3>GPU-Accelerated Rasterization<a class="headerlink" href="#gpu-accelerated-rasterization" title="Link to this heading"></a></h3>
<p>All the rendering steps are implemented on GPU to achieve real-time performance:</p>
<ul class="simple">
<li><p><strong>Memory for Gaussians:</strong> Storing up to 5 million Gaussians, each with position (3 floats), rotation (quaternion, 4 floats), scale (3 floats), color (e.g., 9 SH coefficients × 3 channels = 27 floats), opacity (1 float) can add up. For example, if each Gaussian has ~40 floats, 5 million Gaussians = 200 million floats, which is 800 MB (if 4 bytes each). They may compress some (for instance, color SH could be 1st or 2nd order only, or stored in half precision).</p></li>
<li><p><strong>Spatial data structure:</strong> The tile culling essentially is a spatial index. For rendering, they may pre-bucket Gaussians into some spatial grid or tree to quickly get those in each tile. During training, Gaussians move, so they likely update tile assignments dynamically but efficiently.</p></li>
<li><p><strong>Parallel rasterization:</strong> Modern GPUs support drawing points with programmable size, but anisotropic Gaussians (rotated ellipses) may require a custom approach: either rendering each Gaussian as a small triangle mesh (like an oriented billboard quad) or using a compute kernel.</p></li>
</ul>
<p>The rasterizer has constant overhead per pixel and low memory consumption, which suggests they do not allocate per-pixel large arrays (some earlier methods had per-pixel lists of splats). They probably use blending on the fly, so memory usage is just the final image and maybe a small depth buffer or so.</p>
<p>The implementation also supports <strong>anisotropic splats and fast back-propagation</strong> in rendering. For backprop, they likely store minimal info to compute gradients, possibly keeping a record of which Gaussians contributed to which pixel or recomputing during backprop similarly to forward but accumulating gradient instead of color.</p>
<p>Memory use is mainly:
- The Gaussian list (which could be hundreds of MB)
- The image buffer (e.g., 1920x1080x RGBA)
- Some intermediate buffers for tile indices, sorting keys, etc.</p>
</section>
<section id="memory-considerations">
<h3>Memory Considerations<a class="headerlink" href="#memory-considerations" title="Link to this heading"></a></h3>
<p>The memory footprint of 3D Gaussian Splatting depends on:</p>
<ul class="simple">
<li><p>Number of Gaussians (typically 1-5 million for a complex scene)</p></li>
<li><p>Parameters per Gaussian (position, rotation, scale, color, opacity)</p></li>
</ul>
<p>A typical scene might require hundreds of megabytes of memory, which is manageable on modern GPUs. The approach claims <strong>real-time rendering (≥ 30 fps at 1080p)</strong> for the final scenes, which is remarkable given the complexity.</p>
<p>Speed optimizations include:
- Tile culling to reduce fragment shader work
- Parallel sorting
- Controlled blending
- Careful use of shared memory and warp-synchronous programming in CUDA to handle each tile’s compositing</p>
<p>The method also leverages that speed during training by rendering the scene from random new viewpoints on the fly to compute loss, which accelerates the training process as well.</p>
</section>
</section>
<section id="comparison-with-other-methods">
<h2>Comparison with Other Methods<a class="headerlink" href="#comparison-with-other-methods" title="Link to this heading"></a></h2>
</section>
<section id="nerf-vs-3d-gaussian-splatting">
<h2>NeRF vs. 3D Gaussian Splatting<a class="headerlink" href="#nerf-vs-3d-gaussian-splatting" title="Link to this heading"></a></h2>
<p><strong>Neural Radiance Fields (NeRF)</strong> and <strong>3D Gaussian Splatting</strong> both ultimately represent scenes as a volumetric field of color and density, but their implementations differ drastically:</p>
<ul class="simple">
<li><p><strong>Representation</strong>: NeRF encodes the scene in the weights of a neural network (usually an MLP) which, given a 3D coordinate (and viewing direction), outputs color and density. This is a continuous implicit representation. Gaussian Splatting, on the other hand, uses an explicit list of primitives (Gaussians) with explicit parameters stored for each.</p></li>
<li><p><strong>Rendering</strong>: NeRF uses <strong>ray marching</strong> – sampling many points along each ray and querying the network for density and color, then compositing. This can produce high-quality results but is slow because of the many network evaluations. Gaussian Splatting uses <strong>rasterization</strong> of primitives – no heavy per-sample computation, just blending of already stored colors. This is orders of magnitude faster at render time.</p></li>
<li><p><strong>Training Speed</strong>: Vanilla NeRF is slow to train (hours to days) due to optimizing many network weights and sampling all rays repeatedly. Many improvements (InstantNGP, Plenoxels) sped this up. Gaussian Splatting trains very fast (minutes for a scene) because it directly optimizes a comparatively fewer parameters (millions of Gaussians vs millions of network weights, but often similar order) and uses efficient rendering in the loop.</p></li>
<li><p><strong>Quality and Generalization</strong>: NeRF with enough capacity can often achieve slightly higher peak quality for a given scene. However, recent results show Gaussian Splatting achieving comparable or even superior quality to state-of-the-art NeRF variants for captured scenes.</p></li>
</ul>
<table class="docutils align-default">
<colgroup>
<col style="width: 20.0%" />
<col style="width: 40.0%" />
<col style="width: 40.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Aspect</p></th>
<th class="head"><p>Neural Radiance Fields</p></th>
<th class="head"><p>3D Gaussian Splatting</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Representation</p></td>
<td><p>Implicit (neural network)</p></td>
<td><p>Explicit (set of 3D Gaussians)</p></td>
</tr>
<tr class="row-odd"><td><p>Training Time</p></td>
<td><p>Hours to days</p></td>
<td><p>Minutes</p></td>
</tr>
<tr class="row-even"><td><p>Rendering Speed</p></td>
<td><p>Seconds per frame</p></td>
<td><p>30-135 FPS (real-time)</p></td>
</tr>
<tr class="row-odd"><td><p>Memory Usage</p></td>
<td><p>Low (network weights)</p></td>
<td><p>Moderate (millions of Gaussians)</p></td>
</tr>
<tr class="row-even"><td><p>Quality</p></td>
<td><p>High</p></td>
<td><p>High</p></td>
</tr>
<tr class="row-odd"><td><p>Editability</p></td>
<td><p>Limited (implicit)</p></td>
<td><p>High (explicit primitives)</p></td>
</tr>
</tbody>
</table>
</section>
<section id="voxel-based-representations-vs-gaussians">
<h2>Voxel-Based Representations vs. Gaussians<a class="headerlink" href="#voxel-based-representations-vs-gaussians" title="Link to this heading"></a></h2>
<p>Voxel-based radiance fields (like Plenoxels, DVGO, or Neural Volumes) discretize space into a 3D grid. Each cell stores density and possibly a view-dependent color. Rendering is then a ray-marching through the grid, trilinearly interpolating values.</p>
<ul class="simple">
<li><p><strong>Memory</strong>: Voxel grids can be very memory heavy if high resolution is needed everywhere. Sparse voxel techniques allocate more voxels where needed and prune empty space, but for large scenes, memory can explode or require multi-level grids. Gaussian Splatting is more memory-efficient for large empty regions.</p></li>
<li><p><strong>Granularity</strong>: Voxels provide a fixed resolution (size of each voxel). Features smaller than a voxel cannot be represented unless you subdivide further. Gaussians are continuous and can have arbitrarily small covariance to represent fine features.</p></li>
<li><p><strong>Rendering speed</strong>: A dense voxel grid requires sampling many steps even in empty space (unless skipping empty cells using an octree or occupancy grid). Gaussians inherently skip empty space – no Gaussian, nothing to render.</p></li>
</ul>
<table class="docutils align-default">
<colgroup>
<col style="width: 20.0%" />
<col style="width: 40.0%" />
<col style="width: 40.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Aspect</p></th>
<th class="head"><p>Voxel Grids (e.g., Plenoxels)</p></th>
<th class="head"><p>3D Gaussian Splatting</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Adaptivity</p></td>
<td><p>Limited by grid resolution</p></td>
<td><p>Highly adaptive (varying density)</p></td>
</tr>
<tr class="row-odd"><td><p>Memory Efficiency</p></td>
<td><p>Low (stores empty space)</p></td>
<td><p>High (concentrates on surfaces)</p></td>
</tr>
<tr class="row-even"><td><p>Rendering Speed</p></td>
<td><p>Moderate to high</p></td>
<td><p>Very high</p></td>
</tr>
<tr class="row-odd"><td><p>Detail Representation</p></td>
<td><p>Limited by grid resolution</p></td>
<td><p>Can represent fine details</p></td>
</tr>
</tbody>
</table>
</section>
<section id="traditional-point-based-rendering-vs-gaussian-splatting">
<h2>Traditional Point-Based Rendering vs. Gaussian Splatting<a class="headerlink" href="#traditional-point-based-rendering-vs-gaussian-splatting" title="Link to this heading"></a></h2>
<p>Traditional point-based rendering (PBR) renders point clouds with splats or discs, often for models acquired from real-world scans. Methods from the early 2000s introduced ellipse splatting with EWA filtering to improve quality.</p>
<ul class="simple">
<li><p><strong>Rendering Algorithm</strong>: Traditional PBR like Surfels (Pfister 2000) or EWA Surface Splatting (Zwicker 2001) would take a point cloud (with normals, radiance etc.) and in a single pass project each point as a disk onto the screen, blending with neighbors to fill holes. Gaussian Splatting treats all Gaussians as potentially translucent and renders them with alpha blending in depth order.</p></li>
<li><p><strong>Differentiability and Optimization</strong>: Traditional point rendering took the points as given and did not move them. Gaussian Splatting integrates the rendering into an optimization loop to adjust points.</p></li>
<li><p><strong>Splat shape</strong>: Classic splatting often used circles or ellipses oriented according to surface normal. Gaussian Splatting’s splats are not exactly oriented by a surface normal (they don’t even explicitly store a normal), but effectively the covariance can align with the local surface orientation.</p></li>
</ul>
<table class="docutils align-default">
<colgroup>
<col style="width: 20.0%" />
<col style="width: 40.0%" />
<col style="width: 40.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Aspect</p></th>
<th class="head"><p>Traditional Point-Based Rendering</p></th>
<th class="head"><p>3D Gaussian Splatting</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Point Representation</p></td>
<td><p>Disks or simple splats</p></td>
<td><p>Anisotropic 3D Gaussians</p></td>
</tr>
<tr class="row-odd"><td><p>View-Dependent Effects</p></td>
<td><p>Limited</p></td>
<td><p>Supported via spherical harmonics</p></td>
</tr>
<tr class="row-even"><td><p>Quality</p></td>
<td><p>Moderate</p></td>
<td><p>High</p></td>
</tr>
<tr class="row-odd"><td><p>Differentiability</p></td>
<td><p>Not always</p></td>
<td><p>Fully differentiable</p></td>
</tr>
<tr class="row-even"><td><p>Adaptive Density</p></td>
<td><p>Fixed</p></td>
<td><p>Dynamic (splitting/pruning)</p></td>
</tr>
</tbody>
</table>
</section>
<section id="applications-and-extensions">
<h2>Applications and Extensions<a class="headerlink" href="#applications-and-extensions" title="Link to this heading"></a></h2>
<p>3D Gaussian Splatting has been applied to various domains and extended in multiple directions.</p>
</section>
<section id="static-scene-reconstruction">
<h2>Static Scene Reconstruction<a class="headerlink" href="#static-scene-reconstruction" title="Link to this heading"></a></h2>
<p>The primary application of 3D Gaussian Splatting is static scene reconstruction for novel view synthesis. Given a set of images of a scene (with known camera poses via structure-from-motion), one can use Gaussian Splatting to reconstruct the scene’s appearance such that new viewpoints can be rendered at high quality.</p>
<p>This makes it a drop-in alternative for NeRF in tasks like:
- Virtual tourism
- Heritage preservation
- Architectural visualization
- Content creation for VR/AR</p>
<p>Because of its fast training, this method is appealing for scenarios where turnaround time is important (e.g., on-set reconstruction in filmmaking, or quickly scanning an environment with a phone and getting a 3D model within minutes). The fact it achieves real-time rendering means the reconstructed model can be used in interactive applications, VR/AR, or video games directly, without needing heavy neural network inference at runtime.</p>
</section>
<section id="dynamic-scene-capture">
<h2>Dynamic Scene Capture<a class="headerlink" href="#dynamic-scene-capture" title="Link to this heading"></a></h2>
<p>Extensions to handle dynamic scenes include:</p>
<ul class="simple">
<li><p><strong>Deformable Gaussians</strong>: Adding a deformation field to warp Gaussians over time</p></li>
<li><p><strong>4D Gaussians</strong>: Extending the representation to include time as a fourth dimension</p></li>
<li><p><strong>Dynamic 3D Gaussians</strong>: Tracking the motion of Gaussians through a sequence</p></li>
</ul>
<p>These approaches enable high-quality reconstruction and rendering of dynamic content like humans in motion. A fully 4D Gaussian representation would be where each Gaussian is actually a Gaussian in spacetime (with a mean position in 3D and time, and a covariance that could span time as well).</p>
</section>
<section id="avatar-creation-and-animation">
<h2>Avatar Creation and Animation<a class="headerlink" href="#avatar-creation-and-animation" title="Link to this heading"></a></h2>
<p>3D Gaussian Splatting can be combined with parametric human models like SMPL:</p>
<ol class="arabic simple">
<li><p>Use SMPL as a template for human shape and pose</p></li>
<li><p>Optimize Gaussians to match the appearance of a specific person</p></li>
<li><p>Animate the Gaussians by transferring motion to the underlying skeleton</p></li>
</ol>
<p>This creates animatable avatars with high visual fidelity for applications in virtual reality, gaming, and digital communication.</p>
</section>
<section id="integration-with-neural-rendering">
<h2>Integration with Neural Rendering<a class="headerlink" href="#integration-with-neural-rendering" title="Link to this heading"></a></h2>
<p>Gaussian Splatting can be integrated with neural networks in various ways:</p>
<ul class="simple">
<li><p><strong>Neural Refinement</strong>: After producing a Gaussian-splatting render, one could feed the rendered image (and perhaps depth) into a neural super-resolution or refinement network to enhance details or consistency.</p></li>
<li><p><strong>Hybrid Models</strong>: One could use Gaussians as an intermediate representation inside a neural pipeline. For example, a neural network could predict the Gaussian parameters from input images (instead of optimizing them).</p></li>
<li><p><strong>Neural Shading</strong>: The Gaussian splats currently use simple SH lighting. One could incorporate a small neural network that given viewing direction and perhaps other features outputs a shading multiplier for each Gaussian.</p></li>
<li><p><strong>Feature Integration</strong>: Gaussians could carry not just color but feature vectors that are processed by a network to produce final pixel colors.</p></li>
</ul>
<p>One interesting integration is using Gaussians as an initial guess for NeRF or vice versa. Since Gaussian Splatting is fast, one could get a decent model quickly, then perhaps distill it into a NeRF network for portability or editing.</p>
</section>
<section id="large-scale-scene-rendering">
<h2>Large-Scale Scene Rendering<a class="headerlink" href="#large-scale-scene-rendering" title="Link to this heading"></a></h2>
<p>Large-scale scenes (think a whole city block or a forest) pose challenges in representation and rendering. Gaussian Splatting can handle unbounded scenes, but scaling to truly large scenes with millions of distinct objects might require additional strategies:</p>
<ul class="simple">
<li><p><strong>Tiling the Scene</strong>: Just like tiling the image, one can spatially partition the scene into regions (octree or grid of cells). Each region contains a subset of Gaussians. One can frustum-cull entire regions quickly if they are out of view, and only stream the Gaussians for visible regions to the GPU.</p></li>
<li><p><strong>LOD (Level of Detail)</strong>: For far distance, you might not need all Gaussians. Perhaps combining some into a single Gaussian (merging) could reduce count.</p></li>
<li><p><strong>Out-of-Core management</strong>: If a scene is so large that millions of Gaussians don’t fit in memory, one could load and unload parts based on camera. Since each Gaussian is independent, chunks can be loaded from disk (with some spatial index).</p></li>
</ul>
<p>A noteworthy extension addressing larger scenes is <strong>GaussianPro</strong> (Cheng et al., 2024). GaussianPro specifically tackled the issue that SfM initialization fails on texture-less areas, leading to insufficient Gaussians there and poor reconstruction. They introduced a progressive propagation to add Gaussians in those areas using patch-match stereo cues. This improved quality on large indoor/outdoor scenes.</p>
</section>
<section id="bezier-gaussian-triangles-bg-triangle-for-sharper-rendering">
<h2>Bézier Gaussian Triangles (BG-Triangle) for Sharper Rendering<a class="headerlink" href="#bezier-gaussian-triangles-bg-triangle-for-sharper-rendering" title="Link to this heading"></a></h2>
<p>One limitation of plain 3DGS is its tendency to blur high-frequency details, especially along sharp object edges. Because Gaussian splats overlap and have smooth radial profiles, representing a hard edge or surface boundary would require many tiny Gaussians, and even then edges may appear slightly soft. The BG-Triangle method addresses this by introducing a hybrid primitive that combines Gaussian splats with parametric surface patches to preserve discontinuities. Specifically, Bézier Gaussian Triangles (BG-Triangles) represent local surfaces as curved triangular patches (a Bézier triangle is a curved surface defined by control points, generalizing Bézier curves to 2D domains) rather than a cloud of points. Each such patch carries its own smooth internal texture representation but maintains explicit sharp boundaries at its edges.</p>
<section id="representation">
<h3>Representation<a class="headerlink" href="#representation" title="Link to this heading"></a></h3>
<p>A BG-Triangle is defined by a set of control points forming a Bézier surface patch of a certain degree (e.g. quadratic). This patch can model a continuous surface area. Along with this, BG-Triangle stores an attribute map (akin to a texture) that encodes color and possibly finer details across the surface. The key idea is that the interior of the triangle can still be rendered via splatting sub-primitives, but the edges of the triangle serve as differentiable discontinuities, preventing cross-boundary blurring. During rendering, the curved triangle is tessellated into many small pieces (sub-triangles) if needed. Those pieces are then rasterized using a modified alpha blending that is discontinuity-aware – essentially ensuring that adjacent triangles do not bleed color across their borders. Each sub-triangle or sample on the patch can still be associated with Gaussian kernels for anti-aliasing (hence “Gaussian triangle”), but the structure imposes a sharper geometry.</p>
</section>
<section id="performance">
<h3>Performance<a class="headerlink" href="#performance" title="Link to this heading"></a></h3>
<p>By fitting vector-like surface primitives, BG-Triangle drastically reduces the number of primitives needed to represent a scene while maintaining or improving visual quality. Yao et al. report that BG-Triangle achieves comparable error metrics to 3DGS on standard benchmarks, yet uses far fewer primitives and yields much crisper edges in novel views. For example, in close-up views of the NeRF Synthetic dataset, BG-Triangle renders object boundaries sharply, whereas Gaussian splats produce slight blur or splatter. Table 2 in their paper quantitatively shows BG-Triangle outperforming pure Gaussian methods (3DGS and a mipmapped variant) under equal-capacity settings. The method’s boundary-preserving advantage is evident: as noted, BG-Triangle can even appear sharper than the ground truth at some zoom levels due to its anti-aliasing, while 3DGS appears blurrier. Furthermore, BG-Triangle bridges classical and neural representations – it hints at how vector graphics-style patches can be optimized in a differentiable way from images. This hybrid approach can potentially lead to representations that are easier to edit (since one can move control points of patches) while still being learned from data.</p>
<p>In summary, BG-Triangle extends 3D Gaussian Splatting by introducing structured surface elements. It preserves the benefits of 3DGS (differentiability, volumetric effects, rasterization speed) but mitigates its weaknesses at discontinuities. The cost is a slightly more complex pipeline (tessellation, handling of patch boundaries) and possibly the need for an initial mesh or surface guess to initialize control points. However, robust results have been demonstrated by automatically deriving coarse “boundary meshes” from a point cloud and then optimizing the Bézier patches. BG-Triangle’s success suggests that blending explicit geometric primitives (triangles) with Gaussian splats is a promising direction to achieve sharp, low-memory 3D representations.</p>
</section>
</section>
<section id="human-reconstruction-with-gaussian-splatting-and-priors">
<h2>Human Reconstruction with Gaussian Splatting and Priors<a class="headerlink" href="#human-reconstruction-with-gaussian-splatting-and-priors" title="Link to this heading"></a></h2>
<p>Modeling human subjects, especially from sparse camera views, is a challenging task where Gaussian splatting has recently made inroads. The human body presents complex, articulated geometry with clothing, and purely implicit or point-based methods struggle to generalize across different people and poses. Recent methods leverage human priors – such as parametric body models (SMPL/SMPL-X) or learned shape spaces – in combination with radiance field or splatting techniques to improve both quality and speed of novel-view rendering. Two notable threads are generalizable NeRF-based approaches like EG-HumanNeRF, and Gaussian splat-based approaches like GPS-Gaussian and Generalizable Human Gaussians (GHG). We discuss how each integrates human priors and the role of Gaussian splats.</p>
<section id="eg-humannerf-efficient-generalizable-human-nerf">
<h3>EG-HumanNeRF: Efficient Generalizable Human NeRF<a class="headerlink" href="#eg-humannerf-efficient-generalizable-human-nerf" title="Link to this heading"></a></h3>
<p>EG-HumanNeRF (Wang et al., 2024) is a NeRF-style pipeline that achieves real-time rendering of human scenes from sparse views by heavily incorporating a parametric body prior. While not a Gaussian splatting method per se, it directly addresses the same problem setting as many GS approaches (fast novel view synthesis of humans) and offers a useful comparison between explicit splatting and efficient neural rendering. EG-HumanNeRF’s key ideas include: (1) Using a fitted SMPL-X body mesh as a coarse geometry prior to guide sampling; (2) Reducing ray samples via a two-stage sampling strategy that first intersects rays with an inflated “boundary mesh” around the body (to skip empty space) and then only samples a small number of points near the surface for volume rendering; (3) An occlusion-aware attention mechanism that uses the prior mesh to infer which regions are occluded from each view, helping the network avoid artifacts in unseen areas; and (4) An image-space refinement network to further clean up the rendered output. They also introduce a signed ray distance function (SRDF) loss at each sample to regularize the volume density learning.</p>
<p>The result is a system that outperforms prior human view synthesis methods in quality while running at comparable speeds to the fastest (splatting-based) methods. For instance, EG-HumanNeRF is shown to eliminate artifacts that other methods produce in occluded areas: when input views are sparse, baseline methods had blurriness or missing limbs (e.g. fingers) due to uncertain geometry, whereas EG-HumanNeRF’s use of the SMPL prior plus occlusion reasoning yields intact, sharp renderings. A comparative table from their paper (reproduced in Figure 1 of the source) contrasts EG-HumanNeRF with others: methods like KeypointNeRF and others used some human prior but were slow, while methods like GPS-Gaussian (discussed next) were fast but lacked any human prior and thus could suffer in quality. EG-HumanNeRF hits a sweet spot by using the mesh prior for both speed (drastically fewer ray samples) and quality (guiding the NeRF to plausible human shapes even in unseen regions). In terms of numbers, it can render at real-time rates (on the order of 20–30 FPS) and produces high-fidelity images that surpass prior generalizable NeRFs by &gt;1 dB PSNR in their tests, without test-time fine-tuning.</p>
<p>The success of EG-HumanNeRF underscores the value of integrating parametric models (SMPL-X) with volumetric rendering. By itself, NeRF would require many samples and might falter with few views, but the human prior constrains the solution. This idea carries over to Gaussian splatting approaches as well.</p>
</section>
<section id="gps-gaussian-pixel-wise-gaussian-splatting-for-humans">
<h3>GPS-Gaussian: Pixel-Wise Gaussian Splatting for Humans<a class="headerlink" href="#gps-gaussian-pixel-wise-gaussian-splatting-for-humans" title="Link to this heading"></a></h3>
<p>GPS-Gaussian (Zheng et al., CVPR 2024) is a generalizable human view synthesis method that uses 3D Gaussian splats as the underlying representation, but predicts them in a single forward pass of a network (no per-subject optimization). The motivation is to achieve instantaneous rendering of a new person given only a sparse set of images, by training on a large dataset of human scans. GPS-Gaussian breaks from the typical approach of optimizing a point cloud for each subject; instead it learns a direct mapping from input views to Gaussian parameters.</p>
<p>Key innovations: GPS-Gaussian introduces the concept of pixel-aligned Gaussian parameter maps. Given two (or a few) input images of a person from different views, the method produces for each source view an image-sized map where each pixel stores the parameters of a Gaussian (position, orientation, scale, color, opacity) that corresponds to that pixel’s projection of the person. In essence, it reprojects the problem to 2D: each foreground pixel “emits” a Gaussian into 3D. To lift these into 3D space, a differentiable stereo depth module estimates per-pixel depth using the two source views. Once depth is estimated, the 2D Gaussian parameters from both views are unprojected to 3D, yielding a set of 3D Gaussians covering the human’s surface. These can then be splatted to render a novel view with standard 3DGS rendering. Because some pixels might be erroneous in depth (especially under self-occlusion), they train the depth estimator and the Gaussian prediction jointly, with an iterative refinement and losses that penalize inconsistent depths and improve rendering quality.</p>
<p>Results: The outcome is impressive – GPS-Gaussian achieves 2K resolution novel view synthesis at &gt;25 FPS on a single GPU, with a single network handling arbitrary new subjects. It requires no test-time optimization/fine-tuning; a new human can be rendered nearly instantly after a quick inference of the two-view networks. The use of convolutional image encoders (rather than 3D network queries) makes it very fast. Empirically, it outperforms prior state-of-the-art human view synthesis methods on quality while being real-time. Notably, Zheng et al. report that even compared to explicit point-based approaches, their learned method produces better geometry and appearance for unseen poses. The reliance on large-scale training (they train on “massive 3D human scans with diverse topologies, clothing styles and poses”) means the model has learned a strong human shape prior implicitly. In practice, this is similar to how one might use SMPL: the network has an idea of human morphology, which helps guide the depth predictions and color completion for occluded parts.</p>
<p>The difference from EG-HumanNeRF is that GPS-Gaussian is fully feed-forward. It does not even explicitly use a body model like SMPL; however, it effectively learns one internally via the training data. An advantage of GPS-Gaussian’s explicit splat output is that once the Gaussians are predicted, rendering is extremely fast and also flexible (one could in principle adjust them, apply transformations, etc.). The method demonstrates how Gaussian splatting can be plugged into a deep network pipeline for generalization, combining the best of both worlds: neural priors from data and the efficiency of explicit splats.</p>
</section>
<section id="generalizable-human-gaussians-ghg-with-smpl">
<h3>Generalizable Human Gaussians (GHG) with SMPL<a class="headerlink" href="#generalizable-human-gaussians-ghg-with-smpl" title="Link to this heading"></a></h3>
<p>An alternative approach to human generalization is to explicitly tie the Gaussian representation to a parametric human model. Generalizable Human Gaussians (GHG) by Kwon et al. (arXiv 2024) does this by learning Gaussian parameters on the 2D UV space of a template mesh (SMPL-X). In GHG, each Gaussian is essentially anchored to a location on the SMPL-X surface (via its UV coordinate). This means when a new subject with a fitted SMPL-X mesh is given, one can map the learned Gaussian field onto that mesh, adjusting for the person’s shape and pose. The Gaussian colors and offsets are predicted by a network, similar in spirit to GPS-Gaussian but using the UV parametric space as the domain for convolution (rather than image pixels). A multi-scaffold strategy is introduced to allow additional flexibility – e.g. one scaffold might model coarse body shape while another adds high-frequency clothing wrinkles as offsets to Gaussian positions.</p>
<p>By leveraging the strong geometry prior of SMPL-X, GHG ensures that the learned Gaussians always lie on or near a plausible human surface. This greatly reduces ambiguity from sparse views. Their results show excellent generalization: from as few as 3 input images of a new person, GHG can render photorealistic views without finetuning, outperforming prior generalizable methods on both seen and unseen datasets. Because the representation is explicit (a set of Gaussians on a mesh), it retains real-time rendering capability via splatting. In fact, GHG uses the same volume splatting renderer as 3DGS (they call it a “fast rendering paradigm of volume splatting”). The difference is that the Gaussians are now organized according to a mesh template. This also allows the method to potentially do animation: since the Gaussians move with the SMPL-X pose, one could drive the captured appearance with new poses (though the paper’s focus is novel-view rather than novel-pose).</p>
<p>Integrating SMPL in this way has some trade-offs: it assumes a decent fit of the parametric model to the subject (which is feasible with few images using existing human pose estimators), and it inherits the template’s limitations (e.g. difficulty with very loose clothing or long hair not represented in SMPL-X). However, the clear benefit is consistency and completeness – even if an arm is completely occluded in the input views, the SMPL prior ensures Gaussians are still placed for that arm, and their colors can be inferred from neighboring pixels via the learned UV-space network. This addresses a common failure of unguided methods, which might leave holes or require inpainting for unseen regions.</p>
<p>In summary, these human-centric developments show that Gaussian Splatting can be enhanced by human structural priors in multiple ways. EG-HumanNeRF uses the prior to guide NeRF sampling; GPS-Gaussian trains a feed-forward network (implicitly learning a prior) to predict splats; and GHG explicitly uses the SMPL-X surface as a scaffold for Gaussians. All achieve high-quality, fast human renderings from sparse inputs – a task that is significantly harder than the static scene case. A common theme is that the combination of explicit (Gaussian or mesh) representations with learned priors is very powerful for generalization.</p>
</section>
</section>
<section id="dynamic-scene-reconstruction-with-4d-gaussian-splatting">
<h2>Dynamic Scene Reconstruction with 4D Gaussian Splatting<a class="headerlink" href="#dynamic-scene-reconstruction-with-4d-gaussian-splatting" title="Link to this heading"></a></h2>
<p>Beyond static scenes and single moments, researchers have extended Gaussian splatting to model dynamic, time-varying scenes (a full 4D space-time representation). Traditional dynamic view synthesis with NeRFs often used deformation fields or per-frame NeRFs, but these are computationally heavy and hard to run in real time. Point-based approaches promise faster rendering, and indeed 4D Gaussian Splatting (4DGS) has emerged as a leading explicit method for dynamic scenes. In 4DGS, each primitive is a spatio-temporal Gaussian – effectively an ellipsoid moving or changing over time.</p>
<section id="d-gaussian-splatting-4dgs">
<h3>4D Gaussian Splatting (4DGS)<a class="headerlink" href="#d-gaussian-splatting-4dgs" title="Link to this heading"></a></h3>
<p>Wu et al. (CVPR 2024) introduced one formulation of 4DGS that achieves real-time rendering of dynamic scenes, even for challenging cases like non-rigid motions. Their approach represents the scene with one set of canonical 3D Gaussians (at a reference time) and learns a Gaussian deformation field that maps those Gaussians to their positions at each time frame. In other words, rather than have completely independent Gaussians for each time, they have persistent identity for Gaussians that move over time. A tiny multi-head MLP (the deformation decoder) takes as input a Gaussian’s canonical coordinates and a time <span class="math notranslate nohighlight">\(t\)</span>, and outputs that Gaussian’s offset, rotation, and scale change for time <span class="math notranslate nohighlight">\(t\)</span>. A spatial-temporal encoder processes groups of Gaussians to embed motion cues, allowing the network to infer coherent motion even if a Gaussian was occluded for a while. This approach is akin to learning a continuous motion field for the points. Because only one set of Gaussians is stored (the canonical set) plus a small network, the memory remains manageable.</p>
<p>Advantages: The deformation-field 4DGS has a big advantage in storage and coherence. Naively, one could optimize a separate 3DGS model for every time frame, but for a long sequence that would multiply storage and also not exploit temporal redundancy. Wu et al. show that with their method, they can render dynamic scenes at up to 82 FPS (800×800) or ~30 FPS at HD resolution, with quality on par with or better than dynamic NeRFs. Importantly, their representation is compact and editable: since each Gaussian persists through time, one can track correspondences or even modify motions. They demonstrate basic 4D editing and object tracking as a benefit. This explicit nature is a contrast to neural dynamic radiance fields that often entangle time and view in complex ways.</p>
<p>However, 4DGS in its initial form did face challenges. A major one identified is temporal redundancy: many Gaussians might only “live” for a short period or many Gaussians might be static and yet duplicated across time due to optimization issues. Also, even with deformation fields, the number of Gaussians can still be high (millions) to capture fine details, leading to heavy memory (the original 4DGS could require ~2 GB for a dynamic scene). These issues spurred follow-up research.</p>
</section>
<section id="speed-and-memory-enhancements-4dgs-1k-and-mega">
<h3>Speed and Memory Enhancements (4DGS-1K and MEGA)<a class="headerlink" href="#speed-and-memory-enhancements-4dgs-1k-and-mega" title="Link to this heading"></a></h3>
<p>In 2025, Yuan et al. proposed 4DGS-1K, dubbed for its achievement of over 1000 FPS rendering of dynamic scenes. They systematically address the redundancies: (1) Short-lifespan Gaussians – they introduce a spatio-temporal variation score to prune Gaussians that only contribute briefly, encouraging the model to explain motion with longer-lived Gaussians. (2) Inactive Gaussians per frame – they maintain a per-frame mask of which Gaussians are actually visible/needed, so the rasterizer can skip large portions that do not affect the current view. With these techniques, 4DGS-1K reduces the Gaussian count dramatically and avoids rasterizing unnecessary splats each frame. As a result, they report 1000+ FPS rendering on modern GPUs, a 50× speedup over the original, while keeping image quality largely intact. Storage was also reduced to a fraction of the original (they note using only ~10% of the storage with comparable quality). This pushes Gaussian splatting firmly into the realm of real-time 4D video. Their evaluations on datasets like N3V (Neural 3D Video dataset with 6 multi-view videos) show that 4DGS-1K achieves similar PSNR/SSIM as vanilla 4DGS but with much smaller memory and vastly higher FPS. Such performance makes even interactive applications (e.g. dynamic scene VR) conceivable with explicit splats.</p>
<p>Another effort, MEGA (Zhang et al., 2024) focused on memory efficiency of 4DGS. They observed that a big memory hog was the color storage per Gaussian: original 4DGS stored up to 144 parameters of SH for color. MEGA replaces this with a hybrid approach: each Gaussian has just a 3-parameter base color plus a learned global color predictor that adds view-dependent effects. This cuts down memory enormously (no need for high-order SH per point). They also impose an entropy-based regularization on the deformation field so that each Gaussian covers a larger temporal span (similar spirit to 4DGS-1K’s idea) and they penalize opacity complexity to force fewer Gaussians. Combined with compression (half precision, etc.), they achieved up to 125×–190× reduction in storage on dynamic datasets, again without quality loss. These improvements mean a dynamic scene that might have taken gigabytes can be stored in just tens of MB, which is even smaller than some implicit methods, all while preserving the real-time nature (rasterization remains fast).</p>
</section>
<section id="applications-to-mocap-and-4d-human-rendering">
<h3>Applications to MoCap and 4D Human Rendering<a class="headerlink" href="#applications-to-mocap-and-4d-human-rendering" title="Link to this heading"></a></h3>
<p>Dynamic Gaussian splatting has also been applied specifically to human motion capture and 4D human reconstruction. For instance, some works (e.g. Li et al. 2024, as cited in GPS-Gaussian) learn animatable Gaussian avatars – essentially learning pose-dependent Gaussian distributions for a human, so that given new skeletal poses, the Gaussians move accordingly to render novel poses. These approaches combine the benefits of skeletal animation (via something like SMPL) with Gaussian splat rendering. The Animatable Gaussians work by Li et al. (CVPR 2024) learned pose-dependent 2D Gaussian maps for a human head and body, enabling high-fidelity avatars that can be driven by motion capture data. It indicates that Gaussian splats can serve as an intermediate representation for deformable objects where control signals (like joint angles) modulate the distribution of Gaussians.</p>
<p>Moreover, some early attempts integrate 4DGS with SLAM (simultaneous localization and mapping) for dynamic scenes, demonstrating that one can potentially do online reconstruction of a changing environment by continually updating Gaussian splats in space-time. This could be useful for capturing urban environments with moving vehicles (one reference mentions a “Periodic Vibration Gaussian: Dynamic Urban Scene Reconstruction” – likely a specialized dynamic GS for city scenes). Though not all these are published in major venues, they show the extensibility of Gaussian splatting to various domains.</p>
<p>In the context of motion capture (MOCAP), 4DGS can be seen as an alternative to parametric mesh tracking. Instead of capturing a performer with multi-view cameras and fitting a mesh sequence, one could optimize a collection of Gaussians to the multi-view video. The advantage is that the result is fully textured and can be rendered from any viewpoint with correct appearance (the splats directly store color and learned lighting effects). The disadvantage is that it’s less straightforward to retarget or manipulate than a mesh; however, if combined with a skeleton prior (like attaching Gaussians to SMPL, as GHG or animatable Gaussians do), one gains that control back. We are witnessing the convergence of ideas: some methods are bringing neural volumetric ideas into the mesh world (NeRF + SMPL), and others are bringing graphics primitives into the neural world (Gaussians + networks). Dynamic Gaussian splatting sits in the middle, offering a fully explicit yet dynamic scene representation.</p>
<p>Dynamic Benchmarks: Common datasets to test these methods include N3V (a public multi-view video dataset), the D-NeRF synthetic dynamic dataset (with simple animated objects), and for human-specific methods, sequences from ZJU-MoCap or H36M, as well as CAPE (for clothed humans) or even synthetic data derived from AMASS motions. For example, a dynamic human method might take a sequence of a person from multiple cameras (like the H36M dataset of people moving) and reconstruct a 4D Gaussian scene. The dynamic nature means metrics are computed over space-time reconstructions, looking at consistency and temporal coherence in addition to per-frame quality.</p>
</section>
</section>
<section id="implementation-details-and-real-time-performance">
<h2>Implementation Details and Real-Time Performance<a class="headerlink" href="#implementation-details-and-real-time-performance" title="Link to this heading"></a></h2>
<p>A significant appeal of Gaussian splatting is its practicality: with the right implementation, it can run at high frame rates and scale to large scenes. Here we outline some low-level considerations:</p>
<section id="data-structures">
<h3>Data Structures<a class="headerlink" href="#data-structures" title="Link to this heading"></a></h3>
<p>Efficient spatial indexing of millions of Gaussians is important. In static 3DGS, one can use a tree or grid to cull Gaussians outside the camera frustum or below a certain size at a given distance. The original 3DGS used a visibility buffer to skip splatting Gaussians that don’t contribute (similar to a z-buffer test). For LoD, each Gaussian can have a multiscale representation (mip-mapped radius and pre-filtered color), so that when far from the camera, it can be drawn smaller or not at all. Dynamic 4DGS adds a temporal dimension – there one can precompute per-frame active sets of Gaussians, as 4DGS-1K did with bitmasking.</p>
</section>
<section id="rasterization-shaders">
<h3>Rasterization &amp; Shaders<a class="headerlink" href="#rasterization-shaders" title="Link to this heading"></a></h3>
<p>Gaussian splats can be rendered using point primitives (with a geometry shader expanding to a billboard quad) or as triangle meshes (e.g. an ellipse approximated by a small polygon). A tile-based rasterization approach can also be used: splitting the screen into tiles and only processing Gaussians that intersect each tile. This was hinted in some implementations (e.g. the MEGA paper mentions a GPU-friendly tile rasterizer). One must also sort splats by depth for correct blending; this can be costly if done on CPU for every frame. Instead, a common trick is to exploit hardware blending by drawing back-to-front sorted by an approximate grid. Some research prototypes divided space into depth layers or sorted per tile to manage this at scale.</p>
</section>
<section id="gpu-memory-and-throughput">
<h3>GPU Memory and Throughput<a class="headerlink" href="#gpu-memory-and-throughput" title="Link to this heading"></a></h3>
<p>Each Gaussian carries a non-trivial amount of data (position, covariance, color etc.). Packing these efficiently (e.g. quantizing covariance or using half floats) can save memory bandwidth. The memory footprint was a concern especially for 4DGS, leading to solutions like MEGA’s SH compression. Also, while millions of Gaussians might be stored, at render time only a fraction are actually drawn (those in view and above a size threshold). Efficient culling on the GPU (via compute shaders that mark active splats) can dramatically reduce the fragment shading cost.</p>
</section>
<section id="differentiable-rendering-implementation">
<h3>Differentiable Rendering Implementation<a class="headerlink" href="#differentiable-rendering-implementation" title="Link to this heading"></a></h3>
<p>For training, one often uses a custom CUDA kernel or OpenGL fragment shader that computes the Gaussian coverage of each pixel and its derivative w.rt. Gaussian parameters. This involves calculating the projected ellipse and its intersection with the pixel. Some works rely on prior art like SoftRasterizer or Differentiable Surface Splatting (DSS), but 3DGS typically implemented its own for anisotropic Gaussians with alpha blending. Because the renderer is differentiable, one can directly optimize millions of Gaussian parameters with gradient descent (often using Adam optimizer). It’s noteworthy that despite the large number of parameters, the explicit nature and good initialization (from SfM) allows fairly quick convergence to high quality.</p>
</section>
<section id="training-vs-inference-compute">
<h3>Training vs. Inference Compute<a class="headerlink" href="#training-vs-inference-compute" title="Link to this heading"></a></h3>
<p>Training a 3DGS from scratch on a scene can still be time-consuming (though faster than NeRF in many cases). Kerbl et al. reported competitive training times – on the order of hours – to reach high quality, thanks to not shooting rays through empty space. Generalizable methods like GPS-Gaussian shift the heavy lift to offline training on many scans (taking days on large GPUs), but then inference per scene is seconds. In deployment, rendering is extremely fast: essentially just a draw call of all splats. For example, the official 3DGS codebase uses OpenGL to render splats in real time, and the bottleneck becomes how many splats the GPU can rasterize per frame (modern GPUs handle tens of millions of fragments easily at 60+ FPS).</p>
</section>
<section id="accuracy-vs-speed-trade-offs">
<h3>Accuracy vs. Speed trade-offs<a class="headerlink" href="#accuracy-vs-speed-trade-offs" title="Link to this heading"></a></h3>
<p>Some quality-improving steps can slow down rendering. For instance, alias-free splatting (mip-splatting) requires computing a footprint for each splat at each frame to avoid under-sampling. This is an added per-splat cost, but it prevents shimmering when zooming out. Another example: one could increase the SH basis order to capture more view-dependent effects, but that means more parameters and more compute per pixel to evaluate the SH. Implementations often choose a moderate SH order (like 2nd order, 9 coeffs per color) as a balance. Some of the “compact” variants (Mini-Splatting, Scaffold-GS) introduce small neural networks to predict fewer splats – these hybrid approaches sacrifice a bit of pure speed (due to network inference) in exchange for using fewer primitives overall.</p>
<p>In practice, many of these methods are available as open-source, with highly optimized CUDA code. For example, the original 3DGS authors released a CUDA/OpenGL code that can train on a scene and then export the splat model to be rendered in standard graphics engines. Chaos V-Ray even integrated Gaussian splats into a ray tracer, allowing mixing splats with traditional rendering – this underscores that the format is becoming mature enough for content creation pipelines.</p>
</section>
</section>
<section id="benchmarks-and-comparative-evaluation">
<h2>Benchmarks and Comparative Evaluation<a class="headerlink" href="#benchmarks-and-comparative-evaluation" title="Link to this heading"></a></h2>
<p>To measure progress, researchers evaluate on standard datasets: NeRF Synthetic (Blender) for controlled scenes with ground-truth geometry, LLFF (Real Forward-Facing) for real-world still captures, Tanks and Temples for large-scale outdoor scans, and others for static scenes. For dynamic scenes, benchmarks include D-NeRF (synthetic moving objects), HyperNeRF dataset, and N3V (Neural 3D Video) which provides real multi-view videos. For human-focused methods, common datasets are THuman 2.0 (a set of 3D scanned people with SMPL fits), RenderPeople (a collection of commercial scanned avatars), and sequences like H36M or ZJU-MoCap for moving humans. Additionally, datasets like CAPE (Clothed Avatar Pose dataset) provide sequences of 3D meshes of clothed people, which can be used to evaluate how well methods capture deforming clothing over time.</p>
<section id="static-scene-comparison">
<h3>Static Scene Comparison<a class="headerlink" href="#static-scene-comparison" title="Link to this heading"></a></h3>
<p>On Blender scenes (e.g. Lego, Chair, Drums), 3DGS typically achieves similar PSNR/SSIM to NeRF while rendering 1–2 orders of magnitude faster. BG-Triangle further excels in perceptual metrics (LPIPS) by preserving edges better. For real scenes like LLFF, 3DGS and NeRF are often on par, though 3DGS can struggle if the point cloud initialization is sparse (NeRF might fill in even if few points). Tanks &amp; Temples results in Kerbl et al.’s paper show 3DGS surpassing mip-NeRF360 in some cases – indicating explicit points handle large unbounded scenes well by not blurring distant details (mip-NeRF360 addresses that with elaborate mipmaps, but 3DGS inherently has LoD from splat sizes). One weakness of 3DGS noted is slightly lower accuracy in very fine geometry (e.g. thin wires) because Gaussians might over-smooth them – techniques like mip-splatting or BG-Triangle mitigate this.</p>
</section>
<section id="human-novel-view-comparison">
<h3>Human Novel-View Comparison<a class="headerlink" href="#human-novel-view-comparison" title="Link to this heading"></a></h3>
<p>EG-HumanNeRF vs. GPS-Gaussian vs. GHG provides an interesting study. EG-HumanNeRF was shown to outperform KeypointNeRF, GP-NeRF, etc., in PSNR/SSIM, and even outdo GPS-Gaussian in problematic occluded cases (because of its occlusion handling). However, GPS-Gaussian, being a CVPR 2024 highlight, also demonstrated superior quality to many contemporaries – likely EG-HumanNeRF’s advantage is on very sparse inputs where the learned prior alone might not guess hidden surfaces as well as a mesh-guided approach. GHG, in their cross-dataset tests (train on THuman, test on RenderPeople), reported higher PSNR/SSIM than both neural (KeypointNeRF, etc.) and explicit (prior NeRF-W, etc.) methods, confirming the benefit of UV-based Gaussian learning. In terms of speed, GPS-Gaussian and EG-HumanNeRF both target real-time: GPS-Gaussian inherently renders at 25+ FPS at high res, and EG-HumanNeRF’s design keeps it near that range (they cite competitive speed with “speed-prioritized” methods like GPS). GHG’s runtime is basically that of 3D splatting (very fast), plus whatever time it takes to predict Gaussians via their network (which could be a few hundred milliseconds given it’s a ConvNet on UV maps). All these methods can work with as few as 2–3 views of a person; their relative performance can depend on how complex the outfit is (a learned method might handle clothing learned from data better, whereas a mesh-guided might handle arbitrary occlusion better).</p>
</section>
<section id="dynamic-scene-comparison">
<h3>Dynamic Scene Comparison<a class="headerlink" href="#dynamic-scene-comparison" title="Link to this heading"></a></h3>
<p>Dynamic NeRF methods (e.g. D-NeRF, Nerfies, HyperNeRF, NR-NeRF) often sacrifice speed – some can take minutes per frame to render, which is not real-time. 4DGS and its variants are orders of magnitude faster in rendering (real-time vs. offline), making direct comparison tricky. Quality-wise, early 4DGS achieved comparable PSNR to NeRF-based counterparts on simple benchmarks, and with 4DGS-1K and MEGA, the gap closed further. One interesting comparison is with deformation field methods like NR-NeRF or NSFF: those use a parametric motion field (usually learned via neural networks) to warp a static scene. 4DGS (Wu et al.) similarly uses a learned deformation field but explicitly applied to points. It was noted that NeRF-based deformation methods had much smaller storage (tens of MB) than uncompressed 4DGS (GBs), but with the new compression techniques, 4DGS now can also be in the tens of MB. In terms of visual fidelity, Gaussian methods naturally handle moderate topology changes (e.g. objects splitting) since they are just a cloud of blobs, whereas some NeRF methods that assume a single warp field can struggle with multiple independently moving parts. On the other hand, if a scene has very subtle motions (like a fluttering flag), NeRF might capture it with a neural field smoothly, whereas points might introduce a bit of noise to track it. The 1000+ FPS 4DGS (Yuan et al.) did note a slight reduction in PSNR (&lt;0.2 dB) after aggressive pruning, which is a minor trade-off for the huge speed gain.</p>
</section>
<section id="comparative-summary-table">
<h3>Comparative Summary Table<a class="headerlink" href="#comparative-summary-table" title="Link to this heading"></a></h3>
<table class="docutils align-default" id="id2">
<caption><span class="caption-number">Table 3 </span><span class="caption-text">Qualitative comparison of various methods</span><a class="headerlink" href="#id2" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 12.0%" />
<col style="width: 18.0%" />
<col style="width: 18.0%" />
<col style="width: 15.0%" />
<col style="width: 15.0%" />
<col style="width: 22.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Representation</p></th>
<th class="head"><p>Use of Priors</p></th>
<th class="head"><p>Speed</p></th>
<th class="head"><p>Quality (relative)</p></th>
<th class="head"><p>Notes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>NeRF (Mildenhall’20)</p></td>
<td><p>Implicit (MLP), volumetric</p></td>
<td><p>None (generic)</p></td>
<td><p>Slow (minutes/frame)</p></td>
<td><p>High (photorealistic, but needs many views)</p></td>
<td><p>Heavy per-scene training</p></td>
</tr>
<tr class="row-odd"><td><p>Instant-NGP (2022)</p></td>
<td><p>Implicit (hashed grid)</p></td>
<td><p>None</p></td>
<td><p>Fast training/render (fps-level)</p></td>
<td><p>High (slightly lower on some fine details vs NeRF)</p></td>
<td><p>Large memory use for grid</p></td>
</tr>
<tr class="row-even"><td><p>3DGS (Kerbl’23)</p></td>
<td><p>Explicit Gaussians</p></td>
<td><p>SfM init (structure)</p></td>
<td><p>Real-time (100+ FPS)</p></td>
<td><p>High (≈NeRF quality)</p></td>
<td><p>Struggles at very sharp edges</p></td>
</tr>
<tr class="row-odd"><td><p>Mip-Splatting (2024)</p></td>
<td><p>Explicit Gaussians</p></td>
<td><p>None</p></td>
<td><p>Real-time (slightly slower due to filtering)</p></td>
<td><p>High (sharper, alias-free)</p></td>
<td><p>Solves aliasing with 2D prefilter</p></td>
</tr>
<tr class="row-even"><td><p>BG-Triangle (2025)</p></td>
<td><p>Hybrid Gaussian + surfaces</p></td>
<td><p>None (requires point cloud)</p></td>
<td><p>Interactive (tessellation overhead)</p></td>
<td><p>High (sharp boundaries best)</p></td>
<td><p>Few primitives (compact model)</p></td>
</tr>
<tr class="row-odd"><td><p>KeypointNeRF (2023)</p></td>
<td><p>Implicit (NeRF)</p></td>
<td><p>2D keypoints (pose prior)</p></td>
<td><p>Slow (offline)</p></td>
<td><p>High (good human detail)</p></td>
<td><p>Fails with occlusions, slow inference</p></td>
</tr>
<tr class="row-even"><td><p>EG-HumanNeRF (2024)</p></td>
<td><p>Implicit + mesh guide</p></td>
<td><p>SMPL-X prior (geometry)</p></td>
<td><p>Real-time</p></td>
<td><p>Very High (state of art human)</p></td>
<td><p>Mesh required; volume rendering</p></td>
</tr>
<tr class="row-odd"><td><p>GPS-Gaussian (2024)</p></td>
<td><p>Explicit Gaussians (learned)</p></td>
<td><p>Learned human prior (data)</p></td>
<td><p>Real-time</p></td>
<td><p>Very High (state of art human)</p></td>
<td><p>Needs training dataset; 2-view input</p></td>
</tr>
<tr class="row-even"><td><p>GHG (2024)</p></td>
<td><p>Explicit Gaussians on SMPL</p></td>
<td><p>SMPL-X prior (template)</p></td>
<td><p>Real-time (after inferring Gaussians)</p></td>
<td><p>Very High (state of art human)</p></td>
<td><p>Requires fitted SMPL; generalizes well</p></td>
</tr>
<tr class="row-odd"><td><p>4DGS (Wu’24)</p></td>
<td><p>Explicit 4D Gaussians + MLP</p></td>
<td><p>None (generic motion)</p></td>
<td><p>Real-time (30–80 FPS)</p></td>
<td><p>High (≈ dynamic NeRF)</p></td>
<td><p>One model for whole sequence; editable</p></td>
</tr>
<tr class="row-even"><td><p>4DGS-1K (2025)</p></td>
<td><p>4DGS + pruning</p></td>
<td><p>None</p></td>
<td><p>Ultra real-time (1000 FPS)</p></td>
<td><p>High (≈4DGS quality)</p></td>
<td><p>Minor quality loss for huge speed gain</p></td>
</tr>
<tr class="row-odd"><td><p>MEGA (2024)</p></td>
<td><p>4DGS + compression</p></td>
<td><p>None</p></td>
<td><p>Real-time</p></td>
<td><p>High (≈4DGS quality)</p></td>
<td><p>~100× smaller model size</p></td>
</tr>
</tbody>
</table>
<p>This table illustrates how 3D Gaussian Splatting and its extensions achieve a remarkable balance: real-time performance with excellent visual quality, often matching or surpassing neural field methods that are much slower. By incorporating domain knowledge (like human body models) or by improving the primitive structure (like BG-Triangles), these methods push the envelope in their respective niches.
This table illustrates how 3D Gaussian Splatting and its extensions achieve a remarkable balance: real-time performance with excellent visual quality, often matching or surpassing neural field methods that are much slower. By incorporating domain knowledge (like human body models) or by improving the primitive structure (like BG-Triangles), these methods push the envelope in their respective niches.</p>
</section>
</section>
<section id="datasets-and-resources">
<h2>Datasets and Resources<a class="headerlink" href="#datasets-and-resources" title="Link to this heading"></a></h2>
<p>Several datasets are commonly used for training and evaluating 3D Gaussian Splatting models.</p>
<p>Rich datasets are crucial for training and evaluating 3D Gaussian Splatting models. This section covers the key datasets commonly used in this domain.</p>
<section id="synthetic-nerf-dataset-blender-scenes">
<h3>Synthetic NeRF Dataset (Blender Scenes)<a class="headerlink" href="#synthetic-nerf-dataset-blender-scenes" title="Link to this heading"></a></h3>
<p>The Synthetic NeRF Dataset consists of synthetic 360° scenes rendered in Blender with known camera parameters:</p>
<ul class="simple">
<li><p><strong>Content</strong>: A set of 8 objects (e.g., Lego, Chair, Drums) with realistic materials, rendered from viewpoints primarily on the upper hemisphere, with two full-sphere scenes</p></li>
<li><p><strong>Scale</strong>: Each scene includes approximately 100 rendered images at 800×800 resolution</p></li>
<li><p><strong>Format</strong>: Path-traced images with global illumination and reflections, accompanied by ground-truth camera poses</p></li>
<li><p><strong>Significance</strong>: Creates a controlled environment for novel view synthesis evaluation, with perfect camera calibration and high-quality renders</p></li>
<li><p><strong>Access</strong>: Available through the NeRF project’s Google Drive link, with scripts for downloading included in the official NeRF repository</p></li>
<li><p><strong>Links</strong>:
- NeRF Project Page: <a class="reference external" href="https://www.matthewtancik.com/nerf">https://www.matthewtancik.com/nerf</a>
- GitHub Repository: <a class="reference external" href="https://github.com/bmild/nerf">https://github.com/bmild/nerf</a>
- Original Paper: <a class="reference external" href="https://arxiv.org/abs/2003.08934">https://arxiv.org/abs/2003.08934</a></p></li>
</ul>
<p>This dataset is ideal for validating methods in a controlled setting, as it provides clean, noise-free data with ground truth for both geometry and camera parameters.</p>
</section>
<section id="llff-local-light-field-fusion">
<h3>LLFF (Local Light Field Fusion)<a class="headerlink" href="#llff-local-light-field-fusion" title="Link to this heading"></a></h3>
<p>LLFF consists of forward-facing captures of real-world scenes:</p>
<ul class="simple">
<li><p><strong>Content</strong>: A set of 24 real-world scenes captured with a handheld smartphone, though a subset of 8 scenes (e.g., Fern, Flower, Fortress) is most commonly used</p></li>
<li><p><strong>Scale</strong>: Each scene contains approximately 20-30 images at resolution ~1008×756</p></li>
<li><p><strong>Format</strong>: Forward-facing captures (looking inward toward the scene, covering roughly one side)</p></li>
<li><p><strong>Significance</strong>: Demonstrates that even with sparse, handheld captures, high-quality novel views can be synthesized</p></li>
<li><p><strong>Access</strong>: The most commonly used 8 scenes are available via the NeRF Google Drive, with camera poses estimated using COLMAP</p></li>
<li><p><strong>Links</strong>:
- LLFF Project Page: <a class="reference external" href="https://bmild.github.io/llff">https://bmild.github.io/llff</a>
- GitHub Repository: <a class="reference external" href="https://github.com/Fyusion/LLFF">https://github.com/Fyusion/LLFF</a>
- Used in NeRF Paper: <a class="reference external" href="https://arxiv.org/pdf/2003.08934">https://arxiv.org/pdf/2003.08934</a></p></li>
</ul>
<p>LLFF presents a common benchmark for evaluating view interpolation/extrapolation in a limited forward-facing view band, replicating a realistic capture scenario.</p>
</section>
<section id="tanks-and-temples">
<h3>Tanks and Temples<a class="headerlink" href="#tanks-and-temples" title="Link to this heading"></a></h3>
<p>Tanks and Temples is a high-quality benchmark for large-scale scene reconstruction:</p>
<ul class="simple">
<li><p><strong>Content</strong>: Diverse real scenes captured in indoor and outdoor settings (e.g., a courtyard, a living room with a tank model, a temple structure)</p></li>
<li><p><strong>Scale</strong>: Split into Training (7 scenes), Intermediate (8 scenes: Family, Francis, Horse, Train, etc.), and Advanced (6 scenes: Auditorium, Ballroom, Temple, etc.) sets</p></li>
<li><p><strong>Format</strong>: Each scene is provided as a high-resolution video (4K) and downsampled frames, with ground-truth 3D geometry from industrial laser scanners</p></li>
<li><p><strong>Significance</strong>: The varying complexity of scenes (from small objects to building-scale environments) and precise laser scans create an excellent challenge to test reconstruction fidelity</p></li>
<li><p><strong>Access</strong>: Available from the official Tanks and Temples website, with a Python downloader for grabbing all videos or images</p></li>
<li><p><strong>Links</strong>:
- Tanks and Temples Download Page: <a class="reference external" href="https://www.tanksandtemples.org/download/">https://www.tanksandtemples.org/download/</a></p></li>
</ul>
<p>This dataset is particularly valuable for evaluating how well 3D Gaussian Splatting performs on complex, large-scale real-world environments.</p>
</section>
<section id="multi-object-360-co3d-common-objects-in-3d">
<h3>Multi-Object 360 (CO3D - Common Objects in 3D)<a class="headerlink" href="#multi-object-360-co3d-common-objects-in-3d" title="Link to this heading"></a></h3>
<p>CO3D is a large-scale dataset of real-world object-centric captures:</p>
<ul class="simple">
<li><p><strong>Content</strong>: 50 common object categories (mostly rigid objects from the COCO taxonomy, like cars, chairs, hydrants)</p></li>
<li><p><strong>Scale</strong>: Approximately 19,000 short video sequences (each focusing on a single object), totaling 1.5 million frames</p></li>
<li><p><strong>Format</strong>: Each sequence includes camera poses (extrinsics and intrinsics), a reconstructed point cloud for the object, and foreground mask annotations</p></li>
<li><p><strong>Significance</strong>: Objects are captured with full 360° coverage whenever possible, enabling category-specific 3D reconstruction and novel view synthesis</p></li>
<li><p><strong>Access</strong>: The full dataset (CO3D-v1) is publicly available, with CO3Dv2 (expanded to ~2× sequences with improved quality) also released</p></li>
<li><p><strong>Links</strong>:
- GitHub Repository: <a class="reference external" href="https://github.com/facebookresearch/co3d">https://github.com/facebookresearch/co3d</a>
- Original Paper: <a class="reference external" href="https://arxiv.org/abs/2109.00512">https://arxiv.org/abs/2109.00512</a></p></li>
</ul>
<p>CO3D’s scale and variety make it ideal for training models that can infer 3D shape and appearance of an object from few views or even a single view.</p>
</section>
<section id="amass-archive-of-motion-capture-as-surface-shapes">
<h3>AMASS (Archive of Motion Capture as Surface Shapes)<a class="headerlink" href="#amass-archive-of-motion-capture-as-surface-shapes" title="Link to this heading"></a></h3>
<p>AMASS unifies motion capture data into a common representation:</p>
<ul class="simple">
<li><p><strong>Content</strong>: Aggregates 15 existing mocap datasets (e.g., CMU Mocap, HumanEva, SFU)</p></li>
<li><p><strong>Scale</strong>: Over 40 hours of motion data (about 11,000 motion sequences from 344 subjects)</p></li>
<li><p><strong>Format</strong>: Each sequence is a time-series of poses in SMPL parameter space (body pose parameters, shape, and translational root motion)</p></li>
<li><p><strong>Significance</strong>: Turns diverse mocap data into a single dataset of parametrized human 3D surface motions, enabling data-driven research on human body dynamics</p></li>
<li><p><strong>Access</strong>: Free for research but requires agreeing to a license; users must register on the AMASS site</p></li>
<li><p><strong>Links</strong>:
- AMASS Project Page: <a class="reference external" href="https://amass.is.tue.mpg.de/">https://amass.is.tue.mpg.de/</a>
- MoSh++ GitHub Repository: <a class="reference external" href="https://github.com/nghorbani/moshpp">https://github.com/nghorbani/moshpp</a></p></li>
</ul>
<p>While primarily used for human motion modeling, AMASS is valuable for creating dynamic scenes to test 3D Gaussian Splatting of moving subjects.</p>
</section>
<section id="cape-clothed-auto-person-encoding">
<h3>CAPE (Clothed Auto Person Encoding)<a class="headerlink" href="#cape-clothed-auto-person-encoding" title="Link to this heading"></a></h3>
<p>CAPE provides dynamic clothed human meshes with corresponding body shapes:</p>
<ul class="simple">
<li><p><strong>Content</strong>: 15 subjects (10 male, 5 female) each performing a variety of movements while wearing different outfits</p></li>
<li><p><strong>Scale</strong>: 600+ sequences, with over 140,000 frames of 3D scans</p></li>
<li><p><strong>Format</strong>: Each frame is a 3D mesh of a person in a specific pose and clothing configuration, registered to the standard SMPL body mesh (6,890 vertices)</p></li>
<li><p><strong>Significance</strong>: For every frame, CAPE provides both the clothed mesh and the underlying nude body mesh (in the same pose), plus SMPL pose parameters</p></li>
<li><p><strong>Access</strong>: Available upon request via the CAPE site</p></li>
<li><p><strong>Links</strong>:
- CAPE Dataset Page: <a class="reference external" href="https://cape.is.tue.mpg.de/dataset.html">https://cape.is.tue.mpg.de/dataset.html</a>
- GitHub Utilities: <a class="reference external" href="https://github.com/qianlim/cape_utils">https://github.com/qianlim/cape_utils</a></p></li>
</ul>
<p>CAPE is particularly useful for learning models of clothing deformation on bodies and for training networks to infer body shape under clothing.</p>
</section>
<section id="thuman-thuman2-0">
<h3>THuman / THuman2.0<a class="headerlink" href="#thuman-thuman2-0" title="Link to this heading"></a></h3>
<p>THuman provides high-fidelity 3D human scans with realistic textures:</p>
<ul class="simple">
<li><p><strong>Content</strong>: THuman v1 (2019) contains about 6,000 scans of human subjects in various poses. THuman2.0 (2020) includes 500 distinct scans of humans with high-resolution geometry</p></li>
<li><p><strong>Scale</strong>: THuman2.0 offers 500 scans with 8K textures and detailed geometry (~300K faces per mesh)</p></li>
<li><p><strong>Format</strong>: Each scan in THuman2.0 is an .obj mesh with a photorealistic texture map and fitted SMPL-X body parameters</p></li>
<li><p><strong>Significance</strong>: Used for digital human modeling, 3D avatar creation, and human reconstruction tasks</p></li>
<li><p><strong>Access</strong>: Available for research upon request through the THuman project page</p></li>
<li><p><strong>Links</strong>:
- THuman2.0 GitHub: <a class="reference external" href="https://github.com/ytrock/THuman2.0-Dataset">https://github.com/ytrock/THuman2.0-Dataset</a>
- Project Page: <a class="reference external" href="https://liuyebin.com/dataset.html">https://liuyebin.com/dataset.html</a></p></li>
</ul>
<p>The quality of THuman2.0 (clean topology, detailed textures, varied poses) makes it valuable for creating realistic human renderings and for AR/VR content.</p>
</section>
<section id="renderpeople">
<h3>RenderPeople<a class="headerlink" href="#renderpeople" title="Link to this heading"></a></h3>
<p>RenderPeople is a commercial library of photorealistic 3D human models:</p>
<ul class="simple">
<li><p><strong>Content</strong>: Over 4,500 scanned people covering various ages, ethnicities, clothing, and activities</p></li>
<li><p><strong>Scale</strong>: Each model includes high-resolution meshes with tens of thousands of polygons and 8K textures</p></li>
<li><p><strong>Format</strong>: Models come in three categories: Posed People (static scans), Rigged People (with skeletal armatures), and Animated People (with pre-made motion capture animations)</p></li>
<li><p><strong>Significance</strong>: The high realism and diversity make these models useful for generating synthetic data for training AI systems</p></li>
<li><p><strong>Access</strong>: Commercial dataset with individual models purchasable from the website; free samples are available for testing</p></li>
<li><p><strong>Links</strong>:
- Free 3D People Samples: <a class="reference external" href="https://renderpeople.com/free-3d-people/">https://renderpeople.com/free-3d-people/</a></p></li>
</ul>
<p>While commercial, RenderPeople has been used in research for data augmentation and as ground truth for human reconstruction algorithms.</p>
</section>
<section id="open-source-implementations">
<h3>Open-Source Implementations<a class="headerlink" href="#open-source-implementations" title="Link to this heading"></a></h3>
<p>Several open-source implementations for working with these datasets and 3D Gaussian Splatting include:</p>
<ol class="arabic simple">
<li><p>The official 3D Gaussian Splatting implementation from the original authors (Kerbl et al.): <a class="reference external" href="https://github.com/graphdeco-inria/gaussian-splatting">https://github.com/graphdeco-inria/gaussian-splatting</a></p></li>
<li><p>Community implementations with various extensions and optimizations</p></li>
<li><p>Integration with frameworks like PyTorch3D for broader research applications</p></li>
<li><p>Dataset loaders and utilities specific to each benchmark</p></li>
</ol>
<p>These resources make it easy to get started with 3D Gaussian Splatting for research or applications.</p>
</section>
</section>
<section id="future-directions">
<h2>Future Directions<a class="headerlink" href="#future-directions" title="Link to this heading"></a></h2>
<p>Research in 3D Gaussian Splatting is rapidly evolving. While initial works have demonstrated the core capability of representing scenes with explicit 3D Gaussians at high quality and speed, there remain many open challenges and opportunities. This section outlines several promising future research directions, each addressing current limitations or enabling new applications.</p>
<section id="improved-compression-techniques-for-memory-efficiency">
<h3>Improved Compression Techniques for Memory Efficiency<a class="headerlink" href="#improved-compression-techniques-for-memory-efficiency" title="Link to this heading"></a></h3>
<p>As 3D Gaussian splat representations grow in number of primitives (often millions of Gaussians for a scene), memory and storage efficiency becomes a critical issue. Several compression approaches are being explored:</p>
<ul class="simple">
<li><p><strong>Redundancy Reduction</strong>: Pruning redundant Gaussians and quantizing parameters – recent work on Temporally Compressed 3D Gaussian Splatting showed that by selectively removing less important splats over time and using mixed-precision encoding, one can achieve up to 67× compression with minimal quality loss.</p></li>
<li><p><strong>Hierarchical Encoding</strong>: Organizing Gaussians into multi-resolution clusters or an octree, so that distant or small-detail splats can be stored at lower precision or omitted until needed.</p></li>
<li><p><strong>Attribute Compression</strong>: Compressing color and opacity attributes (e.g., using PCA or learned codebooks) to drastically cut down memory usage.</p></li>
</ul>
<p>The goal is to enable lighter-weight models that can be transmitted and loaded efficiently, which is especially important for mobile or web applications. Moving forward, we expect techniques like on-the-fly streaming of Gaussians, delta encoding between frames (for dynamics), and better quantization of Gaussian parameters to make 3D Gaussian Splatting more memory-friendly.</p>
</section>
<section id="handling-dynamic-and-deformable-scenes">
<h3>Handling Dynamic and Deformable Scenes<a class="headerlink" href="#handling-dynamic-and-deformable-scenes" title="Link to this heading"></a></h3>
<p>While 3D Gaussian Splatting has excelled in static scenes, a frontier is dynamic or deformable scenes – where content moves, articulates, or changes over time. Extending Gaussian splats to the time domain raises questions of how to update the Gaussians’ parameters frame by frame or how to represent spacetime consistently.</p>
<p>Initial research has explored two main paradigms:</p>
<ol class="arabic simple">
<li><p><strong>Per-frame Gaussians with temporal correspondence</strong>: Allowing each Gaussian to move or appear/disappear over time</p></li>
<li><p><strong>Spatio-temporal Gaussians</strong>: Where each Gaussian represents a trajectory or a 4D volume in space-time</p></li>
</ol>
<p>Recent works have begun to tackle these challenges:</p>
<ul class="simple">
<li><p>Methods that introduce sparse control points to drive Gaussian deformation so that a smaller set of key points control clusters of Gaussians, effectively encoding motion fields with fewer parameters</p></li>
<li><p>Strategies that enforce consistency constraints, keeping Gaussian attributes coherent across frames</p></li>
<li><p>Combinations of skeletal motion models (like SMPL for human bodies) with Gaussians attached to the moving parts</p></li>
</ul>
<p>Future research will likely develop hybrid models where an underlying motion field (possibly learned with an MLP) warps a static Gaussian configuration through time, enabling 4D Gaussian Splatting that can handle complex motions.</p>
</section>
<section id="advanced-material-modeling-for-realistic-rendering">
<h3>Advanced Material Modeling for Realistic Rendering<a class="headerlink" href="#advanced-material-modeling-for-realistic-rendering" title="Link to this heading"></a></h3>
<p>Originally, Gaussian splats use a simple emissive model (each Gaussian stores color and opacity, possibly with view-dependent spherical harmonics for slight view effects). This is efficient, but highly reflective or refractive materials are not handled accurately by the basic model.</p>
<p>Recent research directions include:</p>
<ul class="simple">
<li><p><strong>Advanced Shading Models</strong>: Adding shading functions to Gaussians to capture specular highlights and view-dependent reflections</p></li>
<li><p><strong>Normal Estimation</strong>: Estimating a normal vector for each Gaussian (from the covariance shape or adjacent geometry) and then using a microfacet BRDF to modulate the color</p></li>
<li><p><strong>Neural Reflectance</strong>: Using small neural networks to predict reflectance given view angle and a Gaussian’s properties</p></li>
</ul>
<p>Another aspect is global illumination – handling shadows or interreflections between Gaussians. Future work might integrate techniques from point-based global illumination, assigning each Gaussian material properties and computing lighting with environment maps or real-time approximations.</p>
<p>A promising route is deferred rendering with Gaussians, where a first pass splats geometry and basic info, and a second pass computes lighting in image space. Adding richer material representation to 3D Gaussian Splatting will push its visual fidelity closer to traditional rendering, allowing scenes with mirrors, shiny cars, or translucent objects to be rendered correctly.</p>
</section>
<section id="hybrid-approaches-integrating-neural-fields-and-explicit-representations">
<h3>Hybrid Approaches Integrating Neural Fields and Explicit Representations<a class="headerlink" href="#hybrid-approaches-integrating-neural-fields-and-explicit-representations" title="Link to this heading"></a></h3>
<p>A compelling future direction is to combine the benefits of explicit Gaussians with implicit neural networks, forming a hybrid representation. Such approaches could:</p>
<ul class="simple">
<li><p>Use a coarse neural field to guide or condition the placement of Gaussians</p></li>
<li><p>Use Gaussians as a fixed backbone with a small neural network refining details</p></li>
<li><p>Employ a neural feature field (e.g., a tri-plane or voxel grid) alongside Gaussians to encode high-frequency details</p></li>
</ul>
<p>Another approach is progressive implicit-explicit modeling: start with an implicit coarse model of the scene and then spawn Gaussians in areas where fine detail is needed (edges, high texture regions), using the network to fill in what points alone miss.</p>
<p>There is evidence that hybrid models can yield better quality-speed trade-offs – e.g., a recent method combined a fast explicit point model with a small learned component to improve generalization. This direction is also promising for generalization across scenes: a neural field (with learned scene-agnostic features) could help Gaussians represent new scenes without per-scene optimization.</p>
</section>
<section id="scalability-for-large-scale-scenes-city-level-and-beyond">
<h3>Scalability for Large-Scale Scenes (City-Level and Beyond)<a class="headerlink" href="#scalability-for-large-scale-scenes-city-level-and-beyond" title="Link to this heading"></a></h3>
<p>Scaling 3D Gaussian Splatting to very large scenes (e.g., an entire city block or a forest) presents challenges in both representation size and rendering efficiency. Future research is focusing on methods to handle unbounded environments and vast numbers of Gaussians gracefully.</p>
<p>Key approaches include:</p>
<ul class="simple">
<li><p><strong>Level-of-Detail (LOD)</strong>: A city-level scene can be divided into sectors or an octree grid; distant sectors are represented with fewer/lower-resolution Gaussians, while near sectors use the full detail</p></li>
<li><p><strong>Streaming</strong>: As the camera moves, Gaussians could be streamed in and out of memory for the regions entering or leaving the view</p></li>
<li><p><strong>Divide-and-Conquer Training</strong>: Methods like CityGaussian employ a divide-and-conquer training approach and multi-LOD representation to reconstruct an urban scene</p></li>
<li><p><strong>Optimized Rendering</strong>: Techniques like frustum culling for Gaussians, clustering splats into tiles, and GPU acceleration through compute shaders can keep frame rates high</p></li>
<li><p><strong>Foveated Rendering</strong>: Where the density of splats is higher in the viewer’s focus area and lower in the periphery</p></li>
</ul>
<p>Combining 3D Gaussian Splatting with mapping systems (like converting city GIS data to Gaussians, or starting from photogrammetry models and then optimizing Gaussians) could also be explored.</p>
</section>
<section id="real-time-applications-in-ar-vr-and-gaming">
<h3>Real-Time Applications in AR/VR and Gaming<a class="headerlink" href="#real-time-applications-in-ar-vr-and-gaming" title="Link to this heading"></a></h3>
<p>One of the most exciting directions is pushing 3D Gaussian Splatting toward real-time performance for interactive applications such as augmented reality, virtual reality, and video games.</p>
<p>This involves optimizing both the rendering pipeline and the scene representation:</p>
<ul class="simple">
<li><p><strong>Hardware Acceleration</strong>: Exploiting dedicated GPU features – for example, using point cloud rendering pipelines or even hardware ray tracing cores to accelerate splat rasterization</p></li>
<li><p><strong>Web and Mobile Implementation</strong>: Porting Gaussian Splatting to run in the browser or on mobile GPUs (WebGL/WebGPU implementations)</p></li>
<li><p><strong>Low-Latency Updates</strong>: Finding fast methods to update Gaussians in response to tracking data or user input, potentially by training adaptive networks that refine splats on the fly</p></li>
</ul>
<p>In gaming, 3D Gaussian Splatting could be used to quickly capture and insert real-world scenes into game engines, or to render large crowds/vegetation with less performance cost than detailed polygonal models.</p>
<p>Over the next few years, we anticipate research that tightens the feedback loop between splat-based representations and interactive systems, enabling smooth, real-time experiences where users can move through photorealistic environments or interact with neural objects without precomputation.</p>
</section>
<section id="integration-with-existing-graphics-pipelines">
<h3>Integration with Existing Graphics Pipelines<a class="headerlink" href="#integration-with-existing-graphics-pipelines" title="Link to this heading"></a></h3>
<p>Another area of future research is better integration of Gaussian Splatting with traditional graphics pipelines:</p>
<ul class="simple">
<li><p><strong>Engine Integration</strong>: Incorporating Gaussian Splatting into game engines like Unity and Unreal Engine</p></li>
<li><p><strong>Hybrid Rendering</strong>: Combining traditional rasterization of hard surfaces with Gaussian Splatting for complex phenomena like hair, vegetation, or smoke</p></li>
<li><p><strong>Editing Tools</strong>: Developing intuitive interfaces for editing and manipulating Gaussian-based scenes</p></li>
</ul>
<p>This integration would allow developers to leverage the benefits of Gaussian Splatting while still using familiar tools and workflows.</p>
</section>
<section id="learning-from-limited-data">
<h3>Learning from Limited Data<a class="headerlink" href="#learning-from-limited-data" title="Link to this heading"></a></h3>
<p>Current 3D Gaussian Splatting methods typically require multiple images of a scene from different viewpoints. Future research could focus on:</p>
<ul class="simple">
<li><p><strong>Few-Shot Learning</strong>: Reconstructing a scene from just a few images or even a single image</p></li>
<li><p><strong>Cross-Scene Priors</strong>: Leveraging knowledge from previously seen scenes to better reconstruct new scenes</p></li>
<li><p><strong>Text-to-3D</strong>: Using text descriptions to guide the generation or refinement of Gaussian-based scenes</p></li>
</ul>
<p>These advances would make the technology more accessible and usable in scenarios where capturing multiple views is impractical.</p>
<p>In summary, while 3D Gaussian Splatting has already made significant strides in real-time, high-quality novel view synthesis, these future directions promise to expand its capabilities, efficiency, and applicability across numerous domains.</p>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading"></a></h2>
<p>3D Gaussian Splatting represents a paradigm shift in novel view synthesis, successfully bridging the gap between neural rendering quality and traditional graphics efficiency. By leveraging explicit 3D Gaussian primitives that can be efficiently rendered through a specialized pipeline, this approach has fundamentally transformed how we represent and render 3D scenes.</p>
<p>The key contributions of 3D Gaussian Splatting can be summarized as follows:</p>
<ul class="simple">
<li><p><strong>Unprecedented Performance-Quality Balance</strong>: Achieving real-time rendering speeds (30-135 FPS at 1080p resolution) while maintaining visual quality comparable to or exceeding state-of-the-art neural radiance fields</p></li>
<li><p><strong>Accelerated Training</strong>: Reducing optimization time from hours or days to just minutes, enabling rapid scene reconstruction and iteration</p></li>
<li><p><strong>Mathematically Elegant Framework</strong>: Providing a unified approach that combines the volumetric integration of NeRF with the efficiency of point-based rendering through a differentiable, analytically sound formulation</p></li>
<li><p><strong>Explicit Representation</strong>: Making scenes directly editable and manipulable, unlike the “black box” nature of neural network approaches</p></li>
<li><p><strong>Adaptive Detail</strong>: Automatically adjusting the density and distribution of primitives to match scene complexity through its adaptive optimization algorithm</p></li>
</ul>
<p>The impact of 3D Gaussian Splatting extends far beyond technical improvements. It has profound implications for multiple domains:</p>
<p><strong>In Computer Vision and Graphics Research</strong>:
Gaussian Splatting has opened new research directions at the intersection of neural and traditional rendering, demonstrating that explicit representations can match or exceed neural networks for certain tasks. This challenges the previous assumption that implicit neural representations were necessary for high-quality novel view synthesis.</p>
<p><strong>In Content Creation and Media</strong>:
The ability to quickly reconstruct a photorealistic, editable 3D scene from a set of images has transformative potential for film production, game development, and virtual production. Artists and designers can now capture real-world environments and rapidly integrate them into creative workflows.</p>
<p><strong>In Immersive Technologies</strong>:
For virtual and augmented reality, Gaussian Splatting’s real-time performance enables truly photorealistic virtual environments that can run on consumer hardware, potentially solving the long-standing challenge of achieving both realism and responsiveness in immersive experiences.</p>
<p><strong>In Practical Applications</strong>:
Fields such as cultural heritage preservation, architectural visualization, e-commerce, and remote collaboration can leverage this technology to create more accurate, accessible, and useful digital replicas of real-world objects and environments.</p>
<p>The development of 3D Gaussian Splatting also illustrates a broader trend in computer graphics and vision: the convergence of neural and traditional approaches. Rather than neural networks replacing traditional graphics techniques, we’re seeing the emergence of hybrid methods that combine the best aspects of both paradigms. Gaussian Splatting exemplifies this fusion, taking inspiration from classic point-based rendering while incorporating modern optimization techniques and the volumetric rendering framework from neural fields.</p>
<p>Looking ahead, as outlined in the Future Directions section, we can expect rapid advancements in addressing current limitations such as memory efficiency, material modeling, and dynamic scene handling. The ongoing research will likely lead to even more powerful representations that can handle increasingly complex scenarios while maintaining real-time performance.</p>
<p>In conclusion, 3D Gaussian Splatting marks a critical milestone in the evolution of 3D scene representation and rendering. By striking an optimal balance between quality, speed, and controllability, it has not only advanced the state of the art in novel view synthesis but has also made photorealistic neural rendering practical for real-world applications. As the technology continues to mature and combine with other approaches, we can expect it to become a fundamental component in the toolkit of computer vision researchers, graphics engineers, and content creators across multiple industries.</p>
</section>
<section id="glossary">
<h2>Glossary<a class="headerlink" href="#glossary" title="Link to this heading"></a></h2>
<dl class="simple glossary">
<dt id="term-Alpha-Blending">Alpha Blending<a class="headerlink" href="#term-Alpha-Blending" title="Link to this term"></a></dt><dd><p>The process of combining a foreground color with a background color based on the foreground’s alpha (opacity) value.</p>
</dd>
<dt id="term-Alpha-Compositing">Alpha Compositing<a class="headerlink" href="#term-Alpha-Compositing" title="Link to this term"></a></dt><dd><p>Combining multiple semi-transparent layers to form a final image, typically using the “over” operator.</p>
</dd>
<dt id="term-Anisotropic-Gaussian">Anisotropic Gaussian<a class="headerlink" href="#term-Anisotropic-Gaussian" title="Link to this term"></a></dt><dd><p>A Gaussian distribution with different variances along different axes, forming an ellipsoid rather than a sphere.</p>
</dd>
<dt id="term-Covariance-Matrix">Covariance Matrix<a class="headerlink" href="#term-Covariance-Matrix" title="Link to this term"></a></dt><dd><p>A symmetric positive semi-definite matrix that defines the shape, size, and orientation of a Gaussian distribution.</p>
</dd>
<dt id="term-Densification">Densification<a class="headerlink" href="#term-Densification" title="Link to this term"></a></dt><dd><p>The process of adding new Gaussians to improve scene representation in areas with high reconstruction error.</p>
</dd>
<dt id="term-Differentiable-Rendering">Differentiable Rendering<a class="headerlink" href="#term-Differentiable-Rendering" title="Link to this term"></a></dt><dd><p>A rendering process where gradients of the output image with respect to scene parameters can be computed, enabling optimization.</p>
</dd>
<dt id="term-EWA-Elliptical-Weighted-Average">EWA (Elliptical Weighted Average)<a class="headerlink" href="#term-EWA-Elliptical-Weighted-Average" title="Link to this term"></a></dt><dd><p>A technique for high-quality filtering when projecting 3D elements to 2D, used in point-based rendering to avoid aliasing.</p>
</dd>
<dt id="term-Implicit-Representation">Implicit Representation<a class="headerlink" href="#term-Implicit-Representation" title="Link to this term"></a></dt><dd><p>A scene representation that defines geometry or appearance as a continuous function, typically implemented with a neural network.</p>
</dd>
<dt id="term-Jacobian">Jacobian<a class="headerlink" href="#term-Jacobian" title="Link to this term"></a></dt><dd><p>A matrix of partial derivatives representing the local linear approximation of a function (in this context, the projection from 3D to 2D).</p>
</dd>
<dt id="term-L1-Loss">L1 Loss<a class="headerlink" href="#term-L1-Loss" title="Link to this term"></a></dt><dd><p>A loss function that measures the absolute difference between predicted and target values, often used in image comparison.</p>
</dd>
<dt id="term-Light-Field">Light Field<a class="headerlink" href="#term-Light-Field" title="Link to this term"></a></dt><dd><p>A function that describes the amount of light flowing in every direction through every point in space.</p>
</dd>
<dt id="term-Linear-Blend-Skinning-LBS">Linear Blend Skinning (LBS)<a class="headerlink" href="#term-Linear-Blend-Skinning-LBS" title="Link to this term"></a></dt><dd><p>A technique for deforming a mesh according to an underlying skeleton, where each vertex is influenced by multiple joints.</p>
</dd>
<dt id="term-Multi-View-Stereo-MVS">Multi-View Stereo (MVS)<a class="headerlink" href="#term-Multi-View-Stereo-MVS" title="Link to this term"></a></dt><dd><p>A technique to reconstruct dense 3D geometry from multiple images with known camera positions.</p>
</dd>
<dt id="term-NeRF-Neural-Radiance-Field">NeRF (Neural Radiance Field)<a class="headerlink" href="#term-NeRF-Neural-Radiance-Field" title="Link to this term"></a></dt><dd><p>A method that uses a neural network to represent a scene as a continuous function mapping 3D coordinates and viewing directions to color and density.</p>
</dd>
<dt id="term-Novel-View-Synthesis">Novel View Synthesis<a class="headerlink" href="#term-Novel-View-Synthesis" title="Link to this term"></a></dt><dd><p>The task of generating new images of a scene from viewpoints that were not part of the original capture.</p>
</dd>
<dt id="term-Parametric-Model">Parametric Model<a class="headerlink" href="#term-Parametric-Model" title="Link to this term"></a></dt><dd><p>A 3D model controlled by a small set of parameters, such as SMPL for human bodies.</p>
</dd>
<dt id="term-Point-Cloud">Point Cloud<a class="headerlink" href="#term-Point-Cloud" title="Link to this term"></a></dt><dd><p>A set of data points in 3D space, typically representing the external surface of an object.</p>
</dd>
<dt id="term-Pruning">Pruning<a class="headerlink" href="#term-Pruning" title="Link to this term"></a></dt><dd><p>The process of removing Gaussians with negligible contribution to improve efficiency.</p>
</dd>
<dt id="term-Quaternion">Quaternion<a class="headerlink" href="#term-Quaternion" title="Link to this term"></a></dt><dd><p>A four-dimensional extension of complex numbers, often used to represent 3D rotations without gimbal lock.</p>
</dd>
<dt id="term-Radiance-Field">Radiance Field<a class="headerlink" href="#term-Radiance-Field" title="Link to this term"></a></dt><dd><p>A function that defines the color and density at every point in 3D space, potentially varying with viewing direction.</p>
</dd>
<dt id="term-Rasterization">Rasterization<a class="headerlink" href="#term-Rasterization" title="Link to this term"></a></dt><dd><p>The process of converting vector graphics (like triangles or points) into a raster image (pixels).</p>
</dd>
<dt id="term-Ray-Marching">Ray Marching<a class="headerlink" href="#term-Ray-Marching" title="Link to this term"></a></dt><dd><p>A technique used in volume rendering where rays are sampled at discrete steps to accumulate color and opacity.</p>
</dd>
<dt id="term-Ray-Tracing">Ray Tracing<a class="headerlink" href="#term-Ray-Tracing" title="Link to this term"></a></dt><dd><p>A rendering technique that simulates the physical behavior of light by tracing rays from the camera through the scene.</p>
</dd>
<dt id="term-SMPL-Skinned-Multi-Person-Linear-Model">SMPL (Skinned Multi-Person Linear Model)<a class="headerlink" href="#term-SMPL-Skinned-Multi-Person-Linear-Model" title="Link to this term"></a></dt><dd><p>A parametric model of the human body that can be controlled with a small number of shape and pose parameters.</p>
</dd>
<dt id="term-Spherical-Harmonics">Spherical Harmonics<a class="headerlink" href="#term-Spherical-Harmonics" title="Link to this term"></a></dt><dd><p>A set of functions that form an orthogonal basis for representing functions on a sphere, used to encode view-dependent appearance.</p>
</dd>
<dt id="term-Splatting">Splatting<a class="headerlink" href="#term-Splatting" title="Link to this term"></a></dt><dd><p>A rendering technique where each element (point, Gaussian, etc.) is projected onto the image plane as a small disk or ellipse.</p>
</dd>
<dt id="term-SSIM-Structural-Similarity-Index-Measure">SSIM (Structural Similarity Index Measure)<a class="headerlink" href="#term-SSIM-Structural-Similarity-Index-Measure" title="Link to this term"></a></dt><dd><p>A perceptual metric that quantifies image quality degradation based on structural information, luminance, and contrast.</p>
</dd>
<dt id="term-Structure-from-Motion-SfM">Structure-from-Motion (SfM)<a class="headerlink" href="#term-Structure-from-Motion-SfM" title="Link to this term"></a></dt><dd><p>A technique to estimate 3D structures and camera motion from a sequence of 2D images.</p>
</dd>
<dt id="term-Tile-Based-Rendering">Tile-Based Rendering<a class="headerlink" href="#term-Tile-Based-Rendering" title="Link to this term"></a></dt><dd><p>A rendering approach that divides the screen into tiles and processes each tile independently to improve efficiency.</p>
</dd>
<dt id="term-Transmittance">Transmittance<a class="headerlink" href="#term-Transmittance" title="Link to this term"></a></dt><dd><p>The fraction of light that passes through a medium without being absorbed or scattered, used in volumetric rendering.</p>
</dd>
<dt id="term-Volume-Rendering">Volume Rendering<a class="headerlink" href="#term-Volume-Rendering" title="Link to this term"></a></dt><dd><p>A technique for rendering a 2D projection of a 3D discretely sampled dataset, typically by ray marching.</p>
</dd>
<dt id="term-Voxel-Grid">Voxel Grid<a class="headerlink" href="#term-Voxel-Grid" title="Link to this term"></a></dt><dd><p>A 3D grid of volumetric elements (voxels) that discretizes a 3D space.</p>
</dd>
</dl>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="extended_materials_neural_radiance_fields.html" class="btn btn-neutral float-left" title="Neural Radiance Fields: A Historical and Theoretical Overview" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="references.html" class="btn btn-neutral float-right" title="References" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>