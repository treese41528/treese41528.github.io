

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Neural Radiance Fields: A Historical and Theoretical Overview &mdash; Fitting SMPL to IMU Optimization</title>
      <link rel="stylesheet" type="text/css" href="/VirtualHumans/_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="/VirtualHumans/_static/css/theme.css?v=e59714d7" />

  
    <link rel="canonical" href="https://treese41528.github.io/VirtualHumans/extended_materials_neural_radiance_fields.html" />
      <script src="/VirtualHumans/_static/jquery.js?v=5d32c60e"></script>
      <script src="/VirtualHumans/_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="/VirtualHumans/_static/documentation_options.js?v=f2a433a1"></script>
      <script src="/VirtualHumans/_static/doctools.js?v=9bcbadda"></script>
      <script src="/VirtualHumans/_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="/VirtualHumans/_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="3D Gaussian Splatting: A Basic Introduction" href="extendeed_materials_gaussian_splatting.html" />
    <link rel="prev" title="Lecture 07.2: Fitting SMPL to IMU Data Using Learning-Based Methods" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Virtual Humans Lecture
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="lecture_01_1_historical_body_models.html">Lecture 01.1 – Historical Body Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#early-origins-simplified-primitives-and-kinematic-skeletons-1970s1980s">Early Origins: Simplified Primitives and Kinematic Skeletons (1970s–1980s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#advances-in-the-1990s-superquadrics-differentiable-fitting-and-physical-models">Advances in the 1990s: Superquadrics, Differentiable Fitting, and Physical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#the-impact-of-3d-scanning-and-data-from-anthropometry-to-statistical-models-1990s2000s">The Impact of 3D Scanning and Data: From Anthropometry to Statistical Models (1990s–2000s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#scape-and-the-emergence-of-pose-aware-models-mid-2000s">SCAPE and the Emergence of Pose-Aware Models (Mid-2000s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#consolidation-in-the-2010s-smpl-and-integration-with-learning-based-methods">Consolidation in the 2010s: SMPL and Integration with Learning-Based Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#deep-learning-and-neural-implicit-models-late-2010spresent">Deep Learning and Neural Implicit Models (Late 2010s–Present)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#timeline-summary-of-milestones">Timeline Summary of Milestones</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html">Lecture 01.2 – Introduction to Human Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#historical-context-of-human-body-modeling">1. Historical Context of Human Body Modeling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#early-developments">Early Developments</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#mid-20th-century-approaches">Mid-20th Century Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#data-driven-revolution-1990s-2000s">Data-Driven Revolution (1990s-2000s)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#mathematical-foundations-of-human-body-models">2. Mathematical Foundations of Human Body Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#the-smpl-model">The SMPL Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#pca-based-statistical-shape-modeling">PCA-Based Statistical Shape Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#kinematic-modeling">Kinematic Modeling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#applications-of-human-body-models">3. Applications of Human Body Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computer-animation-and-visual-effects">Computer Animation and Visual Effects</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#virtual-humans-and-avatars">Virtual Humans and Avatars</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#biomechanics-and-ergonomics">Biomechanics and Ergonomics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#human-computer-interaction-hci">Human-Computer Interaction (HCI)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computer-vision-and-ai">Computer Vision and AI</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#education-and-training">Education and Training</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#challenges-and-future-directions">4. Challenges and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computational-efficiency">Computational Efficiency</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#accuracy-and-detail">Accuracy and Detail</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#generalization">Generalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#clothing-and-accessories">Clothing and Accessories</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#emerging-approaches">Emerging Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html">Lecture 01.3 – Introduction to Human Models (Overview)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#historical-context">1. Historical Context</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#early-scientific-studies">Early Scientific Studies</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#mid-20th-century-to-digital-era">Mid-20th Century to Digital Era</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#st-century-advances">21st Century Advances</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#mathematical-foundations">2. Mathematical Foundations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#parametric-body-models">Parametric Body Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-surface-representations">Implicit Surface Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#kinematic-modeling">Kinematic Modeling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#image-formation-and-rendering">3. Image Formation and Rendering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#camera-models">Camera Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#shading-and-visibility">Shading and Visibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#differentiable-rendering">Differentiable Rendering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#surface-representation-methods">4. Surface Representation Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#explicit-mesh-models">Explicit Mesh Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-function-models">Implicit Function Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#motion-capture-and-behavior-synthesis">5. Motion Capture and Behavior Synthesis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#capturing-human-motion">Capturing Human Motion</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#behavior-synthesis">Behavior Synthesis</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#clothing-modeling">6. Clothing Modeling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physically-based-simulation">Physically-Based Simulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#data-driven-approaches">Data-Driven Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-clothing-models">Implicit Clothing Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#human-object-interaction">7. Human-Object Interaction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physics-based-methods">Physics-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#learning-based-approaches">Learning-Based Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#hybrid-systems">Hybrid Systems</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#applications">8. Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#entertainment-and-media">Entertainment and Media</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#healthcare-and-biomechanics">Healthcare and Biomechanics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#engineering-and-design">Engineering and Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#human-computer-interaction">Human-Computer Interaction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#scientific-research">Scientific Research</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#challenges-and-future-directions">9. Challenges and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#scalability-and-generalization">Scalability and Generalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#higher-fidelity-dynamics">Higher-Fidelity Dynamics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#data-and-labeling-constraints">Data and Labeling Constraints</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physics-and-learning-integration">Physics and Learning Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#semantic-and-cognitive-aspects">Semantic and Cognitive Aspects</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#realism-vs-controllability">Realism vs. Controllability</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_02_1_image_formation.html">Lecture 02.1 – Image Formation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#historical-developments-in-image-formation">1. Historical Developments in Image Formation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#ancient-and-medieval-optics-camera-obscura">Ancient and Medieval Optics – Camera Obscura</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#renaissance-perspective-and-geometry">Renaissance Perspective and Geometry</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#early-cameras-and-photographic-imaging">Early Cameras and Photographic Imaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#modern-developments">Modern Developments</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#the-pinhole-camera-model">2. The Pinhole Camera Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#coordinate-setup">Coordinate Setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#proof-by-similar-triangles">Proof by Similar Triangles</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#numerical-example">Numerical Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#inadequacy-of-a-simple-pinhole">Inadequacy of a Simple Pinhole</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#camera-intrinsics-and-the-projection-matrix">3. Camera Intrinsics and the Projection Matrix</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#extrinsic-parameters">Extrinsic Parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#full-projection-example">Full Projection Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#image-distortions-correction">4. Image Distortions &amp; Correction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#properties-of-perspective-projection">5. Properties of Perspective Projection</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#advanced-theoretical-extensions">6. Advanced Theoretical Extensions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#light-field-imaging-and-plenoptic-cameras">Light Field Imaging and Plenoptic Cameras</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#non-conventional-imaging-techniques">Non-Conventional Imaging Techniques</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#applications-in-modern-vision-and-graphics">7. Applications in Modern Vision and Graphics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#computer-vision-and-3d-reconstruction">Computer Vision and 3D Reconstruction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#medical-imaging">Medical Imaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#photorealistic-rendering-in-computer-graphics">Photorealistic Rendering in Computer Graphics</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#python-example-simulating-image-formation">8. Python Example: Simulating Image Formation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html">Lecture 02.2 – Rotations and Kinematic Chains</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#representations-of-3d-rotations">1. Representations of 3D Rotations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#a-rotation-matrices">A) Rotation Matrices</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#b-euler-angles">B) Euler Angles</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#c-quaternions">C) Quaternions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#lie-algebra-so-3-and-exponential-map">2. Lie Algebra <span class="math notranslate nohighlight">\(so(3)\)</span> and Exponential Map</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#rodrigues-rotation-formula">3. Rodrigues’ Rotation Formula</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#kinematic-chains-forward-inverse-kinematics">4. Kinematic Chains: Forward &amp; Inverse Kinematics</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#comparison-of-rotation-representations">Comparison of Rotation Representations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_03_1_surface_representations.html">Lecture 03.1 – Surface Representations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#mathematical-foundations-of-surface-representations">1. Mathematical Foundations of Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-parametric-surfaces">A) Parametric Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-implicit-surfaces">B) Implicit Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-explicit-surfaces">C) Explicit Surfaces</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#surface-differential-properties">2. Surface Differential Properties</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-surface-normals">A) Surface Normals</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-fundamental-forms-and-curvature">B) Fundamental Forms and Curvature</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-geodesics">C) Geodesics</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#discrete-surface-representations">3. Discrete Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-polygon-meshes">A) Polygon Meshes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-point-clouds">B) Point Clouds</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-signed-distance-fields-sdf">C) Signed Distance Fields (SDF)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#advanced-surface-representations">4. Advanced Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-bezier-curves-and-surfaces">A) Bézier Curves and Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-b-splines-and-nurbs">B) B-Splines and NURBS</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-subdivision-surfaces">C) Subdivision Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-level-sets">D) Level Sets</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#e-neural-implicit-representations">E) Neural Implicit Representations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#comparative-analysis-and-applications">5. Comparative Analysis and Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-computational-efficiency-and-storage">A) Computational Efficiency and Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-practical-applications">B) Practical Applications</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-operations-complexity">C) Operations Complexity</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#implementation-examples">6. Implementation Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-basic-mesh-processing-python">A) Basic Mesh Processing (Python)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-implicit-surface-utilities">B) Implicit Surface Utilities</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-bezier-curve-implementation">C) Bézier Curve Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-curvature-estimation-on-meshes">D) Curvature Estimation on Meshes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#advanced-topics-and-future-directions">7. Advanced Topics and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-multi-resolution-representations">A) Multi-Resolution Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-machine-learning-for-geometry">B) Machine Learning for Geometry</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-dynamic-surfaces">C) Dynamic Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-non-manifold-geometries">D) Non-Manifold Geometries</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html">Lecture 03.2 – Procrustes Alignment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#goal-learning-a-model-of-pose-and-shape">Goal: Learning a Model of Pose and Shape</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#the-challenge-of-registration">The Challenge of Registration</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#surface-representation-mesh">Surface Representation: Mesh</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#the-procrustes-alignment-problem-mathematical-formulation">The Procrustes Alignment Problem: Mathematical Formulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#rigid-transformations">Rigid Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#procrustes-alignment-solution">Procrustes Alignment Solution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#decoupling-translation-by-centroid-alignment">Decoupling Translation by Centroid Alignment</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#optimal-rotation-via-svd">Optimal Rotation via SVD</a><ul>
<li class="toctree-l4"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#reflection-adjustment">Reflection Adjustment</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#optimal-scale-optional">Optimal Scale (Optional)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#complete-mathematical-derivation">Complete Mathematical Derivation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#translation-derivation">Translation Derivation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#rotation-derivation">Rotation Derivation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#scale-derivation">Scale Derivation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#summary-of-procrustes-alignment-algorithm">Summary of Procrustes Alignment Algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#python-implementation-example">Python Implementation Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#practical-applications">Practical Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#interactive-visualization-ideas">Interactive Visualization Ideas</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_04_1_icp.html">Lecture 4.1: Iterative Closest Point</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#introduction-to-shape-alignment-and-registration">Introduction to Shape Alignment and Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#the-registration-problem">The Registration Problem</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#review-procrustes-analysis">Review: Procrustes Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#problem-unknown-correspondences">Problem: Unknown Correspondences</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#the-iterative-closest-point-icp-algorithm">The Iterative Closest Point (ICP) Algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#basic-icp-algorithm">Basic ICP Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#computational-considerations">Computational Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="lecture_04_1_icp.html#closest-point-computation">Closest Point Computation</a></li>
<li class="toctree-l4"><a class="reference internal" href="lecture_04_1_icp.html#convergence-and-local-minima">Convergence and Local Minima</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#point-to-point-vs-point-to-plane-icp">Point-to-Point vs. Point-to-Plane ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#point-to-point-icp">Point-to-Point ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#point-to-plane-icp">Point-to-Plane ICP</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#gradient-based-icp-for-non-rigid-registration">Gradient-based ICP for Non-Rigid Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#gradient-based-icp-algorithm">Gradient-based ICP Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#advantages-of-gradient-based-icp">Advantages of Gradient-based ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#computing-gradients">Computing Gradients</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#improving-icp-s-robustness">Improving ICP’s Robustness</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#data-association-direction">Data Association Direction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#robust-cost-functions">Robust Cost Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#trimmed-icp">Trimmed ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#ransac-based-approaches">RANSAC-based Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#additional-information">Additional Information</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#point-to-surface-distance">Point-to-Surface Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#icp-variants-and-extensions">ICP Variants and Extensions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#generalized-icp-gicp">Generalized ICP (GICP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#em-icp-and-probabilistic-approaches">EM-ICP and Probabilistic Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#coherent-point-drift-cpd">Coherent Point Drift (CPD)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#multi-scale-approaches">Multi-Scale Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#applications-of-icp">Applications of ICP</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#implementing-icp">Implementing ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#efficient-python-implementation">Efficient Python Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#practical-tips">Practical Tips</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_04_2_body_models.html">Lecture 04.2 - Body Models: Vertex-Based Models and SMPL</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#body-models-as-parameterized-functions">1. Body Models as Parameterized Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#rotations-articulation-and-pose-representation">2. Rotations, Articulation, and Pose Representation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#rotation-representation">2.1 Rotation Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#kinematic-chain">2.2 Kinematic Chain</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#linear-blend-skinning-and-its-limitations">3. Linear Blend Skinning and its Limitations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#linear-blend-skinning">3.1 Linear Blend Skinning</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#problems-with-standard-lbs">3.2 Problems with Standard LBS</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#blend-shapes-for-correcting-lbs-artifacts">3.3 Blend Shapes for Correcting LBS Artifacts</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#the-smpl-body-model">4. The SMPL Body Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#smpl-philosophy">4.1 SMPL Philosophy</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#smpl-model-architecture">4.2 SMPL Model Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#shape-blend-shapes">4.2.1 Shape Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#pose-blend-shapes">4.2.2 Pose Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#joint-regression">4.2.3 Joint Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#model-training">4.3 Model Training</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#comparison-with-scape">5. Comparison with SCAPE</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#the-scape-model">5.1 The SCAPE Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#different-approaches-to-deformation">5.2 Different Approaches to Deformation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#performance-comparison">5.3 Performance Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#other-advantages">5.4 Other Advantages</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#alignment-techniques-procrustes-analysis-and-icp">6. Alignment Techniques: Procrustes Analysis and ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#procrustes-analysis">6.1 Procrustes Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#iterative-closest-point-icp">6.2 Iterative Closest Point (ICP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#fitting-smpl-to-scans">6.3 Fitting SMPL to Scans</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#image-formation-and-the-pinhole-camera-model">7. Image Formation and the Pinhole Camera Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#the-pinhole-camera-model">7.1 The Pinhole Camera Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#lens-distortion">7.2 Lens Distortion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#extensions-and-advanced-applications">8. Extensions and Advanced Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#dynamic-soft-tissue-modeling">8.1 Dynamic Soft Tissue Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#specialized-extensions">8.2 Specialized Extensions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#deep-learning-for-model-fitting">8.3 Deep Learning for Model Fitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#probabilistic-approaches">8.4 Probabilistic Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#hybrid-models">8.5 Hybrid Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_05_1_body_model_training.html">Lecture 5.1 - Training a Body Model and Fitting SMPL to Scans</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#body-models-based-on-triangle-deformations">Body Models Based on Triangle Deformations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#scape-and-blendscape-models">SCAPE and BlendSCAPE Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#triangle-deformation-process">Triangle Deformation Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#comparison-smpl-vs-scape-blendscape">Comparison: SMPL vs. SCAPE/BlendSCAPE</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#training-a-body-model-from-registrations">Training a Body Model from Registrations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#the-challenge-of-raw-scan-data">The Challenge of Raw Scan Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#training-from-registrations">Training from Registrations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#obtaining-registrations-fitting-smpl-to-scans">Obtaining Registrations: Fitting SMPL to Scans</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#non-rigid-registration-process">Non-Rigid Registration Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#iterative-closest-point-icp-review">Iterative Closest Point (ICP) Review</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#registration-objective-formulation">Registration Objective Formulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#point-to-surface-distance">Point-to-Surface Distance</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#multi-stage-optimization-strategy">Multi-Stage Optimization Strategy</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#joint-registration-and-model-training">Joint Registration and Model Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#co-registration-approach">Co-Registration Approach</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_05_2_3d_registration.html">Lecture 05.2 - 3D Registration: From Classical ICP to Modern Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#rigid-registration-and-the-icp-algorithm">1. Rigid Registration and the ICP Algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#the-iterative-closest-point-icp-algorithm">The Iterative Closest Point (ICP) Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#convergence-analysis-and-failure-modes">Convergence Analysis and Failure Modes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#classical-non-rigid-registration">2. Classical Non-Rigid Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#thin-plate-spline-robust-point-matching-tps-rpm">Thin Plate Spline Robust Point Matching (TPS-RPM)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#coherent-point-drift-cpd">Coherent Point Drift (CPD)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#other-non-rigid-methods">Other Non-Rigid Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#parametric-models-and-the-smpl-body-model">3. Parametric Models and the SMPL Body Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#model-structure">Model Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#shape-blend-shapes-identity-variation">Shape Blend Shapes (Identity Variation)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#pose-blend-shapes-pose-dependent-deformation">Pose Blend Shapes (Pose-Dependent Deformation)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#linear-blend-skinning-lbs-for-articulation">Linear Blend Skinning (LBS) for Articulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#learning-smpl">Learning SMPL</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#using-smpl-for-registration">Using SMPL for Registration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#modeling-clothing-and-fine-details-smpl-d">4. Modeling Clothing and Fine Details: SMPL+D</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#why-smpl-d">Why SMPL+D?</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#how-displacements-are-applied">How Displacements Are Applied</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#limitations">Limitations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#applications">Applications</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#survey-of-3d-registration-methods-from-icp-to-deep-learning">5. Survey of 3D Registration Methods: From ICP to Deep Learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#early-pioneering-works-1990s-foundational-rigid-registration">5.1 Early Pioneering Works (1990s) – Foundational Rigid Registration</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#the-2000s-robust-and-non-rigid-registration-emerges">5.2 The 2000s – Robust and Non-Rigid Registration Emerges</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#s-template-based-and-parametric-model-registration">5.3 2010s – Template-based and Parametric Model Registration</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#s-learning-based-parametric-registration-and-hybrid-approaches">5.4 2020s – Learning-Based Parametric Registration and Hybrid Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html">Lecture 06.1 - Fitting the SMPL Model to Images via Optimization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#mathematical-background-pinhole-camera-and-projections">Mathematical Background: Pinhole Camera and Projections</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#perspective-projection">Perspective Projection</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#weak-perspective-projection">Weak-Perspective Projection</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#camera-extrinsics-vs-model-pose">Camera Extrinsics vs. Model Pose</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#d-keypoints-and-projection">2D Keypoints and Projection</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#the-smpl-model-as-a-differentiable-function-of-shape-and-pose">The SMPL Model as a Differentiable Function of Shape and Pose</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#shape-blend-shapes">Shape Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#pose-blend-shapes">Pose Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#joint-positions">Joint Positions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#linear-blend-skinning">Linear Blend Skinning</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#differentiability-of-smpl">Differentiability of SMPL</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#fitting-smpl-to-images-via-optimization-smplify">Fitting SMPL to Images via Optimization (SMPLify)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#objective-function">Objective Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#combined-objective">Combined Objective</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#optimization-strategy">Optimization Strategy</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#optimization-algorithms">Optimization Algorithms</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#automatic-differentiation-and-jacobians">Automatic Differentiation and Jacobians</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#result-of-smplify">Result of SMPLify</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#historical-progression-and-method-comparisons">Historical Progression and Method Comparisons</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#smplify-bogo-et-al-2016">SMPLify (Bogo et al. 2016)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#smplify-x-2019">SMPLify-X (2019)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html">Lecture 06.2 - Learning-Based Fitting of the SMPL Model to Images</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#foundations-of-learning-based-smpl-estimation">Foundations of Learning-Based SMPL Estimation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#integrating-smpl-into-neural-networks">Integrating SMPL into Neural Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#projection-functions">Projection Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#loss-functions">Loss Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#statistical-priors-and-adversarial-losses">Statistical Priors and Adversarial Losses</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#early-regression-approaches-hmr-and-nbf">Early Regression Approaches: HMR and NBF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#human-mesh-recovery-hmr">Human Mesh Recovery (HMR)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#neural-body-fitting-nbf">Neural Body Fitting (NBF)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#evolving-architectures-hybrid-and-improved-regression-methods">Evolving Architectures: Hybrid and Improved Regression Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#spin-optimization-in-the-training-loop">SPIN: Optimization in the Training Loop</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#pymaf-pyramidal-mesh-alignment-feedback">PyMAF: Pyramidal Mesh Alignment Feedback</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#cliff-using-full-frame-context-for-camera-orientation">CLIFF: Using Full-Frame Context for Camera Orientation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#pixie-whole-body-regression-with-part-experts">PIXIE: Whole-Body Regression with Part Experts</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#temporal-methods-from-single-images-to-video-sequences">Temporal Methods: From Single Images to Video Sequences</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#vibe-adversarial-motion-prior-with-grus">VIBE: Adversarial Motion Prior with GRUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#tcmr-temporally-consistent-mesh-recovery">TCMR: Temporally Consistent Mesh Recovery</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#motionbert-transformer-based-motion-representations">MotionBERT: Transformer-Based Motion Representations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#comparison-of-learning-based-methods">Comparison of Learning-Based Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#supervision-and-data">Supervision and Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#network-architecture">Network Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#objective-functions">Objective Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#pose-and-shape-priors">Pose and Shape Priors</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#performance">Performance</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#runtime">Runtime</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#strengths-and-weaknesses">Strengths and Weaknesses</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_2SMPL_model_fitting.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html">Lecture 07.1: Fitting SMPL to IMU Data Using Optimization-Based Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#classical-imu-based-pose-estimation-a-historical-perspective">Classical IMU-Based Pose Estimation: A Historical Perspective</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#attitude-and-heading-reference-systems">Attitude and Heading Reference Systems</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#kalman-filter-approaches">Kalman Filter Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#early-sparse-sensor-approaches">Early Sparse-Sensor Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#model-based-optimization-methods">Model-Based Optimization Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#learning-based-methods">Learning-Based Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#inertial-sensor-fundamentals-and-orientation-representations">Inertial Sensor Fundamentals and Orientation Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#gravity-alignment-and-drift-correction">Gravity Alignment and Drift Correction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#orientation-representations">Orientation Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#sensor-calibration">Sensor Calibration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#optimization-based-smpl-fitting-with-imu-data">Optimization-Based SMPL Fitting with IMU Data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#kinematic-model-and-sensor-prediction">Kinematic Model and Sensor Prediction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#regularization-and-prior-terms">Regularization and Prior Terms</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#gradient-and-jacobian-computation">Gradient and Jacobian Computation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#pseudocode-smpl-pose-estimation-from-imu-sequence">Pseudocode: SMPL Pose Estimation from IMU Sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#imu-based-human-pose-datasets-and-resources">IMU-Based Human Pose Datasets and Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#dip-imu-dataset-2018">DIP-IMU Dataset (2018)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#totalcapture-2017">TotalCapture (2017)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#amass-2019">AMASS (2019)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#other-datasets-and-resources">Other Datasets and Resources</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html">Lecture 07.2: Fitting SMPL to IMU Data Using Learning-Based Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#optimization-based-vs-learning-based-approaches">Optimization-Based vs. Learning-Based Approaches</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#learning-based-imu-to-pose-estimation-historical-overview-of-key-models">Learning-Based IMU-to-Pose Estimation: Historical Overview of Key Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#deep-inertial-poser-dip-2018">Deep Inertial Poser (DIP, 2018)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#transpose-2021">TransPose (2021)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#transformer-inertial-poser-tip-2022">Transformer Inertial Poser (TIP, 2022)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#physics-physical-inertial-poser-pip-2022">Physics/Physical Inertial Poser (PIP, 2022)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#other-notable-models-and-developments">Other Notable Models and Developments</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#problem-formulation-and-learning-task-definition">Problem Formulation and Learning Task Definition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#input-and-output-representations">Input and Output Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#learning-objective-and-loss-functions">Learning Objective and Loss Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#temporal-modeling-approaches">Temporal Modeling Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#supervised-vs-semi-supervised-training-synthetic-data">Supervised vs. Semi-Supervised Training; Synthetic Data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#model-architectures-and-design-considerations">Model Architectures and Design Considerations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#encoding-imu-measurements">Encoding IMU Measurements</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#network-structures">Network Structures</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#training-pipeline-and-pseudocode">Training Pipeline and Pseudocode</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#datasets-benchmarks-and-resources">Datasets, Benchmarks, and Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#dip-imu-dataset-2018">DIP-IMU Dataset (2018)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#totalcapture-2017">TotalCapture (2017)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#amass-2019">AMASS (2019)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#other-datasets-and-resources">Other Datasets and Resources</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#challenges-and-outlook">Challenges and Outlook</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Neural Radiance Fields: A Historical and Theoretical Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#foundations-3d-scene-representation-and-reconstruction-techniques">Foundations: 3D Scene Representation and Reconstruction Techniques</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#voxel-grids">Voxel Grids</a></li>
<li class="toctree-l3"><a class="reference internal" href="#point-clouds">Point Clouds</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mesh-based-surfaces">Mesh-Based Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="#light-fields-and-volumetric-rendering">Light Fields and Volumetric Rendering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#emergence-of-neural-radiance-fields-nerf">Emergence of Neural Radiance Fields (NeRF)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#core-idea">Core Idea</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training-procedure">Training Procedure</a></li>
<li class="toctree-l3"><a class="reference internal" href="#nerf-architecture">NeRF Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hierarchical-sampling">Hierarchical Sampling</a></li>
<li class="toctree-l3"><a class="reference internal" href="#original-results">Original Results</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#theoretical-and-mathematical-analysis-of-nerf">Theoretical and Mathematical Analysis of NeRF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#volume-rendering-formulation-in-nerf">Volume Rendering Formulation in NeRF</a></li>
<li class="toctree-l3"><a class="reference internal" href="#positional-encoding-and-neural-network-architecture">Positional Encoding and Neural Network Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="#loss-function-and-optimization">Loss Function and Optimization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#major-advancements-and-extensions-of-nerf">Major Advancements and Extensions of NeRF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#anti-aliasing-and-unbounded-scenes-mip-nerf-and-nerf">Anti-Aliasing and Unbounded Scenes: mip-NeRF and NeRF++</a></li>
<li class="toctree-l3"><a class="reference internal" href="#efficiency-improvements-instant-nerf-and-plenoctrees">Efficiency Improvements: Instant NeRF and PlenOctrees</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dynamic-and-deformable-nerfs-d-nerf-nerfies-nsff-etc">Dynamic and Deformable NeRFs (D-NeRF, Nerfies, NSFF, etc.)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#neural-radiance-fields-for-human-modeling-with-smpl-and-body-models">Neural Radiance Fields for Human Modeling (with SMPL and Body Models)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#other-notable-extensions">Other Notable Extensions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#comparison-with-other-3d-representations">Comparison with Other 3D Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#vs-polygonal-meshes">Vs. Polygonal Meshes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#vs-point-clouds-3d-splatting">Vs. Point Clouds / 3D Splatting</a></li>
<li class="toctree-l3"><a class="reference internal" href="#vs-voxel-grids-and-volumetric-methods">Vs. Voxel Grids and Volumetric Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="#vs-multi-plane-images-mpis-light-fields">Vs. Multi-Plane Images (MPIs) / Light Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="#accuracy-and-fidelity">Accuracy and Fidelity</a></li>
<li class="toctree-l3"><a class="reference internal" href="#applicability">Applicability</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#datasets-for-nerf-training-and-evaluation">Datasets for NeRF Training and Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#blender-synthetic-nerf-dataset">Blender Synthetic NeRF Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#local-light-field-fusion-llff-real-forward-facing-dataset">Local Light Field Fusion (LLFF) Real Forward-Facing Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tanks-and-temples">Tanks and Temples</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dtu-dataset">DTU Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#human3-6m">Human3.6M</a></li>
<li class="toctree-l3"><a class="reference internal" href="#zju-mocap-dataset">ZJU-MoCap Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#people-snapshot">People-Snapshot</a></li>
<li class="toctree-l3"><a class="reference internal" href="#synthetic-dynamic-scenes">Synthetic dynamic scenes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#other-datasets">Other datasets</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html">3D Gaussian Splatting: A Basic Introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#foundations">Foundations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#what-is-a-3d-scene">What is a 3D Scene?</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-scene-representation">3D Scene Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#computer-graphics-fundamentals">Computer Graphics Fundamentals</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#rasterization-and-ray-tracing">Rasterization and Ray Tracing</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#alpha-blending-and-compositing">Alpha Blending and Compositing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#the-evolution-of-novel-view-synthesis">The Evolution of Novel View Synthesis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#image-based-rendering">Image-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#structure-from-motion-and-multi-view-stereo">Structure-from-Motion and Multi-View Stereo</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#point-based-rendering">Point-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#neural-rendering">Neural Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#accelerated-neural-fields">Accelerated Neural Fields</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussian-splatting-a-convergence-of-approaches">3D Gaussian Splatting: A Convergence of Approaches</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#id1">Point-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#point-clouds-and-their-challenges">Point Clouds and Their Challenges</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#the-concept-of-splatting">The Concept of Splatting</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#elliptical-weighted-average-ewa-filtering">Elliptical Weighted Average (EWA) Filtering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-point-based-rendering">Differentiable Point-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussian-splatting-core-principles">3D Gaussian Splatting: Core Principles</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#key-insight-unifying-points-and-volumes">Key Insight: Unifying Points and Volumes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussians-as-scene-primitives">3D Gaussians as Scene Primitives</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#the-volumetric-rendering-equation">The Volumetric Rendering Equation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#alpha-compositing-with-gaussians">Alpha Compositing with Gaussians</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#mathematical-formulation-of-3d-gaussian-splatting">Mathematical Formulation of 3D Gaussian Splatting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#projecting-3d-gaussians-to-2d">Projecting 3D Gaussians to 2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#parameterization-of-3d-gaussians">Parameterization of 3D Gaussians</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#view-dependent-appearance">View-Dependent Appearance</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-rendering-equations">Differentiable Rendering Equations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#training-and-optimization">Training and Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#photometric-loss">Photometric Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#initial-point-cloud">Initial Point Cloud</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#optimization-process">Optimization Process</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-splatting-pipeline">Differentiable Splatting Pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#adaptive-density-control">Adaptive Density Control</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#implementation-and-real-time-rendering">Implementation and Real-Time Rendering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#tile-based-rendering">Tile-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#fast-sorting-strategies">Fast Sorting Strategies</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#gpu-accelerated-rasterization">GPU-Accelerated Rasterization</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#memory-considerations">Memory Considerations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#comparison-with-other-methods">Comparison with Other Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#nerf-vs-3d-gaussian-splatting">NeRF vs. 3D Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#voxel-based-representations-vs-gaussians">Voxel-Based Representations vs. Gaussians</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#traditional-point-based-rendering-vs-gaussian-splatting">Traditional Point-Based Rendering vs. Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#applications-and-extensions">Applications and Extensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#static-scene-reconstruction">Static Scene Reconstruction</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#dynamic-scene-capture">Dynamic Scene Capture</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#avatar-creation-and-animation">Avatar Creation and Animation</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#integration-with-neural-rendering">Integration with Neural Rendering</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#large-scale-scene-rendering">Large-Scale Scene Rendering</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#bezier-gaussian-triangles-bg-triangle-for-sharper-rendering">Bézier Gaussian Triangles (BG-Triangle) for Sharper Rendering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#representation">Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#performance">Performance</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#human-reconstruction-with-gaussian-splatting-and-priors">Human Reconstruction with Gaussian Splatting and Priors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#eg-humannerf-efficient-generalizable-human-nerf">EG-HumanNeRF: Efficient Generalizable Human NeRF</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#gps-gaussian-pixel-wise-gaussian-splatting-for-humans">GPS-Gaussian: Pixel-Wise Gaussian Splatting for Humans</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#generalizable-human-gaussians-ghg-with-smpl">Generalizable Human Gaussians (GHG) with SMPL</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#dynamic-scene-reconstruction-with-4d-gaussian-splatting">Dynamic Scene Reconstruction with 4D Gaussian Splatting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussian-splatting-4dgs">4D Gaussian Splatting (4DGS)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#speed-and-memory-enhancements-4dgs-1k-and-mega">Speed and Memory Enhancements (4DGS-1K and MEGA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#applications-to-mocap-and-4d-human-rendering">Applications to MoCap and 4D Human Rendering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#implementation-details-and-real-time-performance">Implementation Details and Real-Time Performance</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#data-structures">Data Structures</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#rasterization-shaders">Rasterization &amp; Shaders</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#gpu-memory-and-throughput">GPU Memory and Throughput</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-rendering-implementation">Differentiable Rendering Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#training-vs-inference-compute">Training vs. Inference Compute</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#accuracy-vs-speed-trade-offs">Accuracy vs. Speed trade-offs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#benchmarks-and-comparative-evaluation">Benchmarks and Comparative Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#static-scene-comparison">Static Scene Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#human-novel-view-comparison">Human Novel-View Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#dynamic-scene-comparison">Dynamic Scene Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#comparative-summary-table">Comparative Summary Table</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#datasets-and-resources">Datasets and Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#synthetic-nerf-dataset-blender-scenes">Synthetic NeRF Dataset (Blender Scenes)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#llff-local-light-field-fusion">LLFF (Local Light Field Fusion)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#tanks-and-temples">Tanks and Temples</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#multi-object-360-co3d-common-objects-in-3d">Multi-Object 360 (CO3D - Common Objects in 3D)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#amass-archive-of-motion-capture-as-surface-shapes">AMASS (Archive of Motion Capture as Surface Shapes)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#cape-clothed-auto-person-encoding">CAPE (Clothed Auto Person Encoding)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#thuman-thuman2-0">THuman / THuman2.0</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#renderpeople">RenderPeople</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#open-source-implementations">Open-Source Implementations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#future-directions">Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#improved-compression-techniques-for-memory-efficiency">Improved Compression Techniques for Memory Efficiency</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#handling-dynamic-and-deformable-scenes">Handling Dynamic and Deformable Scenes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#advanced-material-modeling-for-realistic-rendering">Advanced Material Modeling for Realistic Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#hybrid-approaches-integrating-neural-fields-and-explicit-representations">Hybrid Approaches Integrating Neural Fields and Explicit Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#scalability-for-large-scale-scenes-city-level-and-beyond">Scalability for Large-Scale Scenes (City-Level and Beyond)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#real-time-applications-in-ar-vr-and-gaming">Real-Time Applications in AR/VR and Gaming</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#integration-with-existing-graphics-pipelines">Integration with Existing Graphics Pipelines</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#learning-from-limited-data">Learning from Limited Data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#conclusion">Conclusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#glossary">Glossary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a><ul>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-01-1-historical-body-models">Lecture 01.1 (Historical Body Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-01-2-introduction-to-human-models">Lecture 01.2 (Introduction to Human Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-01-3-introduction-to-human-models-continued">Lecture 01.3 (Introduction to Human Models Continued)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-02-1-image-formation">Lecture 02.1 (Image Formation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-02-2-rotations-kinematic-chains">Lecture 02.2 (Rotations &amp; Kinematic Chains)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-03-1-surface-representations">Lecture 03.1 (Surface Representations)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-03-2-procrustes-alignment">Lecture 03.2 (Procrustes Alignment)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-04-1-iterative-closest-points">Lecture 04.1 (Iterative Closest Points)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-04-2-body-models">Lecture 04.2 (Body Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-05-1-body-model-training">Lecture 05.1 (Body Model Training)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-05-2-3d-registration">Lecture 05.2 (3D Registration)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-06-1-fitting-smpl-to-images">Lecture 06.1 (Fitting SMPL to Images)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-06-1-optimization-based-fitting-of-smpl-to-images">Lecture 06.1 (Optimization-Based Fitting of SMPL to Images)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-06-2-learning-based-fitting-of-smpl-to-images">Lecture 06.2 (Learning-Based Fitting of SMPL to Images)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-07-1-fitting-smpl-to-imu-optimization">Lecture 07.1 (Fitting SMPL to IMU Optimization)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-07-2-fitting-smpl-to-imu-learning">Lecture 07.2 (Fitting SMPL to IMU Learning)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="references.html#classic-and-optimization-based-methods">Classic and Optimization-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="references.html#learning-based-methods">Learning-Based Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="references.html#datasets-and-resources">Datasets and Resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#relevant-software-and-libraries">Relevant Software and Libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#neural-radiance-fields-nerf">Neural Radiance Fields (NERF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#gaussian-splatting">Gaussian Splatting</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Virtual Humans Lecture</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Neural Radiance Fields: A Historical and Theoretical Overview</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/extended_materials_neural_radiance_fields.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="neural-radiance-fields-a-historical-and-theoretical-overview">
<span id="lecture-nerf-overview"></span><h1>Neural Radiance Fields: A Historical and Theoretical Overview<a class="headerlink" href="#neural-radiance-fields-a-historical-and-theoretical-overview" title="Link to this heading"></a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h2>
<p>Neural Radiance Fields (NeRF) represent a breakthrough in 3D scene representation and view synthesis, enabling photorealistic rendering of scenes from novel viewpoints given a set of input images. Unlike traditional computer vision pipelines that explicitly reconstruct geometry (point clouds, meshes, voxels, etc.), NeRF learns an implicit scene representation – a continuous function implemented by a neural network that outputs color and density given a 3D position and viewing direction. This representation is then rendered into images using principles of volumetric rendering, allowing end-to-end optimization from images alone.</p>
<p>This document provides a comprehensive overview of NeRF’s development: from its foundations in traditional 3D scene representation techniques through the original NeRF formulation with detailed mathematical analysis, to numerous extensions and improvements. We also compare NeRF to alternative 3D representations and summarize common datasets used for evaluating NeRF and its variants.</p>
</section>
<section id="foundations-3d-scene-representation-and-reconstruction-techniques">
<h2>Foundations: 3D Scene Representation and Reconstruction Techniques<a class="headerlink" href="#foundations-3d-scene-representation-and-reconstruction-techniques" title="Link to this heading"></a></h2>
<p>Before NeRF, 3D scenes were typically represented with explicit geometric structures or discrete volumetric grids. We review key foundational techniques that influenced NeRF’s development.</p>
<section id="voxel-grids">
<h3>Voxel Grids<a class="headerlink" href="#voxel-grids" title="Link to this heading"></a></h3>
<p>A voxel grid is a 3D extension of a pixel grid, dividing space into a regular lattice of small cubes (voxels), each storing properties like occupancy, color, or density. Voxel grids provide an intuitive volumetric representation of shape and appearance – for example, early 3D reconstructions from medical CT or MRI directly produce voxel data. One advantage is their ability to represent complex internal structures that may be hard to capture with just surfaces.</p>
<p>Voxel representations were used in some of the earliest 3D reconstruction approaches (e.g., space carving, where a volume is carved away based on silhouette consistency from multiple images). They gained popularity in the deep learning era as well: Maturana and Scherer’s VoxNet (2015) applied 3D Convolutional Neural Networks (CNNs) on voxel grids for object recognition. In multi-view reconstruction, approaches like Choy et al.’s 3D-R2N2 (2016) learned to predict a voxel grid from images using recurrent networks.</p>
<p>However, dense voxel grids suffer from high memory usage and limited resolution – doubling resolution increases memory by 8×, making it impractical to capture fine details in large scenes. Techniques like octrees (hierarchies of voxels) were introduced to sparsely subdivide space where detail is needed, alleviating memory issues. These voxel-based ideas influenced NeRF, as a volume with spatially-varying density and color is essentially a continuous counterpart of a voxel grid representation, albeit encoded in a network rather than an explicit array.</p>
</section>
<section id="point-clouds">
<h3>Point Clouds<a class="headerlink" href="#point-clouds" title="Link to this heading"></a></h3>
<p>A point cloud represents a 3D shape or scene as an unstructured set of points in space, each with coordinates (and often color). Point clouds are a direct output of many 3D sensors (like LiDAR scanners or multi-view stereo algorithms) and were widely used for reconstruction before surface meshing. Their simplicity is a strength – no topology or connectivity is assumed – which makes them easy to acquire and merge from different views.</p>
<p>However, their lack of connectivity means surfaces are only implicitly defined; rendering point clouds can produce gaps or require interpolation (e.g., splatting each point as a disk or Gaussian). Early computer graphics explored point-based rendering for efficiency (e.g., Grossman &amp; Dally 1998; Rusinkiewicz &amp; Levoy 2000’s QSplat for adaptive point rendering).</p>
<p>In recent years, deep learning networks like PointNet (Qi et al., 2017) process point clouds directly, and techniques like 3D Gaussian splatting (Kerbl et al., 2023) model surfaces as oriented point primitives. Notably, NeRF itself does not use point clouds directly, but some later NeRF variants and editing tools convert NeRF to point sets for faster rendering or manipulation.</p>
</section>
<section id="mesh-based-surfaces">
<h3>Mesh-Based Surfaces<a class="headerlink" href="#mesh-based-surfaces" title="Link to this heading"></a></h3>
<p>Perhaps the most common 3D representation in graphics is a mesh, typically a collection of vertices connected into polygons (usually triangles) forming surfaces. Meshes are efficient to render with hardware and are the backbone of traditional graphics pipelines.</p>
<p>Decades of multi-view stereo (MVS) research were devoted to reconstructing meshes from images: structure-from-motion gives sparse points and cameras, then MVS densifies into either point clouds or directly into mesh surfaces. Techniques like Poisson Surface Reconstruction (Kazhdan et al., 2006) convert point clouds to watertight meshes, and well-established algorithms (e.g., COLMAP by Schönberger &amp; Frahm, 2016) produce textured meshes for real scenes.</p>
<p>Meshes excel at representing explicit geometry with high precision and are memory-efficient (complex surfaces can be represented by millions of triangles, far less data than a comparable voxel grid). However, they mostly capture surfaces (the “shell” of objects), not their volumetric interior or translucent materials. View-dependent appearance like specular highlights or transparency is also not inherent in a static textured mesh; additional material models are required to render such effects.</p>
<p>These limitations of mesh-based approaches set the stage for NeRF’s implicit volumetric method, which naturally handles volume density and view-dependent color. Nonetheless, meshes remain important for comparison – e.g., NeRF’s accuracy is often measured by extracting a mesh via iso-surface of density and comparing to scanned geometry, and recent methods combine NeRF with mesh-based human models (like SMPL) to get the best of both worlds.</p>
</section>
<section id="light-fields-and-volumetric-rendering">
<h3>Light Fields and Volumetric Rendering<a class="headerlink" href="#light-fields-and-volumetric-rendering" title="Link to this heading"></a></h3>
<p>NeRF is deeply connected to concepts from light field rendering and volumetric rendering in graphics. A light field represents the radiance traveling in every direction through every point in space. Adelson and Bergen (1991) introduced the plenoptic function as a 7D function describing light rays (with dimensions for 3D position, 2D direction, time, and wavelength). In free space, light fields reduce to 4D (two angles for direction, two for viewpoint on a focal plane).</p>
<p>Classic work by Levoy and Hanrahan (1996) and Gortler et al. (1996) showed that if you capture a dense set of views of a scene (effectively samples of the 4D light field), you can render new views by interpolation. However, dense sampling is impractical for large baselines, so light field methods often suffered blur if input views were sparse. NeRF can be seen as learning a continuous light field (actually a radiance field with density) from sparse views, thereby interpolating the plenoptic function in a data-driven way rather than storing a huge 4D dataset explicitly.</p>
<p>In volumetric rendering, a scene is represented as a field of emitting/absorbing particles (volume density and color). Kajiya and von Herzen (1984) and Max (1995) established the classic volume rendering equation: through a volume, its color is an integral of emitted radiance attenuated by the accumulated density (treating density as causing exponential attenuation of light). In formula form, for a camera ray <span class="math notranslate nohighlight">\(\mathbf{r}(t)=\mathbf{o}+t\mathbf{d}\)</span> with near and far bounds <span class="math notranslate nohighlight">\(t_n, t_f\)</span>, the expected color is:</p>
<div class="math notranslate nohighlight">
\[C(r)=\int_{t_n}^{t_f}T(t) \sigma(\mathbf{r}(t)) \mathbf{c}(\mathbf{r}(t),\mathbf{d}) dt,\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma(\mathbf{x})\)</span> is the volume density at point <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> (interpreted as the differential probability of a ray terminating at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>), <span class="math notranslate nohighlight">\(\mathbf{c}(\mathbf{x},\mathbf{d})\)</span> is the radiance (color) emitted at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> in direction <span class="math notranslate nohighlight">\(\mathbf{d}\)</span>. <span class="math notranslate nohighlight">\(T(t)=\exp(-\int_{t_n}^{t}\sigma(\mathbf{r}(s))ds)\)</span> is the transmittance (the probability the ray travels unoccluded up to <span class="math notranslate nohighlight">\(t\)</span>). This equation can be thought of as alpha compositing many translucent slices.</p>
<p>Rendering is achieved by numerically approximating this integral (e.g., by ray-marching through the volume and compositing front-to-back). Importantly, this rendering process is differentiable – a small change in density or color at any point affects the rendered pixel continuously – which is a key enabler for NeRF to learn from images.</p>
<p>Volumetric approaches have been used in computer vision before NeRF as well, such as visual hulls (Laurentini, 1994) or space carving (Kutulakos &amp; Seitz, 2000) that carve a voxel volume using silhouettes or photo-consistency. NeRF’s core innovation was to combine a volumetric rendering formulation with a neural implicit representation and optimize it with gradient descent from images. Thus, understanding classical volume rendering is fundamental to understanding NeRF.</p>
<p>These foundational methods – voxel grids, point clouds, meshes, and light field/volumetric rendering – each contributed ideas to NeRF. Voxel volumes and volumetric rendering inspired NeRF’s density+color field and integration technique; point clouds and meshes highlighted the need for representations that can capture fine details and view-dependent effects; light fields provided a target (the plenoptic function) that NeRF effectively learns to approximate. NeRF can be viewed as a neural, continuous extension of earlier volumetric scene representations, optimized using the tools of modern deep learning.</p>
</section>
</section>
<section id="emergence-of-neural-radiance-fields-nerf">
<h2>Emergence of Neural Radiance Fields (NeRF)<a class="headerlink" href="#emergence-of-neural-radiance-fields-nerf" title="Link to this heading"></a></h2>
<p>Neural Radiance Fields were introduced by Mildenhall et al. in their 2020 paper “NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis.” This work demonstrated, for the first time, that an MLP (multilayer perceptron) could implicitly encode a complete 5D representation of a complex scene – 3D spatial coordinates <span class="math notranslate nohighlight">\((x,y,z)\)</span> plus 2D viewing direction <span class="math notranslate nohighlight">\((\theta,\phi)\)</span> – such that photorealistic images from arbitrary viewpoints can be rendered via volume rendering. The original NeRF achieved a step-change in view synthesis quality, surpassing prior methods like deep voxel-based volumes (Lombardi et al. 2019), multi-plane images (MPIs in LLFF by Mildenhall et al. 2019), or learned mesh-based rendering, especially on scenes with intricate geometry and reflective materials.</p>
<section id="core-idea">
<h3>Core Idea<a class="headerlink" href="#core-idea" title="Link to this heading"></a></h3>
<p>NeRF represents a scene as a continuous function <span class="math notranslate nohighlight">\(F_\Theta(\mathbf{x}, \mathbf{d}) \mapsto (\sigma, \mathbf{c})\)</span> – implemented by a neural network with parameters <span class="math notranslate nohighlight">\(\Theta\)</span> – that maps any 3D point <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and viewing direction <span class="math notranslate nohighlight">\(\mathbf{d}\)</span> to a density <span class="math notranslate nohighlight">\(\sigma\)</span> (volume density at that point) and an emitted color <span class="math notranslate nohighlight">\(\mathbf{c}\)</span> (an RGB value). In practice, <span class="math notranslate nohighlight">\(\mathbf{d}\)</span> is typically parameterized as a unit vector or two angular coordinates. This function is fully implicit; there is no explicit voxel grid or mesh stored.</p>
<p>The network is optimized such that when these predicted values are rendered along any camera ray using the volume rendering integral, the resulting pixel color matches the input image. NeRF requires input images with known camera poses (e.g., from structure-from-motion) but no other geometry input – the geometry and appearance are learned by minimizing the photometric error between rendered and real pixels.</p>
</section>
<section id="training-procedure">
<h3>Training Procedure<a class="headerlink" href="#training-procedure" title="Link to this heading"></a></h3>
<p>The original NeRF training loop randomly samples camera rays from the input images, and for each ray samples a set of points <span class="math notranslate nohighlight">\(\{\mathbf{x}_i\}\)</span> along it. The network is queried at each sample to produce <span class="math notranslate nohighlight">\(\sigma_i\)</span> and <span class="math notranslate nohighlight">\(\mathbf{c}_i\)</span>, and these are composited with the volume rendering equation to produce a predicted pixel color <span class="math notranslate nohighlight">\(C_{\text{pred}}\)</span>. The loss is simply the mean squared error (MSE) between <span class="math notranslate nohighlight">\(C_{\text{pred}}\)</span> and the true pixel color <span class="math notranslate nohighlight">\(C_{\text{gt}}\)</span>. Because volume rendering is differentiable, gradients with respect to the network parameters <span class="math notranslate nohighlight">\(\Theta\)</span> can be computed and used to update the MLP. Over many iterations (typically 100k+), the network converges to represent the scene.</p>
<p>Mildenhall et al. showed that this process, while slow, produces significantly higher fidelity novel views than prior approaches – in their results, NeRF achieved ~PSNR 31–40 on synthetic scenes, versus prior state-of-the-art ~22–34. Qualitatively, NeRF’s renderings were crisp and could reproduce even fine details and specular highlights that competing methods blurred or missed.</p>
</section>
<section id="nerf-architecture">
<h3>NeRF Architecture<a class="headerlink" href="#nerf-architecture" title="Link to this heading"></a></h3>
<p>The MLP used in NeRF is a fully-connected network (no convolutions) with depth 8–10 layers and ~256 channels per layer for the main branch. A crucial aspect was the use of a positional encoding on the inputs to enable the network to represent high-frequency details. The network outputs density after some layers, and also produces a feature vector that, together with the view direction, flows into subsequent layers to output the view-dependent color.</p>
<p>This architecture allows the network to model effects like specular reflections by making color a function of viewing direction <span class="math notranslate nohighlight">\(\mathbf{d}\)</span> (while density <span class="math notranslate nohighlight">\(\sigma\)</span> is view-independent). Empirically, including view direction was important: without it, the model reduces to a pure Lambertian scene and cannot reproduce highlights. The NeRF authors illustrated this by showing that training without view input fails to learn the shiny reflections on a bulldozer object.</p>
</section>
<section id="hierarchical-sampling">
<h3>Hierarchical Sampling<a class="headerlink" href="#hierarchical-sampling" title="Link to this heading"></a></h3>
<p>A notable training trick in NeRF is hierarchical sampling. Rather than using a fixed sampling of points along each ray, NeRF employs a two-stage process: a coarse network predicts a rough volume distribution, and then a fine network focuses samples in regions likely to contribute (e.g., where density is non-negligible).</p>
<p>In practice, the coarse model is identical in architecture; it is trained simultaneously using a small number of stratified samples along the ray. It produces a “proposal” distribution (essentially weights proportional to <span class="math notranslate nohighlight">\(\sigma\)</span>) from which additional sample points are drawn for the fine model. The fine model then outputs the final color.</p>
<p>This hierarchical approach acts as an importance sampling scheme, improving efficiency and also acting as regularization. Mildenhall et al. noted that it was one of two key improvements (the other being positional encoding) needed to get high-quality results. The result is that NeRF can represent very high-resolution details despite using a relatively compact MLP – because it samples continuously and not on a fixed grid, it is not limited to a specific voxel resolution. Essentially, NeRF showed that an overfit neural network can serve as a powerful compression of a scene’s light field.</p>
</section>
<section id="original-results">
<h3>Original Results<a class="headerlink" href="#original-results" title="Link to this heading"></a></h3>
<p>The NeRF paper demonstrated compelling results on both synthetic data (objects rendered with complex materials) and real images (photos of scenes captured with a handheld phone). For instance, on the Synthetic-NeRF benchmark (8 Blender scenes with path-traced images), NeRF achieved ~PSNR 31, a ~5 dB improvement over prior methods like DeepVoxels and SRN. On the real LLFF dataset (8 real-world captured scenes), NeRF also outperformed local light field methods, especially in preserving fine geometry (e.g., thin structures like leaves, which NeRF rendered consistently without “floating” artifacts that LLFF showed).</p>
<p>The downside was speed: NeRF required dozens of hours to train per scene (often 1–2 days) and minutes to render a single image with hundreds of MLP evaluations per ray. Nonetheless, the breakthrough in visual fidelity sparked an explosion of research extending and improving NeRF.</p>
</section>
</section>
<section id="theoretical-and-mathematical-analysis-of-nerf">
<h2>Theoretical and Mathematical Analysis of NeRF<a class="headerlink" href="#theoretical-and-mathematical-analysis-of-nerf" title="Link to this heading"></a></h2>
<p>In this section, we examine NeRF’s formulation in detail – from the volume rendering equations it employs, to the positional encoding and network architecture that allow it to succeed, and the loss functions and optimization strategies used.</p>
<section id="volume-rendering-formulation-in-nerf">
<h3>Volume Rendering Formulation in NeRF<a class="headerlink" href="#volume-rendering-formulation-in-nerf" title="Link to this heading"></a></h3>
<p>NeRF’s rendering process relies on classical volume rendering. As described earlier, the color of a camera ray <span class="math notranslate nohighlight">\(C(\mathbf{r})\)</span> is obtained by integrating the radiance emitted along the ray with appropriate attenuation. For completeness, we restate the continuous formulation and then show how NeRF implements it discretely.</p>
<p><strong>Continuous volume rendering equation</strong>: For a ray <span class="math notranslate nohighlight">\(\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}\)</span> from camera origin <span class="math notranslate nohighlight">\(\mathbf{o}\)</span>, in direction <span class="math notranslate nohighlight">\(\mathbf{d}\)</span>, passing through the scene from depth <span class="math notranslate nohighlight">\(t_n\)</span> to <span class="math notranslate nohighlight">\(t_f\)</span>, the integral form is:</p>
<div class="math notranslate nohighlight">
\[C(r)=\int_{t_n}^{t_f}\exp\left(-\int_{t_n}^{t}\sigma(\mathbf{r}(s)) ds\right) \sigma(\mathbf{r}(t)) \mathbf{c}(\mathbf{r}(t),\mathbf{d}) dt.\]</div>
<p>Here <span class="math notranslate nohighlight">\(\sigma(\mathbf{x})\)</span> is the density at point <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{c}(\mathbf{x},\mathbf{d})\)</span> is the emitted color. The inner exponential is the transmittance <span class="math notranslate nohighlight">\(T(t)\)</span> from <span class="math notranslate nohighlight">\(t_n\)</span> up to <span class="math notranslate nohighlight">\(t\)</span>, meaning the fraction of light that hasn’t been absorbed before reaching <span class="math notranslate nohighlight">\(t\)</span>. Intuitively, this equation accumulates color contributions from each differential segment of the ray, weighted by the probability that the ray has not been occluded prior to that segment. This is analogous to compositing translucent layers: <span class="math notranslate nohighlight">\(\sigma\)</span> acts like an opacity at each point, and <span class="math notranslate nohighlight">\(\mathbf{c}\)</span> like the color of a glowing particle at that point.</p>
<p><strong>Discrete approximation</strong>: NeRF implements this integral via numerical quadrature. It samples a set of <span class="math notranslate nohighlight">\(N\)</span> points <span class="math notranslate nohighlight">\(\{t_i\}_{i=1}^N\)</span> along the ray (sorted in increasing depth). Typically, NeRF draws stratified random samples in each of <span class="math notranslate nohighlight">\(N\)</span> equal depth intervals in <span class="math notranslate nohighlight">\([t_n,t_f]\)</span> for the coarse pass, and then uses importance sampling for the fine pass. Given sample points and their densities and colors <span class="math notranslate nohighlight">\((\sigma_i, \mathbf{c}_i) = F_\Theta(\mathbf{x}_i,\mathbf{d})\)</span>, the discrete color is computed as:</p>
<div class="math notranslate nohighlight">
\[\hat{C}(r)=\sum_{i=1}^{N}T_i \alpha_i \mathbf{c}_i, \quad \text{where} \quad \alpha_i=1-\exp(-\sigma_i\delta_i),\]</div>
<p><span class="math notranslate nohighlight">\(T_i = \prod_{j=1}^{i-1}(1-\alpha_j)\)</span>, and <span class="math notranslate nohighlight">\(\delta_i = t_{i+1}-t_i\)</span> is the distance between adjacent sample points. In words, <span class="math notranslate nohighlight">\(\alpha_i\)</span> is the probability of the ray terminating in segment <span class="math notranslate nohighlight">\(i\)</span> (given density <span class="math notranslate nohighlight">\(\sigma_i\)</span> over interval <span class="math notranslate nohighlight">\(\delta_i\)</span>) and <span class="math notranslate nohighlight">\(T_i\)</span> is the transmittance up to the start of segment <span class="math notranslate nohighlight">\(i\)</span>. This formula is essentially the standard front-to-back alpha compositing: each sample’s color contributes, but exponentially farther samples (or those behind high density) contribute less.</p>
<p>NeRF’s implementation ensures that as <span class="math notranslate nohighlight">\(N \to \infty\)</span> with appropriately small <span class="math notranslate nohighlight">\(\delta_i\)</span>, <span class="math notranslate nohighlight">\(\hat{C}(\mathbf{r})\)</span> converges to the true integral <span class="math notranslate nohighlight">\(C(\mathbf{r})\)</span>. In practice <span class="math notranslate nohighlight">\(N\)</span> is on the order of 64 (coarse pass) + 128 (fine pass). The discrete formulation is differentiable with respect to the network outputs <span class="math notranslate nohighlight">\(\sigma_i, \mathbf{c}_i\)</span> since it’s just a chain of multiplications and exponentials. Thus, one can backpropagate the image error gradients through <span class="math notranslate nohighlight">\(\hat{C}(\mathbf{r})\)</span> to each sample’s properties and further into the network parameters <span class="math notranslate nohighlight">\(\Theta\)</span>.</p>
<p><strong>Interpretation</strong>: The density <span class="math notranslate nohighlight">\(\sigma(\mathbf{x})\)</span> learned by NeRF often corresponds to actual scene surfaces, but blurred into a soft occupancy volume. In theory, a scene with opaque surfaces would have <span class="math notranslate nohighlight">\(\sigma \to \infty\)</span> at the surface (a Dirac delta), but NeRF’s MLP represents a smoothed version of geometry (like a translucent shell). Training encourages the MLP to concentrate density in thin regions that explain the occlusions in the images, but due to the continuous representation and MSE loss, it typically finds a reasonable approximation (surfaces of a few mm thickness with high density). The color <span class="math notranslate nohighlight">\(\mathbf{c}(\mathbf{x},\mathbf{d})\)</span> represents the view-dependent radiance, which can encode effects like specular reflectance by varying with <span class="math notranslate nohighlight">\(\mathbf{d}\)</span>.</p>
<p>The use of volume rendering (as opposed to a surface rendering equation) was a clever choice: it bypasses the need to explicitly identify surface geometry or do rasterization. Additionally, volume rendering’s differentiability and continuity make optimization smoother. Max (1995) noted that volumetric compositing is equivalent to traditional alpha blending for emission-absorption models, which NeRF leverages.</p>
<p>It’s worth emphasizing that NeRF does not model more complex light transport (no shadows, indirect light, or reflection between surfaces); it assumes each point emits independently. Essentially, NeRF deals with radiance as observed from the cameras, baking in all lighting effects present in the images. This is fine for view interpolation but means NeRF is not inherently a physical model of lighting – it won’t automatically handle changing illumination, etc., without extensions.</p>
</section>
<section id="positional-encoding-and-neural-network-architecture">
<h3>Positional Encoding and Neural Network Architecture<a class="headerlink" href="#positional-encoding-and-neural-network-architecture" title="Link to this heading"></a></h3>
<p>One of the key technical contributions in NeRF is the use of positional encoding (PE) to map input coordinates to a higher-dimensional space before feeding them to the MLP. This addresses the issue that standard neural networks are biased toward learning low-frequency functions (Rahaman et al., 2019) – a phenomenon that would cause an MLP to struggle with representing fine details like sharp edges or high-frequency textures. NeRF’s positional encoding, also known as a type of Fourier feature mapping (Tancik et al., 2020), enables the network to represent high-frequency variation by providing it with sinusoidal basis functions of the inputs.</p>
<p><strong>Definition</strong>: For each component of a 3D coordinate <span class="math notranslate nohighlight">\(\mathbf{x}=(x,y,z)\)</span> and for each component of the 2D viewing direction <span class="math notranslate nohighlight">\(\mathbf{d}\)</span>, NeRF applies the encoding:</p>
<div class="math notranslate nohighlight">
\[\gamma(p)=(\sin(2^0\pi p),\cos(2^0\pi p),\sin(2^1\pi p),\cos(2^1\pi p),\ldots,\sin(2^{L-1}\pi p),\cos(2^{L-1}\pi p)).\]</div>
<p>This is done separately for each scalar coordinate <span class="math notranslate nohighlight">\(p\)</span> (with <span class="math notranslate nohighlight">\(L\)</span> frequencies). Mildenhall et al. used <span class="math notranslate nohighlight">\(L=10\)</span> for position components and <span class="math notranslate nohighlight">\(L=4\)</span> for direction, meaning the 3D position is expanded to <span class="math notranslate nohighlight">\(3\times 2L = 60\)</span> dimensions and the 3D direction to <span class="math notranslate nohighlight">\(3\times 2L = 24\)</span> dimensions (sometimes they also include the raw <span class="math notranslate nohighlight">\(p\)</span> as well). The intuition is that these sinusoids of increasing frequency allow the network to produce outputs that vary rapidly with input – the first few frequencies capture coarse variation, the higher frequencies allow fine detail. Without PE, a deep network would tend to approximate a low-frequency version of the target function, requiring many more layers or neurons to encode sharp changes. Empirically, NeRF with PE could fit high-detail scenes whereas the same network without PE produced blurry results.</p>
<p>The positional encoding can be viewed as providing a rich set of basis functions that span a wide range of spatial frequencies, which the MLP can then linearly combine in its first layer. Tancik et al. (2020) showed that a random Fourier feature mapping of inputs effectively gives the network a kernel that is capable of representing high-frequency variations in the target function. NeRF’s chosen frequencies <span class="math notranslate nohighlight">\(2^k\)</span> (for <span class="math notranslate nohighlight">\(k=0,...,L-1\)</span>) are a deterministic mapping (not learned), but some later works make the encoding learned or use multiresolution hash grids (see Instant NGP in the extensions section).</p>
<p><strong>Network architecture</strong>: After positional encoding, the encoded position (60-D) is input to an 8-layer MLP (256 units each) with ReLU activations. They include a skip connection that feeds the input of layer 1 into layer 5 (concatenating to the feature vector) – this helps gradients flow and allows later layers to directly access the original coordinates (which can be useful for very fine details that might otherwise be “forgotten” after many layers). The MLP outputs a single scalar <span class="math notranslate nohighlight">\(\sigma\)</span> (density) and a 256-D feature vector. This feature is concatenated with the encoded viewing direction (24-D) and passed to a final 1-layer MLP (128 units) that outputs the RGB color.</p>
<p>So effectively the network is two-headed: one head produces density (view-independent), and one produces view-dependent color. The separation ensures that the density (geometry) doesn’t arbitrarily change with viewing angle, which would make multi-view consistency impossible. Instead, all view-dependent effects must be explained by the color head. In practice, the color head can model phenomena like specular reflection by learning functions that vary with <span class="math notranslate nohighlight">\(\mathbf{d}\)</span>. For example, in a shiny region, the density will represent the surface location, and the color head will output brighter color for view directions near the mirror-reflection direction of a light source, reproducing a highlight.</p>
<p><strong>Activation functions</strong>: NeRF used ReLU activations in most layers. The density output used ReLU (or its exponential for positivity – many implementations use an exponential on the raw density output to ensure <span class="math notranslate nohighlight">\(\sigma \ge 0\)</span> since density should be non-negative). The color output used a sigmoid or simply ReLU-clamped outputs to [0,1] (since they normalized pixel values). They also added a small noise to predicted densities during training to stabilize and discourage “floater” artifacts (a form of regularization).</p>
<p><strong>Why an MLP?</strong> The choice of a fully-connected network to represent a function over <span class="math notranslate nohighlight">\(\mathbb{R}^5\)</span> is interesting. A large enough MLP is a universal function approximator, and with positional encoding, it can approximate the highly complex radiance field. Alternative choices could have been a large 5D tensor (impossible to store for high resolution) or some hybrid (which later works explore). The MLP has the benefit of being memory-efficient (NeRF’s network is only ~5 MB, much smaller than storing a detailed voxel grid). It’s also continuous – it can be sampled at arbitrary coordinates. However, MLP inference is relatively slow, which is why rendering took so long. But at training time, the cost was manageable with modern GPUs since each sample is independent and MLPs vectorize well.</p>
</section>
<section id="loss-function-and-optimization">
<h3>Loss Function and Optimization<a class="headerlink" href="#loss-function-and-optimization" title="Link to this heading"></a></h3>
<p><strong>Loss function</strong>: NeRF’s loss is straightforward: for each sampled ray (pixel), the rendered color <span class="math notranslate nohighlight">\(\hat{C}(\mathbf{r})\)</span> is compared to the ground truth <span class="math notranslate nohighlight">\(C_{\text{gt}}\)</span> from the input image, and they minimize the sum of squared errors (L2 loss):</p>
<div class="math notranslate nohighlight">
\[L=\sum_{\text{rays}}\|\hat{C}(\mathbf{r})-C_{\text{gt}}(\mathbf{r})\|_2^2.\]</div>
<p>They observed that this simple photometric loss was sufficient to achieve excellent results. No explicit regularization on the learned densities or colors was needed (aside from the noise added to <span class="math notranslate nohighlight">\(\sigma\)</span> early in training to reduce floaters, and an implicit regularization from the coarse-to-fine sampling). The reasoning is that any deviation of the predicted image from the real image will cause a direct loss, and because the scene must be explained from all viewpoints, the solution that minimizes error is usually to correctly model the scene’s geometry and appearance.</p>
<p>There is, however, an inherent ambiguity in how the network can explain a set of images – e.g., a brighter surface with lower opacity versus a darker surface with higher opacity can yield the same pixel colors. NeRF largely avoided these shape-radiance ambiguities by how the volume rendering formulation is set up and by injecting noise to densities (which pushes the solution toward finding opacity at surfaces rather than floating semi-transparent clouds). NeRF++ (discussed later) analyzed these ambiguities in more detail.</p>
<p><strong>Optimization</strong>: NeRF is trained using stochastic gradient descent (Adam optimizer) on the above loss. The training is done per-scene (the network is not meant to generalize to new scenes; it “overfits” to one scene). Typically, a batch consists of a number of random rays from the training images (for example, each batch might take 1024 rays from random images). Each ray samples e.g., 64 points for the coarse model and 128 for the fine model; those are fed through the network. The network weights are updated by Adam with a learning rate that may start around <span class="math notranslate nohighlight">\(5\times10^{-4}\)</span> and decay over time. The authors reported training 100k – 300k iterations, which for their implementation took on the order of 1–2 days on a GPU per scene (the exact time depending on image resolution and number of rays per batch). In contrast, a competing method LLFF took only minutes to process a scene but at the cost of much lower quality. Thus, NeRF introduced a new time vs. quality trade-off in 3D reconstruction: one can invest significant compute to optimize an implicit model that then yields exceptional quality novel views.</p>
<p>One important detail: NeRF requires accurate camera poses for all images. These are usually obtained via structure-from-motion (SfM) tools like COLMAP. NeRF does not estimate poses itself (though later works have tackled pose refinement within NeRF). If the poses are wrong, NeRF’s results degrade because the network is trying to explain inconsistent viewpoints.</p>
<p>In summary, the original NeRF formulation is elegant in its simplicity: a plain L2 reconstruction loss on images, a differentiable volumetric rendering procedure, and a simple fully-connected network with a frequency encoding. This simplicity belies the complexity of what the network is achieving: essentially performing a form of analysis-by-synthesis – it explains the input images by constructing a model that could have generated them. By training until convergence, NeRF’s model often encodes the scene’s geometry (in its density field) and appearance (in its color field) very accurately.</p>
</section>
</section>
<section id="major-advancements-and-extensions-of-nerf">
<h2>Major Advancements and Extensions of NeRF<a class="headerlink" href="#major-advancements-and-extensions-of-nerf" title="Link to this heading"></a></h2>
<p>Since the original NeRF paper, there has been a surge of research addressing NeRF’s limitations, improving its quality, speed, and extending it to new domains. We outline several major developments: anti-aliasing and scene scaling (mip-NeRF, NeRF++), efficiency improvements (Instant Neural Graphics Primitives, PlenOctrees), dynamic scenes (deformable/dynamic NeRFs for moving content), and integration with human body models (enabling controllable human avatars). Each of these advances builds on NeRF’s foundation, adjusting the representation or training procedure to broaden its applicability.</p>
<section id="anti-aliasing-and-unbounded-scenes-mip-nerf-and-nerf">
<h3>Anti-Aliasing and Unbounded Scenes: mip-NeRF and NeRF++<a class="headerlink" href="#anti-aliasing-and-unbounded-scenes-mip-nerf-and-nerf" title="Link to this heading"></a></h3>
<p><strong>NeRF++ (2020)</strong>: One assumption in NeRF was that scenes are bounded (contained in a finite volume). For 360° captures of objects, NeRF worked well, but for unbounded scenes (e.g., outdoor panoramas where background goes to infinity), NeRF struggled. NeRF++, introduced by Zhang et al. (2020), analyzed NeRF and proposed improvements for these cases. NeRF++ addressed the parametrization issue when modeling large scenes by splitting the space into an “inner region” (near the cameras) and an “outer region” (far background) and using different parameterizations for each. For the outer region (like sky or distant scenery), they used an inverted sphere parameterization (mapping rays to a finite sphere for background). This allowed NeRF++ to render scenes with an appropriate treatment of infinity (the background essentially acts like a distant environment map).</p>
<p>They also discussed the shape-radiance ambiguity: the fact that an incorrect geometry combined with adjusted color could still explain images. NeRF++ noted that NeRF inherently avoids some ambiguities by how the volume rendering works (preferring the closest surface along a ray to take the color), but to further discourage incorrect solutions they added a regularization (random noise) to the density output during training. The result was improved fidelity for 360° outdoor captures – NeRF++ could handle a camera that spins around on the spot, seeing a full panorama, whereas NeRF (original) would have trouble representing the distant parts. NeRF++ also demonstrated better quality on thin structures and avoided some artifacts present in NeRF.</p>
<p><strong>mip-NeRF (2021)</strong>: Another issue with NeRF is aliasing: if a scene has fine details (e.g., a picket fence) and input images are at different resolutions or if a render camera zooms out, NeRF can produce blurry or flickering results. This is because NeRF sampled only a single ray per pixel, effectively point-sampling the scene function. The straightforward fix of supersampling (many rays per pixel) would be extremely expensive for NeRF (since each ray requires hundreds of network queries).</p>
<p>Mip-NeRF (Barron et al., 2021) introduced a solution by rendering cone-shaped rays instead of infinitesimal rays. They treat each camera ray as a cone that covers a finite area of the scene, especially noticeable when a pixel covers a large area in the scene (e.g., distant objects). Instead of sampling a single point <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> along a ray, mip-NeRF samples a conical frustum and approximates the integral of the NeRF function over that volume. Concretely, they represent a conical frustum as a multivariate Gaussian in space and derive a closed-form integrated positional encoding (IPE) that computes <span class="math notranslate nohighlight">\(E[\gamma(\mathbf{x})]\)</span> – the expected positional encoding over that Gaussian distribution. This IPE can be fed into the network to produce outputs that are effectively averaged over the cone’s cross-section.</p>
<p>The result is an anti-aliased rendering: high-frequency details smaller than a pixel are appropriately averaged (not erroneously point-sampled), preventing Moiré patterns or excessive blur when zooming. Mip-NeRF also merged the coarse and fine networks into one – since they now sample multiple levels of detail inherently by the cone integration, a single network sufficed (making it actually slightly faster and smaller than NeRF). Experiments showed significant improvement: on a multiscale synthetic dataset, mip-NeRF reduced error ~60% compared to NeRF. It also was more robust when the training images had varying distance or resolution (which would confound NeRF).</p>
<p>Mip-NeRF is fully backwards compatible with NeRF – if all pixels are the same scale, it reduces to NeRF. In essence, mip-NeRF introduced a level of scale awareness into NeRF’s representation, borrowing ideas from mipmapping in classic graphics (hence the name). An extension, Mip-NeRF 360 (Barron et al. 2022), later combined mip-NeRF and NeRF++ ideas to handle unbounded, anti-aliased 360° scenes, introducing a non-linear scene parametrization and other improvements.</p>
<p>In summary, NeRF++ and mip-NeRF tackled two important practical issues: representing unbounded scene extent and handling aliasing/mipmapping. These works improved NeRF’s accuracy (by resolving ambiguities and alias artifacts) and applicability (to outdoor and zooming scenarios) without fundamentally changing the NeRF concept of an MLP + volume render. Both have become standard components in subsequent NeRF systems.</p>
</section>
<section id="efficiency-improvements-instant-nerf-and-plenoctrees">
<h3>Efficiency Improvements: Instant NeRF and PlenOctrees<a class="headerlink" href="#efficiency-improvements-instant-nerf-and-plenoctrees" title="Link to this heading"></a></h3>
<p>The original NeRF, while impressive in quality, was computationally intensive. Training took hours to days per scene, and rendering even a single image could take seconds to minutes on a GPU (since hundreds of network evaluations are needed per pixel). A major thrust of research has been making NeRF faster – both faster to train and faster to render – to bring it closer to real-time use.</p>
<p><strong>Instant Neural Graphics Primitives (Instant-NGP, 2022)</strong>: A breakthrough in speed came from Müller et al. (NVIDIA) with Instant Neural Graphics Primitives, often called Instant NeRF. They introduced a new input encoding and data structure that accelerates learning by orders of magnitude. Instead of a fixed sinusoidal positional encoding, they used a multi-resolution hash grid of trainable feature vectors. In practice, they allocate several levels of a sparse voxel grid (at exponentially increasing resolutions); each level stores a hash table of feature vectors. An input coordinate <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is mapped to each level by finding the surrounding voxel’s feature (using a hash of the coordinates to index the table), and trilinearly interpolating features if needed. These features from all levels are concatenated to form the input to a small MLP. The hash tables are trainable parameters that are optimized along with the MLP.</p>
<p>This approach dramatically reduces the network size (their MLP had maybe only a few layers with tens of neurons) because the spatial complexity is mostly captured by the grid, not the neural network. The multi-resolution aspect ensures both coarse and fine details can be represented. Müller et al. implemented this with a highly optimized CUDA code, achieving a combined speedup of several thousand-fold in some cases: training a NeRF in a few seconds and rendering at ~60 FPS in 1080p. Essentially, Instant-NGP trades some memory (for the feature grids) to gain immense speed. The quality remained high – in fact often on par or even better than original NeRF – because the grid can represent high-frequency details more directly than a deep MLP.</p>
<p>This work showed that NeRFs don’t have to be slow. The hash grid idea is a form of learned multiresolution voxel representation, and it has inspired many follow-ups (it’s related to earlier concepts like Sparse Voxel DAGs and Octrees, but with learned features and a hash to keep it memory-efficient). To put numbers: Instant NeRF can train on the NeRF synthetic dataset in a few seconds to a minute (where NeRF took 1-2 days). It made NeRF interactive, enabling uses like quickly scanning an object with a phone and getting a 3D model immediately (NVIDIA even released a tool around it). The approach also extended beyond radiance fields (they applied it to SDFs, images, etc., hence “neural graphics primitives”). The key takeaway is that explicit spatial feature structures (like grids or caches) can massively accelerate NeRF, moving away from the heavy MLP-only approach.</p>
<p><strong>PlenOctrees (2021)</strong>: Another approach to speed is to accelerate rendering by converting a trained NeRF into an explicit data structure that is faster to ray-march. PlenOctrees (Alex Yu et al., 2021) proposed taking a trained NeRF and precomputing an octree that stores radiance field information. Specifically, they optimized NeRF to output spherical harmonics (SH) coefficients for color instead of raw RGB. A spherical harmonic basis can represent view-dependent lighting (like a low-frequency approximation of the reflectance lobes). By doing so, they effectively remove the view direction input from the network – the NeRF outputs, for each point, a density and SH coefficients encoding how color varies with view.</p>
<p>These outputs can then be sampled onto a 3D octree: each node in the octree contains a density and a set of SH coefficients for color. The octree can be rendered by standard volume rendering techniques extremely fast: since it’s just a bunch of voxels, one can traverse the octree along rays (skipping empty space efficiently) and at each sample compute color by evaluating the SH (a small dot product) instead of an expensive network. The result was real-time rendering: PlenOctrees achieved &gt;150 FPS at 800×800 resolution on a high-end GPU, which is &gt;3000× faster than the original NeRF rendering. They reported even mobile devices could render at tens of FPS.</p>
<p>Importantly, this speed gain came after relatively slow training (they still had to train a NeRF or similar first). But they also showed a further step: one can optimize the octree directly (fine-tuning the voxel data to minimize the error) to potentially bypass some of the neural network training. The quality of PlenOctree rendering was on par with NeRF (since it was distilled from NeRF). View-dependent effects were preserved thanks to the SH representation. Essentially, PlenOctrees traded the compactness of NeRF for speed: the octree representation can be quite large in memory (hundreds of MB for a scene, versus NeRF’s 5 MB network) because it explicitly stores a volume. But for applications where inference speed is crucial (like VR/AR), this is a worthwhile trade-off.</p>
<p>Other notable efficiency improvements include Plenoxels (Fridovich-Keil et al. 2022) which cut out the network entirely and directly optimized a sparse grid of density + spherical harmonic coefficients, achieving fast training and fast rendering (within seconds, similar to Instant-NGP, but with a simpler optimization scheme). Also, NSVF (Neural Sparse Voxel Fields) by Liu et al. (2020) used a sparse voxel octree during training to constrain NeRF’s space and accelerated rendering via a hybrid explicit-implicit approach. The general trend is moving toward explicit data structures (grids, octrees, etc.) to assist or replace the neural network. However, even these approaches owe a debt to NeRF’s formulation: they typically still leverage volumetric rendering and often keep a small MLP for interpolation.</p>
<p>In summary, thanks to these innovations, we now have NeRF-like models that are several orders of magnitude faster than the original, making it feasible to use NeRFs in real-time applications. Instant-NGP in particular has become a go-to method for quickly getting NeRF results, and PlenOctrees/Plenoxels demonstrate that once a scene is learned, it can be converted for real-time display. The performance gains came with some trade-offs (memory or preprocessing), but ongoing research continues to close the gap, aiming for both compactness and speed.</p>
</section>
<section id="dynamic-and-deformable-nerfs-d-nerf-nerfies-nsff-etc">
<h3>Dynamic and Deformable NeRFs (D-NeRF, Nerfies, NSFF, etc.)<a class="headerlink" href="#dynamic-and-deformable-nerfs-d-nerf-nerfies-nsff-etc" title="Link to this heading"></a></h3>
<p>The original NeRF (and most early derivatives) assumed a static scene – the input images are all capturing a scene where nothing but the camera moves. Extending NeRF to handle dynamic scenes (where the scene changes over time, such as moving objects or people) is a crucial step toward applications like 3D video, VR/AR with dynamic content, and performance capture. Several key works have addressed this, introducing Dynamic NeRFs that add time or deformation as an additional dimension in the radiance field.</p>
<p><strong>D-NeRF (2020)</strong>: One of the first such works was D-NeRF: Neural Radiance Fields for Dynamic Scenes by Pumarola et al. (2021). D-NeRF’s setting is a monocular video of a moving scene (e.g., a person moving, or a non-rigid object deforming) with known camera trajectory. The challenge is that each frame is a different scene state, so you can’t just throw all images into a static NeRF – you’d get blur or an average of all states. D-NeRF’s solution is to encode time as an input and learn a continuous deformation model. They consider time <span class="math notranslate nohighlight">\(t\)</span> as an extra input coordinate and split the problem into two parts: (1) a canonical NeRF (scene representation at a reference time), and (2) a deformation field that maps any point from canonical space to its pose at time <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>In other words, D-NeRF learns a function <span class="math notranslate nohighlight">\(F_{\text{canon}}(\mathbf{x}) = (\sigma,\mathbf{c})\)</span> for the scene in canonical pose, and another function <span class="math notranslate nohighlight">\(G(\mathbf{x}, t) = \Delta \mathbf{x}\)</span> that warps a 3D point from canonical to the configuration at time <span class="math notranslate nohighlight">\(t\)</span>. When rendering a ray at time <span class="math notranslate nohighlight">\(t\)</span>, they first map sample points from the live frame back to canonical space, query the NeRF there, and composite as usual. Both the NeRF and deformation network are trained jointly to minimize the photometric error across all frames. This is akin to a motion-compensated NeRF – it separates shape from motion.</p>
<p>D-NeRF demonstrated the ability to render novel views at novel times: one can smoothly interpolate time (getting slow-motion or future frames) and view the scene from any angle, effectively reconstructing a 4D space-time radiance field. The results included dynamic synthetic scenes (like a moving humanoid shape) and simple real sequences. A key difficulty is that with only a single camera, the network must learn to infer occluded regions over time – the canonical representation helps because each point’s true color is learned from when it’s visible in some frame. D-NeRF had to combat issues of ambiguity (it could trade off deforming geometry vs. changing colors). They found it important to regularize the deformation field (e.g., assume it’s smooth or small) to get plausible geometry. Overall, D-NeRF showed the viability of NeRF for dynamic scenes with non-rigid motion, given multi-view or time-varying input.</p>
<p><strong>Nerfies (Deformable NeRF, 2021)</strong>: Park et al. concurrently developed Nerfies, focusing on casually captured human portraits (selfies) with slight movement. Their approach also introduced a continuous deformation field <span class="math notranslate nohighlight">\(W(\mathbf{x}, t)\)</span> that warps points into a canonical space. They found that a simple translation field per point was not sufficient and used an SE(3) (rigid) deformation per point to allow rotations (improving stability). They also introduced an elastic regularization inspired by as-rigid-as-possible deformation in graphics, to keep the learned warps reasonable. Nerfies optimized both the canonical radiance field and the deformation field, and used a coarse-to-fine strategy to avoid bad local minima.</p>
<p>The term “nerfies” was used to describe the resulting animated NeRF models of people, which could be rendered from new viewpoints. Nerfies showed convincing results on faces making expressions and small head motions – things where the motion is non-rigid but not too large. They also built a two-camera capture rig for evaluation (so they had two views at the same time for validation). Compared to D-NeRF, Nerfies put more emphasis on regularizing deformations (using elasticity and enforcing that distant points don’t move too differently). The two approaches are similar in spirit and were developed simultaneously. Both demonstrate that adding a learned deformation per frame lets NeRF handle non-rigid scene changes.</p>
<p><strong>Neural Scene Flow Fields (2021)</strong>: Li et al. took a slightly different route with NSFF: Neural Scene Flow Fields for dynamic view synthesis. Instead of an explicit canonical space, they estimated 3D scene flow between consecutive frames and used it to align points. Essentially, NSFF learns per-frame NeRFs plus forward/backward optical flow in 3D, ensuring temporal consistency by penalizing differences between flow-predicted positions and NeRF-predicted geometry. This method was able to take a monocular video and produce a dynamic NeRF without multi-view at each time, by cleverly using the scene flow as a supervisory signal for geometry. NSFF could handle more challenging scenarios like dynamic outdoor scenes (e.g., cars and people moving in street scenes captured by a single driving camera) by leveraging the regularizing power of scene flow.</p>
<p>There are many other dynamic NeRF extensions: NR-NeRF (Tretschk et al. 2021) which also did non-rigid with regularization, Video NeRF approaches that use multi-view videos, Time-of-Flight NeRF (attaching time dimensions), etc. A particularly challenging scenario is where even the lighting changes over time – most dynamic NeRFs assume constant illumination. Some works address that by decomposing radiance into reflectance and illumination, but that’s beyond our scope here.</p>
<p>Common challenges in dynamic NeRFs: ensuring temporal coherence, dealing with occlusions, and the sheer increase in data/complexity (a 5D radiance field becomes 6D with time). Most solutions introduce either an explicit deformation model (which imposes coherence) or a prior like scene flow or a parametric model (e.g., body model, next section). Many dynamic NeRFs also require more data – e.g., multi-view video, or at least knowing the motion via another method – because with a single video there’s an inherent ambiguity in what’s moving vs. what’s static (structure-from-motion itself becomes tricky if the scene moves).</p>
<p>Despite these challenges, by 2021 we saw that NeRFs can indeed be made to handle dynamic scenes, opening the door to 4D reconstruction (3D + time). For example, D-NeRF’s learned canonical model plus deformation essentially yields a 3D model that can be animated (within the range of observed motions) – a primitive form of a captured 3D animation.</p>
</section>
<section id="neural-radiance-fields-for-human-modeling-with-smpl-and-body-models">
<h3>Neural Radiance Fields for Human Modeling (with SMPL and Body Models)<a class="headerlink" href="#neural-radiance-fields-for-human-modeling-with-smpl-and-body-models" title="Link to this heading"></a></h3>
<p>One high-value application of NeRFs is capturing human performances in 3D – for virtual telepresence, VFX, games, etc. However, human bodies are non-rigid and can take on many poses, so a NeRF of a person in one pose might not generalize to other poses. To address this, researchers have combined NeRFs with parametric human body models like SMPL (Loper et al., 2015), which provides a skeletal pose and shape prior for the human. The idea is to leverage the known structure of human geometry to constrain the NeRF, making it controllable by pose parameters and generalizable to new poses.</p>
<p><strong>Neural Body (2021)</strong>: Peng et al. introduced Neural Body, which embeds a deformable human model (SMPL) into a NeRF representation. In Neural Body, each vertex of the SMPL mesh has a learned latent feature vector (this is a “structured latent code” attached to the body). For a given frame with a certain pose, the SMPL model is posed (its vertices move to new positions). Those latent features move accordingly (attached to the bones). Then, for any spatial point <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, to query the NeRF we do the following: transform <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> into the SMPL’s local coordinate system (basically find where that point would be on the canonical T-pose body by applying the inverse skeletal pose). Interpolate the latent codes of nearby SMPL vertices to get a feature for <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. Feed that feature (plus <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and view direction) into an MLP to output density and color.</p>
<p>This way, the radiance field is conditioned on the pose – because the latent codes are fixed to the body parts. At training time, they optimize those latent codes and the MLP so that for all training images (of a person moving), the model produces correct renderings. The use of SMPL ensures that when the person moves to a new pose, the same learned radiance field can be applied, by simply moving the latent codes with the body.</p>
<p>Neural Body demonstrated that with multi-view video training (using the ZJU-MoCap dataset they created, which has 21 cameras capturing people in motion), the system can generate novel views of the person in unseen poses with realistic detail. Essentially, Neural Body learned a dynamic neural avatar: it disentangles human identity/appearance (stored in the latent codes and MLP weights) from pose (controlled by the SMPL parameters).</p>
<p>One advantage of this approach is it enforces consistency across frames – unlike training a separate NeRF per frame (which would treat each frame independently, possibly yielding inconsistent geometry), Neural Body uses one model for all frames, leading to a coherent 3D representation that explains the whole sequence. They found it outperformed prior methods including per-frame NeRF and neural volumetric video (Lombardi’s Neural Volumes) in both quality and ability to handle novel poses. Notably, they showed better geometry and appearance consistency (evaluated against ground-truth scans in ZJU-MoCap).</p>
<p><strong>Animatable NeRF / A-NeRF (2021)</strong>: Concurrently, A-NeRF by Su et al. (NeurIPS 2021) took a similar approach: they equipped a NeRF with a skeleton so that it can be posed. A-NeRF’s key idea was to apply the inverse of forward kinematics to the NeRF’s coordinate input. They define a coordinate system for each bone of the skeleton and learn a latent code in those local frames (or some neural features in a spatial grid around each bone). For a query point, they figure out which bone’s local frame it lies in (by inverting the pose transform for that bone) and use the associated features. Their “inverse kinematics for implicit models” ensures the model can be driven by new poses, similar to Neural Body’s approach but formulated slightly differently. They also refine the pose estimation as part of training (so the method can improve the given pose data). The outcome is a generative model for a human that can synthesize novel views and poses from monocular video.</p>
<p>In essence, A-NeRF and Neural Body both create a NeRF that is conditioned on pose in a learned, part-based manner. The SMPL model provides geometry prior (shape of the person, initial guess for where each point belongs on the body), which dramatically reduces the complexity of learning – the network doesn’t have to figure out human anatomy from scratch, it’s given a template.</p>
<p><strong>Other human NeRF models</strong>: Xu et al. (2022) proposed a Surface-Aligned NeRF – instead of latent codes at vertices, they map query points onto the SMPL mesh surface and use the barycentric coordinates plus a height above the surface to condition the NeRF. This gave a more direct physical meaning to the coordinates and improved generalization to novel poses. Peng et al. also had Neural Human Performer and others that improved fidelity and training. HumanNeRF (Weng et al., 2022) combined SMPL with NeRF in a way that even a single video could be used to train a human avatar by enforcing symmetry and other constraints. Another line of work, not fully NeRF but related, is using implicit surfaces with NeRF – e.g., NeuS (Wang et al., 2021) learn an SDF for the human shape and a radiance field on it. There’s also work integrating texture maps or UV maps from SMPL with NeRF to factor appearance.</p>
<p><strong>Benefits and challenges</strong>: Using a body model makes the problem more tractable and the result controllable – one can drive the learned model with motion capture data or new pose parameters, making it very practical. The challenge is that the quality must compete with image-based rendering or graphics avatars. Current NeRF-human models can produce free-viewpoint videos of a person with realistic clothes and hair, which is impressive. But they might struggle with very loose clothing or long hair (which don’t follow the rigid bones of SMPL) – some methods add secondary deformation fields to handle that. There’s also the question of speed – many of these are still slow to train, though some can render quickly if converted to explicit form (e.g., Neural Body could be accelerated by caching features on the mesh).</p>
<p>Overall, combining NeRF with strong geometric priors like SMPL is a compelling direction, as it brings us closer to controllable neural actors. The NeRF contributes photorealistic rendering (something traditional graphics models have to work hard to achieve for real people), and the SMPL contributes semantic structure (body parts, pose) and generalization. We can expect further work to improve the generalization (e.g., train on many people to get a model that can synthesize arbitrary new people with given shape and pose – some recent works do this with conditional NeRF). Already, results like Neural Body show high-quality novel view synthesis for human motions, and the approach outperforms prior geometry-based methods (like COLMAP or PIFuHD) in capturing dynamic humans.</p>
</section>
<section id="other-notable-extensions">
<h3>Other Notable Extensions<a class="headerlink" href="#other-notable-extensions" title="Link to this heading"></a></h3>
<p>Beyond what was detailed above, NeRF has seen numerous other extensions:</p>
<ul class="simple">
<li><p><strong>NeRF in the Wild (NeRF-W)</strong> by Martin-Brualla et al. handled uncontrolled photo collections by modeling illumination and transient objects</p></li>
<li><p><strong>PixelNeRF</strong> and other generalization methods trained networks to predict NeRFs from as few as one image</p></li>
<li><p><strong>Semantic NeRFs</strong> integrated semantic labels</p></li>
<li><p><strong>Editing NeRFs</strong> allow shape or appearance modifications via learned latent spaces or by manipulating underlying representations (e.g., using point cloud intermediates)</p></li>
<li><p><strong>Hybrid models</strong> like NeRF-SDF that combine implicit surfaces with radiance fields for better geometry</p></li>
</ul>
</section>
</section>
<section id="comparison-with-other-3d-representations">
<h2>Comparison with Other 3D Representations<a class="headerlink" href="#comparison-with-other-3d-representations" title="Link to this heading"></a></h2>
<p>NeRFs represent a new point on the spectrum of 3D scene representation, distinct from classical explicit models. Here we compare NeRF and its variants with other approaches in terms of performance, accuracy, efficiency, and applicability:</p>
<section id="vs-polygonal-meshes">
<h3>Vs. Polygonal Meshes<a class="headerlink" href="#vs-polygonal-meshes" title="Link to this heading"></a></h3>
<p>Meshes (with textures/materials) are very efficient to render with graphics pipelines (real-time achievable) and are the standard in AR/VR applications. They provide explicit surfaces and allow physical simulation or collision detection easily. Compared to NeRF, meshes are parametric (finite list of vertices), whereas NeRF is implicit (infinite continuous field).</p>
<p>In terms of accuracy, a NeRF can capture subtleties like soft shadows, transparency, or view-dependent reflectance directly from images, which a mesh + static texture cannot (one would need complex material/lighting estimation). Mesh reconstruction from images can also struggle with thin structures or non-Lambertian surfaces; NeRF tends to do better in those cases by modeling them as radiance density. However, NeRF lacks an explicit surface – extracting a mesh from NeRF (via marching cubes on the density) can be noisy or less accurate on fine details (though techniques like NeuS improve that).</p>
<p>Efficiency: Mesh pipelines are currently far ahead for realtime – NeRF required significant innovations to approach real-time rendering. Also, NeRF’s memory footprint is small (if just an MLP) but the computation per view is large, whereas a mesh’s memory might be larger (store all vertices) but computation per view is minimal. In terms of editing, meshes are straightforward to deform or edit with existing tools, while editing a NeRF is non-trivial (researchers are working on NeRF editing tools, often converting to mesh or point cloud first).</p>
<p>In summary, NeRFs excel in visual fidelity and automatic scene capture, whereas meshes excel in interactivity, explicitness, and integration into existing graphics pipelines. It’s likely that hybrid approaches will combine them (e.g., using NeRF for rendering appearance on top of a mesh that provides geometry and collision).</p>
</section>
<section id="vs-point-clouds-3d-splatting">
<h3>Vs. Point Clouds / 3D Splatting<a class="headerlink" href="#vs-point-clouds-3d-splatting" title="Link to this heading"></a></h3>
<p>Point clouds alone are not a complete rendering solution without additional measures (like splatting or reconstructing a surface). However, recent methods like 3D Gaussian splatting (Kerbl et al. 2023) have shown that rendering a point-based representation with learned anisotropic Gaussians can achieve quality on par with NeRF at vastly lower rendering cost. These methods optimize a set of points (with position, orientation, radius, color) to fit the input images, rather than a network. The result is essentially an explicit NeRF: a cloud of thousands of translucent discs that approximate the radiance field. They can be rendered extremely fast with graphics techniques (rasterization of ellipses), even allowing real-time performance, and are editable (one can move points, etc.).</p>
<p>The quality, surprisingly, matched or exceeded NeRF on some scenes, and the training is also quite fast (minutes). The drawback is memory – storing millions of point primitives can be heavy (though still often &lt; memory of a dense voxel grid). Point clouds also don’t inherently handle occlusion ordering without a rendering algorithm (but splatting algorithms do handle it by depth-sorting or using depth test).</p>
<p>Compared to NeRF MLP, point-based representations sacrifice the compactness and implicit continuity for direct speed and editability. For performance, methods like Gaussian splats have essentially caught up in quality and far exceed NeRF in rendering speed (5-10 ms per image vs. seconds). This suggests that for many practical purposes, one might convert a NeRF into a point-based format (just as PlenOctree does with voxels) to deploy it.</p>
</section>
<section id="vs-voxel-grids-and-volumetric-methods">
<h3>Vs. Voxel Grids and Volumetric Methods<a class="headerlink" href="#vs-voxel-grids-and-volumetric-methods" title="Link to this heading"></a></h3>
<p>A voxel grid representation of a radiance field (with color and density at each cell) is conceptually similar to NeRF but discretized. If one had infinite memory, one could achieve the same quality as NeRF by a sufficiently fine voxel grid and tri-linear interpolation. NeRF’s advantage was representing that huge grid implicitly with a small MLP.</p>
<p>Early NeRFs were slow, but with Instant-NGP, the gap closed: Instant-NGP essentially uses a hashed voxel grid of features with an MLP, and others like Plenoxels use a sparse voxel grid outright. The performance of voxel methods (with interpolation) is very high – Plenoxels can train in minutes and render quickly. But memory is a challenge: storing a high-res grid (say <span class="math notranslate nohighlight">\(512^3\)</span> or <span class="math notranslate nohighlight">\(1024^3\)</span>) is heavy, though sparse structures mitigate that (most of space is empty typically).</p>
<p>NeRF vs. voxel is a trade-off of continuous vs. discrete: NeRF’s continuous nature avoids grid artifacts and can scale to any resolution (in theory), but voxel methods explicitly capture detail at a set resolution. Also, optimization on a grid (which has millions of parameters) can be more data-hungry, whereas an MLP’s capacity is constrained.</p>
<p>In practice, now with faster hardware and clever data structures, voxel or hybrid approaches often outperform the original NeRF in both speed and quality, because they can directly allocate degrees of freedom to tiny scene details that a NeRF MLP might blur out. For example, NSVF and Instant-NGP show that memory-intensive representations can drastically cut down computation time with minimal quality loss. So depending on the use-case (memory-rich GPU vs. memory-limited scenario), one might choose an explicit voxel field or an implicit network.</p>
</section>
<section id="vs-multi-plane-images-mpis-light-fields">
<h3>Vs. Multi-Plane Images (MPIs) / Light Fields<a class="headerlink" href="#vs-multi-plane-images-mpis-light-fields" title="Link to this heading"></a></h3>
<p>Before NeRF, one popular approach for view synthesis from sparse views was to use MPIs – a set of frontal-parallel planes with semi-transparent textures that approximate the scene (as in LLFF). LLFF (Mildenhall et al. 2019) trained a network to predict an MPI for each input view and blended them to render new views. MPIs can be rendered fast (just alpha compositing a few textured quads) and were effective for small view perturbations (e.g., moving a bit in a captured window).</p>
<p>However, they often require many planes to cover depth range without artifacts, and each MPI is tied to a particular reference view (extrapolating far from that view leads to holes). NeRF can be seen as an infinite continuous collection of planes (every sample along a ray is like a tiny plane). NeRF’s quality surpassed MPI methods, especially for larger view changes or complex geometry, because MPIs had to discretize depth (leading to discretization “slices” artifacts and memory use proportional to number of planes).</p>
<p>In terms of performance, MPIs are faster to train (LLFF took minutes) and faster to render (real-time), but they trade off generality – it’s hard for MPI to represent truly 360° scenes or very disoccluded views, whereas NeRF handles 360° inherently. There have been hybrid approaches (e.g., MVSNeRF uses multiple local NeRFs akin to MPIs for large scenes). In summary, MPIs are a simpler representation – essentially a truncated light field – good for certain view ranges, while NeRF is a more global representation. NeRF’s use of a neural network also gave it more compositing flexibility (it wasn’t limited to a fixed number of planes).</p>
</section>
<section id="accuracy-and-fidelity">
<h3>Accuracy and Fidelity<a class="headerlink" href="#accuracy-and-fidelity" title="Link to this heading"></a></h3>
<p>When it comes to pure novel view synthesis quality (in terms of reproducing pixel-perfect images), NeRF-based methods (including its explicit derivatives) currently achieve state-of-the-art results on many benchmarks (like the Synthetic-NeRF dataset, real indoor scenes, etc.). Traditional methods (COLMAP + mesh + texture) often have lower photometric accuracy and noticeable artifacts because they don’t capture reflectance changes or fine details as well. A quantitative metric like PSNR is usually several points higher for NeRF vs. classical methods on the same data.</p>
<p>On the other hand, if the goal is geometric accuracy (e.g., a precise 3D model for measurement), NeRF’s density may not be as directly useful as a mesh from photogrammetry – one might have to extract a surface and possibly lose some precision. So NeRFs are tuned for view synthesis fidelity rather than exact geometric reconstruction.</p>
</section>
<section id="applicability">
<h3>Applicability<a class="headerlink" href="#applicability" title="Link to this heading"></a></h3>
<p>NeRF’s implicit nature makes it very flexible – it can, in principle, represent any appearance (even things like volumetric participating media, which meshes cannot). It also naturally handles transparency and semi-transparent geometry (e.g., fine foliage, smoke to some extent) by distributing density. However, NeRF currently requires knowing camera intrinsics/extrinsics fairly accurately; other methods like SfM or SLAM handle unknown poses better (though some works integrate pose optimization into NeRF).</p>
<p>For large outdoor scenes, NeRF-like approaches had to evolve (NeRF++, mip-NeRF 360) whereas traditional GIS or photogrammetry might use other cues (like lidar). Another factor: learning-based vs. non-learning. NeRF needs a neural network and a lot of processing, while a method like COLMAP is mostly linear algebra and multiview geometry; the latter might be easier to run on a CPU or integrate into certain pipelines without a GPU. That said, the trend is that NeRF variants are becoming more accessible (some run on browsers now).</p>
<p>In conclusion, NeRFs provide an excellent solution for novel view rendering with unparalleled visual quality, at the expense of initial computational cost and an implicit form that is not as directly usable as explicit models for some tasks. With the rapid improvements in efficiency, NeRFs are closing the gap in speed. We can envision a future system where a NeRF is just part of the toolbox: one might capture a scene, get a NeRF, and then either use it as-is for rendering or convert it to a mesh or point cloud if needed for other purposes. The lines between representations are blurring – e.g., one might use a mesh for collisions, but render it with a NeRF-like texture for realism. NeRF has effectively bridged vision and graphics: it learns from images like a vision model, but produces an asset that can be rendered like a graphics model, thus comparisons depend on what aspect of performance we care about (visual vs. geometric vs. speed vs. memory).</p>
</section>
</section>
<section id="datasets-for-nerf-training-and-evaluation">
<h2>Datasets for NeRF Training and Evaluation<a class="headerlink" href="#datasets-for-nerf-training-and-evaluation" title="Link to this heading"></a></h2>
<p>The development of NeRF models has been facilitated by several key datasets. We list important datasets commonly used for training or evaluating static and dynamic NeRFs, along with their characteristics:</p>
<section id="blender-synthetic-nerf-dataset">
<h3>Blender Synthetic NeRF Dataset<a class="headerlink" href="#blender-synthetic-nerf-dataset" title="Link to this heading"></a></h3>
<p>Introduced by Mildenhall et al. (2020) for the original NeRF, this dataset consists of 8 synthetic objects/scenes (e.g., Lego truck, Hotdog, Mic, Chair, Drums, Ficus plant, etc.). These scenes were rendered using path tracing in Blender, providing ground-truth images with known camera parameters. They feature complex geometry and materials (shiny metals, translucency).</p>
<p>NeRF’s high PSNR on this dataset demonstrated its ability to capture fine detail and reflections. Researchers continue to use this dataset as a baseline for comparing novel view synthesis methods, since it provides an absolute ground truth and unlimited training data (since it’s synthetic, one can render more views). Typical usage: train on 100 views and test on held-out views. The dataset is small (800x800 images usually) but challenging due to the reflectance and occlusions.</p>
</section>
<section id="local-light-field-fusion-llff-real-forward-facing-dataset">
<h3>Local Light Field Fusion (LLFF) Real Forward-Facing Dataset<a class="headerlink" href="#local-light-field-fusion-llff-real-forward-facing-dataset" title="Link to this heading"></a></h3>
<p>From Mildenhall et al. (2019), this dataset contains 8 real-world scenes captured with a handheld smartphone, each with 20–62 images taken roughly forwards (inward-facing). Examples: a room with plants, a flower, a lego bulldozer, etc. The scenes are partial scans (the camera captures a limited viewing hemisphere, not full 360).</p>
<p>LLFF provided camera poses (estimated via structure-from-motion) and images at ~1008×756 resolution. It was used to evaluate NeRF on real data with moderate complexity. LLFF (the method) generates MPIs for these scenes and had some artifacts, whereas NeRF significantly outperformed it, rendering thin structures and filling disocclusions correctly. The LLFF dataset highlights NeRF’s ability to handle real photo imperfections and less-than-ideal capture conditions. It’s a standard benchmark for comparing to other image-based rendering methods.</p>
</section>
<section id="tanks-and-temples">
<h3>Tanks and Temples<a class="headerlink" href="#tanks-and-temples" title="Link to this heading"></a></h3>
<p>Originally by Knapitsch et al. (2017) for multi-view stereo, this is a set of large-scale indoor/outdoor scenes (e.g., a courtyard, a church, tanks, room). NeRF++ and other variants targeting unbounded scenes often test on a subset of Tanks &amp; Temples (like Truck, Barn, etc.) to show they can handle large scenes with varying depth.</p>
<p>These scenes have many images (hundreds), known poses, and challenging geometry (thin wires, large empty spaces). NeRF without modifications struggles here (background infinity issue, etc.), but NeRF++ and mip-NeRF 360 tackle them. Metrics include PSNR, SSIM, and perhaps completeness of geometry. This dataset is useful for evaluating scalability of NeRF methods to bigger, outdoor or mixed content scenes.</p>
</section>
<section id="dtu-dataset">
<h3>DTU Dataset<a class="headerlink" href="#dtu-dataset" title="Link to this heading"></a></h3>
<p>A multi-view scan dataset (indoor objects on a turntable with fixed cameras). Some NeRF works (e.g., those focusing on geometry like NeuS) use DTU to evaluate geometry accuracy, comparing reconstructed surfaces to ground-truth scans. DTU provides ~49 views per scene and scanned ground-truth geometry, so it’s good for checking if NeRF-derived density can match actual surfaces.</p>
</section>
<section id="human3-6m">
<h3>Human3.6M<a class="headerlink" href="#human3-6m" title="Link to this heading"></a></h3>
<p>A widely-used human pose dataset by Ionescu et al. (2014), it has 11 actors performing various actions in a lab, captured by a multi-camera system (4 synchronized HD cameras) with a green screen. It provides accurate 3D joint annotations (from a motion capture suit).</p>
<p>While originally for pose estimation, Human3.6M has been used in NeRF-based human reconstruction. For example, Neural Body (Peng et al. 2021) tested on Human3.6M to show their method can reconstruct a human from as few as 4 camera views. In Human3.6M, backgrounds are usually removed (since green screen) so models can focus on the person. It’s a dynamic dataset (people moving) but multi-view, making it suitable for dynamic NeRF training (each time frame has 4 views). Typically, one subject’s sequence might be used to train, and novel poses of that same person (from the dataset) used to evaluate generalization.</p>
<p>The advantage of Human3.6M is the availability of ground truth SMPL or skeleton data, which methods like Neural Body leverage to condition their models.</p>
</section>
<section id="zju-mocap-dataset">
<h3>ZJU-MoCap Dataset<a class="headerlink" href="#zju-mocap-dataset" title="Link to this heading"></a></h3>
<p>Introduced by Peng et al. (2021) alongside Neural Body, this dataset contains 9 sequences of different human performers doing various motions (e.g., exercise, dancing) captured by 21 synchronized cameras in a dome setup. High-resolution images and calibrated poses are provided. It also includes fitted SMPL models for each frame (which they improved with EasyMocap).</p>
<p>This dataset is designed for free-viewpoint video of humans – one can train on some camera views and test on others. It’s been used to evaluate many human NeRF models. For instance, they measure novel view synthesis quality and the ability to handle novel poses (by holding out some frames). ZJU-MoCap is challenging due to complex motions and clothing, and it’s a good test of how well a method can integrate multi-view information to learn a dynamic model. Neural Body’s results on ZJU-MoCap showed high-fidelity renderings that were temporally consistent. This dataset is now a common benchmark for dynamic human rendering.</p>
</section>
<section id="people-snapshot">
<h3>People-Snapshot<a class="headerlink" href="#people-snapshot" title="Link to this heading"></a></h3>
<p>A dataset of people rotating in an A-pose (from Alldieck et al. 2018). It provides monocular video (one person turning 360 degrees) along with SMPL fits. Neural Body and others use it to test reconstruction from a single video (since the pose is just rotation, it’s like a turntable of a human). It’s simpler than ZJU (no motion, just rotation), but good for evaluating geometry and appearance on a static pose. Neural Body and others compare to methods like PIFuHD on this dataset.</p>
</section>
<section id="synthetic-dynamic-scenes">
<h3>Synthetic dynamic scenes<a class="headerlink" href="#synthetic-dynamic-scenes" title="Link to this heading"></a></h3>
<p>For testing dynamic NeRFs, authors sometimes create simple synthetic scenes where geometry moves in a known way. For example, D-NeRF paper includes synthetic sequences (like a cube moving or a human model deforming) to validate that the method can learn the deformation field correctly. These come with ground-truth canonical frames for quantitative error of deformation.</p>
</section>
<section id="other-datasets">
<h3>Other datasets<a class="headerlink" href="#other-datasets" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>KITTI or Waymo Open Dataset</strong>: Autonomous driving datasets with multi-camera rigs have been used in some large-scale NeRF variants (e.g., Urban NeRF) to reconstruct portions of outdoor scenes.</p></li>
<li><p><strong>Replica/ScanNet</strong>: Indoor scans used to test NeRFs on room-scale data (often to see if NeRF captures fine details and lighting in real indoor environments).</p></li>
<li><p><strong>Phototourism datasets</strong> (like the Dubrovnik, Trevi Fountain from NeRF-W) for testing NeRF under varying illumination. However, those introduce additional complexities (transient objects, lighting changes) so specialized models like NeRF-W were devised.</p></li>
</ul>
<p>In summary, static NeRFs are often benchmarked on Blender synthetic (for absolute quality) and LLFF real (for real-case performance). Dynamic NeRFs are evaluated on multi-view video datasets like Human3.6M and ZJU-MoCap (for humans) or other custom sequences, focusing on temporal consistency and generalization to novel poses. The availability of ground truth geometry (for static scenes) or ground truth motion (for humans) helps quantitatively evaluate how well a NeRF approach is capturing the scene’s 3D structure, beyond just image reproduction error.</p>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading"></a></h2>
<p>Neural Radiance Fields have revolutionized the field of view synthesis and 3D representation in a very short time. They blend concepts from vision and graphics – volume rendering, multi-view geometry, implicit neural modeling – into a single cohesive framework that achieves state-of-the-art results in reconstructing scenes from images. This document traced the lineage of NeRF from earlier representations (voxel grids, point clouds, meshes, light fields), through the details of the original NeRF formulation (volume rendering equations, positional encoding, MLP architecture), and into the many extensions that have arisen (handling aliasing with mip-NeRF, unbounded scenes with NeRF++, speeding up training with Instant-NGP and others, extending to dynamic scenes with D-NeRF/Nerfies, and integrating domain knowledge like human body models via SMPL).</p>
<p>In comparing NeRF to traditional representations, we see that NeRF offers unmatched fidelity and a flexible, continuous scene definition, at the cost of implicitness and initial computational expense. However, ongoing research is rapidly mitigating these costs – making NeRFs faster, smaller, and more user-friendly – while also expanding their capabilities (for example, enabling editing, compositionality, or generalization across scenes). The synergy between neural networks and 3D representations in NeRF has also spurred new lines of research in both computer vision (e.g., using NeRFs for camera pose estimation, SLAM, etc.) and graphics (neural rendering in content creation).</p>
<p>NeRF’s development showcases an interesting paradigm: rather than explicitly programming a 3D reconstruction, we let a neural network learn the 3D structure by rendering it. This opens possibilities to capture phenomena that are hard to model explicitly (like complex materials or translucency) as the network will learn to reproduce them from data. We are likely to see further improvements in NeRF’s theoretical understanding (e.g., what functions can a NeRF represent, how to regularize it for better geometry), as well as more practical systems (perhaps NeRF-based 3D scanners, or NeRF in mobile devices for instant AR scenes).</p>
<p>In conclusion, Neural Radiance Fields represent a significant milestone in 3D scene representation. By building on classical ideas and adding the power of neural function approximation, NeRF has achieved a level of performance in novel view synthesis that was previously unattainable. The historical context and theoretical foundations discussed in this overview underline that NeRF is not an isolated idea, but rather the product of a long evolution in both computer vision and graphics. As research continues, NeRF and its derivatives are poised to become a foundational technology for virtual reality, robotics, movies, and anywhere we need realistic 3D scenes constructed from images.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="lecture_07_2_fitting_SMPL_to_IMU_learning.html" class="btn btn-neutral float-left" title="Lecture 07.2: Fitting SMPL to IMU Data Using Learning-Based Methods" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="extendeed_materials_gaussian_splatting.html" class="btn btn-neutral float-right" title="3D Gaussian Splatting: A Basic Introduction" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>