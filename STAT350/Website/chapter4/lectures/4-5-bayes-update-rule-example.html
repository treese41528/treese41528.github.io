

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>4.5. Bayesian Updating: Sequential Learning &mdash; STAT 350</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=bac617f8" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT350/course_site/chapter4/lectures/4-5-bayes-update-rule-example.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="4.6. Independence of Events" href="4-6-independence-of-events.html" />
    <link rel="prev" title="4.4. Law of Total Probability and Bayes‚Äô Rule" href="4-4-law-of-total-probability-and-bayes-rule.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Orientation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../course-intro.html">Course Introduction &amp; Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#why-statistics-why-now">Why Statistics? Why Now?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#course-roadmap">Course Roadmap</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#a-clear-note-on-using-ai">A Clear Note on Using AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#getting-started-a-checklist">Getting Started: A Checklist</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../course-intro.html#homework-assignments-on-edfinity">Homework Assignments on Edfinity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../course-intro.html#miscellaneous-tips">Miscellaneous Tips</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Chapters</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../chapter1/index.html">1. Introduction to Statistics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter1/lectures/1-1-intro.html">1.1. What Is Statistics?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter1/lectures/1-2-probability-inference.html">1.2. Probability &amp; Statistical Inference: How Are They Associated?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter2/index.html">2. Graphical Summaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-1-understanding-the-structure-of-data-set.html">2.1. Data Set Structure and Variable Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-2-tools-for-categorical-qualitative-data.html">2.2. Tools for Categorical (Qualitative) Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-3-tools-for-numerical-quantitative-data.html">2.3. Tools for Numerical (Quantitative) Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-4-exploring-quantitative-distributions-modality-shape-and-outliers.html">2.4. Exploring Quantitative Distributions: Modality, Shape &amp; Outliers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter3/index.html">3. Numerical Summaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-1-intro-numerical-summaries.html">3.1. Introduction to Numerical Summaries: Notation and Terminology</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-2-measures-of-central-tendency.html">3.2. Measures of Central Tendency</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-3-measures-of-variability-range-variance-and-SD.html">3.3. Measures of Variability - Range, Variance, and Standard Deviation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-4-measures-of-variability-IQR-and-5-number-summary.html">3.4. Measures of Variability - Interquartile Range and Five-Number Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-5-choosing-the-right-measure-resistance-to-extreme-values.html">3.5. Choosing the Right Measure &amp; Comparing Measures Across Data Sets</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">4. Probability</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="4-1-basic-set-theory.html">4.1. Basic Set Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="4-2-probability.html">4.2. Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="4-3-conditional-probability.html">4.3. Conditional Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="4-4-law-of-total-probability-and-bayes-rule.html">4.4. Law of Total Probability and Bayes‚Äô Rule</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">4.5. Bayesian Updating: Sequential Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="4-6-independence-of-events.html">4.6. Independence of Events</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter5/index.html">5. Discrete Distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-1-discrete-rvs-and-pmfs.html">5.1. Discrete Random Variables and Probability Mass Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-2-joint-pmfs.html">5.2. Joint Probability Mass Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-3-expected-value-of-discrete-rv.html">5.3. Expected Value of a Discrete Random Variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-4-variance-of-discrete-rv.html">5.4. Varianace of a Discrete Random Variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-5-covariance-of-dependent-rvs.html">5.5. Covariance of Dependent Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-6-binomial-distribution.html">5.6. The Binomial Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-7-poisson-distribution.html">5.7. Poisson Distribution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter6/index.html">6. Continuous Distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-1-continuous-rvs-and-pdfs.html">6.1. Continuous Random Variables and Probability Density Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-2-expected-value-and-variance-of-cts-rvs.html">6.2. Expected Value and Variance of Continuous Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-3-cdfs.html">6.3. Cumulative Distribution Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-4-normal-distribution.html">6.4. Normal Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-5-uniform-distribution.html">6.5. Uniform Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-6-exponential-distribution.html">6.6. Exponential Distribution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter7/index.html">7. Sampling Distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-1-statistics-and-sampling-distributions.html">7.1. Statistics and Sampling Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-2-sampling-distribution-for-the-sample-mean.html">7.2. Sampling Distribution for the Sample Mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-3-clt.html">7.3. The Central Limit Theorem (CLT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-4-discret-rvs-and-clt.html">7.4. Discrete Random Variables and the CLT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter8/index.html">8. Experimental Design</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-1-experimental-and-sampling-designs.html">8.1. Experimental and Sampling Designs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-2-experimental-design-principles.html">8.2. Experimental Design Principles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-3-basic-types-of-experimental-design.html">8.3. Basic Types of Experimental Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-4-addressing-potential-flaws-in-experimental-design.html">8.4. Addressing Potential Flaws in Experimental Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-5-examples-of-experimental-design.html">8.5. Examples of Experimental Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-6-sampling-design.html">8.6. Sampling Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-7-sampling-bias.html">8.7. Sampling Bias</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter9/index.html">9. Confidence Intervals and Bounds</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-1-intro-statistical-inference.html">9.1. Introduction to Statistical Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-2-ci-sigma-known.html">9.2. Confidence Intervals for the Population Mean, When œÉ is Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-3-precision-of-ci-and-sample-size-calculation.html">9.3. Precision of a Confidence Interval and Sample Size Calculation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-4-cb-sigma-known.html">9.4. Confidence Bounds for the Poulation Mean When œÉ is Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-5-ci-cb-sigma-unknown.html">9.5. Confidence Intervals and Bounds When œÉ is Unknown</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter10/index.html">10. Hypothesis Testing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-1-ht-errors-and-power.html">10.1. Type I Error, Type II Error, and Power</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-2-ht-for-mean-sigma-known.html">10.2. Hypothesis Test for the Population Mean When œÉ is Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-3-ht-for-mean-sigma-unknown.html">10.3. Hypothesis Test for the Population Mean When œÉ Is Unknown</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-4-pvalue-significance-conclusion.html">10.4. P-values, Statistical Significance, and Formal Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter11/index.html">11. Two Sample Procedures</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-1-ci-ht-two-samples.html">11.1. Confidence Interval/Bound and Hypothesis Test for Two Samples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-2-comparing-two-means-independent-sigmas-known.html">11.2. Comparing the Means of Two Independent Populations - Population Variances Are Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-3-comparing-two-means-independent-pooled.html">11.3. Comparing the Means of Two Independent Populations - Pooled Variance Estimator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-4-comparing-two-means-independent-unpooled.html">11.4. Comparing the Means of Two Independent Populations - No Equal Variance Assumption</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-5-mean-of-paired-differences-two-dependent-populations.html">11.5. Analyzing the Mean of Paired Differences Between two Dependent Populations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter12/index.html">12. ANOVA</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-1-intro-one-way-anova.html">12.1. Introduction to One-Way ANOVA</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-2-sources-of-variability.html">12.2. Different Sources of Variability in an ANOVA Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-3-f-test-and-relationship-to-t-test.html">12.3. One-Way ANOVA F-Test and Its Relationship to Two-Sample t-Tests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-4-multiple-comparison-procedures-family-wise-error-rates.html">12.4. Multiple Comparison Procedures and Family-Wise Error Rates</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter13/index.html">13. Simple Linear Regression</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter13/lectures/13-1-intro-to-lr-correlation-scatter-plots.html">13.1. Introduction to Linear Regression: Correlation and Scatter Plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter13/lectures/13-2-simple-linear-regression.html">13.2. Simple Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter13/lectures/13-3-diagnostics-inference.html">13.3. Model Diagnostics and Statistical Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter13/lectures/13-4-prediction-robustness.html">13.4. Prediction, Robustness, and Applied Examples</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html"><span class="section-number">4. </span>Probability</a></li>
      <li class="breadcrumb-item active"><span class="section-number">4.5. </span>Bayesian Updating: Sequential Learning</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/chapter4/lectures/4-5-bayes-update-rule-example.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="video-placeholder">
  <iframe src="https://www.youtube.com/embed/KJK5tMOz89g?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6" allowfullscreen>
  </iframe>
</div><section id="bayesian-updating-sequential-learning">
<h1><span class="section-number">4.5. </span>Bayesian Updating: Sequential Learning<a class="headerlink" href="#bayesian-updating-sequential-learning" title="Link to this heading">ÔÉÅ</a></h1>
<p>Perhaps the most powerful aspect of Bayes‚Äô rule is that it allows us to systematically update our beliefs
as new evidence emerges. Rather than analyzing all evidence at once, we can process information sequentially,
updating our probability assessments step by step. This iterative approach mirrors how humans naturally learn
and refine their understanding of the world. In this chapter, we‚Äôll explore the mechanics of Bayesian updating
and see how it can be applied to real-world problems.</p>
<div class="important admonition">
<p class="admonition-title">Road Map üß≠</p>
<ul class="simple">
<li><p>Understand how Bayes‚Äô rule enables sequential probability updates</p></li>
<li><p>Explore the mechanics of Bayesian updating through a coin flip example</p></li>
<li><p>Recognize how prior information gets updated with each new observation</p></li>
<li><p>Observe how probabilities converge toward the truth with sufficient evidence</p></li>
<li><p>Apply these concepts to scenarios where sequential information becomes available</p></li>
</ul>
</div>
<section id="the-power-of-bayesian-updating">
<h2><span class="section-number">4.5.1. </span>The Power of Bayesian Updating<a class="headerlink" href="#the-power-of-bayesian-updating" title="Link to this heading">ÔÉÅ</a></h2>
<p>Bayes‚Äô rule, as we‚Äôve seen in the previous chapter, allows us to calculate the probability of an event A given evidence B:</p>
<div class="math notranslate nohighlight">
\[P(A|B) = \frac{P(B|A) \times P(A)}{P(B)}\]</div>
<p>What makes this formula particularly powerful is that once we‚Äôve calculated P(A|B), this posterior probability can serve as the prior probability for the next calculation when new evidence C arrives:</p>
<div class="math notranslate nohighlight">
\[P(A|B,C) = \frac{P(C|A,B) \times P(A|B)}{P(C|B)}\]</div>
<p>This sequential updating process reflects how learning typically occurs‚Äîwe start with some initial beliefs, encounter new evidence, update our beliefs, encounter more evidence, update again, and so on. With each update, our probability assessments become more refined and (ideally) closer to the truth.</p>
<p>The Bayesian updating process follows these steps:</p>
<ol class="arabic simple">
<li><p>Start with a prior probability P(A)</p></li>
<li><p>Observe evidence B</p></li>
<li><p>Calculate the posterior probability P(A|B) using Bayes‚Äô rule</p></li>
<li><p>Treat P(A|B) as the new prior</p></li>
<li><p>Observe new evidence C</p></li>
<li><p>Calculate the new posterior P(A|B,C)</p></li>
<li><p>Continue this process as more evidence becomes available</p></li>
</ol>
</section>
<section id="coin-flip-example-detecting-a-biased-coin">
<h2><span class="section-number">4.5.2. </span>Coin Flip Example: Detecting a Biased Coin<a class="headerlink" href="#coin-flip-example-detecting-a-biased-coin" title="Link to this heading">ÔÉÅ</a></h2>
<p>Let‚Äôs explore a concrete example to illustrate Bayesian updating in action.</p>
<p>Scenario: Suppose you have a bag containing 10 coins. One of these coins is biased, with a probability of heads equal to 0.8. The other 9 coins are fair, with a probability of heads equal to 0.5. You reach into the bag, select a coin at random, and flip it 10 times. Each flip results in heads. What is the probability that you selected the biased coin?</p>
<p>To solve this problem, we need to define our events:</p>
<ul class="simple">
<li><p>Let H·µ¢ denote that the coin showed heads on the ith flip</p></li>
<li><p>Let B denote that the biased coin was selected</p></li>
</ul>
<p>We know the following initial probabilities:</p>
<ul class="simple">
<li><p>P(B) = 1/10 (probability of selecting the biased coin)</p></li>
<li><p>P(B‚Äô) = 9/10 (probability of selecting a fair coin)</p></li>
<li><p>P(H·µ¢|B) = 0.8 (probability of heads on any flip, given the biased coin)</p></li>
<li><p>P(H·µ¢|B‚Äô) = 0.5 (probability of heads on any flip, given a fair coin)</p></li>
</ul>
<p>We want to find P(B|H‚ÇÅ,H‚ÇÇ,‚Ä¶,H‚ÇÅ‚ÇÄ), the probability that the selected coin is biased given that all 10 flips resulted in heads. Rather than tackling this all at once, we‚Äôll update our probability assessment after each flip.</p>
<section id="flip-1">
<h3>Flip 1<a class="headerlink" href="#flip-1" title="Link to this heading">ÔÉÅ</a></h3>
<p>After the first flip results in heads, we calculate:</p>
<div class="math notranslate nohighlight">
\[P(B|H_1) = \frac{P(H_1|B) \times P(B)}{P(H_1|B) \times P(B) + P(H_1|B') \times P(B')}\]</div>
<p>Substituting the values:</p>
<div class="math notranslate nohighlight">
\[P(B|H_1) = \frac{0.8 \times \frac{1}{10}}{0.8 \times \frac{1}{10} + 0.5 \times \frac{9}{10}} = \frac{0.08}{0.08 + 0.45} = \frac{0.08}{0.53} = \frac{8}{53} \approx 0.151\]</div>
<p>After seeing one heads, the probability that we have the biased coin has increased from 0.1 to about 0.151. This makes intuitive sense‚Äîgetting heads is more likely with the biased coin, so observing heads should increase our confidence that we have the biased coin. However, the change is modest because heads is a common outcome even with a fair coin.</p>
</section>
<section id="flip-2">
<h3>Flip 2<a class="headerlink" href="#flip-2" title="Link to this heading">ÔÉÅ</a></h3>
<p>For the second flip, we treat our updated probability as the new prior:</p>
<div class="math notranslate nohighlight">
\[P(B|H_1,H_2) = \frac{P(H_2|B,H_1) \times P(B|H_1)}{P(H_2|B,H_1) \times P(B|H_1) + P(H_2|B',H_1) \times P(B'|H_1)}\]</div>
<p>A key insight here is that individual flips of the same coin are independent events. The outcome of the first flip doesn‚Äôt affect the probability of heads on the second flip. This means:</p>
<div class="math notranslate nohighlight">
\[\begin{split}P(H_2|B,H_1) = P(H_2|B) = 0.8 \\
P(H_2|B',H_1) = P(H_2|B') = 0.5\end{split}\]</div>
<p>Using these simplifications:</p>
<div class="math notranslate nohighlight">
\[P(B|H_1,H_2) = \frac{0.8 \times \frac{8}{53}}{0.8 \times \frac{8}{53} + 0.5 \times (1 - \frac{8}{53})} = \frac{0.8 \times \frac{8}{53}}{0.8 \times \frac{8}{53} + 0.5 \times \frac{45}{53}} = \frac{64}{289} \approx 0.221\]</div>
<p>After the second heads, our confidence that we have the biased coin has increased further to about 0.221.</p>
</section>
<section id="flip-3">
<h3>Flip 3<a class="headerlink" href="#flip-3" title="Link to this heading">ÔÉÅ</a></h3>
<p>For the third flip, we update again:</p>
<div class="math notranslate nohighlight">
\[P(B|H_1,H_2,H_3) = \frac{P(H_3|B) \times P(B|H_1,H_2)}{P(H_3|B) \times P(B|H_1,H_2) + P(H_3|B') \times P(B'|H_1,H_2)}\]</div>
<p>Substituting the values:</p>
<div class="math notranslate nohighlight">
\[P(B|H_1,H_2,H_3) = \frac{0.8 \times \frac{64}{289}}{0.8 \times \frac{64}{289} + 0.5 \times (1 - \frac{64}{289})} = \frac{0.8 \times \frac{64}{289}}{0.8 \times \frac{64}{289} + 0.5 \times \frac{225}{289}} = \frac{512}{1637} \approx 0.313\]</div>
<p>The pattern continues for subsequent flips. With each additional heads, we become more confident that we have the biased coin.</p>
</section>
<section id="a-general-update-formula-for-every-flip">
<h3>A General Update Formula for Every Flip<a class="headerlink" href="#a-general-update-formula-for-every-flip" title="Link to this heading">ÔÉÅ</a></h3>
<p>We can derive a more elegant recursive update rule for this process. Let‚Äôs define:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(p_k = P(B|{\text{data up to flip }k})\)</span> ‚Äî our current belief that the coin is biased</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{LR}_H = \frac{P(H|B)}{P(H|B')} = \frac{0.8}{0.5}=1.6\)</span> ‚Äî the likelihood ratio for heads</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{LR}_T = \frac{P(T|B)}{P(T|B')} = \frac{0.2}{0.5}=0.4\)</span> ‚Äî the likelihood ratio for tails</p></li>
</ul>
<p>After each new flip <span class="math notranslate nohighlight">\(X_k \in \{H,T\}\)</span>, the posterior is updated by:</p>
<div class="math notranslate nohighlight">
\[p_k = \frac{\text{LR}_{X_k} \, p_{k-1}}{\text{LR}_{X_k} \, p_{k-1} + (1-p_{k-1})}.
\tag{4.5.1}\]</div>
<p>This formula embodies a simple principle: <strong>multiply by the likelihood ratio of what you just observed, then renormalize to keep the result between 0 and 1</strong>.</p>
<p>Below is a complete table of the posterior probability <span class="math notranslate nohighlight">\(p_k=P(B\mid H_1,\dots,H_k)\)</span> after <strong>each successive head</strong> when the first 10 flips are all heads. Values are rounded to three decimals.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 25.0%" />
<col style="width: 75.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Flip <span class="math notranslate nohighlight">\(k\)</span></p></th>
<th class="head"><p>Posterior <span class="math notranslate nohighlight">\(p_k\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0 (prior)</p></td>
<td><p>0.100</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>0.151</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>0.221</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>0.313</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>0.421</p></td>
</tr>
<tr class="row-odd"><td><p>5</p></td>
<td><p>0.538</p></td>
</tr>
<tr class="row-even"><td><p>6</p></td>
<td><p>0.651</p></td>
</tr>
<tr class="row-odd"><td><p>7</p></td>
<td><p>0.749</p></td>
</tr>
<tr class="row-even"><td><p>8</p></td>
<td><p>0.827</p></td>
</tr>
<tr class="row-odd"><td><p>9</p></td>
<td><p>0.884</p></td>
</tr>
<tr class="row-even"><td><p>10</p></td>
<td><p>0.924</p></td>
</tr>
</tbody>
</table>
<p>Each step uses the update rule:</p>
<div class="math notranslate nohighlight">
\[p_k = \frac{1.6\,p_{k-1}}{1.6\,p_{k-1} + (1-p_{k-1})},\]</div>
<p>where 1.6 is the likelihood-ratio <span class="math notranslate nohighlight">\(\text{LR}_H = P(H\mid B)/P(H\mid B') = 0.8/0.5\)</span>. Notice how evidence accumulates rapidly: after just five heads the biased coin is already more likely than not, and by the tenth head the posterior has climbed to about 0.924.</p>
</section>
<section id="closed-form-after-h-heads-and-t-tails">
<h3>Closed Form After h Heads and t Tails<a class="headerlink" href="#closed-form-after-h-heads-and-t-tails" title="Link to this heading">ÔÉÅ</a></h3>
<p>Because flips are conditionally independent given the coin type, we can derive a closed-form expression for any sequence with <span class="math notranslate nohighlight">\(h\)</span> heads and <span class="math notranslate nohighlight">\(t\)</span> tails:</p>
<div class="math notranslate nohighlight">
\[\begin{split}P(H^h T^t|B) &amp;= (0.8)^{h}(0.2)^{t},\\
P(H^h T^t|B') &amp;= (0.5)^{h+t}.\end{split}\]</div>
<p>Applying Bayes‚Äô rule gives:</p>
<div class="math notranslate nohighlight">
\[P(B|H^h T^t)
  =
  \frac{0.1\,(0.8)^{h}(0.2)^{t}}
       {0.1\,(0.8)^{h}(0.2)^{t}+0.9\,(0.5)^{h+t}}
  =
  \frac{1}{1 + 9\,\bigl(\tfrac{5}{8}\bigr)^{h}
                 \bigl(\tfrac{5}{2}\bigr)^{t}}.
  \tag{4.5.2}\]</div>
<p>For our all-heads run (<span class="math notranslate nohighlight">\(t=0\)</span>), this reduces to:</p>
<div class="math notranslate nohighlight">
\[P(B|10H)=\frac{1}{1+9\,(5/8)^{10}} \approx 0.924\]</div>
<p>This agrees with our sequential computation and confirms that after observing 10 heads in a row, we‚Äôre about 92.4% confident that we have the biased coin. This significant shift from our initial 10% probability demonstrates the power of evidence accumulation through Bayesian updating.</p>
<p>Note that if even one tail appears in the sequence, Equation 4.5.2 automatically down-weights the probability of the biased coin because the factor <span class="math notranslate nohighlight">\((5/2)^t\)</span> in the denominator grows rapidly. If even one tail appears, Eq 4.5.2 automatically down-weights the biased coin because the factor <span class="math notranslate nohighlight">\((5/2)^t\)</span> in the denominator grows.</p>
<p>Equation 4.5.1 is ideal when data arrive one at a time, allowing us to update our beliefs incrementally, while Equation 4.5.2 is more convenient when we can tally all heads and tails at once. Both approaches lead to the same posterior probability.</p>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter4/bayesian-updating-graph.png"><img alt="Graph showing how probability updates with each flip" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter4/bayesian-updating-graph.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 4.18 </span><span class="caption-text"><em>The probability of having the biased coin increases with each observed heads, eventually approaching near certainty.</em></span><a class="headerlink" href="#id1" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<p>Note that if we had observed a tails at any point, our confidence would have decreased instead, since the biased coin is less likely to show tails than a fair coin. Using our closed-form expression (Equation 4.5.2), we can easily calculate that if we had observed, for example, 8 heads and 2 tails, the posterior probability would be approximately 0.63, significantly lower than the 0.924 we obtained with all heads.</p>
</section>
</section>
<section id="the-convergence-of-beliefs">
<h2><span class="section-number">4.5.3. </span>The Convergence of Beliefs<a class="headerlink" href="#the-convergence-of-beliefs" title="Link to this heading">ÔÉÅ</a></h2>
<p>An important property of Bayesian updating is that with sufficient evidence, the posterior probabilities tend to converge toward the truth, regardless of the initial prior (as long as the prior isn‚Äôt exactly 0 or 1, which would represent absolute certainty).</p>
<p>In our coin example, if we had started with a different prior‚Äîperhaps believing there was a 50% chance of selecting the biased coin‚Äîour final probability after 10 heads would still be very close to 92.4%. The evidence overwhelms the initial belief.</p>
<p>This convergence property is why Bayesian methods are widely used in scientific and statistical reasoning: even if researchers begin with different prior beliefs, accumulating evidence will eventually lead them toward similar conclusions.</p>
</section>
<section id="conditional-independence-in-sequential-events">
<h2><span class="section-number">4.5.4. </span>Conditional Independence in Sequential Events<a class="headerlink" href="#conditional-independence-in-sequential-events" title="Link to this heading">ÔÉÅ</a></h2>
<p>In our coin flip example, we made use of a property called conditional independence. The flips are independent of each other given that we know which coin we have. Mathematically, for all i ‚â† j:</p>
<div class="math notranslate nohighlight">
\[P(H_i|B,H_j) = P(H_i|B)\]</div>
<p>This means that once we know whether we have the biased or fair coin, knowing the outcome of any previous flip doesn‚Äôt give us additional information about the probability of future flips.</p>
<p>Conditional independence is a powerful simplification that often applies in sequential events and greatly simplifies the calculations in Bayesian updating. Without it, we would need to consider complex dependencies between events, making the calculations much more difficult.</p>
</section>
<section id="alternative-forms-of-bayesian-updating">
<h2><span class="section-number">4.5.5. </span>Alternative Forms of Bayesian Updating<a class="headerlink" href="#alternative-forms-of-bayesian-updating" title="Link to this heading">ÔÉÅ</a></h2>
<p>For mathematically simpler scenarios like our coin example, there are often more direct ways to calculate the final probability than performing each update sequentially.</p>
<p>For instance, we could have directly calculated:</p>
<div class="math notranslate nohighlight">
\[P(B|H_1,H_2,...,H_{10}) = \frac{P(H_1,H_2,...,H_{10}|B) \times P(B)}{P(H_1,H_2,...,H_{10}|B) \times P(B) + P(H_1,H_2,...,H_{10}|B') \times P(B')}\]</div>
<p>With independent flips:</p>
<div class="math notranslate nohighlight">
\[P(H_1,H_2,...,H_{10}|B) = P(H_1|B) \times P(H_2|B) \times ... \times P(H_{10}|B) = (0.8)^{10}\]</div>
<p>And similarly:</p>
<div class="math notranslate nohighlight">
\[P(H_1,H_2,...,H_{10}|B') = (0.5)^{10}\]</div>
<p>Substituting:</p>
<div class="math notranslate nohighlight">
\[P(B|H_1,H_2,...,H_{10}) = \frac{(0.8)^{10} \times \frac{1}{10}}{(0.8)^{10} \times \frac{1}{10} + (0.5)^{10} \times \frac{9}{10}} \approx 0.924\]</div>
<p>This direct approach yields the same result as sequential updating. However, the sequential approach has advantages:</p>
<ol class="arabic simple">
<li><p>It shows how our beliefs evolve with each new piece of evidence</p></li>
<li><p>It allows us to stop and make decisions at any point in the sequence</p></li>
<li><p>It more naturally accommodates scenarios where evidence arrives over time</p></li>
<li><p>It often involves simpler calculations at each step</p></li>
</ol>
</section>
<section id="applications-of-bayesian-updating">
<h2><span class="section-number">4.5.6. </span>Applications of Bayesian Updating<a class="headerlink" href="#applications-of-bayesian-updating" title="Link to this heading">ÔÉÅ</a></h2>
<p>Bayesian updating has numerous practical applications:</p>
<p><strong>Medical Diagnosis</strong>: Doctors update their assessment of a patient‚Äôs condition as test results come in. Each test result provides new evidence that shifts the probability of various diagnoses.</p>
<p><strong>Spam Filtering</strong>: Email filters calculate the probability that a message is spam based on the words it contains. As the filter encounters new emails, it updates its probability estimates for different words.</p>
<p><strong>Quality Control</strong>: Manufacturers monitor the quality of their products by testing samples. Each test outcome updates their belief about the overall quality of the production batch.</p>
<p><strong>Weather Forecasting</strong>: Meteorologists update their precipitation forecasts as new atmospheric data becomes available, refining their predictions as the forecast day approaches.</p>
<p><strong>Financial Markets</strong>: Investors update their beliefs about asset values as new economic data, earnings reports, and other information becomes available.</p>
<p>In each case, Bayesian updating provides a rigorous framework for incorporating new evidence and refining probability assessments over time.</p>
</section>
<section id="bringing-it-all-together">
<h2><span class="section-number">4.5.7. </span>Bringing It All Together<a class="headerlink" href="#bringing-it-all-together" title="Link to this heading">ÔÉÅ</a></h2>
<div class="important admonition">
<p class="admonition-title">Key Takeaways üìù</p>
<ol class="arabic simple">
<li><p><strong>Bayesian updating</strong> allows us to revise probability assessments sequentially as new evidence emerges.</p></li>
<li><p>The <strong>posterior probability</strong> from one calculation becomes the <strong>prior probability</strong> for the next, creating a chain of updates.</p></li>
<li><p>With <strong>conditional independence</strong>, the calculations simplify considerably for sequential events.</p></li>
<li><p>With sufficient evidence, probabilities tend to <strong>converge toward the truth</strong> regardless of initial priors (unless they‚Äôre 0 or 1).</p></li>
<li><p>Sequential updating reflects how we <strong>naturally learn</strong> and revise our beliefs based on accumulated experience.</p></li>
<li><p>Bayesian updating has numerous <strong>practical applications</strong> in fields ranging from medicine to finance to artificial intelligence.</p></li>
</ol>
</div>
<p>Bayes‚Äô rule and the process of Bayesian updating provide a powerful framework for reasoning under uncertainty. They allow us to start with initial beliefs, incorporate new evidence, and systematically update our probability assessments. This process mirrors the scientific method itself: we form hypotheses, test them against evidence, and refine our understanding based on the results.</p>
<p>In the next chapter, we‚Äôll explore how events can be related (or unrelated) to each other through the concept of independence, which will further enhance our toolkit for probability calculations.</p>
<section id="exercises">
<h3>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">ÔÉÅ</a></h3>
<ol class="arabic simple">
<li><p><strong>Basic Updating</strong>: You have two bags of marbles. Bag A contains 3 red and 7 blue marbles. Bag B contains 8 red and 2 blue marbles. You select a bag at random (equal probability) and draw a marble that turns out to be red. What is the probability that you selected Bag B? If you replace the marble and draw again, getting another red marble, what is the updated probability?</p></li>
<li><p><strong>Medical Testing Sequence</strong>: A rare disease affects 1% of the population. A test for this disease has a sensitivity of 95% and a specificity of 98%. A patient tests positive on the first test. The doctor administers a second, independent test with the same characteristics, and the patient tests positive again. Calculate the probability that the patient has the disease after each test.</p></li>
<li><p><strong>Quality Control</strong>: A factory produces items with a 5% defect rate. Another factory produces the same items with a 2% defect rate. You receive a shipment of these items but don‚Äôt know which factory it came from (both are equally likely). You inspect 5 items and find 1 defect. What is the probability the shipment came from the first factory?</p></li>
<li><p><strong>Modified Coin Example</strong>: Using the same scenario as the coin flip example in this chapter, what would be the probability of having the biased coin if the sequence of 10 flips included exactly 8 heads and 2 tails? How does this compare to the all-heads scenario?</p></li>
<li><p><strong>Challenge Problem</strong>: Three meteorologists predict the weather for tomorrow. Meteorologist A is correct 70% of the time, B is correct 80% of the time, and C is correct 65% of the time. All three predict rain for tomorrow. Assuming their predictions are conditionally independent given the actual weather, what is the probability it will rain tomorrow? (Assume a 30% prior probability of rain.)</p></li>
</ol>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="4-4-law-of-total-probability-and-bayes-rule.html" class="btn btn-neutral float-left" title="4.4. Law of Total Probability and Bayes‚Äô Rule" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="4-6-independence-of-events.html" class="btn btn-neutral float-right" title="4.6. Independence of Events" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>