

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>5.4. Varianace of a Discrete Random Variable &mdash; STAT 350</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=bac617f8" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT350/course_site/chapter5/lectures/5-4-variance-of-discrete-rv.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="5.5. Covariance of Dependent Random Variables" href="5-5-covariance-of-dependent-rvs.html" />
    <link rel="prev" title="5.3. Expected Value of a Discrete Random Variable" href="5-3-expected-value-of-discrete-rv.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Orientation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../course-intro.html">Course Introduction &amp; Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#why-statistics-why-now">Why Statistics? Why Now?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#course-roadmap">Course Roadmap</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#a-clear-note-on-using-ai">A Clear Note on Using AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#getting-started-a-checklist">Getting Started: A Checklist</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../course-intro.html#homework-assignments-on-edfinity">Homework Assignments on Edfinity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../course-intro.html#miscellaneous-tips">Miscellaneous Tips</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Chapters</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../chapter1/index.html">1. Introduction to Statistics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter1/lectures/1-1-intro.html">1.1. What Is Statistics?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter1/lectures/1-2-probability-inference.html">1.2. Probability &amp; Statistical Inference: How Are They Associated?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter2/index.html">2. Graphical Summaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-1-understanding-the-structure-of-data-set.html">2.1. Data Set Structure and Variable Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-2-tools-for-categorical-qualitative-data.html">2.2. Tools for Categorical (Qualitative) Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-3-tools-for-numerical-quantitative-data.html">2.3. Tools for Numerical (Quantitative) Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-4-exploring-quantitative-distributions-modality-shape-and-outliers.html">2.4. Exploring Quantitative Distributions: Modality, Shape &amp; Outliers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter3/index.html">3. Numerical Summaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-1-intro-numerical-summaries.html">3.1. Introduction to Numerical Summaries: Notation and Terminology</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-2-measures-of-central-tendency.html">3.2. Measures of Central Tendency</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-3-measures-of-variability-range-variance-and-SD.html">3.3. Measures of Variability - Range, Variance, and Standard Deviation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-4-measures-of-variability-IQR-and-5-number-summary.html">3.4. Measures of Variability - Interquartile Range and Five-Number Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-5-choosing-the-right-measure-resistance-to-extreme-values.html">3.5. Choosing the Right Measure &amp; Comparing Measures Across Data Sets</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter4/index.html">4. Probability</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-1-basic-set-theory.html">4.1. Basic Set Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-2-probability.html">4.2. Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-3-conditional-probability.html">4.3. Conditional Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-4-law-of-total-probability-and-bayes-rule.html">4.4. Law of Total Probability and Bayes‚Äô Rule</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-5-bayes-update-rule-example.html">4.5. Bayesian Updating: Sequential Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-6-independence-of-events.html">4.6. Independence of Events</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">5. Discrete Distributions</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="5-1-discrete-rvs-and-pmfs.html">5.1. Discrete Random Variables and Probability Mass Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="5-2-joint-pmfs.html">5.2. Joint Probability Mass Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="5-3-expected-value-of-discrete-rv.html">5.3. Expected Value of a Discrete Random Variable</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">5.4. Varianace of a Discrete Random Variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="5-5-covariance-of-dependent-rvs.html">5.5. Covariance of Dependent Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="5-6-binomial-distribution.html">5.6. The Binomial Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="5-7-poisson-distribution.html">5.7. Poisson Distribution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter6/index.html">6. Continuous Distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-1-continuous-rvs-and-pdfs.html">6.1. Continuous Random Variables and Probability Density Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-2-expected-value-and-variance-of-cts-rvs.html">6.2. Expected Value and Variance of Continuous Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-3-cdfs.html">6.3. Cumulative Distribution Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-4-normal-distribution.html">6.4. Normal Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-5-uniform-distribution.html">6.5. Uniform Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-6-exponential-distribution.html">6.6. Exponential Distribution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter7/index.html">7. Sampling Distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-1-statistics-and-sampling-distributions.html">7.1. Statistics and Sampling Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-2-sampling-distribution-for-the-sample-mean.html">7.2. Sampling Distribution for the Sample Mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-3-clt.html">7.3. The Central Limit Theorem (CLT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-4-discret-rvs-and-clt.html">7.4. Discrete Random Variables and the CLT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter8/index.html">8. Experimental Design</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-1-experimental-and-sampling-designs.html">8.1. Experimental and Sampling Designs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-2-experimental-design-principles.html">8.2. Experimental Design Principles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-3-basic-types-of-experimental-design.html">8.3. Basic Types of Experimental Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-4-addressing-potential-flaws-in-experimental-design.html">8.4. Addressing Potential Flaws in Experimental Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-5-examples-of-experimental-design.html">8.5. Examples of Experimental Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-6-sampling-design.html">8.6. Sampling Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-7-sampling-bias.html">8.7. Sampling Bias</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter9/index.html">9. Confidence Intervals and Bounds</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-1-intro-statistical-inference.html">9.1. Introduction to Statistical Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-2-ci-sigma-known.html">9.2. Confidence Intervals for the Population Mean, When œÉ is Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-3-precision-of-ci-and-sample-size-calculation.html">9.3. Precision of a Confidence Interval and Sample Size Calculation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-4-cb-sigma-known.html">9.4. Confidence Bounds for the Poulation Mean When œÉ is Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-5-ci-cb-sigma-unknown.html">9.5. Confidence Intervals and Bounds When œÉ is Unknown</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter10/index.html">10. Hypothesis Testing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-1-ht-errors-and-power.html">10.1. Type I Error, Type II Error, and Power</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-2-ht-for-mean-sigma-known.html">10.2. Hypothesis Test for the Population Mean When œÉ is Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-3-ht-for-mean-sigma-unknown.html">10.3. Hypothesis Test for the Population Mean When œÉ Is Unknown</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-4-pvalue-significance-conclusion.html">10.4. P-values, Statistical Significance, and Formal Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter11/index.html">11. Two Sample Procedures</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-1-ci-ht-two-samples.html">11.1. Confidence Interval/Bound and Hypothesis Test for Two Samples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-2-comparing-two-means-independent-sigmas-known.html">11.2. Comparing the Means of Two Independent Populations - Population Variances Are Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-3-comparing-two-means-independent-pooled.html">11.3. Comparing the Means of Two Independent Populations - Pooled Variance Estimator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-4-comparing-two-means-independent-unpooled.html">11.4. Comparing the Means of Two Independent Populations - No Equal Variance Assumption</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-5-mean-of-paired-differences-two-dependent-populations.html">11.5. Analyzing the Mean of Paired Differences Between two Dependent Populations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter12/index.html">12. ANOVA</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-1-intro-one-way-anova.html">12.1. Introduction to One-Way ANOVA</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-2-sources-of-variability.html">12.2. Different Sources of Variability in an ANOVA Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-3-f-test-and-relationship-to-t-test.html">12.3. One-Way ANOVA F-Test and Its Relationship to Two-Sample t-Tests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-4-multiple-comparison-procedures-family-wise-error-rates.html">12.4. Multiple Comparison Procedures and Family-Wise Error Rates</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter13/index.html">13. Simple Linear Regression</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter13/lectures/13-1-intro-to-lr-correlation-scatter-plots.html">13.1. Introduction to Linear Regression: Correlation and Scatter Plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter13/lectures/13-2-simple-linear-regression.html">13.2. Simple Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter13/lectures/13-3-diagnostics-inference.html">13.3. Model Diagnostics and Statistical Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter13/lectures/13-4-prediction-robustness.html">13.4. Prediction, Robustness, and Applied Examples</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html"><span class="section-number">5. </span>Discrete Distributions</a></li>
      <li class="breadcrumb-item active"><span class="section-number">5.4. </span>Varianace of a Discrete Random Variable</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/chapter5/lectures/5-4-variance-of-discrete-rv.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="video-placeholder" role="group" aria-labelledby="video-ch5-4">
  <iframe
    id="video-ch5-4"
    title="STAT 350 ‚Äì Chapter 5.4 Spread of a Discrete Random Variable Video"
    src="https://www.youtube.com/embed/gA4f4mpjGk0?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
    allowfullscreen>
  </iframe>
</div><section id="varianace-of-a-discrete-random-variable">
<h1><span class="section-number">5.4. </span>Varianace of a Discrete Random Variable<a class="headerlink" href="#varianace-of-a-discrete-random-variable" title="Link to this heading">ÔÉÅ</a></h1>
<p>Just as the expected value tells us about the center of a probability distribution, we often need
to quantify how spread out or dispersed the values are around this center.
Variance and standard deviation provide this crucial second dimension to our
understanding of random variables.</p>
<div class="important admonition">
<p class="admonition-title">Road Map üß≠</p>
<ul class="simple">
<li><p>Define <strong>variance</strong> and <strong>standard deviation</strong> for discrete random variables.</p></li>
<li><p>Explore an alternative <strong>computational formula</strong> for variance.</p></li>
<li><p>Derive key <strong>properties of variance</strong> for linear transformations and sums.</p></li>
</ul>
</div>
<section id="from-sample-to-population-defining-variance">
<h2><span class="section-number">5.4.1. </span>From Sample to Population: Defining Variance<a class="headerlink" href="#from-sample-to-population-defining-variance" title="Link to this heading">ÔÉÅ</a></h2>
<p>In our exploration of sample statistics, we measured the spread of data using
sample variance‚Äîthe average of squared deviations from the mean.
For random variables, we take a similar approach, but with an important twist:
instead of averaging deviations with equal weights, we <strong>weight each deviation by
its probability</strong>.</p>
<section id="definition">
<h3>Definition<a class="headerlink" href="#definition" title="Link to this heading">ÔÉÅ</a></h3>
<p>The variance of a discrete random variable <span class="math notranslate nohighlight">\(X\)</span>, denoted <span class="math notranslate nohighlight">\(Var(X)\)</span> or <span class="math notranslate nohighlight">\(\sigma^2_X\)</span>,
is the expected value of the squared deviation from its mean:</p>
<div class="math notranslate nohighlight">
\[\sigma_X^2 = \text{Var}(X) = E[(X - \mu_X)^2] = \sum_{x\in\text{supp}(X)} (x - \mu_X)^2 p_X(x)\]</div>
<p>The standard deviation, denoted <span class="math notranslate nohighlight">\(\sigma_X\)</span>, is simply the square root of the variance:</p>
<div class="math notranslate nohighlight">
\[\sigma_X = \sqrt{\text{Var}(X)} = \sqrt{E[(X - \mu_X)^2]}\]</div>
<p>Note the <strong>variance has squared units</strong> (e.g., dollars¬≤ if <span class="math notranslate nohighlight">\(X\)</span> is in dollars).
The standard deviation returns us to the original units,
making it often more interpretable in practice.</p>
<p>This definition requires that the series be absolutely convergent for the variance to
be well-defined.</p>
</section>
<section id="a-computational-shortcut-for-variance">
<h3>A Computational Shortcut for Variance<a class="headerlink" href="#a-computational-shortcut-for-variance" title="Link to this heading">ÔÉÅ</a></h3>
<p>Calculating variance directly from its definition can be cumbersome,
especially when the mean <span class="math notranslate nohighlight">\(\mu_X\)</span> is not a simple value. Fortunately,
there‚Äôs an equivalent formula that is typically easier to apply:</p>
<div class="math notranslate nohighlight">
\[\sigma_X^2 = E[X^2] - \mu_X^2.\]</div>
<p>The derivation of this formula follows from expanding the squared
term in the original definition:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\text{Var}(X) &amp;= E[(X - \mu_X)^2] \\
&amp;= E[X^2 - 2X\mu_X + \mu_X^2] \\
&amp;= E[X^2] - 2\mu_X E[X] + \mu_X^2 \\
&amp;= E[X^2] - 2\mu_X \mu_X + \mu_X^2 \\
&amp;= E[X^2] - \mu_X^2
\end{align}\end{split}\]</div>
<p>This computational formula often simplifies the work significantly,
as we‚Äôll see in our examples.</p>
<div class="note admonition">
<p class="admonition-title">Exampleüí°: Bean &amp; Butter</p>
<p><strong>Bean &amp; Butter</strong> is a small campus caf√© that sells only two morning items: coffee
for $4 per cup and pastry for $3 each.
The shop records its sales in <em>waves</em>‚Äîeach wave is short enough that <span class="math notranslate nohighlight">\(X\)</span> (cups of coffee)
and <span class="math notranslate nohighlight">\(Y\)</span> (pastries) follow a stable pattern but long enough to summarize cleanly.</p>
<p>It is known that <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent.
The sales distribution for a single wave is:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Outcome</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(p_X(x)\)</span> (coffee)</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(p_Y(y)\)</span> (pastry)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>0.20</p></td>
<td><p>0.30</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>0.50</p></td>
<td><p>0.40</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>0.30</p></td>
<td><p>0.30</p></td>
</tr>
</tbody>
</table>
<p>Let us first compute the expected sales <em>count</em> of coffee and pastries.</p>
<div class="math notranslate nohighlight">
\[\begin{split}E[X] &amp;= (0) (0.2) + (1) (0.5) + (2) (0.3) = 1.1\\
E[Y] &amp;= (0) (0.3) + (1) (0.4) + (2) (0.3) = 1.0\end{split}\]</div>
<p>On average, 1.1 cups of cofee and 1.0 patry are sold per wave.</p>
<p>For staffing, buying milk, or setting aside cash for the till,
the owner also cares about <em>variability</em> of sales‚Äìhow much
does an individual wave fluctuate from the average?</p>
<p>To answer this question, we compute the variance and standard deviation
of each random variable. For cofee,</p>
<div class="math notranslate nohighlight">
\[\begin{split}E[X^2] &amp;= (0^2) (0.2) + (1^2) (0.5) + (2^2) (0.3) = 1.7\\
\text{Var}(X) &amp;= E[X^2]- (E[X])^2 =1.7 - 1.1^2 = 0.49\\
\sigma_X &amp;= \sqrt{0.49} \approx 0.70\end{split}\]</div>
<p>Similarly for pastries:</p>
<div class="math notranslate nohighlight">
\[\begin{split}E[Y^2] &amp;= (0^2) (0.3) + (1^2) (0.4) + (2^2) (0.3) = 1.6\\
\text{Var}(Y) &amp;= 1.6 - 1.0^2 = 0.60\\
\sigma_Y &amp;= \sqrt{0.60} \approx 0.77\end{split}\]</div>
<p>A standard deviation of about <strong>0.70 coffees</strong> and <strong>0.77 pastries</strong> tells us that,
in a typical wave, each count strays by <strong>roughly three-quarters of an item</strong> from
its own average.</p>
</div>
</section>
</section>
<section id="properties-of-variance">
<h2><span class="section-number">5.4.2. </span>Properties of Variance<a class="headerlink" href="#properties-of-variance" title="Link to this heading">ÔÉÅ</a></h2>
<p>Variance follows several key properties that make calculations more manageable,
especially when dealing with linear transformations of random variables.</p>
<section id="variance-of-linear-transformations">
<h3>1. Variance of Linear Transformations<a class="headerlink" href="#variance-of-linear-transformations" title="Link to this heading">ÔÉÅ</a></h3>
<p>For a linear transformation of a random variable, <span class="math notranslate nohighlight">\(g(X) = aX + b\)</span>,
where <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> are constants:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(aX + b) = a^2 \text{Var}(X).\]</div>
<p>Notice two important implications:</p>
<ul class="simple">
<li><p>Scaling a random variable by a factor of <span class="math notranslate nohighlight">\(a\)</span> multiplies its variance by <span class="math notranslate nohighlight">\(a^2\)</span>.</p></li>
<li><p>Adding a constant <span class="math notranslate nohighlight">\(b\)</span> has no effect on variance.</p></li>
</ul>
<p>This makes intuitive sense. Multiplying all values by <span class="math notranslate nohighlight">\(a\)</span> stretches (or compresses)
the distribution, amplifying (or reducing) the deviations  also by a factor of <span class="math notranslate nohighlight">\(a\)</span>.
But since deviations are squared in the variance calculation,
the variance increases by a factor of <span class="math notranslate nohighlight">\(a^2\)</span>.
Meanwhile, shifting all values by adding a constant <span class="math notranslate nohighlight">\(b\)</span> moves the entire distribution
without stretching or compressing its <em>width</em>.</p>
<p>We can prove this property using the computational formula for variance:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\text{Var}(aX + b) &amp;= E[(aX + b)^2] - (E[aX + b])^2 \\
&amp;= E[a^2X^2 + 2abX + b^2] - (a\mu_X + b)^2 \\
&amp;= a^2E[X^2] + 2abE[X] + b^2 - a^2\mu_X^2 - 2ab\mu_X - b^2 \\
&amp;= a^2E[X^2] + 2ab\mu_X + b^2 - a^2\mu_X^2 - 2ab\mu_X - b^2 \\
&amp;= a^2E[X^2] - a^2\mu_X^2 \\
&amp;= a^2(E[X^2] - \mu_X^2) \\
&amp;= a^2\text{Var}(X)
\end{align}\end{split}\]</div>
</section>
<section id="variance-of-sums-of-independent-rvs">
<h3>2. Variance of Sums of Independent RVs<a class="headerlink" href="#variance-of-sums-of-independent-rvs" title="Link to this heading">ÔÉÅ</a></h3>
<p>For <strong>independent</strong> random variables, the variance of their sum
equals the sum of their individual variances:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(X \pm Y) = \text{Var}(X) + \text{Var}(Y)\]</div>
<p>This extends to any number of <strong>mutually independent</strong> random variables:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(X_1 \pm X_2 \pm \cdots \pm X_n) = \text{Var}(X_1) + \text{Var}(X_2) + \cdots + \text{Var}(X_n)\]</div>
<div class="important admonition">
<p class="admonition-title">Why do the negative signs disappear?</p>
<p>You can think of the negative signs as coefficients (-1)
multiplied to the following random variable. Then using the first
property of variance,</p>
<div class="math notranslate nohighlight">
\[\begin{split}Var(X-Y) &amp;= Var(X + (-1)Y)\\
&amp;= Var(X)+(-1)^2Var(Y) = Var(X) + Var(Y).\end{split}\]</div>
</div>
</section>
<section id="variance-of-linear-combinations-of-independent-rvs">
<h3>3. Variance of Linear Combinations of Independent RVs<a class="headerlink" href="#variance-of-linear-combinations-of-independent-rvs" title="Link to this heading">ÔÉÅ</a></h3>
<p>For linear combinations of independent random variables:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(aX \pm bY) = a^2\text{Var}(X) + b^2\text{Var}(Y)\]</div>
<p>This simply combines the two properties we‚Äôve just seen.</p>
</section>
<section id="using-variance-properties-to-compute-standard-deviation">
<h3>Using Variance Properties to Compute Standard Deviation<a class="headerlink" href="#using-variance-properties-to-compute-standard-deviation" title="Link to this heading">ÔÉÅ</a></h3>
<p>Properties of variances listed in this section does not apply
to standard deviations, in general. To compute the standard
deviation of a linear compbination of random variables, always
<strong>compute the variance first, then take its square root</strong>.</p>
<div class="note admonition">
<p class="admonition-title">Exampleüí°: Bean &amp; Butter, Continued</p>
<p>Consider the revenue per wave at Beans &amp; Butter:</p>
<div class="math notranslate nohighlight">
\[R = 4X + 3Y.\]</div>
<p>Item-wise revenues are first computed by multiplying the price of each item by
its sales count. These individual revenues are then added up to obtain the total revenue.</p>
<p><strong>What is the standard deviation of the total revenue?</strong></p>
<p>We begin by computing the variance of <span class="math notranslate nohighlight">\(R\)</span>. Since <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are
<strong>independent</strong>, we can use the third property of variance:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\text{Var}(R) &amp;= 4^2 \text{Var}(X) + 3^2 \text{Var}(Y)\\
            &amp;= 16(0.49) + 9(0.60) = 13.24\\
\sigma_R &amp;= \sqrt{13.24} \approx \$3.64
\end{aligned}\end{split}\]</div>
<p>The standard deviation of revenue per wave is $3.64.</p>
<p>Suppose a new random variable <span class="math notranslate nohighlight">\(Z\)</span> represents the <strong>cost</strong> per wave of
running the store. It is known that <span class="math notranslate nohighlight">\(\sigma_Z = 2.2\)</span> and that <span class="math notranslate nohighlight">\(Z\)</span>
is independent of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p><strong>What is the standard deviation in the total profit per wave?</strong></p>
<p>The total profit can be expressed as <span class="math notranslate nohighlight">\(P = R - Z\)</span>.</p>
<p>Again, begin by computing the variance of <span class="math notranslate nohighlight">\(P\)</span> first. Because <span class="math notranslate nohighlight">\(R\)</span> and <span class="math notranslate nohighlight">\(Z\)</span>
are independent, we can use the second property of variance:</p>
<div class="math notranslate nohighlight">
\[\begin{split}Var(P) &amp;= Var(R) + Var(Z) = 13.24 + 2.2^2 = 18.08\\
\sigma_P &amp;= \sqrt{18.08} \approx 4.2521.\end{split}\]</div>
<p>Note that the negative sign between <span class="math notranslate nohighlight">\(R\)</span> and <span class="math notranslate nohighlight">\(Z\)</span> disappears.</p>
</div>
</section>
</section>
<section id="common-mistakes-to-avoid">
<h2><span class="section-number">5.4.3. </span>Common Mistakes to Avoid<a class="headerlink" href="#common-mistakes-to-avoid" title="Link to this heading">ÔÉÅ</a></h2>
<p>When working with variance and standard deviation, be careful to avoid these common errors:</p>
<div class="error admonition">
<p class="admonition-title">Common Mistakes to Avoid  üõë</p>
<ol class="arabic simple">
<li><p><strong>Forgetting to square the coefficient in variance</strong></p></li>
</ol>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(Var(aX) = a¬≤Var(X)\)</span>, not <span class="math notranslate nohighlight">\(aVar(X)\)</span>.</p>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p><strong>Not including the negative sign when squaring the coefficient</strong></p></li>
</ol>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(Var(-aX) = (-a)^2Var(X)\)</span>. <span class="math notranslate nohighlight">\((-a)^2\)</span> is positive!</p>
</div></blockquote>
<ol class="arabic simple" start="3">
<li><p><strong>Assuming standard deviations add</strong></p></li>
</ol>
<blockquote>
<div><p>For independent <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, <span class="math notranslate nohighlight">\(\sigma_{X+Y} ‚â† \sigma_X + \sigma_Y.\)</span>
Always add variances first, then take the square root.</p>
</div></blockquote>
<ol class="arabic simple" start="4">
<li><p><strong>Blindly applying the independence formula</strong></p></li>
</ol>
<blockquote>
<div><p>The formula <span class="math notranslate nohighlight">\(Var(X + Y) = Var(X) + Var(Y)\)</span> only applies when
<span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent.</p>
</div></blockquote>
<ol class="arabic simple" start="5">
<li><p><strong>Calculating</strong> <span class="math notranslate nohighlight">\(E[X]^2\)</span> <strong>instead of</strong> <span class="math notranslate nohighlight">\(E[X^2]\)</span></p></li>
</ol>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(E[X]^2\)</span> and <span class="math notranslate nohighlight">\(E[X^2]\)</span> are different! <span class="math notranslate nohighlight">\(E[X^2]\)</span> is found by squaring
individual outcomes first, then taking their expectation.</p>
</div></blockquote>
</div>
</section>
<section id="bringing-it-all-together">
<h2><span class="section-number">5.4.4. </span>Bringing It All Together<a class="headerlink" href="#bringing-it-all-together" title="Link to this heading">ÔÉÅ</a></h2>
<div class="important admonition">
<p class="admonition-title">Key Takeaways üìù</p>
<ol class="arabic simple">
<li><p>The <strong>variance</strong> of a discrete random variable is the expected value of the
squared deviation from its mean, measuring how spread out the distribution is.</p></li>
<li><p>The <strong>standard deviation</strong> is the square root of the variance and has the same
units as the original random variable.</p></li>
<li><p><span class="math notranslate nohighlight">\(Var(X) = E[X^2] - (E[X])^2\)</span> is often used as computational shortcut for variance.</p></li>
<li><p>For linear transformations, <span class="math notranslate nohighlight">\(Var(aX + b) = a^2Var(X)\)</span>, meaning that scaling affects
variance quadratically while shifting has no effect.</p></li>
<li><p>For independent random variables, <span class="math notranslate nohighlight">\(Var(X \pm Y) = Var(X) + Var(Y)\)</span>,
showing that variances (not standard deviations) add for independent variables.</p></li>
<li><p>When calculating any standard deviation, compute the variance first,
then take the square root.</p></li>
</ol>
</div>
<p>In the next section, we‚Äôll explore how to handle dependent random variables,
where the relationship between variables adds another layer of complexity
to our analysis.</p>
<section id="exercises">
<h3>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">ÔÉÅ</a></h3>
<ol class="arabic simple">
<li><p><strong>Basic Calculations</strong>: For a random variable <span class="math notranslate nohighlight">\(X\)</span> with PMF
<span class="math notranslate nohighlight">\(p_X(0) = 0.2, p_X(1) = 0.5\)</span>, and <span class="math notranslate nohighlight">\(p_X(2) = 0.3\)</span>:</p>
<ol class="loweralpha simple">
<li><p>Calculate <span class="math notranslate nohighlight">\(E[X]\)</span> and <span class="math notranslate nohighlight">\(Var(X)\)</span>.</p></li>
<li><p>Calculate <span class="math notranslate nohighlight">\(E[2X + 3]\)</span> and <span class="math notranslate nohighlight">\(Var(2X + 3)\)</span>.</p></li>
</ol>
</li>
<li><p><strong>Game of Chance</strong>: In a certain game, you flip a fair coin. If it lands heads,
you win $5; if it lands tails, you lose $3.</p>
<ol class="loweralpha simple">
<li><p>Let <span class="math notranslate nohighlight">\(X\)</span> be your net gain. Find <span class="math notranslate nohighlight">\(E[X]\)</span> and <span class="math notranslate nohighlight">\(Var(X)\)</span>.</p></li>
<li><p>If you play this game 100 times independently, what is the expected value and
variance of your total net gain?</p></li>
<li><p>What is the standard deviation of your total net gain after 100 plays?</p></li>
</ol>
</li>
</ol>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="5-3-expected-value-of-discrete-rv.html" class="btn btn-neutral float-left" title="5.3. Expected Value of a Discrete Random Variable" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="5-5-covariance-of-dependent-rvs.html" class="btn btn-neutral float-right" title="5.5. Covariance of Dependent Random Variables" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>