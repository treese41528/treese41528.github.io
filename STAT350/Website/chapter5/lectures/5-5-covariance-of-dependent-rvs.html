

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>5.5. Covariance of Dependent Random Variables &mdash; STAT 350</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=bcfce81d" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT350/course_site/chapter5/lectures/5-5-covariance-of-dependent-rvs.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="5.6. The Binomial Distribution" href="5-6-binomial-distribution.html" />
    <link rel="prev" title="5.4. Varianace of a Discrete Random Variable" href="5-4-variance-of-discrete-rv.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Orientation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../course-intro.html">Course Introduction &amp; Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#why-statistics-why-now">Why Statistics? Why Now?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#course-roadmap">Course Roadmap</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#a-clear-note-on-using-ai">A Clear Note on Using AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#getting-started-a-checklist">Getting Started: A Checklist</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../course-intro.html#homework-assignments-on-edfinity">Homework Assignments on Edfinity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../course-intro.html#miscellaneous-tips">Miscellaneous Tips</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Chapters</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../chapter1/index.html">1. Introduction to Statistics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter1/lectures/1-1-intro.html">1.1. What Is Statistics?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter1/lectures/1-2-probability-inference.html">1.2. Probability &amp; Statistical Inference: How Are They Associated?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter2/index.html">2. Graphical Summaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-1-understanding-the-structure-of-data-set.html">2.1. Data Set Structure and Variable Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-2-tools-for-categorical-qualitative-data.html">2.2. Tools for Categorical (Qualitative) Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-3-tools-for-numerical-quantitative-data.html">2.3. Tools for Numerical (Quantitative) Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-4-exploring-quantitative-distributions-modality-shape-and-outliers.html">2.4. Exploring Quantitative Distributions: Modality, Shape &amp; Outliers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter3/index.html">3. Numerical Summaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-1-intro-numerical-summaries.html">3.1. Introduction to Numerical Summaries: Notation and Terminology</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-2-measures-of-central-tendency.html">3.2. Measures of Central Tendency</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-3-measures-of-variability-range-variance-and-SD.html">3.3. Measures of Variability - Range, Variance, and Standard Deviation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-4-measures-of-variability-IQR-and-5-number-summary.html">3.4. Measures of Variability - Interquartile Range and Five-Number Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-5-choosing-the-right-measure-resistance-to-extreme-values.html">3.5. Choosing the Right Measure &amp; Comparing Measures Across Data Sets</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter4/index.html">4. Probability</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-1-basic-set-theory.html">4.1. Basic Set Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-2-probability.html">4.2. Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-3-conditional-probability.html">4.3. Conditional Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-4-law-of-total-probability-and-bayes-rule.html">4.4. Law of Total Probability and Bayes‚Äô Rule</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-5-bayes-update-rule-example.html">4.5. Bayesian Updating: Sequential Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-6-independence-of-events.html">4.6. Independence of Events</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">5. Discrete Distributions</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="5-1-discrete-rvs-and-pmfs.html">5.1. Discrete Random Variables and Probability Mass Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="5-2-joint-pmfs.html">5.2. Joint Probability Mass Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="5-3-expected-value-of-discrete-rv.html">5.3. Expected Value of a Discrete Random Variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="5-4-variance-of-discrete-rv.html">5.4. Varianace of a Discrete Random Variable</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">5.5. Covariance of Dependent Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="5-6-binomial-distribution.html">5.6. The Binomial Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="5-7-poisson-distribution.html">5.7. Poisson Distribution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter6/index.html">6. Continuous Distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-1-continuous-rvs-and-pdfs.html">6.1. Continuous Random Variables and Probability Density Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-2-expected-value-and-variance-of-cts-rvs.html">6.2. Expected Value and Variance of Continuous Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-3-cdfs.html">6.3. Cumulative Distribution Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-4-normal-distribution.html">6.4. Normal Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-5-uniform-distribution.html">6.5. Uniform Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-6-exponential-distribution.html">6.6. Exponential Distribution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter7/index.html">7. Sampling Distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-1-statistics-and-sampling-distributions.html">7.1. Statistics and Sampling Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-2-sampling-distribution-for-the-sample-mean.html">7.2. Sampling Distribution for the Sample Mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-3-clt.html">7.3. The Central Limit Theorem (CLT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-4-discret-rvs-and-clt.html">7.4. Discrete Random Variables and the CLT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter8/index.html">8. Experimental Design</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-1-experimental-and-sampling-designs.html">8.1. Experimental and Sampling Designs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-2-experimental-design-principles.html">8.2. Experimental Design Principles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-3-basic-types-of-experimental-design.html">8.3. Basic Types of Experimental Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-4-addressing-potential-flaws-in-experimental-design.html">8.4. Addressing Potential Flaws in Experimental Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-5-examples-of-experimental-design.html">8.5. Examples of Experimental Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-6-sampling-design.html">8.6. Sampling Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-7-sampling-bias.html">8.7. Sampling Bias</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter9/index.html">9. Confidence Intervals and Bounds</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-1-intro-statistical-inference.html">9.1. Introduction to Statistical Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-2-ci-sigma-known.html">9.2. Confidence Intervals for the Population Mean, When œÉ is Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-3-precision-of-ci-and-sample-size-calculation.html">9.3. Precision of a Confidence Interval and Sample Size Calculation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-4-cb-sigma-known.html">9.4. Confidence Bounds for the Poulation Mean When œÉ is Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-5-ci-cb-sigma-unknown.html">9.5. Confidence Intervals and Bounds When œÉ is Unknown</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter10/index.html">10. Hypothesis Testing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-1-ht-errors-and-power.html">10.1. Type I Error, Type II Error, and Power</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-2-ht-for-mean-sigma-known.html">10.2. Hypothesis Test for the Population Mean When œÉ is Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-3-ht-for-mean-sigma-unknown.html">10.3. Hypothesis Test for the Population Mean When œÉ Is Unknown</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-4-pvalue-significance-conclusion.html">10.4. P-values, Statistical Significance, and Formal Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter11/index.html">11. Two Sample Procedures</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-1-ci-ht-two-samples.html">11.1. Confidence Interval/Bound and Hypothesis Test for Two Samples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-2-comparing-two-means-independent-sigmas-known.html">11.2. Comparing the Means of Two Independent Populations - Population Variances Are Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-3-comparing-two-means-independent-pooled.html">11.3. Comparing the Means of Two Independent Populations - Pooled Variance Estimator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-4-comparing-two-means-independent-unpooled.html">11.4. Comparing the Means of Two Independent Populations - No Equal Variance Assumption</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-5-mean-of-paired-differences-two-dependent-populations.html">11.5. Analyzing the Mean of Paired Differences Between two Dependent Populations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter12/index.html">12. ANOVA</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-1-intro-one-way-anova.html">12.1. Introduction to One-Way ANOVA</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-2-sources-of-variability.html">12.2. Different Sources of Variability in an ANOVA Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-3-f-test-and-relationship-to-t-test.html">12.3. One-Way ANOVA F-Test and Its Relationship to Two-Sample t-Tests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-4-multiple-comparison-procedures-family-wise-error-rates.html">12.4. Multiple Comparison Procedures and Family-Wise Error Rates</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter13/index.html">13. Simple Linear Regression</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter13/lectures/13-1-intro-to-lr-correlation-scatter-plots.html">13.1. Introduction to Linear Regression: Correlation and Scatter Plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter13/lectures/13-2-simple-linear-regression.html">13.2. Simple Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter13/lectures/13-3-diagnostics-inference.html">13.3. Model Diagnostics and Statistical Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter13/lectures/13-4-prediction-robustness.html">13.4. Prediction, Robustness, and Applied Examples</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html"><span class="section-number">5. </span>Discrete Distributions</a></li>
      <li class="breadcrumb-item active"><span class="section-number">5.5. </span>Covariance of Dependent Random Variables</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/chapter5/lectures/5-5-covariance-of-dependent-rvs.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="video-placeholder">
  <iframe
    src="https://www.youtube.com/embed/xP5_W5ZtBYs?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
    allowfullscreen>
  </iframe>
</div><section id="covariance-of-dependent-random-variables">
<h1><span class="section-number">5.5. </span>Covariance of Dependent Random Variables<a class="headerlink" href="#covariance-of-dependent-random-variables" title="Link to this heading">ÔÉÅ</a></h1>
<p>Many real-world scenarios involve random variables that <strong>influence each other</strong>‚Äîdriving violations
may correlate with accident rates, stock prices often move together, and rainfall affects crop yields.
When random variables are dependent, their joint behavior becomes more complex, requiring us to understand
how they vary together.</p>
<div class="important admonition">
<p class="admonition-title">Road Map üß≠</p>
<ul class="simple">
<li><p>Introduce <strong>covariance</strong>, a measure of how random variables change together.</p></li>
<li><p>Define <strong>correlation</strong> as a standardized measure of relationship strength.</p></li>
<li><p>Extend variance formulas for <strong>sums of dependent random variables</strong>.</p></li>
<li><p>Explore the <strong>independence property</strong> and its effect on covariance.</p></li>
</ul>
</div>
<section id="beyond-independence-understanding-covariance">
<h2><span class="section-number">5.5.1. </span>Beyond Independence: Understanding Covariance<a class="headerlink" href="#beyond-independence-understanding-covariance" title="Link to this heading">ÔÉÅ</a></h2>
<p>When analyzing two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> together, we often want to know:
When <span class="math notranslate nohighlight">\(X\)</span> is large, does <span class="math notranslate nohighlight">\(Y\)</span> also tend to be larger?
Or do <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> tend to move in the opposite direction?
Covariance provides a mathematical way to measure this relationship.</p>
<section id="definition">
<h3>Definition<a class="headerlink" href="#definition" title="Link to this heading">ÔÉÅ</a></h3>
<p>The <strong>covariance</strong> between two discrete random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>,
denoted <span class="math notranslate nohighlight">\(Cov(X,Y)\)</span> or <span class="math notranslate nohighlight">\(\sigma_{XY}\)</span>, is defined as:</p>
<div class="math notranslate nohighlight">
\[\sigma_{XY} = \text{Cov}(X,Y) = E[(X - \mu_X)(Y - \mu_Y)]\]</div>
</section>
<section id="interpreting-the-formula">
<h3>Interpreting the formula<a class="headerlink" href="#interpreting-the-formula" title="Link to this heading">ÔÉÅ</a></h3>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> tend to be simultaneously above their means or simultaneously
below their means, their covariance will be positive.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(Y\)</span> tends to be below its mean when <span class="math notranslate nohighlight">\(X\)</span> is above its mean, and vice versa,
their covariance will be negative.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> have no systematic relationship, their covariance will be close to zero.</p></li>
<li><p>In general, the covariance describes the strength (magnitude) and direction (sign) of
the <strong>linear relationship</strong> between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.</p></li>
</ul>
</section>
<section id="computational-shortcut">
<h3>Computational shortcut<a class="headerlink" href="#computational-shortcut" title="Link to this heading">ÔÉÅ</a></h3>
<p>Covariance has a computational shortcut similar to that of variance.</p>
<div class="math notranslate nohighlight">
\[\sigma_{XY}= E[XY] - \mu_X\mu_Y.\]</div>
<p>Its derivation is analogous to the derivation of the shortcut for variance.
We leave it as an indepdent exercise.</p>
<p>Also note that computing covariance requires working with the joint probability mass function:</p>
<div class="math notranslate nohighlight">
\[E[XY] = \sum_{(x,y)\in \text{supp}(X,Y)} xy \, p_{X,Y}(x,y)\]</div>
<div class="note admonition">
<p class="admonition-title">Exampleüí°: Salamander Insurance Company (SIC), Continued</p>
<p>Recall Salamander Insurance Company (SIC), who keeps track of the proababilities of
moving violations (<span class="math notranslate nohighlight">\(X\)</span>) and accidents (<span class="math notranslate nohighlight">\(Y\)</span>) made by there customers.</p>
<table class="docutils align-center" style="width: 80%">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(x\)</span> \ <span class="math notranslate nohighlight">\(y\)</span></p></th>
<th class="head"><p>0</p></th>
<th class="head"><p>1</p></th>
<th class="head"><p>2</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(p_X(x)\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>0.58</p></td>
<td><p>0.015</p></td>
<td><p>0.005</p></td>
<td><p>0.60</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>0.18</p></td>
<td><p>0.058</p></td>
<td><p>0.012</p></td>
<td><p>0.25</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>0.02</p></td>
<td><p>0.078</p></td>
<td><p>0.002</p></td>
<td><p>0.10</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>0.02</p></td>
<td><p>0.029</p></td>
<td><p>0.001</p></td>
<td><p>0.05</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(p_Y(y)\)</span></p></td>
<td><p>0.80</p></td>
<td><p>0.18</p></td>
<td><p>0.02</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
<p>SIC wants to know whether the number of moving violations (<span class="math notranslate nohighlight">\(X\)</span>) and the number
accidents (<span class="math notranslate nohighlight">\(Y\)</span>) made by a customer are linearly associated. To answer this
question, we must compute the covariance of the two random variables.</p>
<p>We already know:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(E[X] = 0.6\)</span> (average number of moving violations)</p></li>
<li><p><span class="math notranslate nohighlight">\(E[Y] = 0.22\)</span> (average number of accidents)</p></li>
</ul>
<p>from the previous examples. To calculate the covariance, we need to find <span class="math notranslate nohighlight">\(E[XY]\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}E[XY] =&amp; \sum_{(x,y)\in \text{supp}(X,Y)} xy \, p_{X,Y}(x,y) \\
=&amp; 1 \cdot 1 \cdot 0.058 + 1 \cdot 2 \cdot 0.012 + 2 \cdot 1 \cdot 0.078 \\
&amp;+ 2 \cdot 2 \cdot 0.002 + 3 \cdot 1 \cdot 0.029 + 3 \cdot 2 \cdot 0.001 \\
=&amp; 0.339\end{split}\]</div>
<p>Now we can compute the covariance:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{Cov}(X,Y) &amp;= E[XY] - E[X]E[Y] = 0.339 - 0.6 \cdot 0.22\\
&amp;= 0.339 - 0.132 = 0.207\end{split}\]</div>
<p>The positive covariance indicates that customers with more moving
violations tend to have more accidents, which aligns with our
intuition about driving behavior. However, it is not easy to assess
the strength of this relationship with covariance alone.
To evaluate the strength more objectively, we now turn to our next topic.</p>
</div>
</section>
</section>
<section id="correlation-a-standardized-measure">
<h2><span class="section-number">5.5.2. </span>Correlation: A Standardized Measure<a class="headerlink" href="#correlation-a-standardized-measure" title="Link to this heading">ÔÉÅ</a></h2>
<p>The sign of the covariance tells us the direction of the relationship,
but its <strong>magnitude is difficult to interpret</strong> since it depends on the scales of X and Y.
For instance, if we measured X in inches and then converted to centimeters,
the covariance would change even though the underlying relationship remains the same.</p>
<p>To address the scale dependency of covariance, we use correlation, which <strong>standardizes
covariance to a value between -1 and +1</strong>.</p>
<section id="id1">
<h3>Definition<a class="headerlink" href="#id1" title="Link to this heading">ÔÉÅ</a></h3>
<p>The <strong>correlation</strong> between two discrete random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, denoted
<span class="math notranslate nohighlight">\(\rho_{XY}\)</span>, is defined as:</p>
<div class="math notranslate nohighlight">
\[\rho_{XY} = \frac{\sigma_{XY}}{\sigma_X \sigma_Y}.\]</div>
<p>From the formula, we can say that the correlation is obtained by <strong>taking the covariance,
then removing the scales</strong> of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> by diving by both <span class="math notranslate nohighlight">\(\sigma_X\)</span>
and <span class="math notranslate nohighlight">\(\sigma_Y\)</span>.</p>
<p>This standardization provides several advantages:</p>
<ul class="simple">
<li><p>Correlation is always between -1 and +1.</p></li>
<li><p>A correlation of +1 indicates a perfect positive linear relationship.</p></li>
<li><p>A correlation of -1 indicates a perfect negative linear relationship.</p></li>
<li><p>A correlation of 0 suggests no linear relationship.</p></li>
<li><p>Being unitless, correlation allows for meaningful comparisons across different variable pairs.</p></li>
</ul>
<figure class="align-center" id="id2" style="width: 90%">
<img alt="Distributions with different correlations" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter5/correlation-plots.png" />
<figcaption>
<p><span class="caption-number">Fig. 5.4 </span><span class="caption-text">Plots of joint distributions witth varying degrees of correlation</span><a class="headerlink" href="#id2" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<div class="note admonition">
<p class="admonition-title">Eaxampleüí°: Salamander Insurance Company (SIC), Continued</p>
<p>Let us compute the correlation between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> for an
objective assessment of the strength of their linear relationship.</p>
<table class="docutils align-center" style="width: 80%">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(x\)</span> \ <span class="math notranslate nohighlight">\(y\)</span></p></th>
<th class="head"><p>0</p></th>
<th class="head"><p>1</p></th>
<th class="head"><p>2</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(p_X(x)\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>0.58</p></td>
<td><p>0.015</p></td>
<td><p>0.005</p></td>
<td><p>0.60</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>0.18</p></td>
<td><p>0.058</p></td>
<td><p>0.012</p></td>
<td><p>0.25</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>0.02</p></td>
<td><p>0.078</p></td>
<td><p>0.002</p></td>
<td><p>0.10</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>0.02</p></td>
<td><p>0.029</p></td>
<td><p>0.001</p></td>
<td><p>0.05</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(p_Y(y)\)</span></p></td>
<td><p>0.80</p></td>
<td><p>0.18</p></td>
<td><p>0.02</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
<p>We already know:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(E[X] = 0.6\)</span> (average number of moving violations)</p></li>
<li><p><span class="math notranslate nohighlight">\(E[Y] = 0.22\)</span> (average number of accidents)</p></li>
<li><p><span class="math notranslate nohighlight">\(E[X^2] = 1.1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Cov(X,Y) = 0.207\)</span></p></li>
</ul>
<p>from the previous examples. To use the formula for correlation, we must find
the standard deviations of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}Var(X) &amp;= E[X^2] - (E[X])^2\\
&amp;= 1.1 - (0.6)^2 = 0.74\\
\sigma_X &amp; = \sqrt{0.74} \approx 0.8602\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}Var(Y) &amp;= E[Y^2] - (E[Y])^2 \\
&amp;= [(1^2)(0.18) + (2^2)(0.02)]- (0.22)^2\\
&amp;= 0.26 - 0.0484 = 0.2116\\
\sigma_Y &amp;= \sqrt{0.2116} = 0.46.\end{split}\]</div>
<p>Now the correlation is</p>
<div class="math notranslate nohighlight">
\[\rho_{X,Y} = \frac{Cov(X,Y)}{\sigma_X \sigma_Y}
= \frac{0.207}{(0.8602)(0.46)} \approx 0.5231.\]</div>
<p>We now see that the positive linear association between
<span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are moderate.</p>
</div>
</section>
</section>
<section id="independence-and-covariance">
<h2><span class="section-number">5.5.3. </span>Independence and Covariance<a class="headerlink" href="#independence-and-covariance" title="Link to this heading">ÔÉÅ</a></h2>
<section id="theorem-independence-implies-zero-covariance">
<h3>Theorem: Independence implies Zero Covariance<a class="headerlink" href="#theorem-independence-implies-zero-covariance" title="Link to this heading">ÔÉÅ</a></h3>
<p>If X and Y are independent random variables, then</p>
<div class="math notranslate nohighlight">
\[\text{Cov}(X,Y) = 0.\]</div>
<section id="proof-of-theorem">
<h4>Proof of theorem<a class="headerlink" href="#proof-of-theorem" title="Link to this heading">ÔÉÅ</a></h4>
<blockquote>
<div><p>We use the expectation independence property:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
E[XY] &amp;= \sum_{x,y: p_{X,Y}(x,y) &gt; 0} xy \, p_{X,Y}(x,y) \\
&amp;= \sum_{x: p_X(x) &gt; 0} \sum_{y: p_Y(y) &gt; 0} xy \, p_X(x)p_Y(y) \\
&amp;= \sum_{x: p_X(x) &gt; 0} x \, p_X(x) \sum_{y: p_Y(y) &gt; 0} y \, p_Y(y) \\
&amp;= E[X] \cdot E[Y] = \mu_X \mu_Y
\end{align}\end{split}\]</div>
<p>Therefore,</p>
<div class="math notranslate nohighlight">
\[\text{Cov}(X,Y) = E[XY] - \mu_X\mu_Y = \mu_X\mu_Y - \mu_X\mu_Y = 0.\]</div>
</div></blockquote>
<p>This property is crucial because it allows us to determine when we can use the simpler variance formulas for sums of independent random variables. If covariance is non-zero, we must account for the dependence.</p>
</section>
</section>
<section id="zero-covariance-does-not-imply-independence">
<h3>Zero Covariance Does Not Imply Independence<a class="headerlink" href="#zero-covariance-does-not-imply-independence" title="Link to this heading">ÔÉÅ</a></h3>
<p>It‚Äôs important to note that the converse of the previous theorem is not always
true‚Äî<strong>a zero covariance does not necessarily imply independence</strong>.
This is because ‚Äúno linear relationship‚Äù does not rule out other types of relationships.
See <a class="reference internal" href="#zero-cov-dependent-plots"><span class="std std-numref">Fig. 5.5</span></a> for some examples:</p>
<figure class="align-center" id="id3" style="width: 90%">
<span id="zero-cov-dependent-plots"></span><img alt="Dependent distributions with zero covariance" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter5/zero-cov-dependent-plots.png" />
<figcaption>
<p><span class="caption-number">Fig. 5.5 </span><span class="caption-text">Dependent distributions with zero covariance</span><a class="headerlink" href="#id3" title="Link to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="variance-of-sums-of-dependent-random-variables">
<h2><span class="section-number">5.5.4. </span>Variance of Sums of Dependent Random Variables<a class="headerlink" href="#variance-of-sums-of-dependent-random-variables" title="Link to this heading">ÔÉÅ</a></h2>
<p>When random variables are dependent, the variance of their sum (or difference)
includes an additional term that accounts for their covariance:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(X \pm Y) = \text{Var}(X) + \text{Var}(Y) \pm 2\text{Cov}(X,Y)\]</div>
<p>For linear combinations:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(aX + bY) = a^2\text{Var}(X) + b^2\text{Var}(Y) + 2ab\text{Cov}(X,Y)\]</div>
<p>These formulas highlight a critical insight: dependence between random variables can either increase or decrease the variance of their sum, depending on whether the covariance is positive (variables tend to move together) or negative (variables tend to offset each other).</p>
<p>For n dependent random variables, the formula extends to:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(X_1 + X_2 + \ldots + X_n) = \sum_{i=1}^{n} \text{Var}(X_i) + 2 \sum_{i=1}^{n} \sum_{j&gt;i}^{n} \text{Cov}(X_i, X_j)\]</div>
<p>This formula includes the variance of each individual random variable plus the covariance between each pair of variables.</p>
<div class="note admonition">
<p class="admonition-title">Exampleüí°: SIC, Continued</p>
<p>SIC is planning a promotional offer based on a risk score <span class="math notranslate nohighlight">\(Z = 2X + 5Y\)</span>,
which combines both factors with different weights. The company wants
to know the average value and standard deviation of <strong>the sum of these scores for
its 35 customers</strong>.</p>
<table class="docutils align-center" style="width: 80%">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(x\)</span> \ <span class="math notranslate nohighlight">\(y\)</span></p></th>
<th class="head"><p>0</p></th>
<th class="head"><p>1</p></th>
<th class="head"><p>2</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(p_X(x)\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>0.58</p></td>
<td><p>0.015</p></td>
<td><p>0.005</p></td>
<td><p>0.60</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>0.18</p></td>
<td><p>0.058</p></td>
<td><p>0.012</p></td>
<td><p>0.25</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>0.02</p></td>
<td><p>0.078</p></td>
<td><p>0.002</p></td>
<td><p>0.10</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>0.02</p></td>
<td><p>0.029</p></td>
<td><p>0.001</p></td>
<td><p>0.05</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(p_Y(y)\)</span></p></td>
<td><p>0.80</p></td>
<td><p>0.18</p></td>
<td><p>0.02</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
<p><strong>Calculate the Expected Value of Z</strong></p>
<p>The expected value of Z is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}E[Z] &amp;= E[2X + 5Y] = 2E[X] + 5E[Y]\\
&amp;= 2 \cdot 0.6 + 5 \cdot 0.22 = 1.2 + 1.1 = 2.3.\end{split}\]</div>
<p>For all 35 customers combined:</p>
<div class="math notranslate nohighlight">
\[E[\sum_{i=1}^{35} Z_i] = 35 \cdot E[Z] = 35 \cdot 2.3 = 80.5\]</div>
<p><strong>Calculate the Variance of Z</strong></p>
<p>For a single customer, using the formula for the variance of a linear combination of dependent variables:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\text{Var}(Z) &amp;= \text{Var}(2X + 5Y) \\
&amp;= 2^2 \text{Var}(X) + 5^2 \text{Var}(Y) + 2 \cdot 2 \cdot 5 \cdot \text{Cov}(X,Y) \\
&amp;= 4 \cdot 0.74 + 25 \cdot 0.2116 + 20 \cdot 0.207 \\
&amp;= 2.96 + 5.29 + 4.14 \\
&amp;= 12.39
\end{align}\end{split}\]</div>
<p>Now, assuming the 35 customers are independent of each other (one customer‚Äôs driving behavior doesn‚Äôt affect another‚Äôs), the variance of the sum is:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(\sum_{i=1}^{35} Z_i) = 35 \cdot \text{Var}(Z) = 35 \cdot 12.39 = 433.65\]</div>
<p><strong>Calculate the Standard Deviation</strong></p>
<p>The standard deviation is the square root of the variance:</p>
<div class="math notranslate nohighlight">
\[\sigma_{\sum Z_i} = \sqrt{433.65} \approx 20.82\]</div>
<p>This standard deviation tells SIC how much typical variation to expect in the sum of
risk scores across their 35 customers‚Äîvaluable information for setting appropriate thresholds for their promotional offer.</p>
<p><strong>The Effect of Dependence on Risk Assessment</strong></p>
<p>It‚Äôs worth noting how the dependency between moving violations and accidents
affects SIC‚Äôs risk calculations. If we had incorrectly assumed that X and Y were independent
(ignoring their positive covariance of 0.207), the variance calculation would have been:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\text{Var}_{incorrect}(Z) &amp;= 4 \cdot 0.74 + 25 \cdot 0.2116 \\
&amp;= 2.96 + 5.29 \\
&amp;= 8.25
\end{align}\end{split}\]</div>
<p>This would have resulted in an underestimation of the variance by approximately 33% and
an underestimation of the standard deviation by about 18%. Such an error could lead to
significant mispricing of insurance policies or inadequate risk management.</p>
</div>
</section>
<section id="bringing-it-all-together">
<h2><span class="section-number">5.5.5. </span>Bringing It All Together<a class="headerlink" href="#bringing-it-all-together" title="Link to this heading">ÔÉÅ</a></h2>
<div class="important admonition">
<p class="admonition-title">Key Takeaways üìù</p>
<ol class="arabic simple">
<li><p><strong>Covariance</strong> measures how two random variables change together. Positive
values indicate that they tend to move in the same direction, and negative values
indicate opposite movements.</p></li>
<li><p><strong>Correlation</strong> standardizes covariance to a unitless measure between -1 and +1,
making it easier to interpret the strength of relationships regardless of variable scales.</p></li>
<li><p>Independent random variables have <strong>zero covariance</strong>, though zero covariance
doesn‚Äôt necessarily imply independence.</p></li>
<li><p>The <strong>variance of a linear combination of dependent random variables</strong> includes an additional
term accounting for their covariances: <span class="math notranslate nohighlight">\(Var(aX + bY) = a^2Var(X) + b^2Var(Y) + ab2Cov(X,Y)\)</span>.</p></li>
<li><p>Positive covariance <strong>increases the variance</strong> of a sum, while negative
covariance <strong>decreases it</strong>‚Äîreflecting how dependencies can either amplify
or mitigate variability.</p></li>
</ol>
</div>
<p>Understanding how random variables covary is essential for modeling complex systems
where independence is the exception rather than the rule. While the mathematics becomes
more involved when accounting for dependencies, the resulting models more faithfully
represent reality, leading to better decisions and predictions.</p>
<p>In our next section, we‚Äôll explore specific named discrete probability distributions
that occur frequently in practice, beginning with the binomial distribution‚Äîa foundational
model for many counting problems.</p>
<section id="exercises">
<h3>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">ÔÉÅ</a></h3>
<ol class="arabic">
<li><p><strong>Understanding Covariance</strong>: Two discrete random variables X and Y have the joint PMF:</p>
<table class="docutils align-center" style="width: 60%">
<thead>
<tr class="row-odd"><th class="head"><p>x \y</p></th>
<th class="head"><p>1</p></th>
<th class="head"><p>2</p></th>
<th class="head"><p>3</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>2</p></td>
<td><p>0.1</p></td>
<td><p>0.2</p></td>
<td><p>0.1</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>0.3</p></td>
<td><p>0.2</p></td>
<td><p>0.1</p></td>
</tr>
</tbody>
</table>
<ol class="loweralpha simple">
<li><p>Calculate the marginal PMFs for <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.</p></li>
<li><p>Calculate <span class="math notranslate nohighlight">\(E[X], E[Y], Var(X)\)</span>, and <span class="math notranslate nohighlight">\(Var(Y)\)</span>.</p></li>
<li><p>Calculate the covariance between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.</p></li>
<li><p>Calculate the correlation between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.</p></li>
<li><p>Interpret the meaning of the correlation in context.</p></li>
</ol>
</li>
<li><p><strong>Dependent vs. Independent Sums</strong>: Random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> have</p>
<p><span class="math notranslate nohighlight">\(Var(X) = 4\)</span>, <span class="math notranslate nohighlight">\(Var(Y) = 9\)</span>, and <span class="math notranslate nohighlight">\(Cov(X,Y) = -3\)</span>.</p>
<ol class="loweralpha simple">
<li><p>Calculate <span class="math notranslate nohighlight">\(Var(X + Y)\)</span> accounting for their dependence.</p></li>
<li><p>What would <span class="math notranslate nohighlight">\(Var(X + Y)\)</span> be if <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> were independent?</p></li>
<li><p>Explain why the variance is lower in the dependent case here.</p></li>
</ol>
</li>
<li><p><strong>Linear Combinations with Dependence</strong>: Random variables <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(Y\)</span>, and <span class="math notranslate nohighlight">\(Z\)</span> have
variances of 2, 3, and 4 respectively. The covariances are</p>
<p><span class="math notranslate nohighlight">\(Cov(X,Y) = 1\)</span>, <span class="math notranslate nohighlight">\(Cov(X,Z) = -1\)</span>, and <span class="math notranslate nohighlight">\(Cov(Y,Z) = 2\)</span>.</p>
<ol class="loweralpha simple">
<li><p>Calculate <span class="math notranslate nohighlight">\(Var(X + Y + Z)\)</span>.</p></li>
<li><p>Calculate <span class="math notranslate nohighlight">\(Var(2X - Y + 3Z)\)</span>.</p></li>
<li><p>Would a portfolio split equally among these three variables have more or less
risk than investing in just one variable? Explain.</p></li>
</ol>
</li>
<li><p><strong>Insurance Portfolio</strong>: An insurance company offers three types of policies:
auto (A), home (H), and life (L). The annual claims (in thousands of dollars)
for each policy type are random variables with</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(E[A] = 1.5, E[H] = 2.0, E[L] = 5.0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Var(A) = 2.25, Var(H) = 4.0  Var(L) = 16.0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Cov(A,H) = 1.5, Cov(A,L) = 0.5, Cov(H,L) = 1.0\)</span>.</p></li>
</ul>
<ol class="loweralpha simple">
<li><p>If the company has 100 auto policies, 80 home policies, and 50 life policies,
calculate the expected total annual claims.</p></li>
<li><p>Calculate the standard deviation of the total annual claims.</p></li>
<li><p>What would the standard deviation be if the claims from different policy types were independent?</p></li>
</ol>
</li>
<li><p><strong>Proving Properties</strong>: Suppose <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are dependent random variables.
Show that the following statements are true:</p>
<ol class="loweralpha simple">
<li><p><span class="math notranslate nohighlight">\(Var(X - Y) = Var(X) + Var(Y) - 2Cov(X,Y)\)</span>.</p></li>
<li><p>For any constant <span class="math notranslate nohighlight">\(c\)</span>, <span class="math notranslate nohighlight">\(Cov(X,c) = 0\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(Cov(X,X) = Var(X)\)</span>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(Z = aX + bY\)</span>, then <span class="math notranslate nohighlight">\(Cov(X,Z) = aVar(X) + bCov(X,Y)\)</span>.</p></li>
</ol>
</li>
</ol>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="5-4-variance-of-discrete-rv.html" class="btn btn-neutral float-left" title="5.4. Varianace of a Discrete Random Variable" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="5-6-binomial-distribution.html" class="btn btn-neutral float-right" title="5.6. The Binomial Distribution" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>