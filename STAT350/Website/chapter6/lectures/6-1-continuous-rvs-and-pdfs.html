

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>6.1. Continuous Random Variables and Probability Density Functions &mdash; STAT 350</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=bac617f8" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT350/course_site/chapter6/lectures/6-1-continuous-rvs-and-pdfs.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="6.2. Expected Value and Variance of Continuous Random Variables" href="6-2-expected-value-and-variance-of-cts-rvs.html" />
    <link rel="prev" title="6. Continuous Distributions" href="../index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Orientation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../course-intro.html">Course Introduction &amp; Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#why-statistics-why-now">Why Statistics? Why Now?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#course-roadmap">Course Roadmap</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#a-clear-note-on-using-ai">A Clear Note on Using AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#getting-started-a-checklist">Getting Started: A Checklist</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../course-intro.html#homework-assignments-on-edfinity">Homework Assignments on Edfinity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../course-intro.html#miscellaneous-tips">Miscellaneous Tips</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Chapters</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../chapter1/index.html">1. Introduction to Statistics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter1/lectures/1-1-intro.html">1.1. What Is Statistics?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter1/lectures/1-2-probability-inference.html">1.2. Probability &amp; Statistical Inference: How Are They Associated?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter2/index.html">2. Graphical Summaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-1-understanding-the-structure-of-data-set.html">2.1. Data Set Structure and Variable Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-2-tools-for-categorical-qualitative-data.html">2.2. Tools for Categorical (Qualitative) Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-3-tools-for-numerical-quantitative-data.html">2.3. Tools for Numerical (Quantitative) Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-4-exploring-quantitative-distributions-modality-shape-and-outliers.html">2.4. Exploring Quantitative Distributions: Modality, Shape &amp; Outliers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter3/index.html">3. Numerical Summaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-1-intro-numerical-summaries.html">3.1. Introduction to Numerical Summaries: Notation and Terminology</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-2-measures-of-central-tendency.html">3.2. Measures of Central Tendency</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-3-measures-of-variability-range-variance-and-SD.html">3.3. Measures of Variability - Range, Variance, and Standard Deviation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-4-measures-of-variability-IQR-and-5-number-summary.html">3.4. Measures of Variability - Interquartile Range and Five-Number Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-5-choosing-the-right-measure-resistance-to-extreme-values.html">3.5. Choosing the Right Measure &amp; Comparing Measures Across Data Sets</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter4/index.html">4. Probability</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-1-basic-set-theory.html">4.1. Basic Set Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-2-probability.html">4.2. Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-3-conditional-probability.html">4.3. Conditional Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-4-law-of-total-probability-and-bayes-rule.html">4.4. Law of Total Probability and Bayes’ Rule</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-5-bayes-update-rule-example.html">4.5. Bayesian Updating: Sequential Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-6-independence-of-events.html">4.6. Independence of Events</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter5/index.html">5. Discrete Distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-1-discrete-rvs-and-pmfs.html">5.1. Discrete Random Variables and Probability Mass Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-2-joint-pmfs.html">5.2. Joint Probability Mass Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-3-expected-value-of-discrete-rv.html">5.3. Expected Value of a Discrete Random Variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-4-variance-of-discrete-rv.html">5.4. Varianace of a Discrete Random Variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-5-covariance-of-dependent-rvs.html">5.5. Covariance of Dependent Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-6-binomial-distribution.html">5.6. The Binomial Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-7-poisson-distribution.html">5.7. Poisson Distribution</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">6. Continuous Distributions</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">6.1. Continuous Random Variables and Probability Density Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="6-2-expected-value-and-variance-of-cts-rvs.html">6.2. Expected Value and Variance of Continuous Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="6-3-cdfs.html">6.3. Cumulative Distribution Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="6-4-normal-distribution.html">6.4. Normal Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="6-5-uniform-distribution.html">6.5. Uniform Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="6-6-exponential-distribution.html">6.6. Exponential Distribution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter7/index.html">7. Sampling Distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-1-statistics-and-sampling-distributions.html">7.1. Statistics and Sampling Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-2-sampling-distribution-for-the-sample-mean.html">7.2. Sampling Distribution for the Sample Mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-3-clt.html">7.3. The Central Limit Theorem (CLT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-4-discret-rvs-and-clt.html">7.4. Discrete Random Variables and the CLT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter8/index.html">8. Experimental Design</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-1-experimental-and-sampling-designs.html">8.1. Experimental and Sampling Designs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-2-experimental-design-principles.html">8.2. Experimental Design Principles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-3-basic-types-of-experimental-design.html">8.3. Basic Types of Experimental Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-4-addressing-potential-flaws-in-experimental-design.html">8.4. Addressing Potential Flaws in Experimental Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-5-examples-of-experimental-design.html">8.5. Examples of Experimental Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-6-sampling-design.html">8.6. Sampling Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-7-sampling-bias.html">8.7. Sampling Bias</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter9/index.html">9. Confidence Intervals and Bounds</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-1-intro-statistical-inference.html">9.1. Introduction to Statistical Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-2-ci-sigma-known.html">9.2. Confidence Intervals for the Population Mean, When σ is Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-3-precision-of-ci-and-sample-size-calculation.html">9.3. Precision of a Confidence Interval and Sample Size Calculation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-4-cb-sigma-known.html">9.4. Confidence Bounds for the Poulation Mean When σ is Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-5-ci-cb-sigma-unknown.html">9.5. Confidence Intervals and Bounds When σ is Unknown</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter10/index.html">10. Hypothesis Testing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-1-ht-errors-and-power.html">10.1. Type I Error, Type II Error, and Power</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-2-ht-for-mean-sigma-known.html">10.2. Hypothesis Test for the Population Mean When σ is Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-3-ht-for-mean-sigma-unknown.html">10.3. Hypothesis Test for the Population Mean When σ Is Unknown</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-4-pvalue-significance-conclusion.html">10.4. P-values, Statistical Significance, and Formal Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter11/index.html">11. Two Sample Procedures</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-1-ci-ht-two-samples.html">11.1. Confidence Interval/Bound and Hypothesis Test for Two Samples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-2-comparing-two-means-independent-sigmas-known.html">11.2. Comparing the Means of Two Independent Populations - Population Variances Are Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-3-comparing-two-means-independent-pooled.html">11.3. Comparing the Means of Two Independent Populations - Pooled Variance Estimator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-4-comparing-two-means-independent-unpooled.html">11.4. Comparing the Means of Two Independent Populations - No Equal Variance Assumption</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-5-mean-of-paired-differences-two-dependent-populations.html">11.5. Analyzing the Mean of Paired Differences Between two Dependent Populations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter12/index.html">12. ANOVA</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-1-intro-one-way-anova.html">12.1. Introduction to One-Way ANOVA</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-2-sources-of-variability.html">12.2. Different Sources of Variability in an ANOVA Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-3-f-test-and-relationship-to-t-test.html">12.3. One-Way ANOVA F-Test and Its Relationship to Two-Sample t-Tests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-4-multiple-comparison-procedures-family-wise-error-rates.html">12.4. Multiple Comparison Procedures and Family-Wise Error Rates</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter13/index.html">13. Simple Linear Regression</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter13/lectures/13-1-intro-to-lr-correlation-scatter-plots.html">13.1. Introduction to Linear Regression: Correlation and Scatter Plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter13/lectures/13-2-simple-linear-regression.html">13.2. Simple Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter13/lectures/13-3-diagnostics-inference.html">13.3. Model Diagnostics and Statistical Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter13/lectures/13-4-prediction-robustness.html">13.4. Prediction, Robustness, and Applied Examples</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html"><span class="section-number">6. </span>Continuous Distributions</a></li>
      <li class="breadcrumb-item active"><span class="section-number">6.1. </span>Continuous Random Variables and Probability Density Functions</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/chapter6/lectures/6-1-continuous-rvs-and-pdfs.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="video-placeholder" role="group" aria-labelledby="video-ch6-1">
  <iframe
    id="video-ch6-1"
    title="STAT 350 – Chapter 6.1 Continuous Random Variables Video"
    src="https://www.youtube.com/embed/F_crmr4FAcg?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
    allowfullscreen>
  </iframe>
</div><section id="continuous-random-variables-and-probability-density-functions">
<h1><span class="section-number">6.1. </span>Continuous Random Variables and Probability Density Functions<a class="headerlink" href="#continuous-random-variables-and-probability-density-functions" title="Link to this heading"></a></h1>
<p>What happens when we want to model <strong>measurements</strong> rather than counts?
How do we handle quantities like height, weight, temperature, or time—variables that can take on any
value within a continuous range? This section explores the shift in mathematical framework that occurs when
we move from the discrete world of “<strong>how many</strong>” to the continuous world of “<strong>how much</strong>.”</p>
<div class="important admonition">
<p class="admonition-title">Road Map 🧭</p>
<ul class="simple">
<li><p>Understand why <strong>continuous random variables</strong> require a different mathematical approach than discrete ones.</p></li>
<li><p>Define <strong>probability density functions (PDFs)</strong> as the continuous analog to probability mass functions.</p></li>
<li><p>Master the crucial concept that <strong>probabilities are areas</strong> under the PDF.</p></li>
<li><p>Learn the essential <strong>properties</strong> that make a function a valid PDF.</p></li>
<li><p>Find probabilities for continuous random variables by integrating the PDF.</p></li>
</ul>
</div>
<section id="discrete-vs-continuous-the-key-distinction">
<h2><span class="section-number">6.1.1. </span>Discrete vs. Continuous: The Key Distinction<a class="headerlink" href="#discrete-vs-continuous-the-key-distinction" title="Link to this heading"></a></h2>
<p>For random variables with a discrete support, we could assign <strong>positive probabilities to individual outcomes</strong>.
It made perfect sense to say “the probability of getting exactly 3 heads in 10 coin flips
is some specific value” because 3 was one of only eleven distinct outcomes (0 through 10 heads).
Even when the support is <strong>countably infinite</strong>—as in the Poisson distribution—we could still assign
probabilities in such a way that each value in the support had a positive probability (however small) while the
total sum remains 1.</p>
<p>But many real-world phenomena involve measurements along a continuous scale, which has a vastly larger support
than that of any discrete random variable. While we might record a person’s height as “5 feet 8 inches,”
the actual height could be 5.75000… feet or 5.750001… feet. Between any two measurements, no matter how close,
there are <strong>uncountably many</strong> possible intermediate values.
If we tried to assign positive probabilities to each possible height—no matter how cleverly—we would
end up with an infinite total, violating the fundamental requirement that all probabilities sum (or integrate) to 1.</p>
<section id="the-resolution-zero-probability-for-any-single-point">
<h3>The Resolution: Zero Probability for Any Single Point<a class="headerlink" href="#the-resolution-zero-probability-for-any-single-point" title="Link to this heading"></a></h3>
<p>We resolve this paradox by accepting that <strong>any single exact
value has probability zero</strong> for continuous random variables.
The probability that someone’s height is exactly 5.750000000…
feet with infinite precision is zero, even though this height is perfectly possible.</p>
<p>This might seem counterintuitive at first. How can something be possible but have zero probability?
Recall that probability can be seen as the relative size of an event compared to the whole.
In the continuous case, we’re dealing with uncountably many possible values packed into any interval,
so many that any single point is negligible in comparison. This makes the
<em>relative size</em>, and thus the probability, of any one value equal to zero.</p>
</section>
<section id="then-what-has-a-positive-probability">
<h3>Then what has a positive probability?<a class="headerlink" href="#then-what-has-a-positive-probability" title="Link to this heading"></a></h3>
<p>For continuous random variables, we discuss probabilities of <em>intervals</em> of values
rather than single points. This aligns naturally with the graphical interpretation of
probabilities as areas under a curve–regions with non-zero width will have a
positive area, while a single point always has zero area.</p>
</section>
</section>
<section id="probability-density-functions-the-continuous-analog-of-probability-mass-functions">
<h2><span class="section-number">6.1.2. </span>Probability Density Functions: The Continuous Analog of Probability Mass Functions<a class="headerlink" href="#probability-density-functions-the-continuous-analog-of-probability-mass-functions" title="Link to this heading"></a></h2>
<section id="from-histograms-to-curves">
<h3>From Histograms to Curves<a class="headerlink" href="#from-histograms-to-curves" title="Link to this heading"></a></h3>
<p>Since we can’t assign probabilities to individual points, we need a different approach than PMF to
describe the distribution of a continuous random variable.
We can think of a continuous probability distribution as a curve that represents the
<strong>limiting behavior of increasingly fine histograms for an increasingly large dataset</strong>.</p>
<p>In <a class="reference internal" href="#hist-to-pdf"><span class="std std-numref">Fig. 6.1</span></a>, the jagged histogram begins to approximate a curve
as we collect more data and make the bins narrower. In the
limit—with infinite data and infinitesimally narrow bins—we get a
<strong>probability density function</strong>.</p>
<figure class="align-center" id="id1" style="width: 90%">
<span id="hist-to-pdf"></span><img alt="Evolution from histogram to probability density function" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/hist-to-pdf.png" />
<figcaption>
<p><span class="caption-number">Fig. 6.1 </span><span class="caption-text">Evolution from histogram to a probability density function</span><a class="headerlink" href="#id1" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="mathematical-definition">
<h3>Mathematical Definition<a class="headerlink" href="#mathematical-definition" title="Link to this heading"></a></h3>
<p>A <strong>probability density function (PDF)</strong> for a continuous random variable <span class="math notranslate nohighlight">\(X\)</span>,
denoted <span class="math notranslate nohighlight">\(f_X(x)\)</span>, specifies how “dense” the probability is around each point.</p>
<p>Mathematically,</p>
<div class="math notranslate nohighlight">
\[f_X(x) = \lim_{\Delta \to 0^+} \frac{P(x &lt; X \leq x + \Delta)}{\Delta}.\]</div>
</section>
<section id="interpreting-a-pdf">
<h3>Interpreting a PDF<a class="headerlink" href="#interpreting-a-pdf" title="Link to this heading"></a></h3>
<p>It is important to note that <span class="math notranslate nohighlight">\(f_X\)</span> evaluated at any point <span class="math notranslate nohighlight">\(x\)</span>
tells us about the <strong>relative</strong> likelihood of values
in that neighborhood. Suppose a random variable <span class="math notranslate nohighlight">\(X\)</span> gives
<span class="math notranslate nohighlight">\(f_X(5.8) = 3\)</span> and <span class="math notranslate nohighlight">\(f_X(6.2) = 1\)</span>. We observe that:</p>
<ul class="simple">
<li><p>Values in a small neighborhood around 5.8 is three times more likely to occur
than values in the neighborhood of 6.2.</p></li>
<li><p><span class="math notranslate nohighlight">\(f_X\)</span> does NOT give probabilities directly. <span class="math notranslate nohighlight">\(f_X(6.2) = 1\)</span> does NOT
mean that the exact value of 6.2 occurs with probability 1.</p></li>
<li><p>Evaluations of <span class="math notranslate nohighlight">\(f_X\)</span> are not restricted
to be at most 1. The set of rules for validity of a PDF will be discussed below.</p></li>
</ul>
</section>
<section id="support">
<h3>Support<a class="headerlink" href="#support" title="Link to this heading"></a></h3>
<figure class="align-center" style="width: 60%">
<img alt="Support shown on a pdf graph" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/support.png" />
</figure>
<p>The <strong>support</strong> of a continuous random variable <span class="math notranslate nohighlight">\(X\)</span> is the set of all
possible values that <span class="math notranslate nohighlight">\(X\)</span> can take, or equivalently, the set of values
where its PDF is strictly positive:</p>
<div class="math notranslate nohighlight">
\[\text{supp}(X) = \{x \in \mathbb{R} \mid f_X(x) &gt; 0\}.\]</div>
</section>
</section>
<section id="computing-probabilities-areas-under-the-pdf">
<h2><span class="section-number">6.1.3. </span>Computing Probabilities: Areas Under the PDF<a class="headerlink" href="#computing-probabilities-areas-under-the-pdf" title="Link to this heading"></a></h2>
<figure class="align-center" style="width: 60%">
<img alt="Probability between a and b on a PDF" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/prob-a-b.png" />
</figure>
<p>For a continuous random variable <span class="math notranslate nohighlight">\(X\)</span> with PDF <span class="math notranslate nohighlight">\(f_X(x)\)</span>, the probability
that <span class="math notranslate nohighlight">\(X\)</span> takes a value between <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> is the area under <span class="math notranslate nohighlight">\(f_X(x)\)</span>
between points <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>. Mathematically,</p>
<div class="math notranslate nohighlight">
\[P(a \leq X \leq b) = \int_a^b f_X(x) \, dx\]</div>
<section id="special-case-probability-at-a-single-point">
<h3>Special Case: Probability at a Single Point<a class="headerlink" href="#special-case-probability-at-a-single-point" title="Link to this heading"></a></h3>
<figure class="align-center" style="width: 60%">
<img alt="Probability of a single point" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/fx-not-prob.png" />
</figure>
<p>For any specific value <span class="math notranslate nohighlight">\(a\)</span>,</p>
<div class="math notranslate nohighlight">
\[P(X = a) = P(a \leq X \leq a) = \int_a^a f_X(x) \, dx = 0\]</div>
<p>Any integral from a point to itself is zero because the interval has zero width.
Note that <span class="math notranslate nohighlight">\(f_X(a)\)</span> evaluated at any <span class="math notranslate nohighlight">\(a\)</span> in the support is positive.
This again highlights the fact that evaluating <span class="math notranslate nohighlight">\(f_X\)</span> at a point does <strong>not</strong> directly
give its probability.</p>
</section>
<section id="an-important-consequence-equality-doesn-t-matter">
<h3>An Important Consequence: Equality Doesn’t Matter<a class="headerlink" href="#an-important-consequence-equality-doesn-t-matter" title="Link to this heading"></a></h3>
<p>Because any single point has probability zero, it doesn’t matter whether we
use strict inequalities or include equality:</p>
<div class="math notranslate nohighlight">
\[P(a \leq X \leq b) = P(a \leq X &lt; b) = P(a &lt; X \leq b) = P(a &lt; X &lt; b)\]</div>
<p>This is a major difference from discrete random variables, where <span class="math notranslate nohighlight">\(P(X = k)\)</span> could be positive,
making the choice between &lt; and ≤ crucial.</p>
</section>
</section>
<section id="properties-of-a-valid-probability-density-function">
<h2><span class="section-number">6.1.4. </span>Properties of A Valid Probability Density Function<a class="headerlink" href="#properties-of-a-valid-probability-density-function" title="Link to this heading"></a></h2>
<p>Not every function can serve as a PDF. A valid PDF must satisfy two essential properties
that parallel those required of discrete probability mass functions.</p>
<section id="property-1-non-negativity">
<h3>Property 1: Non-Negativity<a class="headerlink" href="#property-1-non-negativity" title="Link to this heading"></a></h3>
<p>The PDF must be non-negative everywhere. That is,</p>
<div class="math notranslate nohighlight">
\[f_X(x) \geq 0 \text{ for all } x.\]</div>
<p>This makes intuitive sense—there cannot be a likelihood smaller than none (zero).
However, unlike discrete PMFs, PDFs are not constrained to values less than or equal to 1.
They can take arbitrarily large values at some points, as long as they satisfy
the next property.</p>
</section>
<section id="property-2-total-area-equals-one">
<h3>Property 2: Total Area Equals One<a class="headerlink" href="#property-2-total-area-equals-one" title="Link to this heading"></a></h3>
<p>The total area under the PDF must equal 1, or equivalently,</p>
<div class="math notranslate nohighlight">
\[\int_{-\infty}^{\infty} f_X(x) \, dx = 1.\]</div>
<p>This implies that the total probability of observing any value within the support is equal to 1.</p>
<div class="note admonition">
<p class="admonition-title">Example💡: Validating and Working with a Simple PDF</p>
<p>Suppose <strong>the maximum diameter of a potato chip</strong> (<span class="math notranslate nohighlight">\(X\)</span>) produced at Factory A, in inches, follows
the following probability density:</p>
<div class="math notranslate nohighlight">
\[\begin{split}f_X(x) =
\begin{cases}
   &amp;12(x-0.5)^2(1.5-x), &amp;0.5 \leq x \leq 1.5\\
   &amp;0, &amp;\text{ Otherwise}
\end{cases}\end{split}\]</div>
<ol class="arabic">
<li><p>Identify the support of <span class="math notranslate nohighlight">\(X\)</span>, sketch <span class="math notranslate nohighlight">\(f_X(x)\)</span>, then verify that
it is a legitimate PDF.</p>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/potato-chip-pdf.png"><img alt="Sketch of the pdf of max diameter of potato chips" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/potato-chip-pdf.png" style="width: 60%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 6.2 </span><span class="caption-text">Sketch of the PDF of maximum diamters of potato chips</span><a class="headerlink" href="#id2" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><span class="math notranslate nohighlight">\(\text{supp}(X) = [0.5, 1.5]\)</span>. <span class="math notranslate nohighlight">\(f_X(x)\)</span> takes a non-negative value
at any <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span> as evident from the sketch. We now
verify that the integral of the PDF from <span class="math notranslate nohighlight">\(-\infty\)</span> to <span class="math notranslate nohighlight">\(\infty\)</span>
equals 1:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\int_{-\infty}^\infty f_X(x)dx
&amp;= \int_{-\infty}^{0.5} f_X(x)dx + \int_{0.5}^{1.5}f_X(x)dx + \int_{1.5}^{\infty} f_X(x)dx\\
&amp;= 0 + \int_{0.5}^{1.5} 12(x-0.5)^2(1.5-x) dx + 0\\\end{split}\]</div>
<p>Above, we first split the intetral to the sum of integrals over three
intervals. This step makes it evident that the integral below <span class="math notranslate nohighlight">\(0.5\)</span> and
above <span class="math notranslate nohighlight">\(1.5\)</span> is not arbitrarily omitted from computation–they simply
contribute zero area to the integral. Continuting,</p>
<div class="math notranslate nohighlight">
\[\begin{split}&amp;\int_{0.5}^{1.5} 12(x-0.5)^2(1.5-x) dx\\
&amp;= \int_{0.5}^{1.5} (-12x^3 + 30x^2-21x + 4.5)dx\\
&amp;=\frac{-12x^4}{4} + \frac{30x^3}{3} - \frac{21x^2}{2} + 4.5x \Bigg\rvert_{0.5}^{1.5}\\
&amp;=-3x^4 + 10x^3 - 10.5x^2 + 4.5x \Bigg\rvert_{0.5}^{1.5}\\
&amp;= 1.6875-0.6875 = 1.\end{split}\]</div>
<p>Therefore, <span class="math notranslate nohighlight">\(f_X\)</span> satisfies both requirements for a valid PDF.</p>
</li>
<li><p>For a quality control procedure, managers of the factory has collected all the potato chips whose
maximum diameter is <strong>smaller than 1”</strong>. What is the probability that
a randomly selected potato chip in this pool has a maximum diameter <strong>greater than 0.8”</strong>?</p>
<p>The first task is to write the goal of the problem in correct probability statement.
Since we have the information that the chips will always have a maximum diameter less than 1,</p>
<div class="math notranslate nohighlight">
\[P(X &gt; 0.8 | X &lt; 1) = \frac{P(\{X &gt; 0.8\} \cap \{X &lt; 1 \})}{P(X &lt; 1)}.\]</div>
<p>The diameter can be less than 0.8 <strong>and</strong> less than 1 only if it is between
the two values. We simplify the numerator accordingly:</p>
<div class="math notranslate nohighlight">
\[P(X &lt; 0.8 | X &lt; 1) = \frac{P(0.8 &lt;X &lt; 1)}{P(X &lt; 1)}.\]</div>
<p>Now integrate for each probability:</p>
<ul>
<li><p>Numerator</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}P(0.8 &lt; X &lt; 1) &amp;= \int_{0.8}^{1} f_X(x)dx &amp;=-3x^4 + 10x^3 - 10.5x^2 + 4.5x \Bigg\rvert_{0.8}^{1}\\
&amp;=0.2288\end{split}\]</div>
</div></blockquote>
</li>
<li><p>Denominator</p>
<p>The denominator only has an upper boundary. This is equivalent to having
a lower boundary of negative infinity, which gives us</p>
<div class="math notranslate nohighlight">
\[\begin{split}P(X &lt; 1) &amp;= \int_{-\infty}^{1} f_X(x)dx = \int_{0.5}^1 f_X(x)dx\\\end{split}\]</div>
<p>The final step is true because the integral of <span class="math notranslate nohighlight">\(f_X(x)\)</span> in any region below
<span class="math notranslate nohighlight">\(0.5\)</span> results in 0. Continuing,</p>
<div class="math notranslate nohighlight">
\[P(X &lt; 1) =-3x^4 + 10x^3 - 10.5x^2 + 4.5x \Bigg\rvert_{0.5}^{1} = 0.3125\]</div>
</li>
</ul>
<p>Finally,</p>
<div class="math notranslate nohighlight">
\[P(X &lt; 0.8 | X &lt; 1) = \frac{0.2288}{0.3125} = 0.73216\]</div>
</li>
</ol>
</div>
</section>
</section>
<section id="important-types-of-problems-invloving-pdfs">
<h2><span class="section-number">6.1.5. </span>Important Types of Problems invloving PDFs<a class="headerlink" href="#important-types-of-problems-invloving-pdfs" title="Link to this heading"></a></h2>
<section id="a-handling-complex-distributions">
<h3>A. Handling Complex Distributions<a class="headerlink" href="#a-handling-complex-distributions" title="Link to this heading"></a></h3>
<p>Real-world continuous distributions aren’t always described by simple
functions. Common elevenxamples of more complex distributions are:</p>
<figure class="align-center" id="id3">
<span id="pdf-examples"></span><a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/examples-valid-pdf.png"><img alt="Example of a piecewise-defined probability density function" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/examples-valid-pdf.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 6.3 </span><span class="caption-text"><em>A legitimate PDF can have many different functional forms</em></span><a class="headerlink" href="#id3" title="Link to this image"></a></p>
</figcaption>
</figure>
<ul>
<li><p>PDFs with one or both ends of the support extending to infinity (left two on <a class="reference internal" href="#pdf-examples"><span class="std std-numref">Fig. 6.3</span></a>)</p>
<ul class="simple">
<li><p>These types usually have an exponential function as part of the PDF. Make sure to review
integration of simple exponential functions.</p></li>
</ul>
</li>
<li><p>Piecewise-defined PDFs taking different functional forms over
different regions of the support (right on <a class="reference internal" href="#pdf-examples"><span class="std std-numref">Fig. 6.3</span></a>)</p>
<p>When dealing with piecewise PDFs,</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>Identify all boundaries</strong> where the function forms change.</p></li>
<li><p><strong>Break down</strong> any integrals spanning multiple regions into
a sum of integals, each spanning only a single region.</p></li>
</ol>
</div></blockquote>
</li>
</ul>
<div class="note admonition">
<p class="admonition-title">Example💡: A Piecewise-defined PDF</p>
<p>A professor of STAT 1234 drops by a coffee shop on campus
before or after his  50-minute lecture. Suppose the time
he enters the coffee shop has a PDF shown below.
The time <span class="math notranslate nohighlight">\(x\)</span> is expressed in minutes relative to the start time of his lecture.</p>
<div class="math notranslate nohighlight">
\[\begin{split}f_X(x) =
\begin{cases}
&amp;\frac{7}{500}(x+10), &amp;-10 \leq x \leq 0\\
&amp;\frac{3}{50}, &amp;50 \leq x \leq 55\\
&amp;0, &amp;\text{ Otherwise}
\end{cases}\end{split}\]</div>
<p>What is the probability that the professor enters the cofee shop
within 3 minutes from his lecture?</p>
<p>We are looking for</p>
<div class="math notranslate nohighlight">
\[P(-3 \leq X \leq 53) = \int_{-3}^{53} f_X(x)dx.\]</div>
<p>This interval spans three regions:</p>
<ul class="simple">
<li><p>part of the first non-trivial region where <span class="math notranslate nohighlight">\(-3 \leq x\leq 0\)</span>,</p></li>
<li><p>the middle region <span class="math notranslate nohighlight">\(0 \leq x\leq 50\)</span> where the PDF is zero, then</p></li>
<li><p>part of the second non-trivial region where <span class="math notranslate nohighlight">\(50\leq x\leq 53\)</span>.</p></li>
</ul>
<p>We must break up the integral into three pieces accordingly:</p>
<div class="math notranslate nohighlight">
\[\begin{split}P(-3 \leq X \leq 53) &amp;= \int_{-3}^{0} f_X(x)dx + \int_{0}^{50} f_X(x)dx + \int_{50}^{53} f_X(x)dx\\
&amp;=\int_{-3}^{0} \frac{7}{500}(x+10) dx + \int_{0}^{50} 0 dx + \int_{50}^{53} \frac{3}{50}dx\\
&amp;=\frac{7}{500}\left(\frac{x^2}{2} + 10x\right)\Bigg\rvert_{-3}^0 + \frac{3x}{50}\Bigg\rvert_{50}^{53}\\
&amp;=0.357+0.18 = 0.537\end{split}\]</div>
</div>
</section>
<section id="b-completing-a-partially-known-pdf">
<h3>B. Completing a Partially Known PDF<a class="headerlink" href="#b-completing-a-partially-known-pdf" title="Link to this heading"></a></h3>
<p>Sometimes we encounter functions that have the right shape to be PDFs but don’t integrate to 1.
We must fix this by finding an appropriate normalization constant.</p>
<div class="note admonition">
<p class="admonition-title">Example💡: Finding the normalization constant</p>
<p>Suppose we have a function:</p>
<div class="math notranslate nohighlight">
\[\begin{split}g(x) =
\begin{cases}
&amp; x^2, &amp; 0 \leq x \leq 2\\
&amp;0, &amp;\text{ elsewhere}
\end{cases}.\end{split}\]</div>
<p>Q: What is the constant <span class="math notranslate nohighlight">\(k\)</span> which makes <span class="math notranslate nohighlight">\(kg(x)\)</span> a valid PDF?</p>
<blockquote>
<div><p>We need to find <span class="math notranslate nohighlight">\(k\)</span> such that <span class="math notranslate nohighlight">\(\int_{-\infty}^{\infty} kg(x) \, dx = 1\)</span>.</p>
<p>First, we compute the integral of <span class="math notranslate nohighlight">\(g(x)\)</span>.</p>
<div class="math notranslate nohighlight">
\[\int_{-\infty}^{\infty}g(x)dx = \int_{0}^{2} x^2 \, dx = \left[\frac{x^3}{3}\right]_0^2 = \frac{8}{3}.\]</div>
<p>To make the total area equal 1, we need</p>
<div class="math notranslate nohighlight">
\[k \frac{8}{3} = 1 \implies k = \frac{3}{8}.\]</div>
<p>Therefore, the valid PDF is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}f_X(x) = \begin{cases}
\frac{3}{8}x^2 &amp; \text{for } 0 \leq x \leq 2 \\
0 &amp; \text{elsewhere}
\end{cases}.\end{split}\]</div>
</div></blockquote>
</div>
</section>
</section>
<section id="bringing-it-all-together">
<h2><span class="section-number">6.1.6. </span>Bringing It All Together<a class="headerlink" href="#bringing-it-all-together" title="Link to this heading"></a></h2>
<p>The transition from discrete to continuous random variables represents a fundamental shift in thinking.
Instead of asking “what’s the probability of exactly this value?” we ask “what’s the probability of
values in this range?” This change enables us to model the rich variety of measurement data we encounter.</p>
<div class="important admonition">
<p class="admonition-title">Key Takeaways 📝</p>
<ol class="arabic simple">
<li><p><strong>Continuous random variables</strong> model measurable quantities that can take any value within an interval,
unlike discrete variables that count distinct outcomes.</p></li>
<li><p><strong>Probability density functions (PDFs)</strong> describe how probability is distributed across the continuous
range, with higher values indicating more likely regions.</p></li>
<li><p><strong>Probabilities are areas</strong> under the PDF, computed using integration:
<span class="math notranslate nohighlight">\(P(a &lt; X &lt; b) = \int_a^b f_X(x) \, dx\)</span>.</p></li>
<li><p><strong>Individual points have probability zero</strong> for continuous variables—only intervals have positive probability.</p></li>
<li><p><strong>Valid PDFs must be non-negative everywhere</strong> and <strong>integrate to 1</strong> over their entire support.</p></li>
</ol>
</div>
<p>In the next section, we’ll explore how to calculate expected values and variances for continuous
random variables, extending the concepts we mastered in the discrete case.</p>
<section id="exercises">
<h3>Exercises<a class="headerlink" href="#exercises" title="Link to this heading"></a></h3>
<ol class="arabic simple">
<li><p><strong>Conceptual Understanding</strong>: Explain why <span class="math notranslate nohighlight">\(P(X = 5.5) = 0\)</span> for a continuous random variable,
even when <span class="math notranslate nohighlight">\(X = 5.5\)</span> is a perfectly reasonable outcome.</p></li>
</ol>
<ol class="arabic" start="3">
<li><p><strong>Basic Probability Calculation</strong>: For the PDF <span class="math notranslate nohighlight">\(f_X(x) = 3x^2\)</span>
for <span class="math notranslate nohighlight">\(0 \leq x \leq 1\)</span>, 0 elsewhere:</p>
<ol class="loweralpha simple">
<li><p>Verify this is a legitimate PDF.</p></li>
<li><p>Find <span class="math notranslate nohighlight">\(P(X &lt; 0.5)\)</span>.</p></li>
<li><p>Find <span class="math notranslate nohighlight">\(P(0.2 &lt; X &lt; 0.8)\)</span>.</p></li>
<li><p>Find <span class="math notranslate nohighlight">\(P(X = 0.3)\)</span>.</p></li>
</ol>
</li>
<li><p><strong>Piecewise PDF</strong>: For the following PDF,</p>
<div class="math notranslate nohighlight">
\[\begin{split}f_X(x) = \begin{cases}
kx &amp; \text{for } 0 \leq x \leq 2 \\
k(4-x) &amp; \text{for } 2 &lt; x \leq 4 \\
0 &amp; \text{elsewhere}
\end{cases}\end{split}\]</div>
<ol class="loweralpha simple">
<li><p>Find the value of <span class="math notranslate nohighlight">\(k\)</span> that makes this a valid PDF.</p></li>
<li><p>Sketch the PDF.</p></li>
<li><p>Find <span class="math notranslate nohighlight">\(P(1 &lt; X &lt; 3)\)</span>.</p></li>
</ol>
</li>
</ol>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../index.html" class="btn btn-neutral float-left" title="6. Continuous Distributions" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="6-2-expected-value-and-variance-of-cts-rvs.html" class="btn btn-neutral float-right" title="6.2. Expected Value and Variance of Continuous Random Variables" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>