

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>6.4. Normal Distribution &mdash; STAT 350</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=da1f3f80" />
      <link rel="stylesheet" type="text/css" href="../../_static/credits.css?v=29483a7b" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT350/course_site/chapter6/lectures/6-4-normal-distribution.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      <script src="../../_static/custom.js?v=de5959cf"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="6.5. Uniform Distribution" href="6-5-uniform-distribution.html" />
    <link rel="prev" title="6.3. Cumulative Distribution Functions" href="6-3-cdfs.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Orientation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../course-intro.html">Course Introduction &amp; Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#why-statistics-why-now">Why Statistics? Why Now?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#course-roadmap">Course Roadmap</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#a-clear-note-on-using-ai">A Clear Note on Using AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#getting-started-a-checklist">Getting Started: A Checklist</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../course-intro.html#homework-assignments-on-edfinity">Homework Assignments on Edfinity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../course-intro.html#miscellaneous-tips">Miscellaneous Tips</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Exam Information</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../exams/exams_index.html">Course Examinations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../exams/exams_index.html#general-exam-policies">General Exam Policies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../exams/exam_materials/exam1.html">Exam 1</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/exam1.html#exam-locations-by-section">Exam Locations by Section</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/exam1.html#exam-coverage">Exam Coverage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/exam1.html#preparation-materials">Preparation Materials</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../exams/exam_materials/exam2.html">Exam 2</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/exam2.html#exam-locations-by-section">Exam Locations by Section</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/exam2.html#exam-coverage">Exam Coverage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/exam2.html#additional-resources">Additional Resources</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/exam2.html#preparation-materials">Preparation Materials</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../exams/exam_materials/final_exam.html">Final Exam</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/final_exam.html#about-the-final-exam">About the Final Exam</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/final_exam.html#required-review-materials">Required Review Materials</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/final_exam.html#post-exam-2-preparation-materials">Post Exam 2 Preparation Materials</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/final_exam.html#study-guide-resource">Study Guide Resource</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Worksheets</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../worksheets/worksheets_index.html">Course Worksheets</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../worksheets/worksheets_index.html#pedagogical-philosophy">Pedagogical Philosophy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../worksheets/worksheets_index.html#implementation-guidelines">Implementation Guidelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../worksheets/worksheets_index.html#why-these-worksheets-matter">Why These Worksheets Matter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../worksheets/worksheets_index.html#the-critical-role-of-simulation">The Critical Role of Simulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../worksheets/worksheets_index.html#worksheets">Worksheets</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet1.html">Worksheet 1: Exploring Data with R</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet1.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet1.html#part-1-loading-and-understanding-the-dataset">Part 1: Loading and Understanding the Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet1.html#part-2-initial-data-exploration">Part 2: Initial Data Exploration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet1.html#part-3-frequency-tables">Part 3: Frequency Tables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet1.html#part-4-univariate-analysis-of-uptake">Part 4: Univariate Analysis of Uptake</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet1.html#part-5-grouped-statistics-with-tapply">Part 5: Grouped Statistics with tapply</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet1.html#part-6-comparative-visualization-by-type">Part 6: Comparative Visualization by Type</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet1.html#part-7-exploring-the-concentration-effect">Part 7: Exploring the Concentration Effect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet1.html#part-8-advanced-visualization-with-multiple-categories">Part 8: Advanced Visualization with Multiple Categories</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet1.html#reference-key-functions">Reference: Key Functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet1.html#submission-guidelines">Submission Guidelines</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet2.html">Worksheet 2: Set Theory and Probability Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet2.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet2.html#part-1-set-theory-foundations">Part 1: Set Theory Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet2.html#part-2-probability-axioms">Part 2: Probability Axioms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet2.html#part-3-applying-probability-rules">Part 3: Applying Probability Rules</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet2.html#part-4-the-inclusion-exclusion-principle">Part 4: The Inclusion-Exclusion Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet2.html#simulation-exercise">Simulation Exercise</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet2.html#key-takeaways">Key Takeaways</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet2.html#submission-guidelines">Submission Guidelines</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet3.html">Worksheet 3: Conditional Probability and Bayes’ Theorem</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet3.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet3.html#part-1-understanding-conditional-probability">Part 1: Understanding Conditional Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet3.html#part-2-tree-diagrams-and-sequential-sampling">Part 2: Tree Diagrams and Sequential Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet3.html#part-3-bayes-theorem-and-sequential-updating">Part 3: Bayes’ Theorem and Sequential Updating</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet3.html#key-takeaways">Key Takeaways</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet4.html">Worksheet 4: Independence and Random Variables</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet4.html#part-1-independence-property">Part 1: Independence Property</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet4.html#part-2-independent-vs-mutually-exclusive-events">Part 2: Independent vs. Mutually Exclusive Events</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet4.html#part-3-introduction-to-random-variables">Part 3: Introduction to Random Variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet4.html#part-4-probability-mass-functions">Part 4: Probability Mass Functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet4.html#part-5-joint-probability-mass-functions">Part 5: Joint Probability Mass Functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet4.html#key-takeaways">Key Takeaways</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet5.html">Worksheet 5: Expected Value and Variance</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet5.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet5.html#part-1-expected-value-and-lotus">Part 1: Expected Value and LOTUS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet5.html#part-2-variance-and-its-properties">Part 2: Variance and Its Properties</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet5.html#part-3-sums-of-random-variables">Part 3: Sums of Random Variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet5.html#part-4-joint-probability-mass-functions">Part 4: Joint Probability Mass Functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet5.html#key-takeaways">Key Takeaways</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet6.html">Worksheet 6: Named Discrete Distributions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet6.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet6.html#part-1-the-bernoulli-distribution">Part 1: The Bernoulli Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet6.html#part-2-the-binomial-distribution">Part 2: The Binomial Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet6.html#part-3-the-poisson-distribution">Part 3: The Poisson Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet6.html#part-4-other-named-discrete-distributions">Part 4: Other Named Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet6.html#key-takeaways">Key Takeaways</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet7.html">Worksheet 7: Continuous Random Variables</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet7.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet7.html#part-1-probability-density-functions">Part 1: Probability Density Functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet7.html#part-2-finding-constants-for-valid-pdfs">Part 2: Finding Constants for Valid PDFs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet7.html#part-3-expected-value-and-variance">Part 3: Expected Value and Variance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet7.html#part-4-cumulative-distribution-functions">Part 4: Cumulative Distribution Functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet7.html#key-takeaways">Key Takeaways</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet8.html">Worksheet 8: Uniform and Exponential Distributions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet8.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet8.html#part-1-the-uniform-distribution">Part 1: The Uniform Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet8.html#part-2-the-exponential-distribution">Part 2: The Exponential Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet8.html#key-takeaways">Key Takeaways</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet9.html">Worksheet 9: The Normal Distribution</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet9.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet9.html#part-1-the-normal-distribution">Part 1: The Normal Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet9.html#part-2-the-empirical-rule">Part 2: The Empirical Rule</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet9.html#part-3-the-standard-normal-table">Part 3: The Standard Normal Table</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet9.html#part-4-z-score-transformation">Part 4: Z-Score Transformation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet9.html#key-takeaways">Key Takeaways</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet10.html">Worksheet 10: Checking Normality and Introduction to Sampling Distributions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet10.html#part-1-checking-normality">Part 1: Checking Normality</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet10.html#part-2-introduction-to-sampling-distributions">Part 2: Introduction to Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet10.html#key-takeaways">Key Takeaways</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Computer Assignments</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html">R / RStudio Guide and Function Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#quick-reference-table">Quick Reference Table</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#quick-start-r-rstudio-setup">Quick Start: R / RStudio Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#course-pipeline-at-a-glance">Course Pipeline (At a Glance)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#packages-libraries-course-set">Packages / Libraries (Course Set)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#rstudio-orientation">RStudio Orientation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#getting-started-with-swirl">Getting Started with swirl</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#setup-and-use-swirl">Setup and Use swirl</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#alternative-r-learning-resources">Alternative R Learning Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#base-r">Base R</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#tidyverse">tidyverse</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#r-markdown">R Markdown</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#statistical-computing">Statistical Computing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#video-resources">Video Resources</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#assignment-tutorials-links">Assignment Tutorials (Links)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#function-reference-alphabetized-within-category">Function Reference (Alphabetized within Category)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#data-i-o-housekeeping">Data I/O &amp; Housekeeping</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#data-structures-creation">Data Structures &amp; Creation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#data-wrangling-utilities">Data Wrangling &amp; Utilities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#descriptive-statistics-correlation">Descriptive Statistics &amp; Correlation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#probability-distributions">Probability &amp; Distributions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#simulation-functions">Simulation Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#inference-functions">Inference Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#graphics-ggplot2">Graphics (ggplot2)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#core-components">Core Components</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#geoms-geometric-objects">Geoms (Geometric Objects)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#categorical-data-visualization">Categorical Data Visualization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#plot-customization">Plot Customization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#tables-reporting">Tables &amp; Reporting</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#best-practices-common-pitfalls">Best Practices &amp; Common Pitfalls</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#appendix-quick-links">Appendix: Quick Links</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#setup">Setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#assignments">Assignments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#getting-help">Getting Help</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Chapters</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../chapter1/index.html">1. Introduction to Statistics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter1/lectures/1-1-intro.html">1.1. What Is Statistics?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter1/lectures/1-2-probability-inference.html">1.2. Probability &amp; Statistical Inference: How Are They Associated?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter2/index.html">2. Graphical Summaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-1-understanding-the-structure-of-data-set.html">2.1. Data Set Structure and Variable Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-2-tools-for-categorical-qualitative-data.html">2.2. Tools for Categorical (Qualitative) Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-3-tools-for-numerical-quantitative-data.html">2.3. Tools for Numerical (Quantitative) Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-4-exploring-quantitative-distributions-modality-shape-and-outliers.html">2.4. Exploring Quantitative Distributions: Modality, Skewness &amp; Outliers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter3/index.html">3. Numerical Summaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-1-intro-numerical-summaries.html">3.1. Introduction to Numerical Summaries: Notation and Terminology</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-2-measures-of-central-tendency.html">3.2. Measures of Central Tendency</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-3-measures-of-variability-range-variance-and-SD.html">3.3. Measures of Variability - Range, Variance, and Standard Deviation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-4-measures-of-variability-IQR-and-5-number-summary.html">3.4. Measures of Variability - Interquartile Range and Five-Number Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-5-choosing-the-right-measure-resistance-to-extreme-values.html">3.5. Choosing the Right Measure &amp; Comparing Measures Across Data Sets</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter4/index.html">4. Probability</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-1-basic-set-theory.html">4.1. Basic Set Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-2-probability.html">4.2. Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-3-conditional-probability.html">4.3. Conditional Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-4-law-of-total-probability-and-bayes-rule.html">4.4. Law of Total Probability and Bayes’ Rule</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-5-bayes-update-rule-example.html">4.5. Bayesian Updating: Sequential Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-6-independence-of-events.html">4.6. Independence of Events</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter5/index.html">5. Discrete Distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-1-discrete-rvs-and-pmfs.html">5.1. Discrete Random Variables and Probability Mass Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-2-joint-pmfs.html">5.2. Joint Probability Mass Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-3-expected-value-of-discrete-rv.html">5.3. Expected Value of a Discrete Random Variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-4-variance-of-discrete-rv.html">5.4. Varianace of a Discrete Random Variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-5-covariance-of-dependent-rvs.html">5.5. Covariance of Dependent Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-6-binomial-distribution.html">5.6. The Binomial Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-7-poisson-distribution.html">5.7. Poisson Distribution</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">6. Continuous Distributions</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="6-1-continuous-rvs-and-pdfs.html">6.1. Continuous Random Variables and Probability Density Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="6-2-expected-value-and-variance-of-cts-rvs.html">6.2. Expected Value and Variance of Continuous Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="6-3-cdfs.html">6.3. Cumulative Distribution Functions</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">6.4. Normal Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="6-5-uniform-distribution.html">6.5. Uniform Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="6-6-exponential-distribution.html">6.6. Exponential Distribution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter7/index.html">7. Sampling Distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-1-statistics-and-sampling-distributions.html">7.1. Statistics and Sampling Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-2-sampling-distribution-for-the-sample-mean.html">7.2. Sampling Distribution for the Sample Mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-3-clt.html">7.3. The Central Limit Theorem (CLT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-4-discret-rvs-and-clt.html">7.4. Understanding Binomial and Poisson Distributions through CLT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter8/index.html">8. Experimental Design</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-1-experimental-and-sampling-designs.html">8.1. Experimental and Sampling Designs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-2-experimental-design-principles.html">8.2. Experimental Design Principles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-3-basic-types-of-experimental-design.html">8.3. Basic Types of Experimental Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-4-addressing-potential-flaws-in-experimental-design.html">8.4. Addressing Potential Flaws in Experimental Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-5-examples-of-experimental-design.html">8.5. Examples of Experimental Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-6-sampling-design.html">8.6. Sampling Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-7-sampling-bias.html">8.7. Sampling Bias</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter9/index.html">9. Confidence Intervals and Bounds</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-1-intro-statistical-inference.html">9.1. Introduction to Statistical Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-2-ci-sigma-known.html">9.2. Confidence Intervals for the Population Mean, When σ is Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-3-precision-of-ci-and-sample-size-calculation.html">9.3. Precision of a Confidence Interval and Sample Size Calculation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-4-cb-sigma-known.html">9.4. Confidence Bounds for the Poulation Mean When σ is Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-5-ci-cb-sigma-unknown.html">9.5. Confidence Intervals and Bounds When σ is Unknown</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter10/index.html">10. Hypothesis Testing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-1-ht-errors-and-power.html">10.1. Type I Error, Type II Error, and Power</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-2-ht-for-mean-sigma-known.html">10.2. Hypothesis Test for the Population Mean When σ is Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-3-ht-for-mean-sigma-unknown.html">10.3. Hypothesis Test for the Population Mean When σ Is Unknown</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-4-pvalue-significance-conclusion.html">10.4. P-values, Statistical Significance, and Formal Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter11/index.html">11. Two Sample Procedures</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-1-ci-ht-two-samples.html">11.1. Confidence Interval/Bound and Hypothesis Test for Two Samples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-2-comparing-two-means-independent-sigmas-known.html">11.2. Comparing the Means of Two Independent Populations - Population Variances Are Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-3-comparing-two-means-independent-pooled.html">11.3. Comparing the Means of Two Independent Populations - Pooled Variance Estimator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-4-comparing-two-means-independent-unpooled.html">11.4. Comparing the Means of Two Independent Populations - No Equal Variance Assumption</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-5-mean-of-paired-differences-two-dependent-populations.html">11.5. Analyzing the Mean of Paired Differences Between two Dependent Populations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter12/index.html">12. ANOVA</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-1-intro-one-way-anova.html">12.1. Introduction to One-Way ANOVA</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-2-sources-of-variability.html">12.2. Different Sources of Variability in an ANOVA Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-3-f-test-and-relationship-to-t-test.html">12.3. One-Way ANOVA F-Test and Its Relationship to Two-Sample t-Tests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-4-multiple-comparison-procedures-family-wise-error-rates.html">12.4. Multiple Comparison Procedures and Family-Wise Error Rates</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter13/index.html">13. Simple Linear Regression</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter13/lectures/13-1-intro-to-lr-correlation-scatter-plots.html">13.1. Introduction to Linear Regression: Correlation and Scatter Plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter13/lectures/13-2-simple-linear-regression.html">13.2. Simple Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter13/lectures/13-3-diagnostics-inference.html">13.3. Model Diagnostics and Statistical Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter13/lectures/13-4-prediction-robustness.html">13.4. Prediction, Robustness, and Applied Examples</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html"><span class="section-number">6. </span>Continuous Distributions</a></li>
      <li class="breadcrumb-item active"><span class="section-number">6.4. </span>Normal Distribution</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/chapter6/lectures/6-4-normal-distribution.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="video-placeholder" role="group" aria-labelledby="video-ch6-4">
  <iframe
    id="video-ch6-4"
    title="STAT 350 – Chapter 6.4 Normal Distribution Video"
    src="https://www.youtube.com/embed/O3wz4JgtZsA?si=Qc7lm4xqwW_iUfey"
   allowfullscreen>
  </iframe>
</div><div class="tip admonition">
<p class="admonition-title">Slides</p>
<p><a class="reference external" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/slides/Chapter%206%20Continuous%20Distributions/L12-14--ContinuousRandomVariablesProbabilityDensityCurves%28Chapter%206%29_AC.pptx">Download Chapter 6 slides (PPTX)</a></p>
</div>
<section id="id1">
<h1><span class="section-number">6.4. </span>Normal Distribution<a class="headerlink" href="#id1" title="Link to this heading"></a></h1>
<p>We now encounter the most important continuous distribution in all of statistics:
the normal distribution.</p>
<div class="important admonition">
<p class="admonition-title">Road Map 🧭</p>
<ul class="simple">
<li><p>Understand the <strong>historical development</strong> and significance of the normal distribution.</p></li>
<li><p>Master the <strong>mathematical definition</strong> and properties of the normal PDF.</p></li>
<li><p>Explore how the <strong>parameters μ and σ</strong> control location and shape.</p></li>
<li><p>Learn the famous <strong>empirical rule</strong> for quick probability estimates.</p></li>
<li><p>Understand why <strong>standardization</strong> is essential for normal computations.</p></li>
</ul>
</div>
<section id="the-historical-legacy-from-gauss-to-modern-statistics">
<h2><span class="section-number">6.4.1. </span>The Historical Legacy: From Gauss to Modern Statistics<a class="headerlink" href="#the-historical-legacy-from-gauss-to-modern-statistics" title="Link to this heading"></a></h2>
<p>The normal distribution carries a rich mathematical heritage spanning over two centuries.
While often called the “Gaussian distribution” in honor of Carl Friedrich Gauss (1777-1855),
the distribution’s development involved several brilliant mathematicians who recognized
patterns in natural variation.</p>
<section id="gauss-and-the-method-of-least-squares">
<h3>Gauss and the Method of Least Squares<a class="headerlink" href="#gauss-and-the-method-of-least-squares" title="Link to this heading"></a></h3>
<figure class="align-right" id="id3" style="width: 30%">
<img alt="Carl Friedrich Gauss" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/Gauss.jpg" />
<figcaption>
<p><span class="caption-number">Fig. 6.6 </span><span class="caption-text">Carl Friedrich Gauss (1777-1855)</span><a class="headerlink" href="#id3" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>In the late 1700s and early 1800s, Gauss was working on astronomical calculations and
geodetic surveys—problems requiring precise measurements where small errors were
inevitable. He sought to understand how these measurement errors behaved and how
to optimally combine multiple measurements of the same quantity.</p>
<p>Gauss discovered that measurement errors followed a specific pattern: most errors
were small and clustered around zero, with larger errors becoming increasingly rare.
More importantly, he found that this error distribution had a particular exponential
form with quadratic decay that optimized his least squares fitting procedure.</p>
</section>
<section id="the-connection-to-binomial-distributions">
<h3>The Connection to Binomial Distributions<a class="headerlink" href="#the-connection-to-binomial-distributions" title="Link to this heading"></a></h3>
<p>Gauss recognized that his continuous error distribution emerged as a limiting case
of discrete binomial distributions. When the number of trials becomes very large
while the probability of success becomes very small (in a specific balanced way),
the jagged, discrete binomial distribution smooths into the graceful bell curve we
now call the normal distribution.</p>
<p>This connection between discrete counting processes and continuous measurement errors
revealed a profound unity in probability theory—the same mathematical structure appears
whether we’re flipping coins or measuring stellar positions.</p>
</section>
<section id="a-universal-pattern-in-nature">
<h3>A Universal Pattern in Nature<a class="headerlink" href="#a-universal-pattern-in-nature" title="Link to this heading"></a></h3>
<p>What makes the normal distribution truly remarkable is its ubiquity.
It describes not just measurement errors, but heights and weights of organisms,
intelligence test scores, particle velocities in gases, and countless other
natural phenomena. This universality isn’t coincidental—it emerges from a deep
mathematical principle we’ll encounter later called the Central Limit Theorem.</p>
</section>
</section>
<section id="the-mathematical-definition-anatomy-of-the-bell-curve">
<h2><span class="section-number">6.4.2. </span>The Mathematical Definition: Anatomy of the Bell Curve<a class="headerlink" href="#the-mathematical-definition-anatomy-of-the-bell-curve" title="Link to this heading"></a></h2>
<section id="notation-and-parameters">
<h3>Notation and Parameters<a class="headerlink" href="#notation-and-parameters" title="Link to this heading"></a></h3>
<p>If a random variable <span class="math notranslate nohighlight">\(X\)</span> has normal distribution, we write:</p>
<div class="math notranslate nohighlight">
\[X \sim N(\mu, \sigma^2) \quad \text{or} \quad X \sim N(\mu, \sigma).\]</div>
<p>A normal random variable takes two parameters:</p>
<table class="docutils align-center" style="width: 100%">
<thead>
<tr class="row-odd"><th class="head"></th>
<th class="head"><p>Mean <span class="math notranslate nohighlight">\(\mu\)</span></p></th>
<th class="head"><p>Standard Deviation <span class="math notranslate nohighlight">\(\sigma\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Possible values</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu \in (-\infty, +\infty)\)</span>. It can be any real number.</p></td>
<td><p><span class="math notranslate nohighlight">\(\sigma &gt;0\)</span>. It must be a postive value.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Interpretation</strong></p></td>
<td><p>The <em>location parameter</em>. It represents the center of the distribution of <span class="math notranslate nohighlight">\(X\)</span>.</p></td>
<td><p>The <em>scale parameter</em>. It represents how spread out the distribution of <span class="math notranslate nohighlight">\(X\)</span> is.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Effect on the appearance of the PDF</strong></p></td>
<td><p>Slides the curve left or right, without changing the shape</p></td>
<td><p>Makes the graph tall and narrow (small <span class="math notranslate nohighlight">\(\sigma\)</span>) or wide and flat (large <span class="math notranslate nohighlight">\(\sigma\)</span>). It does not change the
location of the center.</p></td>
</tr>
</tbody>
</table>
<div class="important admonition">
<p class="admonition-title">Variance or Standard Deviation?</p>
<p>It is standard to describe a normal distribution using either variance or
standard deviation, but <strong>we must be explicit about which we’re using</strong>.</p>
<p>The constraints and interpretations of standard deviation transfer almost directly
to variance. Variance must be a positive number, and it controls how wide
the distribution is. The only difference is their scale—variance is in the squared scale,
while standard deviation is on the same scale as <span class="math notranslate nohighlight">\(X\)</span>.</p>
</div>
<figure class="align-center" id="id4" style="width: 60%">
<img alt="Normal pdfs for different sets of parameters mu and sigma" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/normal-pdfs.png" />
<figcaption>
<p><span class="caption-number">Fig. 6.7 </span><span class="caption-text">How different values of μ and σ affect the normal distribution’s appearance</span><a class="headerlink" href="#id4" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="the-normal-pdf">
<h3>The Normal PDF<a class="headerlink" href="#the-normal-pdf" title="Link to this heading"></a></h3>
<p>The PDF of a normal random variable <span class="math notranslate nohighlight">\(X\)</span> takes the form:</p>
<div class="math notranslate nohighlight">
\[f_X(x) = \frac{1}{\sqrt{2\pi \sigma}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2} \quad \text{for all } x \in \mathbb{R}\]</div>
<p>This elegant formula contains several key components:</p>
<ul class="simple">
<li><p>The <strong>normalizing constant</strong> <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{2\pi \sigma}}\)</span> ensures the total
area under the curve equals 1.</p></li>
<li><p>The <strong>exponential function</strong> <span class="math notranslate nohighlight">\(e^{-(\cdot)}\)</span> creates the smooth, continuous decay.</p></li>
<li><p>The <strong>quadratic expression</strong> <span class="math notranslate nohighlight">\(\left(\frac{x-\mu}{\sigma}\right)^2\)</span> in the exponent
produces the symmetric, bell-shaped curve.</p></li>
<li><p>The <strong>parameters</strong> <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> control the distribution’s location and spread.</p></li>
</ul>
</section>
<section id="fundamental-properties">
<h3>Fundamental Properties<a class="headerlink" href="#fundamental-properties" title="Link to this heading"></a></h3>
<p>Regardless of its parameters, every normal distribution satisfies the following properties:</p>
<ol class="arabic">
<li><p>It is <strong>symmetrical</strong> about the mean <span class="math notranslate nohighlight">\(\mu\)</span>.</p></li>
<li><p>It is <strong>unimodal</strong> with a single peak at <span class="math notranslate nohighlight">\(x = \mu\)</span>.</p></li>
<li><p>Since the distribution is perfectly symmetric, the <strong>mean equals the median</strong>:
<span class="math notranslate nohighlight">\(\mu = \tilde{\mu}\)</span>.</p></li>
<li><p>It is <strong>bell-shaped</strong> with smooth, continuous curves.</p></li>
<li><p>The two tails <strong>approach but never reach zero</strong> as <span class="math notranslate nohighlight">\(x \to \pm\infty\)</span>.
This implies that <span class="math notranslate nohighlight">\(\text{supp}(X) = (-\infty, +\infty)\)</span>.</p></li>
<li><p>The points where the normal curve changes from concave down to concave up (its <strong>inflection points</strong>) occur exactly at
<span class="math notranslate nohighlight">\(x = \mu - \sigma\)</span> and <span class="math notranslate nohighlight">\(x = \mu + \sigma\)</span>.</p>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/inflection-points.png"><img alt="Normal distribution showing inflection points at μ ± σ" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/inflection-points.png" style="width: 70%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 6.8 </span><span class="caption-text">The normal curve changes concavity at exactly one standard deviation from the mean</span><a class="headerlink" href="#id5" title="Link to this image"></a></p>
</figcaption>
</figure>
</li>
</ol>
</section>
</section>
<section id="the-empirical-rule-a-practical-tool">
<h2><span class="section-number">6.4.3. </span>The Empirical Rule: A Practical Tool<a class="headerlink" href="#the-empirical-rule-a-practical-tool" title="Link to this heading"></a></h2>
<p>One of the most useful properties of normal distributions is that they all follow the same probability
pattern, regardless of their specific parameter values. This universal pattern is called the
<strong>empirical rule</strong> or <strong>68-95-99.7 rule</strong>.</p>
<p>For any normal distribution <span class="math notranslate nohighlight">\(X \sim N(\mu, \sigma)\)</span>:</p>
<ol class="arabic simple">
<li><p><strong>68% of the probability</strong> lies within one standard deviation:
<span class="math notranslate nohighlight">\(P(\mu - \sigma &lt; X &lt; \mu + \sigma) \approx 0.68\)</span></p></li>
<li><p><strong>95% of the probability</strong> lies within two standard deviations:
<span class="math notranslate nohighlight">\(P(\mu - 2\sigma &lt; X &lt; \mu + 2\sigma) \approx 0.95\)</span></p></li>
<li><p><strong>99.7% of the probability</strong> lies within three standard deviations:
<span class="math notranslate nohighlight">\(P(\mu - 3\sigma &lt; X &lt; \mu + 3\sigma) \approx 0.997\)</span></p></li>
</ol>
<figure class="align-center" id="id6">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/empirical-rule-labeled.png"><img alt="Empirical rule showing 68-95-99.7 percentages" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/empirical-rule-labeled.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 6.9 </span><span class="caption-text">The empirical rule provides quick probability estimates for any normal distribution</span><a class="headerlink" href="#id6" title="Link to this image"></a></p>
</figcaption>
</figure>
<section id="extended-breakdown-of-the-empirical-rule">
<h3>Extended Breakdown of the Empirical Rule<a class="headerlink" href="#extended-breakdown-of-the-empirical-rule" title="Link to this heading"></a></h3>
<ol class="arabic simple">
<li><p><strong>34%</strong> of probability lies in each of <span class="math notranslate nohighlight">\((\mu, \mu + \sigma)\)</span>.
and <span class="math notranslate nohighlight">\((\mu - \sigma, \mu)\)</span></p>
<ul class="simple">
<li><p>Each interval is half of 68%</p></li>
</ul>
</li>
<li><p><strong>13.5%</strong> of probability lies in each of <span class="math notranslate nohighlight">\((\mu + \sigma, \mu + 2\sigma)\)</span>.
and <span class="math notranslate nohighlight">\((\mu - 2\sigma, \mu - \sigma)\)</span></p>
<ul class="simple">
<li><p>Each interval is half of 95%, minus an interval from #1.</p></li>
</ul>
</li>
<li><p><strong>2.35%</strong> of probability lies in each of <span class="math notranslate nohighlight">\((\mu + 2\sigma, \mu + 3\sigma)\)</span>
and <span class="math notranslate nohighlight">\((\mu - 3\sigma, \mu - 2\sigma)\)</span>.</p>
<ul class="simple">
<li><p>Each interval is half of 99.7%, minus an interval from #2 and an interval from #1.</p></li>
</ul>
</li>
<li><p><strong>0.15%</strong> of probability lies beyond <span class="math notranslate nohighlight">\(\mu + 3\sigma\)</span> and
another <strong>0.15%</strong> beyond <span class="math notranslate nohighlight">\(\mu - 3\sigma\)</span>.</p>
<ul class="simple">
<li><p>Each region is half of 100% - 99.7%.</p></li>
</ul>
</li>
</ol>
</section>
<section id="insights-from-the-empirical-rule">
<h3>Insights from the Empirical Rule<a class="headerlink" href="#insights-from-the-empirical-rule" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>About <strong>2/3</strong> of values fall within one standard deviation of the mean.</p></li>
<li><p>About <strong>19 out of 20</strong> values fall within two standard deviations.</p></li>
<li><p><strong>Nearly all values</strong> (99.7%) fall within three standard deviations.</p></li>
</ul>
<div class="note admonition">
<p class="admonition-title">Example💡: Computing Normal Probabilities Using Empirical Rules</p>
<p>A chemical lab reports that the amount of active ingredient in a single tablet of
a medication is normally distributed with a mean of 500 mg and a standard deviation of 5 mg.</p>
<ol class="upperalpha" start="17">
<li><p>What percentage of the tablets contain between 490 mg and 505 mg of active ingredient?</p>
<p><span class="math notranslate nohighlight">\(490 = \mu - 2\sigma \text{ and } 505 = \mu + \sigma\)</span>. Therefore, we are looking for</p>
<div class="math notranslate nohighlight">
\[P(\mu -2\sigma \leq X \leq \mu + \sigma)\]</div>
<p>There are many different ways to solve this using the empirical rule. One way is to view
the probability as</p>
<div class="math notranslate nohighlight">
\[P(\mu -2\sigma \leq X \leq \mu + 2\sigma) - P(\mu+\sigma \leq X \leq \mu +2\sigma)\]</div>
<p>The first term is approximately 0.95 by the empirical rule,
and the second term is approximately 0.135. Then finally,</p>
<div class="math notranslate nohighlight">
\[P(\mu -2\sigma \leq X \leq \mu + \sigma) \approx 0.95 - 0.135 = 0.815\]</div>
</li>
</ol>
</div>
</section>
</section>
<section id="the-standard-normal-distribution-the-foundation-of-all-normal-computations">
<h2><span class="section-number">6.4.4. </span>The Standard Normal Distribution: The Foundation of All Normal Computations<a class="headerlink" href="#the-standard-normal-distribution-the-foundation-of-all-normal-computations" title="Link to this heading"></a></h2>
<p>While normal distributions can have any mean and standard deviation, there’s one particular
normal distribution that serves as the foundation for all normal probability calculations.</p>
<section id="definition-of-the-standard-normal-distribution">
<h3>Definition of the Standard Normal Distribution<a class="headerlink" href="#definition-of-the-standard-normal-distribution" title="Link to this heading"></a></h3>
<p>The <strong>standard normal distribution</strong> is the normal distribution with mean 0 and standard deviation 1.
When a random variable follows the standard normal distribution, we denote it with <span class="math notranslate nohighlight">\(Z\)</span> and write:</p>
<div class="math notranslate nohighlight">
\[Z \sim N(0, 1)\]</div>
<p>Its PDF is obtained by plugging in 0 and 1 for <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>, respectively, in the
general form:</p>
<div class="math notranslate nohighlight">
\[f_Z(z) = \frac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{2}} \quad \text{for all } z \in \mathbb{R}\]</div>
<p>Because the standard normal is so important, it also gets <strong>special notations</strong> for its PDF and CDF:</p>
<ul class="simple">
<li><p><strong>PDF</strong>: <span class="math notranslate nohighlight">\(\phi(z) = f_Z(z)\)</span> (lowercase Greek phi)</p></li>
<li><p><strong>CDF</strong>: <span class="math notranslate nohighlight">\(\Phi(z) = P(Z \leq z)\)</span> (uppercase Greek phi)</p></li>
</ul>
</section>
<section id="standardization-of-normal-random-variables">
<h3>Standardization of Normal Random Variables<a class="headerlink" href="#standardization-of-normal-random-variables" title="Link to this heading"></a></h3>
<p>Any normal random variable can be converted to a standard normal random variable
using the <strong>standardization formula</strong>:</p>
<div class="math notranslate nohighlight">
\[Z = \frac{X - \mu}{\sigma}.\]</div>
<p>If <span class="math notranslate nohighlight">\(X \sim N(\mu, \sigma)\)</span>, then <span class="math notranslate nohighlight">\(Z \sim N(0, 1)\)</span>.</p>
<p><strong>Why Standardization Works</strong></p>
<p>Standardization is essentially a change of variables (u-substitution) that:</p>
<ol class="arabic simple">
<li><p><strong>Centers</strong> the distribution at 0 by subtracting the mean.</p></li>
<li><p><strong>Rescales</strong> the distribution to unit variance by dividing by the standard deviation.</p></li>
</ol>
<p>For a more concrete demonstration, we first need to know a special property of normal distribution:</p>
<ul class="simple">
<li><p>When a normal random variable is <strong>multiplied or added by a constant, the resulting random variable
will still be normal</strong>, just with a new set of mean and variance parameters.</p></li>
</ul>
<p>Since <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> are constants, the operation on <span class="math notranslate nohighlight">\(X\)</span> to get to <span class="math notranslate nohighlight">\(Z\)</span> leaves us
with another normal random variable. Also,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(E[Z] = E\left[\frac{X-\mu}{\sigma}\right]= \frac{E[X]-\mu}{\sigma} = \frac{\mu - \mu}{\sigma}=0\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma^2[Z] = \text{Var}(Z) = \text{Var}\left(\frac{X-\mu}{\sigma}\right) = \frac{\text{Var}(X)}{\sigma^2}= \frac{\sigma^2}{\sigma^2} =1\)</span>.</p></li>
</ul>
</section>
<section id="why-do-we-standardize">
<h3>Why Do We Standardize?<a class="headerlink" href="#why-do-we-standardize" title="Link to this heading"></a></h3>
<p>The fundamental problem with normal distributions is that their CDFs cannot be expressed in terms of
elementary functions. There’s no simple formula for:</p>
<div class="math notranslate nohighlight">
\[P(X\leq x) = \int_{-\infty}^{x} \frac{1}{\sqrt{2\pi \sigma}} e^{-\frac{1}{2}\left(\frac{t-\mu}{\sigma}\right)^2} dt\]</div>
<p>However, we can <strong>numerically approximate these integrals for the standard normal distribution</strong> and tabulate the results.
Instead of creating tables of approximations for all possible pairs of parameters—which would be impossible—we standardize,
so that we can refer to one table for any normal random variables.</p>
</section>
</section>
<section id="forward-problems-x-to-probability">
<h2><span class="section-number">6.4.5. </span>Forward Problems: <span class="math notranslate nohighlight">\(x\)</span> to Probability<a class="headerlink" href="#forward-problems-x-to-probability" title="Link to this heading"></a></h2>
<div class="video-placeholder" role="group" aria-labelledby="video-ch6-4-1">
  <iframe
    id="video-ch6-4-1"
    title="STAT 350 – Chapter 6.4.1 Normal Distribution Forward Calculations Video"
    src="https://www.youtube.com/embed/IGnLAeROI44?si=LvBNyDUXV3hoPd8w"
    allowfullscreen>
  </iframe>
</div><p>Now that we understand the theoretical foundation, let’s learn how to actually compute probabilities
for normal distributions. Since we cannot integrate the normal PDF analytically, we rely on numerical
approximations tabulated in standard normal tables.</p>
<section id="the-standard-normal-table-z-table">
<h3>The Standard Normal Table (Z-Table)<a class="headerlink" href="#the-standard-normal-table-z-table" title="Link to this heading"></a></h3>
<p>Statisticians have computed high-precision numerical approximations for the standard normal CDF
<span class="math notranslate nohighlight">\(\Phi(z) = P(Z \leq z)\)</span> and compiled them into tables. These tables typically provide probabilities
accurate to four decimal places for z-values given to two decimal places.</p>
<p>For example, if we want to find <span class="math notranslate nohighlight">\(P(Z \leq -1.38)\)</span>, first located <span class="math notranslate nohighlight">\(-1.3\)</span> from the row
labels. Then find the column with the label <span class="math notranslate nohighlight">\(0.08\)</span>. The intersection
of the row and the column gives the desired probability. <span class="math notranslate nohighlight">\(P(Z \leq -1.38)=0.0838\)</span>.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/z-table-neg-z.png"><img alt="Half of the Z-table for negative z values" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/z-table-neg-z.png" style="width: 90%;" />
</a>
</figure>
<figure class="align-center">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/z-table-pos-z.png"><img alt="The other half of the Z-table for positive z- values" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/z-table-pos-z.png" style="width: 90%;" />
</a>
</figure>
</section>
<section id="the-strategy-for-non-standard-normal-rvs">
<h3>The Strategy for Non-standard Normal RVs<a class="headerlink" href="#the-strategy-for-non-standard-normal-rvs" title="Link to this heading"></a></h3>
<p>We said we would apply the standardization technique to us the Z-table for any normal distribution.
How will this work? The key steps are the following:</p>
<ol class="arabic simple">
<li><p>Recognize that subtracting the same value on both sides or multiplying by the same positive value on both sides
does not change the truth of an (in)equality. It follows that the probability of the
(in)equality also remains unchanged.</p></li>
<li><p>Using #1,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[P(X \leq a) = P\left(\frac{X-\mu}{\sigma} \leq \frac{a-\mu}{\sigma}\right)
= P\left(Z \leq \frac{a-\mu}{\sigma}\right) = \Phi\left(\frac{a-\mu}{\sigma}\right).\]</div>
</section>
<section id="the-strategy-for-probabilities-which-do-not-match-the-cdf">
<h3>The Strategy for Probabilities Which Do Not Match the CDF<a class="headerlink" href="#the-strategy-for-probabilities-which-do-not-match-the-cdf" title="Link to this heading"></a></h3>
<p>We are often interested in probabilities which are not in the form <span class="math notranslate nohighlight">\(\Phi(z) = P(Z \leq z)\)</span>.</p>
<ul class="simple">
<li><p>For <strong>“greater than” probabilities</strong>, use the complement rule:  <span class="math notranslate nohighlight">\(P(Z &gt; z) = 1 - \Phi(z)\)</span>.</p></li>
<li><p>For <strong>probabilities of intervals</strong>, use <span class="math notranslate nohighlight">\(P(a &lt; Z &lt; b) = \Phi(b) - \Phi(a)\)</span></p></li>
<li><p>Because the standard normal distribution is symmetric <strong>around zero</strong>, we have
an additional tool: <span class="math notranslate nohighlight">\(\Phi(-z) = 1 - \Phi(z)\)</span> (<a class="reference internal" href="#z-symmetry-property"><span class="std std-numref">Fig. 6.10</span></a>).</p></li>
</ul>
<figure class="align-center" id="id7">
<span id="z-symmetry-property"></span><a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/phi-symmetry.png"><img alt="Special property of standard normal CDF due to symmetry around 0" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/phi-symmetry.png" style="width: 40%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 6.10 </span><span class="caption-text">Due to symmetry around zero, the two grey regions have equal probability.</span><a class="headerlink" href="#id7" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="forward-problems">
<h3>Forward Problems<a class="headerlink" href="#forward-problems" title="Link to this heading"></a></h3>
<p>When a problem gives a value and asks for a related probability, we call it a
<strong>forward problem</strong>. The systematic approach is:</p>
<ol class="arabic simple">
<li><p><strong>Identify</strong> what probability you need to calculate in correct probability notation.
<strong>Sketch</strong> the region on a normal PDF plot if needed.</p></li>
<li><p><strong>Standardize</strong> by converting x-values to z-scores using <span class="math notranslate nohighlight">\(z = \frac{x-\mu}{\sigma}\)</span>.</p></li>
<li><p><strong>Modify the probability statement</strong> to an expression involving <span class="math notranslate nohighlight">\(P(Z \leq z)\)</span> only
so the Z-table can be used directly.</p></li>
<li><p><strong>Round the z-score</strong> to two decimal places and look it up in the table.</p></li>
<li><p><strong>Write your conclusion</strong> in the context of the original problem.</p></li>
</ol>
<div class="note admonition">
<p class="admonition-title">Example💡: Systolic Blood Pressure</p>
<p>Systolic blood pressure readings for healthy adults, in mmHg, follow a normal distribution
with <span class="math notranslate nohighlight">\(\mu=112\)</span> and <span class="math notranslate nohighlight">\(\sigma^2= 100\)</span>. Find the probability that a randomly
selected adult has blood pressure between 90 and 134 mmHg.</p>
<figure class="align-right" id="id8" style="width: 30%">
<img alt="Sketch of the probability P(90 &lt; X &lt; 134)." src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/forward-example-sketch.png" />
<figcaption>
<p><span class="caption-number">Fig. 6.11 </span><span class="caption-text">A sketch of <span class="math notranslate nohighlight">\(P(90 &lt; X &lt; 134)\)</span></span><a class="headerlink" href="#id8" title="Link to this image"></a></p>
</figcaption>
</figure>
<ul>
<li><p><strong>Step 1: Write the random variable and its distribution in correct notation</strong></p>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be the blood pressure readings for healthy adults. <span class="math notranslate nohighlight">\(X \sim N(\mu=112, \sigma^2=100)\)</span>.</p>
</li>
<li><p><strong>Step 2: Find the correct probability statement</strong></p>
<p>We are looking for</p>
<div class="math notranslate nohighlight">
\[P(90 &lt; X &lt; 134) = P(X &lt; 134) - P(X &lt; 90).\]</div>
<p>We need to find <span class="math notranslate nohighlight">\(z_1\)</span> and <span class="math notranslate nohighlight">\(z_2\)</span> such that <span class="math notranslate nohighlight">\(P(X &lt; 134) = P(Z&lt; z_2)\)</span> and
<span class="math notranslate nohighlight">\(P(X&lt; 90)=P(Z&lt; z_1)\)</span>.</p>
</li>
<li><p><strong>Step 3: Standardize to find</strong> <span class="math notranslate nohighlight">\(z_1\)</span> <strong>and</strong> <span class="math notranslate nohighlight">\(z_2\)</span></p>
<p>Note that the variance is given for the spread parameter. We must use <span class="math notranslate nohighlight">\(\sigma = \sqrt{100} = 10\)</span>
for standardization.</p>
<div class="math notranslate nohighlight">
\[z_1 = \frac{90 - 112}{10} = \frac{-22}{10} = -2.2 \text{ and }
z_2 = \frac{134 - 112}{10} = \frac{22}{10} = 2.2\]</div>
</li>
<li><p><strong>Step 4: Convert to standard normal probability</strong></p>
<div class="math notranslate nohighlight">
\[P(90 &lt; X &lt; 134) = P(Z&lt; z_2) - P(Z&lt; z_1) = \Phi(2.2) - \Phi(-2.2)\]</div>
</li>
<li><p><strong>Step 5: Use symmetry to simplify</strong></p>
<p>We can look up the CDF values for <span class="math notranslate nohighlight">\(z=2.2\)</span> and <span class="math notranslate nohighlight">\(z=-2.2\)</span>
separately in the Z-table, but when the two z values are negatives of each other,
we can simplify the search step using <span class="math notranslate nohighlight">\(\Phi(-2.2) = 1 - \Phi(2.2)\)</span>.</p>
<div class="math notranslate nohighlight">
\[P(-2.2 &lt; Z &lt; 2.2) = \Phi(2.2) - (1 - \Phi(2.2)) = 2\Phi(2.2) - 1\]</div>
</li>
<li><p><strong>Step 6: Look up in the Z-table and calculate the final answer</strong></p>
<p>From the standard normal table: <span class="math notranslate nohighlight">\(\Phi(2.2) = 0.9861\)</span>. So finally,</p>
<div class="math notranslate nohighlight">
\[P(90 &lt; X &lt; 134) = 2(0.9861) - 1 = 0.9722\]</div>
<p>There is approximately a 0.9722 probability that a randomly selected healthy adult
will have systolic blood pressure between 90 and 134 mmHg.</p>
</li>
</ul>
</div>
</section>
</section>
<section id="backward-problems-probability-to-x-percentile">
<h2><span class="section-number">6.4.6. </span>Backward Problems: Probability to <span class="math notranslate nohighlight">\(x\)</span> (Percentile)<a class="headerlink" href="#backward-problems-probability-to-x-percentile" title="Link to this heading"></a></h2>
<div class="video-placeholder" role="group" aria-labelledby="video-ch6-4-2">
  <iframe
    id="video-ch6-4-2"
    title="STAT 350 – Chapter 6.4.2 Normal Distribution Backward Calculations Video"
    src="https://www.youtube.com/embed/nExxuvoX-gQ?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
    allowfullscreen>
  </iframe>
</div><p>Backward problems reverse the process: given a probability, we must find the corresponding value (percentile).</p>
<section id="walkthrough-of-a-backward-problem">
<h3>Walkthrough of a Backward Problem<a class="headerlink" href="#walkthrough-of-a-backward-problem" title="Link to this heading"></a></h3>
<p>Consider a typical backward question:</p>
<ul class="simple">
<li><p>The gas price on a fixed date in State A follows normal distribution with
mean $3.30 and standard deviation $0.12. If Gas Station B has a price higher than 63% of all gas stations
in the state that day, what is the gas price in Gas Station B?</p></li>
</ul>
<p>In this problem, a <strong>probability is given (63% or 0.63)</strong>, and we are <strong>asked for the cutoff</strong> whose
lower region has area of 0.63 (the 63th percentile).</p>
<p>To solve for this type of problems, we begin by setting up the correct probability statement.</p>
<div class="math notranslate nohighlight">
\[P(X \leq x_{0.63}) = 0.63.\]</div>
<p>Standardize to get a probability statement in terms of <span class="math notranslate nohighlight">\(Z\)</span>:</p>
<div class="math notranslate nohighlight">
\[P(Z \leq \frac{x_{0.63}-\mu}{\sigma})=0.63.\]</div>
<p>The right-hand side of the inequality above fits the definition of the 63th percentile of a standard normal random variable.
That is,</p>
<div class="math notranslate nohighlight">
\[z_{0.63} = \frac{x_{0.63}-\mu}{\sigma}.\]</div>
<p>We will now look for <span class="math notranslate nohighlight">\(z_{0.63}\)</span> and convert back to <span class="math notranslate nohighlight">\(x_{0.63}\)</span> using the above relationship.</p>
<p>To find <span class="math notranslate nohighlight">\(z_{0.63}\)</span>, we locate 0.63 (or the value closest to it) in the <strong>main body</strong> of the table,
then obtain the <span class="math notranslate nohighlight">\(z\)</span> <strong>value from its margins</strong>. 0.6293 is the value closest to 0.63 in the main body,
and its margins give us <span class="math notranslate nohighlight">\(z_{0.63}=0.33\)</span>.</p>
<p>Converting back, <span class="math notranslate nohighlight">\(x_{0.63} = \sigma z_{0.63} +\mu = (0.12)(0.33) + 3.3 = 3.3396\)</span>.</p>
<p>Finally, the price at Gas Station B is around $3.34.</p>
</section>
<section id="summary-of-the-key-steps">
<h3>Summary of the Key Steps<a class="headerlink" href="#summary-of-the-key-steps" title="Link to this heading"></a></h3>
<ol class="arabic simple">
<li><p>Identify the value you need to find using correct probability notation. <strong>Sketch the region</strong>
if needed.</p></li>
<li><p><strong>Find the z-score</strong> by looking up the probability in the body of the standard normal table.</p></li>
<li><p><strong>Convert the z-score to the original scale</strong> using <span class="math notranslate nohighlight">\(x = \mu + \sigma z\)</span>.</p></li>
<li><p><strong>Write your conclusion</strong> in context.</p></li>
</ol>
</section>
<section id="points-that-require-special-attention">
<h3>Points That Require Special Attention<a class="headerlink" href="#points-that-require-special-attention" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>The probability given may correspond to an <strong>upper region</strong> rather than a lower one.
Since percentiles are always based on the area in the lower region, you need to adjust accordingly.
For example, if Gas Station C has a price lower than 23 % of all other stations in the state,
its price corresponds to the (100 – 23)th percentile.</p></li>
<li><p>If the given probability does not have an exact match in the table, take the z-value for the <strong>closest entry</strong>.
If it is exactly in the middle of two values, <strong>take the average between the z-values of the two entries</strong>.</p></li>
</ul>
<div class="note admonition">
<p class="admonition-title">Example💡: Systolic Blood Pressure, Continued</p>
<p>Continue with the RV of blood pressure measurements: <span class="math notranslate nohighlight">\(X \sim N(112, 100)\)</span>.</p>
<ol class="arabic">
<li><p>find the 95th percentile.</p>
<p>We want to find <span class="math notranslate nohighlight">\(x_{0.95}\)</span> such that <span class="math notranslate nohighlight">\(P(X \leq x_{0.95}) = 0.95\)</span>
First, find <span class="math notranslate nohighlight">\(z_{0.95}\)</span> such that <span class="math notranslate nohighlight">\(\Phi(z_{0.95}) = 0.95\)</span></p>
<p>Searching the body of the standard normal table for 0.95, we find it’s between 0.9495 and 0.9505.
Since 0.95 is exactly halfway between these values, we average the corresponding z-values:</p>
<div class="math notranslate nohighlight">
\[z_{0.95} = \frac{1.64 + 1.65}{2} = 1.645.\]</div>
<p>Convertin to the original scale,</p>
<p><span class="math notranslate nohighlight">\(x_{0.95} = \mu + \sigma z_{0.95} = 112 + 10(1.645) = 128.45\)</span>.</p>
<p><em>Conclusion</em>: The 95th percentile of systolic blood pressure is 128.45 mmHg.
This means 95% of healthy adults have blood pressure at or below this value.</p>
</li>
<li><p>Find the cutoffs for the middle 50% of blood pressure measurements.
Using the cutoffs, also compute the interquartile range.</p>
<figure class="align-right" id="id9" style="width: 30%">
<img alt="Sketch of the cutoffs for middle 50%" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/backward-example-sketch.png" />
<figcaption>
<p><span class="caption-number">Fig. 6.12 </span><span class="caption-text">A sketch of problem 2</span><a class="headerlink" href="#id9" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>We need to find two cutoffs: the 25th percentile and the 75th percentile.</p>
<p><em>For the 25th percentile</em>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\Phi(z_{0.25}) = 0.25\)</span></p></li>
<li><p>From the table (using symmetry): <span class="math notranslate nohighlight">\(z_{0.25} = -0.67\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(x_{0.25} = 112 + 10(-0.67) = 105.3\)</span> mmHg</p></li>
</ul>
<p><em>For the 75th percentile</em>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\Phi(z_{0.75}) = 0.75\)</span></p></li>
<li><p>From the table: <span class="math notranslate nohighlight">\(z_{0.75} = 0.67\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(x_{0.75} = 112 + 10(0.67) = 118.7\)</span> mmHg</p></li>
</ul>
<p><em>Conclusion</em>: The middle 50% of systolic blood pressure readings fall between 105.3 and 118.7 mmHg.
The interquartile range is <span class="math notranslate nohighlight">\(118.7 - 105.3 = 13.4\)</span> mmHg.</p>
</li>
</ol>
</div>
</section>
</section>
<section id="proving-the-theoretical-properties-of-normal-distribution">
<h2><span class="section-number">6.4.7. </span>Proving the Theoretical Properties of Normal Distribution<a class="headerlink" href="#proving-the-theoretical-properties-of-normal-distribution" title="Link to this heading"></a></h2>
<section id="validity-of-the-pdf">
<h3>Validity of the PDF<a class="headerlink" href="#validity-of-the-pdf" title="Link to this heading"></a></h3>
<p>To establish that the normal PDF is legitimate, we must verify that it satisfies the two fundamental
requirements for any probability density function.</p>
<section id="property-1-non-negativity">
<h4>Property 1: Non-Negativity<a class="headerlink" href="#property-1-non-negativity" title="Link to this heading"></a></h4>
<p>We need to show that <span class="math notranslate nohighlight">\(f_X(x) \geq 0\)</span> for all <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>Since <span class="math notranslate nohighlight">\(\sigma &gt; 0\)</span>, we have <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{2\pi \sigma}} &gt; 0\)</span>. The exponential function <span class="math notranslate nohighlight">\(e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}\)</span> is always positive because:</p>
<ul class="simple">
<li><p>The exponent <span class="math notranslate nohighlight">\(-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2\)</span> is always negative (or zero).</p></li>
<li><p><span class="math notranslate nohighlight">\(e^{\text{negative number}}\)</span> is always positive.</p></li>
<li><p><span class="math notranslate nohighlight">\(e^0 = 1 &gt; 0\)</span>.</p></li>
</ul>
<p>Therefore, <span class="math notranslate nohighlight">\(f_X(x) &gt; 0\)</span> for all <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span>. ✓</p>
</section>
<section id="property-2-integration-to-unity">
<h4>Property 2: Integration to Unity<a class="headerlink" href="#property-2-integration-to-unity" title="Link to this heading"></a></h4>
<p>We must prove that <span class="math notranslate nohighlight">\(\int_{-\infty}^{\infty} f_X(x) \, dx = 1\)</span>.</p>
<p><strong>Step 1: Change of Variables</strong></p>
<p>Let <span class="math notranslate nohighlight">\(z = \frac{x - \mu}{\sigma}\)</span>, so <span class="math notranslate nohighlight">\(x = \sigma z + \mu\)</span> and <span class="math notranslate nohighlight">\(dx = \sigma \, dz\)</span>.</p>
<p><span class="math notranslate nohighlight">\(z = -\infty\)</span> when <span class="math notranslate nohighlight">\(x = -\infty\)</span>, and <span class="math notranslate nohighlight">\(z = +\infty\)</span> when <span class="math notranslate nohighlight">\(x = +\infty\)</span>.
The integral becomes:</p>
<div class="math notranslate nohighlight">
\[I = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi \sigma}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2} dx = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{2}} dz\]</div>
<p><strong>Step 2: The Squaring Trick</strong></p>
<p>This integral has no elementary antiderivative, so we use a clever approach. Let’s compute <span class="math notranslate nohighlight">\(I^2\)</span>:</p>
<div class="math notranslate nohighlight">
\[I^2 = \left(\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{2}} dz\right)\left(\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{v^2}{2}} dv\right)\]</div>
<p>Since the integrals converge absolutely, we can rewrite this as a double integral:</p>
<div class="math notranslate nohighlight">
\[I^2 = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \frac{1}{2\pi} e^{-\frac{1}{2}(z^2 + v^2)} \, dz \, dv\]</div>
<p><strong>Step 3: Polar Coordinate Transformation</strong></p>
<p>Let:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(z = r\cos\theta\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(v = r\sin\theta\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(z^2 + v^2 = r^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(dz \, dv = r \, dr \, d\theta\)</span></p></li>
</ul>
<p>The integration limits become:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(r\)</span>: from 0 to <span class="math notranslate nohighlight">\(\infty\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\theta\)</span>: from 0 to <span class="math notranslate nohighlight">\(2\pi\)</span></p></li>
</ul>
<p>Therefore:</p>
<div class="math notranslate nohighlight">
\[I^2 = \int_0^{2\pi} \int_0^{\infty} \frac{1}{2\pi} e^{-\frac{r^2}{2}} \cdot r \, dr \, d\theta\]</div>
<p><strong>Step 4: Separating the Integrals</strong></p>
<div class="math notranslate nohighlight">
\[I^2 = \frac{1}{2\pi} \int_0^{2\pi} d\theta \int_0^{\infty} r e^{-\frac{r^2}{2}} dr\]</div>
<p>The first integral gives us <span class="math notranslate nohighlight">\(2\pi\)</span>. For the second integral, use substitution <span class="math notranslate nohighlight">\(u = \frac{r^2}{2}\)</span>, so <span class="math notranslate nohighlight">\(du = r \, dr\)</span>:</p>
<div class="math notranslate nohighlight">
\[\int_0^{\infty} r e^{-\frac{r^2}{2}} dr = \int_0^{\infty} e^{-u} du = \left[-e^{-u}\right]_0^{\infty} = 0 - (-1) = 1\]</div>
<p><strong>Step 5: Final Result</strong></p>
<div class="math notranslate nohighlight">
\[I^2 = \frac{1}{2\pi} \cdot 2\pi \cdot 1 = 1\]</div>
<p>Since <span class="math notranslate nohighlight">\(I &gt; 0\)</span> (the integrand is positive), we have <span class="math notranslate nohighlight">\(I = 1\)</span>. ✓</p>
<p>This completes the proof that the normal PDF is a valid probability density function.</p>
</section>
</section>
<section id="the-parameter-relationships-expected-value-and-variance">
<h3>The Parameter Relationships: Expected Value and Variance<a class="headerlink" href="#the-parameter-relationships-expected-value-and-variance" title="Link to this heading"></a></h3>
<p>To complete our theoretical understanding, we must prove that the parameters <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> are indeed the mean and variance of the distribution.</p>
<section id="theorem-the-expected-value-is">
<h4>Theorem: The Expected Value is μ<a class="headerlink" href="#theorem-the-expected-value-is" title="Link to this heading"></a></h4>
<p>For <span class="math notranslate nohighlight">\(X \sim N(\mu, \sigma)\)</span>, <span class="math notranslate nohighlight">\(E[X] = \mu\)</span>.</p>
<p><em>Proof:</em></p>
<div class="math notranslate nohighlight">
\[E[X] = \int_{-\infty}^{\infty} x \cdot \frac{1}{\sqrt{2\pi \sigma}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2} dx\]</div>
<p>Using the standardization substitution <span class="math notranslate nohighlight">\(z = \frac{x-\mu}{\sigma}\)</span>, we have <span class="math notranslate nohighlight">\(x = \sigma z + \mu\)</span> and <span class="math notranslate nohighlight">\(dx = \sigma \, dz\)</span>.</p>
<div class="math notranslate nohighlight">
\[E[X] = \int_{-\infty}^{\infty} (\sigma z + \mu) \cdot \frac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{2}} dz\]</div>
<p>Distributing the integral,</p>
<div class="math notranslate nohighlight">
\[E[X] = \sigma \int_{-\infty}^{\infty} z \cdot \frac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{2}} dz + \mu \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{2}} dz\]</div>
<p>The second integral equals 1 since it’s the integral of the standard normal PDF.
For the first integral, note that <span class="math notranslate nohighlight">\(z \phi(z)\)</span> is an odd function, and we’re integrating over a symmetric interval, so:</p>
<div class="math notranslate nohighlight">
\[\int_{-\infty}^{\infty} z \cdot \phi(z) \, dz = 0\]</div>
<p>Therefore, <span class="math notranslate nohighlight">\(E[X] = \sigma \cdot 0 + \mu \cdot 1 = \mu\)</span>. ✓</p>
</section>
<section id="theorem-the-variance-is-sigma-2">
<h4>Theorem: The Variance is <span class="math notranslate nohighlight">\(\sigma^2\)</span><a class="headerlink" href="#theorem-the-variance-is-sigma-2" title="Link to this heading"></a></h4>
<p>For <span class="math notranslate nohighlight">\(X \sim N(\mu, \sigma)\)</span>, <span class="math notranslate nohighlight">\(\text{Var}(X) = \sigma^2\)</span> .</p>
<p><em>Proof:</em></p>
<p>Using the standardization <span class="math notranslate nohighlight">\(Z = \frac{X-\mu}{\sigma}\)</span>, we know that <span class="math notranslate nohighlight">\(X = \sigma Z + \mu\)</span>. By the properties of variance:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(X) = \text{Var}(\sigma Z + \mu) = \sigma^2 \text{Var}(Z)\]</div>
<p>So we need to show that <span class="math notranslate nohighlight">\(\text{Var}(Z) = 1\)</span> for the standard normal.</p>
<div class="math notranslate nohighlight">
\[\text{Var}(Z) = E[Z^2] - (E[Z])^2 = E[Z^2] - 0^2 = E[Z^2]\]</div>
<div class="math notranslate nohighlight">
\[E[Z^2] = \int_{-\infty}^{\infty} z^2 \cdot \frac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{2}} dz\]</div>
<p>Using integration by parts with <span class="math notranslate nohighlight">\(u = z\)</span> and <span class="math notranslate nohighlight">\(dv = z e^{-\frac{z^2}{2}} dz\)</span>,
we have <span class="math notranslate nohighlight">\(du = dz\)</span> and <span class="math notranslate nohighlight">\(v = -e^{-\frac{z^2}{2}}\)</span>. Then:</p>
<div class="math notranslate nohighlight">
\[\int z^2 e^{-\frac{z^2}{2}} dz = z(-e^{-\frac{z^2}{2}}) - \int (-e^{-\frac{z^2}{2}}) dz = -ze^{-\frac{z^2}{2}} + \int e^{-\frac{z^2}{2}} dz\]</div>
<p>The boundary term <span class="math notranslate nohighlight">\(\left[-ze^{-\frac{z^2}{2}}\right]_{-\infty}^{\infty} = 0\)</span> since exponential decay dominates linear growth.
Therefore,</p>
<div class="math notranslate nohighlight">
\[E[Z^2] = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\frac{z^2}{2}} dz = \frac{1}{\sqrt{2\pi}} \cdot \sqrt{2\pi} = 1\]</div>
<p>Thus <span class="math notranslate nohighlight">\(\text{Var}(Z) = 1\)</span> and <span class="math notranslate nohighlight">\(\text{Var}(X) = \sigma^2\)</span>. ✓</p>
</section>
</section>
</section>
<section id="assessing-normality-in-practice-why-it-matters">
<h2><span class="section-number">6.4.8. </span>Assessing Normality in Practice: Why It Matters<a class="headerlink" href="#assessing-normality-in-practice-why-it-matters" title="Link to this heading"></a></h2>
<div class="video-placeholder" role="group" aria-labelledby="video-ch6-4-3">
  <iframe
    id="video-ch6-4-3"
    title="STAT 350 – Chapter 6.4.3 Checking Normality of Data Video"
    src="https://www.youtube.com/embed/iuWe6rxgNbI?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
    allowfullscreen>
  </iframe>
</div><p>In statistical practice, we frequently need to determine whether observed data comes from a normal distribution.
This assessment is crucial because many statistical procedures—confidence intervals, t-tests, ANOVA, and
regression—assume normality or rely on estimators whose sampling distributions are approximately normal.</p>
<p>While we’ve established the theoretical foundation of the normal distribution, real data is messy.
Heights, weights, test scores, and measurement errors may approximately follow normal patterns,
but we need systematic methods to evaluate how close our data comes to this idealized mathematical model.</p>
<section id="the-challenge-of-real-world-assessment">
<h3>The Challenge of Real-World Assessment<a class="headerlink" href="#the-challenge-of-real-world-assessment" title="Link to this heading"></a></h3>
<p>Unlike our theoretical examples with known parameters, real data presents several challenges:</p>
<ul class="simple">
<li><p>We don’t know the true population parameters μ and σ</p></li>
<li><p>Sample sizes are finite, introducing sampling variability</p></li>
<li><p>Real phenomena may deviate from perfect normality in subtle ways</p></li>
<li><p>We need to distinguish between minor departures that don’t affect our analyses and serious violations that require different approaches</p></li>
</ul>
</section>
<section id="a-multi-faceted-approach">
<h3>A Multi-Faceted Approach<a class="headerlink" href="#a-multi-faceted-approach" title="Link to this heading"></a></h3>
<p>Assessing normality requires multiple complementary methods because no single approach provides complete information. We combine:</p>
<ol class="arabic simple">
<li><p><strong>Visual methods</strong> that reveal patterns and deviations at a glance</p></li>
<li><p><strong>Numerical checks</strong> that quantify adherence to normal distribution properties</p></li>
<li><p><strong>Formal statistical tests</strong> that provide rigorous hypothesis testing frameworks</p></li>
</ol>
</section>
</section>
<section id="visual-assessments-for-normality">
<h2><span class="section-number">6.4.9. </span>Visual Assessments for Normality<a class="headerlink" href="#visual-assessments-for-normality" title="Link to this heading"></a></h2>
<section id="a-histograms-with-overlaid-curves">
<h3>A. Histograms with Overlaid Curves<a class="headerlink" href="#a-histograms-with-overlaid-curves" title="Link to this heading"></a></h3>
<p>The most intuitive approach overlays three elements on a histogram of the data:</p>
<ul class="simple">
<li><p>The <strong>histogram</strong> itself, showing the actual distribution of observations</p></li>
<li><p>A <strong>kernel density estimate</strong> (smooth red curve) that traces the data’s shape without assuming any particular distribution</p></li>
<li><p>A <strong>normal density curve</strong> (blue curve) fitted using the sample mean and standard deviation</p></li>
</ul>
<figure class="align-center" id="id10" style="width: 70%">
<img alt="Histogram with kernel density and normal curve overlay" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/histogram.png" />
<figcaption>
<p><span class="caption-number">Fig. 6.13 </span><span class="caption-text">Comparing actual data distribution (purple histogram) with its smooth estimate (red) and fitted normal curve (blue)*</span><a class="headerlink" href="#id10" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>When data follows a normal distribution, these three elements align closely. Deviations reveal specific patterns:</p>
<ul class="simple">
<li><p><strong>Skewness</strong>: The red curve shifts away from the blue curve</p></li>
<li><p><strong>Heavy tails</strong>: The red curve extends further than the blue curve</p></li>
<li><p><strong>Light tails</strong>: The red curve falls short of the blue curve’s extent</p></li>
<li><p><strong>Multimodality</strong>: The red curve shows multiple peaks while the blue curve shows only one</p></li>
</ul>
</section>
<section id="b-normal-probability-plots-a-sophisticated-diagnostic">
<h3>B. Normal Probability Plots: A Sophisticated Diagnostic<a class="headerlink" href="#b-normal-probability-plots-a-sophisticated-diagnostic" title="Link to this heading"></a></h3>
<p>Normal probability plots (also called QQ-plots for “quantile-quantile plots”) provide a more sensitive method for
detecting departures from normality. These plots directly compare the quantiles of our data with the quantiles we
would expect if the data truly came from a normal distribution.</p>
<section id="steps-of-constructing-a-qq-plot">
<h4>Steps of Constructing a QQ-Plot<a class="headerlink" href="#steps-of-constructing-a-qq-plot" title="Link to this heading"></a></h4>
<ol class="arabic">
<li><p>Order the Data</p>
<p>Arrange the n observations from smallest to largest: <span class="math notranslate nohighlight">\(x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}\)</span></p>
</li>
<li><p>Assign Theoretical Probabilities</p>
<p>Each ordered observation <span class="math notranslate nohighlight">\(x_{(i)}\)</span> represents approximately the <span class="math notranslate nohighlight">\(\frac{i-0.5}{n}\)</span>
quantile of the data distribution. The adjustment of <span class="math notranslate nohighlight">\(-0.5\)</span> centers each data point within
its expected quantile interval, providing more accurate comparisons.</p>
</li>
<li><p>Find Corresponding Normal Quantiles</p>
<p>For each probability <span class="math notranslate nohighlight">\(p_i = \frac{i-0.5}{n}\)</span>, find the z-value <span class="math notranslate nohighlight">\(z_i\)</span> such that
<span class="math notranslate nohighlight">\(\Phi(z_i) = p_i\)</span>. These are the theoretical quantiles we would expect if the data came
from a standard normal distribution.</p>
</li>
<li><p>Create the Plot</p>
<p>Plot the ordered data values <span class="math notranslate nohighlight">\(x_{(i)}\)</span> (y-axis) against the theoretical quantiles <span class="math notranslate nohighlight">\(z_i\)</span> (x-axis).</p>
</li>
<li><p>Add a Reference Line</p>
<p>The reference line <span class="math notranslate nohighlight">\(y = \bar{x} + s \cdot z\)</span> shows where points would fall if the data
perfectly matched a normal distribution with the sample’s mean and standard deviation.</p>
</li>
</ol>
</section>
<section id="interpreting-qq-plots">
<h4>Interpreting QQ-Plots<a class="headerlink" href="#interpreting-qq-plots" title="Link to this heading"></a></h4>
<p>The power of QQ-plots lies in how different departures from normality create characteristic patterns.</p>
<p><strong>Perfect Normality</strong>: Points fall exactly on the reference line.</p>
<figure class="align-center" id="id11" style="width: 80%">
<img alt="QQ-plot showing perfectly normal data" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/qq-normal.png" />
<figcaption>
<p><span class="caption-number">Fig. 6.14 </span><span class="caption-text">Normal probability plot for normal data</span><a class="headerlink" href="#id11" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Long Tails</strong>: Points begin below the line but curves above for larger values.</p>
<ul class="simple">
<li><p>Data has more extreme values than a normal distribution would predict</p></li>
<li><p>The lower tail extends further left, upper tail extends further right</p></li>
<li><p>Common in financial data, measurement errors with occasional large mistakes</p></li>
</ul>
<figure class="align-center" id="id12" style="width: 80%">
<img alt="QQ-plot showing perfectly normal data" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/qq-long-tail.png" />
<figcaption>
<p><span class="caption-number">Fig. 6.15 </span><span class="caption-text">Normal probability plot for long-tailed data</span><a class="headerlink" href="#id12" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Short Tails</strong>: Points begin above the line but curves below for larger values.</p>
<ul class="simple">
<li><p>Data is more concentrated around the center than normal</p></li>
<li><p>Fewer extreme values than expected</p></li>
<li><p>Sometimes seen in truncated or bounded measurements</p></li>
</ul>
<figure class="align-center" id="id13" style="width: 80%">
<img alt="QQ-plot showing perfectly normal data" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/qq-short-tail.png" />
<figcaption>
<p><span class="caption-number">Fig. 6.16 </span><span class="caption-text">Normal probability plot for short-tailed data</span><a class="headerlink" href="#id13" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Right (Positive) Skewness</strong>: Concave-up curve</p>
<figure class="align-center" id="id14" style="width: 80%">
<img alt="QQ-plot showing perfectly normal data" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/qq-right-skew.png" />
<figcaption>
<p><span class="caption-number">Fig. 6.17 </span><span class="caption-text">Normal probability plot for right-skewed data</span><a class="headerlink" href="#id14" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Left (Negative) Skewness</strong>: Concave-down curve</p>
<figure class="align-center" id="id15" style="width: 80%">
<img alt="QQ-plot showing perfectly normal data" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/qq-left-skew.png" />
<figcaption>
<p><span class="caption-number">Fig. 6.18 </span><span class="caption-text">Normal probability plot for left-skewed data</span><a class="headerlink" href="#id15" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Bimodality</strong>: S-shaped curve with plateaus</p>
<ul class="simple">
<li><p>Points cluster in the middle region of the plot</p></li>
<li><p>Suggests the data might come from a mixture of two populations</p></li>
</ul>
<figure class="align-center" id="id16" style="width: 80%">
<img alt="QQ-plot showing perfectly normal data" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter6/qq-bimodal.png" />
<figcaption>
<p><span class="caption-number">Fig. 6.19 </span><span class="caption-text">Normal probability plot for bimodal data</span><a class="headerlink" href="#id16" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
</section>
<section id="numerical-assessments-for-normality">
<h2><span class="section-number">6.4.10. </span>Numerical Assessments for Normality<a class="headerlink" href="#numerical-assessments-for-normality" title="Link to this heading"></a></h2>
<p>While visual methods provide intuitive insights, numerical methods offer precise,
quantifiable assessments of normality.</p>
<section id="a-the-empirical-rule-in-reverse">
<h3>A. The Empirical Rule in Reverse<a class="headerlink" href="#a-the-empirical-rule-in-reverse" title="Link to this heading"></a></h3>
<p>Instead of using the 68-95-99.7 rule to predict probabilities, we can use it in reverse to check whether our data behaves as a normal distribution should:</p>
<p>For truly normal data,</p>
<ul class="simple">
<li><p>Approximately <strong>68%</strong> of observations should fall within one standard deviation: <span class="math notranslate nohighlight">\(\bar{x} \pm s\)</span>.</p></li>
<li><p>Approximately <strong>95%</strong> should fall within two standard deviations: <span class="math notranslate nohighlight">\(\bar{x} \pm 2s\)</span>.</p></li>
<li><p>Approximately <strong>99.7%</strong> should fall within three standard deviations: <span class="math notranslate nohighlight">\(\bar{x} \pm 3s\)</span>.</p></li>
</ul>
<section id="implementation-steps">
<h4>Implementation Steps<a class="headerlink" href="#implementation-steps" title="Link to this heading"></a></h4>
<ol class="arabic simple">
<li><p>Calculate the sample mean <span class="math notranslate nohighlight">\(\bar{x}\)</span> and sample standard deviation <span class="math notranslate nohighlight">\(s\)</span>.</p></li>
<li><p>Count observations within each interval.</p></li>
<li><p>Compare observed proportions to expected proportions (0.68, 0.95, 0.997).</p></li>
<li><p>Large deviations suggest non-normality.</p></li>
</ol>
</section>
</section>
<section id="b-the-iqr-to-standard-deviation-ratio">
<h3>B. The IQR-to-Standard Deviation Ratio<a class="headerlink" href="#b-the-iqr-to-standard-deviation-ratio" title="Link to this heading"></a></h3>
<p>For normal distributions, there’s a consistent relationship between the interquartile range and the
standard deviation. This relationship arises from the fixed positions of quantiles in any normal distribution.</p>
<p>For any normal distribution <span class="math notranslate nohighlight">\(N(\mu, \sigma)\)</span>:</p>
<ul class="simple">
<li><p>The first quartile (25th percentile) occurs at <span class="math notranslate nohighlight">\(\mu - 0.674\sigma\)</span>.</p></li>
<li><p>The third quartile (75th percentile) occurs at <span class="math notranslate nohighlight">\(\mu + 0.674\sigma\)</span>.</p></li>
<li><p>Therefore: <span class="math notranslate nohighlight">\(IQR = Q_3 - Q_1 = 1.348\sigma\)</span>.</p></li>
<li><p>The ratio <span class="math notranslate nohighlight">\(\frac{IQR}{\sigma} \approx 1.35\)</span> (often rounded to 1.4).</p></li>
</ul>
<section id="id2">
<h4>Implementation Steps<a class="headerlink" href="#id2" title="Link to this heading"></a></h4>
<ol class="arabic simple">
<li><p>Calculate the sample IQR and sample standard deviation <span class="math notranslate nohighlight">\(s\)</span>.</p></li>
<li><p>Compute the ratio <span class="math notranslate nohighlight">\(\frac{IQR}{s}\)</span>.</p></li>
<li><p>Values close to 1.35 suggest normality.</p></li>
<li><p>Values substantially different indicate departures from normality.</p></li>
</ol>
</section>
</section>
</section>
<section id="formal-statistical-tests-for-assessing-normality">
<h2><span class="section-number">6.4.11. </span>Formal Statistical Tests for Assessing Normality<a class="headerlink" href="#formal-statistical-tests-for-assessing-normality" title="Link to this heading"></a></h2>
<p>While visual and numerical methods provide insights, formal statistical tests
such as Shapiro-Wilk Test and Kolmovorov-Smirnov Tests offer rigorous
frameworks for hypothesis testing about normality. These tests are covered in more advanced statistics courses.</p>
</section>
<section id="bringing-it-all-together">
<h2><span class="section-number">6.4.12. </span>Bringing It All Together<a class="headerlink" href="#bringing-it-all-together" title="Link to this heading"></a></h2>
<div class="important admonition">
<p class="admonition-title">Key Takeaways 📝</p>
<ol class="arabic simple">
<li><p>The <strong>normal distribution</strong> emerged from Gauss’s work on measurement errors and has
become the most important continuous distribution in statistics.</p></li>
<li><p>The <strong>PDF</strong> <span class="math notranslate nohighlight">\(f_X(x) = \frac{1}{\sqrt{2\pi \sigma}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}\)</span>
is completely determined by two parameters: location <span class="math notranslate nohighlight">\(\mu\)</span> and scale <span class="math notranslate nohighlight">\(\sigma\)</span>.</p></li>
<li><p><strong>All normal distributions</strong> are symmetric, unimodal, and bell-shaped, with inflection
points at <span class="math notranslate nohighlight">\(\mu \pm \sigma\)</span>.</p></li>
<li><p>The <strong>empirical rule</strong> (68-95-99.7) provides quick probability estimates and applies to every
normal distribution regardless of parameters.</p></li>
<li><p><strong>Mathematical rigor</strong>: We proved the normal PDF is valid through polar coordinate integration and
confirmed that <span class="math notranslate nohighlight">\(E[X] = \mu\)</span> and <span class="math notranslate nohighlight">\(\text{Var}(X) = \sigma^2\)</span>.</p></li>
<li><p>The <strong>standard normal</strong> <span class="math notranslate nohighlight">\(N(0,1)\)</span> serves as the foundation for all normal computations through the
standardization transformation <span class="math notranslate nohighlight">\(Z = \frac{X-\mu}{\sigma}\)</span>.</p></li>
<li><p><strong>Assessing normality</strong> requires multiple approaches: visual methods (histograms, QQ-plots),
numerical checks (empirical rule, IQR ratios), and formal tests (Shapiro-Wilk, Kolmogorov-Smirnov variants).</p></li>
</ol>
</div>
<section id="exercises">
<h3>Exercises<a class="headerlink" href="#exercises" title="Link to this heading"></a></h3>
<ol class="arabic simple">
<li><p><strong>Empirical Rule Applications</strong>: A normal distribution has <span class="math notranslate nohighlight">\(\mu = 100\)</span> and <span class="math notranslate nohighlight">\(\sigma = 15\)</span>.</p>
<ol class="loweralpha simple">
<li><p>Find the intervals containing approximately 68%, 95%, and 99.7% of the probability</p></li>
<li><p>What percentage of values fall between 85 and 130?</p></li>
<li><p>Values beyond what points would be considered unusual (more than 2 standard deviations from the mean)?</p></li>
</ol>
</li>
<li><p><strong>Standardization Practice</strong>: For <span class="math notranslate nohighlight">\(X \sim N(25, 16)\)</span>, find the standardized values corresponding to:</p>
<ol class="loweralpha simple">
<li><p><span class="math notranslate nohighlight">\(x = 25\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(x = 29\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(x = 17\)</span></p></li>
<li><p>What do these z-values tell you about the original x-values?</p></li>
</ol>
</li>
<li><p><strong>Parameter Estimation</strong>: If you know that a normal distribution has its inflection points at <span class="math notranslate nohighlight">\(x = 12\)</span>
and <span class="math notranslate nohighlight">\(x = 18\)</span>, determine <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>.</p></li>
</ol>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="6-3-cdfs.html" class="btn btn-neutral float-left" title="6.3. Cumulative Distribution Functions" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="6-5-uniform-distribution.html" class="btn btn-neutral float-right" title="6.5. Uniform Distribution" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
   
  <div class="footer-credits">
    <a href="../../Website/credits">Website Credits</a>
  </div>


</body>
</html>