.. _5-2-joint-pmfs:

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch5-2">
     <iframe
       id="video-ch5-2"
       title="STAT 350 ‚Äì Chapter 5.2 Joint Probability Mass Function Video"
       src="https://www.youtube.com/embed/eJa8C_Yg0dg?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
       allowfullscreen>
     </iframe>
   </div>

.. admonition:: Slides üìä
   :class: tip
   
   `Download Chapter 5 slides (PPTX) <https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/
   slides/Chapter%205%20Discrete%20Distributions/L9-11-RandomVariables%20DiscreteProbabilityDistributions%28Chapter%205%29_AC.pptx>`_


Joint Probability Mass Functions
================================================

Many real-world scenarios involve multiple random quantities that interact with each other. 
To analyze such situations, we need to understand how random variables behave together. 
Joint probability mass functions provide the mathematical
foundation for analyzing multiple discrete random variables simultaneously.

.. admonition:: Road Map üß≠
   :class: important

   - Define **joint probability mass functions** for multiple discrete random variables.
   - Explore **tabular and functional representations** of joint PMFs.
   - Understand how to derive **marginal distributions** from joint distributions.
   - Identify when random variables are **independent** based on their joint PMF.

Joint Probability Mass Functions
--------------------------------------------

When dealing with a single discrete random variable, we used a probability mass 
function (PMF) to specify the probabilities associated with each possible value. 
We now extend this concept to multiple random variables.

Definition
~~~~~~~~~~~~~

The **joint probability mass function (joint PMF)** for two discrete random variables 
:math:`X` and :math:`Y` is denoted by :math:`p_{X,Y}`, and it gives the probability that 
:math:`X` equals some value :math:`x` **and** 
:math:`Y`  equals some value :math:`y` simultaneously:

.. math::

   p_{X,Y}(x,y) = P(\{X = x\} \cap \{Y = y\}).

Concisely, we also write :math:`p_{X,Y}(x,y) = P(X=x, Y=y).`

This definition extends naturally to more than two random variables. For example, 
the joint PMF for three random variables :math:`X`, :math:`Y`, and :math:`Z` would be 
denoted as :math:`p_{X,Y,Z}(x,y,z)`.

Support
~~~~~~~~

The **support** of a joint PMF is the set of all pairs :math:`(x,y)` for which the 
PMF assigns a positive probability:

.. math::

   \text{supp}(X,Y) = \{(x,y) \mid p_{X,Y}(x,y) > 0\}.


Representations of Joint PMFs
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Joint probability mass functions can be represented in several ways.

**Tabular Form**  

For two discrete random variables with finite supports, we can represent 
the joint PMF as a table. Each cell contains the probability that 
:math:`X` equals the row value and :math:`Y` equals the column value.

.. admonition:: Exampleüí°: Joint PMF Table
  :class: note 

  Consider rolling a fair four-sided die and a fair six-sided die which are
  indepedent. Let :math:`X` represent the outcome of the four-sided die and :math:`Y` represent 
  the outcome of the six-sided die.

  .. flat-table:: 
    :header-rows: 2
    :width: 80%
    :align: center

    * - :cspan:`6` Joint PMF :math:`p_{X,Y}(x,y)` for the fair 4-sided and 6-sided dice

    * - :math:`x \backslash y`
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
    * - 1
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
    * - 2
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
    * - 3
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
    * - 4
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`


  Since the dice are fair and independent, each combination has the 
  same probability: :math:`1/24` (there are :math:`4 \times 6 = 24` possible outcomes).


**Functional Form**  

For certain pairs of random variables, it is possible to express their
joint PMF as a mathematical formula involving :math:`x` and :math:`y.`

.. admonition:: Exampleüí°: Joint PMF in functional form
  :class: note 

  For the dice example, we can express the joint PMF concisely as:

  .. math::

    p_{X,Y}(x,y) = \frac{1}{24}

  for :math:`x \in \{1, 2, 3, 4\}` and :math:`y \in \{1, 2, 3, 4, 5, 6\}.`

Validity of a Joint PMF
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Like single-variable PMFs, joint PMFs must satisfy the follwing two axioms.

1. **Non-negativity**
   
   For all values of :math:`x` and :math:`y`, :math:`0 \leq p_{X,Y}(x,y) \leq 1.`

2. **Total probability of 1**

   The sum of all probabilities in the joint PMF must equal 1:

.. math::

   \sum_{(x,y) \in \text{supp}(X,Y)} p_{X,Y}(x,y) = 1.



Marginal Distributions
-------------------------

Marginal PMF
~~~~~~~~~~~~~~

A **marginal PMF** is the individual probability mass function 
of a random variable that forms a joint PMF with others.

Deriving Marginal PMFs from a Joint PMF
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

One of the most important operations we can perform with a joint PMF is deriving 
marginal PMFs for individual random variables.

To find the marginal PMF :math:`p_X(x)`, we sum the joint PMF for each fixed value
:math:`x` over all possible values of :math:`Y`:

.. math::

   p_X(x) = \sum_{y: p_Y(y) > 0} p_{X,Y}(x,y)

Similarly, to find the marginal PMF :math:`p_Y(y)`, we sum the joint PMF for each fixed 
value :math:`y` over all possible values of :math:`X`:

.. math::

   p_Y(y) = \sum_{x: p_X(x) > 0} p_{X,Y}(x,y)

In tabular form, the marginal PMF values are computed as row-wise or column-wise sums 
of the joint PMF and are
often recorded in the *margins* of the table--hence the name *marginal* PMF.

.. admonition:: Marginal PMFs and the Law of Partitions
  :class: important 

  Deriving a marginal PMF is a direct application of
  the Law of Partitions. In the case of :math:`p_X(x)`, 
  we treat the support of :math:`Y` as a partition of the sample space
  and sum the probabilities of small sections of :math:`\{X=x\}` created by
  its overlap with different events in the partition.

  .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter5/marginal-partitions.png
     :align: center
     :figwidth: 50%
     :alt: marginal PMF derivation explained through the Law of Partitions

     Marginal PMF explained through the Law of Partitions


.. admonition:: Exampleüí°: Calculating marginal PMFs from a Joint PMF
  :class: note 

  Let's calculate the marginal distributions for the independent fair dice example.

  .. flat-table:: 
    :header-rows: 2
    :width: 90%
    :align: center

    * - :cspan:`7` Marginal PMFs from the Joint PMF of for the fair 4-sided and 6-sided dice

    * - :math:`x \backslash y`
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - :math:`p_X(x)`

    * - **1**
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{6}{24}=\tfrac14`
    * - **2**
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{6}{24}=\tfrac14`
    * - **3**
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{6}{24}=\tfrac14`
    * - **4**
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{1}{24}`
      - :math:`\tfrac{6}{24}=\tfrac14`
    * - :math:`p_Y(y)`
      - :math:`\tfrac{4}{24} =\tfrac16`
      - :math:`\tfrac{4}{24} =\tfrac16`
      - :math:`\tfrac{4}{24} =\tfrac16`
      - :math:`\tfrac{4}{24} =\tfrac16`
      - :math:`\tfrac{4}{24} =\tfrac16`
      - :math:`\tfrac{4}{24} =\tfrac16`
      -


Independence of Random Variables
------------------------------------

Definition
~~~~~~~~~~~~~

Two discrete random variables :math:`X` and :math:`Y` are **independent** if and only if 
their joint PMF factors as the product of their marginal PMFs for all values in the support.
Mathematically, they are independent if and only if

.. math::

   p_{X,Y}(x,y) = p_X(x) p_Y(y) \text{ for all } (x,y) \in \text{supp}(X,Y).

What Does It Mean?
~~~~~~~~~~~~~~~~~~~~~

Independence of random variables :math:`X` and :math:`Y` means that knowing the value of 
one provides no information about the value of the other. 

With respect to the previously learned concept of independence of two **events**, 
this means that *any* event
written in terms of :math:`X` is independent of *any* event expressed in terms of :math:`Y`.

.. admonition:: Exampleüí°: Independence of Two Dice Shown Mathematically
  :class: note 

  In our dice example, :math:`X` and :math:`Y` are independent because

  .. math::

    p_{X,Y}(x,y) = \frac{1}{24} = \frac{1}{4} \times \frac{1}{6} = p_X(x) p_Y(y)

  for **all values** of :math:`x` and :math:`y` in the support.

.. admonition:: Be cautious üõë
   :class: danger 

   Independence is an important property that often simplifies probability calculations. 
   However, its convenient properties should only be used when 
   the idependence of :math:`X` and :math:`Y` is provided or
   shown mathematically.

.. admonition:: Exampleüí°: When the Dice Constrain Each Other
  :class: note 

  So far we have relied on independence to keep our calculations simple.  
  But real-world mechanisms often *couple* random quantities, 
  forcing their outcomes to move together. 

  Two ordinary **six-sided dice** are altered so that any roll whose 
  **sum is less than 3 or greater than 9 is physically impossible**.  
  
  Let :math:`X` represent the outcome of the first die and :math:`Y` the outcome of the second die.
  The rule :math:`3 \le X+Y \le 9` prunes the sample space, but
  among the *allowed* pairs, every combination is still equally likely.
  The table below shows the pruned outcomes as ‚ùå as well as the
  probabilities of the possible pairs. Since there are 29 unpruned entries in the table,
  each possible pair :math:`(x,y)` gets :math:`p_{X,Y} (x,y)= 1/29.` 
  
  .. flat-table:: 
    :header-rows: 2
    :width: 90%
    :align: center

    * - :cspan:`7` Joint and marginal PMFs of two dice constraining each other

    * - :math:`x \backslash y`
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - :math:`p_X(x)`

    * - **1**
      - ‚ùå
      - :math:`\tfrac{1}{29}`
      - :math:`\tfrac{1}{29}`
      - :math:`\tfrac{1}{29}`
      - :math:`\tfrac{1}{29}`
      - :math:`\tfrac{1}{29}`
      - :math:`\tfrac{5}{29}`
    * - **2**
      - :math:`\tfrac{1}{29}`
      - :math:`\tfrac{1}{29}`
      - :math:`\tfrac{1}{29}`
      - :math:`\tfrac{1}{29}`
      - :math:`\tfrac{1}{29}`
      - :math:`\tfrac{1}{29}`
      - :math:`\tfrac{6}{29}`
    * - **3**
      - :math:`\tfrac{1}{29}`
      - :math:`\tfrac{1}{29}`
      - :math:`\tfrac{1}{29}`
      - :math:`\tfrac{1}{29}`
      - :math:`\tfrac{1}{29}`
      - :math:`\tfrac{1}{29}`
      - :math:`\tfrac{6}{29}`
    * - **4**
      - :math:`\tfrac{1}{29}`
      - :math:`\tfrac{1}{29}`
      - :math:`\tfrac{1}{29}`
      - :math:`\tfrac{1}{29}`
      - :math:`\tfrac{1}{29}`
      - ‚ùå
      - :math:`\tfrac{5}{29}`
    * - **5**
      - :math:`\tfrac{1}{29}`
      - :math:`\tfrac{1}{29}`
      - :math:`\tfrac{1}{29}`
      - :math:`\tfrac{1}{29}`
      - ‚ùå
      - ‚ùå
      - :math:`\tfrac{4}{29}`
    * - **6**
      - :math:`\tfrac{1}{29}`
      - :math:`\tfrac{1}{29}`
      - :math:`\tfrac{1}{29}`
      - ‚ùå
      - ‚ùå
      - ‚ùå
      - :math:`\tfrac{3}{29}`
    * - :math:`p_Y(y)`
      - :math:`\tfrac{5}{29}`
      - :math:`\tfrac{6}{29}`
      - :math:`\tfrac{6}{29}`
      - :math:`\tfrac{5}{29}`
      - :math:`\tfrac{4}{29}`
      - :math:`\tfrac{3}{29}`
      -

  Both dice are now biased toward lower numbers‚Äîa direct result of our sum constraint.
  
  Let us now **prove or disprove the independence** of :math:`X` and :math:`Y`. 
  If we suspect dependence, it suffices to show that
  the equation :math:`p_{X,Y}(x,y) = p_X(x)p_Y(y)` fails for any single pair.
  Pick the pair :math:`(x=6,\; y=1)`:

  .. math::

    p_{X,Y}(6,1) \;=\; \frac{1}{29}
    \quad\text{but}\quad
    p_X(6)\,p_Y(1)
      \;=\;
      \frac{3}{29} \times \frac{5}{29}
      \;=\;
      \frac{15}{29^{2}}
      \;\neq\;
      \frac{1}{29}.

  Since the requirement for independence is :math:`p_{X,Y}(x,y) = p_X(x)p_Y(y)` for *all* possible
  pairs, :math:`X` and :math:`Y` has already failed the criterion. Therefore, they are dependent.

A joint distribution can encode constraints (a bounded sum in the previous example) that never appear in the marginals alone.  
Whenever the joint PMF doesn't factor, dependence is at play.

Bringing It All Together
---------------------------

.. admonition:: Key Takeaways üìù
  :class: important
  
  1. A **joint probability mass function** specifies the probability of two or more discrete random 
     variables taking on specific values simultaneously.
   
  2. Joint PMFs must satisfy the basic probability axioms: non-negativity and summing to 1 over the entire support.
   
  3. **Marginal distributions** can be derived from a joint PMF by summing over all possible values of the other variable(s).
   
  4. Random variables are **independent** if and only if their joint PMF equals the product of their marginal PMFs for all values in the support.
   
  5. When random variables are **dependent**, their joint distribution contains important information about how they relate to 
     each other that isn't captured by their marginal distributions alone.

Exercises
~~~~~~~~~~~~~~

#. **Marginal Distributions**: Given the joint PMF below, find the marginal PMFs for X and Y.

.. flat-table:: 
   :header-rows: 2
   :width: 50%
   :align: center

   * - :cspan:`3` Joint PMF :math:`p_{X,Y}(x,y)`
   * - :math:`x \backslash y`
     - 1
     - 2
     - 3
   * - 1
     - 0.1
     - 0.2
     - 0.1
   * - 2
     - 0.2
     - 0.3
     - 0.1

#. **Conditional Probability**: Using the joint PMF from problem 2, calculate:

   a) :math:`P(X = 1 | Y = 2)`
   b) :math:`P(Y = 3 | X = 2)`

#. **Testing Independence**: Determine whether the random variables X and Y with the following joint PMF are independent.

.. flat-table:: 
   :header-rows: 2
   :width: 50%
   :align: center

   * - :cspan:`2` Joint PMF :math:`p_{X,Y}(x,y)`
   * - :math:`x \backslash y`
     - 0
     - 1
   * - 0
     - 0.3
     - 0.2
   * - 1
     - 0.1
     - 0.4

#. **Joint PMF Construction**: Two fair six-sided dice are rolled. 
   Let :math:`X` be the minimum of the two values and :math:`Y` be the maximum.

   a) Construct the joint PMF of :math:`X` and :math:`Y`.
   b) Are :math:`X` and :math:`Y` independent? Explain why or why not.