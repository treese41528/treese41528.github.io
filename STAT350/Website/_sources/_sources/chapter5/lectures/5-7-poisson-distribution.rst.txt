.. _5-7-poisson-distribution:


.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch5-7">
     <iframe
       id="video-ch5-7"
       title="STAT 350 ‚Äì Chapter 5.7 The Poisson Distribution Video"
       src="https://www.youtube.com/embed/L9flxu2RCEc?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
       allowfullscreen>
     </iframe>
   </div>

.. admonition:: Slides üìä
   :class: tip
   
   `Download Chapter 5 slides (PPTX) <https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/
   slides/Chapter%205%20Discrete%20Distributions/L9-11-RandomVariables%20DiscreteProbabilityDistributions%28Chapter%205%29_AC.pptx>`_
   
The Poisson Distribution
======================================

While the binomial distribution helps us count successes in a fixed number of trials, 
many real-world situations involve counting rare events that occur randomly over time or space. 
Think about counting phone calls to a help desk during an hour, defects in a length of computer tape, 
or radioactive particles emitted by a radioactive substance. These scenarios share a structure that 
leads us to another fundamental distribution in statistics: the Poisson distribution.

.. admonition:: Road Map üß≠
   :class: important

   ‚Ä¢ Identify situations where the **Poisson distribution** applies.
   ‚Ä¢ Master the **Poisson probability mass function** and its single parameter :math:`\lambda`.
   ‚Ä¢ Derive the Poisson **expected value and variance** using infinite series techniques.
   ‚Ä¢ Understand how the **interval length affects the parameter** :math:`\lambda`.

From Counting Events to Modeling Rates
--------------------------------------------

The Poisson distribution emerges when we count events that occur randomly over continuous 
intervals of time, space, or volume. Unlike a binomial experiment which involves a fixed 
number of trials, the number of events in a Poisson process is theoretically unlimited.

What Makes a Process Poisson?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A Poisson process has three essential properties:

**1. Stationary and Proportional**

* The probability that an event occurs in any interval depends only on the length of 
  that interval, not on where the interval occurs (**stationarity**). 
* Equal-sized intervals have the same probability distribution, and longer intervals have 
  **proportionally** higher probabilities of containing events.

For example, suppose we're counting phone calls to a help desk that has a constant call rate. The probability of 
receiving exactly one call should be the same whether we look at 9-10 AM or 2-3 PM. 
Furthermore, if we expect 3 calls per hour on average, we should expect 6 calls per two-hour period.

**2. Independent Events**

Individual events occur independently of each other. The occurrence of one event 
doesn't influence when the next event will happen. Additionally, the number of 
events in non-overlapping intervals are independent of each other.

In our phone call example, receiving three calls between 9-10 AM doesn't affect 
the number of calls received between 10-11 AM.

**3.Orderliness (no bunching)**

Events cannot occur simultaneously. In sufficiently small intervals,
the chance of two or more events occurring is negligible. Events effectively arrive **one at a time**.

Examples of Poisson Distributions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Several real-world scenarios approximately follow the rules of Poisson processes:

- **Radioactive decay**: The number of alpha particles emitted from uranium-238 in one minute
- **Call centers**: The number of calls received during busy hours on any given day
- **Quality control**: The number of flaws on a computer tape of fixed length
- **Ecology**: The number of dead trees in a square mile of forest
- **Traffic**: The number of accidents at an intersection per month

The Poisson Distribution
------------------------------------------------------

When events follow a Poisson process, we model the count of 
events using a Poisson distribution.

Definition
~~~~~~~~~~~~~~

A Poisson random variable :math:`X` counts the number of independently occurring 
events in a fixed interval, where events occur at some **average rate** :math:`\lambda` (lambda)
**per interval**.

When :math:`X` has a Poisson distribution, we write :math:`X \sim \text{Poisson}(\lambda)`.

Probability Mass Function
~~~~~~~~~~~~~~~~~~~~~~~~~~~

The Poisson PMF gives the probability of observing exactly :math:`x` events:

.. math::

   p_X(x) = \frac{e^{-\lambda} \lambda^x}{x!}

for :math:`x \in \text{supp}(X) = \{0, 1, 2, 3, ...\}`.

Notice several key features of this distribution:

- **Single parameter**: Unlike the binomial distribution which has two parameters 
  (:math:`n` and :math:`p`), Poisson has only one parameter :math:`\lambda`.
- **Unbounded support**: A Poisson random variable can theoretically take any 
  non-negative integer value, unlike binomial whose support is bounded above by :math:`n`.

Expected Value and Variance: The Power of :math:`\lambda`
------------------------------------------------------------

One of the most elegant features of the Poisson distribution is that its parameter :math:`\lambda` 
completely determines both the center and spread of the distribution.

Expected Value
~~~~~~~~~~~~~~~~~~

To find :math:`E[X]` for a Poisson random variable, we use the definition of expected value:

.. math::

   E[X] = \sum_{x=0}^{\infty} x \cdot p_X(x) = \sum_{x=0}^{\infty} x \cdot \frac{e^{-\lambda} \lambda^x}{x!}

Since the first term equals zero with :math:`x = 0`, the summation effectively begins at :math:`x = 1`. 
Then in each term, we can cancel :math:`x` with the :math:`x` contained in :math:`x!` of the denominator.
Since :math:`e^\lambda` does not depend on :math:`x`, we can bring this outside the summation:

.. math::

   E[X] = \sum_{x=1}^{\infty} x \cdot \frac{e^{-\lambda} \lambda^x}{x!} = e^{-\lambda} \sum_{x=1}^{\infty} \frac{\lambda^x}{(x-1)!}

Substituting :math:`u = x - 1`, we get:

.. math::

   E[X] = e^{-\lambda} \lambda \sum_{u=0}^{\infty} \frac{\lambda^u}{u!}

The sum is the Taylor series for :math:`e^Œª`. Therefore:

.. math::

   E[X] = e^{-\lambda} \lambda \cdot e^{\lambda} = \lambda

Variance
~~~~~~~~~~~

For the variance, we use :math:`\text{Var}(X) = E[X^2] - (E[X])^2`. We already know :math:`E[X] = \lambda`, and
we need :math:`E[X^2]`:

.. math::

   E[X^2] = \sum_{x=0}^{\infty} x^2 \cdot \frac{e^{-\lambda} \lambda^x}{x!}

Through similar algebraic manipulation (starting at :math:`x = 1`, canceling factorials, and using substitutions), we can show:

.. math::

   E[X^2] = \lambda^2 + \lambda

Therefore:

.. math::

   \text{Var}(X) = E[X^2] - (E[X])^2 = (\lambda^2 + \lambda) - \lambda^2 = \lambda

Summary of Poisson Distribution Properties
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For :math:`X \sim Poisson(\lambda)`:

.. math::

   &\mu_X = E[X] = \lambda \\
   &\sigma_X^2 = \text{Var}(X) = \lambda \\
   &\sigma_X = \sqrt{\lambda}

The summary reveals a remarkable property of Poisson distribution; 
knowing :math:`\lambda` tells us everything about the distribution's location and spread.

Visualizing Poisson Distributions
----------------------------------

**Small** :math:`\lambda \, (\lambda=1)` 

When events are rare, most probability concentrates at 0 and 1, with a long right tail. 
The distribution is highly right-skewed.

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter5/lambda1.png
   :alt: Poisson distributions with Œª=1
   :align: center
   :width: 80%

   :math:`\lambda=1`

**Moderate** :math:`\lambda \, (\lambda=2.5)` 

As :math:`\lambda` increases, the mode shifts right and the distribution becomes less skewed. 
More probability spreads across multiple values.

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter5/lambda2.5.png
   :alt: Poisson distributions with Œª=2.5
   :align: center
   :width: 80%

   :math:`\lambda=2.5`

**Large** :math:`\lambda \, (\lambda=10)` 

For larger :math:`\lambda` values, the distribution approaches a symmetric, bell-shaped curve 
centered around :math:`\lambda`. The resemblance to a normal distribution becomes quite strong.

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter5/lambda10.png
   :alt: Poisson distributions with different Œª=10
   :align: center
   :width: 80%

   :math:`\lambda=10`

.. admonition:: Exampleüí°: IT Consultant Call Analysis
   :class: note

   An IT consultant receives **an average of 3 calls per hour**. We want to model 
   the number of calls using a Poisson distribution and answer several probability questions.

   **Setting Up the Model**

   Let :math:`X` count number of calls the consultant receives in the next hour.

   - **Stationary and Proportional**: Call rate is constant over time. ‚úì
   - **Independent**: One call doesn't influence when the next occurs. ‚úì  
   - **Orderliness**: Calls don't occur simultaneously. ‚úì

   Therefore: :math:`X \sim \text{Poisson}(\lambda = 3)`.

   **Solving Probability Problems**

   #. Find the probability of exactly one call.

      .. math::

         P(X = 1) = \frac{e^{-3} \cdot 3^1}{1!} = \frac{3e^{-3}}{1} ‚âà 0.1494


   #. Find the probability of more than one call.
      Using the complement rule:

      .. math::

         P(X > 1) = 1 - [P(X = 0) + P(X = 1)] = 1 - [e^{-3} + 3e^{-3}] ‚âà 0.8009

   #. Find the probability of exactly 5 calls in the next 
      **two hours**.

      Since the rate is 3 calls per hour, over two hours we expect 2 √ó 3 = 6 calls on average.
      Let :math:`Y` count the number of calls in the next two hours.
      Then :math:`Y \sim \text{Poisson}(\lambda = 6)`.

      .. math::

         P(Y = 5) = \frac{e^{-6} \cdot 6^5}{5!} ‚âà 0.1606

   #. Find the probability of 1 call in the next hour and 4 calls
      in the following hour.

      Let :math:`X_1` count the calls in the first hour and :math:`X_2`
      the calls in the second hour. Both have an average rate of 3,
      and since the two intervals are non-overlapping, :math:`X_1` and :math:`X_2` are
      **indepdendent**. This allows us to **use the special multiplication rule**:

      .. math::

         P(X_1=1 \cap X_2=4) &= P(X_1=1)P(X_2=4)\\
         &=(0.8009)\left(\frac{e^{-3}3^4}{4!}\right) \\
         &= (0.8009)(0.1680) \approx 0.1346

When to Use Poisson vs. Binomial
-----------------------------------------------

Understanding when to use Poisson versus binomial distributions is crucial for proper 
statistical modeling.

**Use Poisson When:**

- Counting events over continuous intervals (time, space, volume).
- Events are rare relative to opportunity.
- The number of potential events is very large or unlimited.
- You know the average rate but not the total number of trials.

**Use Binomial When:**

- The number of independent trials is fixed.
- Each trial has exactly two outcomes.
- Probability of success is constant across trials.
- You're counting successes among a known number of attempts.

**Poisson as Binomial Approximation**

Interestingly, Poisson can approximate binomial when :math:`n` is large and :math:`p` is small,
with :math:`np \approx \lambda`. This connection highlights how these distributions relate to different 
aspects of the same underlying counting process.

Bringing It All Together
---------------------------

The Poisson distribution serves as a fundamental model for understanding random processes in fields ranging 
from telecommunications and quality control to epidemiology and reliability engineering. 
As we've seen, the key to successful application lies in recognizing when events satisfy the Poisson 
assumptions and correctly interpreting the rate parameter :math:`Œª` in context.

.. admonition:: Key Takeaways üìù
   :class: important

   1. The **Poisson distribution** models counts of rare events occurring over fixed intervals of time, space, or volume.
   
   2. **Three key properties** that define Poisson processes are
      stationarity and proportional rates, independence of events, and unique event occurrences.
   
   3. The **PMF formula** :math:`p_X(x) = \frac{e^{-\lambda}\lambda^x}{x!}` applies to any 
      non-negative interger :math:`x`.

   4. The **single parameter** :math:`\lambda` represents both the mean and variance: :math:`E[X] = \text{Var}(X) = Œª`.
   
   5. The **shape of a Poisson distribution** evolves from highly right-skewed 
      to approximately symmetric as :math:`\lambda` becomes larger.

Exercises
~~~~~~~~~~~~~

1. **Identifying Poisson Processes**: Determine whether each scenario could be modeled using a Poisson distribution. 
   Explain your reasoning.
   
   a) The number of typos in a 10-page document
   b) The number of students who arrive late to a 50-student class
   c) The number of car accidents at a busy intersection per week
   d) The number of defective items in a batch of 100 manufactured products

2. **Basic Probability Calculations**: Emails arrive at a server at an average rate of 2.5 per minute. 
   Assume this follows a Poisson distribution.
   
   a) What is the probability of receiving exactly 3 emails in the next minute?
   b) What is the probability of receiving no emails in the next minute?
   c) What is the probability of receiving more than 4 emails in the next minute?
   d) What are the mean and standard deviation for the number of emails per minute?

3. **Parameter Scaling**: A call center receives an average of 4 calls per hour during peak times.
   
   a) What is the probability of receiving exactly 2 calls in 30 minutes?
   b) What is the probability of receiving at least 10 calls in 3 hours?
   c) In how long an interval would you expect to receive exactly 1 call on average?
..
   Working with Poisson Distributions in R
   ----------------------------------------

   R provides comprehensive functions for working with Poisson distributions, following the same naming convention as other distributions.

   **The Four Essential R Functions**

   - **rpois()**: Generates random samples from a Poisson distribution
   - **dpois()**: Calculates the probability mass function (exact probabilities)  
   - **ppois()**: Calculates the cumulative distribution function (cumulative probabilities)
   - **qpois()**: Finds quantiles (the inverse of ppois)

   **Generating Random Samples with rpois()**

   .. code-block:: r

      # Generate 10 random values from Poisson(Œª = 3)
      # Each value represents count of events in one interval
      set.seed(123)
      random_counts <- rpois(n = 10, lambda = 3)
      print(random_counts)
      # Output: 1 4 5 3 1 1 4 5 4 2
      
      # Generate 1000 samples to see the distribution pattern
      large_sample <- rpois(n = 1000, lambda = 3)
      
      # Check that sample mean approximates theoretical mean
      sample_mean <- mean(large_sample)
      sample_var <- var(large_sample)
      theoretical_mean <- 3  # Œª = 3
      theoretical_var <- 3   # Œª = 3
      
      print(paste("Sample mean:", round(sample_mean, 2)))
      print(paste("Theoretical mean:", theoretical_mean))
      print(paste("Sample variance:", round(sample_var, 2)))
      print(paste("Theoretical variance:", theoretical_var))

   **Calculating Exact Probabilities with dpois()**

   .. code-block:: r

      # Calculate P(X = 2) when X ~ Poisson(Œª = 3)
      prob_exactly_2 <- dpois(x = 2, lambda = 3)
      print(paste("P(X = 2) =", round(prob_exactly_2, 4)))
      
      # Calculate probabilities for multiple values
      x_values <- 0:10
      probabilities <- dpois(x = x_values, lambda = 3)
      
      # Create a probability table
      prob_table <- data.frame(
      x = x_values,
      probability = round(probabilities, 4)
      )
      print(prob_table)
      
      # Verify probabilities sum close to 1 (they won't equal exactly 1 
      # because we're truncating the infinite support)
      total_prob <- sum(dpois(x = 0:20, lambda = 3))
      print(paste("Total probability (x = 0 to 20):", round(total_prob, 6)))

   **Calculating Cumulative Probabilities with ppois()**

   .. code-block:: r

      # Calculate P(X ‚â§ 5) when X ~ Poisson(Œª = 3)
      prob_at_most_5 <- ppois(q = 5, lambda = 3)
      print(paste("P(X ‚â§ 5) =", round(prob_at_most_5, 4)))
      
      # Calculate P(X > 5) = 1 - P(X ‚â§ 5)
      prob_more_than_5 <- 1 - ppois(q = 5, lambda = 3)
      # Alternative: use lower.tail = FALSE
      prob_more_than_5_alt <- ppois(q = 5, lambda = 3, lower.tail = FALSE)
      print(paste("P(X > 5) =", round(prob_more_than_5, 4)))
      
      # Calculate P(2 ‚â§ X ‚â§ 6) = P(X ‚â§ 6) - P(X ‚â§ 1)
      prob_between <- ppois(q = 6, lambda = 3) - ppois(q = 1, lambda = 3)
      print(paste("P(2 ‚â§ X ‚â§ 6) =", round(prob_between, 4)))

   .. code-block:: r

      # Method 1: Using complement rule with dpois
      prob_zero <- dpois(x = 0, lambda = 3)
      prob_one <- dpois(x = 1, lambda = 3)
      prob_more_than_one <- 1 - (prob_zero + prob_one)
      print(paste("P(X > 1) =", round(prob_more_than_one, 4)))
      
      # Method 2: Using ppois with lower.tail = FALSE
      prob_more_than_one_alt <- ppois(q = 1, lambda = 3, lower.tail = FALSE)
      print(paste("P(X > 1) using ppois =", round(prob_more_than_one_alt, 4)))
      
      # Method 3: Direct calculation
      prob_at_most_one <- ppois(q = 1, lambda = 3)
      prob_more_than_one_direct <- 1 - prob_at_most_one
      print(paste("P(X > 1) direct =", round(prob_more_than_one_direct, 4)))

      .. code-block:: r

   # New parameter for 2-hour interval
   lambda_2hours <- 2 * 3  # 2 hours √ó 3 calls/hour
   
   # Calculate P(Y = 5) for the 2-hour interval
   prob_five_calls_2hrs <- dpois(x = 5, lambda = lambda_2hours)
   print(paste("P(5 calls in 2 hours) =", round(prob_five_calls_2hrs, 4)))
   
   # Expected values for comparison
   print(paste("Expected calls in 1 hour:", 3))
   print(paste("Expected calls in 2 hours:", lambda_2hours))
   print(paste("Standard deviation in 1 hour:", round(sqrt(3), 3)))
   print(paste("Standard deviation in 2 hours:", round(sqrt(lambda_2hours), 3)))

   **Comprehensive Analysis with Visualization**

   .. code-block:: r

   # Create comprehensive analysis
   
   # 1-hour analysis
   lambda_1hr <- 3
   x_1hr <- 0:12
   probs_1hr <- dpois(x_1hr, lambda = lambda_1hr)
   
   # 2-hour analysis  
   lambda_2hr <- 6
   x_2hr <- 0:18
   probs_2hr <- dpois(x_2hr, lambda = lambda_2hr)
   
   par(mfrow = c(1, 1))
   
   # Summary statistics
   cat("\nSummary Statistics:\n")
   cat("1-hour interval: Mean =", lambda_1hr, ", SD =", round(sqrt(lambda_1hr), 3), "\n")
   cat("2-hour interval: Mean =", lambda_2hr, ", SD =", round(sqrt(lambda_2hr), 3), "\n")
   cat("P(exactly 1 call in 1 hour) =", round(dpois(1, lambda_1hr), 4), "\n")
   cat("P(exactly 5 calls in 2 hours) =", round(dpois(5, lambda_2hr), 4), "\n")
   cat("P(more than 3 calls in 1 hour) =", round(1 - ppois(3, lambda_1hr), 4), "\n")


   .. code-block:: r

      # Calculate P(X = 1) when Œª = 3
      prob_one_call <- dpois(x = 1, lambda = 3)
      print(paste("P(X = 1) =", round(prob_one_call, 4)))
      
      # Manual calculation for verification
      manual_calc <- exp(-3) * 3^1 / factorial(1)
      print(paste("Manual calculation =", round(manual_calc, 4)))