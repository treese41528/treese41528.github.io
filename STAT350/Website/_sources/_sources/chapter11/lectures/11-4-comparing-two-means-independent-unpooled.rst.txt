.. _11-4-comparing-two-means-independent-unpooled:

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch11-4">
      <iframe
         id="video-ch11-4"
         title="STAT 350 – Chapter 11.4 Comparing Two Population Means Using Independent Samples: No Equal Variance Assumption Video"
         src="https://www.youtube.com/embed/875mJJL5hrQ?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>

.. admonition:: Slides 📊
   :class: tip

   `Download Chapter 11 slides (PPTX) <https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/
   slides/Chapter%2011%20Two%20Sample%20Procedures/CI%20and%20HT%20for%20Two%20Samples%20or%20Treatments_AC.pptx>`_


Independent Two-Sample Analysis - No Equal Variance Assumption
=====================================================================================================

When population standard deviations are unknown and we **cannot reasonably assume that the variances are 
equal** across populations, each variance must be estimated separately. 
This section develops the unpooled approach for indepndent two-sample comparisons and emphasizes 
the importance of choosing the appropriate method—pooled or unpooled—based on evidence provided by the data.

.. admonition:: Road Map 🧭
   :class: important

   * **Estimate the standard error** for independent two-sample comparison **without the equal variance assumption**. 
   * Recognize that the new pivotal method *approximately* follows a :math:`t`-distribution and that 
     its degrees of freedom must also be approximated.
   * Use **Welch-Satterthwaite approximation** for the degrees of freedom.
   * Learn the consequences of incorrectly using the pooled versus unpooled approaches and why
     it is **safer to use the more general unpooled approach** as the default.
   * Know that the unpooled procedure is robust to moderate departures from normality. List the
     characteristics of the data for which we expect the method to work best.

Mathematical Framework
-------------------------------------------------

.. admonition:: The Assumptions
   :class: important

   This lesson still develops upon the core assumptions introduced in Chapter 11.2.1.

Recall the true standard error for the difference of means :math:`\bar{X}_A - \bar{X}_B`:

.. math::

   \sigma_{\bar{X}_A - \bar{X}_B} = \sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}.

Since the two population variance :math:`\sigma^2_A` and :math:`\sigma^2_B` are not assumed
equal anymore, we do not take any further simplification steps and directly replace
the unknown values with their respective estimators, :math:`S^2_A` and :math:`S^2_B`.

The **estimated standard error** is then:

.. math::

   \widehat{SE} = \sqrt{\frac{S^2_A}{n_A} + \frac{S^2_B}{n_B}}.

It follows that the studentization of :math:`\bar{X}_A - \bar{X}_B` is:

.. math::
   :label: approx-test-stat

   T' = \frac{(\bar{X}_A - \bar{X}_B) - (\mu_A -\mu_B)}{\sqrt{\frac{S^2_A}{n_A} + \frac{S^2_B}{n_B}}}.

The pivotal quantity is denoted :math:`T'` with a prime because even when all assumptions hold,
it only *approximately* follows a :math:`t`-distribution. Not only that, the true degrees of freedom for the 
best approximating :math:`t`-distribution depends on the unknown variances and must also be approximated. 

We use the **Welch-Satterthwaite Approximation** for the unknown degrees of freedom:

.. math::

   \nu = \frac{\left(\frac{s^2_A}{n_A} + \frac{s^2_B}{n_B}\right)^2}{\frac{1}{n_A - 1}\left(\frac{s^2_A}{n_A}\right)^2 + \frac{1}{n_B - 1}\left(\frac{s^2_B}{n_B}\right)^2}

Once :math:`\nu` is computed, we treat :math:`T'` as having a :math:`t`-dstribution with the degrees of freedom :math:`\nu`
for further construction of inference methods. This approximation generally shows good performance in practice—it 
tends to produce a slightly conservative inference result (wider confidence regions, less likely to reject :math:`H_0`)
when sample sizes are small.

.. admonition:: The Approximated :math:`\nu` May Not Be an Integer
   :class: danger 

   :math:`\nu` is typically not integer-valued, which is okay. R accepts 
   non-integer values for the ``df`` argument of :math:`t`-related functions. Use the approximated :math:`\nu`
   without making any adjustments.



..
   **The Exact Degrees of Freedom Formula**

   If the population variances were known, the exact degrees of freedom would be:

   .. math::

      df_{exact} = \frac{\left(\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}\right)^2}{\frac{1}{n_A - 1}\left(\frac{\sigma^2_A}{n_A}\right)^2 + \frac{1}{n_B - 1}\left(\frac{\sigma^2_B}{n_B}\right)^2}

   This formula would yield an exact t-distribution for the test statistic. However, since the population variances are unknown, we must approximate.



Hypothesis Tests and Confidence Regions
-----------------------------------------------


.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch11-4-1">
      <iframe
         id="video-ch11-4-1"
         title="STAT 350 – Chapter 11.4.1 Only Using the Unpooled Estimator Video"
         src="https://www.youtube.com/embed/uzpqYPvmcYE?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>

Hypothesis Testing
~~~~~~~~~~~~~~~~~~~~~

Within the four-step framework of hypothesis testing, Steps 1, 2, and 4 of the unpooled approach 
is identical to the other types of independent two-sample comparisons. Revisit Chapter 11.2 for the
related details. We will focus on Step 3, where we compute the test statistic, degrees of freedom,
and p-value.

The observed test statistic takes the familiar form: it is the difference between the
observed point estimate and the null value, standardized by the estimated standard error.

.. math::

   t'_{TS} = \frac{(\bar{x}_A - \bar{x}_B) - \Delta_0}{\sqrt{\frac{s^2_A}{n_A} + \frac{s^2_B}{n_B}}}

Compute the degrees of freedom :math:`\nu` using the Welch-Satterthwaite Approximation formula.
Then the :math:`p`-values are:

- Two-sided: :math:`2P(T_\nu < -|t'_{TS}|)`
- Upper-tailed: :math:`P(T_\nu > |t'_{TS}|)`  
- Left-tailed: :math:`P(T_\nu < -|t'_{TS}|)`


.. admonition:: Example 💡: Teaching Methods Comparison

   To investigate whether directed reading activities improve students' reading ability, 
   students at an elementary school were randomly assigned to either directed reading activities or 
   standard instruction.

   Their performance was measured by Degree of Reading Power (DRP) scores.
   Higher DRP scores indicate better reading ability.
   The observed statistics are:

   .. flat-table:: 
      :align: center
      :width: 80%
      :header-rows: 1

      * - Method
        - New 
        - Standard

      * - Sample size
        - :math:`n_{new} = 21`
        - :math:`n_{std} = 23`

      * - Sample mean 
        - :math:`\bar{x}_{new} = 51.48`
        - :math:`\bar{x}_{std} = 41.52`

      * - Sample standard deviation
        - :math:`s_{new} = 11.01`
        - :math:`s_{std} = 17.15` 

   **Step 1: Define Parameters**

   Use :math:`\mu_{new}` to denote the true mean DRP score for students receiving directed reading instruction
   and :math:`\mu_{std}` for the true mean DRP score for students receiving traditional instruction.

   Both population standard deviations are unknown and will be estimated separately.

   **Step 2: Formulate Hypothesis**

   .. math:: 
      &H_0: \mu_{new} - \mu_{std} \leq 0\\
      &H_a: \mu_{new} - \mu_{std} > 0

   **Step 3-1: Explore Data and Choose Analysis Method** 

   .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter11/reading-instructions-example.png 
      :figwidth: 70%
      :align: center 
      :alt: Boxplots and histograms by groups

      Boxplots and historgrams by groups

   - Both distributions approximately normal with mild skewness in the traditional group.
     Sample sizes are adequate for mild departures from normality.
   - No serious outliers identified in modified box plots.
   - Some evidence that variances may differ between groups.

   We use the unpooled indpendent two-sample approach.

   **Step 3-2: Computate Test Statistic, DF, and p-Value**

   The observed test statistic is:

   .. math::

      t'_{TS} = \frac{(51.48 - 41.52) - 0}{\sqrt{\frac{11.01^2}{21} + \frac{17.15^2}{23}}} = \frac{9.96}{\sqrt{5.77 + 12.78}} = \frac{9.96}{4.31} = 2.31

   For the Welch-Satterthwaite approximate degrees of freedom, we encourage breaking down its computation
   into small components.

   * :math:`\frac{s^2_{new}}{n_{new}} = \frac{11.01^2}{21}=5.77`
   * :math:`\frac{s^2_{std}}{n_{std}} = \frac{17.15^2}{23}=12.78`

   Then finally, 

   .. math::

      \nu = \frac{\left(5.77 + 12.78\right)^2}{\frac{5.77^2}{20} + \frac{12.78^2}{22}} = \frac{343.25}{\frac{33.29}{20} + \frac{163.33}{22}} = \frac{343.25}{1.66 + 7.42} = 37.8

   The :math:`p`-value is :math:`P(T_{37.8} > 2.31)`. Using R,

   .. code-block:: r 

      pt(2.31, df = 37.8, lower.tail = FALSE)

   :math:`p`-value :math:`=0.013`.

   **Step 4: Write the Decision and Conclusion**

   Since p-value :math:`= 0.013 < \alpha = 0.05`, we reject the null hypothesis.
   The data give some support (p-value = 0.013) to the claim that directed reading activities improve 
   elementary school students' reading ability as measured by DRP scores.

Confidence Regions
---------------------------------------------

The confidence regions can be derived using the pivotal method and
:math:`T'` in Eq. :eq:`approx-test-stat`.

.. flat-table::
   :widths: 1 2
   :header-rows: 1

   * - :cspan:`1` Confidence regions for independent two-sample tests (unknown unpooled variances)
   
   * - **Confidence interval**
     - .. math::

         (\bar{x}_A - \bar{x}_B) \pm t_{\alpha/2,\nu} \sqrt{\frac{s^2_A}{n_A} + \frac{s^2_B}{n_B}}
   
   * - **Upper confidence bound**
     - .. math::

         (\bar{x}_A - \bar{x}_B) + t_{\alpha,\nu} \sqrt{\frac{s^2_A}{n_A} + \frac{s^2_B}{n_B}}

   * - **Lower confidence bound** 
     - .. math::

         (\bar{x}_A - \bar{x}_B) - t_{\alpha,\nu} \sqrt{\frac{s^2_A}{n_A} + \frac{s^2_B}{n_B}}

The critical value :math:`t_{\alpha/2,\nu}` (or :math:`t_{\alpha,\nu}`) is computed with
:math:`\nu` approximated using the Welch-Satterthwaite formula.

.. admonition:: Example 💡: Teaching Methods Comparison, Continued

   For the teaching methods comparison problem, compute the confidence region consistent with
   the previously performed hypothesis test. The summary statistics are:

   .. flat-table:: 
      :align: center
      :width: 80%
      :header-rows: 1

      * - Method
        - New 
        - Standard

      * - Sample size
        - :math:`n_{new} = 21`
        - :math:`n_{std} = 23`

      * - Sample mean 
        - :math:`\bar{x}_{new} = 51.48`
        - :math:`\bar{x}_{std} = 41.52`

      * - Sample standard deviation
        - :math:`s_{new} = 11.01`
        - :math:`s_{std} = 17.15` 

   **95% Lower Confidence Bound**

   An upper-tailed hypothesis test is consistent with a lower confidence bound as long as 
   the significance level and the confidence coefficient add to one. Therefore, we need to compute
   a 95% lower confidence bound for the difference :math:`\mu_{new} - \mu_{std}.`
   From the previous example, we already have

   * Observed sample difference: :math:`\bar{x}_{new} - \bar{x}_{std} = 9.96`
   * Estimated standard error: :math:`\widehat{SE} = 4.31`
   * The Welch-Satterthwaite approximate degrees of freedom: :math:`\nu = 37.8` 

   The critical value for a one-sided confidence region is 

   .. code-block:: r

      qt(0.05, df=37.8, lower.tail=FALSE)
      #returns 1.686

   Finally, the lower bound is:

   .. math::

      9.96 - 1.686(4.31) = 9.96 - 7.27 = 2.69

   We are 95% confident that the new teaching method improves DRP scores by more than 2.69 points on average.

.. admonition:: Using t.test() Function fo Inferences
   :class: important

   When we have the complete raw data, we can use ``t.test()`` to perform a hypothesis test and
   compute a confidence region simultaneously. 

   First, organize the data into two vectors.
   
   * The vector ``quantitiativeVariable`` should list all observations from **both** groups.
   * The vector ``categoricalVariable`` should list the group labels of the observations listed
     in the ``quantitativeVariable`` vector. 
     
   Then run the following code after replacing each argument with the appropriate values:

   .. code-block:: r

      t.test(quantitativeVariable ~ categoricalVariable, 
            mu = Delta0,
            conf.level = C,
            paired = FALSE,
            alternative = "alternative_hypothesis",
            var.equal = FALSE)

   For the teaching methods comparison problem, we would set

   * ``mu=0``
   * ``conf.level = 0.95``
   * ``paired=FALSE`` (Paired two-sample analysis if ``TRUE``)
   * ``alternative="greater"`` (other options are ``two.sided`` and ``less``)
   * ``var.equal=FALSE`` (Pooled if ``TRUE``; Unpooled is the default choice for this course)


Pooled vs Unpooled Approaches
--------------------------------------------------------------

When choosing between the pooled and unpooled approaches, it is 
preferable to use the method that best reflects the truth.
If the population variances are indeed equal, the pooled method is more appropriate;
otherwise, the unpooled approach should be used. However, since the true variances are 
typically unknown, this simple rule is often impractical to apply.

Let us examine the consequences of incorrectly using pooled versus unpooled methods and 
explain why we adopt **the unpooled approach as the default choice** in this course.

Incorrectly Using the Unpooled Approach
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

By using the unpooled approach for two populations whose variances are in fact equal, 
we lose efficiency in two aspects:

1. The analysis method becomes unnecessarily complicated. Instead of the exact :math:`t`-distribution 
   and the simple, integer-valued degrees of freedom :math:`df=n_A + n_B -2,` we must use the
   approximation method. 

2. The approach uses the data points less efficiently, leading to decreased power.

Note that incorrectly applying a **more general (unpooled) method to a special case** (equal true variances)
leads to some loss in efficiency but rarely leads to serious errors. 

Incorrectly using the Pooled Approach
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The consequences are typically more serious when a **special-case method is  
applied incorrectly to a general case**. Using a pooled approach on samples drawn from
populations with unequal variances risks not only a loss of efficiency but also a loss of 
theoretical validity.

The problem is especially pronounced when the two sample sizes differ substantially.
Consider a scenario where :math:`n_A = 1500`, :math:`n_B = 200`. The pooled variance estimator becomes:

.. math::

   S^2_p = \frac{(1499)S^2_A + (199)S^2_B}{1697}

Since :math:`n_A` is much larger than :math:`n_B`, the pooled estimator will be heavily weighted 
toward :math:`S^2_A`. 

When :math:`\sigma_A > \sigma_B`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If :math:`\sigma_A > \sigma_B`, then the overall variability will be overestimated, leading to:

1. Overly wide confidence intervals, and 
2. Reduced power—true differences become harder to detect.

When :math:`\sigma_A < \sigma_B`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

On the other hand, if :math:`\sigma_A < \sigma_B`, then the overall variability will be
underestimated. As a result,

2. Confidence interals become too narrow and **fail** to contain the true difference with
   the nominal :math:`100C \%` coverage probability; the true coverage probability will be smaller.
3. The hypothesis tests will have type I error rate greater than :math:`\alpha`; that is,
   the test will make the mistake more often than :math:`100\alpha \%` of the time.

The consequences are especially severe when the true variability is underestimated, as the resulting 
inferences no longer satisfy the theoretical guarantees they are intended to uphold.

.. admonition:: The Unpooled Method is the Default in this Course‼️
   :class: danger 

   Unless directed otherwise, students are expected to solve indpendent two-sample inference problems with unkown
   variances using the **unpooled approach**.

Robustness and Assumption Checking
----------------------------------

The unpooled :math:`t`-procedure is robust to moderate departures from normality, 
with robustness increasing with sample size. Use the following guidelines on the sample sizes and
types of departure from normality. 

A. Sample Size Guidelines
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. list-table::
   :header-rows: 1
   :widths: 25 75

   * - Total Sample Size
     - Normality Requirements
   * - :math:`n_A + n_B < 15`
     - Data must be very close to normal. Requires careful graphical assessment.
   * - :math:`15 \leq n_A + n_B < 40`
     - Can tolerate mild skewness. Strong skewness still problematic.
   * - :math:`n_A + n_B \geq 40`
     - Usually acceptable even with moderate skewness.

When :math:`n_A \approx n_B`, t-procedures are more robust to moderate normality violations.

B. Guidelines on Different Types of Departure from Normality
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In addition to the total sample size, the robustness of :math:`t`-procedures also depends on the specific nature 
of departure from normality. Visualize the data using histograms, QQ plots, and side-by-side boxplots.

- **Outliers** can invalidate procedures regardless of sample size
- **Extreme skewness or heavy tails** may require much larger samples than guidelines suggest
- **Multiple modes** may indicate population heterogeneity

Bringing It All Together
---------------------------

.. admonition:: Key Takeaways 📝
   :class: important

   1. **Unpooled procedures avoid the restrictive equal variance assumption** by estimating the
      standard error with :math:`\widehat{SE} = \sqrt{\frac{S^2_A}{n_A} + \frac{S^2_B}{n_B}}`.
   
   2. The distribution of the pivotal quantity :math:`T'` must be approximated with a :math:`t`-distribution.
      We use **the Welch-Satterthwaite approximation** for its approximate degrees of freedom.
   
   3. **Pooled procedures can result in serious failures** when equal variance assumptions are violated.
   
   4. **Unpooled procedures provide robust inference** that works whether variances are equal or unequal, 
      with only minor efficiency loss when variances are actually equal. This course uses the unpooled procedure
      as the default independent two-sample method.

Exercises
~~~~~~~~~~~~~~~

1. **Degrees of Freedom Calculation**: Two independent samples have the following characteristics:
   
   - Sample A: :math:`n_A = 15`, :math:`s^2_A = 28.4`
   - Sample B: :math:`n_B = 20`, :math:`s^2_B = 45.7`
   
   a) Calculate the Welch-Satterthwaite degrees of freedom.
   b) Compare this to the pooled degrees of freedom :math:`n_A + n_B - 2`.
   c) Explain why the Welch degrees of freedom is typically smaller.

2. **Method Comparison**: A researcher has samples with :math:`n_A = n_B = 25` and approximately equal sample variances. 
   
   a) What are the advantages of using pooled procedures in this scenario?
   b) What are the advantages of using unpooled procedures?
   c) Which approach would you recommend and why?

3. **Robustness Assessment**: You have two samples with total size :math:`n_A + n_B = 30`. One sample shows 
   moderate right skewness and the other has one potential outlier.
   
   a) What additional information would you need to assess the appropriateness of t-procedures?
   b) What graphical tools would you use to evaluate the assumptions?
   c) Under what conditions might you proceed with t-procedures despite these concerns?
