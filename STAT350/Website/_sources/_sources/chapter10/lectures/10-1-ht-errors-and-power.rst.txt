.. _10-1-ht-errors-and-power:

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch10-1">
      <iframe
         id="video-ch10-1"
         title="STAT 350 – Chapter 10.1 Hypothesis Testing for the Mean of a Population and Power Video"
          src="https://www.youtube.com/embed/ZQusNqSNSdY?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>

.. admonition:: Slides 📊
   :class: tip
   
   - `Hypothesis Testing for Single Sample (Part 1) (PPTX) <https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/slides/Chapter%2010%20Hypothesis%20Testing/Hypothesis%20Testing%20for%20Single%20Sample%20Part1_AC.pptx>`_

The Foundation of Hypothesis Testing
======================================================

Hypothesis testing is a key statistical inference framework 
that **assesses whether claims 
about population parameters are reasonable based on data evidence**. 
In this lesson, we establish the basic language of hypothesis testing to prepare
for the formal steps covered in the upcoming lessons.


.. admonition:: Road Map 🧭
   :class: important

   * Learn the building blocks of hypothesis testing.
   * Formulate the **null and alternative hypotheses in the correct format** for a given research question.
   * Understand the logic of constructing a **decision rule**.
   * Recognize the two **types of errors** that can arise in hypothesis testing and understand how they are controlled or
     influenced by different components of the procedure.

The Building Blocks of Hypothesis Testing
-------------------------------------------

A. The Dual Hypothesis Framework
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A **statistical hypothesis** is a claim about one or more population parameters, expressed as a mathematical statement.
The first step in hypothesis testing is to frame the research question as two competing hypotheses:

* **Null hypothesis**, :math:`H_0`: the status quo or a baseline claim, 
  assumed true until there is sufficient evidence to conclude otherwise

* **Alternative hypothesis**, :math:`H_a`: the competing claim to be tested against the null

When testing a claim about the population mean :math:`\mu`,
the hypothesis formulation follows a set of rules summarized by 
:numref:`HT_template0` and the following list.

.. _HT_template0:
.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter10/HT_template.png 
   :figwidth: 70%
   :align: center 
   :alt: hypotheses template

   Template for dual hypothesis

1. In :numref:`HT_template0`, the part in black always stays unchanged.
2. :math:`\mu_0`, called the **null value**, is a point of comparison for :math:`\mu`
   taken from the research context. It is represented with a symbol here, but it takes a **concrete numerical value** in
   applications.
3. :math:`H_0` and :math:`H_a` are complementary—their cases
   must not overlap yet together encompass all possibilities for the parameter.
   :math:`H_0` always includes an equality sign (:math:`=, \leq, \geq`), while
   the inequality in :math:`H_a` is always strict (:math:`\neq, <, >`).

Let us get some practice applying these rules correctly to research questions.

.. admonition:: Example 💡: Writing the Hypotheses Correctly
   :class: note

   For each research scenorio below, write the appropriate set of hypotheses to conduct a hypothesis test.
   Be sure to follow all the rules for hypotheses presentation.

   1. **The census data show that the mean household income in an area is $63K (63 thousand dollars) per year.  
      A market research firm wants to find out whether the mean household income of the shoppers at a mall 
      in this area is HIGHER THAN that of the general population.**

      Let :math:`\mu` denote the true mean household income of the shoppers at this mall. The dual hypothesis is:

      .. math::
         &H_0: \mu \leq 63\\
         &H_a: \mu > 63

      The question raised by the study will always align with the alternative hypothesis.
      Also note that the generalized symbol :math:`\mu_0` in the template (:numref:`HT_template`) is
      replaced with a specific vlaue, 63, from the context.

   2. **Last year, your company’s service technicians took an average of 2.6 hours to respond to 
      calls from customers. Do this year’s data show a DIFFERENT average time?**

      Let :math:`\mu` denote the true mean average response time by service techicians this year.
      The dual hypothesis appropriate for this research question is:

      .. math::
         &H_0: \mu = 2.6\\
         &H_a: \mu \neq 2.6

   3. **The drying time of paint under a specified test condition is known to be normally distributed 
      with mean 75 min and standard deviation 9 min. Chemists have proposed a new additive designed 
      to DECREASE average drying time.  Should the company change to the new additive?**

      Let :math:`\mu` be the true mean drying time of the new paint formula. Then we have:

      .. math::
         &H_0: \mu \geq 75\\
         &H_a: \mu < 75

Three Types of Hypotheses
^^^^^^^^^^^^^^^^^^^^^^^^^^^

From these examples, we see that there are three main ways to formulate
a pair of hypotheses. Focusing on the alternative side, 

* A test with :math:`H_a: \mu > \mu_0` is called an 
  **upper-tailed (right-tailed)** hypothesis test.
* A test with :math:`H_a: \mu < \mu_0` is called a **lower-tailed (left-tailed)** hypothesis test. 
* A test with :math:`H_a: \mu \neq \mu_0` is called a **two-tailed** hypothesis test. 

B. The Significance Level
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Before collecting any data, we must decide how strong the evidence must be to reject the null 
hypothesis. The **significance level**, denoted :math:`\alpha`, is the pre-specified probability 
that represents our **tolerance for the error of rejecting a true null hypothesis**.
A small value, typically less than or equal to :math:`0.1`, is chosen based on 
expert recommendations, legal requirements, or field conventions.
The smaller the :math:`\alpha`, the stronger the evidence must be to reject the null hypothesis.

C. The Test Statistic and the Decision
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Identifying the Goal
^^^^^^^^^^^^^^^^^^^^^^^

For a concrete context, suppose we perform the **upper-tailed hypothesis test** for the true mean
income of shoppers at a mall, taken from the first of the three examples above.

.. math::
   &H_0: \mu \leq 63\\
   &H_a: \mu > 63

Let us also assume that 

1. :math:`X_1, X_2, \cdots, X_n` form an *iid* sample from
   the population :math:`X` with mean :math:`\mu` and variance :math:`\sigma^2`.
2. Either the population :math:`X` is normally distributed, or the sample size :math:`n` is
   sufficiently large for the CLT to hold.
3. The population variance :math:`\sigma^2` is known.

We now need to **develop an objective rule** for rejecting the null hypothesis.
This rule must (1) be applicable in any upper-tailed hypothesis testing scenario 
where the assumptions hold, and (2) satisfy the maximum error tolerance condition given by :math:`\alpha`.

Finding the Decision Rule
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

It is natural to view the sample mean :math:`\bar{X}` as central to the decision, since it is
one of the best indicators of the true location of :math:`\mu`. 
In the simplest terms, if :math:`\bar{X}` 
yields an observed value :math:`\bar{x}` *much larger* than 63 (thousands of dollars), 
we would want to reject the null hypothesis, 
whereas if it is close to or lower than 63,
there would not be enough evidence against it. The key question is, **how large must** :math:`\bar{x}` 
**be to count as sufficient evidence against the null?**

Under the set of assumptions about the distribution of :math:`X` and its sampling conditions, :math:`\bar{X}` (approximately)
follows a normal distribution. In addition, its full distribution can be given by

.. math:: \bar{X} \sim N\left(63, \frac{\sigma^2}{n}\right)

under the null hypothesis. We call this the *null distribution*.

Let us consider rejecting the null hypothesis **only if** :math:`\bar{x}`
**lands above the cutoff that marks an upper area of** :math:`\alpha` 
under the null distribution: 

.. _upper-HT-decision:
.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter10/upper-HT-decision.png 
   :figwidth: 90%
   :align: center 
   :alt: Decision rule for an upper-tailed hypothesis test

   Decision rule for an upper-tailed hypothesis test

(1) **Is this rule objective and universally applicable in other upper-tailed hypothesis tests?** 
    
    Yes. If the same set of assumptions hold, we can make an equivalent
    rule by replacing the values of :math:`\mu_0, \sigma^2`, and :math:`n` appropriately.

(2) **Does this rule limit the false rejection rate to at most** :math:`\alpha` **?**

    Yes. If :math:`H_0` was indeed true, then according to the null
    distribution, :math:`\bar{X}` would generate values above
    the cutoff only :math:`\alpha \cdot 100 \%` of the time. By design, the chance of
    incorrectly rejecting the null hypothesis is limited by how often incorrect answers are generated
    under the null hypothesis.

(3) **What about other potential values under** :math:`H_0` **?**
    
    The null hypothesis :math:`H_0: \mu \leq 63` proposes that :math:`\mu` is 
    anything **less than or equal to** the null value, 63. Is the decision rule also safe
    for candidate values other than 63? The answer is yes.
    When the true mean is strictly less than :math:`\mu_0`, 
    the entire distribution of :math:`\bar{X}` slides to the left and away from the cutoff,
    leaving an upper-tail area smaller than :math:`\alpha`:
    
    .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter10/other-null-cases.png 
      :figwidth: 90%
      :align: center 
      :alt: Other candicate values from the null hypothesis

      Candicate values for :math:`\mu` other than :math:`\mu_0` in the null hypothesis

    Therefore, error-inducing outcomes are generated even less frequently than when the population mean is exactly 63.
    In general, the boundary case of :math:`\mu=\mu_0` addresses the **worst case scenario** in terms of
    false rejection rates. If the rule is safe under the boundary case, then it is safe under all other scenarios
    belonging to the null hypothesis.

    
How to Locate the Cutoff
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The exact location of the cutoff can be computed by viewing it as a :math:`(1-\alpha)\cdot 100`-th 
percentile of the boundary case null distribution. Using the techniques learned in Chapter 6, confirm that the cutoff is 
:math:`z_{\alpha}\frac{\sigma}{\sqrt{n}} + 63` for this example, where :math:`z_\alpha`
is the :math:`z`-critical value used in Chapter 9. In general,

.. math:: \text{cutoff}_{upper} = z_{\alpha}\frac{\sigma}{\sqrt{n}} + \mu_0.

In summary, we **reject the null hypothesis of an upper-tailed hypothesis test** if 
:math:`\bar{x} > z_{\alpha}\frac{\sigma}{\sqrt{n}} + \mu_0` or, by standardizing both sides, if

.. math:: \frac{\bar{x}-\mu_0}{\sigma/\sqrt{n}} > z_{\alpha}.

.. admonition:: What About the Cutoff for a Lower-Tailed Hypothesis Test?
   :class: important

   By making a mirror argument of this section, confirm that
   you would reject the null hypothesis for a **lower-tailed hypothesis test** if
   :math:`\bar{x} < -z_\alpha\frac{\sigma}{\sqrt{n}} + \mu_0 = \text{cutoff}_{lower}`,
   or by standardizing both sides, if 

   .. math:: \frac{\bar{x}-\mu_0}{\sigma/\sqrt{n}} < -z_{\alpha}.
   

The Test Statistic
^^^^^^^^^^^^^^^^^^^^^

A statistic that measures the consistency of the observed data 
with the null hypothesis is called the **test statistic**. For hypothesis tests on a population mean,
:math:`\frac{\bar{X}-\mu_0}{\sigma/\sqrt{n}}` plays this role. 
Its realized value represents **the standardized distance between the
hypothesized true mean** :math:`\mu_0` **and the generated outcome** :math:`\bar{x}`.
It is also used for comparison against a :math:`z`-critical value to draw the final decision.
Since it follows the standard normal distribution under the null hypothesis,
we denote this quantity :math:`Z_{TS}`:

.. math::
   Z_{TS} = \frac{\bar{X}-\mu_0}{\sigma/\sqrt{n}}

and call it the :math:`z`-**test statistic**.

Understanding Type I and Type II Errors
----------------------------------------

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch10-1-1">
      <iframe
         id="video-ch10-1-1"
         title="STAT 350 – Chapter 10.1.1 Type 1, Type 2 Error and Power"
          src="https://www.youtube.com/embed/rc1OOsAohSw?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>

Since the results of hypothesis tests always accompany a degree of uncertainty, it is important to
analyze the likelihood and consequences of the possible errors. There are two types of
errors that can arise in hypothesis testing. 
The error of incorrectly rejecting a true null hypothesis called the **Type I error**, while
the error of failing to reject a false null hypothesis is called the **Type II error**.
The table below summarizes the different combinations of reality and decision.

.. flat-table:: 
   :header-rows: 1

   * - :cspan:`1` Decision 
     - Fail to Reject :math:`H_0`
     - Reject :math:`H_0`
   
   * - :rspan:`1` **Reality**
     - :math:`H_0` is True
     - ✅ Correct 
     - ❌ Type I Error
   
   * - :math:`H_0` is False
     - ❌ Type II Error
     - ✅ Correct

Type I Error: False Positive
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A **Type I error** occurs when a true null hypothesis is rejected.
This error results in a false positive claim of an effect or difference when none actually 
exists. The probability of making a Type I error is denoted :math:`\alpha`. Formally,

.. math::
   \alpha = P(\text{Type I error}) = P(\text{Reject } H_0|H_0 \text{ is true}).

**Examples of Type I errors**

- Concluding that a new drug is effective when it actually has no effect
- Claiming that a manufacturing process has changed when it is operating the same way as before

Type II Error: False Negative
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A **Type II error** occurs when a false null hypothesis is not rejected.
This results in a false negative case where real effect or difference goes undetected. 
The probability of making a Type II error is 

.. math::
   \beta = P(\text{Type II error}) = P(\text{Fail to reject } H_0| H_0 \text{ is false}).

**Examples of Type II errors**

- Failing to detect that a new drug is more effective than placebo
- Failing to recognize that a manufacturing process has deteriorated

Error Trade-offs and Prioritization of the Type I Error
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Type I and Type II errors are inversely related—efforts to reduce one type of error typically 
increase the other. The only way to reduce both error types simultaneously is to increase the sample size, 
collect higher quality data, or improve the measurement process.

When constructing a hypothesis test under limited resources, therefore, we must prioritize one
error over the other. **We typically choose to control** :math:`\alpha`. That is, we 
design the decision-making procedure so that its probability of Type I error reamins below
a pre-specified maximum. We make this choice because falsely claiming change from 
the status quo often carries substantial immediate cost. Such costs can include purchasing new factory equipment, 
setting up a production line and marketing strategy for a new drug, or revising a business contract.

By consequence, we cannot directly control :math:`\beta`. 
Instead, we analyze and try to minimize :math:`\beta` by learning its relationship with
the population distribution, sample size, and the significance level :math:`\alpha`.

.. admonition:: A Legal System Analogy 🧑‍⚖️
   :class: note

   The analogy between hypothesis testing and the American legal system offers useful insight.
   Just as we would rather let a guilty person go free than convict an innocent person, 
   we are generally more concerned about incorrectly rejecting a true null 
   hypothesis than about failing to detect a false one. Further,

   - The **null hypothesis** is like the defendant, presumed innocent until proven guilty.
   - The **alternative hypothesis** is like the prosecutor, trying to establish the defendant's guilt.
   - The **significance level** :math:`\alpha` represents the standard of evidence required for conviction.
   - The **test statistic** summarizes all the evidence presented at trial.
   - The **p-value** (to be discussed in Section 10.2) measures how convincing this evidence is.

Statistical Power: The Ability to Detect True Change
------------------------------------------------------------


.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch10-1-2">
      <iframe
         id="video-ch10-1-2"
         title="STAT 350 – Chapter 10.1.2 Power Calculations"
          src="https://www.youtube.com/embed/pXRyQQt_v_I?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>


Definition
~~~~~~~~~~~~~~

**Statistical power** is the probability that a test will **correctly reject a false null hypothesis**. 
It represents the test's ability to detect an unusual effect when it actually exists. It is also the **complement of
the Type II error probability**, :math:`\beta`.

.. math::

   \text{Power} = P(\text{Reject } H_0 | H_0 \text{ is false}) = 1 - \beta

Power ranges from 0 to 1, with higher values indicating a better ability to detect false 
null hypotheses. A power of 0.80 means that if the null hypothesis is false, the test
correctly rejects it with 80% chance. 

Visualization of :math:`\alpha, \beta`, and Power
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Let us continue with the upper-tailed hypothesis test for the true mean household income of shoppers at a mall.
The dual hypothesis is:

.. math::
   &H_0: \mu \leq 63\\
   &H_a: \mu > 63

where the null value is :math:`\mu_0 = 63`. We agreed to reject the null hypothesis whenever 
:math:`\bar{x}` was "too large", or when

.. math::
   \bar{x} > z_{\alpha}\frac{\sigma}{\sqrt{n}} + 63 = \text{cutoff}_{upper}.

Let us also make the unrealistic assumption that we know the true value of :math:`\mu`; it is
equal to :math:`\mu_a = 65,` making the reality belong to the alternative side. Let us visualize this along with
the resulting locations of :math:`\alpha`, :math:`\beta`, and power:

.. _original-plot:
.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter10/power-analysis0.png 
   :figwidth: 90%
   :align: center 
   :alt: Locations of alpha, beta, and power marked on the null and alternative distributions of X bar

   Plots were generated using :math:`n=35, \mu_0 = 63, \mu_a=65, \sigma=4, \alpha =0.05` in 
   `🔗 Power Simulator <https://treese41528.github.io/STAT350/ShinyApps/Power_Simulator.html>`_ .

The diagram has two normal densities partially overlapping one another. The left curve, centered at :math:`\mu_0 = 63`, represents the
reality assumed by the null hypothesis, while the right curve, centered at :math:`\mu_a=65,` represents the truth. According to our
decision rule, we draw the cutoff where it leaves an upper-tail area of :math:`\alpha` (red) **under the null distribution** and
reject the null hypothesis whenever we see the sample mean land above it.

What happens if, in the meantime, the sample means are actually being generated from the **right (alternative) curve**? 
With probabiliy :math:`\beta` (purple), it will generate an observed sample mean
that will fail to lead to a rejection of :math:`H_0` (Type II error). All other outcomes
will lead to a correct rejection, with the probability represented by the green area (power).

Let us observe how the sizes of these three regions are influenced by different components of the experiment.


What Influcences Power?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Significance Level, :math:`\alpha`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter10/power-analysis-alpha.png 
   :figwidth: 50%
   :align: center 
   :alt: alpha, beta, and power as alpha changes

   :math:`\alpha=0.01, 0.05, 0.1` from top to bottom

The central plot with the blue outline is the original plot, identical to
:numref:`original-plot`.
A **smaller** :math:`\alpha` **pushes the cutoff up in an upper-tailed hypothesis test**, since it calls
for more caution against Type I error and requires a stronger evidence (larger :math:`\bar{x}`) 
for rejection of the null hypothesis.
In response, the probability of Type II error increases (purple) and the power decreases (green).

True Mean, :math:`\mu_a`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter10/power-analysis-mu.png 
   :figwidth: 50%
   :align: center 
   :alt: alpha, beta, and power as the alternative truth mu_a changes

   :math:`\mu_a=64,65,66` from top to bottom

If the hypothesized :math:`\mu_0` and the true effect :math:`\mu_a` are close to each other,
it is naturally harder to separate the two cases. Even though :math:`\alpha` stays constant
(because we explicitly control it), the power decreases and the Type II error probability goes up
as the distance between :math:`\mu_0` and :math:`\mu_a` narrows.

Population Standard Deviation, :math:`\sigma`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter10/power-analysis-sd.png 
   :figwidth: 50%
   :align: center 
   :alt: alpha, beta, and power as the population sd changes

   :math:`\sigma=2.5, 4, 5` from top to bottom

Recall that :math:`\bar{X}` has the standard deviation :math:`\sigma/\sqrt{n}`.
When :math:`\sigma` decreases while :math:`\mu_0` and :math:`\mu_a` stay constant,
the two densities become narrower around their respective means, creating a stronger separation between the two cases.
This leads to a higher power and smaller Type II error probability.

Sample Size, :math:`n`
^^^^^^^^^^^^^^^^^^^^^^^^^^

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter10/power-analysis-n.png 
   :figwidth: 50%
   :align: center 
   :alt: alpha, beta, and power as the sample size changes

   :math:`n=13, 35, 70` from top to bottom

The sample size :math:`n` also affects the spread of the distribution of :math:`\bar{X}`, but in the opposite direction 
of :math:`\sigma`. As :math:`n` decreases, :math:`\sigma/\sqrt{n}` increases, making the densities wider. 
Larger overlap between the distributions leads to decreased power and higher Type II error probability.

.. _power-simulator:
.. admonition:: Power Analysis Simulator 🎮
   :class: interactive
 
   Explore how :math:`\alpha, \beta`, and statistical power
   relate to each other, and reproduce the images used in this section using:
  
   `🔗 Interactive Demo <https://treese41528.github.io/STAT350/ShinyApps/Power_Simulator.html>`_ |
   `📄 R Code <https://treese41528.github.io/STAT350/ShinyApps/Power_Simulator_Shiny.R>`_

Prospective Power Analysis
-----------------------------

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch10-1-3">
      <iframe
         id="video-ch10-1-3"
         title="STAT 350 – Chapter 10.1.3 Sample Size Calculations"
          src="https://www.youtube.com/embed/umlrWPs7qlA?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>


From the previous discussion, we find that the only realistic way to control statistical power is through the sample size, :math:`n`.
Before conducting a study, researchers perform **prospective power analysis** to determine the 
sample size needed to ensure adequate power in their tests.

We continue with the upper-tailed hypothesis test on the true mean household income of shoppers at a mall:

.. math::
   &H_0: \mu \leq 63\\
   &H_a: \mu > 63

Suppose that the researchers expect the test to **detect an increase of 2K in the household income effectively**. Specifically,
such jump should be detected **with a probability at least 0.8**. In other words, we want:
  
.. math:: \text{Power} = 1-\beta \geq 0.8

when :math:`\mu = 63 + 2 = 65`. The magnitude of change to be detected, 2 in this case, is also called
the **effect size**.

Additionally, we still assume:

1. :math:`X_1, X_2, \cdots, X_n` forms an *iid* sample from
   a population :math:`X` with mean :math:`\mu` and variance :math:`\sigma^2`.
2. Either the population :math:`X` is normally distributed, or the sample size :math:`n` is
   sufficiently large for the CLT to hold.
3. The population variance :math:`\sigma^2` is known.

Step 1: Mathematically Clarify the Goal
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In general, :math:`\text{Power} = P(\text{Reject} H_0|H_0 \text{ is false})`.

We replace the general definition with the specific conditions given by our problem.
The event of "rejecting :math:`H_0`" is equivalent to the event :math:`\{\bar{X} > \text{cutoff}_{upper}\},`
and the event that :math:`H_0` is false should now reflect the desired effect size. 
Therefore, our goal is to find :math:`n` satisfying: 

.. math::
   \text{Power} = P\left(\bar{X} > \text{cutoff}_{upper} \Bigg| \mu=65\right) \geq 0.8

or, equivalently,

.. math::
   \beta = P\left(\bar{X} \leq \text{cutoff}_{upper} \Bigg| \mu=65\right) < 0.2.

Denote the value :math:`0.2` by :math:`\beta_{max}`, since we do not allow :math:`\beta`
to be larger than :math:`0.2`.

Step 2: Simplify and Calculate
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Let us break down the latter form of our mathematical goal. 

* From the conditional information, we know that :math:`\bar{X}` is assumed to follow the 
  distribution :math:`N(65, \sigma/\sqrt{n})`. 
* Since the goal is written with a strict inequality, the cutoff must be a value
  strictly less than the 20th (:math:`\beta_{max}\cdot 100`-th) percentile of 
  :math:`N(65, \sigma/\sqrt{n})`. Mathematically,
 
   .. math::
      \text{cutoff}_{upper} < -z_{\beta_{max}}\frac{\sigma}{\sqrt{n}} + 65

   where :math:`z_{\beta_{max}}` is a :math:`z`-critical value computed for the upper-tail area :math:`\beta_{max}`. 

* Replacing the :math:`\text{cutoff}_{upper}` with its complete formula,

  .. math::
     z_\alpha\frac{\sigma}{\sqrt{n}} + 63 < -z_{\beta_{max}}\frac{\sigma}{\sqrt{n}} + 65.

  Isolate :math:`n`:

  .. math::

     n > \left(\frac{(z_{\alpha} + z_{\beta_{max}}) \sigma}{65 - 63}\right)^2.

Since :math:`n` must be an integer, we take the **smallest integer above this lower bound**.

Summary
~~~~~~~~~

In an **upper-tailed hypothesis test**, 
the minimum sample size for a desired power lower bound :math:`1-\beta_{max}` and 
an **effect size** :math:`|\mu_a-\mu_0|` is the **smallest integer** :math:`n` satisfying:

.. math::

   n > \left(\frac{(z_{\alpha} + z_{\beta_{\max}}) \sigma}{|\mu_a - \mu_0|}\right)^2.

.. admonition:: Prospective Power Analysis for Lower-tailed Hypothesis Tests
   :class: important

   By walking through a mirror argument, confirm that the minimum sample size :math:`n` for a desired
   power lower bound :math:`1-\beta_{max}` and an effect size :math:`|\mu_a-\mu_0|` in
   a **lower-tailed hypothesis test** is determined by the **same formula** as the upper-tailed case.

.. admonition:: Example 💡: Compute Power for SAT Scores
   :class: note

   A teacher at STAT High School believes that their students score **higher** on the SAT 
   than the 2013 national average of 1497. Assume the true standard deviation of SAT scores 
   from this school is 200.
   
   **Q1**: The teacher wants to construct a hypothesis test at 0.01 significance level
   that can detect a 20-point increase in the true mean effectively. 
   If the current sample size is 300, what is the power of this test?

   **Step 1: Identify the Components**

   - The dual hypothesis is:
     
     .. math:: 
        &H_0: \mu \leq 1497\\
        &H_a: \mu > 1497
        
   - :math:`\alpha = 0.01` (:math:`z_{0.01} = 2.326348`)
     
     .. code-block:: r
        
        z_alpha <- qnorm(0.01, lower.tail = FALSE)

   - Effect size: :math:`20` points. This makes :math:`\mu_a = \mu_0 + 20 = 1497 = 1517`. 
   - Population standard deviation is :math:`\sigma = 200` points
   - Current sample size: :math:`n=300`

   **Step 2: Find the Cutoff**

   .. math::

      \text{cutoff}_{upper} = 1497 + \frac{200}{\sqrt{300}}(2.326348) = 1497 + 26.862 = 1523.862

   **Step 3: Calculate Power**

   .. math::

      \text{Power} = P(\bar{X} > 1523.862 | \mu = 1517)
   
   Using the conditional information, we compute the probability assuming
   that :math:`\bar{X} \sim N(1517, \sigma/\sqrt{n})`.
   
   .. math::
      \text{Power} &= P\left(\frac{\bar{X}-1517}{\sigma/\sqrt{n}} > \frac{1523.862-1517}{\sigma/\sqrt{n}}\right)\\
      &= P(Z > 0.5943) = 0.2762

   **Result**: The power is only 27.62%. This test is **not sufficiently sensitive** 
   to reliably detect a 20-point improvement.



.. admonition:: Example 💡: Compute Minimum Sample Size for SAT Scores
   :class: note

   **Q2**: Continuing with the SAT scores problem, what is the minimum sample size required for the test to detect a 20-point increase with
   at least 90% chance?

   **Step 1: Identify the Components**

   - :math:`\text{Power} \geq 0.90` is required, so :math:`\beta = 1 - \text{Power} < 0.10 = \beta_{max}`.  
   - :math:`z_{\beta_{max}} = 1.282`

     .. code-block:: r 

        z_betamax <- qnorm(0.1, lower.tail=FALSE)

   **Step 2: Apply the Formula**

   .. math::

      n &> \left[\frac{(z_{\alpha} + z_{\beta_{max}}) \sigma}{|\mu_a - \mu_0|}\right]^2 
         = \left[\frac{(2.326348 + 1.281552)(200)}{1517-1497}\right]^2\\
        &= \left[\frac{(3.6079)(200)}{20}\right]^2 = (36.079)^2 = 1301.69

   **Result**: We would need at least :math:`n = 1302` students to achieve 90% power—much 
   larger than the available sample of 300.




.. admonition:: Example 💡: Average Recovery Time
   :class: note

   A pharmaceutical company wants to test whether a new drug **reduces** average recovery time from a common illness. Historical data shows the standard recovery time is :math:`\mu_0 = 7` days with :math:`\sigma = 2` days. The company wants to detect a reduction to :math:`\mu_a = 6` days (a 1-day improvement) with 90% power at :math:`\alpha = 0.05` significance.

   **Step 1: Identify the Components**

   - The hypotheses

     .. math:: 
        &H_0: \mu \geq 7\\
        &H_a: \mu < 7

   - The significance level: :math:`\alpha = 0.05` :math:`(z_{\alpha} = 1.645)`

     .. code-block:: r 

        z_alpha <- qnorm(0.05, lower.tail=FALSE)

   - :math:`\text{Power} \geq 0.90` is required, so :math:`\beta = 1 - \text{Power} < 0.10 = \beta_{max}`.  
     :math:`(z_{\beta_{max}} = 1.282)`

     .. code-block:: r 

        z_betamax <- qnorm(0.1, lower.tail=FALSE)

   - Effect size: :math:`|\mu_a - \mu_0| = |6 - 7| = 1` day
   - Population standard deviation: :math:`\sigma = 2` days

   **Step 2: Calculate Required Sample Size**

   .. math::

      n &> \left(\frac{(1.645 + 1.282)(2)}{|6 - 7|}\right)^2 \\
      &= \left(\frac{(2.927)(2)}{1}\right)^2 = (5.854)^2 \approx 34.3

   The company needs at least :math:`n = 35` patients to achieve statistical power of at least 90%.

..
   Interactive Power Analysis Visualization
   ----------------------------------------

   To better understand how the various components of hypothesis testing interact, you can explore an interactive simulation that visualizes the relationship between Type I error, Type II error, power, and the factors that affect them.

   This interactive tool allows you to:

   - **Adjust the null and alternative means** to see how effect size affects power
   - **Change the significance level (:math:`\alpha`)** to observe the trade-off between Type I error and power
   - **Modify the sample size** to see its dramatic effect on power
   - **Alter the population standard deviation** to understand how variability affects detectability
   - **Switch between one-tailed and two-tailed tests** to compare their power characteristics



   .. raw:: html

      <div class="embed-container">
      <iframe
         src="https://treese5.shinyapps.io/Power_Sim/"
         title="Power Analysis Interactive Visualization"
         allowfullscreen>
      </iframe>
      </div>


   **Key Observations to Make:**

   1. **Effect Size Impact**: Move the alternative mean closer to or farther from the null mean. Notice how larger effect sizes (greater separation) dramatically increase power.

   2. **Sample Size Power**: Increase the sample size and watch both distributions become narrower (smaller standard error), making it easier to distinguish between null and alternative hypotheses.

   3. **Alpha Trade-off**: Increase :math:`\alpha` and see how the red region (Type I error) grows, but the green region (power) also increases while the purple region (Type II error) shrinks.

   4. **Standard Deviation Effect**: Increase the population standard deviation and observe how both curves become wider, making it harder to detect differences and reducing power.

..
   Understanding the Code Behind the Visualization
   -----------------------------------------------

   The interactive app is built using R Shiny and demonstrates several key statistical computing concepts. Here's how the power calculation works programmatically:

   **Core Power Calculation Logic**

   .. code-block:: r

      # Calculate standard error of the sampling distribution
      std_error <- std_dev / sqrt(sample_size)
      
      # Find critical value based on hypothesis direction
      z_critical <- if (hypothesis == "greater") {
      qnorm(1 - alpha)  # Upper tail critical value
      } else if (hypothesis == "less") {
      qnorm(alpha)      # Lower tail critical value  
      } else {
      qnorm(1 - alpha/2) # Two-tailed critical value
      }
      
      # Calculate the cutoff value in original units
      cutoff <- null_mean + z_critical * std_error
      
      # Calculate power using the alternative distribution
      power <- if (hypothesis == "greater") {
      1 - pnorm(cutoff, alt_mean, std_error)
      } else if (hypothesis == "less") {
      pnorm(cutoff, alt_mean, std_error)
      } else {
      # Two-tailed case (more complex)
      upper_cutoff <- null_mean + z_critical * std_error
      lower_cutoff <- null_mean - z_critical * std_error
      (1 - pnorm(upper_cutoff, alt_mean, std_error)) + 
      pnorm(lower_cutoff, alt_mean, std_error)
      }

   **Visualization Strategy**

   The app creates two normal distributions:

   - **Null distribution**: Centered at :math:`\mu_0` with standard error :math:`\sigma/\sqrt{n}`
   - **Alternative distribution**: Centered at :math:`\mu_a` with the same standard error

   The colored regions represent:

   - **Red areas**: Type I error (:math:`\alpha`) - rejection region under null distribution
   - **Purple areas**: Type II error (:math:`\beta`) - non-rejection region under alternative distribution  
   - **Green areas**: Power (1-:math:`\beta`) - rejection region under alternative distribution

   **Why This Visualization Matters**

   This interactive approach helps students understand several crucial concepts that are often difficult to grasp from static diagrams alone:

   1. **Dynamic relationships**: See how changing one parameter affects all others simultaneously
   2. **Magnitude of effects**: Understand whether a change in sample size from 30 to 40 matters as much as a change from 30 to 100
   3. **Practical constraints**: Recognize why achieving very high power often requires impractically large sample sizes
   4. **Design decisions**: Appreciate the trade-offs researchers face when planning studies

   The app reinforces that power analysis isn't just a mathematical exercise—it's a practical tool for making informed decisions about study design and resource allocation.

Bringing It All Together
------------------------------

.. admonition:: Key Takeaways 📝
   :class: important

   1. **Hypothesis testing provides a framework** for evaluating specific claims about population 
      parameters using sample evidence. It consists of formally presenting the **null and alternative hypotheses**,
      determining the **significance level**, computing a **test statistic**, determining its strength,
      and drawing a **decision**.
   
   2. **Type I error** (false positive) occurs when a true null hypothesis is rejected. Its probability,
      denoted :math:`\alpha`, is the significance level of the test.
   
   3. **Type II error** (false negative) occurs whe a false null hypothesis is not rejected.
      It occurs with probability :math:`\beta`.
   
   4. **Statistical power** (1-:math:`\beta`) measures a test's ability to detect false null hypotheses.
      It depends on the sample size, significance level, and population standard deviation.
   
Exercises
~~~~~~~~~~~~~~

1. **Power Calculation**: A researcher wants to test whether a new teaching method improves average 
   test scores. Historical data shows mean scores of 75 with standard deviation 10. The researcher 
   wants 80% power to detect an improvement to 78 points at :math:`\alpha = 0.05`. Calculate the 
   required sample size.

2. **Error Trade-offs**: If a researcher reduces the significance level from :math:`\alpha = 0.05` 
   to :math:`\alpha = 0.01` while keeping everything else constant, what happens to:

   a) The probability of Type I error?
   b) The probability of Type II error?
   c) The power of the test?
