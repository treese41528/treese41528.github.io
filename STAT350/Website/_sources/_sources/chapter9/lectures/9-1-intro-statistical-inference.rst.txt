.. _9-1-intro-statistical-inference:

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch7-1">
      <iframe
         id="video-ch7-1"
         title="STAT 350 – Chapter 7.1 Statistics and Sampling Distributions Video"
         src="https://www.youtube.com/embed/P3Nyg84h0A8?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>

.. admonition:: Slides 📊
   :class: tip

   `Download Chapter 9 slides (PPTX) <https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/slides/
   Chapter%209%20Confidence%20Intervals/L19-21%20Confidence%20Intervals%20for%20Single%20Sample_AC.pptx>`_
   
Introduction to Statistical Inference
===============================================================

After developing the foundational tools of probability theory, exploring random variables, and 
understanding sampling distributions, we have finally arrived at the core of statistical practice: 
**statistical inference**. This exciting chapter marks our transition from describing uncertainty to 
making decisions under uncertainty—the essence of statistics as a discipline.

.. admonition:: Road Map 🧭
   :class: important
   
   * Recall the relationship between a **population parameter** and its **estimator**. Estimator yields
     **estimates** that vary from sample to sample, and their behavior is described by the **sampling distribution**.
   * Select an optimal estimator out of many candidates by evaluating their distributional properties.
     Know the definitions of **bias and variance**, and use them appropriately as selection criteria. 
     Be able to compute bias for some simple estimators.
   * Know the definition of **Minimum Variance Unbiased Estimator (MVUE)**.

From Population Parameters to Estimators
-------------------------------------------------

In statistical research, we aim to understand certain characteristics of a population that are fixed but 
unknown to us. These characteristics, called **parameters**, include:

- The population mean (:math:`\mu`),
- The population variance (:math:`\sigma^2`), and
- Other quantities describing the population's distribution.

The fundamental challenge we face is that examining every member of a population is typically 
impractical or impossible, even though doing so would be required to determine these characteristics with certainty.
Instead, we rely on a representative sample to make inferences about the popoulation and
specify *how uncertain* we are about the result.

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter9/parameter-estimator.png
   :alt: Diagram showing relationship between population parameters and sample estimates
   :figwidth: 75%
   :align: center

   Relationship of Parameters, estimators, and estimates

Point Estimators and Estimates
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

An **estimator** is a **random variable** which contains instructions on how to use sample data to calculate 
an estimate of a population parameter. When an estimator is designed to yield a
single numerical value as an outcome, it is called a **point estimator**.
The single-valued outcome is called a **point estimate**. 

.. admonition:: Example💡: :math:`\bar{X}` is a Point Estimator of :math:`\mu`
   :class: note

   One possible **point estimator** of the population mean :math:`\mu` is the sample mean :math:`\bar{X}`. 
   Its definition contains instructions on the calculation procedure—adding all 
   observed values and dividing by the sample size. 
   A **point estimate** :math:`\bar{x}` is obtained as a single concrete numerical value 
   (e.g., :math:`\bar{x} = 42.7`) by applying these instructions to an observed sample.



A Parameter Has Many Point Estimators
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

There are many different ways to guess a parameter value using data. For example,
we can choose to estimate the population mean :math:`\mu` with

* The sample mean :math:`\bar{X}` (the typical choice),
* The sample median :math:`\tilde{X}` (for symmetric distributions whose true mean and true median are
  equal, this is reasonable),
* The mean of all data points except :math:`m` most extreme values, etc.

It is easy to generate many *reasonable* candidates. The key question is, then: 
**What objective criteria can we use to determine which estimator is better than others?**

Recall from Chapter 7 that each estimator has its own distribution, called
the **sampling distribution**. To answer the question, we focus on two key properties 
of the sampling distribution: **bias and variance**. 
For the remainder of this section, we use :math:`\theta` (Greek letter "theta") to denote a population parameter,
and :math:`\hat{\theta}` ("theta hat") for an estimator of :math:`\theta`.

Evaluating Estimators
-----------------------------------------

Bias: Does the Estimator Target the Right Value?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The **bias** of an estimator measures whether it systematically overestimates or underestimates 
the parameter of interest. It is mathematically defined as

.. math::

   \text{Bias}(\hat{\theta}) = E[\hat{\theta}] - \theta.

An estimator :math:`\hat{\theta}` is **unbiased** if :math:`\text{Bias}(\hat{\theta})=0`, or equivalently, 
if its expected value equals the parameter it aims to estimate:

.. math::

   \mathbb{E}[\hat{\theta}] = \theta.

.. admonition:: Example 💡: Is the Sample Mean Unbiased?
   :class: note

   We know that :math:`\mathbb{E}[\bar{X}] = \mu`. So the sample mean :math:`\bar{X}` is an **unbiased
   estimator** of :math:`\mu`.

Variance: How Precise is the Estimator?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The **variance** of an estimator quantifies the spread of its sampling distribution—essentially how much the estimator fluctuates 
from sample to sample. Lower variance indicates greater precision and reliability.

**Minimum-Variance Unbiased Estimator (MVUE)**

When choosing froom a set of unbiased estimators, we typically prefer the one with the smaller variance, 
as this reduces the expected "distance" between the estimate and the true parameter. 
An ideal estimator is called **minimum-variance unbiased estimator (MVUE)**—that is,
an estimator with the **smallest possible variance among all unbiased estimators** for a given parameter. 

Whether an estimator is MVUE depends on the population distribution, and proving this property generally requires
advanced theoretical tools. Nonetheless, we will encounter several examples as we explore the properties 
of key estimators.

Bias-Variance Tradeoff
~~~~~~~~~~~~~~~~~~~~~~~~~~

An unbiased estimator is not always a better choice than a biased estimator. If an estimator
is slightly biased but has a significantly lower variance than its unbiased competitor, 
then there may be situations where
the former is more practical. In fact, bias and variance often exhibit a trade-off relationship;
reducing bias in an estimator may increase its variance, and vice versa. 
We should always take the **degree** of both bias and variance into consideration when
choosing an estimator. See Figure :numref:`bias-variance` for a visual illustratiion:

.. _bias-variance:
.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter9/bias-variance-tradeoff.png
   :alt: Comparison of biased versus unbiased estimators
   :align: center
   :figwidth: 90%
   
   Comparison of biased and unbiabsed estimators

Important Estimators and Their Properties
---------------------------------------------

To solidify the concepts of bias and variance in estimators, let's examine several common estimators 
and their properties. In all cases below, suppose :math:`X_1, X_2, \cdots, X_n` form an *iid* sample from
the same population.

A. Sample Mean for Population Mean
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The sample mean :math:`\bar{X} = \frac{1}{n}\sum_{i=1}^{n}X_i`
serves as an estimator for the population mean :math:`\mu`. We know that :math:`\mathbb{E}[\bar{X}] = \mu`, 
which makes it an unbiased estimator of :math:`\mu`.

When sampling from a normal distribution, the sample mean is also a minimum-variance unbiased estimator (MVUE).

B. Sample Proportion for True Probability
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Suppose identical trials are performed :math:`n` times, and whether an event :math:`A` occurs or not in each trial
is recorded using Bernoulli random variables of the following form:

.. math::

   I_i(A) =
   \begin{cases}
   1, & \text{with probability } P(A) \\
   0, & \text{with probability } 1 - P(A)
   \end{cases}

for :math:`i=1,2, \cdots, n`.

Define :math:`\hat{p} = \frac{1}{n}\sum_{i=1}^n I_i(A)`. Then :math:`\hat{p}` is an unbiased
estimator for :math:`P(A)` because

.. math::
   E[\hat{p}] &= E\left[\frac{1}{n}\sum_{i=1}^n I_i(A)\right] = \frac{1}{n}\sum_{i=1}^n E[I_i(A)]\\
   &=\frac{1}{n}\sum_{i=1}^n (1 \cdot P(A) + 0 \cdot (1-P(A))) \\
   &= \frac{n}{n}P(A) = P(A).

This result can be used to define an unbiased estimator for an entire probability distribution.

(a) Estimating a PMF
^^^^^^^^^^^^^^^^^^^^^^

   Suppose :math:`X_1, X_2, \cdots, X_n` form an *iid* sample of a discrete population :math:`X`.
   For each value :math:`x \in \text{supp}(X)`, define:

   .. math::

      \hat{p}_X(x) = \frac{1}{n}\sum_{i=1}^{n}I(X_i = x),

   where :math:`I(X_i = x)=1` if the :math:`i`-th sample point equals :math:`x`, and :math:`0` otherwise.
   Then, by the same logic as the general case, :math:`\hat{p}_X(x)` is an unbiased estimator
   of :math:`p_X(x)` for each :math:`x \in \text{supp}(X)`:

   .. math::
      E[\hat{p}_X(x)] = p_X(x) = P(X = x).

(b) Estimating a CDF
^^^^^^^^^^^^^^^^^^^^^

   For continuous random variables, we can estimate the cumulative distribution function (CDF) at 
   any :math:`x \in \text{supp}(X)` using:

   .. math::

      \hat{F}_X(x) = \frac{1}{n}\sum_{i=1}^{n}I(X_i \leq x).

   :math:`\hat{F}_X(x)` represents the proportion of observations less than or equal to :math:`x`. 
   As another special application of the general case, this is an unbiased 
   estimator of the true CDF :math:`F_X(x)`. That is,

   .. math::

      E[\hat{F}_X(x)] = P(X \leq x) = F_X(x).

C. Sample Variance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

..
   If we **knew the population mean** :math:`\mu`, which we typically don't, we could estimate the population 
   variance as:



   This is an unbiased estimator: :math:`E[\hat{\sigma}^2] = \sigma^2` (show as an exercise).


The sample variance:

.. math::

   S^2 = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X})^2

is an unbiased estimator of :math:`\sigma^2`. To see how, let us compute the expected value.

1. Add and subtract :math:`\mu` inside each squared term:
   :math:`(X_i - \bar{X})^2 = ((X_i - \mu) - (\bar{X} - \mu))^2`.

2. Using Step 1,

   .. math::
      E[S^2]=&E\left[\frac{1}{n-1}\sum_{i=1}^n ((X_i - \mu) - (\bar{X} - \mu))^2\right]\\
      =&\frac{1}{n-1}\sum_{i=1}^nE[((X_i - \mu) - (\bar{X} - \mu))^2]\\
      =&\frac{1}{n-1}\sum_{i=1}^n E[(X_i - \mu)^2 -2(X_i - \mu)(\bar{X} - \mu) + (\bar{X} - \mu)^2]\\
      =&\frac{1}{n-1}\sum_{i=1}^n E[(X_i - \mu)^2] -2E[(X_i - \mu)(\bar{X} - \mu)] + E[(\bar{X} - \mu)^2]\\

3. The first and third expecations are respectively the variances of :math:`X_i` and :math:`\bar{X}`
   by definition.

   .. math::
      &E[(X_i - \mu)^2] = Var(X_i) = \sigma^2\\
      &E[(\bar{X} - \mu)^2] = Var(\bar{X}) = \frac{\sigma^2}{n}

4. The expectation :math:`E[(X_i - \mu)(\bar{X} - \mu)]` can be simplified to 
   
   .. math:: 
      &E[(X_i - \mu)\cdot \frac{1}{n}\sum_{j}(X_j - \mu)]= \frac{1}{n} \sum_{j=1}^n E[(X_i - \mu)(X_j - \mu)]\\
      &= \frac{1}{n}E[(X_i - \mu)(X_i - \mu)] = \frac{1}{n}E[(X_i -\mu)^2] = \frac{\sigma^2}{n}

   All terms involving indices :math:`j\neq i` disappear in the final steps since
   :math:`Cov(X_i, X_j) = E[(X_i-\mu)(X_j - \mu)] =0` due to their independnce.
   
5. Substituting the results back to the final line of Step 2, 
   we can verify that the sum simplifies to :math:`(n-1)\sigma^2`. Therefore,
   
   .. math::
      E[S^2] = \frac{1}{n-1} (n-1)\sigma^2 = \sigma^2.

This shows why we devide by :math:`n-1` instead of :math:`n` when computing a sample variance; 
this is key to making the estimator unbiased.

When sampling from normal populations, the sample variance :math:`S^2` is also the MVUE for the 
population variance :math:`\sigma^2`. 

.. admonition:: Additional Exercise 💡: Sample Variance When :math:`\mu` is Known
   :class: note

   In an unrealistic situation where :math:`\sigma^2` is not known but :math:`\mu` is,
   we must incorporate the known information into our variance estimation. Show that 

   .. math::

      \hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^{n}(X_i - \mu)^2
   
   is an unbiased estimator of :math:`\sigma^2`.

.. admonition:: The Biased Case of Sample Standard Deviation
   :class: important

   While the sample variance :math:`S^2` is an unbiased estimator of :math:`\sigma^2`, 
   the sample standard deviation :math:`S = \sqrt{S^2}` is a **biased estimator** of the 
   population standard deviation :math:`\sigma`.

   Its biasedness can be shown using a concept called *Jensen's inequality* and the fact that the square root 
   is a concave function. The details are beyond the scope of this course, but you are encouraged to
   read about the topic independently.

   We still use :math:`S` as our estimator for :math:`\sigma` because the formula is 
   straightforward and intuitive, while the bias is typically small, especially for larger sample sizes.


Brining It All Together
---------------------------

In this chapter, we've transitioned from probability theory to statistical inference by exploring the 
properties of point estimators.

.. admonition:: Key Takeaway 📝
   :class: important

   1. **Estimators** yield **estimates** intended to approximate a fixed but unknown **population parameter**.
      Estimates vary from sample to sample according to their **sampling distribution**.

   2. The two most important distributional characteristics of an estimator are **bias and variance**.
      Unbiased estimators target the correct parameter on average, while low-variance 
      estimators provide more consistent results across samples.

   3. **Minimum Variance Unbiased Estimators (MVUEs)** have the smallest variance among all unbiased estimators of
      a target parameter.

   4. We can show that :math:`\bar{X}` are :math:`S^2` are unbiased estimators of their respective targets,
      :math:`\mu` and :math:`\sigma^2`. When the population is normal, they are also the MVUEs.
   
