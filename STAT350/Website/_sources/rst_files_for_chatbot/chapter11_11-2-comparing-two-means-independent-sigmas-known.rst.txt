.. _11-2-comparing-two-means-independent-sigmas-known:



.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch11-2">
      <iframe
         id="video-ch11-2"
         title="STAT 350 â€“ Chapter 11.2 Comparing Two Population Means Using Independent Samples Video"
         src="https://www.youtube.com/embed/OKJxoLTK9GY?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>

.. admonition:: Slides ðŸ“Š
   :class: tip

   `Download Chapter 11 slides (PPTX) <https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/
   slides/Chapter%2011%20Two%20Sample%20Procedures/CI%20and%20HT%20for%20Two%20Samples%20or%20Treatments_AC.pptx>`_


Independent Two-Sample Analysis When Population Variances Are Known
=============================================================================================

We now develop the mathematical foundation for comparing the means of two **independent** populations,
assuming that the population variances are known. The simplifying assumption will be lifted
in later lessons when we discuss more realistic scenarios.

.. admonition:: Road Map ðŸ§­
   :class: important

   * List the **three key assumptions** required for the construction of independent two-sample
     inference methods.
   * Construct **hypothesis tests and confidence regions** for the difference in the means
     of two independent populations. Identify the **common underlying principles with one-sample inference**.

The Assumptions
------------------------------------------------------

The validity of independent two-sample procedures rests on three fundamental assumptions 
that extend the single-sample framework to comparisons. These assumptions must 
be carefully verified before applying the methods.
 
**Assumption 1: SRS from Each Population**

   The random variables :math:`X_{A1}, X_{A2}, \ldots, X_{An_A}` form an 
   independent and identically distributed (*iid*) sample from Population A. 
   Similarly, the random variables :math:`X_{B1}, X_{B2}, \ldots, X_{Bn_B}`
   constitute an *iid* sample from Population B.

**Assumption 2: Independence Between Populations**

   The observations from one population are independent of those from the other population. 
   Formally, :math:`X_{Ai}` is independent of :math:`X_{Bj}` for all possible pairs of indices 
   :math:`i \in \{1, 2, \ldots, n_A\}` and :math:`j \in \{1, 2, \ldots, n_B\}`.

**Assumption 3: Normality of Sampling Distributions**

   For each population, either the population distribution is normal, or 
   the sample size is large enough for the CLT to hold. 

The Parameter of Interest and Its Point Estimator
----------------------------------------------------

The Target Parameter
~~~~~~~~~~~~~~~~~~~~~

Recall that our primary interest lies not in the individual population means 
:math:`\mu_A` and :math:`\mu_B`, but rather in their difference. Our parameter of interest is:

.. math::

   \theta = \mu_A - \mu_B

We conceptualize this difference as a **single parameter** that captures the essence of the 
comparison we wish to make. Its sign and magnitude indicate the direction 
and size of any systematic difference between the populations.

The Point Estimator
~~~~~~~~~~~~~~~~~~~~~~~

Since we know that :math:`\bar{X}_A` and :math:`\bar{X}_B` are unbiased estimators of
their respective population means :math:`\mu_A` and :math:`\mu_B`, the natural point estimator 
for :math:`\theta` is:

.. math::

   \hat{\theta} = \bar{X}_A - \bar{X}_B.

Theoretical Properties of the Point Estimator
---------------------------------------------------

As a result of Assumptions 1 and 3, the sampling distributions of
the two sample means are:

.. math::

   &\bar{X}_A \sim N\left(\mu_A, \frac{\sigma_A}{\sqrt{n_A}}\right)\\
   &\bar{X}_B \sim N\left(\mu_B, \frac{\sigma_B}{\sqrt{n_B}}\right)

Furthermore, the two random variables are **independent** since their 
building blocks are independent according to Assumption 2. We establish the theoretical properties 
of the difference estimator :math:`\bar{X}_A - \bar{X}_B`
starting from this baseline.

Unbiasedness
~~~~~~~~~~~~~~~

The difference in sample means is an **unbiased estimator** of the difference in population means. 
To establish this formally:

.. math::

   E[\bar{X}_A - \bar{X}_B] = E[\bar{X}_A] - E[\bar{X}_B] = \mu_A - \mu_B.

Therefore, the bias of the estimator is:

.. math::

   \text{Bias}[\bar{X}_A - \bar{X}_B] = E[\bar{X}_A - \bar{X}_B] - (\mu_A - \mu_B) = 0.

Variance of the Estimator
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The variance of :math:`\bar{X}_A - \bar{X}_B` depends critically on the independence assumption 
between populations. Recall that for two independent random variables, the variance of their difference equals 
the sum of their individual variances:

.. math::

   \text{Var}[\bar{X}_A - \bar{X}_B] = \text{Var}[\bar{X}_A] + \text{Var}[\bar{X}_B]
   = \frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}.

Standard Error
~~~~~~~~~~~~~~~~

The standard deviation of the estimator, or the **standard error**, is obtained by taking the
square root of the variance:

.. math::

   \sigma_{\bar{X}_A - \bar{X}_B} = \sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}.

The Sampling Distribution of the Difference Estimator
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We now know the expected value and variance of the difference estimator :math:`\bar{X}_A - \bar{X}_B`.
Additionally, since :math:`\bar{X}_A` and :math:`\bar{X}_B` are each normally distributed, 
their difference is also normally distributed. Combining these results, we establish the
full sampling distribution of the difference estimator as:

.. math::

   \bar{X}_A - \bar{X}_B \sim N\left(\mu_A - \mu_B, \sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}\right).

Equivalently, its standardization follows the standard normal distribution:

.. math::

   \frac{(\bar{X}_A - \bar{X}_B)-(\mu_A - \mu_B)}{\sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}}
   \sim N(0,1).

Based on these key results, we will now build inference methods for the difference in population means.

Hypothesis Testing for the Difference in Means
----------------------------------------------------------

The four-step hypothesis testing framework extends naturally to the two-sample setting, with modifications 
to accommodate the comparative nature.

Step 1: Parameter Identification
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We must clearly identify both population means using contextually meaningful labels. 
Rather than generic labels like A and B, we encourage the use of descriptive terms that 
reflect the populations being studied. 

For example, suppose we are comparing the systolic blood pressure of two patient groups after assigning 
one group a placebo and the other a newly developed treatment. We can define the relevant populations and
their true means in the following manner:

* Let :math:`\mu_{\text{treatment}}` denote the true mean systolic blood pressure of
  patients who are treated with the new procedure.
* Let :math:`\mu_{\text{control}}` denote the true mean systolic blood pressure of
  patients who are not treated with the new medical procedure. 

The parameter identification should also specify the **units of measurement** and provide sufficient 
context for interpreting the **parameter** and the **target population** within the scope of the research question.

Step 2: Hypothesis Formulation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Follow the template:

.. _HT_template_11_2:
.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter11/2sample-indep-HT-template.png 
   :figwidth: 70%
   :align: center 
   :alt: Template for independent two-sample hypotheses

   Template for independent two-sample hypotheses

You may also refer to the details provided in Chapter 11.1.3.

Step 3: Test Statistic and :math:`p`-Value
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Recall the :math:`z`-test statistic used in one-sample hypothesis testing:

.. math::
   \text{One-sample } Z_{TS} = \frac{\bar{X}-\mu_0}{\sigma/\sqrt{n}} 
   = \frac{\text{estimator}-\text{null value}}{\text{std. error}}.

By providing a **standardized distance between the estimator and the null value**, 
the :math:`z`-test statistic measured how far the sample data is from the null assumption.

We define a **new** :math:`z`-**test statistic** to serve the same purpose
by replacing each one-sample component with the appropriate independent two-sample parallel:

.. math::

   Z_{TS} = \frac{(\bar{X}_A - \bar{X}_B) - \Delta_0}{\sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}}.

As in the one-sample case, **under the the null hypothesis**,

.. math:: 
   Z_{TS} \sim N(0,1).

The :math:`p`-value calculation therefore follows the same principles as in single-sample :math:`z`-tests:

.. flat-table::
   :align: center
   :header-rows: 1
   :widths: 1 2
   :width: 60%

   * - :cspan:`1` Revisiting :math:`p`-Value Computation

   * - Upper-tailed
     - :math:`P(Z > z_{TS})`
   
   * - Lower-tailed
     - :math:`P(Z < z_{TS})`

   * - Two-tailed
     - :math:`2P(Z > |z_{TS}|) = 2P(Z < -|z_{TS}|)`

Step 4: Decision and Conclusion
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The decision rule remains unchanged from single-sample procedures:

- If :math:`p`-value :math:`\leq \alpha`, we reject :math:`H_0`.
- If :math:`p`-value :math:`> \alpha`, we fail to reject :math:`H_0`.

The conclusion template below is adapted to address the comparative nature of two-sample procedures:

   *"The data [does/does not] give [some/strong] support (p-value = [value]) to the claim that [statement of* :math:`H_a` *in context about the difference in population means]."*

The strength descriptors should reflect the magnitude of the :math:`p`-value relative to conventional 
benchmarks and the significance level used in the study.

.. admonition:: Example ðŸ’¡: Shift Scheduling and Work Efficiency
   :class: note

   A retail chain tests two different workforce scheduling systems to see which helps 
   cashiers process more transactions per 8-hour shift. They run independent pilots on 
   different stores:

   - **System A**: :math:`n_A = 25`, :math:`\bar{x}_A = 50`, :math:`\sigma_A = 10` (known)
   - **System B**: :math:`n_B = 30`, :math:`\bar{x}_B = 45`, :math:`\sigma_B = 12` (known)

   Perform a hypothesis test to determine whether the true mean numbers of transactions are different
   at the :math:`\alpha = 0.05` significance level.

   **Step 1: Define the parameters and target populations**

   Let :math:`\mu_A` denote the true mean number of transactions processed by cashiers following
   System A. Likewise, let :math:`\mu_B` be the true mean number of transactions completed by
   employees following System B.

   **Step 2: Write the hypotheses**

   .. math::
      &H_0: \mu_A - \mu_B = 0\\
      &H_a: \mu_A - \mu_B \neq 0

   **Step 3: Compute the test statistic and p-value**

   .. math::
      Z_{TS} =\frac{(\bar{x}_A - \bar{x}_B)-0}{\sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}}
      = \frac{(50-45) - 0}{\sqrt{\frac{10^2}{25} + \frac{12^2}{30}}} 
      \approx 1.68

   This is a two-sided test, so the :math:`p`-value is 
   :math:`2P(Z > 1.68) \approx 0.093`.


   **Step 4: Decision and Conclusion**

   Since :math:`p`-value :math:`= 0.093 > 0.05`, we **fail to reject the null hypothesis** at the 5% significance level.
   We do not have enough evidence to support the claim that the mean number of transactions
   fulfilled by cashiers are different by the scheduling system.

Confidence Regions for the Difference in Means
--------------------------------------------------

Let us begin by constructing a :math:`100C\%` confidence interval.
The goal is to find the margin of error (ME) such that

.. math::
   P((\bar{X}_A - \bar{X}_B) - ME < \mu_A - \mu_B < (\bar{X}_A - \bar{X}_B) + ME) = C.

The Pivotal Method
~~~~~~~~~~~~~~~~~~~~

We begin with the known truth:

.. math::
   P(-z_{\alpha/2} < Z < z_{\alpha/2}) = C.

Replace :math:`Z` with the standardization of the difference estimator, or the **pivotal quantity**:

.. math::
   P\left(-z_{\alpha/2} < \frac{(\bar{X}_A - \bar{X}_B) - (\mu_A - \mu_B)}{\sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}}
 < z_{\alpha/2}\right) = C.

Through algebraic manipulation to isolate :math:`\mu_A - \mu_B` in the center of the inequality, we obtain

.. math::
   P((\bar{X}_A - \bar{X}_B) - ME < \mu_A - \mu_B < (\bar{X}_A - \bar{X}_B) + ME) = C,

with :math:`ME = z_{\alpha/2} \sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}`.

For a single experiment, therefore, the :math:`100C \%` confidence interval is:

.. math::

   (\bar{x}_A - \bar{x}_B) \pm z_{\alpha/2} \sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}


Complete Summary of Confidence Intervals and Bounds
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The one-sided confidence bounds can be derived similarly. We leave the details as an excerciseâ€”use
the confidence interval derivation above and Chapter 9.4 as reference.

.. flat-table::
   :header-rows: 1
   :widths: 1 2
   :align: center

   * - :cspan:`1` :math:`100\cdot (1-\alpha) \%` Confidence Regions for Difference in Means

   * - **Confidence Interval**
     - .. math::

         (\bar{x}_A - \bar{x}_B) \pm z_{\alpha/2} \sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}
   
   * - **Upper Confidence Bound**
     - .. math::

         (\bar{x}_A - \bar{x}_B) + z_{\alpha} \sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}

   * - **Lower Confidence Bound**
     - .. math::

         (\bar{x}_A - \bar{x}_B) - z_{\alpha} \sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}

Note the repeated core elements. In both one-sample and two-sample cases, a confidence region is
**centered at the point estimate** and expands in the appropiate directions by ME, computed as
a **product of a critical value and the standard error**.

Interpreting Confidence Regions for Difference in Means
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The confidence regions provide a range of plausible values for the true difference in population means;
each region captures the true difference :math:`\mu_A - \mu_B` with :math:`100(1-\alpha)\%` confidence.
Their precision depends on the confidence level, the population variances, and the 
sample sizes.

.. admonition:: Example ðŸ’¡: Shift Scheduling and Work Efficiency, Continued
   :class: note

   For the experiment on two workforce scheduling systems with:

   - **System A**: :math:`n_A = 25`, :math:`\bar{x}_A = 50`, :math:`\sigma_A = 10` (known)
   - **System B**: :math:`n_B = 30`, :math:`\bar{x}_B = 45`, :math:`\sigma_B = 12` (known)

   Compute the :math:`95 \%` confidence interval for the difference of the two population means.
   Check if the result is consistent with the hypothesis test performed in the previous example.

   **Identify the components**

   * The observed sample difference
     
      .. math::
         \bar{x}_A - \bar{x}_B = 50 - 45 = 5

   * The standard error
     
     .. math::
        
        \sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}} = \sqrt{\frac{10^2}{25} + \frac{12^2}{30}} \approx 2.97

   * The :math:`z`-critical value
   
     .. code-block:: r 
        
        qnorm(0.025, lower.tail=FALSE)
        # returns 1.96

   **Put the parts together**

   The confidence interval is:

   .. math::

      5 \pm 1.96(2.97) = 5 \pm 5.82 = (-0.82, 10.82)

   **Is it consistent with the hypothesis test?**

   The interval contains zero, which is consistent with our failure to reject the null hypothesis of equal means.

Bringing It All Together
----------------------------

.. admonition:: Key Takeaways ðŸ“
   :class: important

   1. Two-sample independent procedures are designed to provide statistical answers to **comparative questions**. They
      require the key assumptions that (1) each sample is an SRS of the respective population, (2) 
      the two samples are independent from each other, (3) and the CLT holds in each sample.

   2. The **sampling distribution** of the point estimator :math:`\bar{X}_A - \bar{X}_B` is 
      normal with mean :math:`\mu_A - \mu_B` and variance :math:`\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}`. 
      The addition of variances follows from the independence assumption between groups.

   3. The construction of hypothesis tests and confidence regions follows **the same core principles as in
      the one-sample case**.


Exercises
---------


.. admonition:: Exercise 1: Assumptions for Independent Two-Sample z-Procedures
   :class: note

   For each scenario, evaluate whether the assumptions for the independent two-sample z-procedure are satisfied. If not, explain which assumption is violated.

   a. Comparing mean reaction times between gamers (n=50) and non-gamers (n=45). Both groups are randomly sampled, and historical studies suggest Ïƒâ‚ = 25 ms and Ïƒâ‚‚ = 30 ms.

   b. Comparing mean heights between students who eat breakfast (n=20) and those who skip breakfast (n=18) in a single classroom. The population standard deviations are known from large national studies.

   c. Comparing mean battery life between two phone models by testing 10 phones of each model. The manufacturer provides population standard deviations from extensive quality control testing.

   d. Comparing mean customer wait times at two bank branches by sampling 100 customers at each location. Prior studies established Ïƒâ‚ = 5 min and Ïƒâ‚‚ = 7 min.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Assumptions Required:**

      1. SRS from each population (iid within each sample)
      2. Independence between populations
      3. Normality of sampling distributions (either normal populations or large n for CLT)
      4. Population standard deviations Ïƒâ‚ and Ïƒâ‚‚ are known

      **Part (a): Gamers vs. non-gamers â€” SATISFIED âœ“**

      - Random sampling: âœ“
      - Independence between groups: âœ“ (separate populations)
      - Normality/CLT: âœ“ (n=50 and n=45 are large enough)
      - Ïƒ known: âœ“ (from historical studies)

      **Part (b): Breakfast habits â€” VIOLATED âœ—**

      - **Primary violation**: This is a convenience sample from a single classroom, not a random sample from the population of interest. The results may not generalize beyond this specific classroom.
      - **Secondary concern**: The "Ïƒ known from national studies" may not be appropriateâ€”the national population variance may not match this specific classroom population.
      - **Note**: While students in the same classroom might also lack independence, the lack of random sampling is the more fundamental issue.

      **Part (c): Phone battery life â€” POTENTIALLY PROBLEMATIC**

      - Random sampling: Depends on how phones were selected
      - Independence: âœ“
      - **Normality/CLT**: With n=10 each, CLT may not fully apply. Need to assume populations are approximately normal.
      - Ïƒ known: âœ“

      **Part (d): Bank wait times â€” SATISFIED âœ“**

      - Random sampling: Assuming customers were randomly selected
      - Independence: âœ“ (different branches)
      - Normality/CLT: âœ“ (n=100 each is large)
      - Ïƒ known: âœ“

----

.. admonition:: Exercise 2: Calculating the Standard Error
   :class: note

   For each scenario with known population standard deviations, calculate the standard error of :math:`\bar{X}_A - \bar{X}_B`.

   a. :math:`n_A = 25`, :math:`\sigma_A = 10`, :math:`n_B = 36`, :math:`\sigma_B = 12`

   b. :math:`n_A = 100`, :math:`\sigma_A = 15`, :math:`n_B = 100`, :math:`\sigma_B = 15`

   c. :math:`n_A = 16`, :math:`\sigma_A = 8`, :math:`n_B = 25`, :math:`\sigma_B = 10`

   d. How does the standard error change if you double both sample sizes in part (a)?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Formula:**

      .. math::
         SE = \sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}

      **Part (a):**

      .. math::
         SE = \sqrt{\frac{10^2}{25} + \frac{12^2}{36}} = \sqrt{\frac{100}{25} + \frac{144}{36}} = \sqrt{4 + 4} = \sqrt{8} = 2.828

      **Part (b):**

      .. math::
         SE = \sqrt{\frac{15^2}{100} + \frac{15^2}{100}} = \sqrt{\frac{225}{100} + \frac{225}{100}} = \sqrt{4.5} = 2.121

      **Part (c):**

      .. math::
         SE = \sqrt{\frac{8^2}{16} + \frac{10^2}{25}} = \sqrt{\frac{64}{16} + \frac{100}{25}} = \sqrt{4 + 4} = \sqrt{8} = 2.828

      **Part (d): Doubling sample sizes**

      With :math:`n_A = 50` and :math:`n_B = 72`:

      .. math::
         SE_{new} = \sqrt{\frac{100}{50} + \frac{144}{72}} = \sqrt{2 + 2} = \sqrt{4} = 2.0

      The standard error decreases by a factor of :math:`\sqrt{2} \approx 1.414`:

      .. math::
         \frac{SE_{old}}{SE_{new}} = \frac{2.828}{2.0} = 1.414 = \sqrt{2}

      **R verification:**

      .. code-block:: r

         # Part (a)
         sqrt(10^2/25 + 12^2/36)  # 2.828
         
         # Part (b)
         sqrt(15^2/100 + 15^2/100)  # 2.121
         
         # Part (c)
         sqrt(8^2/16 + 10^2/25)  # 2.828

----

.. admonition:: Exercise 3: Computing the Z Test Statistic
   :class: note

   A software company compares the mean processing times of two algorithms. Based on extensive benchmarking, the population standard deviations are known: :math:`\sigma_A = 50` ms and :math:`\sigma_B = 60` ms.

   Sample results:
   
   - Algorithm A: :math:`n_A = 40`, :math:`\bar{x}_A = 320` ms
   - Algorithm B: :math:`n_B = 50`, :math:`\bar{x}_B = 350` ms

   a. Calculate the point estimate for :math:`\mu_A - \mu_B`.

   b. Calculate the standard error.

   c. Calculate the z-test statistic for testing :math:`H_0: \mu_A - \mu_B = 0`.

   d. Interpret the test statistic value.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Point estimate**

      .. math::
         \bar{x}_A - \bar{x}_B = 320 - 350 = -30 \text{ ms}

      **Part (b): Standard error**

      .. math::
         SE = \sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}} = \sqrt{\frac{50^2}{40} + \frac{60^2}{50}} = \sqrt{\frac{2500}{40} + \frac{3600}{50}} = \sqrt{62.5 + 72} = \sqrt{134.5} = 11.60

      **Part (c): Z-test statistic**

      .. math::
         z_{TS} = \frac{(\bar{x}_A - \bar{x}_B) - \Delta_0}{SE} = \frac{-30 - 0}{11.60} = \frac{-30}{11.60} = -2.586

      **Part (d): Interpretation**

      The observed difference of -30 ms is 2.586 standard errors below zero, suggesting Algorithm A is faster than Algorithm B. Statistical significance depends on the chosen Î±; for example, at Î± = 0.05 (two-sided), the critical value is Â±1.96, so |z| = 2.586 > 1.96 would lead to rejecting Hâ‚€.

      **R verification:**

      .. code-block:: r

         xbar_A <- 320; xbar_B <- 350
         sigma_A <- 50; sigma_B <- 60
         n_A <- 40; n_B <- 50
         
         point_est <- xbar_A - xbar_B  # -30
         SE <- sqrt(sigma_A^2/n_A + sigma_B^2/n_B)  # 11.60
         z_ts <- (point_est - 0) / SE  # -2.586

----

.. admonition:: Exercise 4: Two-Tailed Hypothesis Test
   :class: note

   A materials scientist compares the mean tensile strength of steel from two suppliers. Population standard deviations are known from historical quality data: :math:`\sigma_A = 15` MPa and :math:`\sigma_B = 18` MPa.

   Sample results:
   
   - Supplier A: :math:`n_A = 36`, :math:`\bar{x}_A = 420` MPa
   - Supplier B: :math:`n_B = 49`, :math:`\bar{x}_B = 412` MPa

   Test whether the mean tensile strengths differ at Î± = 0.05.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Step 1: Define the parameters**

      Let :math:`\mu_A` = true mean tensile strength (MPa) of steel from Supplier A.
      Let :math:`\mu_B` = true mean tensile strength (MPa) of steel from Supplier B.

      **Step 2: State the hypotheses**

      Testing for any difference:

      .. math::
         &H_0: \mu_A - \mu_B = 0\\
         &H_a: \mu_A - \mu_B \neq 0

      **Step 3: Calculate the test statistic and p-value**

      Standard error:

      .. math::
         SE = \sqrt{\frac{15^2}{36} + \frac{18^2}{49}} = \sqrt{\frac{225}{36} + \frac{324}{49}} = \sqrt{6.25 + 6.61} = \sqrt{12.86} = 3.586

      Test statistic:

      .. math::
         z_{TS} = \frac{(420 - 412) - 0}{3.586} = \frac{8}{3.586} = 2.231

      P-value (two-tailed):

      .. math::
         p\text{-value} = 2 \times P(Z > |2.231|) = 2 \times P(Z > 2.231) = 2 \times 0.0128 = 0.0257

      **Step 4: Decision and Conclusion**

      Since p-value = 0.0257 < Î± = 0.05, **reject Hâ‚€**.

      The data does give support (p-value = 0.026) to the claim that the mean tensile strengths of steel from the two suppliers are different. Supplier A appears to provide stronger steel on average (420 vs. 412 MPa).

      **R verification:**

      .. code-block:: r

         xbar_A <- 420; xbar_B <- 412
         sigma_A <- 15; sigma_B <- 18
         n_A <- 36; n_B <- 49
         
         SE <- sqrt(sigma_A^2/n_A + sigma_B^2/n_B)  # 3.586
         z_ts <- (xbar_A - xbar_B) / SE  # 2.231
         p_value <- 2 * pnorm(abs(z_ts), lower.tail = FALSE)  # 0.0257

----

.. admonition:: Exercise 5: Lower-Tailed Hypothesis Test
   :class: note

   A pharmaceutical company tests whether their new formulation provides faster pain relief than the current formulation. Historical data give :math:`\sigma_{new} = 8` minutes and :math:`\sigma_{current} = 10` minutes.

   Sample results:
   
   - New formulation: :math:`n_{new} = 45`, :math:`\bar{x}_{new} = 22` minutes
   - Current formulation: :math:`n_{current} = 50`, :math:`\bar{x}_{current} = 26` minutes

   Test whether the new formulation provides faster relief (lower time) at Î± = 0.01.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Step 1: Define the parameters**

      Let :math:`\mu_{new}` = true mean time to pain relief (minutes) for the new formulation.
      Let :math:`\mu_{current}` = true mean time to pain relief (minutes) for the current formulation.

      **Step 2: State the hypotheses**

      "Faster relief" means *lower* time for new formulation:

      .. math::
         &H_0: \mu_{new} - \mu_{current} \geq 0\\
         &H_a: \mu_{new} - \mu_{current} < 0

      This is a **lower-tailed** test.

      **Step 3: Calculate the test statistic and p-value**

      Standard error:

      .. math::
         SE = \sqrt{\frac{8^2}{45} + \frac{10^2}{50}} = \sqrt{\frac{64}{45} + \frac{100}{50}} = \sqrt{1.422 + 2.0} = \sqrt{3.422} = 1.850

      Test statistic:

      .. math::
         z_{TS} = \frac{(22 - 26) - 0}{1.850} = \frac{-4}{1.850} = -2.162

      P-value (lower-tailed):

      .. math::
         p\text{-value} = P(Z < -2.162) = 0.0153

      **Step 4: Decision and Conclusion**

      Since p-value = 0.0153 > Î± = 0.01, **fail to reject Hâ‚€**.

      The data does not give support (p-value = 0.015) to the claim that the new formulation provides faster pain relief than the current formulation at the 1% significance level. While the sample shows a 4-minute improvement, this is not statistically significant at Î± = 0.01.

      *Note: At Î± = 0.05, we would reject Hâ‚€.*

      **R verification:**

      .. code-block:: r

         xbar_new <- 22; xbar_current <- 26
         sigma_new <- 8; sigma_current <- 10
         n_new <- 45; n_current <- 50
         
         SE <- sqrt(sigma_new^2/n_new + sigma_current^2/n_current)  # 1.850
         z_ts <- (xbar_new - xbar_current) / SE  # -2.162
         p_value <- pnorm(z_ts, lower.tail = TRUE)  # 0.0153

----

.. admonition:: Exercise 6: Confidence Interval for Difference of Means
   :class: note

   A logistics company compares delivery times between two shipping routes. Historical data provide :math:`\sigma_A = 12` hours and :math:`\sigma_B = 15` hours.

   Sample results:
   
   - Route A: :math:`n_A = 64`, :math:`\bar{x}_A = 48` hours
   - Route B: :math:`n_B = 81`, :math:`\bar{x}_B = 52` hours

   a. Construct a 95% confidence interval for :math:`\mu_A - \mu_B`.

   b. Interpret the confidence interval.

   c. Based on the interval, would you reject :math:`H_0: \mu_A - \mu_B = 0` at Î± = 0.05? Explain.

   d. Construct a 99% confidence interval. How does it compare to the 95% interval?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): 95% Confidence Interval**

      Point estimate: :math:`\bar{x}_A - \bar{x}_B = 48 - 52 = -4` hours

      Standard error:

      .. math::
         SE = \sqrt{\frac{12^2}{64} + \frac{15^2}{81}} = \sqrt{\frac{144}{64} + \frac{225}{81}} = \sqrt{2.25 + 2.778} = \sqrt{5.028} = 2.242

      Critical value: :math:`z_{0.025} = 1.96`

      Margin of error: :math:`ME = 1.96 \times 2.242 = 4.395`

      95% CI: :math:`-4 \pm 4.395 = (-8.395, 0.395)` hours

      **Part (b): Interpretation**

      We are 95% confident that the true difference in mean delivery times (Route A minus Route B) is between -8.4 and 0.4 hours. This suggests Route A is likely faster (negative difference means A takes less time), but the interval includes positive values, so we cannot be certain.

      **Part (c): Hypothesis test connection**

      Since the 95% CI contains 0, we would **fail to reject** :math:`H_0: \mu_A - \mu_B = 0` at Î± = 0.05. The interval includes values consistent with no difference, so we don't have sufficient evidence to conclude the routes differ.

      **Part (d): 99% Confidence Interval**

      Critical value: :math:`z_{0.005} = 2.576`

      Margin of error: :math:`ME = 2.576 \times 2.242 = 5.776`

      99% CI: :math:`-4 \pm 5.776 = (-9.776, 1.776)` hours

      The 99% CI is wider than the 95% CI, reflecting higher confidence at the cost of precision. Both intervals contain 0.

      **R verification:**

      .. code-block:: r

         xbar_A <- 48; xbar_B <- 52
         sigma_A <- 12; sigma_B <- 15
         n_A <- 64; n_B <- 81
         
         point_est <- xbar_A - xbar_B  # -4
         SE <- sqrt(sigma_A^2/n_A + sigma_B^2/n_B)  # 2.242
         
         # 95% CI
         z_crit_95 <- qnorm(0.025, lower.tail = FALSE)  # 1.96
         ME_95 <- z_crit_95 * SE
         c(point_est - ME_95, point_est + ME_95)  # (-8.395, 0.395)
         
         # 99% CI
         z_crit_99 <- qnorm(0.005, lower.tail = FALSE)  # 2.576
         ME_99 <- z_crit_99 * SE
         c(point_est - ME_99, point_est + ME_99)  # (-9.776, 1.776)

----

.. admonition:: Exercise 7: One-Sided Confidence Bound
   :class: note

   A manufacturing engineer wants to show that Machine A produces parts with mean diameter at most 0.5 mm larger than Machine B. From quality records: :math:`\sigma_A = 0.3` mm, :math:`\sigma_B = 0.25` mm.

   Sample results:
   
   - Machine A: :math:`n_A = 50`, :math:`\bar{x}_A = 25.4` mm
   - Machine B: :math:`n_B = 60`, :math:`\bar{x}_B = 25.1` mm

   a. State the appropriate hypotheses to test the engineer's claim.

   b. Compute the 95% upper confidence bound for :math:`\mu_A - \mu_B`.

   c. Does the confidence bound support the engineer's claim? Explain.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Hypotheses**

      The engineer wants to *demonstrate* that :math:`\mu_A - \mu_B \leq 0.5`. 

      **Approach 1 (Confidence Bound):** The preferred approach for demonstrating a claim like "at most 0.5" is to construct an upper confidence bound and show it falls below 0.5. This directly addresses the question.

      **Approach 2 (Hypothesis Test):** If we frame this as a hypothesis test, we would test whether the difference *exceeds* 0.5:

      .. math::
         &H_0: \mu_A - \mu_B \geq 0.5 \quad \text{(claim is violated)}\\
         &H_a: \mu_A - \mu_B < 0.5 \quad \text{(claim is satisfied)}

      Rejecting Hâ‚€ would provide evidence that the claim is satisfied. However, the confidence bound approach is cleaner here.

      **Part (b): 95% Upper Confidence Bound**

      Point estimate: :math:`\bar{x}_A - \bar{x}_B = 25.4 - 25.1 = 0.3` mm

      Standard error:

      .. math::
         SE = \sqrt{\frac{0.3^2}{50} + \frac{0.25^2}{60}} = \sqrt{\frac{0.09}{50} + \frac{0.0625}{60}} = \sqrt{0.0018 + 0.00104} = \sqrt{0.00284} = 0.0533

      Critical value for one-sided 95% bound: :math:`z_{0.05} = 1.645`

      Upper confidence bound:

      .. math::
         UCB = (\bar{x}_A - \bar{x}_B) + z_\alpha \times SE = 0.3 + 1.645 \times 0.0533 = 0.3 + 0.0877 = 0.388

      **Part (c): Interpretation**

      We are 95% confident that :math:`\mu_A - \mu_B \leq 0.388` mm.

      Since 0.388 < 0.5, the confidence bound supports the engineer's claim that Machine A produces parts with mean diameter at most 0.5 mm larger than Machine B.

      **R verification:**

      .. code-block:: r

         xbar_A <- 25.4; xbar_B <- 25.1
         sigma_A <- 0.3; sigma_B <- 0.25
         n_A <- 50; n_B <- 60
         
         point_est <- xbar_A - xbar_B  # 0.3
         SE <- sqrt(sigma_A^2/n_A + sigma_B^2/n_B)  # 0.0533
         
         z_alpha <- qnorm(0.05, lower.tail = FALSE)  # 1.645
         UCB <- point_est + z_alpha * SE  # 0.388

----

.. admonition:: Exercise 8: Complete Analysis with Non-Zero Null Value
   :class: note

   A consumer electronics company claims their premium headphones have at least 5 dB better noise cancellation than their standard model. An independent testing lab wants to verify this claim.

   From extensive testing: :math:`\sigma_{premium} = 4` dB, :math:`\sigma_{standard} = 5` dB.

   Test results:
   
   - Premium: :math:`n_P = 30`, :math:`\bar{x}_P = 38` dB
   - Standard: :math:`n_S = 35`, :math:`\bar{x}_S = 32` dB

   Conduct a hypothesis test at Î± = 0.05 to evaluate whether there is sufficient evidence to support the company's claim.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Step 1: Define the parameters**

      Let :math:`\mu_P` = true mean noise cancellation (dB) for premium headphones.
      Let :math:`\mu_S` = true mean noise cancellation (dB) for standard headphones.

      **Step 2: State the hypotheses**

      The company claims :math:`\mu_P - \mu_S \geq 5` (at least 5 dB better). To evaluate this claim using a one-sided test with complementary hypotheses, we test whether the true difference *exceeds* 5:

      .. math::
         &H_0: \mu_P - \mu_S \leq 5\\
         &H_a: \mu_P - \mu_S > 5

      Here :math:`\Delta_0 = 5` dB.

      *Note: Rejecting Hâ‚€ provides evidence that the difference exceeds 5 dB. Failing to reject means we lack sufficient evidence to conclude the difference is greater than 5â€”it does NOT mean the claim "at least 5" is supported.*

      **Step 3: Calculate the test statistic and p-value**

      Point estimate: :math:`\bar{x}_P - \bar{x}_S = 38 - 32 = 6` dB

      Standard error:

      .. math::
         SE = \sqrt{\frac{4^2}{30} + \frac{5^2}{35}} = \sqrt{\frac{16}{30} + \frac{25}{35}} = \sqrt{0.533 + 0.714} = \sqrt{1.248} = 1.117

      Test statistic:

      .. math::
         z_{TS} = \frac{(6) - 5}{1.117} = \frac{1}{1.117} = 0.895

      P-value (upper-tailed, since Hâ‚ uses >):

      .. math::
         p\text{-value} = P(Z > 0.895) = 1 - 0.815 = 0.185

      **Step 4: Decision and Conclusion**

      Since p-value = 0.185 > Î± = 0.05, **fail to reject Hâ‚€**.

      The data does not give sufficient evidence (p-value = 0.185) to support the claim that the premium headphones provide more than 5 dB better noise cancellation than the standard model. Although the sample difference of 6 dB exceeds the claimed 5 dB, the observed difference is not statistically significantly greater than 5 dB at the Î± = 0.05 level.

      *Important: "Fail to reject" does not mean the claim is falseâ€”it means we lack sufficient evidence to confirm the claim is true.*

      **R verification:**

      .. code-block:: r

         xbar_P <- 38; xbar_S <- 32
         sigma_P <- 4; sigma_S <- 5
         n_P <- 30; n_S <- 35
         Delta_0 <- 5
         
         point_est <- xbar_P - xbar_S  # 6
         SE <- sqrt(sigma_P^2/n_P + sigma_S^2/n_S)  # 1.117
         z_ts <- (point_est - Delta_0) / SE  # 0.895
         p_value <- pnorm(z_ts, lower.tail = FALSE)  # 0.185

----

.. admonition:: Exercise 9: Duality Between CI and Hypothesis Test
   :class: note

   An agricultural researcher compares crop yields (kg/hectare) between two irrigation methods. From historical data: :math:`\sigma_A = 200` kg/ha and :math:`\sigma_B = 180` kg/ha.

   Sample results:
   
   - Method A: :math:`n_A = 25`, :math:`\bar{x}_A = 3200` kg/ha
   - Method B: :math:`n_B = 30`, :math:`\bar{x}_B = 2950` kg/ha

   a. Conduct a two-sided hypothesis test at Î± = 0.05.

   b. Construct a 95% confidence interval.

   c. Verify that the hypothesis test and confidence interval give consistent conclusions.

   d. At what significance level would the test just barely reject Hâ‚€?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Hypothesis Test**

      **Step 1**: Let :math:`\mu_A` and :math:`\mu_B` be the true mean yields for Methods A and B.

      **Step 2**: 

      .. math::
         &H_0: \mu_A - \mu_B = 0\\
         &H_a: \mu_A - \mu_B \neq 0

      **Step 3**:

      SE:

      .. math::
         SE = \sqrt{\frac{200^2}{25} + \frac{180^2}{30}} = \sqrt{1600 + 1080} = \sqrt{2680} = 51.77

      Test statistic:

      .. math::
         z_{TS} = \frac{3200 - 2950}{51.77} = \frac{250}{51.77} = 4.83

      P-value:

      .. math::
         p = 2 \times P(Z > 4.83) < 0.0001

      **Step 4**: Since p < 0.05, **reject Hâ‚€**. The data does give strong support (p < 0.0001) to the claim that the mean yields differ between the two irrigation methods.

      **Part (b): 95% Confidence Interval**

      .. math::
         CI: 250 \pm 1.96 \times 51.77 = 250 \pm 101.5 = (148.5, 351.5)

      **Part (c): Consistency check**

      The 95% CI (148.5, 351.5) does not contain 0. This is consistent with rejecting Hâ‚€ at Î± = 0.05. âœ“

      **Part (d): Smallest Î± for rejection**

      The p-value is the smallest Î± at which we would reject. Using R:

      .. code-block:: r

         p_value <- 2 * pnorm(4.83, lower.tail = FALSE)  # â‰ˆ 0.0000014

      We would reject Hâ‚€ at any Î± â‰¥ 0.0000014.

----

Additional Practice Problems
----------------------------

**True/False Questions** (1 point each)

1. The z-test for two independent samples requires both population standard deviations to be known.

   â“‰ or â’»

2. If :math:`\bar{x}_A - \bar{x}_B = 5` and :math:`\bar{x}_B - \bar{x}_A = -5`, they lead to the same p-value.

   â“‰ or â’»

3. The standard error of :math:`\bar{X}_A - \bar{X}_B` is at least as large as the standard error of either :math:`\bar{X}_A` or :math:`\bar{X}_B` alone (strictly larger when both :math:`\sigma_A, \sigma_B > 0`).

   â“‰ or â’»

4. For a 95% CI that contains zero, the corresponding two-tailed test at Î± = 0.05 fails to reject Hâ‚€.

   â“‰ or â’»

5. The z-test statistic follows a standard normal distribution when Hâ‚€ is true.

   â“‰ or â’»

6. Increasing both sample sizes will decrease the standard error.

   â“‰ or â’»

**Multiple Choice Questions** (2 points each)

7. The formula for the standard error of :math:`\bar{X}_A - \bar{X}_B` (known variances) is:

   â’¶ :math:`\sqrt{\sigma^2_A + \sigma^2_B}`
   
   â’· :math:`\sqrt{\frac{\sigma^2_A}{n_A} - \frac{\sigma^2_B}{n_B}}`
   
   â’¸ :math:`\sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}`
   
   â’¹ :math:`\frac{\sigma_A + \sigma_B}{\sqrt{n_A + n_B}}`

8. For a two-tailed z-test with z_TS = 2.5, the p-value is:

   â’¶ P(Z > 2.5)
   
   â’· P(Z < -2.5)
   
   â’¸ 2 Ã— P(Z > 2.5)
   
   â’¹ 1 - P(Z > 2.5)

9. Which is NOT an assumption for the two-sample z-test?

   â’¶ Known population standard deviations
   
   â’· Equal sample sizes
   
   â’¸ Independence between samples
   
   â’¹ Normal sampling distributions

10. If a 99% CI for :math:`\mu_A - \mu_B` is (2, 8), then:

    â’¶ We reject Hâ‚€: Î¼_A - Î¼_B = 0 at Î± = 0.01
    
    â’· We fail to reject Hâ‚€: Î¼_A - Î¼_B = 0 at Î± = 0.01
    
    â’¸ We accept Hâ‚€: Î¼_A - Î¼_B = 5
    
    â’¹ Cannot determine without the p-value

11. To test if :math:`\mu_A > \mu_B`, the appropriate alternative hypothesis is:

    â’¶ H_a: Î¼_A - Î¼_B = 0
    
    â’· H_a: Î¼_A - Î¼_B < 0
    
    â’¸ H_a: Î¼_A - Î¼_B > 0
    
    â’¹ H_a: Î¼_A - Î¼_B â‰  0

12. The margin of error for a 95% CI for :math:`\mu_A - \mu_B` depends on:

    â’¶ Only the sample sizes
    
    â’· Only the population standard deviations
    
    â’¸ The sample sizes and population standard deviations
    
    â’¹ The sample means and sample sizes

.. dropdown:: Answers to Practice Problems
   :class-container: sd-border-success

   **True/False Answers:**

   1. **True** â€” The z-test formula uses Ïƒ_A and Ïƒ_B directly.

   2. **True** â€” The test statistics differ only in sign, leading to the same |z| and same p-value.

   3. **True** â€” SE of difference = âˆš(ÏƒÂ²_A/n_A + ÏƒÂ²_B/n_B) â‰¥ âˆš(ÏƒÂ²_A/n_A) = SE of :math:`\bar{X}_A` (strictly larger when both Ïƒ values are positive).

   4. **True** â€” This is the duality between CIs and two-sided hypothesis tests.

   5. **True** â€” Under Hâ‚€ and the stated assumptions, z_TS ~ N(0,1).

   6. **True** â€” SE decreases as n_A and n_B increase (they appear in denominators under the square root).

   **Multiple Choice Answers:**

   7. **â’¸** â€” Variances add, then divide by respective sample sizes.

   8. **â’¸** â€” Two-tailed p-value = 2 Ã— P(Z > |z_TS|).

   9. **â’·** â€” Equal sample sizes are not required for the z-test.

   10. **â’¶** â€” The CI doesn't contain 0, so we reject Hâ‚€: Î¼_A - Î¼_B = 0.

   11. **â’¸** â€” Î¼_A > Î¼_B means Î¼_A - Î¼_B > 0.

   12. **â’¸** â€” ME = z Ã— âˆš(ÏƒÂ²_A/n_A + ÏƒÂ²_B/n_B) depends on both.