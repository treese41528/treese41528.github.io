.. _12-2-sources-of-variability:

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch12-2">
      <iframe
         id="video-ch12-2"
         title="STAT 350 ‚Äì Chapter 12.2 One-Way ANOVA Model and the Sources of Variability Video"
         src="https://www.youtube.com/embed/BKEQadpmPzw?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>

.. admonition:: Slides üìä
   :class: tip

   `Download Chapter 12 slides (PPTX) <https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/slides/
   Chapter%2012%20ANOVA/OneWayANOVA_AC.pptx>`_
   
Different Sources of Variability in ANOVA
===============================================================================

In our previous exploration using side-by-side boxplots, we learned that comparing means in 
isolation was not sufficient‚Äîwe had to consider how much variability existed within each group
in comparison with the spread of the group means. In this lesson, we formalize this idea mathematically.


.. admonition:: Road Map üß≠
   :class: important

   * Identify the **three sources of variability** in ANOVA. Quantify the variability from each source using
     formal notation.
   * Recognize the roles of **sum of squares, degrees of freedom and mean of squares** in constructing a
     sample variance.
   * Learn the **special relationship** between the three sums of squares and their degrees of freedom.
   * Organize the components into an **ANOVA table**.

The One-Way ANOVA Model
------------------------------

What is a Statistical Model?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

As we progress to statistical inference methods of higher complexity, it becomes essential to
define a corresponding *statistical model* to concisely express the core ideas. A statistical model 
provides a structural decomposition of the data that must hold under the assumptions of the analysis method.

The One-Way ANOVA Model 
~~~~~~~~~~~~~~~~~~~~~~~~~~~

One-way ANOVA assumes that an observation :math:`X_{ij}` takes the following form:

.. math::

   X_{ij} = \mu_i + \varepsilon_{ij}.

Anove, :math:`\mu_{i}` is the unknown true mean of Group :math:`i`, and :math:`\varepsilon_{ij}`
is the random error that captures everything not explained by the group mean. 

According to the ANOVA assumptions, the random errors are mutually independent and have
an equal variance of :math:`\sigma^2`. Since we have extracted the group means out of the random term,
:math:`\varepsilon_{ij}` would also have an expected value of zero. 

In the ideal case where all populations are normally distributed, therefore, we can write:

.. math::

   \varepsilon_{ij} \sim_{iid} N(0, \sigma^2)

Why Is the Model Helpful?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The ANOVA model allows us to view each data point as the outcome of two components,
each contributing distinctly. 
If all population means are truly equal, the only source of randomness
among observations would be the :math:`\varepsilon_{ij}` terms, whose 
variance is :math:`\sigma^2`.
If the observed variance in the data is significantly larger than :math:`\sigma^2`, therefore, we
must consider the possibility that differences in group means also contribute. 

To formalize this, we first construct the three key measures of variation in ANOVA: variation between groups,
variation within groups, and the total variation.

Three Types of Variability
--------------------------------

As with any sample variance we have seen before, the three measures of variation will take the following common form:

.. math::

   S^2 = \frac{\text{Sum of Squares}}{\text{df}},

where the degrees of freedom are chosen to make each statistic an unbiased estimator of its target.
The degrees of freedom will show the pattern we have previously seen‚Äîit is equal to the difference between
the number of data points and the number of estimated means used for construction of the statistic.

Since each sample variance of this form is an "average" of squares, 
we also call them a **mean of squares (MS)**.

1. SSA and MSA: Between-Group Variation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We first consider the sum of sqaures for between-group variation, or SSA.

.. math::

   \text{SSA} = \sum_{i=1}^k n_i (\bar{X}_{i \cdot} - \bar{X}_{\cdot \cdot})^2

Note that SSA is the sum of squared deviations of group means from the overall mean, each weighted
proportionally to the group size. The degrees of freedom appropriate for SSA is :math:`df_A = k-1` since 
there are :math:`k` group means deviating from a single overall mean.

It follows that the mean of squares for the variation between groups is:

.. math::

   \text{MSA} = \frac{\text{SSA}}{df_A} = \frac{\sum_{i=1}^k n_i (\bar{X}_{i \cdot} - \bar{X}_{\cdot \cdot})^2}{k-1}.

MSA has a special property:

* If :math:`H_0` is true and the equal variance assumption holds, then the MSA is an unbiased estimator of :math:`\sigma^2`. 
* However, if :math:`H_0` is false, then the MSA estimates :math:`\sigma^2` plus additional 
  variation due to differences in population means.

Therefore, an MSA significantly larger than an estimate of :math:`\sigma^2` indicates the existence of 
additional variance due to distinct group means, whereas an MSA comparable to an estimated :math:`\sigma^2`
indicates an absence of strong evidence against the null hypothesis.

.. admonition:: Why Do We call It SS"A"?
   :class: important

   The name SSA stands for "Sum of Squares for Factor A". It originates from the multi-way ANOVA context involving
   multiple factors (Factor A, Factor B, etc.).
   By convention, the sum of squares for the "first" factor is labeled SSA, even when there is only one factor
   in the anlaysis.

.. admonition:: Example üí°: Coffeehouse SSA & MSA ‚òïÔ∏è
   :class: note 

   In the coffeehouse example, SSA and MSA measure how much the store-wise average ages vary from the overall mean.
   If all the coffeehouses attract similar demographics, SSA and MSA should be small. If they attract 
   different age groups, they should be large.

2. SSE and MSE: Within-Group Variation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The sum of squares of errors, or SSE, is an unscaled measure of how observations within each group deviate from their 
respective group means due to the random error, :math:`\varepsilon_{ij}`.

.. math::

   \text{SSE} = \sum_{i=1}^k \sum_{j=1}^{n_i} (X_{ij} - \bar{X}_{i \cdot})^2 = \sum_{i=1}^k (n_i - 1) S^2_{i\cdot}

Confirm the second equality by replacing :math:`S^2_{i\cdot}` with its explicit formula.
The SSE consists of the squared distances of :math:`n` observations from one of :math:`k` group means,
giving us the degrees of freedom :math:`df_E=n-k`.

Bringing SSE to the correct scale with the degrees of freedom, we obtain:

.. math::

   \text{MSE} = \frac{\text{SSE}}{df_E} = \frac{\sum_{i=1}^k \sum_{j=1}^{n_i} (X_{ij} - \bar{X}_{i \cdot})^2}{n-k}.

**MSE is an unbiased estimator of the variance within populations,** :math:`\sigma^2`, 
**regardless of whether** :math:`H_0` **is true.**
As a result, we always estimate the within-population variance with :math:`S^2 = MSE`. The estimator for
population-wise standard deviation is :math:`S= \sqrt{MSE}`.

.. admonition:: Connecting ANOVA and Indepdent Two-Sample Analysis
   :class: important

   MSE is a multi-way extension of the pooled variance estimator for independent two-sample analysis. Confirm this
   by plugging in :math:`k=2` to recover :math:`S^2_p`.

.. admonition:: Example üí°: Coffeehouse SSE & MSE ‚òïÔ∏è
   :class: note 

   In the coffeehouse example, the SSE and MSE measure how much individual customer ages vary within each
   coffeehouse. They represent the natural variation in customer ages that exists regardless of
   any systematic differences between coffeehouses.

3. Total Sum of Squares (SST)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Finally, we also define a measure for the **overall** variability in the data.

.. math::

   \text{SST} = \sum_{i=1}^k \sum_{j=1}^{n_i} (X_{ij} - \bar{X}_{\cdot \cdot})^2

Note that this would be the numerator for the sample variance if the entire dataset was treated as a
single sample. The distances of :math:`n` total observations are measured against one overall mean estimator, 
giving us the degrees of freedom :math:`df_T = n - 1`.

We do not define a mean of sqaures for the total variation, as it does not hold significance in the ANOVA framework.

.. admonition:: Example üí°: Coffeehouse SST ‚òïÔ∏è
   :class: note 

   In the coffeehouse example, SST measures the degree of variation of all customer ages around the single 
   overall sample mean.

The Fundamental ANOVA Identity
--------------------------------

The remarkable mathematical result that makes ANOVA possible is that the sums of squares are related by:

.. math::

   \text{SST} = \text{SSA} + \text{SSE}

Moreover, the degrees of freedom decompose in the same way:

.. math::

   (n - 1) = (k - 1) + (n - k)

Why This Decomposition Works
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We use the trick of adding and subtracting the same terms inside each pair of parentheses in SST:

.. math::

   \text{SST} &= \sum_{i=1}^k \sum_{j=1}^{n_i} (X_{ij} - \bar{X}_{\cdot \cdot})^2 \\
   &= \sum_{i=1}^k \sum_{j=1}^{n_i} [(X_{ij} - \bar{X}_{i \cdot}) + (\bar{X}_{i \cdot} - \bar{X}_{\cdot \cdot})]^2\\
   &= \sum_{i=1}^k \sum_{j=1}^{n_i} [(X_{ij} - \bar{X}_{i \cdot})^2  + (\bar{X}_{i \cdot} - \bar{X}_{\cdot \cdot})^2 
      + 2(X_{ij} - \bar{X}_{i \cdot})(\bar{X}_{i \cdot} - \bar{X}_{\cdot \cdot})] \\
   &= SSE + SSA + \sum_{i=1}^k \sum_{j=1}^{n_i}2(X_{ij} - \bar{X}_{i \cdot})(\bar{X}_{i \cdot} - \bar{X}_{\cdot \cdot})

The cross-product term can be shown to equal zero by taking the following steps:

* Since :math:`2(\bar{X}_{i \cdot} - \bar{X}_{\cdot \cdot})` does not depend on
  :math:`j`, we can factor it out of the inner sum.
* The inner sum is then
  
  .. math:: 
     \sum_{j=1}^{n_i} (X_{ij} - \bar{X}_{i \cdot}) = \sum_{j=1}^{n_i}X_{ij} - n_{i}\cdot\frac{\sum_{j=1}^{n_i}X_{ij}}{n_i} = 0.


The ANOVA Table
------------------

The components of ANOVA are often organized into a table, with rows representing the 
three difference sources of variability, and the columns representing various characteristics of 
each source.

.. flat-table::
   :header-rows: 1
   :width: 95%
   :widths: 15 10 25 10 10 10

   * - Source
     - df
     - SS
     - MS
     - F
     - :math:`p`-value

   * - Factor A
     - :math:`k-1`
     - :math:`\sum_{i=1}^k n_i(\bar{x}_{i \cdot} - \bar{x}_{\cdot \cdot})^2`
     - :math:`\frac{\text{SSA}}{k-1}`
     - ?
     - ?

   * - Error
     - :math:`n-k`
     - :math:`\sum_{i=1}^k \sum_{j=1}^{n_i}(x_{ij} - \bar{x}_{i \cdot})^2`
     - :math:`\frac{\text{SSE}}{n-k}`
     - 
     - 

   * - Total
     - :math:`n-1`
     - :math:`\sum_{i=1}^k \sum_{j=1}^{n_i}(x_{ij} - \bar{x}_{\cdot \cdot})^2`
     - 
     - 
     - 

The total row is often omitted for conciseness as their entries can be computed by adding up
the other dfs and the sums of squares.
The entries corresponding to :math:`F` and :math:`p`-value will be discussed in the upcoming lesson.


.. admonition:: Example üí°: Coffeehouse ANOVA Table ‚òïÔ∏è
   :class: note

   Using the data summary and the partial ANOVA table of the coffeehouse example,

   * Fill in the blank entries of the ANOVA table.
   * Provide an estimate of the population standard deviation, :math:`\sigma`.

   üìä `Download the coffeehouse dataset (CSV) <https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/Data/AgeCoffee.csv>`_

   **Data Summary** 

   .. list-table:: Sample Statistics
      :header-rows: 1
      :widths: 30 20 25 25
      :align: center

      * - Sample (Levels of Factor Variable)
        - Sample Size
        - Mean
        - Variance
      * - Population 1
        - :math:`n_1 = 39`
        - :math:`\bar{x}_{1.} = 39.13`
        - :math:`s_1^2 = 62.43`
      * - Population 2
        - :math:`n_2 = 38`
        - :math:`\bar{x}_{2.} = 46.66`
        - :math:`s_2^2 = 168.34`
      * - Population 3
        - :math:`n_3 = 42`
        - :math:`\bar{x}_{3.} = 40.50`
        - :math:`s_3^2 = 119.62`
      * - Population 4
        - :math:`n_4 = 38`
        - :math:`\bar{x}_{4.} = 26.42`
        - :math:`s_4^2 = 48.90`
      * - Population 5
        - :math:`n_5 = 43`
        - :math:`\bar{x}_{5.} = 34.07`
        - :math:`s_5^2 = 98.50`
      * - **Combined**
        - :math:`n = 200`
        - :math:`\bar{x}_{..} = 37.35`
        - :math:`s^2 = 142.14`

   **Partially Complete ANOVA Table** 

   .. list-table::
      :header-rows: 1
      :widths: 20 15 15 15
      :align: center

      * - Source
        - df
        - SS
        - MS
      * - Factor A
        - **(1)**
        - **(4)**
        - **(6)**
      * - Error
        - **(2)**
        - **(5)**
        - 99.8
      * - Total
        - **(3)**
        - 28285
        - 

   Let us begin with the degrees of freedom since they can be obtained directly from the data summary.
   We have :math:`n=200` and :math:`k=5`. Therefore,

   * **(1)** :math:`df_A = k - 1 = 4`
   * **(2)** :math:`df_E = n - k = 200 - 5 = 195`
   * **(3)** :math:`df_T = n - 1 = 199`

   We can use :math:`df_T = df_A + df_E` as a second check.
   
   **(5)** :math:`MSE = 99.8 = \frac{SSE}{195}`, so
     
     .. math::
        SSE = 99.8 \times 195 = 19461

   **(4)** Then, using the fact that SSA and SSE add up to SST, 

     .. math:: 
        SSA = SST - SSE = 28285 - 19461 = 8834

   **(6)** Finally, 

   .. math::
      MSA = \frac{SSA}{df_A} = \frac{8824}{4} = 2208.4

   The MSE as a random variable is an unbiased estimator of :math:`\sigma^2`. We can use the square root 
   of its observed value as an estimate for :math:`\sigma`.

   .. math:: 
      \hat{\sigma} = \sqrt{MSE} = \sqrt{99.8} = 9.99

Bringing It All Together
---------------------------

.. admonition:: Key Takeaways üìù
   :class: important

   1. The **ANOVA model** decomposes each observation 
      into a group effect plus random error: :math:`X_{ij} = \mu_i + \varepsilon_{ij}`.

   2. Total Sum of Squares (SST) satisfies :math:`SST = SSA + SSE`. 
      Their degrees of freedom have a similar association: :math:`df_T = df_A + df_E`.

   3. Since :math:`E(MSE) = \sigma^2` always holds, we use the **MSE as the estimator of the
      common variance** :math:`\sigma^2`, and also denote its observed value as :math:`s^2`. 

   4. MSA is an unbiased astimator of :math:`\sigma^2` **only when** :math:`H_0` **is true**
      ‚Äîits true target is greater than :math:`\sigma^2` when :math:`H_0` is false. Therefore,
      comparing MSA against MSE gives us a measure of data evidence against the null hypothesis.


Exercises
---------

.. admonition:: Exercise 1: Computing the Overall Mean
   :class: note

   A quality engineer compares the tensile strength (in MPa) of welds produced by four different welding techniques. Summary statistics are:

   .. list-table::
      :header-rows: 1
      :widths: 25 15 20

      * - Technique
        - Sample Size
        - Mean Strength
      * - MIG (Group 1)
        - 12
        - 485.2
      * - TIG (Group 2)
        - 15
        - 492.8
      * - Stick (Group 3)
        - 10
        - 478.5
      * - Flux-cored (Group 4)
        - 13
        - 488.1

   a. Calculate the total sample size :math:`n`.

   b. Calculate the overall (grand) mean :math:`\bar{x}_{..}` using the weighted average formula.

   c. Verify that the grand mean is NOT simply the average of the four group means. Why does this matter?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Total sample size**

      .. math::
         n = n_1 + n_2 + n_3 + n_4 = 12 + 15 + 10 + 13 = 50

      **Part (b): Overall mean**

      The grand mean is a weighted average of group means:

      .. math::
         \bar{x}_{..} = \frac{\sum_{i=1}^{k} n_i \bar{x}_{i.}}{n} = \frac{12(485.2) + 15(492.8) + 10(478.5) + 13(488.1)}{50}

      .. math::
         = \frac{5822.4 + 7392 + 4785 + 6345.3}{50} = \frac{24344.7}{50} = 486.89 \text{ MPa}

      **Part (c): Comparison with simple average**

      Simple average of group means:

      .. math::
         \frac{485.2 + 492.8 + 478.5 + 488.1}{4} = \frac{1944.6}{4} = 486.15 \text{ MPa}

      The weighted mean (486.89) differs from the simple average (486.15) because groups have unequal sample sizes. Groups with larger n contribute more to the overall mean. This matters because:

      - TIG (n=15, highest mean) pulls the weighted average up
      - Stick (n=10, lowest mean) has less influence
      - Using the wrong formula would lead to incorrect SSA calculations

      **R verification:**

      .. code-block:: r

         n <- c(12, 15, 10, 13)
         xbar <- c(485.2, 492.8, 478.5, 488.1)
         
         n_total <- sum(n)  # 50
         grand_mean <- sum(n * xbar) / n_total  # 486.894
         simple_avg <- mean(xbar)  # 486.15

----

.. admonition:: Exercise 2: Computing SSA (Between-Group Variation)
   :class: note

   Using the welding data from Exercise 1 (with :math:`\bar{x}_{..} = 486.89` MPa):

   a. Write out the formula for SSA and explain what each component represents.

   b. Calculate SSA step by step.

   c. Calculate the degrees of freedom for SSA (:math:`df_A`).

   d. Calculate MSA.

   e. Interpret what a large MSA would indicate about the welding techniques.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): SSA formula**

      .. math::
         \text{SSA} = \sum_{i=1}^{k} n_i (\bar{x}_{i.} - \bar{x}_{..})^2

      Components:
      
      - :math:`n_i` = sample size for group i (weights each group's contribution)
      - :math:`\bar{x}_{i.}` = sample mean for group i
      - :math:`\bar{x}_{..}` = overall grand mean
      - :math:`(\bar{x}_{i.} - \bar{x}_{..})^2` = squared deviation of group mean from grand mean

      SSA measures how much the group means vary around the overall mean.

      **Part (b): SSA calculation**

      First, compute each group's contribution:

      .. list-table::
         :header-rows: 1
         :widths: 15 15 20 20 20

         * - Group
           - :math:`n_i`
           - :math:`\bar{x}_{i.}`
           - :math:`\bar{x}_{i.} - \bar{x}_{..}`
           - :math:`n_i(\bar{x}_{i.} - \bar{x}_{..})^2`
         * - MIG
           - 12
           - 485.2
           - -1.69
           - 34.29
         * - TIG
           - 15
           - 492.8
           - 5.91
           - 523.87
         * - Stick
           - 10
           - 478.5
           - -8.39
           - 703.92
         * - Flux-cored
           - 13
           - 488.1
           - 1.21
           - 19.04

      .. math::
         \text{SSA} = 34.29 + 523.87 + 703.92 + 19.04 = 1281.12

      **Part (c): Degrees of freedom**

      .. math::
         df_A = k - 1 = 4 - 1 = 3

      **Part (d): MSA calculation**

      .. math::
         \text{MSA} = \frac{\text{SSA}}{df_A} = \frac{1281.12}{3} = 427.04

      **Part (e): Interpretation**

      A large MSA indicates that the group means are spread widely around the grand mean‚Äîsuggesting that the welding techniques produce systematically different tensile strengths. If MSA is significantly larger than MSE (the within-group variability), this provides evidence that at least one technique differs from the others.

      **R verification:**

      .. code-block:: r

         n <- c(12, 15, 10, 13)
         xbar <- c(485.2, 492.8, 478.5, 488.1)
         grand_mean <- sum(n * xbar) / sum(n)  # 486.894
         
         SSA <- sum(n * (xbar - grand_mean)^2)  # 1281.12
         df_A <- length(n) - 1  # 3
         MSA <- SSA / df_A  # 427.04

----

.. admonition:: Exercise 3: Computing SSE (Within-Group Variation)
   :class: note

   Continuing with the welding data, the sample standard deviations for each technique are:

   .. list-table::
      :header-rows: 1
      :widths: 25 15 20 20

      * - Technique
        - :math:`n_i`
        - :math:`\bar{x}_{i.}`
        - :math:`s_i`
      * - MIG
        - 12
        - 485.2
        - 8.5
      * - TIG
        - 15
        - 492.8
        - 7.2
      * - Stick
        - 10
        - 478.5
        - 9.8
      * - Flux-cored
        - 13
        - 488.1
        - 8.1

   a. Write the formula for SSE and explain its relationship to the individual group variances.

   b. Calculate SSE using the shortcut formula :math:`\text{SSE} = \sum_{i=1}^{k}(n_i - 1)s_i^2`.

   c. Calculate :math:`df_E` and MSE.

   d. Provide an estimate of the common population standard deviation :math:`\sigma`.

   e. Check the equal variance assumption using the rule of thumb.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): SSE formula**

      .. math::
         \text{SSE} = \sum_{i=1}^{k} \sum_{j=1}^{n_i} (x_{ij} - \bar{x}_{i.})^2 = \sum_{i=1}^{k}(n_i - 1)s_i^2

      The shortcut works because :math:`s_i^2 = \frac{\sum_{j=1}^{n_i}(x_{ij} - \bar{x}_{i.})^2}{n_i - 1}`, so multiplying by :math:`(n_i - 1)` recovers the sum of squares for each group.

      SSE measures the total variability within all groups‚Äîvariation that cannot be explained by group membership.

      **Part (b): SSE calculation**

      .. list-table::
         :header-rows: 1
         :widths: 20 15 15 20

         * - Group
           - :math:`n_i - 1`
           - :math:`s_i^2`
           - :math:`(n_i - 1)s_i^2`
         * - MIG
           - 11
           - 72.25
           - 794.75
         * - TIG
           - 14
           - 51.84
           - 725.76
         * - Stick
           - 9
           - 96.04
           - 864.36
         * - Flux-cored
           - 12
           - 65.61
           - 787.32

      .. math::
         \text{SSE} = 794.75 + 725.76 + 864.36 + 787.32 = 3172.19

      **Part (c): df_E and MSE**

      .. math::
         df_E = n - k = 50 - 4 = 46

      .. math::
         \text{MSE} = \frac{\text{SSE}}{df_E} = \frac{3172.19}{46} = 68.96

      **Part (d): Estimate of œÉ**

      .. math::
         \hat{\sigma} = \sqrt{\text{MSE}} = \sqrt{68.96} = 8.30 \text{ MPa}

      This is our best estimate of the common within-group standard deviation.

      **Part (e): Equal variance check**

      .. math::
         \frac{\max(s_i)}{\min(s_i)} = \frac{9.8}{7.2} = 1.36

      Since 1.36 ‚â§ 2, **the equal variance assumption is satisfied** ‚úì

      **R verification:**

      .. code-block:: r

         n <- c(12, 15, 10, 13)
         s <- c(8.5, 7.2, 9.8, 8.1)
         
         SSE <- sum((n - 1) * s^2)  # 3172.19
         df_E <- sum(n) - length(n)  # 46
         MSE <- SSE / df_E  # 68.96
         sigma_hat <- sqrt(MSE)  # 8.30
         
         max(s) / min(s)  # 1.36 - passes rule of thumb

----

.. admonition:: Exercise 4: The Fundamental ANOVA Identity
   :class: note

   The fundamental ANOVA identity states: :math:`\text{SST} = \text{SSA} + \text{SSE}`.

   a. Using SSA = 1281.12 and SSE = 3172.19 from the previous exercises, calculate SST.

   b. Verify the degrees of freedom relationship: :math:`df_T = df_A + df_E`.

   c. If you were given SST = 5000 and SSA = 1500, what would SSE be?

   d. Explain why MST (if we computed it) would NOT equal MSA + MSE, even though SST = SSA + SSE.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Calculate SST**

      .. math::
         \text{SST} = \text{SSA} + \text{SSE} = 1281.12 + 3172.19 = 4453.31

      **Part (b): Verify df relationship**

      - :math:`df_A = k - 1 = 3`
      - :math:`df_E = n - k = 46`
      - :math:`df_T = n - 1 = 49`

      Check: :math:`df_A + df_E = 3 + 46 = 49 = df_T` ‚úì

      **Part (c): Finding SSE from SST and SSA**

      .. math::
         \text{SSE} = \text{SST} - \text{SSA} = 5000 - 1500 = 3500

      **Part (d): Why MS doesn't add**

      Mean squares are sums of squares divided by their respective degrees of freedom:

      - :math:`\text{MSA} = \frac{\text{SSA}}{df_A}`
      - :math:`\text{MSE} = \frac{\text{SSE}}{df_E}`
      - :math:`\text{MST} = \frac{\text{SST}}{df_T}` (if computed)

      Even though SST = SSA + SSE, dividing by different denominators breaks the additive relationship:

      .. math::
         \text{MST} = \frac{\text{SST}}{df_T} = \frac{\text{SSA} + \text{SSE}}{df_A + df_E} \neq \frac{\text{SSA}}{df_A} + \frac{\text{SSE}}{df_E}

      This is analogous to why :math:`\frac{a+b}{c+d} \neq \frac{a}{c} + \frac{b}{d}` in general.

----

.. admonition:: Exercise 5: Completing a Partial ANOVA Table
   :class: note

   An aerospace engineer tests the fatigue life (in thousands of cycles) of turbine blades made from three different alloys. The partial ANOVA table is:

   .. list-table::
      :header-rows: 1
      :widths: 20 15 20 20

      * - Source
        - df
        - SS
        - MS
      * - Alloy
        - **(a)**
        - **(d)**
        - 845.6
      * - Error
        - **(b)**
        - 2536.8
        - **(e)**
      * - Total
        - **(c)**
        - 4228.0
        - 

   The study used 12 blades from each of the 3 alloys.

   Complete all missing entries (a) through (e).

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Given information:**
      
      - k = 3 alloys
      - n_i = 12 for each alloy ‚Üí n = 36 total
      - MSA = 845.6
      - SSE = 2536.8
      - SST = 4228.0

      **Step 1: Degrees of freedom**

      **(a)** :math:`df_A = k - 1 = 3 - 1 = 2`

      **(b)** :math:`df_E = n - k = 36 - 3 = 33`

      **(c)** :math:`df_T = n - 1 = 35`

      Verification: :math:`2 + 33 = 35` ‚úì

      **Step 2: Calculate SSA**

      From :math:`\text{MSA} = \frac{\text{SSA}}{df_A}`:

      **(d)** :math:`\text{SSA} = \text{MSA} \times df_A = 845.6 \times 2 = 1691.2`

      Verification: SSA + SSE = 1691.2 + 2536.8 = 4228.0 = SST ‚úì

      **Step 3: Calculate MSE**

      **(e)** :math:`\text{MSE} = \frac{\text{SSE}}{df_E} = \frac{2536.8}{33} = 76.87`

      **Complete ANOVA Table:**

      .. list-table::
         :header-rows: 1
         :widths: 20 15 20 20

         * - Source
           - df
           - SS
           - MS
         * - Alloy
           - 2
           - 1691.2
           - 845.6
         * - Error
           - 33
           - 2536.8
           - 76.87
         * - Total
           - 35
           - 4228.0
           - 

      **R verification:**

      .. code-block:: r

         k <- 3
         n <- 36
         MSA <- 845.6
         SSE <- 2536.8
         SST <- 4228.0
         
         df_A <- k - 1  # 2
         df_E <- n - k  # 33
         df_T <- n - 1  # 35
         
         SSA <- MSA * df_A  # 1691.2
         MSE <- SSE / df_E  # 76.87
         
         # Verification
         SSA + SSE  # Should equal SST = 4228

----

.. admonition:: Exercise 6: MSE as the Pooled Variance Estimator
   :class: note

   This exercise connects ANOVA to the pooled variance from Chapter 11.

   a. Write the formula for the pooled variance estimator :math:`s_p^2` from two-sample inference (Chapter 11).

   b. Write the formula for MSE in ANOVA.

   c. Show that when k = 2, MSE reduces to :math:`s_p^2`.

   d. Why is this connection important?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Pooled variance from Chapter 11**

      For two independent samples:

      .. math::
         s_p^2 = \frac{(n_A - 1)s_A^2 + (n_B - 1)s_B^2}{n_A + n_B - 2}

      **Part (b): MSE formula**

      .. math::
         \text{MSE} = \frac{\text{SSE}}{n - k} = \frac{\sum_{i=1}^{k}(n_i - 1)s_i^2}{n - k}

      **Part (c): Showing MSE = s_p¬≤ when k = 2**

      When k = 2, let the groups be A and B:

      .. math::
         \text{MSE} = \frac{(n_A - 1)s_A^2 + (n_B - 1)s_B^2}{(n_A + n_B) - 2} = \frac{(n_A - 1)s_A^2 + (n_B - 1)s_B^2}{n_A + n_B - 2}

      This is exactly the formula for :math:`s_p^2`!

      **Part (d): Why this matters**

      This connection shows that:
      
      1. **ANOVA generalizes two-sample methods**: MSE extends the pooled variance concept to k ‚â• 2 groups
      
      2. **Consistent variance estimation**: Whether using two-sample t-tests or ANOVA, we use the same approach‚Äîpooling information from all groups to estimate the common variance
      
      3. **Unified framework**: ANOVA with k = 2 is mathematically equivalent to the pooled two-sample t-test (as we'll see in Section 12.3)
      
      4. **Equal variance assumption**: Both methods assume equal population variances; MSE is only an unbiased estimator of œÉ¬≤ when this assumption holds

----

.. admonition:: Exercise 7: Interpreting Sums of Squares
   :class: note

   A computer scientist compares the runtime (in seconds) of sorting algorithms on datasets of similar size. After collecting data:

   - SSA = 2450 (between-algorithm variability)
   - SSE = 1890 (within-algorithm variability)
   - k = 5 algorithms
   - n = 60 total test runs

   a. What proportion of the total variability is explained by differences between algorithms?

   b. What proportion is due to random variation within algorithms?

   c. Based on these proportions alone (without a formal test), do you expect differences between algorithms to be statistically significant? Explain.

   d. Calculate MSA and MSE. What does the ratio MSA/MSE suggest?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Proportion explained by algorithms**

      First, calculate SST:

      .. math::
         \text{SST} = \text{SSA} + \text{SSE} = 2450 + 1890 = 4340

      Proportion explained by between-group differences:

      .. math::
         \frac{\text{SSA}}{\text{SST}} = \frac{2450}{4340} = 0.565 = 56.5\%

      **Part (b): Proportion due to within-group variation**

      .. math::
         \frac{\text{SSE}}{\text{SST}} = \frac{1890}{4340} = 0.435 = 43.5\%

      (Or simply: 100% - 56.5% = 43.5%)

      **Part (c): Prediction about significance**

      With 56.5% of variation attributable to algorithm differences, there's a good chance the F-test will be significant. The between-group variability is larger than the within-group variability, which is the pattern we'd expect when group means truly differ.

      However, we need the formal F-test to account for degrees of freedom and determine if this proportion is large enough to be statistically significant at a given Œ± level.

      **Part (d): MSA/MSE ratio**

      .. math::
         df_A = k - 1 = 4, \quad df_E = n - k = 55

      .. math::
         \text{MSA} = \frac{2450}{4} = 612.5

      .. math::
         \text{MSE} = \frac{1890}{55} = 34.36

      .. math::
         F = \frac{\text{MSA}}{\text{MSE}} = \frac{612.5}{34.36} = 17.83

      This F-ratio of 17.83 is much larger than 1, strongly suggesting that the between-group variance exceeds what we'd expect under H‚ÇÄ. This will almost certainly lead to rejecting H‚ÇÄ.

      **R verification:**

      .. code-block:: r

         SSA <- 2450
         SSE <- 1890
         k <- 5
         n <- 60
         
         SST <- SSA + SSE  # 4340
         SSA/SST  # 0.565 (56.5% explained)
         
         df_A <- k - 1  # 4
         df_E <- n - k  # 55
         MSA <- SSA/df_A  # 612.5
         MSE <- SSE/df_E  # 34.36
         F_stat <- MSA/MSE  # 17.83
         
         pf(F_stat, df_A, df_E, lower.tail = FALSE)  # Very small p-value

----

.. admonition:: Exercise 8: Building an ANOVA Table from Raw Data
   :class: note

   A biomedical engineer tests the accuracy (% error) of three blood glucose monitoring devices. Each device is tested on 5 blood samples:

   **Device A:** 2.1, 3.4, 2.8, 1.9, 2.3
   
   **Device B:** 4.2, 3.8, 4.5, 3.9, 4.1
   
   **Device C:** 2.9, 3.2, 2.5, 3.5, 2.4

   a. Calculate :math:`\bar{x}_{1.}, \bar{x}_{2.}, \bar{x}_{3.}` and :math:`\bar{x}_{..}`.

   b. Calculate :math:`s_1, s_2, s_3`.

   c. Check the equal variance assumption using both the SD ratio **and** side-by-side boxplots.

   d. Check the normality assumption using faceted histograms and/or QQ-plots.

   e. Calculate SSA, SSE, and SST.

   f. Construct the complete ANOVA table (without F and p-value for now).

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Group means and grand mean**

      Device A: :math:`\bar{x}_{1.} = \frac{2.1 + 3.4 + 2.8 + 1.9 + 2.3}{5} = \frac{12.5}{5} = 2.50`

      Device B: :math:`\bar{x}_{2.} = \frac{4.2 + 3.8 + 4.5 + 3.9 + 4.1}{5} = \frac{20.5}{5} = 4.10`

      Device C: :math:`\bar{x}_{3.} = \frac{2.9 + 3.2 + 2.5 + 3.5 + 2.4}{5} = \frac{14.5}{5} = 2.90`

      Grand mean: :math:`\bar{x}_{..} = \frac{12.5 + 20.5 + 14.5}{15} = \frac{47.5}{15} = 3.167`

      **Part (b): Sample standard deviations**

      Device A: :math:`s_1 = \sqrt{\frac{(2.1-2.5)^2 + (3.4-2.5)^2 + (2.8-2.5)^2 + (1.9-2.5)^2 + (2.3-2.5)^2}{4}}`
      
      :math:`= \sqrt{\frac{0.16 + 0.81 + 0.09 + 0.36 + 0.04}{4}} = \sqrt{\frac{1.46}{4}} = \sqrt{0.365} = 0.604`

      Device B: :math:`s_2 = \sqrt{\frac{(4.2-4.1)^2 + (3.8-4.1)^2 + (4.5-4.1)^2 + (3.9-4.1)^2 + (4.1-4.1)^2}{4}}`
      
      :math:`= \sqrt{\frac{0.01 + 0.09 + 0.16 + 0.04 + 0}{4}} = \sqrt{0.075} = 0.274`

      Device C: :math:`s_3 = \sqrt{\frac{(2.9-2.9)^2 + (3.2-2.9)^2 + (2.5-2.9)^2 + (3.5-2.9)^2 + (2.4-2.9)^2}{4}}`
      
      :math:`= \sqrt{\frac{0 + 0.09 + 0.16 + 0.36 + 0.25}{4}} = \sqrt{0.215} = 0.464`

      **Part (c): Equal variance check**

      *Numerical check (SD ratio):*

      .. math::
         \frac{\max(s_i)}{\min(s_i)} = \frac{0.604}{0.274} = 2.20

      Since 2.20 > 2, **the equal variance assumption is questionable** ‚ö†Ô∏è

      *Visual check (boxplots):*

      .. code-block:: r

         library(ggplot2)
         
         # Create data frame
         glucose <- data.frame(
           Error = c(2.1, 3.4, 2.8, 1.9, 2.3,
                     4.2, 3.8, 4.5, 3.9, 4.1,
                     2.9, 3.2, 2.5, 3.5, 2.4),
           Device = factor(rep(c("A", "B", "C"), each = 5))
         )
         
         # Side-by-side boxplots
         ggplot(glucose, aes(x = Device, y = Error, fill = Device)) +
           stat_boxplot(geom = "errorbar", width = 0.3) +
           geom_boxplot() +
           stat_summary(fun = mean, geom = "point", shape = 18, 
                        size = 3, color = "black") +
           ggtitle("Blood Glucose Device Accuracy") +
           xlab("Device") +
           ylab("Measurement Error (%)") +
           theme_minimal() +
           theme(legend.position = "none")

      The boxplots show Device A has noticeably greater spread than Device B. With such small samples (n=5), this may still be acceptable, but should be noted as a limitation.

      .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/Exercises/ch12-2/fig5a_glucose_boxplots.png
         :alt: Blood glucose device boxplots
         :align: center
         :width: 70%

         Side-by-side boxplots for equal variance assessment.

      **Part (d): Normality check**

      With only n = 5 per group, formal normality assessment is limited. Visual checks:

      .. code-block:: r

         # Faceted histograms (limited with n=5)
         xbar <- tapply(glucose$Error, glucose$Device, mean)
         s <- tapply(glucose$Error, glucose$Device, sd)
         
         glucose$normal.density <- mapply(function(val, dev) {
           dnorm(val, mean = xbar[dev], sd = s[dev])
         }, glucose$Error, glucose$Device)
         
         ggplot(glucose, aes(x = Error)) +
           geom_histogram(aes(y = after_stat(density)), 
                          bins = 4, fill = "grey", col = "black") +
           geom_density(col = "red", linewidth = 1) +
           geom_line(aes(y = normal.density), col = "blue", linewidth = 1) +
           facet_wrap(~ Device) +
           ggtitle("Normality Check: Histograms by Device") +
           xlab("Measurement Error (%)") +
           ylab("Density") +
           theme_minimal()
         
         # Faceted QQ-plots
         ggplot(glucose, aes(sample = Error)) +
           stat_qq() +
           stat_qq_line(color = "red", linewidth = 1) +
           facet_wrap(~ Device) +
           ggtitle("Normality Check: QQ-Plots by Device") +
           xlab("Theoretical Quantiles") +
           ylab("Sample Quantiles") +
           theme_minimal()

      .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/Exercises/ch12-2/fig5b_glucose_hist.png
         :alt: Blood glucose device histograms
         :align: center
         :width: 90%

         Faceted histograms with kernel density (red) and normal overlay (blue).

      .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/Exercises/ch12-2/fig5c_glucose_qq.png
         :alt: Blood glucose device QQ-plots
         :align: center
         :width: 90%

         Faceted QQ-plots for normality assessment.

      **Note**: With only 5 observations per group, histograms are very rough and QQ-plots have few points. For such small samples, normality is difficult to assess visually. We generally rely on the robustness of ANOVA to moderate non-normality when sample sizes are equal.

      **Part (e): Sums of Squares**

      **SSA** (between groups):

      .. math::
         \text{SSA} = 5(2.50 - 3.167)^2 + 5(4.10 - 3.167)^2 + 5(2.90 - 3.167)^2

      .. math::
         = 5(0.445) + 5(0.869) + 5(0.071) = 2.22 + 4.35 + 0.36 = 6.93

      **SSE** (within groups):

      .. math::
         \text{SSE} = (5-1)(0.365) + (5-1)(0.075) + (5-1)(0.215) = 1.46 + 0.30 + 0.86 = 2.62

      **SST** (total):

      .. math::
         \text{SST} = \text{SSA} + \text{SSE} = 6.93 + 2.62 = 9.55

      **Part (f): ANOVA Table**

      .. list-table::
         :header-rows: 1
         :widths: 20 15 20 20

         * - Source
           - df
           - SS
           - MS
         * - Device
           - 2
           - 6.93
           - 3.47
         * - Error
           - 12
           - 2.62
           - 0.22
         * - Total
           - 14
           - 9.55
           - 

      **R verification:**

      .. code-block:: r

         A <- c(2.1, 3.4, 2.8, 1.9, 2.3)
         B <- c(4.2, 3.8, 4.5, 3.9, 4.1)
         C <- c(2.9, 3.2, 2.5, 3.5, 2.4)
         
         xbar <- c(mean(A), mean(B), mean(C))  # 2.50, 4.10, 2.90
         grand_mean <- mean(c(A, B, C))  # 3.167
         s <- c(sd(A), sd(B), sd(C))  # 0.604, 0.274, 0.464
         
         max(s)/min(s)  # 2.20 - borderline
         
         n <- c(5, 5, 5)
         SSA <- sum(n * (xbar - grand_mean)^2)  # 6.93
         SSE <- sum((n-1) * s^2)  # 2.62
         SST <- SSA + SSE  # 9.55
         
         MSA <- SSA / 2  # 3.47
         MSE <- SSE / 12  # 0.22

----

.. admonition:: Exercise 9: Conceptual Understanding of Variability Sources
   :class: note

   Answer each question with a brief explanation.

   a. Under the null hypothesis :math:`H_0: \mu_1 = \mu_2 = ... = \mu_k`, what should the expected value of MSA be approximately equal to? Why?

   b. Is the expected value of MSE affected by whether :math:`H_0` is true or false? Explain.

   c. A researcher obtains MSA = 15.2 and MSE = 48.7. What does this suggest about the null hypothesis?

   d. Can SSA ever be larger than SST? Why or why not?

   e. If all observations in a dataset were identical (e.g., all equal to 50), what would SST, SSA, and SSE each equal?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Expected value of MSA under H‚ÇÄ**

      Under :math:`H_0`, :math:`E(\text{MSA}) \approx \sigma^2`.

      When all population means are equal, the only source of variation in group sample means is random sampling variability. The group means :math:`\bar{X}_{i.}` will vary around the common population mean, and this variation is determined by :math:`\sigma^2`. Thus, MSA estimates :math:`\sigma^2` when H‚ÇÄ is true.

      **Part (b): E(MSE) and H‚ÇÄ**

      **No, E(MSE) = œÉ¬≤ regardless of whether H‚ÇÄ is true or false.**

      MSE measures variability within groups‚Äîhow observations deviate from their respective group means. This within-group variability is determined by œÉ¬≤ (assuming equal variances) and is not affected by whether the population means differ. MSE is always an unbiased estimator of œÉ¬≤.

      **Part (c): MSA = 15.2, MSE = 48.7**

      This suggests there is **no evidence against H‚ÇÄ**.

      The ratio F = MSA/MSE = 15.2/48.7 = 0.31, which is less than 1. This indicates that the between-group variability is actually smaller than the within-group variability‚Äîthe opposite of what we'd expect if the means truly differed. We would fail to reject H‚ÇÄ.

      **Part (d): Can SSA > SST?**

      **No, SSA can never exceed SST.**

      By the fundamental identity: SST = SSA + SSE

      Since SSE ‚â• 0 (it's a sum of squared deviations), we must have SSA ‚â§ SST. Equality (SSA = SST) occurs only when SSE = 0, which happens only if all observations within each group are identical to their group mean.

      **Part (e): All observations identical**

      If all observations equal 50:

      - All group means equal 50
      - The grand mean equals 50
      - :math:`\text{SST} = \sum\sum(x_{ij} - 50)^2 = 0`
      - :math:`\text{SSA} = \sum n_i(50 - 50)^2 = 0`
      - :math:`\text{SSE} = \sum\sum(x_{ij} - 50)^2 = 0`

      All sums of squares equal zero because there is no variability whatsoever.

----

Additional Practice Problems
----------------------------

**True/False Questions** (1 point each)

1. SSE measures the variability in the data that can be explained by group membership.

   ‚ìâ or ‚íª

2. If SSA = 500 and SST = 1200, then SSE = 700.

   ‚ìâ or ‚íª

3. MSE is an unbiased estimator of œÉ¬≤ only when the null hypothesis is true.

   ‚ìâ or ‚íª

4. The degrees of freedom for SST always equals the sum of df_A and df_E.

   ‚ìâ or ‚íª

5. A small value of SSA relative to SSE suggests that group means differ substantially.

   ‚ìâ or ‚íª

**Multiple Choice Questions** (2 points each)

6. In an ANOVA with k = 4 groups and n = 48 total observations, df_E equals:

   ‚í∂ 3
   
   ‚í∑ 44
   
   ‚í∏ 47
   
   ‚íπ 48

7. The formula :math:`\text{SSE} = \sum_{i=1}^{k}(n_i - 1)s_i^2` requires which assumption?

   ‚í∂ Equal sample sizes
   
   ‚í∑ Normal populations
   
   ‚í∏ Equal population variances
   
   ‚íπ None‚Äîit's always valid

8. If MSA = 250 and df_A = 5, then SSA equals:

   ‚í∂ 50
   
   ‚í∑ 255
   
   ‚í∏ 1250
   
   ‚íπ 245

9. Which quantity can be used to estimate the common population standard deviation œÉ?

   ‚í∂ ‚àöMSA
   
   ‚í∑ ‚àöMSE
   
   ‚í∏ ‚àöSST
   
   ‚íπ SSE/SST

10. In the ANOVA model :math:`X_{ij} = \mu_i + \varepsilon_{ij}`, the term :math:`\varepsilon_{ij}` represents:

    ‚í∂ The population mean for group i
    
    ‚í∑ The random error for observation j in group i
    
    ‚í∏ The between-group variability
    
    ‚íπ The grand mean

11. If all group sample means were identical (:math:`\bar{x}_{1.} = \bar{x}_{2.} = ... = \bar{x}_{k.}`), what would SSA equal?

    ‚í∂ SST
    
    ‚í∑ SSE
    
    ‚í∏ 0
    
    ‚íπ Cannot determine

.. dropdown:: Answers to Practice Problems
   :class-container: sd-border-success

   **True/False Answers:**

   1. **False** ‚Äî SSE measures within-group variability (unexplained by group membership). SSA measures between-group variability (explained by groups).

   2. **True** ‚Äî From SST = SSA + SSE: SSE = 1200 - 500 = 700.

   3. **False** ‚Äî MSE is an unbiased estimator of œÉ¬≤ regardless of whether H‚ÇÄ is true, as long as the equal variance assumption holds.

   4. **True** ‚Äî df_T = df_A + df_E is part of the fundamental ANOVA identity.

   5. **False** ‚Äî Small SSA relative to SSE suggests group means are similar (little between-group variation). Large SSA suggests means differ.

   **Multiple Choice Answers:**

   6. **‚í∑** ‚Äî df_E = n - k = 48 - 4 = 44

   7. **‚íπ** ‚Äî The formula is always valid; it's a mathematical identity for computing SSE from group statistics.

   8. **‚í∏** ‚Äî SSA = MSA √ó df_A = 250 √ó 5 = 1250

   9. **‚í∑** ‚Äî œÉÃÇ = ‚àöMSE is the standard estimator for the common standard deviation.

   10. **‚í∑** ‚Äî Œµ_ij represents the random error component for observation j in group i.

   11. **‚í∏** ‚Äî If all group means equal the grand mean, then (xÃÑ_i. - xÃÑ_..)¬≤ = 0 for all i, so SSA = 0.