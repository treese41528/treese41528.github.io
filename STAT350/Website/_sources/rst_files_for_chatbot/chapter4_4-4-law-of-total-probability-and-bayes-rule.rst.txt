.. _4-4-law-of-total-probability-and-bayes-rule:


.. raw:: html

   <div class="video-placeholder">
     <iframe
       src="https://www.youtube.com/embed/4XDj9VRCVtE?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
       allowfullscreen>
     </iframe>
   </div>

.. admonition:: Slides üìä
   :class: tip
 
   `Download Chapter 4 slides (PPTX) <https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/
   stat-350-assets/slides/Chapter%204%20Probability/L6-8-Probability%20%28Chapter%204%29_AC.pptx>`_
   
Law of Total Probability and Bayes' Rule
===============================================

When we make decisions under uncertainty, we often need to **revise our probability assessments 
as new information emerges**. Medical diagnoses, legal judgments, and even everyday decisions typically 
involve updating our beliefs based on partial evidence. In this chapter, we'll develop the 
foundational principles of **Bayes' Rule**, which provides a
framework for this fundamental process of learning from evidence.

.. admonition:: Road Map üß≠
   :class: important

   ‚Ä¢ Define **partitions** of the sample space and derive the **law of partitions**.
   ‚Ä¢ Build upon this to establish the **law of total probability**.
   ‚Ä¢ Develop **Bayes' rule** for inverting conditional probabilities.

Law of Partitions
---------------------------------------------------

What is a Partition?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A collection of events :math:`\{A_1, A_2, \cdots, A_n\}` 
forms a **partition** of the sample space :math:`\Omega` if the following
two conditions are satisfied.

1. The events are **mutually exclusive**: 
   
   .. math:: A_i \cap A_j = \emptyset \text{ for all } i \neq j.

2. The events are **exhaustive**: 
   
   .. math:: A_1 \cup A_2 \cup \cdots \cup A_n = \Omega.

In other words, a partition divides the sample space into **non-overlapping** pieces that, 
when combined, **reconstruct the entire space**. You can think of a partition as pizza slices‚Äîeach 
slice represents an event, the slices do not overlap, and together they make up a whole pizza.

The law of partitions provides a way to calculate 
the probability of a new event by examining how it intersects with each part of a partition.

.. admonition:: Note ‚úèÔ∏è
   :class: important

   The simplest example of a partition consists of just two events: any **event** :math:`A`
   **and its complement** :math:`A'`. 
   These two events are

   * mutually exclusive because :math:`A \cap A' = \emptyset`, and 
   * exhaustive because together they cover the entire sample space (:math:`A \cup A' = \Omega`).

Law of Partitions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If :math:`\{A_1, A_2, \cdots, A_n\}` forms a partition of the sample space :math:`\Omega`, 
then for any event :math:`B` in the same sample space:

.. math::

   P(B) = \sum_{i=1}^{n} P(A_i \cap B)

What Does It Say?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. _law-of-partitions:
.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter4/law-of-partitions.png
   :alt: Visual representation of the law of partitions
   :align: center
   :width: 50%

   Law of partitions

Take a partition that consists of three events as in :numref:`law-of-partitions`.
Then, the Law of Partitions can be expanded to

.. math:: P(B) = P(A_1 \cap B) + P(A_2 \cap B) + P(A_3 \cap B).

The left-hand side of the equation points to the *relative area* of the whole blue region,
while each term on the right-hand side points to the *relative area* of a smaller piece created 
by **the overlap of** :math:`B` **with one of the events in the partion**.

The core message of the Law of Partitions is quite simple; the probability of the whole is equal to the sum of
the probabilities of its parts.

Law of Total Probability
-----------------------------

The Law of Total Probability takes the Law of Partitions one step further by
**rewriting the intersection probabilities using the general multiplication rule**.

.. admonition:: Reminderüîé: The General Multiplication Rule
   :class: important

   For any two events :math:`C` and :math:`D`, :math:`P(C \cap D) = P(C|D) P(D) = P(D|C) P(C).`

Statement
~~~~~~~~~~~~~~

If :math:`\{A_1, A_2, \cdots, A_n\}` forms a partition of the sample space :math:`\Omega`, 
then for any event :math:`B \subseteq \Omega`,

.. math:: P(B) = \sum_{i=1}^{n} P(B|A_i) P(A_i).

What Does It Say?
~~~~~~~~~~~~~~~~~~~~


.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter4/law-of-partitions.png
   :alt: Visual representation of the law of total probability
   :align: center
   :width: 50%

   Law of Total Probability

Let us continue to use the simple three-event partition. The Law of Total Probability says

.. math:: P(B) = P(B|A_1)P(A_1) + P(B|A_2)P(A_2) + P(B|A_3)(PA_3).

The Law of Total Probability now expresses the probability 
of event :math:`B` **as a weighted average of conditional probabilities**. 
Each weight :math:`P(A_i)` represents the probability of a particular part in the sample space, and each conditional 
probability :math:`P(B|A_i)` represents the likelihood of :math:`B` given that we are in that part.



The Law of Total Probability on a Tree diagram
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Recall that in a tree diagram, the set of branches extending from the same node must 
represent *all possible outcomes* given the preceding path. This requirement is, in fact, another way of 
saying that **these branches must form a partition**. As a result, a tree diagram 
provides an ideal setting for applying the Law of Total Probability.

Computing a single-stage probability :math:`P(B)` using the Law of Total Probability is equivalent to

1. finding the path probabilties of all paths involving :math:`B`,
2. then summing the probabilities.

Try writing these steps down in mathematical notation and confirm that
they are identical to applying the Law of Total Probability directly.

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter4/LTP-tree.png
   :alt: the law of total probability on a tree diagram
   :align: center
   :figwidth: 50%

   Using the Law of Total Probability with a tree diagram

.. admonition:: Exampleüí°: The Law of Partitions and the Law of Total Probability
   :class: note

   .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter4/full-tree-indy.png
      :alt: The tree diagram for the Indianapolis problem
      :align: center
      :figwidth: 60%

      Tree diagram for the Indianapolis problem


   Recall the Indianapolis example from the previous section. In this problem,
   what is the probabity that it rains?

   .. math::

      P(R) &= P(R \cap Sun) + P(R \cap Sat) + P(R \cap Fri) \\
      &= P(Sun)P(R|Sun) + P(Sat)P(R|Sat) + P(Fri)P(R|Fri)\\
      &= 1/10 + 1/8 + 3/40 \\
      &= 0.1 + 0.125 + 0.075 \\
      &= 0.3

   * First equality uses the Law of Partitions. The second equality uses the Law of Total Probability.
   * Confirm that the mathematical steps and the final outcome are identical when the tree diagram is used.

Bayes' Rule
--------------

Bayes' rule allows us to **invert** conditional probabilities. That is,
it allows us to compute :math:`P(A|B)` from our knowledge of :math:`P(B|A).`

Statement
~~~~~~~~~~~~~~

If :math:`\{A_1, A_2, \cdots, A_n\}` forms a partition of the sample space :math:`\Omega`, 
and :math:`B` is an event with :math:`P(B) > 0`, then for any :math:`i=1,2,\cdots,n`,

.. math::

   P(A_i|B) = \frac{P(B|A_i)P(A_i)}{\sum_{j=1}^{n} P(B|A_j)P(A_j)}.

For the simplified case of a three-event partition, Bayes' rule for
:math:`P(A_1|B)` is:

.. math::

   P(A_1|B) = \frac{P(B|A_1)P(A_1)}{P(B|A_1)P(A_1) + P(B|A_2)P(A_2) + P(B|A_3)P(A_3)}.

Graphically, this equation represents the ratio of the area of the first blue piece (:math:`A_1 \cap B`) over
the whole area of :math:`B` in :numref:`bayes-rule`.

.. _bayes-rule:
.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter4/law-of-partitions.png
   :alt: Visual representation of the law of total probability
   :align: center
   :width: 40%

   Visual aid for Bayes' Rule


Derivation of Bayes' Rule
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. math::
   P(A_i|B) = \frac{P(A_i \cap B)}{P(B)} = \frac{P(A_i \cap B)}{\sum_{j=1}^{n} P(B|A_j)P(A_j)} 
   = \frac{P(B|A_i)P(A_i)}{\sum_{j=1}^{n} P(B|A_j)P(A_j)} 

* First equality: definition of conditional probability
* Second equality: Law of Total Probability for the denominator
* Third equality: the general multiplication rule for the numerator



.. admonition:: Exampleüí°: Bayes' Rule
   :class: note 

   The Indianapolis example is continued. Knowing that it didn't rain on the day Glen and Jia went 
   to Indianapolis, find the probability that this was Friday.

   .. math::

      P(Fri|R') = \frac{P(Fri \cap R')}{P(R')} = \frac{P(Fri \cap R')}{1 - P(R)} = \frac{11/120}{1 - 0.3} \approx 0.131

   * :math:`P(R')` can be computed directly using the tree diagram or
     the Law of Total Probability. However, using the complement rule is
     more convenient since we already have :math:`P(R)` from the previous part.

Understanding the Bayesian Approach to Probability through Bayes' Rule
--------------------------------------------------------------------------

Bayes' rule forms the foundation of the Bayesian approach to probability, which 
interprets probabilities as degrees of belief that can be updated as new evidence emerges.

Each component of Bayes' rule has a Bayesian interpretation:

1. :math:`P(A_i)`: the **prior** probability

   The initial assessment of the probability of event :math:`A_1`

2. :math:`P(B|A_i)`: the likelihood

   The probability of observing a new evidence :math:`B` given that :math:`A_1` holds. 
   This measures **how consistent** the evidence is with :math:`A_i`.

3. :math:`P(A_i|B)`: the **posterior** probability

   The updated probability of :math:`A_i` accounting for the evidence :math:`B`. 

4. :math:`P(B)`: the normalizing constant
   
   Once the evidence :math:`B` is observed, the sample space shrinks to
   only the region that would have made :math:`B` possible. Computing a posterior
   probability involves a step where we divide probabilities by :math:`P(B)`, the
   size of a new *whole* (see the second step of deriving Bayes rule).

As we gather more evidence, we can repeatedly apply Bayes' rule, using the posterior 
probability from one calculation as the prior probability for the next. This iterative 
process allows our probability assessments to continuously improve as we incorporate 
new information.

.. admonition:: Comprehensive Exampleüí°: Medical Testing
   :class: note
   
   Consider a disease that affects a small percentage of the population and a diagnostic test used to detect it.

   - Let :math:`D` be the event that a person has the disease.
   - Let :math:`+`: be the event that the test gives a positive result.
   - Define :math:`D'` and :math:`-` as the complements of :math:`D` and :math:`+`, respectively

   Given these events, we can identify:

   - :math:`P(D)`: The prevalence of the disease in the population (**prior** probability)
   - :math:`P(+|D)`: The sensitivity of the test (true positive rate)
   - :math:`P(+|D')`: The false positive rate (1 - specificity)

   What doctors and patients typically want to know is :math:`P(D|+)`, the probability that a person 
   has the disease given a positive test result. This **posterior** probability can be calculated using Bayes' rule:

   .. math::

      P(D|+) = \frac{P(+|D)P(D)}{P(+|D) P(D) + P(+|D')P(D')}

   Suppose a disease affects 1% of the population, the test has a sensitivity of 95%, 
   and a specificity of 90%. What is the probability that someone with a positive 
   test result actually has the disease?

   **Step 1**: Write the building blocks in mathematical notation

   * :math:`P(D) = 0.01`
   * :math:`P(+|D) = 0.95`
   * :math:`P(+|D') = 1-P(-|D') = 1 - 0.9 = 0.1`
   
   **Step 2**: Compute the posterior probability

   .. math::

      P(D|+) &= \frac{(0.95)(0.01)}{(0.95)(0.01) + (0.1)(0.99)} \\
      &= \frac{0.0095}{0.0095 + 0.099} \\
      &= \frac{0.0095}{0.1085} \\
      &\approx 0.0876


   Despite the test being quite accurate (95% sensitivity, 90% specificity), 
   the probability that a positive result indicates disease is less than 9%. This illustrates the importance of considering the base rate (prior probability) when interpreting 
   test results, especially for rare conditions. Even a very accurate test will generate many false positives 
   when applied to a population where the condition is uncommon.

   * Also try solving this problem using a tree diagram, and confirm that the results are consistent.

Bringing It All Together
--------------------------

.. admonition:: Key Takeaways üìù
   :class: important
   
   #. The **Law of Partitions** decomposes the probability of an event across a partition.
   
   #. The **Law of Total Probability** expresses an event's probability as a weighted 
      average of conditional probabilities.
   
   #. **Bayes' rule** lets us calculate "inverse" conditional probabilities.
   
   #. Tree diagrams serve as an assisting tool for the three rules above.
   
   #. Bayes' rule forms the **foundation of the Bayesian approach** to probability.

Exercises
---------

These exercises develop your skills in applying the Law of Total Probability and Bayes' Rule to solve multi-stage probability problems.

.. admonition:: Exercise 1: Identifying Partitions
   :class: note

   For each scenario, determine whether the given collection of events forms a valid partition of the sample space. If not, explain which condition (mutually exclusive or exhaustive) is violated.

   a. Sample space: All students in a class.
      Events: :math:`A_1` = "freshman", :math:`A_2` = "sophomore", :math:`A_3` = "junior", :math:`A_4` = "senior"

   b. Sample space: All possible outcomes when rolling a six-sided die.
      Events: :math:`B_1` = "even number", :math:`B_2` = "odd number"

   c. Sample space: All employees at a company.
      Events: :math:`C_1` = "works in engineering", :math:`C_2` = "works in marketing", :math:`C_3` = "has been with the company more than 5 years"

   d. Sample space: All real numbers from 0 to 100.
      Events: :math:`D_1 = [0, 50)`, :math:`D_2 = [50, 100]`

   e. Sample space: Results of a software test.
      Events: :math:`E_1` = "test passes", :math:`E_2` = "test fails with minor error"

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Valid Partition** ‚úì

      - **Mutually exclusive**: A student can only be in one class year at a time. ‚úì
      - **Exhaustive**: Every student must be a freshman, sophomore, junior, or senior. ‚úì

      **Part (b): Valid Partition** ‚úì

      - **Mutually exclusive**: A number cannot be both even and odd. ‚úì
      - **Exhaustive**: Every integer is either even or odd. ‚úì

      Note: :math:`B_1` and :math:`B_2` form the simplest partition ‚Äî an event and its complement.

      **Part (c): NOT a Valid Partition** ‚úó

      - **Mutually exclusive**: VIOLATED. An engineer could also have been with the company more than 5 years. The events overlap.
      - **Exhaustive**: VIOLATED. An employee in finance with 2 years tenure wouldn't be in any of these events.

      **Part (d): Valid Partition** ‚úì

      - **Mutually exclusive**: :math:`[0, 50)` and :math:`[50, 100]` don't overlap (50 is only in the second set). ‚úì
      - **Exhaustive**: Together they cover all numbers from 0 to 100. ‚úì

      **Part (e): NOT a Valid Partition** ‚úó

      - **Mutually exclusive**: These could be mutually exclusive if properly defined. ‚úì
      - **Exhaustive**: VIOLATED. What about "test fails with major error" or "test crashes"? The events don't cover all possible outcomes.

----

.. admonition:: Exercise 2: Law of Total Probability
   :class: note

   A data center has three server clusters that handle incoming requests:

   - Cluster A handles 50% of all requests
   - Cluster B handles 30% of all requests
   - Cluster C handles 20% of all requests

   Due to different hardware and configurations, the probability of a request being processed successfully varies by cluster:

   - Cluster A: 99% success rate
   - Cluster B: 97% success rate
   - Cluster C: 95% success rate

   a. Define appropriate events and write out the given information using probability notation.

   b. Use the Law of Total Probability to find the overall probability that a randomly selected request is processed successfully.

   c. What is the probability that a randomly selected request fails?

   d. Draw a tree diagram representing this situation and verify your answer to part (b).

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Define Events and Notation**

      Let:
      
      - :math:`A` = request handled by Cluster A
      - :math:`B` = request handled by Cluster B
      - :math:`C` = request handled by Cluster C
      - :math:`S` = request processed successfully

      Given information:
      
      - :math:`P(A) = 0.50`, :math:`P(B) = 0.30`, :math:`P(C) = 0.20`
      - :math:`P(S|A) = 0.99`, :math:`P(S|B) = 0.97`, :math:`P(S|C) = 0.95`

      Note: :math:`\{A, B, C\}` forms a partition since clusters are mutually exclusive and exhaustive.

      **Part (b): Law of Total Probability**

      .. math::

         P(S) &= P(S|A)P(A) + P(S|B)P(B) + P(S|C)P(C) \\
         &= (0.99)(0.50) + (0.97)(0.30) + (0.95)(0.20) \\
         &= 0.495 + 0.291 + 0.190 \\
         &= 0.976

      The overall success rate is **97.6%**.

      **Part (c): Probability of Failure**

      Using the complement rule:

      .. math::

         P(S') = 1 - P(S) = 1 - 0.976 = 0.024

      The overall failure rate is **2.4%**.

      **Part (d): Tree Diagram**

      .. code-block:: text

                           [Start]
                          /   |   \
                   0.50  /    |0.30 \  0.20
                        /     |      \
                      [A]    [B]     [C]
                     / \    /  \    /  \
              0.99  /   \  /    \  /    \ 0.05
                   /  0.01\0.97 0.03\0.95  \
                 [S]  [S'] [S] [S'] [S]  [S']
                  |     |   |    |   |     |
               0.495  0.005 0.291 0.009 0.190 0.010

      Sum of success paths: :math:`0.495 + 0.291 + 0.190 = 0.976` ‚úì

----

.. admonition:: Exercise 3: Bayes' Rule ‚Äî Manufacturing
   :class: note

   An electronics manufacturer sources microprocessors from three suppliers:

   - Supplier X provides 40% of processors with a 2% defect rate
   - Supplier Y provides 35% of processors with a 3% defect rate
   - Supplier Z provides 25% of processors with a 5% defect rate

   A randomly selected processor is found to be defective.

   a. What is the probability that it came from Supplier X?

   b. What is the probability that it came from Supplier Y?

   c. What is the probability that it came from Supplier Z?

   d. Verify that your answers to parts (a), (b), and (c) sum to 1.

   e. Which supplier is most likely responsible for a defective processor? Is this the same as the supplier with the highest defect rate?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Setup:**

      Let :math:`X, Y, Z` denote the suppliers and :math:`D` = "processor is defective."

      Given:
      
      - :math:`P(X) = 0.40`, :math:`P(D|X) = 0.02`
      - :math:`P(Y) = 0.35`, :math:`P(D|Y) = 0.03`
      - :math:`P(Z) = 0.25`, :math:`P(D|Z) = 0.05`

      **First, find P(D) using Law of Total Probability:**

      .. math::

         P(D) &= P(D|X)P(X) + P(D|Y)P(Y) + P(D|Z)P(Z) \\
         &= (0.02)(0.40) + (0.03)(0.35) + (0.05)(0.25) \\
         &= 0.008 + 0.0105 + 0.0125 \\
         &= 0.031

      **Part (a): P(X|D)**

      .. math::

         P(X|D) = \frac{P(D|X)P(X)}{P(D)} = \frac{(0.02)(0.40)}{0.031} = \frac{0.008}{0.031} \approx 0.2581

      **Part (b): P(Y|D)**

      .. math::

         P(Y|D) = \frac{P(D|Y)P(Y)}{P(D)} = \frac{(0.03)(0.35)}{0.031} = \frac{0.0105}{0.031} \approx 0.3387

      **Part (c): P(Z|D)**

      .. math::

         P(Z|D) = \frac{P(D|Z)P(Z)}{P(D)} = \frac{(0.05)(0.25)}{0.031} = \frac{0.0125}{0.031} \approx 0.4032

      **Part (d): Verification**

      :math:`P(X|D) + P(Y|D) + P(Z|D) = 0.2581 + 0.3387 + 0.4032 = 1.0000` ‚úì

      This must be true since :math:`\{X, Y, Z\}` partitions the sample space.

      **Part (e): Interpretation**

      **Supplier Z** is most likely responsible for a defective processor (40.32% probability).

      This **is** the same as the supplier with the highest defect rate (5%). However, it's not always the case! If Supplier Z only provided 5% of processors instead of 25%, the answer would change. Bayes' Rule accounts for both:
      
      - The defect rate (likelihood)
      - The proportion of supply (prior)

----

.. admonition:: Exercise 4: Bayes' Rule ‚Äî Diagnostic Testing
   :class: note

   A new screening test for a rare genetic condition is being evaluated. The condition affects 1 in 500 people in the general population. Clinical trials show:

   - **Sensitivity** (true positive rate): 98% ‚Äî P(+|Disease) = 0.98
   - **Specificity** (true negative rate): 96% ‚Äî P(‚àí|No Disease) = 0.96

   a. Calculate the probability that a person who tests positive actually has the condition. (This is called the **positive predictive value**.)

   b. Calculate the probability that a person who tests negative does not have the condition. (This is called the **negative predictive value**.)

   c. If the test is used to screen 10,000 people, approximately how many will test positive? Of those, how many actually have the condition?

   d. Why is the positive predictive value so much lower than the sensitivity, even though both the sensitivity and specificity are quite high?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Setup:**

      - :math:`P(D) = 1/500 = 0.002` (prevalence)
      - :math:`P(D') = 0.998`
      - :math:`P(+|D) = 0.98` (sensitivity)
      - :math:`P(-|D') = 0.96` (specificity)
      - :math:`P(+|D') = 1 - 0.96 = 0.04` (false positive rate)

      **Part (a): Positive Predictive Value P(D|+)**

      First, find P(+) using Law of Total Probability:

      .. math::

         P(+) &= P(+|D)P(D) + P(+|D')P(D') \\
         &= (0.98)(0.002) + (0.04)(0.998) \\
         &= 0.00196 + 0.03992 \\
         &= 0.04188

      Now apply Bayes' Rule:

      .. math::

         P(D|+) = \frac{P(+|D)P(D)}{P(+)} = \frac{(0.98)(0.002)}{0.04188} = \frac{0.00196}{0.04188} \approx 0.0468

      **Positive Predictive Value ‚âà 4.68%**

      **Part (b): Negative Predictive Value P(D'|‚àí)**

      First, find P(‚àí):

      .. math::

         P(-) = 1 - P(+) = 1 - 0.04188 = 0.95812

      Find :math:`P(-|D) = 1 - P(+|D) = 1 - 0.98 = 0.02` (false negative rate)

      .. math::

         P(D'|-) = \frac{P(-|D')P(D')}{P(-)} = \frac{(0.96)(0.998)}{0.95812} = \frac{0.95808}{0.95812} \approx 0.99996

      **Negative Predictive Value ‚âà 99.996%**

      **Part (c): Screening 10,000 People**

      Expected positive tests: :math:`10,000 \times P(+) = 10,000 \times 0.04188 \approx 419` people

      Of those with positive tests, expected true positives:
      
      - People with disease: :math:`10,000 \times 0.002 = 20`
      - True positives (disease AND positive): :math:`20 \times 0.98 \approx 20` people

      So approximately **419 test positive**, but only about **20 actually have the condition**.

      **Part (d): Why is PPV So Low?**

      Even though the test has high sensitivity (98%) and specificity (96%), the **base rate** of the disease is very low (0.2%).

      When the disease is rare:
      
      - The number of **true positives** is small (98% of a small number)
      - The number of **false positives** is much larger (4% of a very large number)

      With 10,000 people:
      
      - True positives: ~20 (from the 20 people with disease)
      - False positives: ~399 (4% of the 9,980 healthy people)
      - Total positives: ~419, but only ~20 are real!

      This is the **base rate fallacy** ‚Äî people often overestimate PPV when the condition is rare.

----

.. admonition:: Exercise 5: Multi-Stage Problem with Tree Diagram
   :class: note

   A delivery company has two distribution centers (DC1 and DC2) that ship packages to customers. DC1 handles 60% of packages and DC2 handles 40%.

   - Packages from DC1 are shipped via Ground (80%) or Express (20%)
   - Packages from DC2 are shipped via Ground (50%) or Express (50%)

   The on-time delivery rates are:
   
   - DC1 Ground: 92% on-time
   - DC1 Express: 98% on-time
   - DC2 Ground: 88% on-time
   - DC2 Express: 99% on-time

   a. Draw a complete three-stage tree diagram for this problem.

   b. What is the probability that a randomly selected package is delivered on time?

   c. Given that a package was delivered late, what is the probability it came from DC1?

   d. Given that a package was delivered late, what is the probability it was shipped via Ground?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Tree Diagram**

      .. code-block:: text

                                 [Start]
                                /       \
                         0.60  /         \ 0.40
                              /           \
                           [DC1]         [DC2]
                          /    \         /    \
                    0.80 /      \0.20   /0.50  \ 0.50
                        /        \     /        \
                      [G]       [E]  [G]        [E]
                     /  \      /  \  /  \      /   \
               0.92 / \0.08 0.98\0.02 0.88\0.12 0.99\ 0.01
                   /   \    /   \   /    \   /     \
                 [OT] [L] [OT] [L] [OT]  [L] [OT]  [L]

      Path probabilities:
      
      - DC1, Ground, On-Time: 0.60 √ó 0.80 √ó 0.92 = 0.4416
      - DC1, Ground, Late: 0.60 √ó 0.80 √ó 0.08 = 0.0384
      - DC1, Express, On-Time: 0.60 √ó 0.20 √ó 0.98 = 0.1176
      - DC1, Express, Late: 0.60 √ó 0.20 √ó 0.02 = 0.0024
      - DC2, Ground, On-Time: 0.40 √ó 0.50 √ó 0.88 = 0.1760
      - DC2, Ground, Late: 0.40 √ó 0.50 √ó 0.12 = 0.0240
      - DC2, Express, On-Time: 0.40 √ó 0.50 √ó 0.99 = 0.1980
      - DC2, Express, Late: 0.40 √ó 0.50 √ó 0.01 = 0.0020

      **Part (b): P(On-Time)**

      Sum all on-time paths:

      .. math::

         P(OT) = 0.4416 + 0.1176 + 0.1760 + 0.1980 = 0.9332

      **93.32% of packages are delivered on time.**

      **Part (c): P(DC1|Late)**

      First, find P(Late):

      .. math::

         P(L) = 1 - P(OT) = 1 - 0.9332 = 0.0668

      P(DC1 ‚à© Late) = P(DC1, Ground, Late) + P(DC1, Express, Late) = 0.0384 + 0.0024 = 0.0408

      .. math::

         P(DC1|L) = \frac{P(DC1 \cap L)}{P(L)} = \frac{0.0408}{0.0668} \approx 0.6108

      **Given a late package, there's about a 61.08% chance it came from DC1.**

      **Part (d): P(Ground|Late)**

      P(Ground ‚à© Late) = P(DC1, Ground, Late) + P(DC2, Ground, Late) = 0.0384 + 0.0240 = 0.0624

      .. math::

         P(G|L) = \frac{P(G \cap L)}{P(L)} = \frac{0.0624}{0.0668} \approx 0.9341

      **Given a late package, there's about a 93.41% chance it was shipped via Ground.**

----

.. admonition:: Exercise 6: Prior and Posterior Probabilities
   :class: note

   A machine learning model classifies emails as "spam" or "not spam." Before seeing any features of an email:

   - **Prior probability**: 30% of incoming emails are spam

   The model uses the presence of the word "FREE" as a feature:

   - P("FREE" appears | Spam) = 0.60
   - P("FREE" appears | Not Spam) = 0.10

   a. An email arrives containing the word "FREE." What is the posterior probability that it is spam?

   b. An email arrives that does NOT contain the word "FREE." What is the posterior probability that it is spam?

   c. Compare the prior and posterior probabilities. How much does observing "FREE" change our belief about whether the email is spam?

   d. The model now also considers the presence of "URGENT." Given that an email is spam:
      - P("URGENT" | Spam) = 0.40
      
      Given that an email is not spam:
      - P("URGENT" | Not Spam) = 0.05
      
      If an email contains BOTH "FREE" and "URGENT" (assume these are conditionally independent given spam status), what is the posterior probability it is spam?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Setup:**

      - :math:`P(S) = 0.30` (prior ‚Äî email is spam)
      - :math:`P(S') = 0.70` (prior ‚Äî email is not spam)
      - :math:`P(F|S) = 0.60` ("FREE" given spam)
      - :math:`P(F|S') = 0.10` ("FREE" given not spam)

      **Part (a): P(Spam | "FREE" appears)**

      First, find P(F) using Law of Total Probability:

      .. math::

         P(F) = P(F|S)P(S) + P(F|S')P(S') = (0.60)(0.30) + (0.10)(0.70) = 0.18 + 0.07 = 0.25

      Apply Bayes' Rule:

      .. math::

         P(S|F) = \frac{P(F|S)P(S)}{P(F)} = \frac{(0.60)(0.30)}{0.25} = \frac{0.18}{0.25} = 0.72

      **Posterior probability of spam given "FREE": 72%**

      **Part (b): P(Spam | "FREE" does NOT appear)**

      First, find P(F') = 1 ‚àí 0.25 = 0.75

      Find :math:`P(F'|S) = 1 - P(F|S) = 1 - 0.60 = 0.40`

      .. math::

         P(S|F') = \frac{P(F'|S)P(S)}{P(F')} = \frac{(0.40)(0.30)}{0.75} = \frac{0.12}{0.75} = 0.16

      **Posterior probability of spam given NO "FREE": 16%**

      **Part (c): Comparison**

      - Prior P(Spam) = 0.30 (before seeing any features)
      - Posterior P(Spam | "FREE") = 0.72 (increased dramatically)
      - Posterior P(Spam | no "FREE") = 0.16 (decreased)

      Observing "FREE" increases our belief that the email is spam from 30% to 72% ‚Äî more than doubling it. The absence of "FREE" decreases our belief to 16% ‚Äî nearly halving it.

      The word "FREE" is a strong indicator because it's 6 times more likely to appear in spam (60%) than in legitimate email (10%).

      **Part (d): P(Spam | "FREE" AND "URGENT")**

      With conditional independence, we can update sequentially. Start with the posterior from part (a) as the new prior:

      - New prior: P(S) = 0.72 (given "FREE")
      - P(U|S) = 0.40
      - P(U|S') = 0.05

      Find P(U | already observed "FREE"):

      .. math::

         P(U) = P(U|S) \cdot P(S|F) + P(U|S') \cdot P(S'|F) = (0.40)(0.72) + (0.05)(0.28) = 0.288 + 0.014 = 0.302

      Apply Bayes' Rule:

      .. math::

         P(S|F,U) = \frac{P(U|S) \cdot P(S|F)}{P(U)} = \frac{(0.40)(0.72)}{0.302} = \frac{0.288}{0.302} \approx 0.9536

      **Posterior probability of spam given BOTH "FREE" and "URGENT": ~95.4%**

      Each piece of evidence updates our belief. Starting from 30%, "FREE" brings us to 72%, and "URGENT" further increases it to 95.4%.

----

Additional Practice Problems
----------------------------

**True/False Questions** (1 point each)

1. A partition of a sample space must consist of at least three events.

   ‚ìâ or ‚íª

2. The Law of Total Probability requires that the conditioning events form a partition.

   ‚ìâ or ‚íª

3. Bayes' Rule allows us to compute :math:`P(A|B)` when we know :math:`P(B|A)`.

   ‚ìâ or ‚íª

4. In Bayes' Rule, the denominator is computed using the Law of Total Probability.

   ‚ìâ or ‚íª

5. If a medical test has high sensitivity (99%), then a positive result means the patient almost certainly has the disease.

   ‚ìâ or ‚íª

6. When using Bayes' Rule, the posterior probabilities for all events in the partition must sum to 1.

   ‚ìâ or ‚íª

**Multiple Choice Questions** (2 points each)

7. Factory A produces 60% of a product with 2% defective. Factory B produces 40% with 5% defective. What is P(defective)?

   ‚í∂ 0.012
   
   ‚í∑ 0.020
   
   ‚í∏ 0.032
   
   ‚íπ 0.035

8. Using the information from Question 7, if a product is defective, what is P(from Factory A)?

   ‚í∂ 0.375
   
   ‚í∑ 0.400
   
   ‚í∏ 0.600
   
   ‚íπ 0.625

9. A disease affects 2% of the population. A test has 95% sensitivity and 90% specificity. What is P(Disease | Positive)?

   ‚í∂ About 16%
   
   ‚í∑ About 50%
   
   ‚í∏ About 90%
   
   ‚íπ About 95%

10. In the Bayesian framework, which term describes :math:`P(A_i)` in Bayes' Rule?

    ‚í∂ Likelihood
    
    ‚í∑ Posterior probability
    
    ‚í∏ Prior probability
    
    ‚íπ Normalizing constant

.. dropdown:: Answers to Practice Problems
   :class-container: sd-border-success

   **True/False Answers:**

   1. **False** ‚Äî The simplest partition consists of just two events: an event A and its complement A'.

   2. **True** ‚Äî The Law of Total Probability requires that the events :math:`\{A_1, A_2, \ldots, A_n\}` be mutually exclusive and exhaustive (i.e., form a partition).

   3. **True** ‚Äî This is exactly what Bayes' Rule does: it "inverts" conditional probabilities.

   4. **True** ‚Äî The denominator :math:`\sum P(B|A_i)P(A_i)` is the Law of Total Probability applied to find P(B).

   5. **False** ‚Äî High sensitivity means the test correctly identifies most people with the disease. But if the disease is rare, most positive results may still be false positives (low positive predictive value).

   6. **True** ‚Äî Since the partition events cover the entire sample space, the posterior probabilities must sum to 1: :math:`\sum P(A_i|B) = 1`.

   **Multiple Choice Answers:**

   7. **‚í∏** ‚Äî P(D) = P(D|A)P(A) + P(D|B)P(B) = (0.02)(0.60) + (0.05)(0.40) = 0.012 + 0.020 = 0.032

   8. **‚í∂** ‚Äî P(A|D) = P(D|A)P(A) / P(D) = (0.02)(0.60) / 0.032 = 0.012 / 0.032 = 0.375

   9. **‚í∂** ‚Äî P(+) = (0.95)(0.02) + (0.10)(0.98) = 0.019 + 0.098 = 0.117. P(D|+) = 0.019/0.117 ‚âà 0.162 ‚âà 16%

   10. **‚í∏** ‚Äî :math:`P(A_i)` is the prior probability ‚Äî our initial belief before observing evidence B.