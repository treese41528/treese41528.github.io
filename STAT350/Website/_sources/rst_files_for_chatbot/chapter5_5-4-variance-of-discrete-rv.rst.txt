.. _5-4-variance-of-discrete-rv:

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch5-4">
     <iframe
       id="video-ch5-4"
       title="STAT 350 ‚Äì Chapter 5.4 Spread of a Discrete Random Variable Video"
       src="https://www.youtube.com/embed/gA4f4mpjGk0?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
       allowfullscreen>
     </iframe>
   </div>

.. admonition:: Slides üìä
   :class: tip
   
   `Download Chapter 5 slides (PPTX) <https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/
   slides/Chapter%205%20Discrete%20Distributions/L9-11-RandomVariables%20DiscreteProbabilityDistributions%28Chapter%205%29_AC.pptx>`_
   
Varianace of a Discrete Random Variable
==================================================

Just as the expected value tells us about the center of a probability distribution, we often need 
to quantify how spread out or dispersed the values are around this center. 
Variance and standard deviation provide this crucial second dimension to our 
understanding of random variables.

.. admonition:: Road Map üß≠
   :class: important

   ‚Ä¢ Define **variance** and **standard deviation** for discrete random variables.
   ‚Ä¢ Explore an alternative **computational formula** for variance.
   ‚Ä¢ Derive key **properties of variance** for linear transformations and sums.

From Sample to Population: Defining Variance
------------------------------------------------

In our exploration of sample statistics, we measured the spread of data using 
sample variance‚Äîthe average of squared deviations from the mean. 
For random variables, we take a similar approach, but with an important twist. 
Instead of averaging deviations with equal weights, we **weight each deviation by 
its probability**.

Definition
~~~~~~~~~~~~~~~~~~~~

The variance of a discrete random variable :math:`X`, denoted :math:`Var(X)` or :math:`\sigma^2_X`, 
is the expected value of the squared deviation of :math:`X` from its mean:

.. math::

   \sigma_X^2 = \text{Var}(X) = E[(X - \mu_X)^2] = \sum_{x\in\text{supp}(X)} (x - \mu_X)^2 p_X(x).

The standard deviation, denoted :math:`\sigma_X`, is simply the square root of the variance:

.. math::

   \sigma_X = \sqrt{\text{Var}(X)} = \sqrt{E[(X - \mu_X)^2]}.


Note the **variance has squared units** (e.g., dollars¬≤ if :math:`X` is in dollars). 
The standard deviation returns us to the original units, 
making it often more interpretable in practice.

This definition requires that the series be absolutely convergent for the variance to
be well-defined. 

A Computational Shortcut for Variance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Calculating variance directly from its definition can be cumbersome. Fortunately, 
there's an equivalent formula that is typically easier to apply:

.. math::

   \sigma_X^2 = E[X^2] - \mu_X^2.

This can be derived by expanding the squared 
term in the original definition then simplifying:

.. math::

   \begin{align}
   \text{Var}(X) &= E[(X - \mu_X)^2] \\
   &= E[X^2 - 2X\mu_X + \mu_X^2] \\
   &= E[X^2] - 2\mu_X E[X] + \mu_X^2 \\
   &= E[X^2] - 2\mu_X \mu_X + \mu_X^2 \\
   &= E[X^2] - \mu_X^2
   \end{align}

This computational formula often simplifies the work significantly, 
as we'll see in our examples.

.. admonition:: Exampleüí°: Bean & Butter
   :class: note 

   **Bean & Butter** is a small campus caf√© that sells only two morning items: coffee 
   for $4 per cup and pastry for $3 each.
   The shop records its sales in *waves*‚Äîeach wave is short enough that :math:`X` (cups of coffee) 
   and :math:`Y` (pastries) follow a stable pattern but long enough to summarize cleanly.

   It is known that :math:`X` and :math:`Y` are independent. 
   The sales distribution for a single wave is:

   .. flat-table::
      :header-rows: 1

      * - Outcome
        - :math:`p_X(x)` (coffee)
        - :math:`p_Y(y)` (pastry)

      * - 0
        - 0.20
        - 0.30

      * - 1
        - 0.50
        - 0.40

      * - 2
        - 0.30
        - 0.30

   Let us first compute the expected sales *count* of coffee and pastries.

   .. math::
      E[X] &= (0) (0.2) + (1) (0.5) + (2) (0.3) = 1.1\\
      E[Y] &= (0) (0.3) + (1) (0.4) + (2) (0.3) = 1.0

   On average, 1.1 cups of cofee and 1.0 patry are sold per wave.

   For staffing, buying milk, or setting aside cash for the till, 
   the owner also cares about *variability* of sales‚Äîhow much
   does an individual wave fluctuate from the average?

   To answer this question, we compute the variance and standard deviation
   of each random variable. For cofee,

   .. math::
      E[X^2] &= (0^2) (0.2) + (1^2) (0.5) + (2^2) (0.3) = 1.7\\
      \text{Var}(X) &= E[X^2]- (E[X])^2 =1.7 - 1.1^2 = 0.49\\
      \sigma_X &= \sqrt{0.49} \approx 0.70

   Similarly for pastries:

   .. math::
      E[Y^2] &= (0^2) (0.3) + (1^2) (0.4) + (2^2) (0.3) = 1.6\\
      \text{Var}(Y) &= 1.6 - 1.0^2 = 0.60\\
      \sigma_Y &= \sqrt{0.60} \approx 0.77

   A standard deviation of about **0.70 coffees** and **0.77 pastries** tells us that, 
   in a typical wave, each count strays by **roughly three-quarters of an item** from 
   its own average.

Properties of Variance
-----------------------

Variance has several key properties that make calculations more manageable, 
especially when dealing with linear transformations of random variables.

A. Variance of Linear Transformations 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For a linear transformation of a random variable, :math:`g(X) = aX + b`, 
where :math:`a` and :math:`b` are constants:

.. math::

   \text{Var}(aX + b) = a^2 \text{Var}(X).

Notice two important implications:

* Scaling a random variable by a factor of :math:`a` multiplies its variance by :math:`a^2`.
* Adding a constant :math:`b` has no effect on variance.

This makes intuitive sense. Multiplying all values by :math:`a` stretches (or compresses) 
the distribution, amplifying (or reducing) the deviations  also by a factor of :math:`a`. 
But since deviations are squared in the variance calculation, 
the variance increases by a factor of :math:`a^2`. 
Meanwhile, shifting all values by adding a constant :math:`b` moves the entire distribution 
without stretching or compressing its *width*.

We can prove this property using the computational formula for variance:

.. math::

   \begin{align}
   \text{Var}(aX + b) &= E[(aX + b)^2] - (E[aX + b])^2 \\
   &= E[a^2X^2 + 2abX + b^2] - (a\mu_X + b)^2 \\
   &= a^2E[X^2] + 2abE[X] + b^2 - a^2\mu_X^2 - 2ab\mu_X - b^2 \\
   &= a^2E[X^2] + 2ab\mu_X + b^2 - a^2\mu_X^2 - 2ab\mu_X - b^2 \\
   &= a^2E[X^2] - a^2\mu_X^2 \\
   &= a^2(E[X^2] - \mu_X^2) \\
   &= a^2\text{Var}(X)
   \end{align}

B. Variance of Sums of Independent RVs
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


For **independent** random variables, the variance of their sum
equals the sum of their individual variances:

.. math::

   \text{Var}(X \pm Y) = \text{Var}(X) + \text{Var}(Y)

This extends to any number of **mutually independent** random variables:

.. math::

   \text{Var}(X_1 \pm X_2 \pm \cdots \pm X_n) = \text{Var}(X_1) + \text{Var}(X_2) + \cdots + \text{Var}(X_n)

.. admonition:: Why do the negative signs disappear?
   :class: important

   You can think of the negative signs as coefficients (-1)
   multiplied to the following random variable. Then using the first
   property of variance,

   .. math:: 
      Var(X-Y) &= Var(X + (-1)Y)\\
      &= Var(X)+(-1)^2Var(Y) = Var(X) + Var(Y).
..
   A crucial caveat: this property holds only when the random variables are independent. 
   When variables are dependent, we need to account for their covariance, which we'll 
   explore in a later section.

   An important consequence: for standard deviation, we cannot simply add the standard 
   deviations of independent random variables. Instead:

   .. math::

      \sigma_{X \pm Y} = \sqrt{\sigma_X^2 + \sigma_Y^2}

C. Variance of Linear Combinations of Independent RVs
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For linear combinations of independent random variables:

.. math::

   \text{Var}(aX \pm bY) = a^2\text{Var}(X) + b^2\text{Var}(Y)

This simply combines the two properties we've just seen.




Using Variance Properties to Compute Standard Deviation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Properties of variance **do not apply 
to standard deviations**, in general. To compute the standard
deviation of a linear compbination of random variables, always
**compute the variance first, then take its square root**.

.. admonition:: Exampleüí°: Bean & Butter, Continued
   :class: note

   Consider the revenue per wave at Beans & Butter:

   .. math::

      R = 4X + 3Y.

   Item-wise revenues are first computed by multiplying the price of each item by
   its sales count. These individual revenues are then added up to obtain the total revenue.
   
   **What is the standard deviation of the total revenue?**

   We begin by computing the variance of :math:`R`. Since :math:`X` and :math:`Y` are 
   **independent**, we can use the third property of variance:

   .. math::

      \begin{aligned}
      \text{Var}(R) &= 4^2 \text{Var}(X) + 3^2 \text{Var}(Y)\\
                  &= 16(0.49) + 9(0.60) = 13.24\\
      \sigma_R &= \sqrt{13.24} \approx \$3.64
      \end{aligned}

   The standard deviation of revenue per wave is $3.64.

   Suppose a new random variable :math:`Z` represents the **cost** per wave of 
   running the store. It is known that :math:`\sigma_Z = 2.2` and that :math:`Z`
   is independent of :math:`X` and :math:`Y`.
   
   **What is the standard deviation in the total profit per wave?**

   The total profit can be expressed as :math:`P = R - Z`.
   
   Again, begin by computing the variance of :math:`P` first. Because :math:`R` and :math:`Z`
   are independent, we can use the second property of variance:

   .. math:: 
      Var(P) &= Var(R) + Var(Z) = 13.24 + 2.2^2 = 18.08\\
      \sigma_P &= \sqrt{18.08} \approx 4.2521.

   Note that the negative sign between :math:`R` and :math:`Z` disappears.


..
   Illustrating the Concepts: The Dice üí°
   --------------------------------------------

   Let's apply these concepts to a clear example: rolling multiple fair four-sided dice 
   and finding the variance of their sum.

   Suppose we roll n independent four-sided dice (with faces labeled 1, 2, 3, and 4) and 
   let X·µ¢ be the outcome of the ith die. We want to understand both the expected value 
   and the variance of the sum of these dice.

   First, let's determine the properties of a single die. Since each face is equally 
   likely to appear, the PMF for each die is:

   .. math::

      p_{X_i}(x_i) = \frac{1}{4} \text{ for } x_i \in \{1, 2, 3, 4\} \text{ and for all } i \in \{1, 2, \ldots, n\}

   The expected value of each die is:

   .. math::

      \mu_{X_i} = E[X_i] = 1 \times \frac{1}{4} + 2 \times \frac{1}{4} + 3 \times \frac{1}{4} + 4 \times \frac{1}{4} = \frac{10}{4} = 2.5

   To calculate the variance, we first find E[X_i¬≤]:

   .. math::

      E[X_i^2] = 1^2 \times \frac{1}{4} + 2^2 \times \frac{1}{4} + 3^2 \times \frac{1}{4} + 4^2 \times \frac{1}{4} = \frac{1 + 4 + 9 + 16}{4} = \frac{30}{4} = 7.5

   Now we can compute the variance:

   .. math::

      \text{Var}(X_i) = E[X_i^2] - \mu_{X_i}^2 = 7.5 - 2.5^2 = 7.5 - 6.25 = 1.25

   The standard deviation is:

   .. math::

      \sigma_{X_i} = \sqrt{\text{Var}(X_i)} = \sqrt{1.25} \approx 1.12

   Now, let's consider the sum of all n dice, which we'll denote as S = X‚ÇÅ + X‚ÇÇ + ... + X‚Çô. Since the dice are independent, we can apply our properties:

   1. Expected value of the sum:
      
      .. math::
      
         E[S] = E[X_1 + X_2 + \cdots + X_n] = E[X_1] + E[X_2] + \cdots + E[X_n] = n \times 2.5 = 2.5n

   2. Variance of the sum:
      
      .. math::
      
         \text{Var}(S) = \text{Var}(X_1 + X_2 + \cdots + X_n) = \text{Var}(X_1) + \text{Var}(X_2) + \cdots + \text{Var}(X_n) = n \times 1.25 = 1.25n

   3. Standard deviation of the sum:
      
      .. math::
      
         \sigma_S = \sqrt{\text{Var}(S)} = \sqrt{1.25n}

   These results tell us that:

   - The average sum when rolling n four-sided dice is 2.5n
   - The variance of this sum is 1.25n
   - The standard deviation grows with the square root of n

   This example illustrates a fundamental principle in probability: as we add more independent random variables, the expected value grows linearly with n, but the standard deviation grows with ‚àön. This is a preview of what we'll see more formally in the Central Limit Theorem in later chapters.







Common Mistakes to Avoid
-------------------------

When working with variance and standard deviation, be careful to avoid these common errors:

.. admonition:: Common Mistakes to Avoid  üõë
   :class: error

   1. **Forgetting to square the coefficient in variance**
     
     :math:`Var(aX) = a¬≤Var(X)`, not :math:`aVar(X)`.

   2. **Not including the negative sign when squaring the coefficient**
     
     :math:`Var(-aX) = (-a)^2Var(X)`. :math:`(-a)^2` is positive!

   3. **Assuming standard deviations add**
     
     For independent :math:`X` and :math:`Y`, :math:`\sigma_{X+Y} ‚â† \sigma_X + \sigma_Y.` 
     Always add variances first, then take the square root.

   4. **Blindly applying the independence formula**
     
     The formula :math:`Var(X + Y) = Var(X) + Var(Y)` only applies when 
     :math:`X` and :math:`Y` are independent.
   
   5. **Calculating** :math:`E[X]^2` **instead of** :math:`E[X^2]`
     
     :math:`E[X]^2` and :math:`E[X^2]` are different! :math:`E[X^2]` is found by squaring 
     individual outcomes first, then taking their expectation.

Bringing It All Together
---------------------------


.. admonition:: Key Takeaways üìù
   :class: important

   1. The **variance** of a discrete random variable is the expected value of the 
      squared deviation from its mean, measuring how spread out the distribution is.
   
   2. The **standard deviation** is the square root of the variance and has the same 
      units as the original random variable.
   
   3. :math:`Var(X) = E[X^2] - (E[X])^2` is often used as computational shortcut for variance.

   4. For linear transformations, :math:`Var(aX + b) = a^2Var(X)`, meaning that scaling affects 
      variance quadratically while shifting has no effect.
   
   5. For independent random variables, :math:`Var(X \pm Y) = Var(X) + Var(Y)`, 
      showing that variances (not standard deviations) add for independent variables.
   
   6. When calculating any standard deviation, compute the variance first, 
      then take the square root.

In the next section, we'll explore how to handle dependent random variables, 
where the relationship between variables adds another layer of complexity 
to our analysis.

Exercises
---------

These exercises develop your skills in computing variance and standard deviation, applying the computational shortcut formula, and using variance properties for linear transformations and sums of independent random variables.

.. admonition:: Exercise 1: Basic Variance Calculation
   :class: note

   A network engineer monitors packet loss :math:`X` (number of dropped packets per minute) on a router. The PMF is:

   .. flat-table:: PMF for Packet Loss
      :header-rows: 1
      :widths: 20 20 20 20 20

      * - :math:`x`
        - 0
        - 1
        - 2
        - 3
      * - :math:`p_X(x)`
        - 0.50
        - 0.30
        - 0.15
        - 0.05

   a. Calculate :math:`E[X]`.

   b. Calculate :math:`E[X^2]`.

   c. Calculate :math:`\text{Var}(X)` using the computational shortcut :math:`\text{Var}(X) = E[X^2] - (E[X])^2`.

   d. Calculate :math:`\sigma_X`, the standard deviation.

   e. Interpret the standard deviation in context.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): E[X]**

      .. math::

         E[X] = (0)(0.50) + (1)(0.30) + (2)(0.15) + (3)(0.05) = 0 + 0.30 + 0.30 + 0.15 = 0.75

      **Part (b): E[X¬≤]**

      .. math::

         E[X^2] = (0)^2(0.50) + (1)^2(0.30) + (2)^2(0.15) + (3)^2(0.05)

      .. math::

         = 0 + 0.30 + 0.60 + 0.45 = 1.35

      **Part (c): Var(X)**

      .. math::

         \text{Var}(X) = E[X^2] - (E[X])^2 = 1.35 - (0.75)^2 = 1.35 - 0.5625 = 0.7875

      **Part (d): Standard deviation**

      .. math::

         \sigma_X = \sqrt{0.7875} \approx 0.887

      **Part (e): Interpretation**

      The standard deviation of about **0.89 packets** tells us that the number of dropped packets per minute typically deviates from the average (0.75) by roughly 0.89 packets. This gives the engineer a sense of the typical variability in packet loss.

----

.. admonition:: Exercise 2: Variance of Linear Transformations
   :class: note

   A quality control inspector measures the diameter :math:`X` of manufactured bolts (in mm). Historical data shows :math:`E[X] = 10.0` mm and :math:`\sigma_X = 0.2` mm.

   The bolt specifications require converting to inches using :math:`Y = 0.03937X` (since 1 mm ‚âà 0.03937 inches).

   a. Find :math:`E[Y]`, the expected diameter in inches.

   b. Find :math:`\text{Var}(Y)`.

   c. Find :math:`\sigma_Y`, the standard deviation in inches.

   d. A machinist claims "The standard deviation in inches is just 0.03937 times the standard deviation in mm." Is this correct? Explain.

   e. Suppose measurements are reported as deviations from the target of 10 mm: :math:`D = X - 10`. Find :math:`\text{Var}(D)`.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): E[Y]**

      Using linearity: :math:`E[Y] = E[0.03937X] = 0.03937 \cdot E[X] = 0.03937 \times 10.0 = 0.3937` inches.

      **Part (b): Var(Y)**

      Using :math:`\text{Var}(aX) = a^2\text{Var}(X)`:

      .. math::

         \text{Var}(Y) = (0.03937)^2 \cdot \text{Var}(X) = (0.03937)^2 \times (0.2)^2 = 0.00155 \times 0.04 = 0.000062

      **Part (c): œÉ_Y**

      .. math::

         \sigma_Y = \sqrt{0.000062} = 0.00787 \text{ inches}

      **Part (d): Is the machinist correct?**

      **Yes, the machinist is correct!** For standard deviation (unlike variance):

      .. math::

         \sigma_Y = |a| \cdot \sigma_X = 0.03937 \times 0.2 = 0.00787 \text{ inches}

      This works because :math:`\sigma_{aX} = \sqrt{a^2\text{Var}(X)} = |a|\sqrt{\text{Var}(X)} = |a|\sigma_X`.

      **Part (e): Var(D)**

      Using :math:`\text{Var}(X + b) = \text{Var}(X)` (shifting doesn't change variance):

      .. math::

         \text{Var}(D) = \text{Var}(X - 10) = \text{Var}(X) = (0.2)^2 = 0.04 \text{ mm}^2

----

.. admonition:: Exercise 3: Variance of Independent Sums
   :class: note

   A small online store processes orders from two independent sources:

   - Website orders :math:`W`: :math:`E[W] = 50`, :math:`\text{Var}(W) = 100`
   - Mobile app orders :math:`M`: :math:`E[M] = 30`, :math:`\text{Var}(M) = 64`

   Assume :math:`W` and :math:`M` are independent.

   a. Find :math:`E[W + M]`, the expected total orders.

   b. Find :math:`\text{Var}(W + M)`, the variance of total orders.

   c. Find :math:`\sigma_{W+M}`, the standard deviation of total orders.

   d. A manager incorrectly calculates :math:`\sigma_{W+M} = \sigma_W + \sigma_M = 10 + 8 = 18`. What is the correct value, and why is the manager's approach wrong?

   e. The store's profit per order is $5 from website and $3 from mobile. Find the variance of total daily profit :math:`P = 5W + 3M`.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): E[W + M]**

      .. math::

         E[W + M] = E[W] + E[M] = 50 + 30 = 80 \text{ orders}

      **Part (b): Var(W + M)**

      Since W and M are independent:

      .. math::

         \text{Var}(W + M) = \text{Var}(W) + \text{Var}(M) = 100 + 64 = 164

      **Part (c): œÉ_{W+M}**

      .. math::

         \sigma_{W+M} = \sqrt{164} \approx 12.81

      **Part (d): Manager's error**

      The manager got **18**, but the correct answer is **12.81**.

      **Standard deviations do NOT add!** Variances add for independent random variables, not standard deviations. The correct approach is:

      1. Add variances: :math:`100 + 64 = 164`
      2. Take square root: :math:`\sqrt{164} \approx 12.81`

      The manager's error of adding standard deviations overestimates the variability.

      **Part (e): Var(5W + 3M)**

      Since W and M are independent:

      .. math::

         \text{Var}(5W + 3M) = 5^2\text{Var}(W) + 3^2\text{Var}(M) = 25(100) + 9(64) = 2500 + 576 = 3076

      The variance of daily profit is **$3076** (in dollars¬≤).

----

.. admonition:: Exercise 4: The E[X¬≤] = Var(X) + (E[X])¬≤ Trick
   :class: note

   This exercise demonstrates a powerful technique: rearranging the variance formula to find :math:`E[X^2]` when you're given :math:`E[X]` and :math:`\text{Var}(X)`.

   .. admonition:: Key Identity üîë
      :class: tip

      From :math:`\text{Var}(X) = E[X^2] - (E[X])^2`, we can rearrange to get:

      .. math::

         E[X^2] = \text{Var}(X) + (E[X])^2

   A data center monitors humidity deviation :math:`X` from the ideal level (in percentage points). Studies show that :math:`E[X] = 0` and :math:`\text{Var}(X) = 1.86`.

   The efficiency loss :math:`Y` (in percentage points) due to humidity deviation is modeled as:

   .. math::

      Y = 0.1X^2 + 2

   a. Explain why you cannot directly apply linearity of expectation to find :math:`E[Y]`.

   b. Use the identity :math:`E[X^2] = \text{Var}(X) + (E[X])^2` to find :math:`E[X^2]`.

   c. Calculate :math:`E[Y]`, the expected efficiency loss.

   d. If the mean humidity deviation shifts to :math:`E[X] = 1` while :math:`\text{Var}(X)` remains 1.86, what is the new :math:`E[X^2]` and expected efficiency loss?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Why linearity doesn't directly apply**

      Linearity of expectation states :math:`E[aX + b] = aE[X] + b`, which only works for **linear** functions of X.

      The function :math:`Y = 0.1X^2 + 2` is **not linear** in X because of the :math:`X^2` term. We cannot simply substitute :math:`E[X]` for X.

      Specifically, :math:`E[0.1X^2 + 2] \neq 0.1(E[X])^2 + 2` in general.

      **Part (b): Find E[X¬≤] using the trick**

      Using the rearranged variance formula:

      .. math::

         E[X^2] = \text{Var}(X) + (E[X])^2 = 1.86 + (0)^2 = 1.86

      **Part (c): Calculate E[Y]**

      Now we can use linearity on :math:`E[X^2]`:

      .. math::

         E[Y] = E[0.1X^2 + 2] = 0.1 \cdot E[X^2] + 2 = 0.1(1.86) + 2 = 0.186 + 2 = 2.186

      The expected efficiency loss is **2.186 percentage points**.

      **Part (d): New expected loss with shifted mean**

      With :math:`E[X] = 1` and :math:`\text{Var}(X) = 1.86`:

      .. math::

         E[X^2] = \text{Var}(X) + (E[X])^2 = 1.86 + (1)^2 = 2.86

      .. math::

         E[Y] = 0.1(2.86) + 2 = 0.286 + 2 = 2.286

      The new expected efficiency loss is **2.286 percentage points**.

      Notice that shifting the mean (even though variance stayed the same) increased the expected loss because :math:`E[X^2]` depends on **both** variance and the squared mean.

----

.. admonition:: Exercise 5: Working with Abstract Functions (Don't Overthink It!)
   :class: note

   A machine learning engineer is analyzing model performance. Let :math:`X` be the number of training epochs (a discrete random variable with a known PMF). Define:

   .. math::

      Y = \ln(1 + e^{X^2 - 3X + 2})

   You are told that :math:`Y` has been analyzed and the following quantities are known:

   - :math:`E[Y] = 5`
   - :math:`\text{Var}(Y) = 12`

   Independently, let :math:`W` represent a noise term with :math:`E[W] = 0` and :math:`\text{Var}(W) = 3`.

   The total model loss is defined as:

   .. math::

      L = 2Y - 3W + 10

   a. Find :math:`E[L]`.

   b. Find :math:`\text{Var}(L)`.

   c. Find :math:`\sigma_L`.

   d. Find :math:`E[L^2]` using the identity :math:`E[L^2] = \text{Var}(L) + (E[L])^2`.

   e. A colleague says "We need to find the PMF of :math:`Y` first before we can answer these questions." Explain why they are wrong.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **The Key Insight:** The complicated function :math:`Y = \ln(1 + e^{X^2 - 3X + 2})` is irrelevant! We're given :math:`E[Y]` and :math:`\text{Var}(Y)` directly. Just let :math:`Y` be a random variable with these properties and proceed.

      **Part (a): E[L]**

      Using linearity of expectation:

      .. math::

         E[L] = E[2Y - 3W + 10] = 2E[Y] - 3E[W] + 10 = 2(5) - 3(0) + 10 = 20

      **Part (b): Var(L)**

      Since :math:`Y` and :math:`W` are independent:

      .. math::

         \text{Var}(L) = \text{Var}(2Y - 3W + 10) = 4\text{Var}(Y) + 9\text{Var}(W)

      .. math::

         = 4(12) + 9(3) = 48 + 27 = 75

      **Part (c): œÉ_L**

      .. math::

         \sigma_L = \sqrt{75} \approx 8.66

      **Part (d): E[L¬≤]**

      Using :math:`E[L^2] = \text{Var}(L) + (E[L])^2`:

      .. math::

         E[L^2] = 75 + 20^2 = 75 + 400 = 475

      **Part (e): Why we don't need the PMF of Y**

      The colleague is wrong because:

      1. **Expected value and variance are sufficient** for the calculations we need. The properties of expectation (linearity, additivity) and variance (scaling, additivity for independent RVs) only require knowing :math:`E[Y]` and :math:`\text{Var}(Y)`.

      2. **We never need to compute** :math:`E[\ln(1 + e^{X^2 - 3X + 2})]` directly‚Äîit's given as 5.

      3. This is the power of working with **abstract properties**: once we know the mean and variance of a random variable, we can analyze linear combinations without knowing the full distribution.

      This is a common exam trick: students waste time trying to compute complicated transformations when all necessary information is already provided!

----

.. admonition:: Exercise 6: LOTUS for Variance (Table Given)
   :class: note

   A cybersecurity team monitors the number of intrusion attempts :math:`X` per hour on a server. The PMF is:

   .. flat-table:: PMF for Intrusion Attempts
      :header-rows: 1
      :widths: 20 20 20 20 20

      * - :math:`x`
        - 0
        - 1
        - 2
        - 3
      * - :math:`p_X(x)`
        - 0.50
        - 0.30
        - 0.15
        - 0.05

   The security response cost (in hundreds of dollars) is modeled by:

   .. math::

      C(X) = \frac{X^3 + 2X}{X + 1}

   a. Create a table showing :math:`x`, :math:`p_X(x)`, :math:`C(x)`, :math:`C(x) \cdot p_X(x)`, and :math:`C(x)^2 \cdot p_X(x)`.

   b. Calculate :math:`E[C(X)]` using LOTUS.

   c. Calculate :math:`E[C(X)^2]` using LOTUS.

   d. Calculate :math:`\text{Var}(C(X))`.

   e. If the company budgets for the expected cost plus two standard deviations, what should their hourly budget be (in dollars)?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Build the table**

      First, compute :math:`C(x) = \frac{x^3 + 2x}{x + 1}` for each value:

      - :math:`C(0) = \frac{0 + 0}{0 + 1} = 0`
      - :math:`C(1) = \frac{1 + 2}{1 + 1} = \frac{3}{2} = 1.5`
      - :math:`C(2) = \frac{8 + 4}{2 + 1} = \frac{12}{3} = 4`
      - :math:`C(3) = \frac{27 + 6}{3 + 1} = \frac{33}{4} = 8.25`

      .. flat-table:: LOTUS Calculation Table
         :header-rows: 1
         :widths: 12 12 15 20 20

         * - :math:`x`
           - :math:`p_X(x)`
           - :math:`C(x)`
           - :math:`C(x) \cdot p_X(x)`
           - :math:`C(x)^2 \cdot p_X(x)`
         * - 0
           - 0.50
           - 0
           - 0
           - 0
         * - 1
           - 0.30
           - 1.5
           - 0.45
           - 0.675
         * - 2
           - 0.15
           - 4
           - 0.60
           - 2.40
         * - 3
           - 0.05
           - 8.25
           - 0.4125
           - 3.403
         * - **Sum**
           - 1.00
           - ‚Äî
           - **1.4625**
           - **6.478**

      **Part (b): E[C(X)]**

      .. math::

         E[C(X)] = \sum_{x} C(x) \cdot p_X(x) = 0 + 0.45 + 0.60 + 0.4125 = 1.4625

      Expected cost is **$146.25** per hour (since C is in hundreds).

      **Part (c): E[C(X)¬≤]**

      .. math::

         E[C(X)^2] = \sum_{x} C(x)^2 \cdot p_X(x) = 0 + 0.675 + 2.40 + 3.403 = 6.478

      **Part (d): Var(C(X))**

      .. math::

         \text{Var}(C(X)) = E[C(X)^2] - (E[C(X)])^2 = 6.478 - (1.4625)^2

      .. math::

         = 6.478 - 2.139 = 4.339

      **Part (e): Budget calculation**

      Standard deviation: :math:`\sigma_{C(X)} = \sqrt{4.339} \approx 2.083` (in hundreds)

      Budget = :math:`E[C(X)] + 2\sigma_{C(X)} = 1.4625 + 2(2.083) = 1.4625 + 4.166 = 5.63` (in hundreds)

      Hourly budget should be approximately **$563**.

      .. admonition:: Note on Approach
         :class: tip

         Even though :math:`C(X) = \frac{X^3 + 2X}{X + 1}` looks complicated, LOTUS lets us:
         
         1. Evaluate :math:`C(x)` at each value in the support
         2. Weight by the original probabilities :math:`p_X(x)`
         3. Sum to get the expected value
         
         We never need to find the PMF of :math:`C(X)` itself!

----

.. admonition:: Exercise 7: Sample Mean Properties
   :class: note

   Let :math:`X_1, X_2, X_3, X_4` be independent and identically distributed (i.i.d.) random variables with :math:`E[X_i] = 4` and :math:`\text{Var}(X_i) = 32`.

   Define the sample mean: :math:`\bar{X} = \frac{1}{4}(X_1 + X_2 + X_3 + X_4)`

   a. Find :math:`E[\bar{X}]`.

   b. Find :math:`\text{Var}(\bar{X})`.

   c. Find :math:`\text{Var}(2\bar{X} + 1)`.

   d. Find :math:`E[(2\bar{X} + 1)^2]`.

   e. Compare :math:`\text{Var}(\bar{X})` to :math:`\text{Var}(X_i)`. What happens to the variance as we average more observations?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): E[XÃÑ]**

      Using linearity and additivity:

      .. math::

         E[\bar{X}] = E\left[\frac{1}{4}(X_1 + X_2 + X_3 + X_4)\right] = \frac{1}{4}(E[X_1] + E[X_2] + E[X_3] + E[X_4])

      .. math::

         = \frac{1}{4}(4 + 4 + 4 + 4) = \frac{16}{4} = 4

      **Part (b): Var(XÃÑ)**

      Since the :math:`X_i` are independent:

      .. math::

         \text{Var}(\bar{X}) = \text{Var}\left(\frac{1}{4}(X_1 + X_2 + X_3 + X_4)\right) = \frac{1}{16}\text{Var}(X_1 + X_2 + X_3 + X_4)

      .. math::

         = \frac{1}{16}(\text{Var}(X_1) + \text{Var}(X_2) + \text{Var}(X_3) + \text{Var}(X_4)) = \frac{1}{16}(32 \times 4) = \frac{128}{16} = 8

      **Part (c): Var(2XÃÑ + 1)**

      Using :math:`\text{Var}(aX + b) = a^2\text{Var}(X)`:

      .. math::

         \text{Var}(2\bar{X} + 1) = 4 \cdot \text{Var}(\bar{X}) = 4 \times 8 = 32

      **Part (d): E[(2XÃÑ + 1)¬≤]**

      Using :math:`E[Y^2] = \text{Var}(Y) + (E[Y])^2`:

      Let :math:`Y = 2\bar{X} + 1`. We have:

      - :math:`E[Y] = 2E[\bar{X}] + 1 = 2(4) + 1 = 9`
      - :math:`\text{Var}(Y) = 32` (from part c)

      Therefore:

      .. math::

         E[(2\bar{X} + 1)^2] = E[Y^2] = \text{Var}(Y) + (E[Y])^2 = 32 + 81 = 113

      **Part (e): Comparison**

      - :math:`\text{Var}(X_i) = 32`
      - :math:`\text{Var}(\bar{X}) = 8 = \frac{32}{4}`

      In general, :math:`\text{Var}(\bar{X}) = \frac{\text{Var}(X)}{n}` for i.i.d. random variables.

      As we average more observations, the variance of the sample mean **decreases** by a factor of :math:`n`. This is why averaging reduces variability‚Äîthe sample mean becomes more stable (less variable) with larger samples. This is a preview of concepts in sampling distributions (Chapter 7).

----

.. admonition:: Exercise 8: Variance from Definition vs. Shortcut
   :class: note

   A startup tracks daily user signups :math:`X` with the following PMF:

   .. flat-table:: PMF for Daily Signups
      :header-rows: 1
      :widths: 25 25 25 25

      * - :math:`x`
        - 10
        - 20
        - 30
      * - :math:`p_X(x)`
        - 0.25
        - 0.50
        - 0.25

   a. Calculate :math:`E[X]`.

   b. Calculate :math:`\text{Var}(X)` using the **definition**: :math:`\text{Var}(X) = E[(X - \mu)^2] = \sum_x (x - \mu)^2 p_X(x)`.

   c. Calculate :math:`\text{Var}(X)` using the **shortcut**: :math:`\text{Var}(X) = E[X^2] - (E[X])^2`.

   d. Verify that both methods give the same answer.

   e. Which method required fewer calculations? When might the definition method be preferable?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): E[X]**

      .. math::

         E[X] = (10)(0.25) + (20)(0.50) + (30)(0.25) = 2.5 + 10 + 7.5 = 20

      **Part (b): Var(X) using the definition**

      With :math:`\mu = 20`:

      .. math::

         \text{Var}(X) = \sum_x (x - 20)^2 p_X(x)

      .. math::

         = (10 - 20)^2(0.25) + (20 - 20)^2(0.50) + (30 - 20)^2(0.25)

      .. math::

         = (-10)^2(0.25) + (0)^2(0.50) + (10)^2(0.25)

      .. math::

         = 100(0.25) + 0 + 100(0.25) = 25 + 0 + 25 = 50

      **Part (c): Var(X) using the shortcut**

      First find :math:`E[X^2]`:

      .. math::

         E[X^2] = (10)^2(0.25) + (20)^2(0.50) + (30)^2(0.25)

      .. math::

         = 100(0.25) + 400(0.50) + 900(0.25) = 25 + 200 + 225 = 450

      Then:

      .. math::

         \text{Var}(X) = E[X^2] - (E[X])^2 = 450 - 20^2 = 450 - 400 = 50

      **Part (d): Verification**

      Both methods give :math:`\text{Var}(X) = 50` ‚úì

      **Part (e): Comparison**

      The **shortcut method** typically requires fewer calculations because:
      - You compute :math:`E[X]` and :math:`E[X^2]` (which you often need anyway)
      - You avoid computing :math:`(x - \mu)^2` for each value

      The **definition method** might be preferable when:
      - You want to see how each value contributes to variance
      - The mean is a "nice" number (like 0) that makes :math:`(x - \mu)^2` easy to compute
      - You're verifying your shortcut calculation

----

Additional Practice Problems
----------------------------

**True/False Questions** (1 point each)

1. Variance can be negative if the random variable takes negative values.

   ‚ìâ or ‚íª

2. For any constant :math:`c`, :math:`\text{Var}(X + c) = \text{Var}(X) + c`.

   ‚ìâ or ‚íª

3. :math:`\text{Var}(2X) = 2\text{Var}(X)`.

   ‚ìâ or ‚íª

4. For independent :math:`X` and :math:`Y`, :math:`\text{Var}(X - Y) = \text{Var}(X) - \text{Var}(Y)`.

   ‚ìâ or ‚íª

5. The standard deviation has the same units as the original random variable.

   ‚ìâ or ‚íª

6. If :math:`E[X] = 0`, then :math:`E[X^2] = \text{Var}(X)`.

   ‚ìâ or ‚íª

**Multiple Choice Questions** (2 points each)

7. A random variable :math:`X` has :math:`E[X] = 5` and :math:`\text{Var}(X) = 9`. What is :math:`\text{Var}(3X - 2)`?

   ‚í∂ 25
   
   ‚í∑ 27
   
   ‚í∏ 79
   
   ‚íπ 81

8. If :math:`X` and :math:`Y` are independent with :math:`\sigma_X = 3` and :math:`\sigma_Y = 4`, what is :math:`\sigma_{X+Y}`?

   ‚í∂ 5
   
   ‚í∑ 7
   
   ‚í∏ 12
   
   ‚íπ 25

9. A random variable :math:`X` has :math:`E[X] = 2` and :math:`E[X^2] = 8`. What is :math:`\text{Var}(X)`?

   ‚í∂ 2
   
   ‚í∑ 4
   
   ‚í∏ 6
   
   ‚íπ 8

10. Let :math:`X` have :math:`E[X] = 3` and :math:`\text{Var}(X) = 4`. What is :math:`E[X^2]`?

    ‚í∂ 7
    
    ‚í∑ 9
    
    ‚í∏ 12
    
    ‚íπ 13

.. dropdown:: Answers to Practice Problems
   :class-container: sd-border-success

   **True/False Answers:**

   1. **False** ‚Äî Variance is always non-negative: :math:`\text{Var}(X) \geq 0`. It equals zero only if X is a constant. The sign of X's values doesn't affect this.

   2. **False** ‚Äî Adding a constant shifts the distribution but doesn't change its spread. :math:`\text{Var}(X + c) = \text{Var}(X)`, not :math:`\text{Var}(X) + c`.

   3. **False** ‚Äî :math:`\text{Var}(aX) = a^2\text{Var}(X)`, so :math:`\text{Var}(2X) = 4\text{Var}(X)`, not :math:`2\text{Var}(X)`.

   4. **False** ‚Äî For independent RVs, variances **add** regardless of the sign: :math:`\text{Var}(X - Y) = \text{Var}(X) + \text{Var}(Y)`.

   5. **True** ‚Äî Since :math:`\sigma_X = \sqrt{\text{Var}(X)}`, the standard deviation has the same units as X, while variance has squared units.

   6. **True** ‚Äî From :math:`\text{Var}(X) = E[X^2] - (E[X])^2`, if :math:`E[X] = 0`, then :math:`\text{Var}(X) = E[X^2] - 0 = E[X^2]`.

   **Multiple Choice Answers:**

   7. **‚íπ** ‚Äî :math:`\text{Var}(3X - 2) = 3^2 \text{Var}(X) = 9 \times 9 = 81`.

   8. **‚í∂** ‚Äî :math:`\text{Var}(X + Y) = \sigma_X^2 + \sigma_Y^2 = 9 + 16 = 25`, so :math:`\sigma_{X+Y} = \sqrt{25} = 5`.

   9. **‚í∑** ‚Äî :math:`\text{Var}(X) = E[X^2] - (E[X])^2 = 8 - 4 = 4`.

   10. **‚íπ** ‚Äî Using the trick: :math:`E[X^2] = \text{Var}(X) + (E[X])^2 = 4 + 9 = 13`.
