.. _9-5-ci-cb-sigma-unknown:

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch7-1">
      <iframe
         id="video-ch7-1"
         title="STAT 350 – Chapter 7.1 Statistics and Sampling Distributions Video"
         src="https://www.youtube.com/embed/3ZhAnYsmILo?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>

.. admonition:: Slides 📊
   :class: tip

   `Download Chapter 9 slides (PPTX) <https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/slides/
   Chapter%209%20Confidence%20Intervals/L19-21%20Confidence%20Intervals%20for%20Single%20Sample_AC.pptx>`_

Confidence Intervals and Bounds When σ is Unknown
==============================================================================================

So far, we developed confidence regions under the 
simplifying but unrealistic assumption that the population standard deviation :math:`\sigma` is known. 
**In practice, we rarely know** :math:`\sigma` **and must estimate it.**

This creates a fundamental challenge. Using a sample standard deviation :math:`S` in place of the unknown :math:`\sigma` 
introduces **additional uncertainty** that must be accounted for. 
The standard normal distribution is no longer appropriate because it does not capture this 
extra layer of uncertainty.

The solution to this problem comes from a distribution developed by William Sealy Gosset in the 
early 1900s: the **Student's t distribution**.

.. admonition:: Road Map 🧭
   :class: important
   
   * Recognize that in most practical scenarios, :math:`\sigma` **is unknown**
     and must be estimated by :math:`S`.
   * Understand that when :math:`S` replaces :math:`\sigma`, the new pivotal quantity follows a *t*-**distribution**.
   * Derive confidence intervals and bounds based on the new *t*-distribution.
   * Understand the basic properties of *t*-distributions.
   * Learn what it means for a statistical procedure to be **robust**. Recognize the requirements for
     *t*-based procedures to be robust.


William Gosset and the Birth of Student's *t*-Distribution
-----------------------------------------------------------------

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter9/William-Gosset.jpg
   :alt: Portrait of William Sealy Gosset (Student)
   :figwidth: 35%
   :align: right

   William S. Gosset (1876-1937)

In 1908, William Sealy Gosset, a chemist and statistician employed by the Guinness brewery in Dublin, 
Ireland, published a paper titled "The Probable Error of a Mean" in the journal *Biometrika*. 
Due to Guinness company policy that prohibited employees from publishing their research, Gosset published 
under the pseudonym "Student"—leading to the now-famous *Student's* *t*-distribution.

Gosset's work at Guinness involved quality control for beer production. He needed statistical methods that 
worked reliably with small samples, as testing large quantities of beer would have been wasteful. 
Specifically, he faced the challenge of making inferences about a population mean when the population 
standard deviation was unknown and had to be estimated from the same limited sample.

His mathematical solution—the *t*-distribution—accounts for the added uncertainty of estimating :math:`\sigma`
with :math:`S`. This breakthrough has become one of the most widely used 
statistical tools across virtually all fields of scientific inquiry.

The *t*-Statistic and Its Distribution
---------------------------------------

To construct confidence regions, we have so far relied on the fact that the 
pivotal quantity :math:`\frac{\bar{X}-\mu}{\sigma/\sqrt{n}}` 
follows a standard normal distribution under certain assumptions. When 
the unknown :math:`\sigma` is replaced by
its estimator :math:`S`, however, the resulting statistic

.. math::
   T_{n-1} = \frac{\bar{X} - \mu}{S/\sqrt{n}}

**no longer follows a standard normal distribution**. Instead, it follows a
*t*-**distribution**. 

The *t*-distribution is a family of continuous distributions parameterized by
:math:`\nu` (Greek letter "nu"; also called the **degrees of freedom or df**). 
A *t*-statistic constructed using a sample of size :math:`n` has
:math:`\nu = n-1`. The subscript in :math:`T_{n-1}` reflects this fact, although it is often
ommitted when the context makes it clear or when the detail is unnecessary.

.. admonition:: Standardization, Studentization, and Pivotal Quantity
   :class: important

   So far, we have called the transformation of a general random variable :math:`X` into 
   :math:`\frac{X-\mu_X}{\sigma_X}` the **standardization** of :math:`X`. 
   When the sample standard deviation :math:`S_X` is used instead of :math:`\sigma_X`, giving

   .. math:: 
      \frac{X-\mu_X}{S_X},

   we call this the **studentization** of :math:`X`.

   Both transformations are variants of **pivotal quantities**, which are functions of :math:`X`
   constructed so that their distributions do not depend on the unkown parameters of :math:`X`. 

Properties of *t*-Distributions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter9/t-pdfs.png
   :alt: Student t density with various df overlaying the standard normal
   :figwidth: 60%
   :align: center

   *t*-densities with various degrees of freedom; the curve corresponding to
   :math:`+\infty` is the standard normal PDF.

1. A *t*-distribution is **symmetric around zero**, similar to the standard normal distribution.
2. It has **heavier tails** than the standard normal distribution, 
   reflecting the additional uncertainty from estimating :math:`\sigma`. 
   This means that a *t*-distribution is **always more spread out** than the standard normal distribution
   for any finite degrees of freedom.
3. The smaller the sample size, the heavier the tails. 
   The distribution **approaches the standard normal distribution** as the degrees of freedom increase.

.. admonition:: The PDF of a *t*-distribution
   :class: important 

   The probability density function of a *t*-distribution is given by:

   .. math::
      f_T(t) = \frac{1}{\sqrt{\nu\pi}} \frac{\Gamma(\frac{\nu+1}{2})}{\Gamma(\frac{\nu}{2})} \left(1 + \frac{t^2}{\nu}\right)^{-(\nu+1)/2}

   Where :math:`\Gamma`, the gamma function, is a generalization of the factorial function.
   Just like normal distributions, we rely on tables or software to compute probabilities
   and percentiles involving *t*-distributions.

Deriving *t*-Based Confidence Regions
--------------------------------------------------------

Preliminaries and Assumptions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The derivation of *t*-based confidence intervals requires a similar set of assumptions as
before. The only difference is that :math:`\sigma` is now unknown.

1. The data :math:`X_1, X_2, \cdots, X_n` must be an *iid* sample 
   from a population with mean :math:`\mu` and variance :math:`\sigma^2.`
2. Either the population is normally distributed, or we have sufficiently large :math:`n`
   for the CLT to hold.
3. Both :math:`\mu` and :math:`\sigma` are unknown.

We also need to define the *t*-critical values. A *t*-critical value, denoted :math:`t_{\alpha/2, \nu}`,
is the point on the *t*-distribution with :math:`\nu` degrees of freedom such that its
upper-tail area equals :math:`\alpha/2`. The notation includes an additional subscript 
for the degrees of freedom, since its location also depends on
the specific *t*-distribution on which it is defined.

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter9/CI-t-critical-value-labeled.png 
   :figwidth: 90%
   :align: center 
   :alt: Locations of t-critical values on a t density curve

Derivation of the Confidence Interval
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Similar to the case with known :math:`\sigma`, we derive a confidence interval for :math:`\mu` using 
the pivotal method. For the degrees of freedom :math:`n-1`, the following statement is true by
the definition of :math:`t_{\alpha/2, n-1}`:

.. math::
   P\left(-t_{\alpha/2, n-1} < T_{n-1} < t_{\alpha/2, n-1}\right) = C.

Replace :math:`T_{n-1}` with the new pivotal quantity:

.. math::
   P\left(-t_{\alpha/2, n-1} < \frac{\bar{X} - \mu}{S/\sqrt{n}} < t_{\alpha/2, n-1}\right) = C.

Through algebraic pivoting, we isolate :math:`\mu` to obtain:

.. math::
   P\left( \bar{X} - t_{\alpha/2, n-1} \frac{S}{\sqrt{n}}< \mu < \bar{X} + t_{\alpha/2, n-1} \frac{S}{\sqrt{n}}\right) = C.

Therefore, the :math:`C\cdot100\%` confidence interval is:

.. math::
   \bar{X} \pm t_{\alpha/2, n-1} \frac{S}{\sqrt{n}}.

Summary of *t*-Based Confidence Intervals and Bounds
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We leave it to the reader to work out the details of deriving the upper and lower
confidence bounds under a *t*-distribution. The results follow the same pattern as their 
:math:`z` equivalents; the margin of error will be computed with 
a smaller critical value :math:`t_{\alpha, n-1}` instead of :math:`t_{\alpha/2, n-1}`.

In summary, when we have :math:`\bar{x}` and :math:`s` from an observed sample,
we use the following formulas to compute confidence regions.


.. flat-table::
   :header-rows: 1
   :widths: 2 3
   :align: center
   :width: 70%

   * - :cspan:`1` Confidence Regions When :math:`\sigma` Is Unkonwn

   * - **Confidence Interval**
     - .. math::
         \bar{x} \pm t_{\alpha/2, n-1} \frac{s}{\sqrt{n}}

   * - **Lower Confidence Bound**
     - .. math::
         \bar{x} - t_{\alpha, n-1} \frac{s}{\sqrt{n}}

   * - **Upper Confidence Bound**
     - .. math::
         \bar{x} + t_{\alpha, n-1} \frac{s}{\sqrt{n}}








.. admonition:: Example 💡: Cholesterol Reduction Study

   A pharmaceutical company is testing a new drug designed to lower LDL cholesterol levels. 
   In a clinical trial, 15 patients with high cholesterol received the drug for eight weeks, 
   and the reduction in their LDL cholesterol (in mg/dL) was measured.

   The sample mean reduction was :math:`\bar{x} = 23.4` mg/dL with a sample standard deviation 
   of :math:`s = 6.8` mg/dL. Construct a 95% confidence interval for the true mean reduction :math:`\mu`.

   **Step 1**: Identify the key information

      - Sample size: :math:`n = 15`
      - Sample mean: :math:`\bar{x} = 23.4` mg/dL
      - Sample standard deviation: :math:`s = 6.8` mg/dL
      - Confidence level: :math:`95\%` (:math:`\alpha = 0.05`)
      - Degrees of freedom: :math:`\nu = n - 1 = 14`

   **Step 2**: Find the critical value
      
   .. math::
      t_{0.025, 14} = 2.145
   
   .. code-block:: r

      qt(0.025, df = 14, lower.tail=FALSE)  # Returns 2.145


   **Step 3**: Calculate the margin of error

   .. math::
      \text{ME} = t_{0.025, 14} \cdot \frac{s}{\sqrt{n}} = 2.145 \cdot \frac{6.8}{\sqrt{15}} \approx 3.76 \text{ mg/dL}

   **Step 4**: Construct the confidence interval

   .. math::
      \bar{x} \pm \text{ME} = 23.4 \pm 3.76 = [19.64, 27.16] \text{ mg/dL}

   **Interpretation**: We are 95% confident that the true mean reduction in LDL cholesterol with 
   this drug is captured by the region between 19.64 and 27.16 mg/dL.

The Effect of Sample Size on *t*-Confidence Regions
----------------------------------------------------------

As with :math:`z`-confidence regions, a large :math:`n` makes :math:`t`-confidence regions more
precise in general. However, the ways in which :math:`n` influences this phenomenon are more multifaceted 
for *t*-based methods:

1. A larger :math:`n` reduces the true standard error, :math:`\sigma/n`.
2. Although the true standard error is unknown, its estimator :math:`S/n` targets it
   more accurately with larger :math:`n`.
3. The critical value itself decreases as :math:`n` increases, which further narrows the confidence region.

To see how the third point holds, see :numref:`dfs-comparison` below:

.. _dfs-comparison:
.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter9/dfs-comparison.png 
   :figwidth: 70%
   :align: center 
   :alt: t critical values with different degrees of freedom

   Two-sided *t*-critical values for :math:`\alpha = 0.05` with different degrees of freedom

In :numref:`dfs-comparison`, the upper tails of two *t*-distributions are compared: one with
:math:`df=n-1=99` and the other with :math:`df=n-1=9`. Recall that higher degrees of freedom
(and larger sample size) are associated with a lighter tail on a *t*-distribution.
As :math:`n` grows from 10 to 100, the difference in tail weight causes the critical value
to **move closer to the center (zero)** in order to maintain an area of :math:`\alpha/2` on its right.

In general, for the same confidence level and any two sample sizes :math:`n_1 < n_2`, 
it always holds that 

.. math::
   t_{\alpha/2, n_1-1} > t_{\alpha/2, n_2-1}.

A smaller critical value leads to a smaller margin of error if :math:`s` is held constant, 
which in turn results in a more precise (narrower) confidence region.
This relationship does not hold strictly in practice since :math:`S` fluctuates with data,
but the overall tendency remains.

Comparison with :math:`z`-Confidence Regions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The above result implies that the :math:`z`-confidence regions—
which can loosely be considered *t*-confidence regions with an
"infinite" dataset for sample variance computation—are more precise on average than their *t*-based parallels.

When is a :math:`t`-Confidence Region Appropriate?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In practice, the true variance :math:`\sigma^2` is rarely known, leaving *t*-procedures 
as our only option. On the rare occasions when :math:`\sigma^2` is known, it is
always preferabale to use this true information.

Sample Size Planning When σ Is Unknown
----------------------------------------

Sample size planning is more challenging when :math:`\sigma` is unknown. 
Suppose we want :math:`n` such that

.. math::
   \text{ME} = t_{\alpha/2, n-1} \frac{s}{\sqrt{n}} \leq ME_{max}

for some given maximum margin of error, :math:`ME_{max}`. 
By taking similar steps as in Chapter 9.3.2, we have 

.. math::
   n \geq \left(\frac{t_{\alpha/2, n-1}s}{ME_{max}}\right)^2.

We now see that the problem is circular. To determine :math:`n`, we need the :math:`t`-critical value, 
which depends on :math:`n`. Additionally, we need a value for :math:`s`, which we don't 
have before collecting data. To address this issue, we use an iterative approach involving the following steps:

1. **Obtain a planning value** :math:`s_*`. This can be done by

   - Using :math:`s` from a pilot study or previous research,
   - Making an educated guess based on the expected range, or
   - Using a conservative upper bound when uncertainty is high.

2. **Update** :math:`n` **iteratively**.
   
   - Start with an initial guess using the :math:`z`-critical value: 
     :math:`n_0 = \left(\frac{z_{\alpha/2} s_*}{ME_{max}}\right)^2`.
   - Calculate the *t*-critical value using :math:`df = n_0 - 1`.
   - Recalculate :math:`n` using the *t*-critical value.
   - Repeat until convergence.

This process typically converges quickly, often in just a few iterations.

Robustness of the :math:`t`-Procedures
------------------------------------------

A statistical procedure is considered **robust** if it performs reasonably well even when its 
assumptions are somewhat violated. The :math:`t`-procedures show good robustness against moderate 
departures from normality, especially as sample size increases.

.. flat-table:: 
   :header-rows: 1
   :widths: 1 4

   * - :cspan:`1` Guidelines for Using *t*-Procedures When Normality May Not Hold

   * - :math:`n < 15`
     - The population distribution should be approximately normal. 
       Check the sample data with normal probability plots.

   * - :math:`15 ≤ n < 40`
     - A :math:`t`-procedure works well with some mild skewness. Avoid using with strongly skewed data 
       or data containing outliers.

   * - :math:`n ≥ 40`
     - A :math:`t`-procedure is generally reliable even with moderately skewed distributions, 
       thanks to the Central Limit Theorem.

Regardless of sample size, the procedure is sensitive to outliers, which can strongly 
influence both :math:`\bar{x}` and :math:`s`. **Always inspect your data for outliers** 
before applying a :math:`t`-procedure.

Bringing It All Together
-----------------------------

.. admonition:: Key Takeaways 📝
   :class: important

   * The **Student's** *t*-**distribution** provides the appropriate framework for quantifying uncertainty 
     about a population mean when the population standard deviation is unknown.
   * The pivotal quantity :math:`T_{n-1}= \frac{\bar{X}-\mu}{S/\sqrt{n}}` now follows a *t*-distribution 
     with :math:`n-1` degrees of freedom.
   * The resulting confidence regions account for the additional uncertainty in estimating :math:`\sigma`,
     and are **wider than their** :math:`z` **parallels on average**.
   * The *t*-procedures are **robust to moderate violations of the normality assumption**. The robustness grows with 
     the sample size.

Exercises
~~~~~~~~~~~~~

1. A quality control engineer wants to estimate the mean tensile strength of steel cables. 
   A sample of 25 cables yields a mean strength of 3450 N with a standard deviation of 120 N. 
   Construct a 99% confidence interval for the mean strength.

2. A pilot study with 8 observations yielded a sample standard deviation of :math:`s = 15`. 
   If a researcher wants to estimate the population mean with a margin of error of no more 
   than 5 units at 95% confidence, how many observations should be planned for the full study?