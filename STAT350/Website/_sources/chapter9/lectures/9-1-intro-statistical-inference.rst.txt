.. _9-1-intro-statistical-inference:

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch7-1">
      <iframe
         id="video-ch7-1"
         title="STAT 350 ‚Äì Chapter 7.1 Statistics and Sampling Distributions Video"
         src="https://www.youtube.com/embed/P3Nyg84h0A8?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>

.. admonition:: Slides üìä
   :class: tip

   `Download Chapter 9 slides (PPTX) <https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/slides/
   Chapter%209%20Confidence%20Intervals/L19-21%20Confidence%20Intervals%20for%20Single%20Sample_AC.pptx>`_
   
Introduction to Statistical Inference
===============================================================

After developing the foundational tools of probability theory, exploring random variables, and 
understanding sampling distributions, we have finally arrived at the core of statistical practice: 
**statistical inference**. This exciting chapter marks our transition from describing uncertainty to 
making decisions under uncertainty‚Äîthe essence of statistics as a discipline.

.. admonition:: Road Map üß≠
   :class: important
   
   * Recall the relationship between a **population parameter** and its **estimator**. Estimator yields
     **estimates** that vary from sample to sample, and their behavior is described by the **sampling distribution**.
   * Select an optimal estimator out of many candidates by evaluating their distributional properties.
     Know the definitions of **bias and variance**, and use them appropriately as selection criteria. 
     Be able to compute bias for some simple estimators.
   * Know the definition of **Minimum Variance Unbiased Estimator (MVUE)**.

From Population Parameters to Estimators
-------------------------------------------------

In statistical research, we aim to understand certain characteristics of a population that are fixed but 
unknown to us. These characteristics, called **parameters**, include:

- The population mean (:math:`\mu`),
- The population variance (:math:`\sigma^2`), and
- Other quantities describing the population's distribution.

The fundamental challenge we face is that examining every member of a population is typically 
impractical or impossible, even though doing so would be required to determine these characteristics with certainty.
Instead, we rely on a representative sample to make inferences about the popoulation and
specify *how uncertain* we are about the result.

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter9/parameter-estimator.png
   :alt: Diagram showing relationship between population parameters and sample estimates
   :figwidth: 75%
   :align: center

   Relationship of Parameters, estimators, and estimates

Point Estimators and Estimates
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

An **estimator** is a **random variable** which contains instructions on how to use sample data to calculate 
an estimate of a population parameter. When an estimator is designed to yield a
single numerical value as an outcome, it is called a **point estimator**.
The single-valued outcome is called a **point estimate**. 

.. admonition:: Exampleüí°: :math:`\bar{X}` is a Point Estimator of :math:`\mu`
   :class: note

   One possible **point estimator** of the population mean :math:`\mu` is the sample mean :math:`\bar{X}`. 
   Its definition contains instructions on the calculation procedure‚Äîadding all 
   observed values and dividing by the sample size. 
   A **point estimate** :math:`\bar{x}` is obtained as a single concrete numerical value 
   (e.g., :math:`\bar{x} = 42.7`) by applying these instructions to an observed sample.



A Parameter Has Many Point Estimators
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

There are many different ways to guess a parameter value using data. For example,
we can choose to estimate the population mean :math:`\mu` with

* The sample mean :math:`\bar{X}` (the typical choice),
* The sample median :math:`\tilde{X}` (for symmetric distributions whose true mean and true median are
  equal, this is reasonable),
* The mean of all data points except :math:`m` most extreme values, etc.

It is easy to generate many *reasonable* candidates. The key question is, then: 
**What objective criteria can we use to determine which estimator is better than others?**

Recall from Chapter 7 that each estimator has its own distribution, called
the **sampling distribution**. To answer the question, we focus on two key properties 
of the sampling distribution: **bias and variance**. 
For the remainder of this section, we use :math:`\theta` (Greek letter "theta") to denote a population parameter,
and :math:`\hat{\theta}` ("theta hat") for an estimator of :math:`\theta`.

Evaluating Estimators
-----------------------------------------

Bias: Does the Estimator Target the Right Value?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The **bias** of an estimator measures whether it systematically overestimates or underestimates 
the parameter of interest. It is mathematically defined as

.. math::

   \text{Bias}(\hat{\theta}) = E[\hat{\theta}] - \theta.

An estimator :math:`\hat{\theta}` is **unbiased** if :math:`\text{Bias}(\hat{\theta})=0`, or equivalently, 
if its expected value equals the parameter it aims to estimate:

.. math::

   \mathbb{E}[\hat{\theta}] = \theta.

.. admonition:: Example üí°: Is the Sample Mean Unbiased?
   :class: note

   We know that :math:`\mathbb{E}[\bar{X}] = \mu`. So the sample mean :math:`\bar{X}` is an **unbiased
   estimator** of :math:`\mu`.

Variance: How Precise is the Estimator?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The **variance** of an estimator quantifies the spread of its sampling distribution‚Äîessentially how much the estimator fluctuates 
from sample to sample. Lower variance indicates greater precision and reliability.

**Minimum-Variance Unbiased Estimator (MVUE)**

When choosing froom a set of unbiased estimators, we typically prefer the one with the smaller variance, 
as this reduces the expected "distance" between the estimate and the true parameter. 
An ideal estimator is called **minimum-variance unbiased estimator (MVUE)**‚Äîthat is,
an estimator with the **smallest possible variance among all unbiased estimators** for a given parameter. 

Whether an estimator is MVUE depends on the population distribution, and proving this property generally requires
advanced theoretical tools. Nonetheless, we will encounter several examples as we explore the properties 
of key estimators.

Bias-Variance Tradeoff
~~~~~~~~~~~~~~~~~~~~~~~~~~

An unbiased estimator is not always a better choice than a biased estimator. If an estimator
is slightly biased but has a significantly lower variance than its unbiased competitor, 
then there may be situations where
the former is more practical. In fact, bias and variance often exhibit a trade-off relationship;
reducing bias in an estimator may increase its variance, and vice versa. 
We should always take the **degree** of both bias and variance into consideration when
choosing an estimator. See Figure :numref:`bias-variance` for a visual illustratiion:

.. _bias-variance:
.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter9/bias-variance-tradeoff.png
   :alt: Comparison of biased versus unbiased estimators
   :align: center
   :figwidth: 90%
   
   Comparison of biased and unbiabsed estimators

Important Estimators and Their Properties
---------------------------------------------

To solidify the concepts of bias and variance in estimators, let's examine several common estimators 
and their properties. In all cases below, suppose :math:`X_1, X_2, \cdots, X_n` form an *iid* sample from
the same population.

A. Sample Mean for Population Mean
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The sample mean :math:`\bar{X} = \frac{1}{n}\sum_{i=1}^{n}X_i`
serves as an estimator for the population mean :math:`\mu`. We know that :math:`\mathbb{E}[\bar{X}] = \mu`, 
which makes it an unbiased estimator of :math:`\mu`.

When sampling from a normal distribution, the sample mean is also a minimum-variance unbiased estimator (MVUE).

B. Sample Proportion for True Probability
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Suppose identical trials are performed :math:`n` times, and whether an event :math:`A` occurs or not in each trial
is recorded using Bernoulli random variables of the following form:

.. math::

   I_i(A) =
   \begin{cases}
   1, & \text{with probability } P(A) \\
   0, & \text{with probability } 1 - P(A)
   \end{cases}

for :math:`i=1,2, \cdots, n`.

Define :math:`\hat{p} = \frac{1}{n}\sum_{i=1}^n I_i(A)`. Then :math:`\hat{p}` is an unbiased
estimator for :math:`P(A)` because

.. math::
   E[\hat{p}] &= E\left[\frac{1}{n}\sum_{i=1}^n I_i(A)\right] = \frac{1}{n}\sum_{i=1}^n E[I_i(A)]\\
   &=\frac{1}{n}\sum_{i=1}^n (1 \cdot P(A) + 0 \cdot (1-P(A))) \\
   &= \frac{n}{n}P(A) = P(A).

This result can be used to define an unbiased estimator for an entire probability distribution.

(a) Estimating a PMF
^^^^^^^^^^^^^^^^^^^^^^

   Suppose :math:`X_1, X_2, \cdots, X_n` form an *iid* sample of a discrete population :math:`X`.
   For each value :math:`x \in \text{supp}(X)`, define:

   .. math::

      \hat{p}_X(x) = \frac{1}{n}\sum_{i=1}^{n}I(X_i = x),

   where :math:`I(X_i = x)=1` if the :math:`i`-th sample point equals :math:`x`, and :math:`0` otherwise.
   Then, by the same logic as the general case, :math:`\hat{p}_X(x)` is an unbiased estimator
   of :math:`p_X(x)` for each :math:`x \in \text{supp}(X)`:

   .. math::
      E[\hat{p}_X(x)] = p_X(x) = P(X = x).

(b) Estimating a CDF
^^^^^^^^^^^^^^^^^^^^^

   For continuous random variables, we can estimate the cumulative distribution function (CDF) at 
   any :math:`x \in \text{supp}(X)` using:

   .. math::

      \hat{F}_X(x) = \frac{1}{n}\sum_{i=1}^{n}I(X_i \leq x).

   :math:`\hat{F}_X(x)` represents the proportion of observations less than or equal to :math:`x`. 
   As another special application of the general case, this is an unbiased 
   estimator of the true CDF :math:`F_X(x)`. That is,

   .. math::

      E[\hat{F}_X(x)] = P(X \leq x) = F_X(x).

C. Sample Variance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

..
   If we **knew the population mean** :math:`\mu`, which we typically don't, we could estimate the population 
   variance as:



   This is an unbiased estimator: :math:`E[\hat{\sigma}^2] = \sigma^2` (show as an exercise).


The sample variance:

.. math::

   S^2 = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X})^2

is an unbiased estimator of :math:`\sigma^2`. To see how, let us compute the expected value.

1. Add and subtract :math:`\mu` inside each squared term:
   :math:`(X_i - \bar{X})^2 = ((X_i - \mu) - (\bar{X} - \mu))^2`.

2. Using Step 1,

   .. math::
      E[S^2]=&E\left[\frac{1}{n-1}\sum_{i=1}^n ((X_i - \mu) - (\bar{X} - \mu))^2\right]\\
      =&\frac{1}{n-1}\sum_{i=1}^nE[((X_i - \mu) - (\bar{X} - \mu))^2]\\
      =&\frac{1}{n-1}\sum_{i=1}^n E[(X_i - \mu)^2 -2(X_i - \mu)(\bar{X} - \mu) + (\bar{X} - \mu)^2]\\
      =&\frac{1}{n-1}\sum_{i=1}^n E[(X_i - \mu)^2] -2E[(X_i - \mu)(\bar{X} - \mu)] + E[(\bar{X} - \mu)^2]\\

3. The first and third expecations are respectively the variances of :math:`X_i` and :math:`\bar{X}`
   by definition.

   .. math::
      &E[(X_i - \mu)^2] = Var(X_i) = \sigma^2\\
      &E[(\bar{X} - \mu)^2] = Var(\bar{X}) = \frac{\sigma^2}{n}

4. The expectation :math:`E[(X_i - \mu)(\bar{X} - \mu)]` can be simplified to 
   
   .. math:: 
      &E[(X_i - \mu)\cdot \frac{1}{n}\sum_{j}(X_j - \mu)]= \frac{1}{n} \sum_{j=1}^n E[(X_i - \mu)(X_j - \mu)]\\
      &= \frac{1}{n}E[(X_i - \mu)(X_i - \mu)] = \frac{1}{n}E[(X_i -\mu)^2] = \frac{\sigma^2}{n}

   All terms involving indices :math:`j\neq i` disappear in the final steps since
   :math:`Cov(X_i, X_j) = E[(X_i-\mu)(X_j - \mu)] =0` due to their independnce.
   
5. Substituting the results back to the final line of Step 2, 
   we can verify that the sum simplifies to :math:`(n-1)\sigma^2`. Therefore,
   
   .. math::
      E[S^2] = \frac{1}{n-1} (n-1)\sigma^2 = \sigma^2.

This shows why we devide by :math:`n-1` instead of :math:`n` when computing a sample variance; 
this is key to making the estimator unbiased.

When sampling from normal populations, the sample variance :math:`S^2` is also the MVUE for the 
population variance :math:`\sigma^2`. 

.. admonition:: Additional Exercise üí°: Sample Variance When :math:`\mu` is Known
   :class: note

   In an unrealistic situation where :math:`\sigma^2` is not known but :math:`\mu` is,
   we must incorporate the known information into our variance estimation. Show that 

   .. math::

      \hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^{n}(X_i - \mu)^2
   
   is an unbiased estimator of :math:`\sigma^2`.

.. admonition:: The Biased Case of Sample Standard Deviation
   :class: important

   While the sample variance :math:`S^2` is an unbiased estimator of :math:`\sigma^2`, 
   the sample standard deviation :math:`S = \sqrt{S^2}` is a **biased estimator** of the 
   population standard deviation :math:`\sigma`.

   Its biasedness can be shown using a concept called *Jensen's inequality* and the fact that the square root 
   is a concave function. The details are beyond the scope of this course, but you are encouraged to
   read about the topic independently.

   We still use :math:`S` as our estimator for :math:`\sigma` because the formula is 
   straightforward and intuitive, while the bias is typically small, especially for larger sample sizes.


Brining It All Together
---------------------------

In this chapter, we've transitioned from probability theory to statistical inference by exploring the 
properties of point estimators.

.. admonition:: Key Takeaway üìù
   :class: important

   1. **Estimators** yield **estimates** intended to approximate a fixed but unknown **population parameter**.
      Estimates vary from sample to sample according to their **sampling distribution**.

   2. The two most important distributional characteristics of an estimator are **bias and variance**.
      Unbiased estimators target the correct parameter on average, while low-variance 
      estimators provide more consistent results across samples.

   3. **Minimum Variance Unbiased Estimators (MVUEs)** have the smallest variance among all unbiased estimators of
      a target parameter.

   4. We can show that :math:`\bar{X}` are :math:`S^2` are unbiased estimators of their respective targets,
      :math:`\mu` and :math:`\sigma^2`. When the population is normal, they are also the MVUEs.
   

.. _9-1-exercises:

=============================================================
Exercises: Introduction to Statistical Inference
=============================================================

.. contents:: Table of Contents
   :local:
   :depth: 2

.. admonition:: Learning Objectives üéØ
   :class: info

   These exercises will help you:

   ‚Ä¢ Distinguish between **parameters** and **statistics/estimators**
   ‚Ä¢ Understand and interpret the **bias** of an estimator
   ‚Ä¢ Understand **variance** and **standard error** as measures of estimator precision
   ‚Ä¢ Recognize that among unbiased estimators, **lower variance is preferred**
   ‚Ä¢ Connect sample size to the **precision** of the sample mean

.. admonition:: Key Terminology
   :class: tip

   - **Parameter**: A fixed (but often unknown) numerical value describing a population (e.g., Œº, œÉ)
   - **Statistic**: A numerical summary computed from sample data (e.g., xÃÑ, s)
   - **Estimator**: A statistic used to estimate a specific population parameter
   - **Bias**: :math:`\text{Bias}(\hat{\theta}) = E[\hat{\theta}] - \theta`; an unbiased estimator has :math:`E[\hat{\theta}] = \theta`
   - **Standard Error**: The standard deviation of an estimator's sampling distribution; for :math:`\bar{X}`: :math:`SE = \sigma/\sqrt{n}`

Exercises
---------

.. admonition:: Exercise 1: Parameter vs Statistic Identification
   :class: note

   For each of the following, identify whether the quantity described is a **parameter** (fixed, unknown population characteristic) or a **statistic** (calculated from sample data).

   a. The average response time of all requests to a company's web server over the past year.

   b. The sample mean CPU temperature calculated from 50 randomly selected measurements during a stress test.

   c. The proportion of all manufactured semiconductors from a production line that contain defects.

   d. The median battery life observed in a sample of 30 laptops tested under standard conditions.

   e. The true variance of tensile strength for all steel cables produced by a manufacturer.

   f. The sample standard deviation of fuel efficiency measurements from 25 test vehicles.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Parameter**

      This describes the average for the *entire population* of web server requests over the year. It is a fixed value, denoted :math:`\mu`.

      **Part (b): Statistic**

      This is computed from *sample data* (50 measurements). It will vary depending on which measurements are selected, denoted :math:`\bar{x}`.

      **Part (c): Parameter**

      This describes a characteristic of the *entire population* of semiconductors. It is a fixed proportion, denoted :math:`p`.

      **Part (d): Statistic**

      This is the median from a *sample* of 30 laptops. It is computed from sample data and would vary with different samples.

      **Part (e): Parameter**

      This is the true variance for *all* steel cables‚Äîthe entire population. It is denoted :math:`\sigma^2`.

      **Part (f): Statistic**

      This is computed from a *sample* of 25 vehicles. It is denoted :math:`s` and serves as an estimate of :math:`\sigma`.

      **Key distinction**: Parameters describe populations and are typically unknown; statistics are computed from samples and serve as estimates of parameters.

----

.. admonition:: Exercise 2: Understanding Estimators
   :class: note

   A quality control engineer wants to estimate the mean diameter :math:`\mu` of ball bearings produced by a manufacturing process. She considers using the following estimators based on a random sample :math:`X_1, X_2, \ldots, X_n`:

   - **Estimator A**: :math:`\hat{\mu}_A = \bar{X} = \frac{1}{n}\sum_{i=1}^{n} X_i` (sample mean)

   - **Estimator B**: :math:`\hat{\mu}_B = \tilde{X}` (sample median)

   - **Estimator C**: :math:`\hat{\mu}_C = \frac{X_{(1)} + X_{(n)}}{2}` (midrange: average of min and max)

   a. The sample mean :math:`\bar{X}` is always unbiased for :math:`\mu`. For symmetric distributions, the median and midrange are also reasonable estimators that target the center. If multiple estimators target the same parameter, what property would you use to choose between them?

   b. Which estimator uses all :math:`n` observations in its calculation? Which uses only 2 observations?

   c. The midrange (Estimator C) uses only the minimum and maximum values. What does this suggest about its reliability compared to the sample mean?

   d. If you had to choose one estimator for a symmetric population, which would you choose and why?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Choosing among estimators**

      When multiple estimators target the same parameter, we choose based on **variance** (or equivalently, standard error). Among estimators that are centered on the parameter, we prefer the one with the **smallest variance** because it produces estimates that are more consistently close to the true value.

      **Important note**: :math:`\bar{X}` is unbiased for :math:`\mu` always. For symmetric distributions, the median and midrange are centered on the distribution's midpoint, but finite-sample unbiasedness for :math:`\mu` is not guaranteed in general.

      **Part (b): Observations used**
      
      - **Sample mean** (Estimator A): Uses all :math:`n` observations
      - **Sample median** (Estimator B): Uses all :math:`n` observations (though only the middle values directly determine the result)
      - **Midrange** (Estimator C): Uses only 2 observations (the minimum and maximum)

      **Part (c): Reliability of midrange**

      The midrange uses only the two most extreme observations, making it:
      
      - Highly sensitive to outliers
      - More variable from sample to sample
      - Less reliable than estimators that use all the data
      
      The sample mean "averages out" individual variations across all observations, leading to more stable (lower variance) estimates.

      **Part (d): Best choice**

      The **sample mean** is the best choice because:
      
      - It uses all available information (all :math:`n` observations)
      - It is guaranteed unbiased for :math:`\mu`
      - It has the lowest variance among common estimators for normal populations
      - It is not unduly influenced by any single observation (unlike the midrange)

----

.. admonition:: Exercise 3: Understanding Bias
   :class: note

   a. Define what it means for an estimator to be **unbiased**.

   b. The sample mean :math:`\bar{X}` is an unbiased estimator of the population mean :math:`\mu`. Explain what this means in practical terms.

   c. Consider using the sample maximum :math:`X_{(n)}` to estimate the population mean. Would this be biased or unbiased? If biased, in which direction (overestimate or underestimate)?

   d. A researcher uses :math:`\frac{1}{n}\sum_{i=1}^{n}(X_i - \bar{X})^2` instead of :math:`S^2 = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X})^2` to estimate variance. 

      (i) Which formula is unbiased for :math:`\sigma^2`?
      
      (ii) Does the biased formula overestimate or underestimate :math:`\sigma^2` on average?
      
      (iii) For :math:`n = 10`, by approximately what percentage does the biased formula underestimate :math:`\sigma^2`?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Definition of unbiased**

      An estimator is **unbiased** if its expected value equals the parameter it's estimating:

      .. math::
         E[\hat{\theta}] = \theta

      In other words, if we could take many, many samples and compute the estimator each time, the average of all those estimates would equal the true parameter value.

      **Part (b): Practical meaning**

      Saying :math:`\bar{X}` is unbiased for :math:`\mu` means:
      
      - On average, the sample mean equals the population mean
      - There's no systematic tendency to overestimate or underestimate
      - Individual samples may give :math:`\bar{x}` above or below :math:`\mu`, but these errors balance out in the long run
      - :math:`E[\bar{X}] = \mu` regardless of sample size

      **Part (c): Sample maximum as estimator**

      The sample maximum :math:`X_{(n)}` would be a **biased** estimator of :math:`\mu`:
      
      - It would **overestimate** :math:`\mu` on average
      - The maximum of a sample is typically above the center of the distribution
      - Only in unusual samples would the maximum be below :math:`\mu`

      **Part (d): Variance formula comparison**

      **(i)** The formula with :math:`n-1` in the denominator (:math:`S^2`) is **unbiased** for :math:`\sigma^2`.

      **(ii)** The formula with :math:`n` in the denominator **underestimates** :math:`\sigma^2` on average.

      **(iii)** For :math:`n = 10`:

      The biased formula gives :math:`\frac{n-1}{n} = \frac{9}{10} = 0.9` of the true variance on average.

      This is an underestimate of :math:`1 - 0.9 = 0.1 = 10\%`.

      **Note**: This is why statistical software uses :math:`n-1` by default‚Äîthe correction ensures we get unbiased estimates of population variance.

----

.. admonition:: Exercise 4: Why n-1 in Sample Variance?
   :class: note

   A data scientist is analyzing algorithm runtimes and debates whether to use :math:`n` or :math:`n-1` in the denominator when computing sample variance.

   a. Define both estimators:

      - :math:`\hat{\sigma}^2_{biased} = \frac{1}{n}\sum_{i=1}^{n}(X_i - \bar{X})^2`
      - :math:`S^2 = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X})^2`

      Which one is unbiased for :math:`\sigma^2`?

   b. Express :math:`\hat{\sigma}^2_{biased}` in terms of :math:`S^2`.

   c. For a sample of size :math:`n = 5`, by what percentage does :math:`\hat{\sigma}^2_{biased}` underestimate :math:`\sigma^2` on average?

   d. As :math:`n \to \infty`, what happens to the difference between these two estimators? Why does this make the choice less important for large samples?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Unbiased estimator**

      :math:`S^2 = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X})^2` is **unbiased** for :math:`\sigma^2`.

      The formula with :math:`n` in the denominator is biased (underestimates :math:`\sigma^2`).

      **Part (b): Relationship**

      .. math::
         \hat{\sigma}^2_{biased} = \frac{n-1}{n} S^2

      **Part (c): Percentage underestimate for n = 5**

      .. math::
         E[\hat{\sigma}^2_{biased}] = \frac{n-1}{n} \sigma^2 = \frac{4}{5} \sigma^2 = 0.8 \sigma^2

      The biased estimator underestimates by :math:`1 - 0.8 = 0.2 = 20\%` on average.

      **Part (d): Large sample behavior**

      As :math:`n \to \infty`:

      .. math::
         \frac{n-1}{n} \to 1

      The two estimators become essentially identical. For :math:`n = 100`, the difference is only 1%. For :math:`n = 1000`, it's 0.1%.

      This makes the choice less important for large samples because the bias becomes negligible compared to the sampling variability.

----

.. admonition:: Exercise 5: Variance of Estimators
   :class: note

   An aerospace engineer is estimating the mean thrust :math:`\mu` of a rocket engine. Two different sampling strategies yield the following estimators:

   - **Strategy 1**: Take :math:`n = 16` independent measurements and compute :math:`\hat{\mu}_1 = \bar{X}_{16}`.

   - **Strategy 2**: Take :math:`n = 64` independent measurements and compute :math:`\hat{\mu}_2 = \bar{X}_{64}`.

   Assume the population standard deviation is :math:`\sigma = 200` Newtons.

   a. Both estimators are unbiased. Compute the variance of each estimator.

   b. Compute the standard error (standard deviation of the sampling distribution) for each estimator.

   c. By what factor does the standard error decrease when going from Strategy 1 to Strategy 2?

   d. If the engineer wants to reduce the standard error by half (compared to Strategy 1), how many measurements are needed?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Variance of each estimator**

      For :math:`\bar{X}`: :math:`Var(\bar{X}) = \frac{\sigma^2}{n}`

      - Strategy 1 (:math:`n = 16`): :math:`Var(\hat{\mu}_1) = \frac{200^2}{16} = \frac{40000}{16} = 2500` N¬≤
      
      - Strategy 2 (:math:`n = 64`): :math:`Var(\hat{\mu}_2) = \frac{200^2}{64} = \frac{40000}{64} = 625` N¬≤

      **Part (b): Standard error**

      - Strategy 1: :math:`SE_1 = \sqrt{2500} = 50` N
      
      - Strategy 2: :math:`SE_2 = \sqrt{625} = 25` N

      **Part (c): Factor of decrease**

      .. math::
         \frac{SE_1}{SE_2} = \frac{50}{25} = 2

      The standard error decreased by a factor of **2**.

      Note: Sample size increased by factor of 4, and :math:`\sqrt{4} = 2`.

      **Part (d): Sample size to halve SE**

      To halve the SE from Strategy 1 (from 50 to 25 N), we need:

      .. math::
         \frac{200}{\sqrt{n}} = 25 \implies \sqrt{n} = 8 \implies n = 64

      This confirms Strategy 2. In general, to halve SE, **quadruple** the sample size.

      **R verification:**

      .. code-block:: r

         sigma <- 200
         
         # Variance
         sigma^2 / 16  # 2500
         sigma^2 / 64  # 625
         
         # Standard error
         sigma / sqrt(16)  # 50
         sigma / sqrt(64)  # 25

----

.. admonition:: Exercise 6: Bias vs Variance Tradeoff (Conceptual)
   :class: note

   Consider two estimators for a parameter :math:`\theta`:

   - **Estimator A**: Unbiased (hits the target on average), but high variability
   - **Estimator B**: Slightly biased (consistently a bit off), but very low variability

   a. Draw (or describe) what the sampling distributions of these two estimators might look like relative to the true value :math:`\theta`.

   b. If Estimator A gives values that range from 80 to 120 (centered at 100), and Estimator B gives values that range from 98 to 104 (centered at 102), and the true value is :math:`\theta = 100`, which estimator would you prefer for a single estimate? Explain.

   c. A dartboard analogy is often used: Estimator A hits all around the bullseye (unbiased but imprecise), while Estimator B consistently hits the same spot slightly off-center (biased but precise). Which pattern would give you a better score on average?

   d. In practice, why do statisticians often prefer unbiased estimators even when a biased estimator might occasionally be more accurate?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Sampling distributions**

      - **Estimator A**: A wide, spread-out distribution centered exactly at :math:`\theta = 100`. Values could be far from 100 in either direction, but they average to 100.
      
      - **Estimator B**: A narrow, concentrated distribution centered at 102 (slightly off from :math:`\theta = 100`). Values are consistently close to 102, never exactly hitting 100.

      **Part (b): Preference for single estimate**

      - **Estimator A** (80-120, centered at 100): Could give you anything from 80 to 120. Highly unpredictable.
      
      - **Estimator B** (98-104, centered at 102): Will give you something between 98 and 104. Very predictable, but always slightly high.

      For a single estimate, **Estimator B** might be preferred because:
      
      - The worst case (104) is closer to 100 than Estimator A's worst cases (80 or 120)
      - Even though it's biased, the estimates are all "in the ballpark"
      - Estimator A has a wide range where individual estimates could be quite wrong

      **Part (c): Dartboard analogy**

      - **Estimator A** (all around bullseye): Some darts hit close to center, but many are far away
      - **Estimator B** (consistent off-center): All darts land in a tight cluster, just not at the center

      For overall accuracy, **Estimator B** would typically score better because the consistent small miss beats the highly variable hits and misses of Estimator A.

      **Part (d): Why prefer unbiased estimators**

      1. **Systematic errors compound**: In repeated use or when combining estimates, bias doesn't cancel out, but random errors do
      
      2. **Interpretability**: Unbiased estimates are easier to explain and trust
      
      3. **Unknown bias**: We often don't know the size of the bias, so we can't correct for it
      
      4. **Statistical theory**: Many confidence interval and hypothesis testing procedures assume unbiased estimators
      
      5. **Scientific standards**: Many fields require unbiased methods for credibility

----

.. admonition:: Exercise 7: Standard Error of the Sample Mean
   :class: note

   A software company tests :math:`n = 100` randomly selected user sessions to estimate the mean session duration. Based on historical data, the population standard deviation is known to be :math:`\sigma = 8` minutes.

   a. What is the standard error of the sample mean?

   b. If the company increases the sample size to :math:`n = 400`, what is the new standard error?

   c. By what factor did the standard error decrease when :math:`n` increased from 100 to 400?

   d. How large should :math:`n` be to achieve a standard error of at most 0.5 minutes?

   e. A colleague suggests that doubling the sample size will halve the standard error. Is this correct? Explain.

   .. dropdown:: Solution
      :class-container: sd-border-success

      Given: :math:`\sigma = 8` minutes

      **Part (a): SE with n = 100**

      .. math::
         SE = \frac{\sigma}{\sqrt{n}} = \frac{8}{\sqrt{100}} = \frac{8}{10} = 0.8 \text{ minutes}

      **Part (b): SE with n = 400**

      .. math::
         SE = \frac{8}{\sqrt{400}} = \frac{8}{20} = 0.4 \text{ minutes}

      **Part (c): Factor of decrease**

      .. math::
         \frac{SE_{n=100}}{SE_{n=400}} = \frac{0.8}{0.4} = 2

      The SE decreased by a factor of 2. Note: :math:`n` increased by a factor of 4, and :math:`\sqrt{4} = 2`.

      **Part (d): Required n for SE ‚â§ 0.5**

      .. math::
         \frac{8}{\sqrt{n}} \leq 0.5

      .. math::
         \sqrt{n} \geq \frac{8}{0.5} = 16

      .. math::
         n \geq 256

      Need at least **n = 256** sessions.

      **Part (e): Doubling sample size**

      **Incorrect**. Doubling the sample size reduces the standard error by a factor of :math:`\sqrt{2} \approx 1.41`, not by half.

      To halve the standard error, you must **quadruple** the sample size because:

      .. math::
         SE \propto \frac{1}{\sqrt{n}}

      If you want :math:`SE_{new} = \frac{1}{2} SE_{old}`, then :math:`\sqrt{n_{new}} = 2\sqrt{n_{old}}`, so :math:`n_{new} = 4n_{old}`.

      **R verification:**

      .. code-block:: r

         sigma <- 8
         
         # SE for different n
         sigma / sqrt(100)   # 0.8
         sigma / sqrt(400)   # 0.4
         
         # Required n for SE = 0.5
         (sigma / 0.5)^2     # 256

----

.. admonition:: Exercise 8: True/False Conceptual Questions
   :class: note

   Determine whether each statement is **True** or **False**. Provide a brief justification.

   a. An unbiased estimator will always give the exact value of the parameter.

   b. The sample mean :math:`\bar{X}` is an unbiased estimator of the population mean :math:`\mu` for any sample size.

   c. If two estimators are both unbiased, the one with smaller variance is generally preferred.

   d. The sample variance :math:`S^2 = \frac{1}{n-1}\sum(X_i - \bar{X})^2` is an unbiased estimator of :math:`\sigma^2`.

   e. Increasing sample size reduces the variance of the sample mean.

   f. A biased estimator is always worse than an unbiased estimator.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): False**

      Being unbiased means the estimator is correct *on average* across many samples. Any single sample's estimate may be above or below the true parameter. Unbiased ‚â† perfect.

      **Part (b): True**

      :math:`E[\bar{X}] = \mu` regardless of the population distribution or sample size. This is a fundamental property of the sample mean.

      **Part (c): True**

      Among unbiased estimators, lower variance means estimates are more consistently close to the true parameter value. We want precision along with accuracy.

      **Part (d): True**

      The formula :math:`S^2 = \frac{1}{n-1}\sum(X_i - \bar{X})^2` is unbiased for :math:`\sigma^2` regardless of the underlying distribution, as long as the observations are independent with the same variance.

      **Part (e): True**

      :math:`Var(\bar{X}) = \frac{\sigma^2}{n}`, which decreases as :math:`n` increases. Larger samples give more precise estimates.

      **Part (f): False**

      A biased estimator with very low variance might actually give estimates that are, on average, closer to the true value than an unbiased estimator with high variance. The "best" estimator depends on the context and what properties matter most.

----

.. admonition:: Exercise 9: Application - Sensor Calibration
   :class: note

   A biomedical engineer is calibrating a blood pressure sensor. She tests the sensor against a known reference standard with true value :math:`\mu = 120` mmHg. After :math:`n = 20` independent measurements, she records the following data:

   .. code-block:: text

      118.2, 121.5, 119.8, 122.1, 117.9, 120.4, 123.2, 118.6, 121.0, 119.5,
      120.8, 117.5, 122.8, 119.2, 121.7, 118.9, 120.1, 122.4, 119.6, 120.5

   a. Compute the sample mean :math:`\bar{x}` and sample standard deviation :math:`s`.

   b. The observed "bias" in this sample is :math:`\bar{x} - \mu`. Calculate this value. Does the sensor appear to overestimate or underestimate?

   c. If the engineer wanted to correct for this observed bias, how might she adjust future readings?

   d. Can we conclude from this single sample that the estimator :math:`\bar{X}` is biased? Explain why or why not.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Summary statistics**

      .. code-block:: r

         measurements <- c(118.2, 121.5, 119.8, 122.1, 117.9, 120.4, 123.2, 118.6, 
                           121.0, 119.5, 120.8, 117.5, 122.8, 119.2, 121.7, 118.9, 
                           120.1, 122.4, 119.6, 120.5)
         
         mean(measurements)  # 120.285
         sd(measurements)    # 1.704

      - Sample mean: :math:`\bar{x} = 120.285` mmHg
      - Sample standard deviation: :math:`s = 1.704` mmHg

      **Part (b): Observed bias**

      .. math::
         \bar{x} - \mu = 120.285 - 120 = 0.285 \text{ mmHg}

      The sensor appears to be **slightly overestimating** the true pressure, but the difference is small.

      **Part (c): Calibration correction**

      If this bias is consistent, the engineer could apply a **calibration correction**:

      - Corrected reading = Raw reading ‚àí 0.285
      - Or equivalently, use an adjusted estimator: :math:`\hat{\mu}_{corrected} = \bar{X} - 0.285`

      **Part (d): Can we conclude bias?**

      We cannot conclude the estimator :math:`\bar{X}` is biased because:

      - **Bias is a property of the sampling distribution**, not a single sample
      - Even an unbiased estimator will not equal :math:`\mu` exactly in any given sample
      - The deviation :math:`\bar{x} - \mu = 0.285` is well within the expected sampling variability
      - With estimated :math:`SE = s/\sqrt{n} = 1.704/\sqrt{20} \approx 0.38` mmHg (using :math:`s` since :math:`\sigma` is unknown), a deviation of 0.285 is not unusual
      - To assess true bias, we would need **repeated calibration tests** and examine whether the average of many :math:`\bar{x}` values systematically differs from :math:`\mu`

----

Additional Practice Problems
----------------------------

**True/False Questions** (1 point each)

1. The population mean :math:`\mu` is a random variable.

   ‚ìâ or ‚íª

2. Different samples from the same population will yield different values of :math:`\bar{x}`.

   ‚ìâ or ‚íª

3. An estimator with zero bias will always produce estimates equal to the true parameter.

   ‚ìâ or ‚íª

4. The standard error of :math:`\bar{X}` depends on the sample size.

   ‚ìâ or ‚íª

5. Among unbiased estimators, we prefer the one with the smallest variance.

   ‚ìâ or ‚íª

6. Quadrupling the sample size will halve the standard error.

   ‚ìâ or ‚íª

**Multiple Choice Questions** (2 points each)

7. Which of the following is a parameter?

   ‚í∂ The average height of 50 randomly selected students
   
   ‚í∑ The standard deviation of test scores for all students in a school
   
   ‚í∏ The sample proportion of defective items in a shipment
   
   ‚íπ The median income from a survey of 1,000 households

8. The sample variance formula uses :math:`n-1` instead of :math:`n` in the denominator because:

   ‚í∂ It makes calculations easier
   
   ‚í∑ It produces an unbiased estimator of :math:`\sigma^2`
   
   ‚í∏ It produces larger variance estimates
   
   ‚íπ It is required for the normal distribution

9. If :math:`\sigma = 20` and :math:`n = 100`, the standard error of :math:`\bar{X}` is:

   ‚í∂ 20
   
   ‚í∑ 2
   
   ‚í∏ 0.2
   
   ‚íπ 200

10. To reduce the standard error by a factor of 3, the sample size must be increased by a factor of:

    ‚í∂ 3
    
    ‚í∑ 6
    
    ‚í∏ 9
    
    ‚íπ :math:`\sqrt{3}`

11. Which quantity describes the precision of an estimator?

    ‚í∂ Bias
    
    ‚í∑ Variance
    
    ‚í∏ Expected value
    
    ‚íπ Sample size

12. An unbiased estimator with variance 100 vs. a biased estimator (bias = 2) with variance 25. Which has smaller expected squared error from the true parameter?

    ‚í∂ The unbiased estimator (expected squared error = 100)
    
    ‚í∑ The biased estimator (expected squared error = 29)
    
    ‚í∏ They are equal
    
    ‚íπ Cannot be determined

.. dropdown:: Answers to Practice Problems
   :class-container: sd-border-success

   **True/False Answers:**

   1. **False** ‚Äî :math:`\mu` is a fixed (though unknown) population constant, not a random variable.

   2. **True** ‚Äî Due to sampling variability, different samples produce different sample means.

   3. **False** ‚Äî Zero bias means correct *on average* over many samples, not correct every time.

   4. **True** ‚Äî :math:`SE = \sigma/\sqrt{n}`, which depends on :math:`n`.

   5. **True** ‚Äî Lower variance means more precision (estimates cluster more tightly around the parameter).

   6. **True** ‚Äî :math:`SE \propto 1/\sqrt{n}`, so if :math:`n` increases by 4, SE decreases by :math:`\sqrt{4} = 2`.

   **Multiple Choice Answers:**

   7. **‚í∑** ‚Äî "All students in a school" indicates this is a population characteristic, making it a parameter.

   8. **‚í∑** ‚Äî The :math:`n-1` correction (Bessel's correction) makes :math:`S^2` unbiased for :math:`\sigma^2`.

   9. **‚í∑** ‚Äî :math:`SE = \sigma/\sqrt{n} = 20/\sqrt{100} = 20/10 = 2`.

   10. **‚í∏** ‚Äî To reduce SE by factor of 3, need :math:`\sqrt{n_{new}/n_{old}} = 3`, so :math:`n_{new}/n_{old} = 9`.

   11. **‚í∑** ‚Äî Variance measures how spread out the estimates are around their expected value (precision).

   12. **‚í∑** ‚Äî Expected squared error = Variance + Bias¬≤. Unbiased: 100 + 0 = 100. Biased: 25 + 4 = 29. The biased estimator has smaller expected squared error.