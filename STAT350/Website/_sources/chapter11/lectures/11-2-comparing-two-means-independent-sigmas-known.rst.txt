.. _11-2-comparing-two-means-independent-sigmas-known:



.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch11-2">
      <iframe
         id="video-ch11-2"
         title="STAT 350 – Chapter 11.2 Comparing Two Population Means Using Independent Samples Video"
         src="https://www.youtube.com/embed/OKJxoLTK9GY?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>

Comparing the Means of Two Independent Populations - Population Variances Are Known
=============================================================================================

Having established the conceptual framework for two-sample procedures, we now develop the mathematical 
foundation for comparing population means when samples are collected independently. This section focuses 
on the theoretical case where population standard deviations are known, providing the groundwork for 
understanding more realistic scenarios where these parameters must be estimated.

.. admonition:: Road Map 🧭
   :class: important

   • **Problem we will solve** – How to construct rigorous hypothesis tests and confidence intervals for the difference between two population means using independent samples
   • **Tools we'll learn** – The sampling distribution theory for differences in sample means and z-procedures for two independent samples
   • **How it fits** – This establishes the mathematical foundation that will be extended to t-procedures when population standard deviations are unknown

Mathematical Foundations: Assumptions and Framework
------------------------------------------------------

The validity of two-sample independent procedures rests on three fundamental assumptions that extend our single-sample framework to the comparative setting. These assumptions must be carefully verified before applying the methods we develop.

**Assumption 1: Independent Simple Random Sampling from Distinct Populations**

We assume that each group represents a simple random sample from its respective population of interest. This requirement has several components:

For Population A, we have random variables :math:`X_{A1}, X_{A2}, \ldots, X_{An_A}` that are **independently and identically distributed** from a population with unknown mean :math:`\mu_A` and known variance :math:`\sigma^2_A`. The independence condition means that :math:`X_{Ai}` is independent of :math:`X_{Aj}` for all :math:`i \neq j`, and the identical distribution condition ensures that :math:`E[X_{Ai}] = \mu_A` and :math:`\text{Var}[X_{Ai}] = \sigma^2_A` for all :math:`i \in \{1, 2, \ldots, n_A\}`.

Similarly, for Population B, we have random variables :math:`X_{B1}, X_{B2}, \ldots, X_{Bn_B}` that are independently and identically distributed from a population with unknown mean :math:`\mu_B` and known variance :math:`\sigma^2_B`.

**Assumption 2: Independence Between Populations**

The observations from one population are completely independent of those from the other population. Formally, this means that :math:`X_{Ai}` is independent of :math:`X_{Bj}` for all possible pairs of indices :math:`i \in \{1, 2, \ldots, n_A\}` and :math:`j \in \{1, 2, \ldots, n_B\}`.

This independence between populations is what distinguishes independent sample procedures from paired sample procedures. It ensures that the sampling process for one group has no effect on or relation to the selection of individuals or objects from the other group.

**Assumption 3: Normality of Sampling Distributions**

We require that the sampling distributions for the estimators of the means follow normal distributions. This can be satisfied through two pathways:

First, if the underlying populations are normally distributed, then the sample means will be exactly normally distributed regardless of sample size. Second, if the populations are not normal but the sample sizes are sufficiently large, the Central Limit Theorem ensures that the sample means are approximately normally distributed.

Under these conditions, we have:

.. math::

   \bar{X}_A \sim N\left(\mu_A, \frac{\sigma_A}{\sqrt{n_A}}\right)

.. math::

   \bar{X}_B \sim N\left(\mu_B, \frac{\sigma_B}{\sqrt{n_B}}\right)

The Parameter of Interest and Natural Estimator
-----------------------------------------------

In two-sample procedures, our primary interest lies not in the individual population means :math:`\mu_A` and :math:`\mu_B`, but rather in their difference. This shift in focus from individual parameters to comparative parameters is fundamental to the two-sample approach.

**The Target Parameter**

Our parameter of interest is:

.. math::

   \theta = \mu_A - \mu_B

We can conceptualize this difference as a single parameter that captures the essence of the comparison we wish to make. The sign and magnitude of this difference tell us both the direction and size of any systematic difference between the populations.

**The Point Estimator**

Since we know that :math:`\bar{X}_A` is an unbiased estimator of :math:`\mu_A` and :math:`\bar{X}_B` is an unbiased estimator of :math:`\mu_B`, the natural point estimator for their difference is:

.. math::

   \hat{\theta} = \bar{X}_A - \bar{X}_B

This estimator leverages the independent sampling structure by estimating each population mean separately and then computing their difference.

Theoretical Properties of the Point Estimator
---------------------------------------------

**Unbiasedness**

The difference in sample means provides an unbiased estimate of the difference in population means. To establish this formally:

.. math::

   E[\bar{X}_A - \bar{X}_B] = E[\bar{X}_A] - E[\bar{X}_B]

This equality follows from the linearity of expectation. Since each sample mean is an unbiased estimator of its respective population mean:

.. math::

   E[\bar{X}_A] = \mu_A \quad \text{and} \quad E[\bar{X}_B] = \mu_B

Therefore:

.. math::

   E[\bar{X}_A - \bar{X}_B] = \mu_A - \mu_B

The bias of our estimator is:

.. math::

   \text{Bias}[\bar{X}_A - \bar{X}_B] = E[\bar{X}_A - \bar{X}_B] - (\mu_A - \mu_B) = 0

**Variance Calculation**

The variance of the difference in sample means depends critically on the independence assumption between populations. For independent random variables, the variance of their difference equals the sum of their individual variances:

.. math::

   \text{Var}[\bar{X}_A - \bar{X}_B] = \text{Var}[\bar{X}_A] + \text{Var}[\bar{X}_B]

From single-sample theory, we know that the variance of a sample mean is the population variance divided by the sample size:

.. math::

   \text{Var}[\bar{X}_A] = \frac{\sigma^2_A}{n_A} \quad \text{and} \quad \text{Var}[\bar{X}_B] = \frac{\sigma^2_B}{n_B}

Consequently:

.. math::

   \text{Var}[\bar{X}_A - \bar{X}_B] = \frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}

**Standard Deviation**

The standard deviation of our estimator is:

.. math::

   \sigma_{\bar{X}_A - \bar{X}_B} = \sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}

It is crucial to note that this expression cannot be simplified to :math:`\frac{\sigma_A}{\sqrt{n_A}} + \frac{\sigma_B}{\sqrt{n_B}}` because the square root function does not distribute across addition. The correct procedure is to first compute the variance by adding the individual variance terms, then take the square root of the entire sum.

The Sampling Distribution of the Difference
-------------------------------------------

Under our stated assumptions, we can derive the complete sampling distribution of the difference in sample means.

**The Central Result**

The difference :math:`\bar{X}_A - \bar{X}_B` follows a normal distribution with mean :math:`\mu_A - \mu_B` and standard deviation :math:`\sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}`:

.. math::

   \bar{X}_A - \bar{X}_B \sim N\left(\mu_A - \mu_B, \sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}\right)

**Theoretical Justification**

This result follows from the fundamental properties of normal distributions. Since :math:`\bar{X}_A` and :math:`\bar{X}_B` are each normally distributed and are independent of each other, their difference is also normally distributed. The mean of the difference equals the difference of the means, and the variance of the difference equals the sum of the variances (due to independence).

**Statistical Efficiency**

When the original populations are normally distributed, this estimator achieves the Cramér-Rao lower bound, making it the minimum variance unbiased estimator (MVUE) for the difference in means. This efficiency property provides theoretical justification for the widespread use of this approach.

Hypothesis Testing Procedures
-----------------------------

The four-step hypothesis testing framework extends naturally to the two-sample setting, with modifications to accommodate the comparative nature of the research questions.

**Step 1: Parameter Identification**

In two-sample procedures, we must clearly identify both population means using contextually meaningful labels. Rather than generic labels like A and B, we should use descriptive terms that reflect the actual populations being studied. For example:

- :math:`\mu_{\text{treatment}}` = true mean response for the treatment group
- :math:`\mu_{\text{control}}` = true mean response for the control group

The parameter identification should also specify the units of measurement and provide sufficient context for interpreting the parameters within the scope of the research question.

**Step 2: Hypothesis Formulation**

The hypotheses in two-sample procedures focus on the difference between population means. The general structure is:

.. math::

   H_0: \mu_A - \mu_B = \Delta_0

.. math:

   H_a: \mu_A - \mu_B \neq \Delta_0 \quad \text{(or } > \Delta_0 \text{ or } < \Delta_0\text{)}

The null value :math:`\Delta_0` represents the hypothesized difference under the null hypothesis. In most applications, :math:`\Delta_0 = 0`, corresponding to the hypothesis of equal population means. However, other values of :math:`\Delta_0` are possible and meaningful in certain research contexts, such as non-inferiority testing or equivalence studies.

The choice of alternative hypothesis depends on the research question:

- **Two-sided alternative** (:math:`H_a: \mu_A - \mu_B \neq \Delta_0`): Used when we want to detect any difference from the null value
- **Right-sided alternative** (:math:`H_a: \mu_A - \mu_B > \Delta_0`): Used when we specifically want to establish that :math:`\mu_A` exceeds :math:`\mu_B` by more than :math:`\Delta_0`
- **Left-sided alternative** (:math:`H_a: \mu_A - \mu_B < \Delta_0`): Used when we specifically want to establish that :math:`\mu_A` is less than :math:`\mu_B` by more than :math:`\Delta_0`

**Step 3: Test Statistic and P-Value Calculation**

When population standard deviations are known, we standardize our estimator to obtain a test statistic that follows a standard normal distribution:

.. math::

   Z_{TS} = \frac{(\bar{X}_A - \bar{X}_B) - \Delta_0}{\sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}}

Under the null hypothesis, this test statistic follows a standard normal distribution: :math:`Z_{TS} \sim N(0,1)`.

In the common case where :math:`\Delta_0 = 0`, this simplifies to:

.. math::

   Z_{TS} = \frac{\bar{X}_A - \bar{X}_B}{\sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}}

The p-value calculation follows the same principles as in single-sample z-tests, with the specific approach depending on the alternative hypothesis:

- **Two-sided test**: :math:`\text{p-value} = 2P(Z > |Z_{TS}|)`
- **Right-sided test**: :math:`\text{p-value} = P(Z > Z_{TS})`  
- **Left-sided test**: :math:`\text{p-value} = P(Z < Z_{TS})`

**Step 4: Decision and Conclusion**

The decision rule remains unchanged from single-sample procedures: compare the p-value to the predetermined significance level :math:`\alpha`.

- If p-value ≤ :math:`\alpha`: Reject :math:`H_0`
- If p-value > :math:`\alpha`: Fail to reject :math:`H_0`

The conclusion template must be adapted to address the comparative nature of two-sample procedures:

*"The data [does/does not] give [some/strong] support (p-value = [value]) to the claim that [statement of* :math:`H_a` *in context about the difference in population means]."*

The strength descriptors should reflect the magnitude of the p-value relative to conventional benchmarks and the significance level used in the study.

Confidence Intervals for the Difference in Means
------------------------------------------------

Confidence intervals for the difference in population means follow the same pivotal quantity approach used in single-sample procedures, adapted for the two-sample context.

**The Pivotal Quantity**

Our pivotal quantity is constructed by standardizing the difference in sample means:

.. math::

   Z = \frac{(\bar{X}_A - \bar{X}_B) - (\mu_A - \mu_B)}{\sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}}

This quantity satisfies the requirements for a pivotal quantity:

1. Its distribution (standard normal) does not depend on any unknown parameters
2. It contains exactly one unknown parameter of interest: :math:`\mu_A - \mu_B`
3. It has a known, tractable distribution that allows for probability calculations

**Confidence Interval Construction**

For a :math:`100(1-\alpha)\%` confidence interval, we begin with the probability statement:

.. math::

   P\left(-z_{\alpha/2} < \frac{(\bar{X}_A - \bar{X}_B) - (\mu_A - \mu_B)}{\sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}} < z_{\alpha/2}\right) = 1 - \alpha

Through algebraic manipulation to isolate :math:`\mu_A - \mu_B` in the center of the inequality, we obtain:

.. math:

   (\bar{x}_A - \bar{x}_B) \pm z_{\alpha/2} \sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}

**Interpretation**

The confidence interval provides a range of plausible values for the true difference in population means. Specifically, we can state with :math:`100(1-\alpha)\%` confidence that the true difference :math:`\mu_A - \mu_B` lies within the computed interval.

The width of the confidence interval reflects the precision of our estimate, with narrower intervals indicating more precise estimation. The interval width depends on the confidence level, the population variances, and the sample sizes.

Illustrative Example: Understanding Through Application
----------------------------------------------------------

To demonstrate these concepts concretely, consider a simplified example based on the theoretical framework we have established.

**Scenario Setup**

Suppose we have two independent samples with the following characteristics:

- **Group A**: :math:`n_A = 25`, :math:`\bar{x}_A = 50`, :math:`\sigma_A = 10` (known)
- **Group B**: :math:`n_B = 30`, :math:`\bar{x}_B = 45`, :math:`\sigma_B = 12` (known)

We wish to test :math:`H_0: \mu_A - \mu_B = 0` versus :math:`H_a: \mu_A - \mu_B \neq 0` at the :math:`\alpha = 0.05` significance level.

**Test Statistic Calculation**

The point estimate is:

.. math:

   \bar{x}_A - \bar{x}_B = 50 - 45 = 5

The standard error is:

.. math:

   \sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}} = \sqrt{\frac{10^2}{25} + \frac{12^2}{30}} = \sqrt{\frac{100}{25} + \frac{144}{30}} = \sqrt{4 + 4.8} = \sqrt{8.8} \approx 2.97

The test statistic is:

.. math:

   Z_{TS} = \frac{5 - 0}{2.97} \approx 1.68

**P-Value and Decision**

For a two-sided test, the p-value is :math:`2P(Z > 1.68) \approx 2(0.0465) \approx 0.093`.

Since 0.093 > 0.05, we fail to reject the null hypothesis at the 5% significance level.

**95% Confidence Interval**

The 95% confidence interval is:

.. math:

   5 \pm 1.96(2.97) = 5 \pm 5.82 = (-0.82, 10.82)

This interval contains zero, which is consistent with our failure to reject the null hypothesis of equal means.

The Transition to Unknown Standard Deviations
-----------------------------------------------

While the assumption of known population standard deviations provides valuable theoretical insight, it is rarely 
realistic in practice. When :math:`\sigma_A` and :math:`\sigma_B` must be estimated from sample data, our procedures 
undergo important modifications:

**Changes in Test Statistics**

The test statistic becomes a t-statistic rather than a z-statistic, with the population standard deviations replaced by their sample estimates.

**Degrees of Freedom Considerations**

The determination of appropriate degrees of freedom depends on whether we can assume equal population variances, leading to two distinct approaches: pooled and unpooled procedures.

**Distribution Theory**

The sampling distribution changes from normal to t, requiring different critical values and p-value calculations.

These extensions, while maintaining the same fundamental logic, introduce additional complexity that will be addressed in subsequent sections.

.. admonition:: Key Takeaways 📝
   :class: important

   1. **Two-sample independent procedures extend single-sample methods** to comparative questions while requiring three fundamental assumptions: independent simple random sampling, independence between groups, and normal sampling distributions.
   
   2. **The parameter of interest shifts from individual means to their difference** (:math:`\mu_A - \mu_B`), with the natural point estimator being :math:`\bar{X}_A - \bar{X}_B`.
   
   3. **The point estimator is unbiased with variance** :math:`\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}`, where the addition of variances follows from the independence assumption between groups.
   
   4. **Test statistics follow standard normal distributions** when population standard deviations are known, using :math:`Z_{TS} = \frac{(\bar{X}_A - \bar{X}_B) - \Delta_0}{\sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}}`.
   
   5. **The four-step hypothesis testing framework applies directly**, with conclusions addressing differences between population means rather than individual parameter values.
   
   6. **Confidence intervals follow the pivotal quantity approach**, providing ranges of plausible values for the true difference in population means.
   
   7. **Sample sizes may differ between groups** without affecting the validity of the procedures, though equal sample sizes often provide optimal statistical efficiency.
   
   8. **This theoretical foundation extends to t-procedures** when population standard deviations must be estimated, maintaining the same logical structure while adapting to distributional changes.

Exercises
~~~~~~~~~~~~

1. **Theoretical Understanding**: Explain why the variance of :math:`\bar{X}_A - \bar{X}_B` equals :math:`\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}` rather than :math:`\frac{\sigma^2_A}{n_A} - \frac{\sigma^2_B}{n_B}`. What assumption is crucial for this result?

2. **Assumption Analysis**: For each of the following research scenarios, identify which assumptions might be violated and explain the potential consequences:

   a) Comparing test scores between students in the same classroom, where some students work together
   b) Measuring reaction times before and after caffeine consumption using the same participants
   c) Comparing heights between adult males and females using a convenience sample from a shopping mall

3. **Hypothesis Formulation**: A manufacturer claims their new battery lasts at least 2 hours longer than the competitor's battery. Set up appropriate hypotheses for testing this claim, clearly defining your parameters and explaining your choice of :math:`\Delta_0`.

4. **Standard Error Calculation**: Two independent samples have :math:`n_A = 16`, :math:`\sigma_A = 8`, :math:`n_B = 25`, and :math:`\sigma_B = 10`. Calculate the standard error of :math:`\bar{X}_A - \bar{X}_B` and explain what this value represents in practical terms.

5. **Sample Size Effects**: How would the standard error of :math:`\bar{X}_A - \bar{X}_B` change if:

   a) Both sample sizes were doubled?
   b) Only :math:`n_A` were doubled while :math:`n_B` remained constant?
   c) The total sample size remained constant but was redistributed equally between groups?

6. **Confidence Interval Interpretation**: A 90% confidence interval for :math:`\mu_A - \mu_B` is calculated as (1.2, 7.8). Provide three different but equivalent ways to interpret this interval, and explain what happens to the interval width if the confidence level is increased to 95%.