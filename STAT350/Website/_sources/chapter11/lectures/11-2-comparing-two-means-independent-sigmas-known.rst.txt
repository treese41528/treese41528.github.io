.. _11-2-comparing-two-means-independent-sigmas-known:



.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch11-2">
      <iframe
         id="video-ch11-2"
         title="STAT 350 – Chapter 11.2 Comparing Two Population Means Using Independent Samples Video"
         src="https://www.youtube.com/embed/OKJxoLTK9GY?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>

.. admonition:: Slides 📊
   :class: tip

   `Download Chapter 11 slides (PPTX) <https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/
   slides/Chapter%2011%20Two%20Sample%20Procedures/CI%20and%20HT%20for%20Two%20Samples%20or%20Treatments_AC.pptx>`_


Independent Two-Sample Analysis When Population Variances Are Known
=============================================================================================

We now develop the mathematical foundation for comparing the means of two **independent** populations,
assuming that the population variances are known. The simplifying assumption will be lifted
in later lessons when we discuss more realistic scenarios.

.. admonition:: Road Map 🧭
   :class: important

   * List the **three key assumptions** required for the construction of independent two-sample
     inference methods.
   * Construct **hypothesis tests and confidence regions** for the difference in the means
     of two independent populations. Identify the **common underlying principles with one-sample inference**.

The Assumptions
------------------------------------------------------

The validity of independent two-sample procedures rests on three fundamental assumptions 
that extend the single-sample framework to comparisons. These assumptions must 
be carefully verified before applying the methods.
 
**Assumption 1: SRS from Each Population**

   The random variables :math:`X_{A1}, X_{A2}, \ldots, X_{An_A}` form an 
   independent and identically distributed (*iid*) sample from Population A. 
   Similarly, the random variables :math:`X_{B1}, X_{B2}, \ldots, X_{Bn_B}`
   constitute an *iid* sample from Population B.

**Assumption 2: Independence Between Populations**

   The observations from one population are independent of those from the other population. 
   Formally, :math:`X_{Ai}` is independent of :math:`X_{Bj}` for all possible pairs of indices 
   :math:`i \in \{1, 2, \ldots, n_A\}` and :math:`j \in \{1, 2, \ldots, n_B\}`.

**Assumption 3: Normality of Sampling Distributions**

   For each population, either the population distribution is normal, or 
   the sample size is large enough for the CLT to hold. 

The Parameter of Interest and Its Point Estimator
----------------------------------------------------

The Target Parameter
~~~~~~~~~~~~~~~~~~~~~

Recall that our primary interest lies not in the individual population means 
:math:`\mu_A` and :math:`\mu_B`, but rather in their difference. Our parameter of interest is:

.. math::

   \theta = \mu_A - \mu_B

We conceptualize this difference as a **single parameter** that captures the essence of the 
comparison we wish to make. Its sign and magnitude indicate the direction 
and size of any systematic difference between the populations.

The Point Estimator
~~~~~~~~~~~~~~~~~~~~~~~

Since we know that :math:`\bar{X}_A` and :math:`\bar{X}_B` are unbiased estimators of
their respective population means :math:`\mu_A` and :math:`\mu_B`, the natural point estimator 
for :math:`\theta` is:

.. math::

   \hat{\theta} = \bar{X}_A - \bar{X}_B.

Theoretical Properties of the Point Estimator
---------------------------------------------------

As a result of Assumptions 1 and 3, the sampling distributions of
the two sample means are:

.. math::

   &\bar{X}_A \sim N\left(\mu_A, \frac{\sigma_A}{\sqrt{n_A}}\right)\\
   &\bar{X}_B \sim N\left(\mu_B, \frac{\sigma_B}{\sqrt{n_B}}\right)

Furthermore, the two random variables are **independent** since their 
building blocks are independent according to Assumption 2. We establish the theoretical properties 
of the difference estimator :math:`\bar{X}_A - \bar{X}_B`
starting from this baseline.

Unbiasedness
~~~~~~~~~~~~~~~

The difference in sample means is an **unbiased estimator** of the difference in population means. 
To establish this formally:

.. math::

   E[\bar{X}_A - \bar{X}_B] = E[\bar{X}_A] - E[\bar{X}_B] = \mu_A - \mu_B.

Therefore, the bias of the estimator is:

.. math::

   \text{Bias}[\bar{X}_A - \bar{X}_B] = E[\bar{X}_A - \bar{X}_B] - (\mu_A - \mu_B) = 0.

Variance of the Estimator
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The variance of :math:`\bar{X}_A - \bar{X}_B` depends critically on the independence assumption 
between populations. Recall that for two independent random variables, the variance of their difference equals 
the sum of their individual variances:

.. math::

   \text{Var}[\bar{X}_A - \bar{X}_B] = \text{Var}[\bar{X}_A] + \text{Var}[\bar{X}_B]
   = \frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}.

Standard Error
~~~~~~~~~~~~~~~~

The standard deviation of the estimator, or the **standard error**, is obtained by taking the
square root of the variance:

.. math::

   \sigma_{\bar{X}_A - \bar{X}_B} = \sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}.

The Sampling Distribution of the Difference Estimator
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We now know the expected value and variance of the difference estimator :math:`\bar{X}_A - \bar{X}_B`.
Additionally, since :math:`\bar{X}_A` and :math:`\bar{X}_B` are each normally distributed, 
their difference is also normally distributed. Combining these results, we establish the
full sampling distribution of the difference estimator as:

.. math::

   \bar{X}_A - \bar{X}_B \sim N\left(\mu_A - \mu_B, \sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}\right).

Equivalently, its standardization follows the standard normal distribution:

.. math::

   \frac{(\bar{X}_A - \bar{X}_B)-(\mu_A - \mu_B)}{\sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}}
   \sim N(0,1).

Based on these key results, we will now build inference methods for the difference in population means.

Hypothesis Testing for the Difference in Means
----------------------------------------------------------

The four-step hypothesis testing framework extends naturally to the two-sample setting, with modifications 
to accommodate the comparative nature.

Step 1: Parameter Identification
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We must clearly identify both population means using contextually meaningful labels. 
Rather than generic labels like A and B, we encourage the use of descriptive terms that 
reflect the populations being studied. 

For example, suppose we are comparing the systolic blood pressure of two patient groups after assigning 
one group a placebo and the other a newly developed treatment. We can define the relevant populations and
their true means in the following manner:

* Let :math:`\mu_{\text{treatment}}` denote the true mean systolic blood pressure of
  patients who are treated with the new procedure.
* Let :math:`\mu_{\text{control}}` denote the true mean systolic blood pressure of
  patients who are not treated with the new medical procedure. 

The parameter identification should also specify the **units of measurement** and provide sufficient 
context for interpreting the **parameter** and the **target population** within the scope of the research question.

Step 2: Hypothesis Formulation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Follow the template:

.. _HT_template:
.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter11/2sample-indep-HT-template.png 
   :figwidth: 70%
   :align: center 
   :alt: Template for independent two-sample hypotheses

   Template for independent two-sample hypotheses

You may also refer to the details provided in Chapter 11.1.3.

Step 3: Test Statistic and :math:`p`-Value
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Recall the :math:`z`-test statistic used in one-sample hypothesis testing:

.. math::
   \text{One-sample } Z_{TS} = \frac{\bar{X}-\mu_0}{\sigma/\sqrt{n}} 
   = \frac{\text{estimator}-\text{null value}}{\text{std. error}}.

By providing a **standardized distance between the estimator and the null value**, 
the :math:`z`-test statistic measured how far the sample data is from the null assumption.

We define a **new** :math:`z`-**test statistic** to serve the same purpose
by replacing each one-sample component with the appropriate independent two-sample parallel:

.. math::

   Z_{TS} = \frac{(\bar{X}_A - \bar{X}_B) - \Delta_0}{\sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}}.

As in the one-sample case, **under the the null hypothesis**,

.. math:: 
   Z_{TS} \sim N(0,1).

The :math:`p`-value calculation therefore follows the same principles as in single-sample :math:`z`-tests:

.. flat-table::
   :align: center
   :header-rows: 1
   :widths: 1 2
   :width: 60%

   * - :cspan:`1` Revisiting :math:`p`-Value Computation

   * - Upper-tailed
     - :math:`P(Z > z_{TS})`
   
   * - Lower-tailed
     - :math:`P(Z < z_{TS})`

   * - Two-tailed
     - :math:`2P(Z > |z_{TS}|) = 2P(Z < -|z_{TS}|)`

Step 4: Decision and Conclusion
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The decision rule remains unchanged from single-sample procedures:

- If :math:`p`-value :math:`\leq \alpha`, we reject :math:`H_0`.
- If :math:`p`-value :math:`> \alpha`, we fail to reject :math:`H_0`.

The conclusion template below is adapted to address the comparative nature of two-sample procedures:

   *"The data [does/does not] give [some/strong] support (p-value = [value]) to the claim that [statement of* :math:`H_a` *in context about the difference in population means]."*

The strength descriptors should reflect the magnitude of the :math:`p`-value relative to conventional 
benchmarks and the significance level used in the study.

.. admonition:: Example 💡: Shift Scheduling and Work Efficiency
   :class: note

   A retail chain tests two different workforce scheduling systems to see which helps 
   cashiers process more transactions per 8-hour shift. They run independent pilots on 
   different stores:

   - **System A**: :math:`n_A = 25`, :math:`\bar{x}_A = 50`, :math:`\sigma_A = 10` (known)
   - **System B**: :math:`n_B = 30`, :math:`\bar{x}_B = 45`, :math:`\sigma_B = 12` (known)

   Perform a hypothesis test to determine whether the true mean numbers of transactions are different
   at the :math:`\alpha = 0.05` significance level.

   **Step 1: Define the parameters and target populations**

   Let :math:`\mu_A` denote the true mean number of transactions processed by cashiers following
   System A. Likewise, let :math:`\mu_B` be the true mean number of transactions completed by
   employees following System B.

   **Step 2: Write the hypotheses**

   .. math::
      &H_0: \mu_A - \mu_B = 0\\
      &H_a: \mu_A - \mu_B \neq 0

   **Step 3: Compute the test statistic and p-value**

   .. math::
      Z_{TS} =\frac{(\bar{x}_A - \bar{x}_B)-0}{\sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}}
      = \frac{(50-45) - 0}{\sqrt{\frac{10^2}{25} + \frac{12^2}{30}}} 
      \approx 1.68

   This is a two-sided test, so the :math:`p`-value is 
   :math:`2P(Z > 1.68) \approx 0.093`.


   **Step 4: Decision and Conclusion**

   Since :math:`p`-value :math:`= 0.093 > 0.05`, we **fail to reject the null hypothesis** at the 5% significance level.
   We do not have enough evidence to support the claim that the mean number of transactions
   fulfilled by cashiers are different by the scheduling system.

Confidence Regions for the Difference in Means
--------------------------------------------------

Let us begin by constructing a :math:`100C\%` confidence interval.
The goal is to find the margin of error (ME) such that

.. math::
   P((\bar{X}_A - \bar{X}_B) - ME < \mu_A - \mu_B < (\bar{X}_A - \bar{X}_B) + ME) = C.

The Pivotal Method
~~~~~~~~~~~~~~~~~~~~

We begin with the known truth:

.. math::
   P(-z_{\alpha/2} < Z < z_{\alpha/2}) = C.

Replace :math:`Z` with the standardization of the difference estimator, or the **pivotal quantity**:

.. math::
   P\left(-z_{\alpha/2} < \frac{(\bar{X}_A - \bar{X}_B) - (\mu_A - \mu_B)}{\sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}}
 < z_{\alpha/2}\right) = C.

Through algebraic manipulation to isolate :math:`\mu_A - \mu_B` in the center of the inequality, we obtain

.. math::
   P((\bar{X}_A - \bar{X}_B) - ME < \mu_A - \mu_B < (\bar{X}_A - \bar{X}_B) + ME) = C,

with :math:`ME = z_{\alpha/2} \sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}`.

For a single experiment, therefore, the :math:`100C \%` confidence interval is:

.. math::

   (\bar{x}_A - \bar{x}_B) \pm z_{\alpha/2} \sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}


Complete Summary of Confidence Intervals and Bounds
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The one-sided confidence bounds can be derived similarly. We leave the details as an excercise—use
the confidence interval derivation above and Chapter 9.4 as reference.

.. flat-table::
   :header-rows: 1
   :widths: 1 2
   :align: center

   * - :cspan:`1` :math:`100\cdot (1-\alpha) \%` Confidence Regions for Difference in Means

   * - **Confidence Interval**
     - .. math::

         (\bar{x}_A - \bar{x}_B) \pm z_{\alpha/2} \sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}
   
   * - **Upper Confidence Bound**
     - .. math::

         (\bar{x}_A - \bar{x}_B) + z_{\alpha} \sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}

   * - **Lower Confidence Bound**
     - .. math::

         (\bar{x}_A - \bar{x}_B) - z_{\alpha} \sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}}

Note the repeated core elements. In both one-sample and two-sample cases, a confidence region is
**centered at the point estimate** and expands in the appropiate directions by ME, computed as
a **product of a critical value and the standard error**.

Interpreting Confidence Regions for Difference in Means
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The confidence regions provide a range of plausible values for the true difference in population means;
each region captures the true difference :math:`\mu_A - \mu_B` with :math:`100(1-\alpha)\%` confidence.
Their precision depends on the confidence level, the population variances, and the 
sample sizes.

.. admonition:: Example 💡: Shift Scheduling and Work Efficiency, Continued
   :class: note

   For the experiment on two workforce scheduling systems with:

   - **System A**: :math:`n_A = 25`, :math:`\bar{x}_A = 50`, :math:`\sigma_A = 10` (known)
   - **System B**: :math:`n_B = 30`, :math:`\bar{x}_B = 45`, :math:`\sigma_B = 12` (known)

   Compute the :math:`95 \%` confidence interval for the difference of the two population means.
   Check if the result is consistent with the hypothesis test performed in the previous example.

   **Identify the components**

   * The observed sample difference
     
      .. math::
         \bar{x}_A - \bar{x}_B = 50 - 45 = 5

   * The standard error
     
     .. math::
        
        \sqrt{\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}} = \sqrt{\frac{10^2}{25} + \frac{12^2}{30}} \approx 2.97

   * The :math:`z`-critical value
   
     .. code-block:: r 
        
        qnorm(0.025, lower.tail=FALSE)
        # returns 1.96

   **Put the parts together**

   The confidence interval is:

   .. math::

      5 \pm 1.96(2.97) = 5 \pm 5.82 = (-0.82, 10.82)

   **Is it consistent with the hypothesis test?**

   The interval contains zero, which is consistent with our failure to reject the null hypothesis of equal means.

Bringing It All Together
----------------------------

.. admonition:: Key Takeaways 📝
   :class: important

   1. Two-sample independent procedures are designed to provide statistical answers to **comparative questions**. They
      require the key assumptions that (1) each sample is an SRS of the respective population, (2) 
      the two samples are independent from each other, (3) and the CLT holds in each sample.

   2. The **sampling distribution** of the point estimator :math:`\bar{X}_A - \bar{X}_B` is 
      normal with mean :math:`\mu_A - \mu_B` and variance :math:`\frac{\sigma^2_A}{n_A} + \frac{\sigma^2_B}{n_B}`. 
      The addition of variances follows from the independence assumption between groups.

   3. The construction of hypothesis tests and confidence regions follows **the same core principles as in
      the one-sample case**.

Exercises
~~~~~~~~~~~~

2. **Assumption Analysis**: For each of the following research scenarios, identify which assumptions might 
   be violated and explain the potential consequences:

   a) Comparing test scores between students in the same classroom, where some students work together.
   b) Measuring reaction times before and after caffeine consumption using the same participants.
   c) Comparing heights between adult males and females using a convenience sample from a shopping mall

3. **Hypothesis Formulation**: A manufacturer claims their new battery lasts at least 2 hours longer than 
   the competitor's battery. Set up appropriate hypotheses for testing this claim, clearly defining your 
   parameters and explaining your choice of :math:`\Delta_0`.

4. **Standard Error Calculation**: Two independent samples have :math:`n_A = 16`, :math:`\sigma_A = 8`, 
   :math:`n_B = 25`, and :math:`\sigma_B = 10`. Calculate the standard error of :math:`\bar{X}_A - \bar{X}_B` 
   and explain what this value represents in practical terms.