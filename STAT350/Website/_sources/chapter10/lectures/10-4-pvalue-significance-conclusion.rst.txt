.. _10-4-pvalue-significance-conclusion:



.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch10-4">
      <iframe
         id="video-ch10-4"
         title="STAT 350 ‚Äì Chapter 10.4 What Is A Test of Significance Video"
         src="https://www.youtube.com/embed/igQdAxeXEr8?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>

.. admonition:: Slides üìä
   :class: tip
   
   - `Hypothesis Testing for Single Sample (Part 4) (PPTX) <https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/slides/Chapter%2010%20Hypothesis%20Testing/Hypothesis%20Testing%20for%20Single%20Sample%20Part4_AC.pptx>`_

The Four Steps to Hypothesis Testing and Understanding the Result
=========================================================================

We conclude our introduction to hypothesis testing by stepping back to examine the broader picture. 
We present a **standardized four-step framework** to hypothesis testing and discuss important subtleties in
its interpretation. In particular, we address what hypothesis tests can and cannot tell us, 
how to interpret results responsibly, and common pitfalls to avoid in practice.

.. admonition:: Road Map üß≠
   :class: important

   * Organize the hypothesis testing workflow into **four steps**: (1) define the parameter(s),
     (2) state the hypotheses, (3) compute the test statistic, the df, and the :math:`p`-value, (4) draw a conclusion.
   * **Correctly interpret** :math:`p`-values and statistically significant results.
   * Recognize **ethical and practical concerns** regarding statistical inference.

The Four-Step Process to Hypothesis Testing
--------------------------------------------

Throughout our exploration of hypothesis testing, we have been following an implicit structure. 
Now let us formalize this into a systematic four-step process that ensures consistency and completeness 
in our analysis.

Step 1: Identify and Describe the Parameter(s) of Interest
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Make a concrete connection between the context of the problem and the mathematical 
notation to be used in the formal test. This step should include:

- The population being studied
- The symbol for the parameter of interest (e.g., :math:`\mu`)
- What the parameter represents in practical terms
- Units of measurement
- Any other contextual details needed for interpretation

.. admonition:: Example üí°: Testing Water Recycling Performance‚ÄìStep 1
   :class: note
   
   The Whimsical Wet 'n' Wobble Water Wonderland Waterpark has implemented a new water recycling system. 
   This recycling system is supposed to ensure water loss only due to evaporation and splashing by 
   the park patrons. However, after its implementation, 
   they suspect a higher daily water loss than the pre-implementation average of 230,000 gallons per day.

   To investigate, the park collected 21 days of water usage data during the first year of
   implementation. Perform a hypothesis test to determine if the system underperforms expectations. 
   Use a :math:`\alpha=0.05` significance level.


   **Step 1 of the Hypothesis Test**

   Let :math:`\mu` represent the true average daily water loss (in thousands of gallons) at the 
   Whimsical Wet 'n' Wobble Water Wonderland Waterpark after implementing the new recycling system.

Step 2: State the Hypotheses
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

State the paired hypotheses in the correct mathematical format. Refer to the
rules listed in Chapter 10.1.1.

.. admonition:: Example üí°: Testing Water Recycling Performance‚ÄìStep 2

   Continuing from the context provided in the previous example,

   **Step 2 of the Hypothesis Test**

   .. math:: 
      &H_0: \mu \leq 230\\
      &H_a: \mu > 230

Step 3: Calculate the Test Statistic and P-Value
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Perform all the computational steps. This includes:

- Checking that the normality assumption is reasonably met
- Deciding between a :math:`z`-test or a :math:`t`-test based on whether the
  population standard deviation is known
- Computing the test statistic and stating the appropriate degrees of freedom, if any
- Calculating the :math:`p`-value

.. admonition:: Example üí°: Testing Water Recycling Performance‚ÄìStep 3
   :class: note

   The 21 daily water loss measurements, in thousands of gallons, are:

   .. flat-table::
      
      * - 190.1 
        - 244.6
        - 244.1
        - 270.1
        - 269.6
        - 201.0
        - 234.3
      
      * - 292.3
        - 205.7
        - 242.3
        - 263.0
        - 219.0
        - 233.3
        - 229.0

      * - 293.5
        - 290.4
        - 264.0
        - 248.6
        - 260.5
        - 210.4
        - 236.9

   **Step 3 of the Hypothesis Test**

   Graphical analysis of the data's distribution:

   .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter10/water-usage-example.png 
      :figwidth: 90%
      :align: center 
      :alt: Graphical assessment of normality for water usage example

      Graphical assessment of normality

   The data distribution shows moderate deviation from normality, for which
   the sample size of :math:`n=21` is large enough. In addition, the population standard deviation
   is unknown. Therefore, we use the :math:`t`-test procedure.

   The numerical summaries:

   * :math:`n=21`
   * :math:`\bar{x} = 244.8905`
   * :math:`s=29.81`
   * :math:`df = n-1 = 20`

   The observed test statistic:
     
   .. math::
      t_{TS} = \frac{244.8905-230}{29.81/\sqrt{n}} = 2.2891
   
   The :math:`p`-value:
      
   .. math::
      p = P(T_{n-1} > t_{TS}) = P(T_{20} > 2.2891) = 0.01654,

   which can be computed using the R code below:

   .. code-block:: r

      pvalue <- pt(2.2891, df=20, lower.tail=FALSE)

Step 4: Make the Decision and State the Conclusion
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This step consists of **two parts**. 

First, draw a **formal decision**: compare
the :math:`p`-value computed in Step 3 to :math:`\alpha` and state whether the null hypothes is
is rejected. We never "reject the alternative" or "accept" anything. The only two
choices available for a formal decision is to 

* reject the null hypothesis, or
* fail to reject the null hypothesis.

Then make a **contextual conclusion** using the template below:

   *"The data [does/does not] give [some/strong] support (p-value = [actual value]) to the 
   claim that [statement of* :math:`H_a` *in context]."*

.. admonition:: Example üí°: Testing Water Recycling Performance‚ÄìStep 4
   :class: note

   The waterpark required the test to have the significance level :math:`\alpha=0.05`.

   **Step 4 of Hypothesis Testing** 

   Since :math:`p`-value :math:`= 0.01654 < 0.05`, the **null hypothesis is rejected**.
   There is sufficient evidence from data (:math:`p`-value :math:`= 0.01654`) to the claim that the true mean daily water loss
   after implementing the new water recycling system is greater than 230,000 gallons.

Understanding Statistical Significance
--------------------------------------------------------

We call a statistical result whose behavior may be attributed to more than just random chance
**statistically significant.** In hypothesis testing, 
results that lead to rejection of the null hypothesis are conventionally regarded as
statistically significant.

However, when we encounter a :math:`p`-value smaller than :math:`\alpha` in practice, the information conveyed
can be more nuanced than it may first appear. A statistically significant result can arise to
a combination of the following three reasons:

**1. Existence of True Effect (What We Hope For)**

   We may encounter a statistically significant result because 
   the null hypothesis is actually false and our test correctly identified a genuine effect. 
   This represents the ideal scenario where statistical significance corresponds to a real phenomenon.

**2. Rare Event Under True Null**

   Even if the null hypothesis is true, we could observe extreme data purely by chance. Our significance 
   level :math:`\alpha` represents our tolerance for this type of error (Type I error). 
   If :math:`\alpha = 0.05`, we expect to falsely reject true null hypotheses about 5% of the time in the long run.

**3. Assumption Violations**

   Hypothesis tests rely on assumptions about the population and the sampling procedure. 
   The data can be flagged as unusual by a hyopothesis test when the background assumptions are
   incorrect.

The Key Message
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A statistically significant result indicates **inconsistency between the data and the assumptions**.
For the result to be meaningful, we must ensure that the only assumption that can be violated is
the null hypothesis. This is why checking the distributional assumptions thoroughly before inference is essential.

What :math:`p`-Values Are and What They Are Not
------------------------------------------------------

Recall that a :math:`p`-value is 
**the probability of obtaining a test statistic at least as extreme as the one observed, 
assuming the null hypothesis and all other model assumptions are true**.
It quantifies how incompatible the data are with the null hypothesis.

..
   - They represent the **minimum significance level** of the same-sided test at which we could 
   reject :math:`H_0` with the observed data.

What :math:`p`-Values Are NOT
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**‚ùå The p-value is NOT the probability that the null hypothesis is true.**

   A :math:`p`-value of 0.03 does not mean there's a 3% chance 
   the null hypothesis is correct.

**‚ùå The p-value is NOT the probability that observations occurred by chance.**

   The :math:`p`-value is computed under the assumption that the null hypothesis and all model assumptions are true. 
   It's the probability of the data given these assumptions, not the probability that chance alone explains the results.

**‚ùå A small p-value does NOT prove the null hypothesis false.**

   A small :math:`p`-value flags the data as **unusual** under our assumptions. This could mean that:

   - The null hypothesis is false (what we hope).
   - A rare event occurred under a true null hypothesis.
   - Model assumptions are violated.

**‚ùå A large p-value does NOT prove the null hypothesis true**

   A large :math:`p`-value simply indicates that the data is consistent with the null hypothesis. This could mean
   that:

   - The null hypothesis is actually true.
   - Our sample size was too small to detect an existing effect.
   - The effect size is too small to detect with our current design.

The Problem of :math:`p`-Value Hacking
---------------------------------------------

Unfortunately, the pressure to publish statistically significant results has led to unethical practices 
collectively known as :math:`p`-**value hacking**, **data dredging**, or **fishing for significance**.

Common Forms of :math:`p`-Value Hacking
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* Conducting multiple analyses but **selectively reporting only those yielding significant results**. 
* **Data Manipulation**, such as increasing the data size until reaching significance, 
  excluding problematic observations, and trying different response variables until finding a significant one
* **Model Shopping**, or trying different statistical procedures until finding one that yields significance, 
  without proper justification for the model choice.

Preventing :math:`p`-Value Hacking
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:math:`p`-value hacking leads to increased false positives, undermining confidence in research findings.
Forgood research practice, a researcher should:

- document all procedures, including any data exclusions,
- report all analyses conducted, not just significant ones, and
- specify hypotheses and analysis plans **before** data collection.

..
   A Note on Practical Significance
   -----------------------------------------------------

   It is important to acknowledge that statistical significance does not automatically imply practical importance. 
   A result can be statistically significant while being too small to matter in practical terms.

   While **practical significance** is crucial in real research, we will not focus on effect size calculations 
   or practical significance assessments in this introductory course. Our goal is to master the fundamental 
   concepts of statistical significance and proper interpretation of hypothesis tests.


Bringing It All Together
----------------------------------------------------------

.. admonition:: Key Takeaways üìù
   :class: important

   1. Organize your hypothesis testing workflow in **four steps**: 
      
      (a) parameter identification, 
      (b) hypothesis statement, 
      (c) calculation, and 
      (d) decision and conclusion in context.
   
   2. Know the correct implications of statistical significance and :math:`p`-values.
   
   3. :math:`p`-value hacking undermines scientific integrity. For good research practice, 
      determine the complete analysis plan prior to data collection and document all
      procedures transparently.

Exercises
---------

.. admonition:: Exercise 1: Correct P-value Interpretation
   :class: note

   For each statement about a p-value of 0.03, indicate whether it is **CORRECT** or **INCORRECT**. Explain why.

   a. "There is a 3% probability that the null hypothesis is true."

   b. "If the null hypothesis is true, there is a 3% probability of observing data at least as extreme as what we obtained."

   c. "There is a 3% probability that our results occurred by random chance."

   d. "The probability of a Type I error is 3%."

   e. "We have proven that the alternative hypothesis is true with 97% confidence."

   f. "If we repeated this study many times when H‚ÇÄ is true, about 3% of the studies would yield a test statistic this extreme or more."

   .. dropdown:: Solution
      :class-container: sd-border-success

      .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/Exercises/ch10-4/fig11_pvalue_meaning.png
         :align: center
         :width: 80%
         :alt: P-value interpretation

         The p-value is the probability of data this extreme or more extreme, assuming H‚ÇÄ is true.

      **Part (a): INCORRECT ‚úó**

      The p-value is NOT the probability that H‚ÇÄ is true. P-values tell us about P(data | H‚ÇÄ), not P(H‚ÇÄ | data). The null hypothesis is either true or false‚Äîit doesn't have a probability in the frequentist framework.

      **Part (b): CORRECT ‚úì**

      This is the correct definition of a p-value. It's a conditional probability: given that H‚ÇÄ is true, what's the probability of data this extreme?

      **Part (c): INCORRECT ‚úó**

      The p-value assumes H‚ÇÄ is true in its calculation. It doesn't measure the probability that chance alone explains the results. Also, "random chance" is vague and misleading.

      **Part (d): INCORRECT ‚úó**

      The probability of Type I error is Œ± (the significance level), not the p-value. Œ± is set before the study; the p-value is calculated from the data.

      **Part (e): INCORRECT ‚úó**

      Hypothesis testing never "proves" anything. A small p-value provides evidence against H‚ÇÄ, but doesn't prove H‚Çê. Also, 1 - p-value ‚â† confidence in H‚Çê.

      **Part (f): CORRECT ‚úì**

      This is another valid way to express the p-value's meaning‚Äîthe long-run frequency interpretation under repeated sampling when H‚ÇÄ is true.

----

.. admonition:: Exercise 2: Common Misconceptions
   :class: note

   A research article states: "The treatment group showed significant improvement (p = 0.04). Therefore, there is only a 4% chance that the null hypothesis is true, and we can be 96% confident that the treatment works."

   a. Identify at least three errors in this statement.

   b. Rewrite the conclusion correctly.

   c. What additional information would strengthen the conclusion?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Errors in the statement**

      1. **"4% chance that H‚ÇÄ is true"**: The p-value is not P(H‚ÇÄ true). It's P(data this extreme | H‚ÇÄ true).

      2. **"96% confident the treatment works"**: Confidence levels apply to intervals, not hypothesis tests. Also, 1 - p-value ‚â† confidence in the alternative.

      3. **Implies proof**: "Therefore" suggests logical certainty. Statistical significance doesn't prove causation or even that H‚ÇÄ is false‚Äîit could be a Type I error.

      4. **Missing effect size**: Statistical significance doesn't tell us if the effect is practically meaningful.

      5. **Missing context**: What was the sample size? A small p-value with a huge sample might reflect a tiny, clinically irrelevant effect.

      **Part (b): Corrected conclusion**

      "The data does give support (p-value = 0.04) to the claim that the treatment group showed improvement compared to the control group. If the treatment had no effect, we would expect to see results this extreme or more in about 4% of similar studies."

      **Part (c): Additional information needed**

      - **Effect size**: How large was the improvement? Is it clinically meaningful?
      - **Sample size**: Were there enough participants for adequate power?
      - **Confidence interval**: What range of effect sizes is plausible?
      - **Study design**: Was this randomized? Double-blind? Any potential confounders?

----

.. admonition:: Exercise 3: Writing Proper Conclusions
   :class: note

   For each scenario, write a proper conclusion in context.

   a. Testing H‚ÇÄ: Œº ‚â§ 50 vs H‚Çê: Œº > 50, p-value = 0.008, Œ± = 0.05, for mean customer satisfaction score.

   b. Testing H‚ÇÄ: Œº = 100 vs H‚Çê: Œº ‚â† 100, p-value = 0.12, Œ± = 0.05, for mean IQ of a sample.

   c. Testing H‚ÇÄ: Œº ‚â• 30 vs H‚Çê: Œº < 30, p-value = 0.03, Œ± = 0.01, for mean minutes to complete a task.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): p = 0.008 < Œ± = 0.05 ‚Üí Reject H‚ÇÄ**

      "The data does give support (p-value = 0.008) to the claim that the true mean customer satisfaction score exceeds 50."

      **Part (b): p = 0.12 > Œ± = 0.05 ‚Üí Fail to reject H‚ÇÄ**

      "The data does not give support (p-value = 0.12) to the claim that the true mean IQ differs from 100."

      *Note: We do NOT say "the mean IQ equals 100" or "we accept that mean IQ is 100."*

      **Part (c): p = 0.03 > Œ± = 0.01 ‚Üí Fail to reject H‚ÇÄ**

      "The data does not give support (p-value = 0.03) to the claim that the true mean time to complete the task is less than 30 minutes."

      *Note*: At Œ± = 0.01, p-value = 0.03 > 0.01 means we fail to reject. The data might give support at Œ± = 0.05, but not at the stricter Œ± = 0.01 level.

      *Note: Even though p = 0.03 is small, it exceeds our stringent Œ± = 0.01 threshold. At Œ± = 0.05, we would have rejected.*

----

.. admonition:: Exercise 4: "Fail to Reject" vs "Accept"
   :class: note

   A student concludes: "Since p = 0.45 is very large, we accept the null hypothesis that the mean equals 100."

   a. What is wrong with this conclusion?

   b. Why do we say "fail to reject H‚ÇÄ" instead of "accept H‚ÇÄ"?

   c. Give three possible reasons why a test might fail to reject H‚ÇÄ even when H‚ÇÄ is actually false.

   d. Rewrite the student's conclusion correctly.

   .. dropdown:: Solution
      :class-container: sd-border-success

      .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/Exercises/ch10-4/fig12_decision_outcomes.png
         :align: center
         :width: 80%
         :alt: Decision outcomes table

         The four possible outcomes in hypothesis testing: correct decisions vs. errors.

      **Part (a): What's wrong**

      We never "accept" the null hypothesis. A large p-value means the data are consistent with H‚ÇÄ, but it doesn't prove H‚ÇÄ is true. Absence of evidence is not evidence of absence.

      **Part (b): Why "fail to reject"**

      - H‚ÇÄ is assumed true from the start‚Äîwe can't "accept" what we already assumed
      - A large p-value could result from H‚ÇÄ being true OR from insufficient power to detect a false H‚ÇÄ
      - The burden of proof is on rejecting H‚ÇÄ, not confirming it
      - "Accept" implies certainty that isn't justified

      **Part (c): Three reasons for failing to reject a false H‚ÇÄ**

      1. **Sample size too small**: Insufficient power to detect the true effect
      2. **Effect size too small**: The true difference exists but is subtle
      3. **High variability**: Large œÉ obscures the signal
      4. **Bad luck**: Random sampling variation produced an unrepresentative sample (Type II error)

      **Part (d): Corrected conclusion**

      "The data does not give support (p-value = 0.45) to the claim that the mean differs from 100. The data are consistent with a mean of 100, but we cannot rule out other values."

----

.. admonition:: Exercise 5: Effect of Sample Size on Significance
   :class: note

   Consider testing H‚ÇÄ: Œº = 100 vs H‚Çê: Œº ‚â† 100 with :math:`\bar{x} = 102` and œÉ = 10.

   a. Calculate the p-value for n = 25, 50, 100, 200, 400.

   b. At what sample size does the result first become significant at Œ± = 0.05?

   c. The true difference (102 - 100 = 2) is the same in all cases. What does this tell you about the relationship between statistical significance and practical significance?

   d. A researcher with n = 400 reports "highly significant results (p < 0.001)." What caution should readers exercise?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): P-values at different sample sizes**

      For each n: :math:`z_{TS} = \frac{102 - 100}{10/\sqrt{n}} = \frac{2\sqrt{n}}{10} = 0.2\sqrt{n}`

      .. list-table::
         :header-rows: 1

         * - n
           - z_TS
           - p-value (two-tailed)
         * - 25
           - 1.00
           - 0.3173
         * - 50
           - 1.41
           - 0.1573
         * - 100
           - 2.00
           - 0.0455
         * - 200
           - 2.83
           - 0.0047
         * - 400
           - 4.00
           - 0.00006

      **Part (b): First significant at Œ± = 0.05**

      The result first becomes significant at **n = 100** (p = 0.0455 < 0.05).

      **Part (c): Statistical vs practical significance**

      .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/Exercises/ch10-4/fig10_pvalue_vs_n.png
         :align: center
         :width: 80%
         :alt: P-value decreases with sample size

         With the same effect size, p-value decreases as sample size increases.

      The effect size (2 points) is constant, but statistical significance depends heavily on sample size. With a large enough sample, even tiny, practically meaningless differences become statistically significant.

      **Key insight**: Statistical significance ‚â† practical importance. Always consider:
      
      - Effect size (how big is the difference?)
      - Confidence interval (what's the range of plausible values?)
      - Context (does this difference matter in practice?)

      **Part (d): Caution with large n**

      With n = 400, even a small effect (2 points out of 100) produces a highly significant p-value. Readers should:

      - Ask about the **effect size** and whether it's meaningful
      - Look at the **confidence interval** (which would be narrow: approximately 101 to 103)
      - Consider whether a 2% increase is practically important
      - Not confuse "highly significant" with "large effect"

      **Best practice**: Always report the estimated effect and a confidence interval, not just the p-value. A complete report would state: "The mean was 102 (95% CI: 101.0 to 103.0), representing a 2-point increase (p < 0.001)."

      **R verification:**

      .. code-block:: r

         xbar <- 102; mu_0 <- 100; sigma <- 10
         n_values <- c(25, 50, 100, 200, 400)
         
         for (n in n_values) {
           z_ts <- (xbar - mu_0) / (sigma / sqrt(n))
           p_val <- 2 * pnorm(abs(z_ts), lower.tail = FALSE)
           cat("n =", n, ": z =", round(z_ts, 2), ", p =", round(p_val, 4), "\n")
         }

----

.. admonition:: Exercise 6: Identifying P-value Hacking
   :class: note

   For each scenario, identify whether p-value hacking (or questionable research practices) may be occurring. Explain why.

   a. A researcher tests 20 different outcome variables and reports only the one with p < 0.05.

   b. A researcher collects data until p < 0.05, then stops.

   c. A researcher removes "outliers" that make the p-value larger.

   d. A researcher pre-registers their hypothesis, sample size, and analysis plan before collecting data.

   e. A researcher switches from a two-tailed to a one-tailed test after seeing the data.

   f. A researcher reports "marginally significant results (p = 0.08)" as evidence supporting their hypothesis.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Testing 20 outcomes, reporting 1 ‚Üí P-VALUE HACKING**

      With 20 independent tests at Œ± = 0.05, we expect about 1 false positive even if all null hypotheses are true. Selective reporting inflates the false positive rate. This should use multiple comparison corrections.

      **Part (b): Collecting until p < 0.05 ‚Üí P-VALUE HACKING**

      "Optional stopping" inflates Type I error. The longer you keep sampling, the more likely you'll eventually get p < 0.05 by chance. Sample size should be determined before data collection.

      **Part (c): Removing "outliers" to improve p ‚Üí P-VALUE HACKING**

      If outlier removal is based on its effect on p-value (rather than pre-specified criteria like data entry errors), this is manipulation. Legitimate outlier handling should be decided before seeing results.

      **Part (d): Pre-registration ‚Üí GOOD PRACTICE ‚úì**

      Pre-registering hypotheses and analysis plans prevents many forms of p-hacking. It makes the research transparent and ensures the analysis wasn't tailored to get desired results.

      **Part (e): Switching to one-tailed after seeing data ‚Üí P-VALUE HACKING**

      The test type should be determined by the research question before seeing data. Switching to one-tailed after observing the direction essentially halves the p-value, inflating false positives.

      **Part (f): "Marginally significant" p = 0.08 ‚Üí QUESTIONABLE**

      While not strictly p-hacking, this represents "spin"‚Äîframing non-significant results as if they're meaningful. The result did not meet the stated threshold. It's honest to say "we found no significant effect (p = 0.08)" rather than implying near-success.

----

.. admonition:: Exercise 7: Complete Four-Step Analysis
   :class: note

   A coffee shop claims their large cups contain at least 16 oz of coffee. A consumer group suspects the cups are underfilled. They purchase n = 25 large coffees and measure:

   - :math:`\bar{x} = 15.6` oz
   - s = 1.2 oz

   Conduct a complete hypothesis test at Œ± = 0.05 using the four-step framework.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Step 1: Define the Parameter**

      Let Œº = true mean volume of coffee (oz) in large cups at this coffee shop.

      **Step 2: State the Hypotheses**

      The consumer group suspects underfilling (less than claimed):

      .. math::
         H_0: \mu \geq 16 \quad \text{vs} \quad H_a: \mu < 16

      This is a lower-tailed test.

      **Step 3: Calculate Test Statistic and P-value**

      Since œÉ is unknown, use t-test. df = 25 - 1 = 24.

      .. math::
         t_{TS} = \frac{15.6 - 16}{1.2 / \sqrt{25}} = \frac{-0.4}{0.24} = -1.667

      P-value (lower-tailed):

      .. math::
         p\text{-value} = P(T_{24} < -1.667) = 0.0543

      **Step 4: Decision and Conclusion**

      Since p-value = 0.0543 > Œ± = 0.05, **fail to reject H‚ÇÄ**.

      **Conclusion**: The data does not give support (p-value = 0.054) to the claim that the mean coffee volume is less than 16 oz. While the sample mean (15.6 oz) is below the claimed amount, the difference is not statistically significant. The consumer group cannot definitively conclude underfilling based on this sample.

      **Note**: The p-value is very close to 0.05. A slightly larger sample might yield significant results. The 95% UCB would be useful here.

      **R verification:**

      .. code-block:: r

         xbar <- 15.6; mu_0 <- 16; s <- 1.2; n <- 25
         t_ts <- (xbar - mu_0) / (s / sqrt(n))  # -1.667
         df <- n - 1  # 24
         p_value <- pt(t_ts, df, lower.tail = TRUE)  # 0.0543

----

.. admonition:: Exercise 8: Interpreting Non-significant Results
   :class: note

   A clinical trial tests whether a new drug lowers blood pressure more than a placebo. With n = 30 patients per group, they find p = 0.15.

   a. State the formal conclusion at Œ± = 0.05.

   b. A journalist writes: "Study proves new drug doesn't work." Is this accurate? Why or why not?

   c. What are possible explanations for the non-significant result?

   d. What would you recommend for follow-up research?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Formal conclusion**

      At the 5% significance level, **fail to reject H‚ÇÄ**. The data does not give support (p-value = 0.15) to the claim that the new drug lowers blood pressure more than the placebo.

      **Part (b): Journalist's interpretation**

      **Not accurate.** "Proves doesn't work" overstates what we can conclude. Non-significance means:

      - We failed to detect an effect, NOT that no effect exists
      - The study may have been underpowered
      - There could be a real but small effect

      Better headline: "Study fails to find evidence that new drug works" or "Results inconclusive for new blood pressure drug."

      **Part (c): Possible explanations**

      1. **Drug truly doesn't work** (H‚ÇÄ is true)
      2. **Underpowered study**: n = 30 per group may be too small to detect a real effect
      3. **Effect size smaller than expected**: The drug works but the effect is subtle
      4. **High variability**: Individual differences in blood pressure response may obscure the drug effect
      5. **Wrong outcome measure**: Perhaps the drug works but takes longer than the study period
      6. **Type II error**: Random chance produced an unrepresentative sample

      **Part (d): Recommendations for follow-up**

      - **Power analysis**: Determine sample size needed to detect a clinically meaningful effect
      - **Larger study**: Conduct a trial with adequate power (perhaps n = 100+ per group)
      - **Report effect size and CI**: Even if non-significant, show the estimated effect and uncertainty
      - **Meta-analysis**: Combine with other studies if available
      - **Explore subgroups**: Some patient populations may respond differently

----

.. admonition:: Exercise 9: Comprehensive Application
   :class: note

   A university wants to evaluate whether a new tutoring program improves student performance. They randomly assign 40 students to receive tutoring and compare their exam scores to the historical mean of Œº‚ÇÄ = 72.

   Results:
   
   - n = 40
   - :math:`\bar{x} = 75.8`
   - s = 12.5

   a. Perform a complete hypothesis test at Œ± = 0.05.

   b. Calculate and interpret a 95% confidence interval.

   c. A skeptic says: "A 3.8-point increase is trivial. This result is not practically significant." How would you respond?

   d. Another researcher notes: "But the p-value is barely below 0.05. This is weak evidence." Evaluate this criticism.

   e. What additional information would help evaluate the tutoring program's effectiveness?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Hypothesis Test**

      **Step 1**: Let Œº = true mean exam score for students receiving tutoring.

      **Step 2**: H‚ÇÄ: Œº ‚â§ 72 vs H‚Çê: Œº > 72 (upper-tailed test)

      **Step 3**:

      .. math::
         t_{TS} = \frac{75.8 - 72}{12.5 / \sqrt{40}} = \frac{3.8}{1.976} = 1.923

      df = 39

      P-value = P(T‚ÇÉ‚Çâ > 1.923) = 0.031

      **Step 4**: Since 0.031 < 0.05, **reject H‚ÇÄ**.

      At the 5% significance level, **reject H‚ÇÄ**. The data does give support (p-value = 0.031) to the claim that students receiving tutoring have a higher mean exam score than the historical average of 72.

      **Part (b): 95% Confidence Interval**

      t‚ÇÄ.‚ÇÄ‚ÇÇ‚ÇÖ,‚ÇÉ‚Çâ = 2.023

      .. math::
         CI: 75.8 \pm 2.023 \times 1.976 = 75.8 \pm 4.0 = (71.8, 79.8)

      **Interpretation**: We are 95% confident that the true mean exam score for tutored students is between 71.8 and 79.8 points.

      **Part (c): Practical significance response**

      The skeptic raises a valid point. Statistical significance ‚â† practical importance. To evaluate:

      - **Context matters**: Is 3.8 points educationally meaningful? On a 100-point exam, it's nearly 4%‚Äîpotentially the difference between letter grades.
      - **Cost-benefit**: How much does the tutoring cost? If inexpensive, even a modest improvement may be worthwhile.
      - **Effect size**: Cohen's d ‚âà 3.8/12.5 = 0.30, a "small-to-medium" effect by conventional standards.
      - **The CI (71.8, 79.8)** suggests the true effect could range from essentially zero to nearly 8 points.

      **Part (d): "Barely significant" criticism**

      This is a fair observation:

      - P = 0.031 is not far from 0.05; with slightly different data, we might not reject
      - The evidence, while meeting the threshold, is not overwhelming
      - This underscores why we shouldn't treat Œ± = 0.05 as a magic cutoff
      - Replication would strengthen confidence in the finding

      **Part (e): Additional information needed**

      - **Control group**: Were non-tutored students tested simultaneously? (Historical comparison is weaker than randomized control)
      - **Random assignment verification**: How were students assigned to tutoring?
      - **Baseline scores**: Were groups comparable before tutoring?
      - **Long-term effects**: Do improvements persist?
      - **Cost analysis**: What's the cost per point of improvement?
      - **Subgroup analysis**: Do some students benefit more than others?
      - **Multiple outcomes**: What about other measures (retention, satisfaction)?

      **R verification:**

      .. code-block:: r

         xbar <- 75.8; mu_0 <- 72; s <- 12.5; n <- 40
         SE <- s / sqrt(n)  # 1.976
         t_ts <- (xbar - mu_0) / SE  # 1.923
         df <- n - 1  # 39
         
         # P-value
         pt(t_ts, df, lower.tail = FALSE)  # 0.031
         
         # 95% CI
         t_crit <- qt(0.025, df, lower.tail = FALSE)  # 2.023
         c(xbar - t_crit * SE, xbar + t_crit * SE)  # (71.8, 79.8)

----

Additional Practice Problems
----------------------------

**True/False Questions** (1 point each)

1. A p-value of 0.001 proves that H‚ÇÄ is false.

   ‚ìâ or ‚íª

2. A large p-value (e.g., 0.60) means we should accept H‚ÇÄ.

   ‚ìâ or ‚íª

3. Statistical significance always implies practical significance.

   ‚ìâ or ‚íª

4. P-value hacking inflates the Type I error rate.

   ‚ìâ or ‚íª

5. With a large enough sample, virtually any small effect can become statistically significant.

   ‚ìâ or ‚íª

6. Pre-registration of studies helps prevent p-value hacking.

   ‚ìâ or ‚íª

**Multiple Choice Questions** (2 points each)

7. A p-value is defined as:

   ‚í∂ The probability that H‚ÇÄ is true
   
   ‚í∑ The probability of the observed data, assuming H‚ÇÄ is true
   
   ‚í∏ The probability of data at least as extreme as observed, assuming H‚ÇÄ is true
   
   ‚íπ The probability of making a Type I error

8. "Failing to reject H‚ÇÄ" means:

   ‚í∂ H‚ÇÄ is proven true
   
   ‚í∑ H‚Çê is proven false
   
   ‚í∏ There is insufficient evidence to conclude H‚Çê
   
   ‚íπ The test was conducted incorrectly

9. Which is an example of p-value hacking?

   ‚í∂ Pre-registering hypotheses before data collection
   
   ‚í∑ Using Bonferroni correction for multiple tests
   
   ‚í∏ Removing outliers that make p larger
   
   ‚íπ Reporting non-significant results

10. A researcher finds p = 0.04 with n = 10,000. Which is the best interpretation?

    ‚í∂ The effect is definitely large and important
    
    ‚í∑ The result is statistically significant; effect size should be examined
    
    ‚í∏ The null hypothesis is proven false
    
    ‚íπ The test was too conservative

11. A proper conclusion when p = 0.08 and Œ± = 0.05 is:

    ‚í∂ "We accept H‚ÇÄ"
    
    ‚í∑ "The results are marginally significant"
    
    ‚í∏ "There is insufficient evidence to reject H‚ÇÄ"
    
    ‚íπ "The alternative hypothesis is false"

12. Statistical significance depends on all EXCEPT:

    ‚í∂ Sample size
    
    ‚í∑ Effect size
    
    ‚í∏ Population size
    
    ‚íπ Variability in the data

.. dropdown:: Answers to Practice Problems
   :class-container: sd-border-success

   **True/False Answers:**

   1. **False** ‚Äî No p-value can "prove" anything. Small p-values provide evidence against H‚ÇÄ but could be Type I errors.

   2. **False** ‚Äî We never "accept" H‚ÇÄ. Large p-values mean insufficient evidence to reject.

   3. **False** ‚Äî A small p-value can arise from large n even when the effect is trivially small.

   4. **True** ‚Äî By selectively reporting or manipulating to get p < Œ±, the actual false positive rate exceeds Œ±.

   5. **True** ‚Äî SE = œÉ/‚àön decreases with n, so even tiny effects become significant with large n.

   6. **True** ‚Äî Pre-registration commits researchers to their analysis plan before seeing data.

   **Multiple Choice Answers:**

   7. **‚í∏** ‚Äî P-value = P(data at least as extreme as observed | H‚ÇÄ true).

   8. **‚í∏** ‚Äî Failing to reject means the evidence was insufficient, not that H‚ÇÄ is true.

   9. **‚í∏** ‚Äî Removing outliers based on their effect on p-value is manipulation.

   10. **‚í∑** ‚Äî With n = 10,000, statistical significance is easy to achieve; effect size matters more.

   11. **‚í∏** ‚Äî This is the proper phrasing when p > Œ±.

   12. **‚í∏** ‚Äî Population size doesn't affect significance (except in finite population corrections rarely used). Sample size, effect size, and variability all matter.